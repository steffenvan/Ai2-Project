<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986281">
The ARC A3 Project:
Terminology Acquisition Tools: Evaluation Method and Task
</title>
<author confidence="0.837828">
Widad Mustafa El Hadi I s m a ï l Timimi Annette Béguin Marcilio De Brito
</author>
<bodyText confidence="0.22309075">
mustafa@univ-lille3.fr timimi@univ-lille3.fr beguin@univ-lille3.fr mdebrito@noos.fr
UFR IDIST &amp; CERSATES (CNRS UMR 8529)
Université Charles De Gaulle, Lille 3
BP 149, F-59 653 Villeneuve D&apos;Ascq, France
</bodyText>
<sectionHeader confidence="0.849916" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916205882353">
This paper describes the work achieved in
the Concerted Research Project ARC A3
supported and coordinated by the AUF1,
former Aupelf-Uref2. The project deals
with the evaluation of term and semantic
relation extraction from corpora in French.
Eight participants, both from public
institutions and industrial corporations
were involved in this project and were
responsible for producing corpora suitable
for extraction tasks and elaborating a
protocol in order to evaluate objectively
terminology acquisition tools. This
expression covers respectively, term
extractors, classifiers and semantic relation
extractors. The paper also reports on the
methodology used for comparing four term
extractors, one classifier and three
semantic relation extractors during the
2000 evaluation campaign. There are also
several by-products of this campaign: first,
two corpora which can be used for NLP
system development and evaluation as the
AUF recommended; and then terminology
products: for each corpus a list of terms
characterizing the field is available. We are
not giving details about the results but
rather an assessment of what the evaluation
of Terminology Extraction Tools is: how
was it done, what were the difficulties,
which are the advantages and
disadvantages of the adopted protocol,
what are the limits and how should we
proceed for future testing.
</bodyText>
<note confidence="0.598057">
1 The Association des Universités Francophones
</note>
<footnote confidence="0.992537">
2 AUPELF is the &amp;quot;Association des Universités Entièrement
ou Partiellement de Langue Française&amp;quot;, an NGO whose
mission is to promote the dissemination of French as a
scientific medium.
</footnote>
<sectionHeader confidence="0.455928" genericHeader="method">
1 The ARC A3 Program
</sectionHeader>
<bodyText confidence="0.997970130434783">
ARC A3 is a project of the ILEC3 group
coordinated and founded by AUF. It was started
in 1995 in order to promote research in the field
of terminology acquisition. The ARC A3,
“Term and Semantic Relation Extraction from
Corpora in French” project aim is to test
software capabilities in term and semantic
relation extraction from corpora in French.
Systems submitted to this evaluation are
designed by French and Canadian research
institutions (National Scientific Research Center
and Universities) and/or private businesses.
These systems have been extensively described
in our previous work (cf. Béguin, et al., 1997,
2000; Jouis et al., 1997; Mustafa El Hadi et al.,
1996a, 1996b, 1997 &amp; 1998;). The first phase of
the project has been directed towards testing the
systems on one corpus4 (trial run) and towards
elaborating a workable protocol based on this
experience. The first results were presented
during the first conference of JST5 (cf. Béguin et
al., 1997, 2000). This article reports on the
second and final evaluation campaign.
</bodyText>
<sectionHeader confidence="0.799953" genericHeader="method">
2 ARC A3 Organization
</sectionHeader>
<bodyText confidence="0.999734444444444">
ARC A3 brings together four kinds of actors: a
coordinator who plays an organizational role
(schedule, quality control of corpora, data
production, etc.), corpora providers; participants
of the test and two scientific advisors. The
action has been coordinated by the University of
Lille 3. The organizing team in cooperation with
the discussion group made up of representatives
of each participating team and two scientific
</bodyText>
<footnote confidence="0.890584666666667">
3 Ingénierie de la Langue, Linguistique-informatique et
Corpus écrits.
4 SPIRALE, a periodical dealing with education and
pedagogy issues. Each periodical sizes around 200 pages.
5 Journées Scientifiques e Techniques de Francil, Avignon,
France, 1997.
</footnote>
<bodyText confidence="0.973539">
advisors are supposed to co-operate in defining a
methodology for testing the systems.
</bodyText>
<subsectionHeader confidence="0.996835">
2.1 Participating Systems
</subsectionHeader>
<bodyText confidence="0.999183615384615">
The systems are designed by French and
Canadian research institutions. There were ten
registered participants at the beginning of the
project and three withdrew later for a variety of
reasons. The organizers then launched another
call for participation in July 1999 and three
more participants joined the project (two private
enterprises (Xerox and Logos) and a public
institution (the University of Grenoble). Logos
and the University team later dropped out for
reasons unrelated to the program. When the final
campaign was launched in 2000 there were eight
systems remaining under evaluation.
</bodyText>
<figureCaption confidence="0.942642">
Fig. 1. Participating Systems
</figureCaption>
<note confidence="0.836377888888889">
Software Affiliation
Acabit IRIN (Nantes)
Ana IRIN (Nantes)
Conterm LANCI (Montréal)
Iota CLIPS-IMAG (Grenoble)
Lexter ERSS (Toulouse)
Seek-Java CAMS-LALIC (Paris 4)
Loria6 LORIA (Nancy)
Xerox-Termfinder XEROX (Grenoble)
</note>
<subsectionHeader confidence="0.991515">
2.2 Overview of the Tested Tools
</subsectionHeader>
<bodyText confidence="0.999922777777778">
Terminology plays a major role in information
processing and management and in specialized
communication. Its role has been enhanced by
the spread of automation and by the availability
of electronic corpora. These two factors have
had a massive impact on many different
applications: systematic terminology7 building,
natural-language interface design, lexical units
management for specific use in some sub-
</bodyText>
<footnote confidence="0.863689857142857">
6 This name is used for practical reasons since no final
software name has yet been chosen
7 A holistic list of terms drawn from a representative corpus
characterizing and describing a field of knowledge. In
order to be of any use this type of list must be subject to a
structuring which is an important step towards exploiting
extraction results.
</footnote>
<bodyText confidence="0.999905230769231">
languages and technical writing, thesaurus
construction, translation and indexing as well as
the recent growth of cross-language information
retrieval (CLIR).
If we focus on the tools, presented in our
evaluation project, from the point of view of
their functions and of the purposes for which
they were designed), there are three categories:
&amp;quot;Term Extractors&amp;quot;, &amp;quot;Classifying Tools&amp;quot;, and
&amp;quot;Semantic relations extraction tools&amp;quot;. As we
already mentioned, these systems were
extensively described in our previous
publications.
</bodyText>
<subsectionHeader confidence="0.9773">
2.2.1 Term Extractors (TE)
</subsectionHeader>
<bodyText confidence="0.999966785714286">
We will briefly describe the basic idea
underlying TE tools. Most of the extracting tools
consider terms as noun phrases. Systems
identify terms by using frequency, distribution
and category-pattern matching (Daille et al.
1995; Dagan, 1996; Lauriston, 1994). All lexical
units contained in a given text are analyzed and
matched to patterns (typical forms of
terminological units) described in rules. More
term extractors are accounted for elsewhere
(L&apos;Homme, 1996; Kageura et al., 1996; Dagan
et al., 1994). Some of the systems described by
these authors are tested in the framework of our
valuation project (Acabit, Lexter, and Ana).
</bodyText>
<subsectionHeader confidence="0.9011505">
2.2.2 Classifiers and Semantic Relation
Extractors (SRE)
</subsectionHeader>
<bodyText confidence="0.999694463414634">
Terminology resources are increasingly seen as
structured data i.e. as a network of terms
organized by relations. Pure alphabetical lists
can hardly be used except for bilingual reference
tools. The variety of tools, their functions and
the different possible uses offered within the
framework of ARC A3 shows this need.
Consequently such lists of terms are quite
difficult to evaluate except by specialists in the
relevant fields which makes it a rather
constraining process.
Structuring terms by semantic relations or in
classes is useful for the following applications:
Index-making for on-line technical
documentation; browsing; information access
and retrieval; building thesaurus and ontologies
for information systems.
Many applications and extraction methods
relevant to these tools have been described in
the literature. The systems tested in the AUF
framework are geared towards a variety of
applications ranging from rough semantic
relation extraction, through indexing, thesaurus
construction to knowledge-based system
modeling (see figure 2).
Classifiers and semantic relation extractors are
tested within the same framework as the one
used for evaluating term extractors. The first
category is characterized as classifying tools.
Their role is to build classes of networks of
terms linked to a major one. This category
consists of statistical and/or connectionist
models such as Conterm. It is the only classifier
tested within the framework of this campaign.
The second category includes semantic relation
extractors which focus particularly on semantic
relations (Iota, Loria and Seek-Java). A
complete description of all the systems which
were tested (main characteristics and purposes,
description as far as approaches are concerned)
is documented in previous work.
</bodyText>
<sectionHeader confidence="0.828548" genericHeader="method">
3 Evaluation paradigm
</sectionHeader>
<bodyText confidence="0.9998873">
Evaluation activities are a corollary of the quick
development of NLP tools in general and of
terminology extraction in particular. It thus
became necessary to evaluate these tools on
objectively based criteria in order to have a clear
picture of the state-of-the-art, assess the needs in
this sector and hence promote research in this
specific field. Moreover, the principal aim of
existing testing methods, as reported in the
literature, is to come across software errors and
then try to adapt them for a particular user
environment.
Evaluation paradigm is basically dependant
upon two major steps: (i) Creation of textual
data: raw or tagged corpora and test material. A
corpus-based research is part of the
infrastructure for the development of advanced
language processing applications; (ii) Test and
comparison of systems on a similar data
(Cavazza, 1993; Adda et al., 2000).
</bodyText>
<subsectionHeader confidence="0.995592">
3.1 The ARC A3 Evaluation Approach
</subsectionHeader>
<bodyText confidence="0.9997425">
The approach we adopted is a black-box
qualitative approach8 The results are compared
</bodyText>
<footnote confidence="0.5624084">
8 This approach is adopted and validated by the vast
majority of participants to the test in June 1999. The
organizers have slightly adapted the protocol because more
participants joined the ARC after the validation of the
protocol.
</footnote>
<bodyText confidence="0.995878017857143">
with the human performance of a task (either
experts examining results or using reference lists
or both). Moreover comparisons are made with
other systems performing the same task. The
results are finally calculated and translated in
terms of traditional IR measures9.
The conventional distinction between black-box
and glass-box is the following: the former
considers only system input-out-put relations
without regard to the specific mechanisms by
which the outputs were obtained while the latter
examines the mechanisms linking input and
output. (Sparck-Jones, 1996 p. 26; King, 1996;
1999, among many others).
The qualitative evaluation measures as described
by Sparck-Jones 1996, pp. 61-122, are based on
observation or interviewing and are broadly
designed to obtain a more holistic, less reductive
or fragmented view of the situation. It is
moreover more naturalistic. This type of
evaluation naturally fits an end-free style. In our
case the quality of the results is evaluated by
domain experts. We distinguish two types of
experts: experts for the three applications tested
(systematic terminology, translation and
indexing); and experts in the two domains of
corpora (biotechnology and pedagogy).
Both quantitative and qualitative approaches are
goal-oriented, that is focusing on discrepancies
between performance results and initial system
requirements. Sparck-Jones points out how the
two types of measures are deeply interwoven
although different in their nature:
- Recall is a quantitative measure of system
performance while
- Declared Satisfaction is a qualitative one (i.e.
such a measure is really qualitative even if the
result of applying it to a set of users is a
percentage figure).
The qualitative approach in the evaluation
process is the easiest one for end users. It means
giving a value judgment on how the system
globally works (Cavazza, 1993; Chaudiron,
2000). The dominant approach today is towards
quantitative evaluations which are considered as
more objective and reproducible than the
qualitative approach (EAGLES-1 1996; ISLE
2001). The main attempt of these approaches is
9 We chose to accompany the qualitative approach (mainly
based on manual evaluations) by a translation of the
manual evaluations into numerical scales of values (see
below for more details).
to translate the concepts of relevance and quality
into numerical data. Statistical approaches such
as MUC 2 and TREC 3 are frequently used for
this type of evaluation. (Chaudiron, 2000).
</bodyText>
<subsectionHeader confidence="0.776831">
3.1.1 The merits of a black-box evaluation
</subsectionHeader>
<bodyText confidence="0.999694225">
Obviously this approach has its pros and cons.
But it can be justified on the following basis:
- Since most developers cannot provide us (as
test organizers) with their systems, the only way
was to send them the text corpora and let them
provide us with the results. A glass-box
evaluation would have required an examination
of the systems by the organizers which would
have been impossible except for Xerox’s
TermFinder and Logos System’s Knowledge
Discovery, two commercialized systems.
- Even if this approach may be criticized on
account of its subjective side, end-users like it
because of its usefulness when comparing two
or more systems which differ in all their
parameter settings. (Chaudiron 2000; Cavazza
1993).
- A black-box evaluation is more oriented
towards system’s end-user when compared to a
glass-box evaluation. For the latter the test will
involve analyzing the system’s functioning by
looking at its different components. Each
component is evaluated separately in itself. Such
an approach allows for spotting and
understanding the causes of dysfunctional
results. It is a long term process which requires
access to the internal parts of the system and an
understanding of the architecture and global
strategy of the software. This is obviously a
developer oriented approach and not an end-
user one (Chaudiron 2000; Cavazza 1993).
- In spite of its limited scope the evaluation
protocol we adopted is used in more
complicated NLP tools, such as MT tools.
Evaluators examine the systems’ output without
considering the differences between them (cf.
L’Homme, 2001). Last Spring our team took
part in a workshop organized by ISSCO
(University of Geneva) where we and all the
other participants adopted this approach.
</bodyText>
<subsectionHeader confidence="0.725375333333333">
3.2 Elements of the Evaluation Protocol of
the 2000 Campaign
3.2.1 Evaluation Task
</subsectionHeader>
<bodyText confidence="0.999995631578947">
The extraction of terms, of classes and of
semantic relations was necessary to test the tools
performance in the three following tasks:
Systematic terminology (characterizing the
tested corpora); (ii) Translation; (iii) Indexing.
This means in practice: what is the relevance of
terms, classes and semantic relations provided
by the systems being tested? Do the terms,
classes and semantic relations satisfy minimum
requirements? Do we need to define a minimum
level of terms, classes, semantic production?
Are discrepancies meaningful? For example, it
could be that most of the systems being tested
are having qualitatively poor outputs, while only
one or two produce worthwhile results. Within
this perspective the idea was to submit the
results to specialists. We distinguished for the
purpose of this campaign two types of human
expertise as we mentioned above.
</bodyText>
<subsectionHeader confidence="0.98696">
3.2.2 Test material
</subsectionHeader>
<bodyText confidence="0.999851166666667">
Evaluation data can normally be divided into
two different categories (i) representative
samples of the tested corpora (ii) test material,
which, in our evaluation framework, is made up
of both custom-designed lists and real life lists /
thesaurus.
</bodyText>
<subsectionHeader confidence="0.910639">
3.2.2.1 Corpus
</subsectionHeader>
<bodyText confidence="0.999929166666667">
Two corpora were tested: Spirale10 and INRA11.
We have chosen a sample representing 10% of
each corpus: for Spirale n° 19 was chosen. As
for INRA corpus, the providers of this corpus
suggested 8 articles (603, 604, 607, 609, 631,
666, 732, 740).
</bodyText>
<subsectionHeader confidence="0.952832">
3.2.2.2 Reference Lists
</subsectionHeader>
<bodyText confidence="0.9996677">
These lists are standard human professional
results which can be used as performance
exemplars or norms for comparison. This type
of data is considered to be a gold standard (see
SensEval, Kilgarrif 1998; ISLE 2001).
For the INRA corpus the following lists have
been created:
For translation two lists were processed (i) a list
created by a novice translator (ii) another one by
a confirmed professional translator.
</bodyText>
<page confidence="0.692622">
10 423 texts, 16 mega bytes
11 51 texts, 2,2 mega bytes.
</page>
<bodyText confidence="0.996641319148936">
For indexing: six lists were created both by
professional and by non professional indexers.
We are not developing these lists in this paper
given the limited scope of this type of evaluation
from an indexing point of view. Hence the
limited interest of term extraction tools for
human indexing. We will however comment on
the terminology lists provided by the two corpus
providers, INRA (Institut National pour la
Recherche Agronomique i.e. National Institute
for Agronomic Research), the Francis list of
INIST12) and the translation lists.
As far as INRA corpus is concerned:
We think that our evaluation task could have
given better results if the lists had been more
representative of a systematic terminology
activity. For the INRA corpora, for example,
only 113 terms were chosen by the experts to
represent their terminology. Our estimation is
that, 113 terms only constitute a poor
representation of an activity. It would have been
a good idea to have specialists establish the lists
of terms and to compare those to the systems’
output. Even if this work is time consuming it
makes for a better evaluation of the systems’
productivity. As far as indexing is concerned the
interest of these lists is quite limited and we
think that a lot of time has been lost in drawing
them up and even grooming them. From a
general point of view the tools we have
considered, especially term extraction ones, only
have a limited interest for indexing contrary to
other tools (semantic relation extractors) they
have not been conceived for this purpose. This
point of view is shared by their own designers.
However, some of the semantic extraction tools
are adapted for indexing among their other
applications (Iota and Loria, for instance).
As for Spirale corpus:
Terminology (i) Thesaurus Mobis, (educational
sciences section) (ii) Francis list (of the INIST,
covering the complete volume on educational
sciences section).
Three lists for indexing: - Dictionnaire
encyclopédique de l&apos;éducation et de la
formation13. - CRDP list14 de Lille. - Bréhier list
(PRCE in documentation ).
</bodyText>
<footnote confidence="0.981153">
12 INIST is the National Institute of Scientific and
Technical Information. The list they provided is used to
index their data-base to complete this part.
</footnote>
<page confidence="0.866676">
13 P. Champy et C. Etevé. Index pp 1059-1097.
14 Centre Régional de la Documentation Pédagogique.
</page>
<subsectionHeader confidence="0.410945">
3.2.2.3 Unified Presentation Format
</subsectionHeader>
<bodyText confidence="0.999885538461538">
The protocol we suggested was based on the
previous evaluation sessions. The layout of
some results could at times make the task of
evaluation difficult. In some cases, good graphic
presentation (conceptual graphs, etc.) could hide
a poor term extraction and hence influence the
evaluation. Conversely a system which has the
capacity to extract relevant terms and semantic
relations but whose layout is poor can influence
the evaluation process. To prevent this,
participants have been asked to adopt a unified
format for their presentations for 2000
evaluation campaign.
</bodyText>
<sectionHeader confidence="0.484676" genericHeader="method">
3.2.2.4. Non-unified Tagging
</sectionHeader>
<bodyText confidence="0.999956285714286">
Given the fact that system designers have
different processing possibilities, some of the
systems use an independent tagger, others have
an integrated one which is part and parcel of
their system. The organizers decided to allow
the participants their own choice in terms of
tagging methods.
</bodyText>
<subsectionHeader confidence="0.723193">
3.2.2.5. Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999936">
Given the three tasks to be performed (indexing,
systematic terminology and translation), the
usual notions of recall and precision can be used
to evaluate the quality of results when matched
with a manually-produced reference list.
Performance failure at this level can be
interpreted in terms of silence and noise (see
below).
</bodyText>
<subsectionHeader confidence="0.712653">
3.2.2.6. Automatic Matching by EvalTerm
</subsectionHeader>
<bodyText confidence="0.953005127659574">
If the qualitative approach offers the easiest
form of systems evaluation it nevertheless
retains two major drawbacks: (i) it makes up for
a very boring job when there are too many
results (ii) judgments can easily be slanted by
the subjective approach of the expert.
Our protocol being based on the qualitative
black-box principle where parameters are hard
to quantify we chose to apply traditional IR
measures, recall and precision which normally
accompany qualitative evaluations:
R = number of correct extractions / number of
reference extractions.
P = number of correct extractions / number of
proposed extractions15
Since the manual matching of lists proved to be
long and complicated due to the huge size of the
15 Or their equivalents in terms of noise and silence:
Silence = 1 – Recall, Noise = 1 – Precision
processed data and to a variety of other
inconveniences, we chose to automatically
calculate these measures. We then decided to
duplicate the manual evaluation with its
conversion into numerical scales of values.
For this purpose we developed a program which
matches the results provided by the software
with the reference lists16 The program compares
two lists: L1 represents the results given by a
software and list L2 is a reference list proposed
by an expert17. The program output consists of
two files: file (a) which contains the elements of
L2 found in L1 (the relevant terms which the
software was able to find). And file (b) which
contains some elements of L2 which have not
been identified and consequently were not
mentioned in L1 (the correct terms not found by
the software). Through a simple subtraction we
can get a file containing the noisy terms of each
software.
In our automatic matching we have not included
any linguistic treatment for fear of introducing
new parameters which would influence the
results. Right from the beginning we have
noticed that over-productive systems such as
Ana or Term Finder are difficult to compare
with reference lists because the noise rate
becomes irrelevant.
</bodyText>
<sectionHeader confidence="0.968142" genericHeader="method">
4 An Overview of the Results
</sectionHeader>
<subsectionHeader confidence="0.997822">
4.1 Term Extraction on the two Corpora
</subsectionHeader>
<bodyText confidence="0.999966357142857">
We will now comment globally on how the term
extractors performed when run on the two
corpora for the three different tasks (indexing,
systematic terminology and translation):
First, automatic matching concurred with human
experience which notices that the systems
produce many “ noisy ” terms while on the
contrary there are many terms not included in
the reference lists but which the experts
considered as relevant for systematic
terminology. Hence the interest of some of these
“ noisy ” terms for enriching and updating
reference lists and terminology data bases.
Matching the results of the different systems has
</bodyText>
<footnote confidence="0.7765856">
16These lists can be: a) existing lists, real-life lists ( thesauri
or alphabetical lists, such as Francis List); b) established
by the evaluators/indexers (specifically tailored for the
three tasks, indexing, terminology and translation).
17 They are many lists proposed by our experts.
</footnote>
<bodyText confidence="0.998301533333333">
showed a great similarity between Lexter and
Acabit.
As for indexing, if the systems could generally
provide relevant and effective help for
terminology (systematic terminology, and
translation) their contribution to indexing is less
obvious. Indexing supposes other mental
operations than those needed for terminology
construction and simply picking out candidate-
descriptors is not enough to supply a reliable
form of indexing.
The three core criteria of good indexing are:
reliability, selectivity and exhaustiveness. The
indexer must hold a balance between
exhaustiveness and selectivity. Having too many
terms leads to noise and too few to silence. It is
on this criteria of selectivity that human
processing varies.
Softwares based on term extraction offer a large
number of potential candidate terms, connecting
them with more or less precise criteria of
relevance, mostly of a statistical nature. At this
level of processing the indexer has recourse to
authorized lists and thesauri i.e. he or she refers
to the work of terminologists in structuring the
field and attributing a label to each and every
concept. The systems which we tried to assess
are not yet likely to provide a very effective help
to indexing since the results are over-productive
in view of the needs.
</bodyText>
<sectionHeader confidence="0.79385" genericHeader="method">
5 The Classifier and the Semantic Relation
</sectionHeader>
<subsectionHeader confidence="0.963572">
Extraction (SRE) Tools
</subsectionHeader>
<bodyText confidence="0.999977052631579">
The protocol we adopted specifies the
evaluation of semantic relation and class
validity, coherence and comprehensiveness on
all of the three tasks (i.e. semantic relations
examined from the point of view of systematic
terminology, translation and indexing). The
classes and semantic relations extracted were
subject to a comparison with the human
performance of these tasks (experts and
reference lists), plus a comparison with other
systems performing the same task. This
qualitative evaluation is measured by the
traditional IR performance measures (silence,
noise, recall and precision). The first thing we
can remark on is that it is very difficult to fulfill
the evaluation within our proposed terms of
reference. We are presenting hereunder the
reasons limiting the scope of our protocol when
applied to SRE results.
</bodyText>
<subsectionHeader confidence="0.892129">
5.1 An Overview of the Results of SRE on
the Two Corpora
</subsectionHeader>
<bodyText confidence="0.96533092">
What we observed is that these tools are too
different to allow a useful comparison for the
following reasons:
- SRE extract different types of relations and
hence are incomparable.
- This difference is linked to the different forms
of semantic model implementation. Conversely
some extractors are based on models that will
not allow the type of relations required for the
three evaluation tasks.
- SRE are designed for different functions and
have different objectives or carry out different
tasks.
- These differences are reflected in the type of
output or results.
- Another problem came from the fact that
INRA could not provide us with a structured list
corresponding to the eight selected texts. Even if
this list had been available, comparing it to the
results would have been of limited interest only.
The remaining solution was to submit the results
to a field specialist.
- Difficulties in interpreting the non-labeled
Semantic Relations. Fig. Two shows these
differences:
</bodyText>
<figureCaption confidence="0.977158">
Fig. 2. Synthetic comparison table for SRE
</figureCaption>
<table confidence="0.992415315789474">
IOTA LORIA SEEK-JAVA
Objectives Building Scientific &amp; a) Cognitive text
indexes of technical organization
one or more watch b) Extraction of Labeled
levels (layers) (identifying Semantic relations between
for document rare or new terms in a thesaurus or a
retrieval information network of terms
c)Constructing/modeling
Knowledge-based systems
Functions Information Semantic Semantic relations
seeking relations extraction and
systems, extraction representation
automatic Presentation in a conceptual
extraction of graph fashion
complex Building relational data
indexes bases
Out-put Lists of non-labeled A descriptive network/graph
potential Logico- of terms linked with
candidate Semantic semantic relations between
</table>
<bodyText confidence="0.700488888888889">
terms ranked Relations complex or simple terms,
by with between terms on the one hand and a triplet
frequency Classes of of argument-relation-
Terminology terms argument on the other
networks assembled in a relational
data-base
- Moreover, it is difficult or even impossible to
measure silence using a protocol based on IR
systems performance measure.
</bodyText>
<listItem confidence="0.998453875">
• Without a prior knowledge of the missing
possible relations one cannot account for the
silence measure.
• To account for noise, a thorough knowledge
of both the semantic model and the field of
knowledge is required.
• These observation are also valid for recall
and precision measures.
</listItem>
<bodyText confidence="0.999404222222222">
We can thus say for the time being that SRE
cannot be assessed by the protocol since their
results cannot be matched.
The field specialist18 gave the following account:
“It is essential to have an interface to manipulate
and interpret the relations. Everything seemed
somewhat inconclusive. At times the relation
“fits well”, at times it does not at all. Results are
not always relevant and it is difficult to trust this
type of analysis on its own if one is not at the
same time be conversant with the domain, since
some of the relations can be wrong.
For Iota, concept extraction seems generally
quite relevant. However one has to wonder
about the relevance of a number of extracted
concepts which are not at all relevant to the
field. How did these non-specific concepts get
extracted more easily than others ?
As for the table on Conceptual Semantic
Dependence19 it is hard to draw any conclusions
from it since it offers only one semantic label for
any relation.
The Iota approach is more global than the Seek-
Java one since the relations are based on the
whole document and not only at the level of one
sentence. These two softwares are thus difficult
to compare since their purpose is not the same”.
</bodyText>
<subsectionHeader confidence="0.9865875">
5.2 Conterm, the Classifier: an ad hoc
Evaluation
</subsectionHeader>
<bodyText confidence="0.99998825">
Given the difficulties we listed above and the
fact that it was impossible to compare Conterm
with other systems performing the same task.
The only possible evaluation for Conterm would
have been a progress evaluation for this sole
classifier of the campaign20. This problem shows
again the limits of our Protocol. The Conterm
lists were matched to an automatically produced
</bodyText>
<subsectionHeader confidence="0.754891">
18 Patricia Volland-Neil, from INRA-Tours
</subsectionHeader>
<bodyText confidence="0.998222">
19 The evaluator is referring to the tables accompanying the
results provided by the system’s designer.
20 The protocol is not suitable for its evaluation. After the
withdrawal of another participant who had also presented a
classifier, only this one remained.
untagged list of terms which corresponds to the
eight texts of the INRA corpus. The most
important element in its evaluation is not that we
matched its results with a tagged list but that the
results had been matched with indexers’ and/or
experts lists and that we could observe the
correspondence between Conterm’s output and
the lists. It does not mean that Conterm is good
for indexing but that the classes suggested by
this tool embody conceptual attributes which are
close to the logic underlying the human
selection of candidate-terms suitable for
indexing, namely its rich lexico-semantic
network.
</bodyText>
<sectionHeader confidence="0.731868" genericHeader="method">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.997786710526316">
- This evaluating action provided us with an
awareness of the State-of-the-art in the field of
terminology acquisition tools. It also allowed us
to test evaluation paradigms, demonstrating how
difficult it was to apply a single evaluation
protocol to a variety of systems operating along
different lines.
- The discussions among participants aiming at
the creation of a testing protocol resulted in the
definition of an evaluation procedure and in an
assessment of their relative merits. The
comparative study of the systems’ out-put also
enabled a better understanding of the
performances of the wide range of techniques
involved. As by-products of the project two
corpora can be used in further evaluation
campaigns and a set of material tests (real-life
and constructed or specifically tailored one that
can be shared during future evaluations).
- The evaluation results can be used predictively
for system design, development or modification
The limits of our evaluation approach can be
sketched in the following manner:
- If the adopted protocol based upon reference
list can be applicable to the two tasks
(translation and terminology) it is hardly
applicable to indexing tasks.
- It is not adequate to account neither for the
classifiers nor for the SRE.
- Several questions remain unanswered:
a) first, is it possible to fully automate
evaluation procedures? Then is it possible to
abandon test material, such as reference lists or
other type of human-made data, which are
considered as a kind of gold standard reusable
for other evaluation campaigns? (see our recent
experience in MT evaluation workshop, April
200121.
</bodyText>
<listItem confidence="0.660404428571429">
b) As far as semantic relation extraction is
concerned, is it possible to automate SRE
valuation procedure in the way Grefensttete
(1994) does?
7 Future Directions
1. Exploiting Results: the Campaign’s Side
Benefits:
</listItem>
<bodyText confidence="0.987564444444444">
Full treatment of the Spirale corpus will allow
the creation of an index of all the reviews past
numbers, which fulfills the moral contract made
with its Editorial Board in exchange for getting
the corpus free of charge. In addition, these
results can help broaden the terminological
repository for the education sciences, especially
in drawing up the Francis Thesaurus which
covers all education sciences.
</bodyText>
<listItem confidence="0.944223">
2. Towards Trans-Systemic Integration: The
output of the systems are divergent but can in
</listItem>
<bodyText confidence="0.950675447761194">
some cases be complementary. In fact the
preliminary results drawn from the first
evaluation in 1997 (cf. Béguin et al. 2000) have
led us to consider the feasibility of trans-
systemic integration for strengthening their
automatic terms identification capabilities. The
idea is to combine two or three different types of
systems in order to specify various integrated
production processes. Systems could
21 “Setting a methodology for Machine Translation
evaluation”. The context: evaluation of a translation
made by an MT System on the following source text:
INRA corpus text N°604 “ corpus biotechnologique
sur la reproduction chez l’animal ” Source language:
French - Target language: English. We carried out
some manual testing but with the objective of setting
a rough methodology that might be irrelevant for
translating huge size corpora. The tool we used was a
non interactive French / English MT System with a
basic French/English dictionary that does not include
any specific terminology. We had two indexes (a
French index and an English index of domain
specific expressions, but they are not aligned). They
have been provided by the INRA and considered as
gold standard. We used the indexes to create a
specific dictionary in order to feed the MT systems
with this specific lexical data. The next step is to
assess the impact of specific terminology when
integrated to an MT system by comparing the results
of the two translations we get: with and without
specific terminology.
increasingly be seen as parts of these integrated
production processes.
3. Towards User-Oriented Evaluations: in the
light of the results obtained in this campaign the
most suitable type of evaluation would be a
user-oriented one. Other types of approaches22
can be designed, such as adequacy evaluation23
which can to some extent be adopted for our
case but we have to define a more strict user
profile.
4. Towards developing interfaces for validating
the results: even if we opted for a unified
presentation format for the reasons mentioned in
section 3.2.2.3, we however think it is essential
for future campaign organizers to have an
interface to manipulate and interpret the results
(validating term, relations and classes). This
type of interface can dramatically facilitate the
interaction with the evaluators and the end-user
of these tools.
5. Designing tools for generic bi-lingual
production, allowing ad hoc extractions through
ad hoc interfaces.
6. Capability to share resources in the future
(test material such as gold standard lists, real-
life and/or constructed ones).
7. Developing automatic evaluation tools such
as Evalterm which can be reused in similar
future evaluations.
8. Hypothesis are still to be tested for semantic
relations extraction: results of the various
semantic extractors will be of different quality
depending on the type and nature of corpora
(domain and genre) chosen (cf. also Condamines
et al. 98; Davidson et al 98, among many
others).
</bodyText>
<sectionHeader confidence="0.998205" genericHeader="conclusions">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.949544923076923">
The authors gratefully acknowledge the
financial assistance provided by the AUF
(Association des Universit6s Francophones) in
funding the general research project within
which this paper was written.
22 Spark-Jones et al. 1996; King 1999, among many others,
identified more three types of evaluation processes: the
progress evaluation, the adequacy evaluation and the
diagnostic evaluation. The first and second types are used
for comparative benchmarking.
23 Adequacy evaluation aims at finding out whether a
system or product is adequate to someone’s needs. This
type is typically done when thinking of acquiring a system.
</bodyText>
<sectionHeader confidence="0.986756" genericHeader="references">
9. References
</sectionHeader>
<reference confidence="0.989715502702703">
Adda, G., Lecompte, J., Mariani, J., Paroubek, P.,
Rajman, M. (2000). Les proc6dures de mesure
automatique de l’action GRACE pour l’6valuation
des assignateurs de partie du discours pour le
français, Chibout, K., Mariani, J., Masson, N.,
Neel, F. 6ds., (2000). Ressources et évaluation en
ingénierie de la langue, Duculot, Coll. Champs
linguistiques, et Collection Universit6s
Francophones (AUF), pp. 645-664.
B6guin, A, Jouis, C, Mustafa El Hadi , W, (1997):
&amp;quot;Evaluation d’outils d’aide à la construction de
terminologie et de relations s6mantiques entre
termes à partir de corpus&amp;quot;, In JST&apos;97, FRANCIL,
AUPELF-UREF, Avignon, avril 1997, pp. 419-
426. This article is published in Chibout et al.
2001 (eds.) pp 161-179.
B6guin, A., Jouis, Ch., Mustafa Elhadi, W. (2000).
Evaluation d’outils d’aide à l’extraction et à la
construction automatiques de termes et de relations
s6mantiques. In Chibout, K., Mariani, J., Masson,
N., Neel, F. 6ds., (2000). Ressources et évaluation
en ingénierie de la langue, Duculot, Coll. Champs
linguistiques, et Collection Universit6s
Francophones (AUF), pp 161-179.
Bourigault, D. (1993). “Analyse syntaxique locale
pour le rep6rage de termes complexes dans un
texte”, Traitement automatique des langues 34(2),
pp. 105-117.
Bourigault, D., Jacquemin Ch. &amp;. L’Homme M-C.
6ds. (1998). Computerm ’98, First Workshop on
Computational Terminology, COLING-ACL’98,
15 August 1998.
Bruandet, M.F. (1989). Outline of a knowledge base
model for an intelligent Information Retrieval
system. In Information Processing and
management, Vol 25, N° 3, 1989.
Cavazza, M., (1993). M6thodes d&apos;6valuation des
logiciels incorporant des technologies
d&apos;informatique linguistique, Paris, Rapport MRE-
DIST, 1993.
Chaudiron, S. (2000). The Relevance of Quality
Model for NLP Applications, in Proceedings of
RIAO, pp 1568-1577, Paris 12-I4 April 2000.
Condamines, A. et Rebeyrolle, J. (1998). CTKB: A
corpus-based approch to a Terminological
Knowledge Base, In Bourrigault, D., Jacquemin
Ch. &amp;. L’Homme M-C, 6ds.(1998).
Computerm’98, First Workshop on Computational
Terminology, COLING-ACL’98, 15 August 1998,
pp. 29-35.
Dagan. I. and Church, K. (1994). Termight:
Identifying and Translating Technical
Terminology. In: Proceedings of 4th Applied NLP
Conference 1994, 34-40.
Daille, B., (1994). ACABIT: une maquette d’aide à
la construction automatique de banques
terminologiques monolingues ou bilingues. In
Class, A., Thoiron, P, , B6joint (eds)
Lexicomatique et Dictionnairiques, pp. 123-136,
Beyrouth 1996.
Daille, B. B. Habert, C. Jacquemin et J. Royaut6
(1995). “Empirical Observation of Term Variation
and Principles for their Description”, Terminology
3(2), pp. 197-257.
Davidson, L., Kavanagh, J., Mackintoch, K., Meyer,
I., Skuce, D. (1998). Semi-Automatic Extraction of
Knowledge-Rich Contents from Corpora, In
Bourrigault, D., Jacquemin, Ch. &amp;. L’Homme M-
C, 6ds. (1998). Computerm’98, First Workshop on
Computational Terminology, COLING-ACL’98,
15 August 1998, pp. 51-56.
EAGLES (1996)
http://www.issco.unige.ch/projects/eagles.
Enguehard, C. (1993). Acquisition de terminologie à
partir de gros corpus. Informatique et langue
naturelle ILN’ 93, Nantes, pp. 373-384, d6cembre
1993.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery, Boston: Kluwer Academic-
Press.
[Gaussier, E. (1995). Modèles statistiques et patrons
morphosyntaxiques pour l’extraction de lexiques
bilingues de termes, Thèse de doctorat, Paris:
Universit6 Paris VII.
ISLE (2001) MT Evaluation Classification,
Expanded Classification (2001).
http://www.isi.edu/natural-language/mteval/2b-
MT-classification.htm.
Jouis, C., (1993). Contribution à la Conceptualisation
et à la Mod6lisation des connaissances à partir
d&apos;une analyse linguistique de textes. R6alisation
d&apos;un prototype: le système SEEK. Thèse de
doctorat. EHESS. Paris. 1993.
Jouis, C., Mustafa El Hadi, W. (1997), AUPELF
Project: Term and Semantic Relation Extraction
Tools. Evaluation Paradigms, In Proc. of the
Speech and Language Technology Club Workshop
“ Evaluation in Speech and Language
Technology ”, Univ. of Sheffied, June 17-18,
Sheffield, UK, pp. 106-113.
Kageura, K. and Umino, B. (1996). Methods of
Automatic Term Recognition: A Review. In:
Terminology Vol. 3(2), 1996, 259-289.
Kilgarrif, A., Rosenzweig, J. (1998). English
SENSVAL: Reports and Results.
King (1999) EAGLES Evaluation Working Group,
report, http://www.issco.unige.ch/projects/eagles.
King M. (1996) EAGLES, Workshop, University of
Geneva http://www.issco.unige.ch/projects/eagles.
Le Priol, F, Chavallet, J-P., Bruandet, M-F., Desclès,
J-P. (1998). Int6gration d’un système statistique
(IOTA) et d’un système s6mantique (SEEK) dans
une chaîne de traitement permettant l’extraction de
terminologies. Actes Ingénierie des Connaissances,
Pont-à-Mousson, pages 33-40. 1998.
L’Homme, Marie-Claude, Benali, Loubna, Bertrand,
Claudine and Lauduique, Claudine. (1996).
Definition of an Evaluation Grid for Term-
Extraction Software. In: Terminology Vol. 3(2),
1996, 291-312.
L’Homme M-C. (2001). Évaluation d’outils d’aide à
la construction automatique de terminologie et de
relations s6mantiques entre termes à partir de
corpus ARC-A3 Rapport final, Montr6al, mars
2001.
Lauriston, A. (1994). Automatic Recognition of
Complex Terms: Problems and the TERMINO
Solution. In: Terminology 1(1), 147-170.
Manzi S. (1999) Test Material for Evaluation, Sandra
Manzi, ISSCO - University of Geneva
http://www.issco.unige.ch/projects/eagles/ewg99.
Mariani, J. Masson, N., Neel, F., 6ds. (2000).
Ressources et évaluation en ingénierie de la
langue, Duculot, Coll. Champs linguistiques, et
Collection Universit6s Francophones (AUF), 2000,
pp. 13-24, actes des 1ères Journ6es Francil autour
du thème: L&apos;ing6nierie de la Langue: de la
Recherche au produit, Avignon 15-16 avril 1997.
MUC-3, (1991). Proceedings of the Third Message
Understanding Conference. Morgan Kaufmann.
MUC-4, (1992). Proceedings of the Fourth Message
Understanding Conference. Morgan Kaufmann.
Mustafa El Hadi, W. &amp; Jouis, C. (1998),
Terminology Extraction and Acquisition from
Textual Data: Criteria for Evaluating Tools and
Method, Proceedings of the First International
Conference on Language Resources and
Evaluation, Grenada, Spain may 1998, pp. 11750-
1178.
Mustafa El Hadi, W. &amp; Jouis, C. (1997), &amp;quot;Natural
language processing techniques and their use in
data modeling and information retrieval&amp;quot;. In:
Proceedings of the sixth international study
conference on classification research, Knowledge
Organization for Information Retrieval, University
College London, London, 16-18 June 1997. The
Hague: FID, 157-161.
Mustafa El Hadi, W., Jouis, C. (1996a), Evaluating
Natural Language Processing Systems as a Tool
for Building Terminological Databases, In
Proceedings of the Fourth International ISKO
Conference: Knowledge Organization and
Change, Washington, Library of Congress, July
1996, Advances in Knowledge Organization, Vol.
5, INDEX Verlag, Frankfurt/Main, pp. 346-355.
Mustafa El Hadi, W., Jouis, C. (1996b), Natural
Language Processing-based Systems for
Terminological Construction and their
Contribution to Information Retrieval. In
Proceedings of the Fourth International Congress
on Terminology and Knowledge Engineering
(TKE&apos;96), Vienna, INDEX Verlag,
Frankfurt/Main. pp. 118-130.
Seffah, A. Meunier, J.G. (1995). ALADIN :un atelier
orienté objet pour l’analyse et la lecture de textes
assistée par ordinateur. In : International
Conference on statistics and texts
Sparck-Jones K., Gallier, J.R. (1996). Evaluating
Natural Language Processing Systems: An Analysis
and Review, Springer, Berlin.
Toussaint, Y. Namer F., Daille, B., Jacquemein, C.,
Royauté, J., Hathou N., (1998). Une approche
linguistique et statistique pour l’analyse de
l’information en corpus. In : TALN’ 98, Paris,
France, 1998.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9599285">The ARC A3 Project: Terminology Acquisition Tools: Evaluation Method and Task</title>
<author confidence="0.985167">Widad Mustafa El_Hadi I s m a ï l Timimi Annette Béguin Marcilio De_Brito</author>
<email confidence="0.885588">beguin@univ-lille3.frmdebrito@noos.fr</email>
<affiliation confidence="0.95187">UFR IDIST &amp; CERSATES (CNRS UMR</affiliation>
<address confidence="0.7417915">Université Charles De Gaulle, Lille 3 BP 149, F-59 653 Villeneuve D&apos;Ascq, France</address>
<abstract confidence="0.991569802083333">This paper describes the work achieved in the Concerted Research Project ARC A3 and coordinated by the The project deals with the evaluation of term and semantic relation extraction from corpora in French. Eight participants, both from public institutions and industrial corporations were involved in this project and were responsible for producing corpora suitable for extraction tasks and elaborating a protocol in order to evaluate objectively acquisition This expression covers respectively, term extractors, classifiers and semantic relation extractors. The paper also reports on the methodology used for comparing four term extractors, one classifier and three semantic relation extractors during the 2000 evaluation campaign. There are also several by-products of this campaign: first, two corpora which can be used for NLP system development and evaluation as the AUF recommended; and then terminology products: for each corpus a list of terms characterizing the field is available. We are not giving details about the results but rather an assessment of what the evaluation of Terminology Extraction Tools is: how was it done, what were the difficulties, which are the advantages and disadvantages of the adopted protocol, what are the limits and how should we proceed for future testing. des Universités Francophones is the &amp;quot;Association des Universités Entièrement ou Partiellement de Langue Française&amp;quot;, an NGO whose mission is to promote the dissemination of French as a scientific medium. ARC A3 Program A3 is a project of the group coordinated and founded by AUF. It was started in 1995 in order to promote research in the field of terminology acquisition. The ARC A3, and Semantic Relation Extraction from in French” aim is to test software capabilities in term and semantic relation extraction from corpora in French. Systems submitted to this evaluation are designed by French and Canadian research institutions (National Scientific Research Center and Universities) and/or private businesses. These systems have been extensively described our previous work (cf. Béguin, 1997, Jouis 1997; Mustafa El Hadi 1996a, 1996b, 1997 &amp; 1998;). The first phase of the project has been directed towards testing the on one (trial run) and towards elaborating a workable protocol based on this experience. The first results were presented the first conference of (cf. Béguin 1997, 2000). This article reports on the second and final evaluation campaign. A3 Organization ARC A3 brings together four kinds of actors: a coordinator who plays an organizational role (schedule, quality control of corpora, data production, etc.), corpora providers; participants of the test and two scientific advisors. The action has been coordinated by the University of Lille 3. The organizing team in cooperation with the discussion group made up of representatives of each participating team and two scientific de la Langue, Linguistique-informatique et Corpus écrits. a periodical dealing with education and pedagogy issues. Each periodical sizes around 200 pages. Scientifiques e Techniques de Francil, Avignon, France, 1997. advisors are supposed to co-operate in defining a methodology for testing the systems. Systems The systems are designed by French and Canadian research institutions. There were ten registered participants at the beginning of the project and three withdrew later for a variety of reasons. The organizers then launched another call for participation in July 1999 and three more participants joined the project (two private enterprises (Xerox and Logos) and a public institution (the University of Grenoble). Logos and the University team later dropped out for reasons unrelated to the program. When the final campaign was launched in 2000 there were eight systems remaining under evaluation.</abstract>
<title confidence="0.494327666666667">Fig. 1. Participating Systems Software Affiliation Acabit IRIN (Nantes)</title>
<author confidence="0.43563">Ana IRIN</author>
<affiliation confidence="0.787836">Conterm LANCI (Montréal) Iota CLIPS-IMAG (Grenoble) Lexter ERSS (Toulouse)</affiliation>
<degree confidence="0.418244">Seek-Java CAMS-LALIC (Paris 4) LORIA (Nancy) Xerox-Termfinder XEROX (Grenoble) of the Tested Tools</degree>
<abstract confidence="0.984499729110514">Terminology plays a major role in information processing and management and in specialized communication. Its role has been enhanced by the spread of automation and by the availability of electronic corpora. These two factors have had a massive impact on many different systematic building, natural-language interface design, lexical units for specific use in some subname is used for practical reasons since no final software name has yet been chosen of terms drawn from a representative corpus characterizing and describing a field of knowledge. In order to be of any use this type of list must be subject to a structuring which is an important step towards exploiting extraction results. languages and technical writing, thesaurus construction, translation and indexing as well as the recent growth of cross-language information retrieval (CLIR). If we focus on the tools, presented in our evaluation project, from the point of view of their functions and of the purposes for which they were designed), there are three categories: &amp;quot;Term Extractors&amp;quot;, &amp;quot;Classifying Tools&amp;quot;, and &amp;quot;Semantic relations extraction tools&amp;quot;. As we already mentioned, these systems were extensively described in our previous publications. Extractors (TE) We will briefly describe the basic idea underlying TE tools. Most of the extracting tools consider terms as noun phrases. Systems identify terms by using frequency, distribution category-pattern matching (Daille 1995; Dagan, 1996; Lauriston, 1994). All lexical units contained in a given text are analyzed and matched to patterns (typical forms of terminological units) described in rules. More term extractors are accounted for elsewhere 1996; Kageura 1996; Dagan 1994). Some of the systems described by these authors are tested in the framework of our valuation project (Acabit, Lexter, and Ana). and Semantic Relation Extractors (SRE) Terminology resources are increasingly seen as structured data i.e. as a network of terms organized by relations. Pure alphabetical lists can hardly be used except for bilingual reference tools. The variety of tools, their functions and the different possible uses offered within the framework of ARC A3 shows this need. Consequently such lists of terms are quite difficult to evaluate except by specialists in the relevant fields which makes it a rather constraining process. Structuring terms by semantic relations or in classes is useful for the following applications: Index-making for on-line documentation; browsing; information access and retrieval; building thesaurus and ontologies for information systems. Many applications and extraction methods relevant to these tools have been described in the literature. The systems tested in the AUF framework are geared towards a variety of applications ranging from rough semantic relation extraction, through indexing, thesaurus construction to knowledge-based system modeling (see figure 2). Classifiers and semantic relation extractors are tested within the same framework as the one used for evaluating term extractors. The first category is characterized as classifying tools. Their role is to build classes of networks of terms linked to a major one. This category consists of statistical and/or connectionist models such as Conterm. It is the only classifier tested within the framework of this campaign. The second category includes semantic relation extractors which focus particularly on semantic relations (Iota, Loria and Seek-Java). A complete description of all the systems which were tested (main characteristics and purposes, description as far as approaches are concerned) is documented in previous work. paradigm Evaluation activities are a corollary of the quick development of NLP tools in general and of terminology extraction in particular. It thus became necessary to evaluate these tools on objectively based criteria in order to have a clear picture of the state-of-the-art, assess the needs in this sector and hence promote research in this specific field. Moreover, the principal aim of existing testing methods, as reported in the literature, is to come across software errors and then try to adapt them for a particular user environment. Evaluation paradigm is basically dependant upon two major steps: (i) Creation of textual data: raw or tagged corpora and test material. A corpus-based research is part of the infrastructure for the development of advanced language processing applications; (ii) Test and comparison of systems on a similar data 1993; Adda 2000). ARC A3 Evaluation Approach The approach we adopted is a black-box The results are compared approach is adopted and validated by the vast majority of participants to the test in June 1999. The organizers have slightly adapted the protocol because more participants joined the ARC after the validation of the protocol. with the human performance of a task (either experts examining results or using reference lists or both). Moreover comparisons are made with other systems performing the same task. The results are finally calculated and translated in of traditional IR conventional distinction between the following: the former considers only system input-out-put relations without regard to the specific mechanisms by which the outputs were obtained while the latter examines the mechanisms linking input and output. (Sparck-Jones, 1996 p. 26; King, 1996; 1999, among many others). The qualitative evaluation measures as described by Sparck-Jones 1996, pp. 61-122, are based on observation or interviewing and are broadly designed to obtain a more holistic, less reductive or fragmented view of the situation. It is moreover more naturalistic. This type of evaluation naturally fits an end-free style. In our case the quality of the results is evaluated by domain experts. We distinguish two types of experts: experts for the three applications tested terminology, translation and and experts in the two domains of corpora (biotechnology and pedagogy). Both quantitative and qualitative approaches are goal-oriented, that is focusing on discrepancies between performance results and initial system requirements. Sparck-Jones points out how the two types of measures are deeply interwoven although different in their nature: - Recall is a quantitative measure of system performance while - Declared Satisfaction is a qualitative one (i.e. such a measure is really qualitative even if the result of applying it to a set of users is a percentage figure). The qualitative approach in the evaluation process is the easiest one for end users. It means giving a value judgment on how the system globally works (Cavazza, 1993; Chaudiron, 2000). The dominant approach today is towards quantitative evaluations which are considered as more objective and reproducible than the qualitative approach (EAGLES-1 1996; ISLE 2001). The main attempt of these approaches is chose to accompany the qualitative approach (mainly based on manual evaluations) by a translation of the manual evaluations into numerical scales of values (see below for more details). to translate the concepts of relevance and quality into numerical data. Statistical approaches such as MUC 2 and TREC 3 are frequently used for this type of evaluation. (Chaudiron, 2000). merits of a black-box evaluation Obviously this approach has its pros and cons. But it can be justified on the following basis: - Since most developers cannot provide us (as test organizers) with their systems, the only way was to send them the text corpora and let them provide us with the results. A glass-box evaluation would have required an examination of the systems by the organizers which would have been impossible except for Xerox’s TermFinder and Logos System’s Knowledge Discovery, two commercialized systems. - Even if this approach may be criticized on account of its subjective side, end-users like it because of its usefulness when comparing two or more systems which differ in all their parameter settings. (Chaudiron 2000; Cavazza 1993). - A black-box evaluation is more oriented towards system’s end-user when compared to a glass-box evaluation. For the latter the test will involve analyzing the system’s functioning by looking at its different components. Each component is evaluated separately in itself. Such an approach allows for spotting and understanding the causes of dysfunctional results. It is a long term process which requires access to the internal parts of the system and an understanding of the architecture and global strategy of the software. This is obviously a oriented approach not an endone 2000; Cavazza 1993). - In spite of its limited scope the evaluation protocol we adopted is used in more complicated NLP tools, such as MT tools. Evaluators examine the systems’ output without considering the differences between them (cf. L’Homme, 2001). Last Spring our team took part in a workshop organized by ISSCO (University of Geneva) where we and all the other participants adopted this approach. of the Evaluation Protocol of the 2000 Campaign Task The extraction of terms, of classes and of semantic relations was necessary to test the tools performance in the three following tasks: Systematic terminology (characterizing the tested corpora); (ii) Translation; (iii) Indexing. This means in practice: what is the relevance of terms, classes and semantic relations provided by the systems being tested? Do the terms, classes and semantic relations satisfy minimum requirements? Do we need to define a minimum level of terms, classes, semantic production? Are discrepancies meaningful? For example, it could be that most of the systems being tested are having qualitatively poor outputs, while only one or two produce worthwhile results. Within this perspective the idea was to submit the to We distinguished for the purpose of this campaign two types of human expertise as we mentioned above. material Evaluation data can normally be divided into two different categories (i) representative samples of the tested corpora (ii) test material, which, in our evaluation framework, is made up of both custom-designed lists and real life lists / thesaurus. 3.2.2.1 Corpus corpora were tested: and We have chosen a sample representing 10% of each corpus: for Spirale n° 19 was chosen. As for INRA corpus, the providers of this corpus suggested 8 articles (603, 604, 607, 609, 631, 666, 732, 740). 3.2.2.2 Reference Lists These lists are standard human professional results which can be used as performance exemplars or norms for comparison. This type of data is considered to be a gold standard (see SensEval, Kilgarrif 1998; ISLE 2001). For the INRA corpus the following lists have been created: translation lists were processed (i) a list created by a novice translator (ii) another one by a confirmed professional translator. texts, 16 mega bytes texts, 2,2 mega bytes. six lists were created both by professional and by non professional indexers. We are not developing these lists in this paper given the limited scope of this type of evaluation from an indexing point of view. Hence the limited interest of term extraction tools for human indexing. We will however comment on the terminology lists provided by the two corpus providers, INRA (Institut National pour la Recherche Agronomique i.e. National Institute Agronomic Research), the list and the translation lists. As far as INRA corpus is concerned: We think that our evaluation task could have given better results if the lists had been more representative of a systematic terminology activity. For the INRA corpora, for example, only 113 terms were chosen by the experts to represent their terminology. Our estimation is that, 113 terms only constitute a poor representation of an activity. It would have been a good idea to have specialists establish the lists of terms and to compare those to the systems’ output. Even if this work is time consuming it makes for a better evaluation of the systems’ productivity. As far as indexing is concerned the interest of these lists is quite limited and we think that a lot of time has been lost in drawing them up and even grooming them. From a general point of view the tools we have considered, especially term extraction ones, only have a limited interest for indexing contrary to other tools (semantic relation extractors) they have not been conceived for this purpose. This point of view is shared by their own designers. However, some of the semantic extraction tools are adapted for indexing among their other applications (Iota and Loria, for instance). As for Spirale corpus: Thesaurus (educational section) (ii) (of the INIST, covering the complete volume on educational sciences section). lists for indexing: encyclopédique de l&apos;éducation et de la - CRDP de Lille. Bréhier list documentation ). is the National Institute of Scientific and Technical Information. The list they provided is used to index their data-base to complete this part. Champy et C. Etevé. Index pp 1059-1097. Régional de la Documentation 3.2.2.3 Unified Presentation Format The protocol we suggested was based on the previous evaluation sessions. The layout of some results could at times make the task of evaluation difficult. In some cases, good graphic presentation (conceptual graphs, etc.) could hide a poor term extraction and hence influence the evaluation. Conversely a system which has the capacity to extract relevant terms and semantic relations but whose layout is poor can influence the evaluation process. To prevent this, participants have been asked to adopt a unified format for their presentations for 2000 evaluation campaign. 3.2.2.4. Non-unified Tagging Given the fact that system designers have different processing possibilities, some of the systems use an independent tagger, others have an integrated one which is part and parcel of their system. The organizers decided to allow the participants their own choice in terms of tagging methods. 3.2.2.5. Evaluation Measures Given the three tasks to be performed (indexing, systematic terminology and translation), the usual notions of recall and precision can be used to evaluate the quality of results when matched with a manually-produced reference list. Performance failure at this level can be interpreted in terms of silence and noise (see below). 3.2.2.6. Automatic Matching by EvalTerm If the qualitative approach offers the easiest form of systems evaluation it nevertheless retains two major drawbacks: (i) it makes up for a very boring job when there are too many results (ii) judgments can easily be slanted by the subjective approach of the expert. Our protocol being based on the qualitative black-box principle where parameters are hard to quantify we chose to apply traditional IR measures, recall and precision which normally accompany qualitative evaluations: R = number of correct extractions / number of reference extractions. P = number of correct extractions / number of Since the manual matching of lists proved to be long and complicated due to the huge size of the their equivalents in terms of noise and silence: Silence = 1 – Recall, Noise = 1 – Precision processed data and to a variety of other inconveniences, we chose to automatically calculate these measures. We then decided to duplicate the manual evaluation with its conversion into numerical scales of values. For this purpose we developed a program which matches the results provided by the software the reference The program compares lists: the results given by a and list a reference list proposed an The program output consists of two files: file (a) which contains the elements of in relevant terms which the software was able to find). And file (b) which some elements of have not been identified and consequently were not in correct terms not found by the software). Through a simple subtraction we can get a file containing the noisy terms of each software. In our automatic matching we have not included any linguistic treatment for fear of introducing new parameters which would influence the results. Right from the beginning we have noticed that over-productive systems such as Ana or Term Finder are difficult to compare with reference lists because the noise rate becomes irrelevant. Overview of the Results Extraction on the two Corpora We will now comment globally on how the term extractors performed when run on the two corpora for the three different tasks (indexing, systematic terminology and translation): First, automatic matching concurred with human experience which notices that the systems produce many “ noisy ” terms while on the contrary there are many terms not included in the reference lists but which the experts considered as relevant for systematic terminology. Hence the interest of some of these “ noisy ” terms for enriching and updating reference lists and terminology data bases. Matching the results of the different systems has lists can be: a) existing lists, real-life lists ( thesauri or alphabetical lists, such as Francis List); b) established by the evaluators/indexers (specifically tailored for the three tasks, indexing, terminology and translation). are many lists proposed by our experts. showed a great similarity between Lexter and Acabit. As for indexing, if the systems could generally provide relevant and effective help for terminology (systematic terminology, and translation) their contribution to indexing is less obvious. Indexing supposes other mental operations than those needed for terminology construction and simply picking out candidatedescriptors is not enough to supply a reliable form of indexing. The three core criteria of good indexing are: reliability, selectivity and exhaustiveness. The indexer must hold a balance between exhaustiveness and selectivity. Having too many terms leads to noise and too few to silence. It is on this criteria of selectivity that human processing varies. Softwares based on term extraction offer a large number of potential candidate terms, connecting them with more or less precise criteria of relevance, mostly of a statistical nature. At this level of processing the indexer has recourse to authorized lists and thesauri i.e. he or she refers to the work of terminologists in structuring the field and attributing a label to each and every concept. The systems which we tried to assess are not yet likely to provide a very effective help to indexing since the results are over-productive in view of the needs. Classifier and the Semantic Relation Extraction (SRE) Tools The protocol we adopted specifies the evaluation of semantic relation and class validity, coherence and comprehensiveness on all of the three tasks (i.e. semantic relations examined from the point of view of systematic terminology, translation and indexing). The classes and semantic relations extracted were subject to a comparison with the human performance of these tasks (experts and reference lists), plus a comparison with other systems performing the same task. This qualitative evaluation is measured by the traditional IR performance measures (silence, noise, recall and precision). The first thing we can remark on is that it is very difficult to fulfill the evaluation within our proposed terms of reference. We are presenting hereunder the reasons limiting the scope of our protocol when applied to SRE results. Overview of the Results of SRE on the Two Corpora What we observed is that these tools are too different to allow a useful comparison for the following reasons: - SRE extract different types of relations and hence are incomparable. - This difference is linked to the different forms of semantic model implementation. Conversely some extractors are based on models that will not allow the type of relations required for the three evaluation tasks. - SRE are designed for different functions and have different objectives or carry out different tasks. - These differences are reflected in the type of output or results. - Another problem came from the fact that INRA could not provide us with a structured list corresponding to the eight selected texts. Even if this list had been available, comparing it to the results would have been of limited interest only. The remaining solution was to submit the results to a field specialist. - Difficulties in interpreting the non-labeled Semantic Relations. Fig. Two shows these differences: Fig. 2. Synthetic comparison table for SRE IOTA LORIA SEEK-JAVA Objectives Building indexes of one or more levels (layers) for document retrieval Scientific &amp; technical watch (identifying rare or new information a) Cognitive text organization b) Extraction of Labeled Semantic relations between terms in a thesaurus or a network of terms c)Constructing/modeling Knowledge-based systems Functions Information seeking systems, automatic extraction of complex indexes extraction Semantic relations extraction and representation Presentation in a conceptual graph fashion Building relational data bases Out-put Lists of potential candidate terms ranked by with frequency non-labeled Logico- Semantic Relations between terms A descriptive network/graph of terms linked with semantic relations between complex or simple terms, on the one hand and a triplet of argument-relation-argument on the other assembled in a relational data-base Terminology networks Classes of terms - Moreover, it is difficult or even impossible to measure silence using a protocol based on IR systems performance measure. • Without a prior knowledge of the missing possible relations one cannot account for the • To account for noise, a thorough knowledge of both the semantic model and the field of knowledge is required. These observation are also valid for We can thus say for the time being that SRE cannot be assessed by the protocol since their results cannot be matched. field gave the following “It is essential to have an interface to manipulate and interpret the relations. Everything seemed somewhat inconclusive. At times the relation “fits well”, at times it does not at all. Results are not always relevant and it is difficult to trust this type of analysis on its own if one is not at the same time be conversant with the domain, since some of the relations can be wrong. For Iota, concept extraction seems generally one has to wonder about the relevance of a number of extracted concepts which are not at all relevant to the field. How did these non-specific concepts get extracted more easily than others ? As for the table on Conceptual Semantic it is hard to draw any conclusions from it since it offers only one semantic label for any relation. The Iota approach is more global than the Seek- Java one since the relations are based on the whole document and not only at the level of one sentence. These two softwares are thus difficult to compare since their purpose is not the same”. the Classifier: an ad hoc Evaluation Given the difficulties we listed above and the fact that it was impossible to compare Conterm with other systems performing the same task. The only possible evaluation for Conterm would been a evaluation this sole of the This problem shows again the limits of our Protocol. The Conterm lists were matched to an automatically produced Volland-Neil, from INRA-Tours evaluator is referring to the tables accompanying the results provided by the system’s designer. protocol is not suitable for its evaluation. After the withdrawal of another participant who had also presented a classifier, only this one remained. untagged list of terms which corresponds to the eight texts of the INRA corpus. The most important element in its evaluation is not that we matched its results with a tagged list but that the results had been matched with indexers’ and/or experts lists and that we could observe the correspondence between Conterm’s output and the lists. It does not mean that Conterm is good for indexing but that the classes suggested by this tool embody conceptual attributes which are close to the logic underlying the human selection of candidate-terms suitable for indexing, namely its rich lexico-semantic network. Remarks - This evaluating action provided us with an awareness of the State-of-the-art in the field of terminology acquisition tools. It also allowed us to test evaluation paradigms, demonstrating how difficult it was to apply a single evaluation protocol to a variety of systems operating along different lines. - The discussions among participants aiming at the creation of a testing protocol resulted in the definition of an evaluation procedure and in an assessment of their relative merits. The comparative study of the systems’ out-put also enabled a better understanding of the performances of the wide range of techniques involved. As by-products of the project two corpora can be used in further evaluation campaigns and a set of material tests (real-life and constructed or specifically tailored one that can be shared during future evaluations). - The evaluation results can be used predictively for system design, development or modification The limits of our evaluation approach can be sketched in the following manner: - If the adopted protocol based upon reference list can be applicable to the two tasks (translation and terminology) it is hardly applicable to indexing tasks. - It is not adequate to account neither for the classifiers nor for the SRE. - Several questions remain unanswered: a) first, is it possible to fully automate evaluation procedures? Then is it possible to abandon test material, such as reference lists or other type of human-made data, which are considered as a kind of gold standard reusable for other evaluation campaigns? (see our recent experience in MT evaluation workshop, April b) As far as semantic relation extraction is concerned, is it possible to automate SRE valuation procedure in the way Grefensttete (1994) does? Directions 1. Exploiting Results: the Campaign’s Side Benefits: treatment of the will allow the creation of an index of all the reviews past numbers, which fulfills the moral contract made with its Editorial Board in exchange for getting the corpus free of charge. In addition, these results can help broaden the terminological repository for the education sciences, especially drawing up the Thesaurus covers all education sciences. 2. Towards Trans-Systemic Integration: The output of the systems are divergent but can in some cases be complementary. In fact the preliminary results drawn from the first in 1997 (cf. Béguin 2000) have led us to consider the feasibility of transsystemic integration for strengthening their automatic terms identification capabilities. The idea is to combine two or three different types of systems in order to specify various integrated production processes. Systems could 21“Setting a methodology for Machine Translation evaluation”. The context: evaluation of a translation made by an MT System on the following source text: INRA corpus text N°604 “ corpus biotechnologique sur la reproduction chez l’animal ” Source language: French - Target language: English. We carried out some manual testing but with the objective of setting a rough methodology that might be irrelevant for translating huge size corpora. The tool we used was a non interactive French / English MT System with a basic French/English dictionary that does not include any specific terminology. We had two indexes (a French index and an English index of domain specific expressions, but they are not aligned). They have been provided by the INRA and considered as gold standard. We used the indexes to create a specific dictionary in order to feed the MT systems with this specific lexical data. The next step is to assess the impact of specific terminology when integrated to an MT system by comparing the results of the two translations we get: with and without specific terminology. increasingly be seen as parts of these integrated production processes. 3. Towards User-Oriented Evaluations: in the light of the results obtained in this campaign the most suitable type of evaluation would be a one. Other types of be designed, such as adequacy which can to some extent be adopted for our case but we have to define a more strict user profile. 4. Towards developing interfaces for validating the results: even if we opted for a unified presentation format for the reasons mentioned in section 3.2.2.3, we however think it is essential for future campaign organizers to have an interface to manipulate and interpret the results (validating term, relations and classes). This type of interface can dramatically facilitate the interaction with the evaluators and the end-user of these tools. 5. Designing tools for generic bi-lingual allowing hoc through hoc 6. Capability to share resources in the future (test material such as gold standard lists, reallife and/or constructed ones). 7. Developing automatic evaluation tools such can be reused in similar future evaluations. 8. Hypothesis are still to be tested for semantic relations extraction: results of the various semantic extractors will be of different quality depending on the type and nature of corpora (domain and genre) chosen (cf. also Condamines 98; Davidson al among many others). The authors gratefully acknowledge the assistance provided by (Association des Universit6s Francophones) in funding the general research project within which this paper was written. et al. 1996; King 1999, among many others, identified more three types of evaluation processes: the adequacy evaluation and evaluation. first and second types are used for comparative benchmarking. evaluation at finding out whether a system or product is adequate to someone’s needs. This type is typically done when thinking of acquiring a system. 9. References Adda, G., Lecompte, J., Mariani, J., Paroubek, P., Rajman, M. (2000). Les proc6dures de mesure automatique de l’action GRACE pour l’6valuation des assignateurs de partie du discours pour le français, Chibout, K., Mariani, J., Masson, N., F. 6ds., (2000). et évaluation en de la Duculot, Coll. Champs linguistiques, et Collection Francophones (AUF), pp. 645-664. B6guin, A, Jouis, C, Mustafa El Hadi , W, (1997): &amp;quot;Evaluation d’outils d’aide à la construction de terminologie et de relations s6mantiques entre à partir de corpus&amp;quot;, In AUPELF-UREF, Avignon, avril 1997, pp. 419- This article is published in Chibout 2001 (eds.) pp 161-179. B6guin, A., Jouis, Ch., Mustafa Elhadi, W. (2000). Evaluation d’outils d’aide à l’extraction et à la construction automatiques de termes et de relations s6mantiques. In Chibout, K., Mariani, J., Masson, Neel, F. 6ds., (2000). et évaluation ingénierie de la Duculot, Coll. Champs linguistiques, et Collection Francophones (AUF), pp 161-179. Bourigault, D. (1993). “Analyse syntaxique locale pour le rep6rage de termes complexes dans un automatique des langues pp. 105-117. Bourigault, D., Jacquemin Ch. &amp;. L’Homme M-C. (1998). ’98, First Workshop on Terminology, 15 August 1998. Bruandet, M.F. (1989). Outline of a knowledge base model for an intelligent Information Retrieval system. In Information Processing and management, Vol 25, N° 3, 1989. Cavazza, M., (1993). M6thodes d&apos;6valuation des logiciels incorporant des d&apos;informatique linguistique, Paris, Rapport MRE-</abstract>
<note confidence="0.963544151515151">DIST, 1993. Chaudiron, S. (2000). The Relevance of Quality Model for NLP Applications, in Proceedings of RIAO, pp 1568-1577, Paris 12-I4 April 2000. Condamines, A. et Rebeyrolle, J. (1998). CTKB: A corpus-based approch to a Terminological Knowledge Base, In Bourrigault, D., Jacquemin Ch. &amp;. L’Homme M-C, 6ds.(1998). Computerm’98, First Workshop on Computational COLING-ACL’98, 15 August 1998, pp. 29-35. Dagan. I. and Church, K. (1994). Termight: Identifying and Translating Technical Terminology. In: Proceedings of 4th Applied NLP Conference 1994, 34-40. Daille, B., (1994). ACABIT: une maquette d’aide à la construction automatique de banques terminologiques monolingues ou bilingues. In Class, A., Thoiron, P, , B6joint (eds) Lexicomatique et Dictionnairiques, pp. 123-136, Beyrouth 1996. Daille, B. B. Habert, C. Jacquemin et J. Royaut6 (1995). “Empirical Observation of Term Variation Principles for their Description”, 3(2), pp. 197-257. Davidson, L., Kavanagh, J., Mackintoch, K., Meyer, I., Skuce, D. (1998). Semi-Automatic Extraction of Knowledge-Rich Contents from Corpora, In Bourrigault, D., Jacquemin, Ch. &amp;. L’Homme M- C, 6ds. (1998). Computerm’98, First Workshop on Computational Terminology, COLING-ACL’98, 15 August 1998, pp. 51-56. EAGLES (1996)</note>
<web confidence="0.874755">http://www.issco.unige.ch/projects/eagles.</web>
<note confidence="0.837133380952381">Enguehard, C. (1993). Acquisition de terminologie à partir de gros corpus. Informatique et langue naturelle ILN’ 93, Nantes, pp. 373-384, d6cembre 1993. Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery, Boston: Kluwer Academic- Press. [Gaussier, E. (1995). Modèles statistiques et patrons morphosyntaxiques pour l’extraction de lexiques bilingues de termes, Thèse de doctorat, Paris: Universit6 Paris VII. ISLE (2001) MT Evaluation Classification, Expanded Classification http://www.isi.edu/natural-language/mteval/2b- MT-classification.htm. Jouis, C., (1993). Contribution à la Conceptualisation et à la Mod6lisation des connaissances à partir d&apos;une analyse linguistique de textes. R6alisation d&apos;un prototype: le système SEEK. Thèse de doctorat. EHESS. Paris. 1993. Jouis, C., Mustafa El Hadi, W. (1997), AUPELF</note>
<title confidence="0.8137945">Project: Term and Semantic Relation Extraction Tools. Evaluation Paradigms, In Proc. of the Speech and Language Technology Club Workshop “ Evaluation in Speech and Language</title>
<note confidence="0.9385787">Technology ”, Univ. of Sheffied, June 17-18, Sheffield, UK, pp. 106-113. Kageura, K. and Umino, B. (1996). Methods of Automatic Term Recognition: A Review. In: Terminology Vol. 3(2), 1996, 259-289. Kilgarrif, A., Rosenzweig, J. (1998). English SENSVAL: Reports and Results. King (1999) EAGLES Evaluation Working Group, report, http://www.issco.unige.ch/projects/eagles. King M. (1996) EAGLES, Workshop, University of</note>
<abstract confidence="0.7339214">Geneva http://www.issco.unige.ch/projects/eagles. Le Priol, F, Chavallet, J-P., Bruandet, M-F., Desclès, J-P. (1998). Int6gration d’un système statistique (IOTA) et d’un système s6mantique (SEEK) dans une chaîne de traitement permettant l’extraction de</abstract>
<note confidence="0.7964945">Actes Ingénierie des Pont-à-Mousson, pages 33-40. 1998. L’Homme, Marie-Claude, Benali, Loubna, Bertrand, Claudine and Lauduique, Claudine. (1996).</note>
<title confidence="0.74077">Definition of an Evaluation Grid for Term-</title>
<author confidence="0.542408">In Terminology Vol</author>
<abstract confidence="0.842688666666667">1996, 291-312. L’Homme M-C. (2001). Évaluation d’outils d’aide à la construction automatique de terminologie et de relations s6mantiques entre termes à partir de corpus ARC-A3 Rapport final, Montr6al, mars 2001.</abstract>
<note confidence="0.714494666666667">Lauriston, A. (1994). Automatic Recognition of Complex Terms: Problems and the TERMINO Solution. In: Terminology 1(1), 147-170. Manzi S. (1999) Test Material for Evaluation, Sandra Manzi, ISSCO - University of Geneva http://www.issco.unige.ch/projects/eagles/ewg99. Mariani, J. Masson, N., Neel, F., 6ds. (2000). Ressources et évaluation en ingénierie de la Duculot, Coll. Champs linguistiques, et Collection Universit6s Francophones (AUF), 2000, pp. 13-24, actes des 1ères Journ6es Francil autour du thème: L&apos;ing6nierie de la Langue: de la Recherche au produit, Avignon 15-16 avril 1997. MUC-3, (1991). Proceedings of the Third Message Understanding Conference. Morgan Kaufmann. MUC-4, (1992). Proceedings of the Fourth Message Understanding Conference. Morgan Kaufmann. Mustafa El Hadi, W. &amp; Jouis, C. (1998), Terminology Extraction and Acquisition from Textual Data: Criteria for Evaluating Tools and Method, Proceedings of the First International Conference on Language Resources and Evaluation, Grenada, Spain may 1998, pp. 11750- 1178. Mustafa El Hadi, W. &amp; Jouis, C. (1997), &amp;quot;Natural language processing techniques and their use in data modeling and information retrieval&amp;quot;. In: Proceedings of the sixth international study conference on classification research, Knowledge Organization for Information Retrieval, University College London, London, 16-18 June 1997. The Hague: FID, 157-161. Mustafa El Hadi, W., Jouis, C. (1996a), Evaluating Natural Language Processing Systems as a Tool for Building Terminological Databases, In Proceedings of the Fourth International ISKO Conference: Knowledge Organization and Library of Congress, July in Knowledge Vol. 5, INDEX Verlag, Frankfurt/Main, pp. 346-355. Mustafa El Hadi, W., Jouis, C. (1996b), Natural Language Processing-based Systems for Terminological Construction and their to Information Retrieval. Proceedings of the Fourth International Congress on Terminology and Knowledge Engineering INDEX Frankfurt/Main. pp. 118-130. Seffah, A. Meunier, J.G. (1995). ALADIN :un atelier orienté objet pour l’analyse et la lecture de textes assistée par ordinateur. In : International Conference on statistics and texts Sparck-Jones K., Gallier, J.R. (1996). Evaluating Natural Language Processing Systems: An Analysis and Review, Springer, Berlin. Toussaint, Y. Namer F., Daille, B., Jacquemein, C., Royauté, J., Hathou N., (1998). Une approche linguistique et statistique pour l’analyse de l’information en corpus. In : TALN’ 98, Paris, France, 1998.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>G Adda</author>
<author>J Lecompte</author>
<author>J Mariani</author>
<author>P Paroubek</author>
<author>M Rajman</author>
</authors>
<title>Les proc6dures de mesure automatique de l’action GRACE pour l’6valuation des assignateurs de partie du discours pour le français,</title>
<date>2000</date>
<pages>645--664</pages>
<contexts>
<context position="9381" citStr="Adda et al., 2000" startWordPosition="1404" endWordPosition="1407">e-art, assess the needs in this sector and hence promote research in this specific field. Moreover, the principal aim of existing testing methods, as reported in the literature, is to come across software errors and then try to adapt them for a particular user environment. Evaluation paradigm is basically dependant upon two major steps: (i) Creation of textual data: raw or tagged corpora and test material. A corpus-based research is part of the infrastructure for the development of advanced language processing applications; (ii) Test and comparison of systems on a similar data (Cavazza, 1993; Adda et al., 2000). 3.1 The ARC A3 Evaluation Approach The approach we adopted is a black-box qualitative approach8 The results are compared 8 This approach is adopted and validated by the vast majority of participants to the test in June 1999. The organizers have slightly adapted the protocol because more participants joined the ARC after the validation of the protocol. with the human performance of a task (either experts examining results or using reference lists or both). Moreover comparisons are made with other systems performing the same task. The results are finally calculated and translated in terms of t</context>
</contexts>
<marker>Adda, Lecompte, Mariani, Paroubek, Rajman, 2000</marker>
<rawString>Adda, G., Lecompte, J., Mariani, J., Paroubek, P., Rajman, M. (2000). Les proc6dures de mesure automatique de l’action GRACE pour l’6valuation des assignateurs de partie du discours pour le français, Chibout, K., Mariani, J., Masson, N., Neel, F. 6ds., (2000). Ressources et évaluation en ingénierie de la langue, Duculot, Coll. Champs linguistiques, et Collection Universit6s Francophones (AUF), pp. 645-664.</rawString>
</citation>
<citation valid="true">
<title>Evaluation d’outils d’aide à la construction de terminologie et de relations s6mantiques entre termes à partir de corpus&amp;quot;,</title>
<date>1997</date>
<booktitle>In JST&apos;97, FRANCIL, AUPELF-UREF,</booktitle>
<pages>419--426</pages>
<editor>B6guin, A, Jouis, C, Mustafa El Hadi , W,</editor>
<location>Avignon, avril</location>
<marker>1997</marker>
<rawString>B6guin, A, Jouis, C, Mustafa El Hadi , W, (1997): &amp;quot;Evaluation d’outils d’aide à la construction de terminologie et de relations s6mantiques entre termes à partir de corpus&amp;quot;, In JST&apos;97, FRANCIL, AUPELF-UREF, Avignon, avril 1997, pp. 419-426. This article is published in Chibout et al. 2001 (eds.) pp 161-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B6guin</author>
<author>Ch Jouis</author>
<author>Mustafa Elhadi</author>
<author>W</author>
</authors>
<title>Evaluation d’outils d’aide à l’extraction et à la construction automatiques de termes et de relations s6mantiques. In</title>
<date>2000</date>
<pages>161--179</pages>
<marker>B6guin, Jouis, Elhadi, W, 2000</marker>
<rawString>B6guin, A., Jouis, Ch., Mustafa Elhadi, W. (2000). Evaluation d’outils d’aide à l’extraction et à la construction automatiques de termes et de relations s6mantiques. In Chibout, K., Mariani, J., Masson, N., Neel, F. 6ds., (2000). Ressources et évaluation en ingénierie de la langue, Duculot, Coll. Champs linguistiques, et Collection Universit6s Francophones (AUF), pp 161-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Analyse syntaxique locale pour le rep6rage de termes complexes dans un texte”, Traitement automatique des langues 34(2),</title>
<date>1993</date>
<pages>105--117</pages>
<marker>Bourigault, 1993</marker>
<rawString>Bourigault, D. (1993). “Analyse syntaxique locale pour le rep6rage de termes complexes dans un texte”, Traitement automatique des langues 34(2), pp. 105-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>&amp; L’Homme M-C 6ds</author>
</authors>
<date>1998</date>
<booktitle>Computerm ’98, First Workshop on Computational Terminology, COLING-ACL’98,</booktitle>
<pages>15</pages>
<marker>6ds, 1998</marker>
<rawString>Bourigault, D., Jacquemin Ch. &amp;. L’Homme M-C. 6ds. (1998). Computerm ’98, First Workshop on Computational Terminology, COLING-ACL’98, 15 August 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Bruandet</author>
</authors>
<title>Outline of a knowledge base model for an intelligent Information Retrieval system.</title>
<date>1989</date>
<booktitle>In Information Processing and management, Vol 25, N° 3,</booktitle>
<marker>Bruandet, 1989</marker>
<rawString>Bruandet, M.F. (1989). Outline of a knowledge base model for an intelligent Information Retrieval system. In Information Processing and management, Vol 25, N° 3, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cavazza</author>
</authors>
<title>M6thodes d&apos;6valuation des logiciels incorporant des technologies d&apos;informatique linguistique,</title>
<date>1993</date>
<location>Paris, Rapport MREDIST,</location>
<contexts>
<context position="9361" citStr="Cavazza, 1993" startWordPosition="1402" endWordPosition="1403">the state-of-the-art, assess the needs in this sector and hence promote research in this specific field. Moreover, the principal aim of existing testing methods, as reported in the literature, is to come across software errors and then try to adapt them for a particular user environment. Evaluation paradigm is basically dependant upon two major steps: (i) Creation of textual data: raw or tagged corpora and test material. A corpus-based research is part of the infrastructure for the development of advanced language processing applications; (ii) Test and comparison of systems on a similar data (Cavazza, 1993; Adda et al., 2000). 3.1 The ARC A3 Evaluation Approach The approach we adopted is a black-box qualitative approach8 The results are compared 8 This approach is adopted and validated by the vast majority of participants to the test in June 1999. The organizers have slightly adapted the protocol because more participants joined the ARC after the validation of the protocol. with the human performance of a task (either experts examining results or using reference lists or both). Moreover comparisons are made with other systems performing the same task. The results are finally calculated and tran</context>
<context position="11609" citStr="Cavazza, 1993" startWordPosition="1746" endWordPosition="1747">al-oriented, that is focusing on discrepancies between performance results and initial system requirements. Sparck-Jones points out how the two types of measures are deeply interwoven although different in their nature: - Recall is a quantitative measure of system performance while - Declared Satisfaction is a qualitative one (i.e. such a measure is really qualitative even if the result of applying it to a set of users is a percentage figure). The qualitative approach in the evaluation process is the easiest one for end users. It means giving a value judgment on how the system globally works (Cavazza, 1993; Chaudiron, 2000). The dominant approach today is towards quantitative evaluations which are considered as more objective and reproducible than the qualitative approach (EAGLES-1 1996; ISLE 2001). The main attempt of these approaches is 9 We chose to accompany the qualitative approach (mainly based on manual evaluations) by a translation of the manual evaluations into numerical scales of values (see below for more details). to translate the concepts of relevance and quality into numerical data. Statistical approaches such as MUC 2 and TREC 3 are frequently used for this type of evaluation. (C</context>
<context position="12990" citStr="Cavazza 1993" startWordPosition="1963" endWordPosition="1964">developers cannot provide us (as test organizers) with their systems, the only way was to send them the text corpora and let them provide us with the results. A glass-box evaluation would have required an examination of the systems by the organizers which would have been impossible except for Xerox’s TermFinder and Logos System’s Knowledge Discovery, two commercialized systems. - Even if this approach may be criticized on account of its subjective side, end-users like it because of its usefulness when comparing two or more systems which differ in all their parameter settings. (Chaudiron 2000; Cavazza 1993). - A black-box evaluation is more oriented towards system’s end-user when compared to a glass-box evaluation. For the latter the test will involve analyzing the system’s functioning by looking at its different components. Each component is evaluated separately in itself. Such an approach allows for spotting and understanding the causes of dysfunctional results. It is a long term process which requires access to the internal parts of the system and an understanding of the architecture and global strategy of the software. This is obviously a developer oriented approach and not an enduser one (C</context>
</contexts>
<marker>Cavazza, 1993</marker>
<rawString>Cavazza, M., (1993). M6thodes d&apos;6valuation des logiciels incorporant des technologies d&apos;informatique linguistique, Paris, Rapport MREDIST, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chaudiron</author>
</authors>
<title>The Relevance of Quality Model for NLP Applications,</title>
<date>2000</date>
<booktitle>in Proceedings of RIAO,</booktitle>
<pages>1568--1577</pages>
<location>Paris</location>
<contexts>
<context position="11627" citStr="Chaudiron, 2000" startWordPosition="1748" endWordPosition="1749">at is focusing on discrepancies between performance results and initial system requirements. Sparck-Jones points out how the two types of measures are deeply interwoven although different in their nature: - Recall is a quantitative measure of system performance while - Declared Satisfaction is a qualitative one (i.e. such a measure is really qualitative even if the result of applying it to a set of users is a percentage figure). The qualitative approach in the evaluation process is the easiest one for end users. It means giving a value judgment on how the system globally works (Cavazza, 1993; Chaudiron, 2000). The dominant approach today is towards quantitative evaluations which are considered as more objective and reproducible than the qualitative approach (EAGLES-1 1996; ISLE 2001). The main attempt of these approaches is 9 We chose to accompany the qualitative approach (mainly based on manual evaluations) by a translation of the manual evaluations into numerical scales of values (see below for more details). to translate the concepts of relevance and quality into numerical data. Statistical approaches such as MUC 2 and TREC 3 are frequently used for this type of evaluation. (Chaudiron, 2000). 3</context>
<context position="12975" citStr="Chaudiron 2000" startWordPosition="1961" endWordPosition="1962">s: - Since most developers cannot provide us (as test organizers) with their systems, the only way was to send them the text corpora and let them provide us with the results. A glass-box evaluation would have required an examination of the systems by the organizers which would have been impossible except for Xerox’s TermFinder and Logos System’s Knowledge Discovery, two commercialized systems. - Even if this approach may be criticized on account of its subjective side, end-users like it because of its usefulness when comparing two or more systems which differ in all their parameter settings. (Chaudiron 2000; Cavazza 1993). - A black-box evaluation is more oriented towards system’s end-user when compared to a glass-box evaluation. For the latter the test will involve analyzing the system’s functioning by looking at its different components. Each component is evaluated separately in itself. Such an approach allows for spotting and understanding the causes of dysfunctional results. It is a long term process which requires access to the internal parts of the system and an understanding of the architecture and global strategy of the software. This is obviously a developer oriented approach and not an</context>
</contexts>
<marker>Chaudiron, 2000</marker>
<rawString>Chaudiron, S. (2000). The Relevance of Quality Model for NLP Applications, in Proceedings of RIAO, pp 1568-1577, Paris 12-I4 April 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A et Rebeyrolle Condamines</author>
<author>J</author>
</authors>
<title>CTKB: A corpus-based approch to a Terminological Knowledge Base,</title>
<date>1998</date>
<booktitle>In Bourrigault, D., Jacquemin Ch. &amp;. L’Homme M-C, 6ds.(1998). Computerm’98, First Workshop on Computational Terminology, COLING-ACL’98, 15</booktitle>
<pages>29--35</pages>
<marker>Condamines, J, 1998</marker>
<rawString>Condamines, A. et Rebeyrolle, J. (1998). CTKB: A corpus-based approch to a Terminological Knowledge Base, In Bourrigault, D., Jacquemin Ch. &amp;. L’Homme M-C, 6ds.(1998). Computerm’98, First Workshop on Computational Terminology, COLING-ACL’98, 15 August 1998, pp. 29-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I</author>
<author>K Church</author>
</authors>
<title>Termight: Identifying and Translating Technical Terminology. In:</title>
<date>1994</date>
<booktitle>Proceedings of 4th Applied NLP Conference</booktitle>
<pages>34--40</pages>
<marker>I, Church, 1994</marker>
<rawString>Dagan. I. and Church, K. (1994). Termight: Identifying and Translating Technical Terminology. In: Proceedings of 4th Applied NLP Conference 1994, 34-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>ACABIT: une maquette d’aide à la construction automatique de banques terminologiques monolingues ou bilingues.</title>
<date>1994</date>
<booktitle>In Class, A., Thoiron, P, , B6joint (eds) Lexicomatique et Dictionnairiques,</booktitle>
<pages>123--136</pages>
<marker>Daille, 1994</marker>
<rawString>Daille, B., (1994). ACABIT: une maquette d’aide à la construction automatique de banques terminologiques monolingues ou bilingues. In Class, A., Thoiron, P, , B6joint (eds) Lexicomatique et Dictionnairiques, pp. 123-136, Beyrouth 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B B Habert Daille</author>
<author>C Jacquemin et J</author>
</authors>
<title>Empirical Observation of Term Variation and Principles for their Description”,</title>
<date>1995</date>
<journal>Terminology</journal>
<volume>3</volume>
<issue>2</issue>
<pages>197--257</pages>
<marker>Daille, J, 1995</marker>
<rawString>Daille, B. B. Habert, C. Jacquemin et J. Royaut6 (1995). “Empirical Observation of Term Variation and Principles for their Description”, Terminology 3(2), pp. 197-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Davidson</author>
<author>J Kavanagh</author>
<author>K Mackintoch</author>
<author>I Meyer</author>
<author>D Skuce</author>
</authors>
<title>Semi-Automatic Extraction of Knowledge-Rich Contents from Corpora,</title>
<date>1998</date>
<booktitle>In Bourrigault, D., Jacquemin, Ch. &amp;. L’Homme MC, 6ds.</booktitle>
<pages>51--56</pages>
<marker>Davidson, Kavanagh, Mackintoch, Meyer, Skuce, 1998</marker>
<rawString>Davidson, L., Kavanagh, J., Mackintoch, K., Meyer, I., Skuce, D. (1998). Semi-Automatic Extraction of Knowledge-Rich Contents from Corpora, In Bourrigault, D., Jacquemin, Ch. &amp;. L’Homme MC, 6ds. (1998). Computerm’98, First Workshop on Computational Terminology, COLING-ACL’98, 15 August 1998, pp. 51-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES</author>
</authors>
<date>1996</date>
<note>http://www.issco.unige.ch/projects/eagles.</note>
<marker>EAGLES, 1996</marker>
<rawString>EAGLES (1996) http://www.issco.unige.ch/projects/eagles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Enguehard</author>
</authors>
<title>Acquisition de terminologie à partir de gros corpus.</title>
<date>1993</date>
<booktitle>Informatique et langue naturelle ILN’ 93, Nantes,</booktitle>
<pages>373--384</pages>
<marker>Enguehard, 1993</marker>
<rawString>Enguehard, C. (1993). Acquisition de terminologie à partir de gros corpus. Informatique et langue naturelle ILN’ 93, Nantes, pp. 373-384, d6cembre 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery,</title>
<date>1994</date>
<publisher>Kluwer AcademicPress.</publisher>
<location>Boston:</location>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery, Boston: Kluwer AcademicPress.</rawString>
</citation>
<citation valid="true">
<title>Modèles statistiques et patrons morphosyntaxiques pour l’extraction de lexiques bilingues de termes, Thèse de doctorat,</title>
<date>1995</date>
<location>Paris: Universit6 Paris VII.</location>
<marker>1995</marker>
<rawString>[Gaussier, E. (1995). Modèles statistiques et patrons morphosyntaxiques pour l’extraction de lexiques bilingues de termes, Thèse de doctorat, Paris: Universit6 Paris VII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ISLE</author>
</authors>
<title>MT Evaluation Classification, Expanded Classification</title>
<date>2001</date>
<note>http://www.isi.edu/natural-language/mteval/2bMT-classification.htm.</note>
<contexts>
<context position="11805" citStr="ISLE 2001" startWordPosition="1772" endWordPosition="1773">ent in their nature: - Recall is a quantitative measure of system performance while - Declared Satisfaction is a qualitative one (i.e. such a measure is really qualitative even if the result of applying it to a set of users is a percentage figure). The qualitative approach in the evaluation process is the easiest one for end users. It means giving a value judgment on how the system globally works (Cavazza, 1993; Chaudiron, 2000). The dominant approach today is towards quantitative evaluations which are considered as more objective and reproducible than the qualitative approach (EAGLES-1 1996; ISLE 2001). The main attempt of these approaches is 9 We chose to accompany the qualitative approach (mainly based on manual evaluations) by a translation of the manual evaluations into numerical scales of values (see below for more details). to translate the concepts of relevance and quality into numerical data. Statistical approaches such as MUC 2 and TREC 3 are frequently used for this type of evaluation. (Chaudiron, 2000). 3.1.1 The merits of a black-box evaluation Obviously this approach has its pros and cons. But it can be justified on the following basis: - Since most developers cannot provide us</context>
<context position="15721" citStr="ISLE 2001" startWordPosition="2392" endWordPosition="2393">which, in our evaluation framework, is made up of both custom-designed lists and real life lists / thesaurus. 3.2.2.1 Corpus Two corpora were tested: Spirale10 and INRA11. We have chosen a sample representing 10% of each corpus: for Spirale n° 19 was chosen. As for INRA corpus, the providers of this corpus suggested 8 articles (603, 604, 607, 609, 631, 666, 732, 740). 3.2.2.2 Reference Lists These lists are standard human professional results which can be used as performance exemplars or norms for comparison. This type of data is considered to be a gold standard (see SensEval, Kilgarrif 1998; ISLE 2001). For the INRA corpus the following lists have been created: For translation two lists were processed (i) a list created by a novice translator (ii) another one by a confirmed professional translator. 10 423 texts, 16 mega bytes 11 51 texts, 2,2 mega bytes. For indexing: six lists were created both by professional and by non professional indexers. We are not developing these lists in this paper given the limited scope of this type of evaluation from an indexing point of view. Hence the limited interest of term extraction tools for human indexing. We will however comment on the terminology list</context>
</contexts>
<marker>ISLE, 2001</marker>
<rawString>ISLE (2001) MT Evaluation Classification, Expanded Classification (2001). http://www.isi.edu/natural-language/mteval/2bMT-classification.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jouis</author>
</authors>
<title>Contribution à la Conceptualisation et à la Mod6lisation des connaissances à partir d&apos;une analyse linguistique de textes. R6alisation d&apos;un prototype: le système SEEK. Thèse de doctorat. EHESS.</title>
<date>1993</date>
<location>Paris.</location>
<marker>Jouis, 1993</marker>
<rawString>Jouis, C., (1993). Contribution à la Conceptualisation et à la Mod6lisation des connaissances à partir d&apos;une analyse linguistique de textes. R6alisation d&apos;un prototype: le système SEEK. Thèse de doctorat. EHESS. Paris. 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jouis</author>
<author>Mustafa El Hadi</author>
<author>W</author>
</authors>
<title>AUPELF Project: Term and Semantic Relation Extraction Tools. Evaluation Paradigms,</title>
<date>1997</date>
<booktitle>In Proc. of the Speech and Language Technology Club Workshop “ Evaluation in Speech and Language Technology ”, Univ. of Sheffied,</booktitle>
<pages>106--113</pages>
<location>Sheffield, UK,</location>
<marker>Jouis, El Hadi, W, 1997</marker>
<rawString>Jouis, C., Mustafa El Hadi, W. (1997), AUPELF Project: Term and Semantic Relation Extraction Tools. Evaluation Paradigms, In Proc. of the Speech and Language Technology Club Workshop “ Evaluation in Speech and Language Technology ”, Univ. of Sheffied, June 17-18, Sheffield, UK, pp. 106-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kageura</author>
<author>B Umino</author>
</authors>
<title>Methods of Automatic Term Recognition: A Review.</title>
<date>1996</date>
<journal>In: Terminology</journal>
<volume>3</volume>
<issue>2</issue>
<pages>259--289</pages>
<marker>Kageura, Umino, 1996</marker>
<rawString>Kageura, K. and Umino, B. (1996). Methods of Automatic Term Recognition: A Review. In: Terminology Vol. 3(2), 1996, 259-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarrif</author>
<author>J Rosenzweig</author>
</authors>
<title>English SENSVAL: Reports and Results.</title>
<date>1998</date>
<marker>Kilgarrif, Rosenzweig, 1998</marker>
<rawString>Kilgarrif, A., Rosenzweig, J. (1998). English SENSVAL: Reports and Results.</rawString>
</citation>
<citation valid="true">
<authors>
<author>King</author>
</authors>
<title>EAGLES Evaluation Working Group,</title>
<date>1999</date>
<note>report, http://www.issco.unige.ch/projects/eagles.</note>
<marker>King, 1999</marker>
<rawString>King (1999) EAGLES Evaluation Working Group, report, http://www.issco.unige.ch/projects/eagles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M King</author>
</authors>
<date>1996</date>
<institution>EAGLES, Workshop, University of Geneva</institution>
<note>http://www.issco.unige.ch/projects/eagles.</note>
<contexts>
<context position="10323" citStr="King, 1996" startWordPosition="1549" endWordPosition="1550">dation of the protocol. with the human performance of a task (either experts examining results or using reference lists or both). Moreover comparisons are made with other systems performing the same task. The results are finally calculated and translated in terms of traditional IR measures9. The conventional distinction between black-box and glass-box is the following: the former considers only system input-out-put relations without regard to the specific mechanisms by which the outputs were obtained while the latter examines the mechanisms linking input and output. (Sparck-Jones, 1996 p. 26; King, 1996; 1999, among many others). The qualitative evaluation measures as described by Sparck-Jones 1996, pp. 61-122, are based on observation or interviewing and are broadly designed to obtain a more holistic, less reductive or fragmented view of the situation. It is moreover more naturalistic. This type of evaluation naturally fits an end-free style. In our case the quality of the results is evaluated by domain experts. We distinguish two types of experts: experts for the three applications tested (systematic terminology, translation and indexing); and experts in the two domains of corpora (biotech</context>
</contexts>
<marker>King, 1996</marker>
<rawString>King M. (1996) EAGLES, Workshop, University of Geneva http://www.issco.unige.ch/projects/eagles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Le Priol</author>
<author>J-P Chavallet</author>
<author>M-F Bruandet</author>
<author>J-P Desclès</author>
</authors>
<title>Int6gration d’un système statistique (IOTA) et d’un système s6mantique (SEEK) dans une chaîne de traitement permettant l’extraction de terminologies. Actes Ingénierie des Connaissances, Pont-à-Mousson,</title>
<date>1998</date>
<pages>33--40</pages>
<marker>Le Priol, Chavallet, Bruandet, Desclès, 1998</marker>
<rawString>Le Priol, F, Chavallet, J-P., Bruandet, M-F., Desclès, J-P. (1998). Int6gration d’un système statistique (IOTA) et d’un système s6mantique (SEEK) dans une chaîne de traitement permettant l’extraction de terminologies. Actes Ingénierie des Connaissances, Pont-à-Mousson, pages 33-40. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Claude L’Homme</author>
<author>Loubna Benali</author>
<author>Claudine Bertrand</author>
<author>Claudine Lauduique</author>
</authors>
<title>Definition of an Evaluation Grid for TermExtraction Software.</title>
<date>1996</date>
<journal>In: Terminology</journal>
<volume>3</volume>
<issue>2</issue>
<pages>291--312</pages>
<marker>L’Homme, Benali, Bertrand, Lauduique, 1996</marker>
<rawString>L’Homme, Marie-Claude, Benali, Loubna, Bertrand, Claudine and Lauduique, Claudine. (1996). Definition of an Evaluation Grid for TermExtraction Software. In: Terminology Vol. 3(2), 1996, 291-312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-C L’Homme</author>
</authors>
<title>Évaluation d’outils d’aide à la construction automatique de terminologie et de relations s6mantiques entre termes à partir de corpus ARC-A3 Rapport final,</title>
<date>2001</date>
<location>Montr6al, mars</location>
<marker>L’Homme, 2001</marker>
<rawString>L’Homme M-C. (2001). Évaluation d’outils d’aide à la construction automatique de terminologie et de relations s6mantiques entre termes à partir de corpus ARC-A3 Rapport final, Montr6al, mars 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lauriston</author>
</authors>
<title>Automatic Recognition of Complex Terms: Problems and the TERMINO Solution. In:</title>
<date>1994</date>
<journal>Terminology</journal>
<volume>1</volume>
<issue>1</issue>
<pages>147--170</pages>
<contexts>
<context position="6260" citStr="Lauriston, 1994" startWordPosition="938" endWordPosition="939">ted in our evaluation project, from the point of view of their functions and of the purposes for which they were designed), there are three categories: &amp;quot;Term Extractors&amp;quot;, &amp;quot;Classifying Tools&amp;quot;, and &amp;quot;Semantic relations extraction tools&amp;quot;. As we already mentioned, these systems were extensively described in our previous publications. 2.2.1 Term Extractors (TE) We will briefly describe the basic idea underlying TE tools. Most of the extracting tools consider terms as noun phrases. Systems identify terms by using frequency, distribution and category-pattern matching (Daille et al. 1995; Dagan, 1996; Lauriston, 1994). All lexical units contained in a given text are analyzed and matched to patterns (typical forms of terminological units) described in rules. More term extractors are accounted for elsewhere (L&apos;Homme, 1996; Kageura et al., 1996; Dagan et al., 1994). Some of the systems described by these authors are tested in the framework of our valuation project (Acabit, Lexter, and Ana). 2.2.2 Classifiers and Semantic Relation Extractors (SRE) Terminology resources are increasingly seen as structured data i.e. as a network of terms organized by relations. Pure alphabetical lists can hardly be used except f</context>
</contexts>
<marker>Lauriston, 1994</marker>
<rawString>Lauriston, A. (1994). Automatic Recognition of Complex Terms: Problems and the TERMINO Solution. In: Terminology 1(1), 147-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manzi</author>
</authors>
<date>1999</date>
<institution>Test Material for Evaluation, Sandra Manzi, ISSCO - University of Geneva</institution>
<note>http://www.issco.unige.ch/projects/eagles/ewg99.</note>
<marker>Manzi, 1999</marker>
<rawString>Manzi S. (1999) Test Material for Evaluation, Sandra Manzi, ISSCO - University of Geneva http://www.issco.unige.ch/projects/eagles/ewg99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Masson Mariani</author>
<author>N Neel</author>
<author>F</author>
</authors>
<title>Ressources et évaluation en ingénierie de la langue, Duculot, Coll. Champs linguistiques, et Collection Universit6s Francophones (AUF),</title>
<date>2000</date>
<booktitle>actes des 1ères Journ6es Francil autour du thème: L&apos;ing6nierie de la Langue: de la Recherche au produit,</booktitle>
<pages>13--24</pages>
<location>Avignon</location>
<marker>Mariani, Neel, F, 2000</marker>
<rawString>Mariani, J. Masson, N., Neel, F., 6ds. (2000). Ressources et évaluation en ingénierie de la langue, Duculot, Coll. Champs linguistiques, et Collection Universit6s Francophones (AUF), 2000, pp. 13-24, actes des 1ères Journ6es Francil autour du thème: L&apos;ing6nierie de la Langue: de la Recherche au produit, Avignon 15-16 avril 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-3</author>
</authors>
<date>1991</date>
<booktitle>Proceedings of the Third Message Understanding Conference.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>MUC-3, 1991</marker>
<rawString>MUC-3, (1991). Proceedings of the Third Message Understanding Conference. Morgan Kaufmann. MUC-4, (1992). Proceedings of the Fourth Message Understanding Conference. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mustafa El Hadi</author>
<author>W</author>
<author>C Jouis</author>
</authors>
<title>Terminology Extraction and Acquisition from Textual Data: Criteria for Evaluating Tools and</title>
<date>1998</date>
<booktitle>Method, Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<pages>11750--1178</pages>
<location>Grenada, Spain</location>
<marker>El Hadi, W, Jouis, 1998</marker>
<rawString>Mustafa El Hadi, W. &amp; Jouis, C. (1998), Terminology Extraction and Acquisition from Textual Data: Criteria for Evaluating Tools and Method, Proceedings of the First International Conference on Language Resources and Evaluation, Grenada, Spain may 1998, pp. 11750-1178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mustafa El Hadi</author>
<author>W</author>
<author>C Jouis</author>
</authors>
<title>Natural language processing techniques and their use in data modeling and information retrieval&amp;quot;. In:</title>
<date>1997</date>
<booktitle>Proceedings of the sixth international study conference on classification research, Knowledge Organization for Information Retrieval,</booktitle>
<pages>157--161</pages>
<location>University College London, London,</location>
<marker>El Hadi, W, Jouis, 1997</marker>
<rawString>Mustafa El Hadi, W. &amp; Jouis, C. (1997), &amp;quot;Natural language processing techniques and their use in data modeling and information retrieval&amp;quot;. In: Proceedings of the sixth international study conference on classification research, Knowledge Organization for Information Retrieval, University College London, London, 16-18 June 1997. The Hague: FID, 157-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mustafa El Hadi</author>
<author>W Jouis</author>
<author>C</author>
</authors>
<title>Evaluating Natural Language Processing Systems as a Tool for Building Terminological Databases,</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth International ISKO Conference: Knowledge Organization and</booktitle>
<marker>El Hadi, Jouis, C, 1996</marker>
<rawString>Mustafa El Hadi, W., Jouis, C. (1996a), Evaluating Natural Language Processing Systems as a Tool for Building Terminological Databases, In Proceedings of the Fourth International ISKO Conference: Knowledge Organization and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Washington Change</author>
</authors>
<title>Library of Congress,</title>
<date>1996</date>
<booktitle>Advances in Knowledge Organization,</booktitle>
<volume>5</volume>
<pages>346--355</pages>
<publisher>INDEX Verlag, Frankfurt/Main,</publisher>
<marker>Change, 1996</marker>
<rawString>Change, Washington, Library of Congress, July 1996, Advances in Knowledge Organization, Vol. 5, INDEX Verlag, Frankfurt/Main, pp. 346-355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mustafa El Hadi</author>
<author>W Jouis</author>
<author>C</author>
</authors>
<title>Natural Language Processing-based Systems for Terminological Construction and their Contribution to Information Retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth International Congress on Terminology and Knowledge Engineering (TKE&apos;96),</booktitle>
<pages>118--130</pages>
<publisher>Verlag, Frankfurt/Main.</publisher>
<location>Vienna, INDEX</location>
<marker>El Hadi, Jouis, C, 1996</marker>
<rawString>Mustafa El Hadi, W., Jouis, C. (1996b), Natural Language Processing-based Systems for Terminological Construction and their Contribution to Information Retrieval. In Proceedings of the Fourth International Congress on Terminology and Knowledge Engineering (TKE&apos;96), Vienna, INDEX Verlag, Frankfurt/Main. pp. 118-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meunier Seffah</author>
<author>J G</author>
</authors>
<title>ALADIN :un atelier orienté objet pour l’analyse et la lecture de textes assistée par ordinateur.</title>
<date>1995</date>
<booktitle>In : International Conference on statistics and texts</booktitle>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<marker>Seffah, G, 1995</marker>
<rawString>Seffah, A. Meunier, J.G. (1995). ALADIN :un atelier orienté objet pour l’analyse et la lecture de textes assistée par ordinateur. In : International Conference on statistics and texts Sparck-Jones K., Gallier, J.R. (1996). Evaluating Natural Language Processing Systems: An Analysis and Review, Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Namer F Toussaint</author>
<author>B Daille</author>
<author>C Jacquemein</author>
<author>J Royauté</author>
<author>N Hathou</author>
</authors>
<title>Une approche linguistique et statistique pour l’analyse de l’information en corpus.</title>
<date>1998</date>
<booktitle>In : TALN’ 98,</booktitle>
<location>Paris, France,</location>
<marker>Toussaint, Daille, Jacquemein, Royauté, Hathou, 1998</marker>
<rawString>Toussaint, Y. Namer F., Daille, B., Jacquemein, C., Royauté, J., Hathou N., (1998). Une approche linguistique et statistique pour l’analyse de l’information en corpus. In : TALN’ 98, Paris, France, 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>