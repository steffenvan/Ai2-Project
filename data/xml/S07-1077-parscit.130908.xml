<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035639">
<title confidence="0.9979445">
UBC-UPC: Sequential SRL Using Selectional Preferences.
An aproach with Maximum Entropy Markov Models
</title>
<author confidence="0.915517">
Be˜nat Zapirain, Eneko Agirre
</author>
<affiliation confidence="0.921808">
IXA NLP Group
University of the Basque Country
Donostia, Basque Country
</affiliation>
<email confidence="0.997494">
{benat.zapirain,e.agirre}@ehu.es
</email>
<author confidence="0.953524">
Lluis M`arquez
</author>
<affiliation confidence="0.984981">
TALP Research Center
Technical University of Catalonia
</affiliation>
<address confidence="0.930071">
Barcelona, Catalonia
</address>
<email confidence="0.998995">
lluism@lsi.upc.edu
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997275">
We present a sequential Semantic Role La-
beling system that describes the tagging
problem as a Maximum Entropy Markov
Model. The system uses full syntactic in-
formation to select BIO-tokens from input
data, and classifies them sequentially us-
ing state-of-the-art features, with the addi-
tion of Selectional Preference features. The
system presented achieves competitive per-
formance in the CoNLL-2005 shared task
dataset and it ranks first in the SRL subtask
of the Semeval-2007 task 17.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999584827586207">
In Semantic Role Labeling (SRL) the goal is to iden-
tify word sequences or arguments accompanying the
predicate and assign them labels depending on their
semantic relation. In this task we disambiguate ar-
gument structures in two ways: predicting VerbNet
(Kipper et al., 2000) thematic roles and PropBank
(Palmer et al., 2005) numbered arguments, as well
as adjunct arguments.
In this paper we describe our system for the SRL
subtask of the Semeval2007 task 17. It is based on
the architecture and features of the system named
‘model 2’ of (Surdeanu et al., forthcoming), but it
introduces two changes: we use Maximum Entropy
for learning instead of AdaBoost and we enlarge the
feature set with combined features and other seman-
tic features.
Traditionally, most of the features used in SRL
are extracted from automatically generated syntac-
tic and lexical annotations. In this task, we also ex-
periment with provided hand labeled semantic infor-
mation for each verb occurrence such as the Prop-
Bank predicate sense and the Levin class. In addi-
tion, we use automatically learnt Selectional Prefer-
ences based on WordNet to generate a new kind of
semantic based features.
We participated in both the “close” and the “open”
tracks of Semeval2007 with the same system, mak-
ing use, in the second case, of the larger CoNLL-
2005 training set.
</bodyText>
<sectionHeader confidence="0.968481" genericHeader="method">
2 System Description
</sectionHeader>
<subsectionHeader confidence="0.988818">
2.1 Data Representation
</subsectionHeader>
<bodyText confidence="0.999934357142857">
In order to make learning and labeling easier, we
change the input data representation by navigating
through provided syntactic structures and by extract-
ing BIO-tokens from each of the propositions to be
annotated as shown in (Surdeanu et al., forthcom-
ing). These sequential tokens are selected by ex-
ploring the sentence spans or regions defined by the
clause boundaries, and they are labeled with BIO
tags depending on the location of the token: at the
beginning, inside, or outside of a verb argument. Af-
ter this data pre-processing step, we obtain a more
compact and easier to process data representation,
making also impossible overlapping and embedded
argument predictions.
</bodyText>
<subsectionHeader confidence="0.998553">
2.2 Feature Representation
</subsectionHeader>
<bodyText confidence="0.999139333333333">
Apart from Selectional Preferences (cf. Section 3)
and those extracted from provided semantic infor-
mation, most of the features we used are borrowed
from the existing literature (Gildea and Jurafsky,
2002; Xue and Palmer, 2004; Surdeanu et al., forth-
coming).
</bodyText>
<page confidence="0.987127">
354
</page>
<bodyText confidence="0.888819666666667">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357,
Prague, June 2007. c�2007 Association for Computational Linguistics
On the verb predicate:
</bodyText>
<listItem confidence="0.989333086956522">
• Form; Lemma; POS tag; Chunk type and Type
of verb phrase; Verb voice; Binary flag indicat-
ing if the verb is a start/end of a clause.
• Subcategorization, i.e., the phrase structure rule
expanding the verb parent node.
• VerbNet class of the verb (in the ”close” track
only).
On the focus constituent:
• Type; Head;
• First and last words and POS tags of the con-
stituent.
• POS sequence.
• Bag-of-words of nouns, adjectives, and adverbs
in the constituent.
• TOP sequence: right-hand side of the rule ex-
panding the constituent node; 2/3/4-grams of
the TOP sequence.
• Governing category as described in (Gildea
and Jurafsky, 2002).
Context of the focus constituent:
• Previous and following words and POS tags of
the constituent.
• The same features characterizing focus con-
</listItem>
<bodyText confidence="0.9171005">
stituents are extracted for the two previous and
following tokens, provided they are inside the
clause boundaries of the codified region.
Relation between predicate and constituent:
</bodyText>
<listItem confidence="0.998157772727273">
• Relative position; Distance in words and
chunks; Level of embedding with respect to the
constituent: in number of clauses.
• Binary position; if the argument is after or be-
fore the predicate.
• Constituent path as described in (Gildea and
Jurafsky, 2002); All 3/4/5-grams of path con-
stituents beginning at the verb predicate or end-
ing at the constituent.
• Partial parsing path as described in (Carreras
et al., 2004)); All 3/4/5-grams of path elements
beginning at the verb predicate or ending at the
constituent.
• Syntactic frame as described by Xue and
Palmer (2004)
Combination Features
• Predicate and Phrase Type
• Predicate and binary position
• Head Word and Predicate
• Predicate and PropBank frame sense
• Predicate, PropBank frame sense, VerbNet
class (in the ”close” track only)
</listItem>
<subsectionHeader confidence="0.946021">
2.3 Maximum Entropy Markov Models
</subsectionHeader>
<bodyText confidence="0.974681333333333">
Maximum Entropy Markov Models are a discrimi-
native model for sequential tagging that models the
local probability P(sn  |sn−1, o), where o is the
context of the observation.
Given a MEMM, the most likely state sequence is
the one that maximizes the following
</bodyText>
<equation confidence="0.987438">
n
5 = argmax P(si  |si−1, o)
i=1
</equation>
<bodyText confidence="0.997363363636364">
Translating the problem to SRL, we have
role/argument labels connected to each state in the
sequence (or proposition), and the observations are
the features extracted in these points (token fea-
tures). We get the most likely label sequence finding
out the most likely state sequence (Viterbi).
All the conditional probabilities are given by the
Maximum Entropy classifier with a tunable Gaus-
sian prior from the Mallet Toolkit1.
Some restrictions are considered when we search
the most likely sequence2:
</bodyText>
<listItem confidence="0.999655214285714">
1. No duplicate argument classes for A0-A5 and
thematic roles.
2. If there is a R-X argument (reference), then
there has to be a X argument before (refer-
enced).
3. If there is a C-X argument (continuation), then
there has to be a X argument before.
4. Before a I-X token, there has to be a B-X or I-X
token (because of the BIO encoding).
5. Given a predicate and its PropBank sense, only
some arguments are allowed (e.g. not all the
verbs support A2 argument).
6. Given a predicate and its Verbnet class, only
some thematic roles are allowed.
</listItem>
<sectionHeader confidence="0.967274" genericHeader="method">
3 Including Selectional Preferences
</sectionHeader>
<bodyText confidence="0.999839625">
Selectional Preferences (SP) try to capture the fact
that linguistic elements prefer arguments of a cer-
tain semantic class, e.g. a verb like ‘eat’ prefers as
subject edible things, and as subject animate entities,
as in “She was eating an apple” They can be learned
from corpora, generalizing from the observed argu-
ment heads (e.g. ‘apple’, ‘biscuit’, etc.) into ab-
stract classes (e.g. edible things). In our case we
</bodyText>
<footnote confidence="0.997104666666667">
1http://mallet.cs.umass.edu
2Restriction 5 applies to PropBank output. Restriction 6 ap-
plies to VerbNet output
</footnote>
<page confidence="0.997313">
355
</page>
<bodyText confidence="0.99952971875">
follow (Agirre and Martinez, 2001) and use Word-
Net (Fellbaum, 1998) as the generalization classes
(the concept &lt;food,nutrient&gt;).
The aim of using Selectional Preferences (SP) in
SRL is to generalize from the argument heads in
the training instances into general word classes. In
theory, using word classes might overcome the data
sparseness problem for the head-based features, but
at the cost of introducing some noise.
More specifically, given a verb, we study the oc-
currences of the target verb in a training corpus (e.g.
the PropBank corpus), and learn a set of SPs for
each argument and adjunct of that verb. For in-
stance, given the verb ‘kill’ we would have 2 SPs
for each argument type, and 4 SPs for some of the
observed adjuncts: kill A0, kill A1, kill AM-
LOC, kill AM-MNR, kill AM-PNC and kill AM-
TMP.
Rather than coding the SPs directly as features,
we code the predictions instead, i.e. for each propo-
sition in the training and testing set, we check the
SPs for all the argument (and adjunct) headwords,
and the SP which best fits the headword (see below)
is the one that is selected. We codify the predicted
argument (or adjunct) label as features, and we insert
them among the corresponding argument features.
For instance, let’s assume that the word ‘railway’
appears as the headword of a candidate argument of
‘kill’. WordNet 1.6 yields the following hypernyms
for ‘railway’ (from most general to most specific, we
include the WordNet 1.6 concept numbers preceded
by their specifity level);
</bodyText>
<figure confidence="0.78738775">
1 00001740 1 00017954
2 00009457 2 05962976
3 00011937 3 05997592
4 03600463 4 06004580
5 03243979 5 06008236
6 03526208 6 06005839
7 03208595 7 02927599
8 03209020
</figure>
<bodyText confidence="0.9979639">
Note that we do not care about the sense ambigu-
ity and the explosion of concepts that it carries. Our
algorithm will check each of the hypernyms of rail-
way and match them with the concepts in the SPs of
‘kill’, giving preference to the most specific concept.
In case that equally specific concepts match different
SPs, we will choose the SP that has the concept that
ranks highest in the SP, and code the SP feature with
the label of the SP where the match succeeds. In the
example, these are the most specific matches:
</bodyText>
<table confidence="0.3679615">
AM-LOC Con:03243979 Level:5 Ranking:32
A0 Con:06008236 Level:5 Ranking:209
</table>
<bodyText confidence="0.999726555555556">
There is a tie in the level, so we choose the one
with the highest rank. All in all, this means that ac-
cording to the learnt SPs we would predict that ‘rail-
way’ is a location feature for ‘kill’, and we would
therefore insert the ‘SP:AM-LOC’ feature among
the argument features.
If ‘railway’ appears as the headword of other
verbs, the predicted argument might be different.
See for instance, the following verbs:
</bodyText>
<table confidence="0.9770655">
destroy:A1 Con:03243979 Level:5 Ranking:43
go:A0 Con:02927599 Level:7 Ranking:131
go:A2 Con:02927599 Level:7 Ranking:721
build:A1 Con:03209020 Level:8 Ranking:294
</table>
<bodyText confidence="0.999913133333333">
Note that our training examples did not contain
‘railway’ as an argument of any of these verbs, but
due to the SPs we are able to code into a feature that
‘railway’ belongs to a concrete semantic class which
contains conceptually similar headwords.
We decided to code the prediction of the SPs,
rather than the SPs themselves, in order to be more
robust to noise.
There is a further subtlety with our SP system. In
order to label training and testing sets in similar con-
ditions and avoid overfitting problems as much as
possible, we split the training set into five folds and
tagged each one with SPs learnt from the other four.
For extracting SP features from test set examples,
we use SPs learnt in the whole training set.
</bodyText>
<sectionHeader confidence="0.99684" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.9992172">
We participated in the “close” and the “open” tracks
with the same classification model, but using dif-
ferent training sets in each one. In the close track
we only use the provided training set, and in the
open, the CoNLL-2005 training set (without Verb-
Net classes or thematic roles).
Before our participation, we tested the system in
the CoNLL-2005 close track setting and it achieved
competitive performance in comparison to the state-
of-the-art results published in that challenge.
</bodyText>
<subsectionHeader confidence="0.99602">
4.1 Semeval2007 setting
</subsectionHeader>
<bodyText confidence="0.99811375">
The data provided in the close track consists of the
propositions of 50 different verb lemmas from Prop-
Bank (sections 02-21). The data for the CoNLL-
2005 is also a subset of the PropBank data, but it
</bodyText>
<page confidence="0.995956">
356
</page>
<table confidence="0.99964725">
Track Label rank prec. rec. F1
Close VerbNet 1st 85.31 82.08 83.66
Close PropBank 1st 85.04 82.07 83.52
Open PropBank 1st 84.51 82.24 83.36
</table>
<tableCaption confidence="0.943388">
Table 1: Results in the SRL subtask of SemEval-
2007 task 17
</tableCaption>
<bodyText confidence="0.997982">
includes all the propositions in sections 02-21 and
no VerbNet classes nor thematic roles for learning.
There is a total of 21 argument types for Prop-
Bank and 47 roles for VerbNet, which amounts to
21 * 2 + 1 = 43 BIO-labels for PropBank predic-
tions and 47 * 2 + 1 = 95 for VerbNet. We filtered
the less frequent (&lt;5).
We trained the Maximum Entropy classifiers with
114,380 examples for the close track, and with
828,811 for the open track. We tuned the classifier
by setting the Exponential Gaussian prior in 0.1
</bodyText>
<sectionHeader confidence="0.799404" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.99998">
In the close track we trained two classifiers, one
to label PropBank numbered arguments and a sec-
ond to label VerbNet thematic roles. Due to lack
of time, we only trained the PropBank labels in the
open track. Table 1 shows the results obtained in the
SRL subtask. We ranked first in all of them, out of
two participants.
</bodyText>
<subsectionHeader confidence="0.948086">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999993653846154">
The results indicate that in the close track the system
performs similarly on both PropBank arguments and
VerbNet roles. The absence of VerbNet class-based
features in the CoNLL-2005 training data could
cause the loss of performance in the open track. We
plan to perform the experiment on VerbNet roles for
the open track to check the ability of the classifier to
generalize across verbs.
Regarding the use of SP features, nowadays, we
have not obtained relevant improvements in the pre-
dictions of the classifiers. It is our first approach to
these kind of semantic features and there are more
sophisticated but evident extraction variants which
we are exploring.
Although the general performance is very simi-
lar without SP features, using them our system ob-
tains better results in ARG3 core arguments and in
the most frequent adjuncts such as location (LOC),
general-purpose (ADV) and temporal (TMP).
We reproduced this improvements in experiments
realized with CoNLL-2005 larger test sets. In that
case, we improved ARG3-ARG4 core arguments as
well as the mentioned adjuncts. There were more
examples to be classified and we get better overall
performance, but we need further experiments to be
more conclusive.
</bodyText>
<sectionHeader confidence="0.999721" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999987714285714">
We have presented a sequential semantic role la-
beling system for the Semeval-2007 task 17 (SRL).
Based on Maximum Entropy Markov Models, it ob-
tains competitive and promising results. We also
have introduced semantic features extracted from
Selectional Restrictions but we only have prelimi-
nary evidence of their usefulness.
</bodyText>
<sectionHeader confidence="0.999024" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999025">
We thank David Martinez for kindly providing the
software that learnt the selectional preferences. This
work has been partially funded by the Spanish ed-
ucation ministry (KNOW). Be˜nat is supported by a
PhD grant from the University of the Basque Coun-
try.
</bodyText>
<sectionHeader confidence="0.999457" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999954956521739">
E. Agirre and D. Martinez. 2001. Learning class-to-class
selectional preferences. In Proceedings of CoNLL-
2001, Toulouse, France.
X. Carreras, L. M`arquez, and G. Chrupała. 2004. Hi-
erarchical recognition of propositional arguments with
perceptrons. In Proceedings of CoNLL 2004.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics, 28(3).
K. Kipper, Hoa Trang Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon. In Pro-
ceedings of AAAI-2000 Seventeenth National Confer-
ence on Artificial Intellingence, Austin, TX.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
M. Surdeanu, L. M`arquez, X. Carreras, and P. Comas.
(forthcoming). Combination strategies for semantic
role labeling. In Journal of Artificial Intelligence Re-
search.
N. Xue and M. Palmer. 2004. Calibrating features for se-
mantic role labeling. In Proceedings ofEMNLP-2004.
</reference>
<page confidence="0.998198">
357
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.327729">
<title confidence="0.963689">UBC-UPC: Sequential SRL Using Selectional Preferences. An aproach with Maximum Entropy Markov Models</title>
<author confidence="0.846361">Eneko Agirre Zapirain</author>
<affiliation confidence="0.997524">IXA NLP Group University of the Basque Country</affiliation>
<address confidence="0.453891">Donostia, Basque Country</address>
<author confidence="0.875469">Lluis M`arquez</author>
<affiliation confidence="0.9994905">TALP Research Center Technical University of Catalonia</affiliation>
<address confidence="0.961771">Barcelona, Catalonia</address>
<email confidence="0.999764">lluism@lsi.upc.edu</email>
<abstract confidence="0.995492769230769">We present a sequential Semantic Role Labeling system that describes the tagging problem as a Maximum Entropy Markov Model. The system uses full syntactic information to select BIO-tokens from input data, and classifies them sequentially using state-of-the-art features, with the addition of Selectional Preference features. The system presented achieves competitive performance in the CoNLL-2005 shared task dataset and it ranks first in the SRL subtask of the Semeval-2007 task 17.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
</authors>
<title>Learning class-to-class selectional preferences.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL2001,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="7110" citStr="Agirre and Martinez, 2001" startWordPosition="1135" endWordPosition="1138">atic roles are allowed. 3 Including Selectional Preferences Selectional Preferences (SP) try to capture the fact that linguistic elements prefer arguments of a certain semantic class, e.g. a verb like ‘eat’ prefers as subject edible things, and as subject animate entities, as in “She was eating an apple” They can be learned from corpora, generalizing from the observed argument heads (e.g. ‘apple’, ‘biscuit’, etc.) into abstract classes (e.g. edible things). In our case we 1http://mallet.cs.umass.edu 2Restriction 5 applies to PropBank output. Restriction 6 applies to VerbNet output 355 follow (Agirre and Martinez, 2001) and use WordNet (Fellbaum, 1998) as the generalization classes (the concept &lt;food,nutrient&gt;). The aim of using Selectional Preferences (SP) in SRL is to generalize from the argument heads in the training instances into general word classes. In theory, using word classes might overcome the data sparseness problem for the head-based features, but at the cost of introducing some noise. More specifically, given a verb, we study the occurrences of the target verb in a training corpus (e.g. the PropBank corpus), and learn a set of SPs for each argument and adjunct of that verb. For instance, given </context>
</contexts>
<marker>Agirre, Martinez, 2001</marker>
<rawString>E. Agirre and D. Martinez. 2001. Learning class-to-class selectional preferences. In Proceedings of CoNLL2001, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L M`arquez</author>
<author>G Chrupała</author>
</authors>
<title>Hierarchical recognition of propositional arguments with perceptrons.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<marker>Carreras, M`arquez, Chrupała, 2004</marker>
<rawString>X. Carreras, L. M`arquez, and G. Chrupała. 2004. Hierarchical recognition of propositional arguments with perceptrons. In Proceedings of CoNLL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7143" citStr="Fellbaum, 1998" startWordPosition="1143" endWordPosition="1144">nal Preferences Selectional Preferences (SP) try to capture the fact that linguistic elements prefer arguments of a certain semantic class, e.g. a verb like ‘eat’ prefers as subject edible things, and as subject animate entities, as in “She was eating an apple” They can be learned from corpora, generalizing from the observed argument heads (e.g. ‘apple’, ‘biscuit’, etc.) into abstract classes (e.g. edible things). In our case we 1http://mallet.cs.umass.edu 2Restriction 5 applies to PropBank output. Restriction 6 applies to VerbNet output 355 follow (Agirre and Martinez, 2001) and use WordNet (Fellbaum, 1998) as the generalization classes (the concept &lt;food,nutrient&gt;). The aim of using Selectional Preferences (SP) in SRL is to generalize from the argument heads in the training instances into general word classes. In theory, using word classes might overcome the data sparseness problem for the head-based features, but at the cost of introducing some noise. More specifically, given a verb, we study the occurrences of the target verb in a training corpus (e.g. the PropBank corpus), and learn a set of SPs for each argument and adjunct of that verb. For instance, given the verb ‘kill’ we would have 2 S</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3137" citStr="Gildea and Jurafsky, 2002" startWordPosition="484" endWordPosition="487"> selected by exploring the sentence spans or regions defined by the clause boundaries, and they are labeled with BIO tags depending on the location of the token: at the beginning, inside, or outside of a verb argument. After this data pre-processing step, we obtain a more compact and easier to process data representation, making also impossible overlapping and embedded argument predictions. 2.2 Feature Representation Apart from Selectional Preferences (cf. Section 3) and those extracted from provided semantic information, most of the features we used are borrowed from the existing literature (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., forthcoming). 354 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357, Prague, June 2007. c�2007 Association for Computational Linguistics On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase; Verb voice; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. • VerbNet class of the verb (in the ”close” track only). On the focus constituent: • Type; Head; • First and last words and POS t</context>
<context position="4601" citStr="Gildea and Jurafsky, 2002" startWordPosition="720" endWordPosition="723"> as described in (Gildea and Jurafsky, 2002). Context of the focus constituent: • Previous and following words and POS tags of the constituent. • The same features characterizing focus constituents are extracted for the two previous and following tokens, provided they are inside the clause boundaries of the codified region. Relation between predicate and constituent: • Relative position; Distance in words and chunks; Level of embedding with respect to the constituent: in number of clauses. • Binary position; if the argument is after or before the predicate. • Constituent path as described in (Gildea and Jurafsky, 2002); All 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent. • Partial parsing path as described in (Carreras et al., 2004)); All 3/4/5-grams of path elements beginning at the verb predicate or ending at the constituent. • Syntactic frame as described by Xue and Palmer (2004) Combination Features • Predicate and Phrase Type • Predicate and binary position • Head Word and Predicate • Predicate and PropBank frame sense • Predicate, PropBank frame sense, VerbNet class (in the ”close” track only) 2.3 Maximum Entropy Markov Models Maximum Entropy Markov Model</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>Hoa Trang Dang</author>
<author>M Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI-2000 Seventeenth National Conference on Artificial Intellingence,</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="1128" citStr="Kipper et al., 2000" startWordPosition="159" endWordPosition="162">c information to select BIO-tokens from input data, and classifies them sequentially using state-of-the-art features, with the addition of Selectional Preference features. The system presented achieves competitive performance in the CoNLL-2005 shared task dataset and it ranks first in the SRL subtask of the Semeval-2007 task 17. 1 Introduction In Semantic Role Labeling (SRL) the goal is to identify word sequences or arguments accompanying the predicate and assign them labels depending on their semantic relation. In this task we disambiguate argument structures in two ways: predicting VerbNet (Kipper et al., 2000) thematic roles and PropBank (Palmer et al., 2005) numbered arguments, as well as adjunct arguments. In this paper we describe our system for the SRL subtask of the Semeval2007 task 17. It is based on the architecture and features of the system named ‘model 2’ of (Surdeanu et al., forthcoming), but it introduces two changes: we use Maximum Entropy for learning instead of AdaBoost and we enlarge the feature set with combined features and other semantic features. Traditionally, most of the features used in SRL are extracted from automatically generated syntactic and lexical annotations. In this </context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, Hoa Trang Dang, and M. Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of AAAI-2000 Seventeenth National Conference on Artificial Intellingence, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1178" citStr="Palmer et al., 2005" startWordPosition="167" endWordPosition="170">, and classifies them sequentially using state-of-the-art features, with the addition of Selectional Preference features. The system presented achieves competitive performance in the CoNLL-2005 shared task dataset and it ranks first in the SRL subtask of the Semeval-2007 task 17. 1 Introduction In Semantic Role Labeling (SRL) the goal is to identify word sequences or arguments accompanying the predicate and assign them labels depending on their semantic relation. In this task we disambiguate argument structures in two ways: predicting VerbNet (Kipper et al., 2000) thematic roles and PropBank (Palmer et al., 2005) numbered arguments, as well as adjunct arguments. In this paper we describe our system for the SRL subtask of the Semeval2007 task 17. It is based on the architecture and features of the system named ‘model 2’ of (Surdeanu et al., forthcoming), but it introduces two changes: we use Maximum Entropy for learning instead of AdaBoost and we enlarge the feature set with combined features and other semantic features. Traditionally, most of the features used in SRL are extracted from automatically generated syntactic and lexical annotations. In this task, we also experiment with provided hand labele</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="false">
<title>Combination strategies for semantic role labeling.</title>
<journal>In Journal of Artificial Intelligence Research.</journal>
<marker></marker>
<rawString>M. Surdeanu, L. M`arquez, X. Carreras, and P. Comas. (forthcoming). Combination strategies for semantic role labeling. In Journal of Artificial Intelligence Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>M Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMNLP-2004.</booktitle>
<contexts>
<context position="3159" citStr="Xue and Palmer, 2004" startWordPosition="488" endWordPosition="491">sentence spans or regions defined by the clause boundaries, and they are labeled with BIO tags depending on the location of the token: at the beginning, inside, or outside of a verb argument. After this data pre-processing step, we obtain a more compact and easier to process data representation, making also impossible overlapping and embedded argument predictions. 2.2 Feature Representation Apart from Selectional Preferences (cf. Section 3) and those extracted from provided semantic information, most of the features we used are borrowed from the existing literature (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., forthcoming). 354 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354–357, Prague, June 2007. c�2007 Association for Computational Linguistics On the verb predicate: • Form; Lemma; POS tag; Chunk type and Type of verb phrase; Verb voice; Binary flag indicating if the verb is a start/end of a clause. • Subcategorization, i.e., the phrase structure rule expanding the verb parent node. • VerbNet class of the verb (in the ”close” track only). On the focus constituent: • Type; Head; • First and last words and POS tags of the constituent</context>
<context position="4917" citStr="Xue and Palmer (2004)" startWordPosition="773" endWordPosition="776">ion. Relation between predicate and constituent: • Relative position; Distance in words and chunks; Level of embedding with respect to the constituent: in number of clauses. • Binary position; if the argument is after or before the predicate. • Constituent path as described in (Gildea and Jurafsky, 2002); All 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent. • Partial parsing path as described in (Carreras et al., 2004)); All 3/4/5-grams of path elements beginning at the verb predicate or ending at the constituent. • Syntactic frame as described by Xue and Palmer (2004) Combination Features • Predicate and Phrase Type • Predicate and binary position • Head Word and Predicate • Predicate and PropBank frame sense • Predicate, PropBank frame sense, VerbNet class (in the ”close” track only) 2.3 Maximum Entropy Markov Models Maximum Entropy Markov Models are a discriminative model for sequential tagging that models the local probability P(sn |sn−1, o), where o is the context of the observation. Given a MEMM, the most likely state sequence is the one that maximizes the following n 5 = argmax P(si |si−1, o) i=1 Translating the problem to SRL, we have role/argument </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>N. Xue and M. Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings ofEMNLP-2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>