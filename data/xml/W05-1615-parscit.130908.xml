<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.983103">
Evaluation of an NLG System using Post-Edit Data: Lessons Learnt
</title>
<author confidence="0.875746">
Somayajulu G. Sripada and Ehud Reiter and Lezan Hawizy
</author>
<affiliation confidence="0.995527">
Department of Computing Science
University of Aberdeen
</affiliation>
<address confidence="0.979032">
Aberdeen, AB24 3UE, UK
</address>
<email confidence="0.999179">
{ssripada,ereiter,lhawizy}@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.99564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999543625">
Post-editing is commonly performed on computer-
generated texts, whether from Machine Translation
(MT) or NLG systems, to make the texts accept-
able to end users. MT systems are often evaluated
using post-edit data. In this paper we describe our
experience of using post-edit data to evaluate
SUMTIME-MOUSAM, an NLG system that pro-
duces marine weather forecasts.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999624833333333">
Natural Language Generation (NLG) systems must of
course be evaluated, like all NLP systems. Previous work on
NLG evaluation has focused on either experiments con-
ducted with users who read the generated texts, or on com-
parisons of generated texts to corpora of human-written
texts. In this paper we describe an evaluation technique,
which looks at how much humans need to post-edit gener-
ated texts before they are released to users. Post-edit
evaluations are common in machine translation, but we be-
lieve that ours is the first large-scale post-edit evaluation of
an NLG system.
The system being evaluated is SUMTIME-MOUSAM [Sri-
pada et al, 2003], an NLG system, which generates marine
weather forecasts from Numerical Weather Prediction
(NWP) data. SUMTIME-MOUSAM is operational and is used
by Weathernews (UK) Ltd to generate 150 draft forecasts
per day, which are post-edited by Weathernews forecasters
before being released to clients.
</bodyText>
<sectionHeader confidence="0.997208" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.992845">
2.1 Evaluating NLG Systems
</subsectionHeader>
<bodyText confidence="0.9945055">
Common evaluation techniques for NLG systems [Mellish
and Dale, 1998] include:
</bodyText>
<listItem confidence="0.992606">
• Showing generated texts to users, and measuring how
effective they are at achieving their goal, compared to
some control text (for example, [Young, 1999])
• Asking experts to rate computer-generated texts in
various ways, and comparing this to their rating of
</listItem>
<bodyText confidence="0.886053894736842">
manually authored texts (for example, [Lester and
Porter, 1997])
• Automatically comparing generated texts to a corpus of
human authored texts (for example, [Bangalore et al,
2000]).
Each of these techniques is effective under different ap-
plication contexts in which NLG systems operate. For in-
stance, a corpus based technique is effective when a high
quality corpus is available. The appeal of post-edit evalua-
tion as done with SUMTIME-MOUSAM is that (A) the edits
should indicate actual mistakes instead of just differences in
how things can be said and (B) the amount of post-editing
required is a very important practical measure of how useful
the system is to real users (forecasters in our case).
Post-edit evaluations are a standard technique in Machine
Translation [Hutchins and Somers, 1992]. The only previ-
ous use of post-edit evaluation in NLG that we are aware of
is Mitkov and An Ha [2003], but their evaluation is rela-
tively small, and they give little information about it.
</bodyText>
<subsectionHeader confidence="0.99361">
2.2 SUMTIME-MOUSAM
</subsectionHeader>
<bodyText confidence="0.999737125">
SUMTIME-MOUSAM [Sripada et al, 2003] is an NLG system
that generates textual weather forecasts from numerical
weather prediction (NWP) data. The forecasts are marine
forecasts for offshore oilrigs. Table 1 shows a small extract
from the NWP data for 12-06-2002, and Table 2 shows part
of the textual forecast that SUMTIME-MOUSAM generates
from the NWP data. The Wind statements in Table 2 are
mostly based on the NWP data in Table 1.
</bodyText>
<table confidence="0.998081555555555">
Time Wind Wind Spd Wind Spd Gust Gust
Dir 10m 50m 10m 50m
06:00 W 10.0 12.0 12.0 16.0
09:00 W 11.0 14.0 14.0 17.0
12:00 WSW 10.0 12.0 12.0 16.0
15:00 SW 7.0 9.0 9.0 11.0
18:00 SSW 8.0 10.0 10.0 12.0
21:00 S 9.0 11.0 11.0 14.0
00:00 S 12.0 15.0 15.0 19.0
</table>
<tableCaption confidence="0.441781">
Table 1. Weather Data produced by an NWP model for 12-
Jun 2002
SUMTIME-MOUSAM generates texts in three stages
[Reiter and Dale, 2000].
Document Planning: Text structure is specified by
</tableCaption>
<bodyText confidence="0.998680052631579">
Weathernews, via a control file. The key content-
determination task is selecting ‘important’ or ‘significant’
data points from the underlying weather data to be included
in the forecast text. SUMTIME-MOUSAM uses a bottom-up
segmentation algorithm for this task [Sripada et al, 2002].
Micro-planning: The key decisions here are lexical selec-
tion, aggregation, and ellipsis. SUMTIME-MOUSAM uses
rules for this that are derived from corpus analysis and other
knowledge acquisition activities [Reiter et al, 2003; Sripada
et al, 2003].
Realization: SUMTIME-MOUSAM uses a simple realiser
that is tuned to the Weathernews weather sublanguage.
SUMTIME-MOUSAM is partially controlled by a control
data file that Weathernews can edit. For example, this file
specifies error function data that controls the segmentation
process for content determination. The error function data
decides the level of abstraction achieved by the segmenta-
tion process – the larger the error function value the higher
the level of abstraction achieved by segmentation.
</bodyText>
<subsectionHeader confidence="0.999546">
2.3 SUMTIME-MOUSAM at Weathernews
</subsectionHeader>
<bodyText confidence="0.999884111111111">
Weathernews (UK) Ltd, a private sector weather services
company, uses SUMTIME-MOUSAM to generate draft fore-
casts. The process is illustrated in Figure 1. Forecasters
load the NWP data for the forecast into Marfors, which is
Weathernews’ forecasting tool. Using Marfors, forecasters
edit the NWP data, using their meteorological expertise and
additional information such as satellite weather maps. They
then invoke SUMTIME-MOUSAM to generate an initial draft
of the forecast. This initial draft helps the forecaster under-
stand the NWP data, and often suggests further edits to the
NWP data. The generate-and-edit-data process may be re-
peated. When the forecaster is satisfied with the NWP data,
he invokes SUMTIME-MOUSAM again to generate a final
draft textual forecast, marked ‘Pre-edited Text’ in Figure 1.
The forecaster then uses Marfors to post-edit the textual
forecast. When the forecaster is done, Marfors assembles
the complete forecast from the individual fields, and sends it
to the customer.
</bodyText>
<table confidence="0.999807461538461">
Section 2. FORECAST 6 - 24 GMT, Wed 12-Jun 2002
Field Text
WIND(KTS) 10M W 8-13 backing SW by mid after-
noon and S 10-15 by midnight.
WIND(KTS) 50M W 10-15 backing SW by mid after-
noon and S 13-18 by midnight.
WAVES(M) 0.5-1.0 mainly SW swell.
SIG HT
WAVES(M) 1.0-1.5 mainly SW swell falling 1.0
MAX HT or less mainly SSW swell by after-
noon, then rising 1.0-1.5 by mid-
night.
WAVE PERIOD Wind wave 2-4 mainly 6 second
(SEC) SW swell.
WINDWAVE 2-4.
PERIOD (SEC)
SWELL PERIOD 5-7.
(SEC)
WEATHER Mainly cloudy with light rain
showers becoming overcast around
midnight.
VISIBILITY Greater than 10.
(NM)
AIR TEMP(C) 8-10 rising 9-11 around midnight.
CLOUD 4-6 ST/SC 400-600 lifting 6-8
(OKTAS/FT) ST/SC 700-900 around midnight.
</table>
<tableCaption confidence="0.9979195">
Table 2. Extract from SUMTIME-MOUSAM Forecast Pro-
duced for 12-Jun 2002 (AM).
</tableCaption>
<figure confidence="0.760135875">
Post-
edited
Text
SUMTIME-
MOUSAM
Text 1
Data 1
Marfors Text Editor
</figure>
<figureCaption confidence="0.993326">
Figure 1. Schematic Showing SUMTIME-MOUSAM Used at Weathernews
</figureCaption>
<figure confidence="0.99058">
Marfors
Data Editor
SUMTIME-
MOUSAM
Pre-edited
Text
Edited Data
NWP Data
Marfors
Data Editor
</figure>
<bodyText confidence="0.992158357142857">
Note that SUMTIME-MOUSAM is used for two purposes
by Weathernews; to help forecasters understand and there-
fore edit the NWP data, and to help generate texts for cus-
tomers. In this paper we focus on evaluating the second
usage of the system (generating texts for customers).
When a forecast is complete, Marfors saves the final ed-
ited NWP data, marked ‘Edited data’ in Figure 1 and the
final edited forecast marked ‘Post-edited Text’ into a data-
base. This data is forwarded to us for 150 sites per day; this
is the basis of our post-edit evaluation. Marfors does not
directly save the SUMTIME-MOUSAM text that forecasters
edit (‘Pre-edited Text’ in Figure 1), but we can reconstruct
this text by running the system on the final edited NWP
data.
</bodyText>
<sectionHeader confidence="0.995786" genericHeader="method">
3 Post-Edit Evaluation
</sectionHeader>
<subsectionHeader confidence="0.98842">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.959832">
The evaluation was carried out on 2728 forecasts, collected
during period June to August 2003. Each forecast was
roughly of 400 words, so there are about 1 million words in
all in the corpus.
For each forecast, we have the following data
</bodyText>
<listItem confidence="0.978041">
• Data: The final edited NWP data
• Pre-edit text: The final draft forecast produced by
SUMTIME-MOUSAM, which we reconstruct as de-
scribed in Section 2.3.
• Post-edit text: The manually post-edited forecast,
which was sent to the client.
• Background information: includes date, location, and
forecaster
</listItem>
<bodyText confidence="0.9997775">
We do not currently use the NWP data (other than for
reconstructing SUMTIME-MOUSAM texts), although we
hope in the future to include it in our analyses, in a manner
roughly analogous to Reiter and Sripada [2003]. This data
set continues to grow, we receive approximately 150 new
forecasts per day.
</bodyText>
<subsectionHeader confidence="0.99991">
3.2 Analysis Procedure
</subsectionHeader>
<bodyText confidence="0.999910571428572">
The following procedure is performed automatically by a
software tool. First, we perform some data transformation
and cleaning. This includes breaking sentences up into
phrases, where each phrase describes the weather at one
point in time.
For example, the pre-edit text in Figure 2 would be bro-
ken up into three phrases:
</bodyText>
<footnote confidence="0.705322">
A1 SW 20-25
A2 backing SSW 28-33 by midday
A3 then gradually increasing 34-39 by midnight
</footnote>
<listItem confidence="0.908368">
A. Pre-edit Text: SW 20-25 backing SSW 28-33 by
midday, then gradually increasing 34-39 by midnight.
B. Post-edit Text: SW 22-27 gradually increasing
SSW 34-39.
</listItem>
<figureCaption confidence="0.97896">
Figure 2. Example pre-edit and post-edit texts from the post-
edit corpus
</figureCaption>
<bodyText confidence="0.792154">
The Figure 2 post-edit text is divided into two phrases:
</bodyText>
<equation confidence="0.5397605">
B1 SW 22-27
B2 gradually increasing SSW 34-39
</equation>
<bodyText confidence="0.9997439">
The second step is to align phrases from these two tables
as a preparation for comparison in the next step. Alignment
is a complex activity and is described in detail next. To start
with we generate an exhaustive list of all the possible com-
binations of phase alignments.
For example, consider the texts in Figure 2. Here we gen-
erate the following list of possible alignments:
{(A1, B1), (A1, B2), (A2, B1), (A2, B2), (A3, B1), (A3,
B2)}
Next, we compute match scores for each of these possi-
ble alignments and use them for selecting the right align-
ments. For each unedited phrase Ai, the alignment with the
highest matching score is selected. For the purpose of com-
puting the match scores, phrases are parsed using ‘parts of
speech’ designed for weather sublanguage such as direction,
speed and time. The total match score of a pair of phrases is
computed as the sum of the match scores for their constitu-
ents. Match score (MS) for a pair of constituents depends
upon their part of speech and also their degree of match. MS
is defined as a product of two terms as explained below:
</bodyText>
<listItem confidence="0.981969875">
• Match score due to degree of match: we assign a match
score of 2 for exact matches, 1 for partial matches and
0 for mismatches.
• Weight factor denoting importance of constituents for
alignment: Constituents belonging to certain parts of
speech (POS) are more significant for alignment than
others. For example, times are more significant for
alignment than verbs. Also weights are varied for the
</listItem>
<bodyText confidence="0.819931777777778">
same POS based on its context in the phrase. For ex-
ample, direction receives higher weight if it occurs in
a phrase without a time or speed. This is because in
such phrases direction is the only means for align-
ment.
Continuing with our example sentences in Figure 2, we
show below how we find an alignment for A3. As described
earlier, A3 can be aligned to either B1 or B2. The MS for
(A3, B1) is zero as shown in Table 3.
</bodyText>
<table confidence="0.999752571428571">
POS A3 B1 MS
conjunction Then &lt;none&gt; 0
Adverb Gradually &lt;none&gt; 0
Verb Increasing &lt;none&gt; 0
Direction &lt;none&gt; SW 0
Speed range 34-39 22-27 0
Time By midnight &lt;none&gt; 0
</table>
<tableCaption confidence="0.997368333333333">
Table 3 Match Score for A3 and B1
The MS for (A3, B2) is 2*(2*w1+w2) where w1 is the
weight for Adverb/verb and w2 (&gt;w1) for speed as shown in
Table 4. Based on the match scores computed above A3 is
aligned with B2. Similarly A1 is aligned with B1. A2 is
unaligned, and treated as a deleted phrase.
</tableCaption>
<table confidence="0.999968714285714">
POS A3 B2 MS
conjunction Then &lt;none&gt; 0
Adverb Gradually Gradually w1*2
Verb Increasing Increasing w1*2
Direction &lt;none&gt; SSW 0
Speed range 34-39 34-39 w2*2
Time By midnight &lt;none&gt; 0
</table>
<tableCaption confidence="0.998502">
Table 4. Match Score for A3 and B2
</tableCaption>
<bodyText confidence="0.999875615384615">
The third step is to compare aligned phrases, such as A1
and B1. One evaluation metric is based on comparing
aligned phrases as a whole. Here we simply record ‘match’
or ‘mismatch’. For example, both (A1, B1) and (A3, B2)
are mismatches. We then compare constituents in the
phrases to determine more details about the mismatches.
For this detailed comparison we use the domain-specific
part-of-speech tags described earlier. Each part-of-speech
should occur at most once in a phrase (in our weather sub-
language), so we simply align on the basis of the tag. After
constituents are aligned, we label each pre-edit/post-edit
pair as match, replace, add, or delete. For example, A and B
are analysed as in Table 5.
</bodyText>
<table confidence="0.999805333333333">
POS A B label
Direction SW SW match
Speed 20-25 22-27 replace
Conjunction then &lt;none&gt; delete
Adverb gradually gradually match
Verb increasing increasing match
Direction &lt;none&gt; SSW add
Speed 34-39 34-39 match
Time by midnight &lt;none&gt; delete
</table>
<tableCaption confidence="0.998529">
Table 5. Detailed Edit Analysis
</tableCaption>
<subsectionHeader confidence="0.999622">
3.3 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999898238095238">
We processed 2728 forecast pairs (pre-edited and post-
edited). These were divided into 73041 phrases. Out of
these, the alignment procedure failed to align 7608 (10%)
phrases. For instance, in the example of Section 3.2, phrase
A2 was not aligned with any B phrase. Alignment failure
generally indicates that the forecaster is unhappy with
SUMTIME-MOUSAM’s segmentation that is with the sys-
tem’s content determination. We have manually analysed
some of these cases, and in general it seems the forecasters
are performing more sophisticated data analysis than
SUMTIME-MOUSAM, and are also more sensitive to which
changes are significant enough to be reported to the user.
We have manually inspected alignment quality of 100
random phrase pairs to determine cases where our alignment
procedure erroneously aligned phrases. We found one case
of improper alignment. The pre-edited phrase ‘soon becom-
ing variable’ has not been aligned to its corresponding iden-
tical post-edited phrase. Inspection of the rest of the corpus
showed that this error repeated 54 times in the whole cor-
pus. These cases have been classified as alignment failures
and therefore do not affect the post-edit analysis.
</bodyText>
<table confidence="0.9999313">
Time (Hours) Direction Speed
00 ESE 12
03 ESE 12
06 ESE 11
09 ESE 11
12 ESE 10
15 ESE 8
18 ESE 9
21 ESE 11
24 ESE 13
</table>
<tableCaption confidence="0.973598">
Table 6. Wind 10m data for 14 Jul 2003
</tableCaption>
<bodyText confidence="0.944789571428571">
For example, consider the Wind 10m data shown in Table
6. Our content determination algorithm first segments the
data in table 6 (see Sripada et al [2002] for more details).
Segmentation is the process of fitting straight lines to a data
set in such a way that a minimum error is introduced by the
lines. Since the direction data is constant at ESE, there is
only one segment for this data.
</bodyText>
<figureCaption confidence="0.8116025">
Figure 3. Segmentation of Wind speed data shown in Ta-
ble 6.
</figureCaption>
<bodyText confidence="0.9892945">
Wind speed data however is segmented by two lines as
shown in Figure 3, one line joining the point (0,12) with
(15,8) and the second joining the point (15,8) with (24,13).
Our content selection algorithm therefore selects data points
</bodyText>
<figure confidence="0.988497916666667">
Segmentation of Wind 10m data
0 3 6 9 12 15 18 21 24
Wind Speed
14
12
10
8
6
4
2
0
Time
</figure>
<bodyText confidence="0.986479833333333">
(0,12), (15,8) and (24,13) to be included in the forecast. In
this case our system produced:
“ESE 10-15 gradually easing 10 or less by mid afternoon
then increasing 11-16 by midnight”
However, forecasters view this data as a special case and
don’t segment it the way we do. Here the wind speed is al-
ways in the range of ‘10-15’ except at 1500 and 1800 hours.
Therefore they mention the change as an additional informa-
tion to an otherwise constant wind speed. In this case, the
forecaster edited text is:
“ESE 10-15 decreasing 10 or less for a time later”.
Talking about the segmentation differences, one of the
forecasters at Weathernews told us that another factor af-
fecting segmentation is related to the end user. End users of
the marine forecasts are oil company staff who schedule
activities on the oilrigs in the North Sea. Over the years
forecasters at Weathernews have acquired a good under-
standing of the informational needs of the oil company staff.
So they use the forecast statements as messages to the end
users about the weather and know what kind of messages
will be useful to the end users. In the example texts shown
in Figure 2 the forecaster could have thought that the impor-
tant message to communicate about wind is that it is in-
creasing monotonically and is likely to be in the range be-
tween 22 (the actual initial wind speed) and 39. Everything
else distracts this primary message and therefore needs to be
avoided. Once again there is post segmentation reasoning
used by the forecasters. We are investigating better pattern
matching techniques and better user models to improve our
content selection.
</bodyText>
<table confidence="0.997999125">
S. No. Mismatch Type Freq. %
Ellipses (word additions 35874 65
and deletions)
Data Related Replacements 10781 20
(range and direction re-
placements)
Lexical Replacements 8264 15
Total 54919
</table>
<tableCaption confidence="0.8508915">
Table 7. Results of the Evaluation showing summary cate-
gories and their frequencies
</tableCaption>
<bodyText confidence="0.99939025">
Going back to the successfully aligned phrases, 43914
(60%) are perfect matches, and the remaining 21519 (30%)
are mismatches. Table 7 summarises the mismatches.
Here, each mismatch is classified as
</bodyText>
<listItem confidence="0.9240245">
• Ellipses: additions and deletions. For example, delet-
ing the time phrase by midnight in the (A3, B2) pair.
These generally indicate problems with SUMTrME-
MOUSAM’s aggregation and ellipsis.
• Data replacements: changes (replaces) to constituents
that directly convey NWP data, such as wind speed
</listItem>
<bodyText confidence="0.852904166666667">
and direction. For example, changing 20-25 to 22-27
in the (A1, B1) pair. These can indicate content prob-
lems. They also occur when forecasters believe the
NWP data is incorrect but decide to just correct the
forecast text and not the data (eg, skip generate-and-
edit step described in section 2.3).
</bodyText>
<listItem confidence="0.847111">
• Lexical replacements: All other changes (replaces). For
example, if the conjunction ‘then’ was replaced by
</listItem>
<bodyText confidence="0.999469131578948">
‘and’. This generally indicates a problem in
SUMTrME-MOUSAM’s lexicalisation strategy.
For each pair of phrases compared in the evaluation, we
have counted the number of times each edit operation such
as add, delete and replace is performed by forecasters. For
example consider the two phrase pairs shown in Table 5.
For the first phrase pair of ‘SW 20-25’ and ‘SW 22-27’ fore-
casters performed zero add, zero delete and one replace
operation (‘20-25’ is replaced by ‘22-27’). For the second
phrase pair of ‘then gradually increasing 34-39 by mid-
night’ and ‘gradually increasing SSW 34-39’ forecasters
performed one add (added ‘SSW’), two delete (deleted
‘then’ and ‘by midnight’) and zero replace operations. We
hypothesized that forecasters were making significantly
more add and delete operations than replace operations. For
verifying this, we have performed a pairwise t-test. Vari-
able1 for the t-test represents the sum of the counts of add
and delete operations for each pair of phrases. Variable2
represents the count of replace operations. For example, for
the two phrase pairs shown in Table 5, variable1 has values
of zero and three where as variable2 has values of one and
zero. This test showed (with a p value less than 10-20) that
forecasters were performing more additions and deletions
than replacements. In other words, ellipsis is the main prob-
lem in our system. Most (25235 out of 35874, 70%) of these
errors are deletions, where the forecaster deletes words from
SUMTrME-MOUSAM’s texts.
A manual analysis of some ellipsis cases has highlighted
some general phenomena. First of all, many ellipsis cases
are “downstream” consequences of earlier changes. For ex-
ample, if we look at the (A3, B2) pair above, this contains
three ellipsis changes: then was deleted, SSW was added,
and by midnight was deleted. The first two of these changes
are a direct consequence of the deletion of phrase A2. If
SUMTrME-MOUSAM’s content determination system was
changed so that it did not generate A2, then the micro plan-
ner would have expressed A3 as gradually increasing SSW
34-39 by midnight, which is identical to B2 except for by
midnight.
The deletion of by midnight is an example of another
common phenomenon, which is disagreement among indi-
viduals as to how text should be written. As described in
[Reiter et al, 2003], some forecasters elide the last time
phrase in simple sentences such as this one, and some do
not. An earlier version of SUMTrME-MOUSAM in fact
would have elided this time phrase, but we changed the be-
havior of the system in this regard after consultation. Ellip-
sis errors are inevitable in cases where the different fore-
casters disagree about when to elide. However, since post-
editors can delete words more quickly than they can add
words, it probably makes sense from a practical perspective
to be conservative about elision, and only elide in unambi-
guous cases. We will not further discuss data replacement
errors, since they reflect either content problems or cases
where NWP data was not corrected at the input time but
edited directly in the final text.
We have discussed lexical replacement errors in detail
elsewhere [Reiter and Sripada, 2002]. In general terms,
some errors reflect problems with SUMTIME-MOUSAM; for
example, the system overuses then as a connective, so fore-
casters often replaced then by alternative connectives such
as and. However, many lexical replacement errors simply
reflected the lexical preferences of individual forecasters
[Reiter and Sripada, 2002]. For example, SUMTIME-
MOUSAM always uses the verb easing to indicate a reduc-
tion in wind speed. Most forecasters were happy with this,
but 3 individuals usually changed this to decreasing.
A general observation is that some forecasters post-edited
texts much more than others. For example, while overall
28% of phrases were edited, edit rates by individual fore-
casters varied from 4% to 93%. We do not know why edit
rates vary so much, although it may be significant the indi-
vidual with the highest (93%) edit rate is one of the most
experienced forecasters, who takes well-justified pride in
producing well-crafted forecasts.
Summarizing the results of our evaluation:
</bodyText>
<listItem confidence="0.83150375">
1. SUMTIME-MOUSAM’s content determination can defi-
nitely be improved, by using more sophisticated segmenta-
tion techniques.
2. SUMTIME-MOUSAM’s micro-planner can certainly be
</listItem>
<bodyText confidence="0.993188466666667">
improved in places, for example by varying connectives.
However, many post-edits are due to individual differences,
which we cannot do anything about.
We are currently carrying out another evaluation of SUM-
TIME-MOUSAM by the end users, oilrig staff and other ma-
rine staff who regularly read weather forecasts. In this study
we compare user’s comprehension of weather information
from human written and computer generated forecast texts.
We also measure user ratings (preference) of human written
and computer generated texts. Preliminary results from our
study indicate that users make fewer mistakes on compre-
hension questions when they are shown texts that use com-
puter generated words with human selected content. Gener-
ally users seem to prefer computer generated texts to human
written texts given the same underlying weather data.
</bodyText>
<sectionHeader confidence="0.957813" genericHeader="method">
4 Lessons from our Post-Edit Evaluation
</sectionHeader>
<bodyText confidence="0.9996616875">
As stated in Section 2.1, we were attracted to post-edit
evaluation because we believed that (A) people would only
edit things that were clearly wrong; and (B) post-editing was
an important usefulness metric from the perspective of our
users (forecasters).
Looking back, (B) was certainly true. The amount of
post-editing that generated texts require is a crucial compo-
nent of the cost of using SUMTIME-MOUSAM, and hence of
the attractiveness of the system to users (forecasters). Al-
though we have not measured the time required for perform-
ing post-edits, we have used edit-distance measures used in
MT evaluations as an approximate cost metric. We have
computed our cost metric by setting different cost (weight)
values to different edit operations. Cost of add and replace
operations is set to 5 and cost of delete is set to 1 as used in
Su et al [1992]. The ratio of the cost of edits and the cost of
writing the entire forecast manually (adding all the words) is
computed to be 0.15. (A) however was perhaps less true
than we had hoped. Wagner [1998] also described post-
edited texts in MT as at times noisy. Our analysis of manu-
ally written forecasts [Reiter and Sripada, 2002] had high-
lighted a number of “noise” elements that made it more dif-
ficult to extract information from such corpora. Basically
there are many ways of communicating information in text,
and the fact that a generated text doesn’t match a corpus text
does not mean that the generated text is wrong. We as-
sumed that people would only post-edit mistakes, where the
generated text was wrong or sub-optimal, and hence post-
edit data would be better for evaluation purposes than cor-
pus comparisons.
In fact, however, there were many justifications for post-
edits:
</bodyText>
<listItem confidence="0.978723444444444">
1. Fixing problems in the generated texts (such as
overuse of then);
2. Refining/optimizing the texts (such as using for
a time);
3. Individual preferences (such as easing vs de-
creasing); and
4. Downstream consequences of earlier changes
(such as introducing SSW in B2, in the example
of Section 3.2).
</listItem>
<bodyText confidence="0.999996826086957">
We wanted to use our post-edit data to improve the sys-
tem, not just to quantify its performance, and we discovered
that we could not do this without attempting to analyze why
post-edits were made. Probably the best way of doing this
was to discuss post-edits with the forecasters. Alternatively,
we could have asked forecasters to fill in problem sheets to
capture their explanation of post-edits. Such feedback from
the forecasters would have allowed us to reason with post-
edit data to improve our system. In [Reiter et al, 2003] we
explained that we found that analysis of human-written cor-
pora was more useful if it was combined with directly work-
ing with domain experts; and essentially this (perhaps not
surprisingly) is our conclusion about post-edit data as well.
One of the lessons we learnt from this exercise has been
that post-edit evaluations are useful to compute a cost metric
to quantify the usefulness of a system. For example, as de-
scribed earlier, we have computed a cost metric, 0.15 signi-
fying the post-editing effort. Post-edit evaluations are also
useful in revealing general problem areas in a system. For
example, as described in section 3.3, our evaluation showed
that ellipsis related problems are more serious in our system
than others. However, post-edit evaluations are not affective
in discovering specific problems in a system. The main rea-
son for this is that many post-edits, as stated earlier, do not
actually fix problems in the generated text at all. The real
post-edits that fixed problems in the generated text were
buried among the other noisy post-edits.
This lesson of course is the result of our method of post-
edit evaluation. Post-editing was not supported by
SUMTIME-MOUSAM and forecasters used Marfors (see sec-
tion 2.3) to perform post-editing. Therefore, we had to ac-
cept the post-edit data with all the noise. In MT, post-editors
often work under predefined guidelines on post-editing and
also use post-editing tools. For example, post-editing tools
automatically revise texts to fix ‘down-stream’ conse-
quences of human edits. If post-edit tools are similarly inte-
grated into NLG systems, there is going to be a significant
reduction in the number of noisy post-edits allowing us to
focus on real post-edits.
Because post-editing is subjective varying from individ-
ual to individual, we need to understand the post-editing
behaviour of individuals to analyze the noisy post-edit data.
Although we have data on forecaster variations in our post-
edit corpus, these variations have not been observed from
different forecasters post-editing the same text. This we
could have achieved by performing a pilot before the actual
evaluation. For the pilot all the forecasters post-edit the
same set of forecasts, thus revealing their individual prefer-
ences. Post-edit data from the pilot would have enabled us
to factor out the effects of forecaster variation from the real
evaluation data. As described above noise in the post-edit
data can be reduced by using post-edit tools and by perform-
ing a pilot before the real evaluation. This means that post-
edit evaluations need preparation in the form of developing
post-edit tools and carrying out pilot studies. This is another
lesson we learnt from our current evaluation.
Although analyzing the post-edit data was a major en-
deavour for us, the overall cost of post-edit evaluation was
not much compared to the effort that would have been re-
quired to conduct end user experiments on 2728 texts. Of
course, this was only true because SUMTIME-MOUSAM
texts were being post-edited in any case by Weathernews.
The cost-effectiveness of post-edit evaluation is less clear if
the evaluators must organize and pay for the post-editing, as
Mitkov and An Ha [2003] did. In this context we should
speculate that when more and more NLG systems are de-
ployed in the real world, post-editing will be accepted as a
component in the process of automatic text generation much
in the same way post-editing is now a part of MT.
</bodyText>
<sectionHeader confidence="0.999271" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9997056">
Evaluation is a key aspect of NLG; we need to know how
well theories and systems work. We have used analysis of
post-edits, a popular evaluation technique in machine trans-
lation, to evaluate SUMTIME-MOUSAM, an NLG system
that generates marine weather forecasts. We encountered
some problems, such as the need to identify why post-edits
were made which make post-edit data hard to discover spe-
cific clues for system improvement. However, post-edit
evaluation can reveal problem areas in the system and also
quantify system utility for real users.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999836148148148">
[Bangalore et al., 2000] Srinivas Bangalore, Owen Ram-
bow, and Steve Whittaker. 2000. Evaluation metrics for
generation. In Proc. of the First International Natural
Language Generation Conference (INLG2000), Israel.
[Hutchins and Somers, 1992] John Hutchins and Harold L.
Somers, 1992. An Introduction to Machine Translation,
Academic Press.
[Lester and Porter, 1997] James Lester and Bruce Porter.
1997. Developing and empirically evaluating robust
explanation generators: The KNIGHT experiments.
Computational Linguistics, 23-1:65-103.
[Mellish and Dale, 1998] Chris Mellish and Robert Dale,
1998. Evaluation in the context of natural language gen-
eration, Computer Speech and Language 12:349-373.
[Mitkov and An Ha, 2003] Ruslan Mitkov and Le An Ha,
2003. Computer-Aided Generation of Multiple-Choice
Tests, In Proc. of the HLT-NAACL03 Workshop on
Building Educational Applications Using NLP, Edmon-
ton, Canada, pp. 17-22.
[Reiter and Dale, 2000] Ehud Reiter and Robert Dale, 2000.
Building Natural Language Generation Systems. Cam-
bridge University Press.
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu G.
Sripada, 2002. Human Variation and Lexical Choice.
Computational Linguistics 28:545-553.
[Reiter et al., 2003] Ehud Reiter, Somayajulu G. Sripada,
and Roma Robertson, 2003. Acquiring Correct Knowl-
edge for Natural Language Generation. Journal of Artifi-
cial Intelligence Research, 18: 491-516, 2003.
[Reiter and Sripada, 2003] Ehud Reiter and Somayajulu G.
Sripada, 2003. Learning the Meaning and Usage of Time
Phrases from a Parallel Text-Data Corpus. In Proc. of the
HLT-NAACL03 Workshop on Learning Word Meaning
from Non-Linguistic Data, pp 78-85.
[Sripada et al., 2002] Somayajulu, G. Sripada, Ehud Reiter,
Jim Hunter and Jin Yu. 2002 Segmenting Time Series
for Weather Forecasting. In: Macintosh, A., Ellis, R. and
Coenen, F. (ed) Proc. of ES2002, pp. 193-206.
[Sripada et al., 2003] Somayajulu G. Sripada, Ehud Reiter,
and Ian Davy, 2003. SUMTIME-MOUSAM: Configur-
able Marine Weather Forecast Generator. Expert Update,
6(3):4-10.
[Su et al., 1992] Keh-Yih Su, Ming-Wen Wu and Jing-Shin
Chang, 1992, A new quantitative quality measure for
machine translation systems. In Proceedings of
COLING-92, Nantes, pp 433-439.
[Wagner, 1998] Simone Wagner, 1998. Small Scale Evalua-
tion Methods In: Rita Nübel; Uta Seewald-Heeg (eds.):
Evaluation of the Linguistic Performance of Machine
Translation Systems. Proc. of the Workshop at the
KONVENS-98. Bonn, pp 93-105.
[Young, 1999] Michael Young, 1999. Using Grice’s maxim
of quantity to select the content of plan description, Arti-
ficial Intelligence 115:215-256.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822051">
<title confidence="0.999181">Evaluation of an NLG System using Post-Edit Data: Lessons Learnt</title>
<author confidence="0.988779">Somayajulu G Sripada</author>
<author confidence="0.988779">Ehud Reiter</author>
<author confidence="0.988779">Lezan</author>
<affiliation confidence="0.9994015">Department of Computing University of</affiliation>
<address confidence="0.862194">Aberdeen, AB24 3UE,</address>
<email confidence="0.997447">ssripada@csd.abdn.ac.uk</email>
<email confidence="0.997447">ereiter@csd.abdn.ac.uk</email>
<email confidence="0.997447">lhawizy@csd.abdn.ac.uk</email>
<abstract confidence="0.996100222222222">Post-editing is commonly performed on computergenerated texts, whether from Machine Translation (MT) or NLG systems, to make the texts acceptable to end users. MT systems are often evaluated using post-edit data. In this paper we describe our experience of using post-edit data to evaluate an NLG system that produces marine weather forecasts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
<author>Steve Whittaker</author>
</authors>
<title>Evaluation metrics for generation.</title>
<date>2000</date>
<booktitle>In Proc. of the First International Natural Language Generation Conference (INLG2000),</booktitle>
<marker>[Bangalore et al., 2000]</marker>
<rawString>Srinivas Bangalore, Owen Rambow, and Steve Whittaker. 2000. Evaluation metrics for generation. In Proc. of the First International Natural Language Generation Conference (INLG2000), Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hutchins</author>
<author>Harold L Somers</author>
</authors>
<title>An Introduction to Machine Translation,</title>
<date>1992</date>
<publisher>Academic Press.</publisher>
<marker>[Hutchins and Somers, 1992]</marker>
<rawString>John Hutchins and Harold L. Somers, 1992. An Introduction to Machine Translation, Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Lester</author>
<author>Bruce Porter</author>
</authors>
<title>Developing and empirically evaluating robust explanation generators: The KNIGHT experiments.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<marker>[Lester and Porter, 1997]</marker>
<rawString>James Lester and Bruce Porter. 1997. Developing and empirically evaluating robust explanation generators: The KNIGHT experiments. Computational Linguistics, 23-1:65-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Robert Dale</author>
</authors>
<title>Evaluation in the context of natural language generation,</title>
<date>1998</date>
<journal>Computer Speech and Language</journal>
<pages>12--349</pages>
<marker>[Mellish and Dale, 1998]</marker>
<rawString>Chris Mellish and Robert Dale, 1998. Evaluation in the context of natural language generation, Computer Speech and Language 12:349-373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
<author>Le An Ha</author>
</authors>
<title>Computer-Aided Generation of Multiple-Choice Tests,</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL03 Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>17--22</pages>
<location>Edmonton, Canada,</location>
<marker>[Mitkov and An Ha, 2003]</marker>
<rawString>Ruslan Mitkov and Le An Ha, 2003. Computer-Aided Generation of Multiple-Choice Tests, In Proc. of the HLT-NAACL03 Workshop on Building Educational Applications Using NLP, Edmonton, Canada, pp. 17-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>[Reiter and Dale, 2000]</marker>
<rawString>Ehud Reiter and Robert Dale, 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu G Sripada</author>
</authors>
<title>Human Variation and Lexical Choice.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<pages>28--545</pages>
<marker>[Reiter and Sripada, 2002]</marker>
<rawString>Ehud Reiter and Somayajulu G. Sripada, 2002. Human Variation and Lexical Choice. Computational Linguistics 28:545-553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu G Sripada</author>
<author>Roma Robertson</author>
</authors>
<title>Acquiring Correct Knowledge for Natural Language Generation.</title>
<date>2003</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>18</volume>
<pages>491--516</pages>
<marker>[Reiter et al., 2003]</marker>
<rawString>Ehud Reiter, Somayajulu G. Sripada, and Roma Robertson, 2003. Acquiring Correct Knowledge for Natural Language Generation. Journal of Artificial Intelligence Research, 18: 491-516, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Somayajulu G Sripada</author>
</authors>
<title>Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data Corpus.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL03 Workshop on Learning Word Meaning from Non-Linguistic Data,</booktitle>
<pages>78--85</pages>
<marker>[Reiter and Sripada, 2003]</marker>
<rawString>Ehud Reiter and Somayajulu G. Sripada, 2003. Learning the Meaning and Usage of Time Phrases from a Parallel Text-Data Corpus. In Proc. of the HLT-NAACL03 Workshop on Learning Word Meaning from Non-Linguistic Data, pp 78-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sripada Somayajulu</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Jin Yu</author>
</authors>
<title>Segmenting Time Series for Weather Forecasting.</title>
<date>2002</date>
<booktitle>Proc. of ES2002,</booktitle>
<pages>193--206</pages>
<location>In: Macintosh,</location>
<marker>[Sripada et al., 2002]</marker>
<rawString>Somayajulu, G. Sripada, Ehud Reiter, Jim Hunter and Jin Yu. 2002 Segmenting Time Series for Weather Forecasting. In: Macintosh, A., Ellis, R. and Coenen, F. (ed) Proc. of ES2002, pp. 193-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu G Sripada</author>
<author>Ehud Reiter</author>
<author>Ian Davy</author>
</authors>
<date>2003</date>
<booktitle>SUMTIME-MOUSAM: Configurable Marine Weather Forecast Generator. Expert Update,</booktitle>
<pages>6--3</pages>
<marker>[Sripada et al., 2003]</marker>
<rawString>Somayajulu G. Sripada, Ehud Reiter, and Ian Davy, 2003. SUMTIME-MOUSAM: Configurable Marine Weather Forecast Generator. Expert Update, 6(3):4-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Ming-Wen Wu</author>
<author>Jing-Shin Chang</author>
</authors>
<title>A new quantitative quality measure for machine translation systems.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92, Nantes,</booktitle>
<pages>433--439</pages>
<marker>[Su et al., 1992]</marker>
<rawString>Keh-Yih Su, Ming-Wen Wu and Jing-Shin Chang, 1992, A new quantitative quality measure for machine translation systems. In Proceedings of COLING-92, Nantes, pp 433-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Wagner</author>
</authors>
<title>Small Scale Evaluation Methods In: Rita Nübel; Uta Seewald-Heeg (eds.): Evaluation of the Linguistic Performance of Machine Translation Systems.</title>
<date>1998</date>
<booktitle>Proc. of the Workshop at the KONVENS-98.</booktitle>
<pages>93--105</pages>
<location>Bonn,</location>
<marker>[Wagner, 1998]</marker>
<rawString>Simone Wagner, 1998. Small Scale Evaluation Methods In: Rita Nübel; Uta Seewald-Heeg (eds.): Evaluation of the Linguistic Performance of Machine Translation Systems. Proc. of the Workshop at the KONVENS-98. Bonn, pp 93-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Young</author>
</authors>
<title>Using Grice’s maxim of quantity to select the content of plan description,</title>
<date>1999</date>
<journal>Artificial Intelligence</journal>
<pages>115--215</pages>
<marker>[Young, 1999]</marker>
<rawString>Michael Young, 1999. Using Grice’s maxim of quantity to select the content of plan description, Artificial Intelligence 115:215-256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>