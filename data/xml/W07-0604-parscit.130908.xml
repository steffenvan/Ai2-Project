<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000036">
<title confidence="0.987641">
High-accuracy Annotation and Parsing of CHILDES Transcripts
</title>
<author confidence="0.987552">
Kenji Sagae
</author>
<affiliation confidence="0.998475">
Department of Computer Science
University of Tokyo
</affiliation>
<address confidence="0.822544">
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
</address>
<email confidence="0.998663">
sagae@is.s.u-tokyo.ac.jp
</email>
<author confidence="0.977416">
Eric Davis
</author>
<affiliation confidence="0.967478">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.532121">
Pittsburgh, PA 15213
</address>
<email confidence="0.99772">
dhdavis@cs.cmu.edu
</email>
<author confidence="0.989748">
Alon Lavie
</author>
<affiliation confidence="0.850769333333333">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
</affiliation>
<email confidence="0.992041">
alavie@cs.cmu.edu
</email>
<author confidence="0.998215">
Brian MacWhinney
</author>
<affiliation confidence="0.9865155">
Department of Psychology
Carnegie Mellon University
</affiliation>
<address confidence="0.579104">
Pittsburgh, PA 15213
</address>
<email confidence="0.998618">
macw@cmu.edu
</email>
<author confidence="0.993616">
Shuly Wintner
</author>
<affiliation confidence="0.864833666666667">
Department of Computer Science
University of Haifa
31905 Haifa, Israel
</affiliation>
<email confidence="0.990199">
shuly@cs.haifa.ac.il
</email>
<sectionHeader confidence="0.995544" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952294117647">
Corpora of child language are essential for
psycholinguistic research. Linguistic anno-
tation of the corpora provides researchers
with better means for exploring the develop-
ment of grammatical constructions and their
usage. We describe an ongoing project that
aims to annotate the English section of the
CHILDES database with grammatical re-
lations in the form of labeled dependency
structures. To date, we have produced a cor-
pus of over 65,000 words with manually cu-
rated gold-standard grammatical relation an-
notations. Using this corpus, we have devel-
oped a highly accurate data-driven parser for
English CHILDES data. The parser and the
manually annotated data are freely available
for research purposes.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994478261904762">
In order to investigate the development of child lan-
guage, corpora which document linguistic interac-
tions involving children are needed. The CHILDES
database (MacWhinney, 2000), containing tran-
scripts of spoken interactions between children at
various stages of language development with their
parents, provides vast amounts of useful data for lin-
guistic, psychological, and sociological studies of
child language development. The raw information in
CHILDES corpora was gradually enriched by pro-
25
viding a layer of morphological information. In par-
ticular, the English section of the database is aug-
mented by part of speech (POS) tags for each word.
However, this information is usually insufficient for
investigations dealing with the syntactic, semantic
or pragmatic aspects of the data.
In this paper we describe an ongoing effort aim-
ing to annotate the English portion of the CHILDES
database with syntactic information based on gram-
matical relations represented as labeled dependency
structures. Although an annotation scheme for syn-
tactic information in CHILDES data has been pro-
posed (Sagae et al., 2004), until now no significant
amount of annotated data had been made publicly
available. In the process of manually annotating sev-
eral thousands of words, we updated the annotation
scheme, mostly by extending it to cover syntactic
phenomena that occur in real data but were unac-
counted for in the original annotation scheme.
The contributions of this work fall into three main
categories: revision and extension of the annota-
tion scheme for representing syntactic information
in CHILDES data; creation of a manually annotated
65,000 word corpus with gold-standard syntactic
analyses; and implementation of a complete parser
that can automatically annotate additional data with
high accuracy. Both the gold-standard annotated
data and the parser are freely available. In addi-
tion to introducing the parser and the data, we re-
port on many of the specific annotation issues that
we encountered during the manual annotation pro-
</bodyText>
<subsectionHeader confidence="0.575302">
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25–32,
Prague, Czech Republic, June 2007 c�2007 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999958666666667">
cess, which should be helpful for those who may
use the annotated data or the parser. The anno-
tated corpora and the parser are freely available from
http://childes.psy.cmu.edu/.
We describe the annotation scheme in the next
section, along with issues we faced during the pro-
cess of manual annotation. Section 3 describes the
parser, and an evaluation of the parser is presented in
section 4. We analyze the remaining parsing errors
in section 5 and conclude with some applications of
the parser and directions for future research in sec-
tion 6.
</bodyText>
<sectionHeader confidence="0.960626" genericHeader="method">
2 Syntactic annotation
</sectionHeader>
<bodyText confidence="0.999978444444444">
The English section of the CHILDES database is
augmented with automatically produced ambiguous
part-of-speech and morphological tags (MacWhin-
ney, 2000). Some of these data have been manually
disambiguated, but we found that some annotation
decisions had to be revised to facilitate syntactic an-
notation. We discuss below some of the revisions we
introduced, as well as some details of the syntactic
constructions that we account for.
</bodyText>
<subsectionHeader confidence="0.996495">
2.1 The morphological annotation scheme
</subsectionHeader>
<bodyText confidence="0.99306705">
The English morphological analyzer incorporated
in CHILDES produces various part-of-speech tags
(there are 31 distinct POS tags in the CHILDES
tagset), including ADJective, ADVerb, COmmuni-
cator, CONJunction, DETerminer, FILler, Noun,
NUMeral, ONomatopoeia, PREPosition, PROnoun,
ParTicLe, QuaNtifier, RELativizer and Verb1. In
most cases, the correct annotation of a word is obvi-
ous from the context in which the word occurs, but
sometimes a more subtle distinction must be made.
We discuss some common problematic issues below.
Adverb vs. preposition vs. particle The words
about, across, after, away, back, down, in, off, on,
out, over, up belong to three categories: ADVerb,
PREPosition and ParTicLe. To correctly annotate
them in context, we apply the following criteria.
First, a preposition must have a prepositional ob-
ject, which is typically realized as a noun phrase
(which may be topicalized, or even elided). Sec-
ond, a preposition forms a constituent with its noun
</bodyText>
<footnote confidence="0.961166">
1We use capital letters to denote the actual tag names in the
CHILDES tagset.
</footnote>
<bodyText confidence="0.999629575">
phrase object. Third, a prepositional object can be
fronted (for example, he sat on the chair becomes
the chair on which he sat), whereas a particle-NP
sequence cannot (*the phone number up which he
looked cannot be obtained from he looked up the
phone number). Finally, a manner adverb can be
placed between the verb and a preposition, but not
between a verb and a particle.
To distinguish between an adverb and a particle,
the meaning of the head verb is considered. If the
meaning of the verb and the target word, taken to-
gether, cannot be predicted from the meanings of the
verb and the target word separately, then the target
word is a particle. In all other cases it is an adverb.
Verbs vs. auxiliaries Distinguishing between
Verb and AUXiliary is often straightforward, but
special attention is given when tagging the verbs be,
do and have. If the target word is accompanied by an
non-finite verb in the same clause, as in I have had
enough or I do not like eggs, it is an auxiliary. Ad-
ditionally, in interrogative sentences, the auxiliary is
moved to the beginning of the clause, as in have I
had enough? and do Ilike eggs?, whereas the main
verb is not. However, this test does not always work
for the verb be, which may head a non-verbal pred-
icate, as in John is a teacher, vs. John is smiling. In
verb-participle constructions headed by the verb be,
if the participle is in the progressive tense, then the
head verb is labeled as auxiliary.
Communicators vs. locative adverbs COmmu-
nicators can be hard to distinguish from locative ad-
verbs, especially at the beginning of a sentence. Our
convention is that CO must modify an entire sen-
tence, so if a word appears by itself, it cannot be a
CO. For example, utterances like here or there are
labeled as ADVerb. However, if these words appear
at the beginning of a sentence, are followed by a
break or pause, and do not clearly express a location,
then they are labeled CO. Additionally, in here/there
you are/go, here and there are labeled CO.
</bodyText>
<subsectionHeader confidence="0.99901">
2.2 The syntactic annotation scheme
</subsectionHeader>
<bodyText confidence="0.9998328">
Our annotation scheme for representing grammati-
cal relations, or GRs (such as subjects, objects and
adjuncts), in CHILDES transcripts is a slightly ex-
tended version of the scheme proposed by Sagae et
al. (2004), which was inspired by a general annota-
</bodyText>
<page confidence="0.762547">
26
</page>
<bodyText confidence="0.919115575471698">
tion scheme for grammatical relations (Carroll et al., Finally, we added some specific relations for han-
1998), but adapted specifically for CHILDES data. dling problematic issues. For example, we use
Our scheme contains 37 distinct GR types. Sagae ENUMeration for constructions such as one, two,
et al. reported 96.5% interannotator agreement, and three, go or a, b, c. In COORDination construc-
we do not believe our minor updates to the annota- tions, each conjunct is marked as a dependent of the
tion scheme should affect interannotator agreement conjunction (e.g., go and get your telephone). We
significantly. use TOPicalization to indicate an argument that is
The scheme distinguishes among SUBJects, (fi- topicalized, as in tapioca, there is no tapioca. We
nite) Clausal SUBJects2 (e.g., that he cried moved use SeRiaL to indicate serial verbs as in come see
her) and XSUBJects (eating vegetables is impor- if we can find it or go play with your toys. Finally,
tant). Similarly, we distinguish among OBJects, we mark sequences of proper names which form the
OBJect2, which is the second object of a ditran- same entity (e.g., New York) as NAME.
sitive verb, and IOBjects, which are required verb The format of the grammatical relation (GR) an-
complements introduced by prepositions. Verb com- notation, which we use in the examples that follow,
plements that are realized as clauses are labeled associates with each word in a sentence a triple i|j|g,
COMP if they are finite (I think that was Fraser) and where i is the index of the word in the sentence, j the
XCOMP otherwise (you stop throwing the blocks). index of the word’s syntactic head, and g is the name
Additionally, we mark required locative adjectival of the grammatical relation represented by the syn-
or prepositional phrase arguments of verbs as LOCa- tactic dependency between the i-th and j-th words.
tives, as in put the toys in the box/back. If the topmost head of the utterance is the i-th word,
PREDicates are nominal, adjectival or prepo- it is labeled i|0|ROOT. For example, in:
sitional complements of verbs such as get, be a cookie .
and become, as in I’m not sure. Again, we 1|2|DET 2|0|ROOT 3|2|PUNCT
specifically mark Clausal PREDicates (This is
how I drink my coffee) and XPREDicates (My goal
is to win the competition).
the first word a is a DETerminer of word 2 (cookie),
which is itself the ROOT of the utterance.
Adjuncts (denoted by JCT) are optional modi- 2.3 Manual annotation of the corpus
fiers of verbs, adjectives or adverbs, and we dis- We focused our manual annotation on a set of
tinguish among non-clausal ones (That’s much bet- CHILDES transcripts for a particular child, Eve
ter; sit on the stool), finite clausal ones (CJCT, Mary (Brown, 1973), and we refer to these transcripts,
left after she saw John) and non-finite clausal ones distributed in a set of 20 files, as the Eve corpus.
(XJCT, Mary left after seeing John). We hand-annotated (including correcting POS tags)
MODifiers, which modify or complement nouns, the first 15 files of the Eve corpus following the
again come in three flavors: MOD (That’s a nice GR scheme outlined above. The annotation pro-
box); CMOD (the movie that I saw was good); and cess started with purely manual annotation of 5,000
XMOD (the student reading a book is tall). words. This initial annotated corpus was used to
We then identify AUXiliary verbs, as in did you train a data-driven parser, as described later. This
do it?; NEGation (Fraser is not drinking his coffee); parser was then used to label an additional 20,000
DETerminers (a fly); QUANTifiers (some juice); the words automatically, followed by a thorough manual
objects of prepositions (POBJ, on the stool); verb checking stage, where each syntactic annotation was
ParTicLes (can you get the blocks out?); ComPle- manually verified and corrected if necessary. We re-
mentiZeRs (wait until the noodles are cool); COM- trained the parser with the newly annotated data, and
municators (oh, I took it); the INfinitival to; VOCa- proceeded in this fashion until 15 files had been an-
tives (Thank you, Eve); and TAG questions (you notated and thoroughly manually checked.
know how to count, don’t you?). Annotating child language proved to be challeng-
ing, and as we progressed through the data, we no-
ticed grammatical constructions that the GRs could
2As with the POS tags, we use capital letters to represent the
actual GR tags used in the annotation scheme.
27
not adequately handle. For example, the original GR probabilistic shift-reduce algorithm, working left-to-
scheme did not differentiate between locative argu- right to find labeled dependencies one at a time. The
ments and locative adjuncts, so we created a new GR algorithm is essentially a dependency version of the
label, LOC, to handle required verbal locative argu- data-driven constituent parsing algorithm for prob-
ments such as on in put it on the table. Put licenses abilistic GLR-like parsing described by Sagae and
a prepositional argument, and the existing JCT rela- Lavie (2006). Because CHILDES syntactic annota-
tion could not capture this requirement. tions are represented as labeled dependencies, using
In addition to adding new GRs, we also faced a dependency parsing approach allows us to work
challenges with telegraphic child utterances lack- with that representation directly.
ing verbs or other content words. For instance, This dependency parser has been shown to have
Mommy telephone could have one of several mean- state-of-the-art accuracy in the CoNLL shared tasks
ings: Mommy this is a telephone, Mommy I want on dependency parsing (Buchholz and Marsi, 2006;
the telephone, that is Mommy’s telephone, etc. We Nivre, 2007)3. Sagae and Tsujii (2007) present a
tried to be as consistent as possible in annotating detailed description of the parsing approach used in
such utterances and determined their GRs from con- our work, including the parsing algorithm. In sum-
text. It was often possible to determine the VOC mary, the parser uses an algorithm similar to the LR
reading vs.the MOD (Mommy’s telephone) reading parsing algorithm (Knuth, 1965), keeping a stack of
by looking at context. If it was not possible to deter- partially built syntactic structures, and a queue of
mine the correct annotation from context, we anno- remaining input tokens. At each step in the pars-
tated such utterances as VOC relations. ing process, the parser can apply a shift action (re-
After annotating the 15 Eve files, we had 18,863 move a token from the front of the queue and place
fully hand-annotated utterances, 10,280 adult it on top of the stack), or a reduce action (pop the
and 8,563 child. The utterances consist of 84,226 two topmost stack items, and push a new item com-
GRs (including punctuation) and 65,363 words. posed of the two popped items combined in a sin-
The average utterance length is 5.3 words (in- gle structure). This parsing approach is very similar
cluding punctuation) for adult utterances, 3.6 for to the one used successfully by Nivre et al. (2006),
child, 4.5 overall. The annotated Eve corpus but we use a maximum entropy classifier (Berger et
is available at http://childes.psy.cmu. al., 1996) to determine parser actions, which makes
edu/data/Eng-USA/brown.zip. It was used parsing extremely fast. In addition, our parsing ap-
for the Domain adaptation task at the CoNLL-2007 proach performs a search over the space of possible
dependency parsing shared task (Nivre, 2007). parser actions, while Nivre et al.’s approach is de-
3 Parsing terministic. See Sagae and Tsujii (2007) for more
Although the CHILDES annotation scheme pro- information on the parser.
posed by Sagae et al. (2004) has been used in prac- Features used in classification to determine
tice for automatic parsing of child language tran- whether the parser takes a shift or a reduce action
scripts (Sagae et al., 2004; Sagae et al., 2005), such at any point during parsing are derived from the
work relied mainly on a statistical parser (Char- parser’s current configuration (contents of the stack
niak, 2000) trained on the Wall Street Journal por- and queue) at that point. The specific features used
tion of the Penn Treebank, since a large enough cor- are:4
pus of annotated CHILDES data was not available • Word and its POS tag: s(1), q(2), and q(1).
to train a domain-specific parser. Having a corpus • POS: s(3) and q(2).
of 65,000 words of CHILDES data annotated with
grammatical relations represented as labeled depen-
dencies allows us to develop a parser tailored for the
CHILDES domain.
Our overall parsing approach uses a best-first
28
3The parser used in this work is the same as the probabilistic
shift-reduce parser referred to as “Sagae” in the cited shared
task descriptions. In the 2007 shared task, an ensemble of shift-
reduce parsers was used, but only a single parser is used here.
4s(n) denotes the n-th item from the top of the stack (where
s(1) is the item on the top of the stack), and q(n) denotes the
n-th item from the front of the queue.
</bodyText>
<listItem confidence="0.994228333333333">
• The dependency label of the most recently at-
tached dependent of: s(1) and s(2).
• The previous parser action.
</listItem>
<sectionHeader confidence="0.991728" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.895887">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.99998454">
We first evaluate the parser by 15-fold cross-
validation on the 15 manually curated gold-standard
Eve files (to evaluate the parser on each file, the re-
maining 14 files are used to train the parser). Single-
word utterances (excluding punctuation) were ig-
nored, since their analysis is trivial and their inclu-
sion would artificially inflate parser accuracy mea-
surements. The size of the Eve evaluation corpus
(with single-word utterances removed) was 64,558
words (or 59,873 words excluding punctuation). Of
these, 41,369 words come from utterances spoken
by adults, and 18,504 come from utterances spo-
ken by the child. To evaluate the parser’s portabil-
ity to other CHILDES corpora, we also tested the
parser (trained only on the entire Eve set) on two ad-
ditional sets, one taken from the MacWhinney cor-
pus (MacWhinney, 2000) (5,658 total words, 3,896
words in adult utterances and 1,762 words in child
utterances), and one taken from the Seth corpus (Pe-
ters, 1987; Wilson and Peters, 1988) (1,749 words,
1,059 adult and 690 child).
The parser is highly efficient: training on the en-
tire Eve corpus takes less that 20 minutes on stan-
dard hardware, and once trained, parsing the Eve
corpus takes 18 seconds, or over 3,500 words per
second.
Following recent work on dependency parsing
(Nivre, 2007), we report two evaluation measures:
labeled accuracy score (LAS) and unlabeled accu-
racy score (UAS). LAS is the percentage of tokens
for which the parser predicts the correct head-word
and dependency label. UAS ignores the dependency
labels, and therefore corresponds to the percentage
of words for which the correct head was found. In
addition to LAS and UAS, we also report precision
and recall of certain grammatical relations.
For example, compare the parser output of go buy
an apple to the gold standard (Figure 1). This se-
quence of GRs has two labeled dependency errors
and one unlabeled dependency error. 1|2|COORD
for the parser versus 1|2|SRL is a labeled error be-
cause the dependency label produced by the parser
(COORD) does not match the gold-standard anno-
tation (SRL), although the unlabeled dependency is
correct, since the headword assignment, 1|2, is the
same for both. On the other hand, 5|1|PUNCT ver-
sus 5|2|PUNCT is both a labeled dependency error
and an unlabeled dependency error, since the head-
word assignment produced by the parser does not
match the gold-standard.
</bodyText>
<sectionHeader confidence="0.607203" genericHeader="method">
4.2 Results
</sectionHeader>
<bodyText confidence="0.797488166666667">
Trained on domain-specific data, the parser per-
formed well on held-out data, even though the train-
ing corpus is relatively small (about 60,000 words).
The results are listed in Table 1.
LAS UAS
Eve cross-validation 92.0
</bodyText>
<tableCaption confidence="0.989642">
Table 1: Average cross-validation results, Eve
</tableCaption>
<bodyText confidence="0.999976695652174">
The labeled dependency error rate is about 8%
and the unlabeled error rate is slightly over 6%. Per-
formance in individual files ranged between the best
labeled error rate of 6.2% and labeled error rate of
4.4% for the fifth file, and the worst error rates of
8.9% and 7.8% for labeled and unlabeled respec-
tively in the fifteenth file. For comparison, Sagae et
al. (2005) report 86.9% LAS on about 2,000 words
of Eve data, using the Charniak (2000) parser with
a separate dependency-labeling step. Part of the rea-
son we obtain levels of accuracy higher than usu-
ally reported for dependency parsers is that the aver-
age sentence length in CHILDES transcripts is much
lower than in, for example, newspaper text. The av-
erage sentence length for adult utterances in the Eve
corpus is 6.1 tokens, and 4.3 tokens for child utter-
ances5.
Certain GRs are easily identifiable, such as DET,
AUX, and INF. The parser has precision and recall
of nearly 1.00 for those. For all GRs that occur more
than 1,000 times in the Eve corpus (which contrains
more than 60,000 tokens), precision and recall are
above 0.90, with the exception of COORD, which
</bodyText>
<footnote confidence="0.744804666666667">
5This differs from the figures in section 2.3 because for the
purpose of parser evaluation we ignore sentences composed
only of a single word plus punctuation.
</footnote>
<page confidence="0.947179">
93.8
29
</page>
<figure confidence="0.521883">
go buy an apple .
parser: 1|2|COORD 2|0|ROOT 3|4|DET 4|2|OBJ 5|1|PUNCT
gold: 1|2|SRL 2|0|ROOT 3|4|DET 4|2|OBJ 5|2|PUNCT
</figure>
<figureCaption confidence="0.99967">
Figure 1: Example output: parser vs. gold annotation
</figureCaption>
<bodyText confidence="0.4814">
occurs 1,163 times in the gold-standard data. The
parser’s precision for COORD is 0.73, and recall
is 0.84. Other interesting GRs include SUBJ, OBJ,
JCT (adjunct), COM, LOC, COMP, XCOMP, CJCT
(subordinate clause acting as an adjunct), and PTL
(verb particle, easily confusable with prepositions
</bodyText>
<table confidence="0.952578153846154">
and adverbs). Their precision and recall is shown
in table 2.
GR Precision Recall F-score
SUBJ 0.96 0.96 0.96
OBJ 0.93 0.94 0.93
JCT 0.91 0.90 0.90
COM 0.96 0.95 0.95
LOC 0.95 0.90 0.92
COMP 0.83 0.86 0.84
XCOMP 0.86 0.87 0.87
CJCT 0.61 0.59 0.60
PTL 0.97 0.96 0.96
COORD 0.73 0.84 0.78
</table>
<tableCaption confidence="0.990525">
Table 2: Precision, recall and f-score of selected
</tableCaption>
<bodyText confidence="0.973181">
GRs in the Eve corpus
We also tested the accuracy of the parser on child
utterances and adult utterances separately. To do
this, we split the gold standard files into child and
adult utterances, producing gold standard files for
both child and adult utterances. We then trained
the parser on 14 of the 15 Eve files with both child
and adult utterances, and parsed the individual child
and adult files. Not surprisingly, the parser per-
formed slightly better on the adult utterances due to
their grammaticality and the fact that there was more
adult training data than child training data. The re-
sults are listed in Table 3.
</bodyText>
<footnote confidence="0.453123666666667">
LAS UAS
Eve - Child
Eve - Adult
</footnote>
<tableCaption confidence="0.997778">
Table 3: Average child vs. adult results, Eve
</tableCaption>
<bodyText confidence="0.999886405405405">
Our final evaluation of the parser involved test-
ing the parser on data taken from a different parts of
the CHILDES database. First, the parser was trained
on all gold-standard Eve files, and tested on man-
ually annotated data taken from the MacWhinney
transcripts. Although accuracy was lower for adult
utterances (85.8% LAS) than on Eve data, the accu-
racy for child utterances was slightly higher (92.3%
LAS), even though child utterances were longer on
average (4.7 tokens) than in the Eve corpus.
Finally, because a few aspects of the many tran-
script sets in the CHILDES database may vary in
ways not accounted for in the design of the parser
or the annotation of the training data, we also re-
port results on evaluation of the Eve-trained parser
on a particularly challenging test set, the Seth cor-
pus. Because the Seth corpus contains transcriptions
of language phenomena not seen in the Eve corpus
(see section 5), parser performance is expected to
suffer. Although accuracy on adult utterances is high
(92.2% LAS), accuracy on child utterances is very
low (72.7% LAS). This is due to heavy use of a GR
label that does not appear at all in the Eve corpus
that was used to train the parser. This GR is used to
represent relations involving filler syllables, which
appear in nearly 45% of the child utterances in the
Seth corpus. Accuracy on the sentences that do not
contain filler syllables is at the same level as in the
other corpora (91.1% LAS). Although we do not ex-
pect to encounter many sets of transcripts that are as
problematic as this one in the CHILDES database, it
is interesting to see what can be expected from the
parser under unfavorable conditions.
The results of the parser on the MacWhinney and
Seth test sets are summarized in table 4, where Seth
(clean) refers to the Seth corpus without utterances
that contain filler sylables.
</bodyText>
<sectionHeader confidence="0.998373" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.993422">
A major source for parser errors on the Eve cor-
pus (112 out of 5181 errors) was telegraphic speech,
</bodyText>
<page confidence="0.996755">
90.0
93.1
91.7
94.8
30
</page>
<table confidence="0.999562777777778">
LAS UAS
MacWhinney - Child 92.3 94.8
MacWhinney - Adult 85.8 89.4
MacWhinney - Total 88.0 91.2
Seth - Child 72.7 82.0
Seth - Adult 92.2 94.4
Seth - Total 84.6 89.5
Seth (clean) - Child 91.1 92.7
Seth (clean) - Total 92.0 93.9
</table>
<tableCaption confidence="0.93104">
Table 4: Training on Eve, testing on MacWhinney
and Seth
</tableCaption>
<bodyText confidence="0.999900212765957">
as in Mommy telephone or Fraser tape+recorder
floor. Telegraphic speech may be the most chal-
lenging, since even for a human annotator, deter-
mining a GR is difficult. The parser usually labeled
such utterances with the noun as the ROOT and the
proper noun as the MOD, while the gold annotation
is context-dependent as described above.
Another category of errors, with about 150 in-
stances, is XCOMP errors. The majority of the er-
rors in this category revolve around dropped words
in the main clause, for example want eat cookie. Of-
ten, the parser labels such utterances with COMP
GRs, because of the lack of to. Exclusive training on
utterances of this type may resolve the issue. Many
of the errors of this type occur with want: the parser
could be conditioned to assign an XCOMP GR with
want as the ROOT of an utterance.
COORD and PRED errors would both benefit
from more data as well. The parser performs ad-
mirably on simple coordination and predicate con-
structions, but has troubles with less common con-
structions such as PRED GRs with get, e.g., don’t
let your hands get dirty (69 errors), and coordina-
tion of prepositional objects, as in a birthday cake
with Cathy and Becky (154 errors).
The performance drop on the Seth corpus can be
explained by a number of factors. First and fore-
most, Seth is widely considered in the literature to
be the child who is most likely to invalidate any the-
ory (Wilson and Peters, 1988). He exhibits false
starts and filler syllables extensively, and his syn-
tax violates many “universal” principles. This is
reflected in the annotation scheme: the Seth cor-
pus, following the annotation of Peters (1983), is
abundant with filler syllables. Because there was
no appropriate GR label for representing the syn-
tactic relationships involving the filler syllables, we
annotated those with a special GR (not used during
parser training), which the parser is understandably
not able to produce. Filler syllables usually occur
near the start of the sentence, and once the parser
failed to label them, it could not accurately label the
remaining GRs. Other difficulties in the Seth cor-
pus include the usage of dates, of which there were
no instances in the Eve corpus. The parser had not
been trained on the new DATE GR and subsequently
failed to parse it.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999997125">
We described an annotation scheme for represent-
ing syntactic information as grammatical relations
in CHILDES data, a manually curated gold-standard
corpus of 65,000 words annotated according to this
GR scheme, and a parser that was trained on the an-
notated corpus and produces highly accurate gram-
matical relations for both child and adult utterances.
These resources are now freely available to the re-
search community, and we expect them to be in-
strumental in psycholinguistic investigations of lan-
guage acquisition and child language.
Syntactic analysis of child language transcripts
using a GR scheme of this kind has already been
shown to be effective in a practical setting, namely
in automatic measurement of syntactic development
in children (Sagae et al., 2005). That work relied on
a phrase-structure statistical parser (Charniak, 2000)
trained on the Penn Treebank, and the output of that
parser had to be converted into CHILDES grammat-
ical relations. Despite the obvious disadvantage of
using a parser trained on a completely different lan-
guage genre, Sagae et al. (2005) demonstrated how
current natural language processing techniques can
be used effectively in child language work, achiev-
ing results that are close to those obtained by man-
ual computation of syntactic development scores for
child transcripts. Still, the use of tools not tailored
for child language and extra effort necessary to make
them work with community standards for child lan-
guage transcription present a disincentive for child
language researchers to incorporate automatic syn-
tactic analysis into their work. We hope that the GR
</bodyText>
<page confidence="0.999581">
31
</page>
<bodyText confidence="0.999986782608696">
representation scheme and the parser presented here
will make it possible and convenient for the child
language community to take advantage of some of
the recent developments in natural language parsing,
as was the case with part-of-speech tagging when
CHILDES specific tools were first made available.
Our immediate plans include continued improve-
ment of the parser, which can be achieved at least in
part by the creation of additional training data from
other English CHILDES corpora. We also plan to
release automatic syntactic analyses for the entire
English portion of CHILDES.
Although we have so far focused exclusively on
English CHILDES data, dependency schemes based
on functional relationships exist for a number of lan-
guages (Buchholz and Marsi, 2006), and the general
parsing techniques used in the present work have
been shown to be effective in several of them (Nivre
et al., 2006). As future work, we plan to adapt
existing dependency-based annotation schemes and
apply our current syntactic annotation and pars-
ing framework to other languages in the CHILDES
database.
</bodyText>
<sectionHeader confidence="0.997761" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998205">
We thank Marina Fedner for her help with annota-
tion of the Eve corpus. This work was supported in
part by the National Science Foundation under grant
IIS-0414630.
</bodyText>
<sectionHeader confidence="0.999247" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894952380952">
A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39–71.
Roger Brown. 1973. A first language: the early stages.
George Allen &amp; Unwin Ltd., London.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning (CoNLL-X), pages 149–164,
New York City, June. Association for Computational
Linguistics.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new proposal.
In Proceedings of the 1st International Conference on
Language Resources and Evaluation, pages 447–454,
Granada, Spain.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the first conference on North
American chapter of the Association for Computa-
tional Linguistics, pages 132–139, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
D. Knuth. 1965. On the translation of languages from
left to right. Information and Control, 8(6):607–639.
Brian MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,
and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector ma-
chines. In Proceedings of the Tenth Conference on
Computational Natural Language Learning.
Joakim Nivre, editor. 2007. CoNLL-XI Shared Task on
Multilingual Dependency Parsing, Prague, June. As-
sociation for Computational Linguistics.
Ann M. Peters. 1983. The Units of Language Acquisi-
tion. Monographs in Applied Psycholinguistics. Cam-
bridge University Press, New York.
Ann M. Peters. 1987. The role of immitation in the de-
veloping syntax of a blind child. Text, 7:289–311.
Kenji Sagae and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 691–698, Sydney, Australia, July. Association
for Computational Linguistics.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency pars-
ing and domain adaptation with lr models and parser
ensembles. In Proceedings of the Eleventh Conference
on Computational Natural Language Learning.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004.
Adding syntactic annotations to transcripts of parent-
child dialogs. In Proceedings of the Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2004), Lisbon, Portugal.
Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005.
Automatic measurement of syntactic development in
child language. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL’05), pages 197–204, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
B. Wilson and Ann M. Peters. 1988. What are you
cookin’ on a hot?: A three-year-old blind child’s ‘vi-
olation’ of universal constraints on constituent move-
ment. Language, 64:249–273.
</reference>
<page confidence="0.999301">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.197246">
<title confidence="0.999946">High-accuracy Annotation and Parsing of CHILDES Transcripts</title>
<author confidence="0.942776">Kenji</author>
<affiliation confidence="0.9998925">Department of Computer University of</affiliation>
<address confidence="0.964687">Hongo 7-3-1, Bunkyo-ku, Tokyo,</address>
<email confidence="0.991464">sagae@is.s.u-tokyo.ac.jp</email>
<author confidence="0.863811">Eric</author>
<affiliation confidence="0.9395485">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.894074">Pittsburgh, PA</address>
<email confidence="0.999057">dhdavis@cs.cmu.edu</email>
<author confidence="0.612035">Alon</author>
<affiliation confidence="0.8079035">Language Technologies Carnegie Mellon</affiliation>
<address confidence="0.822352">Pittsburgh, PA</address>
<email confidence="0.997567">alavie@cs.cmu.edu</email>
<author confidence="0.999764">Brian MacWhinney</author>
<affiliation confidence="0.999927">Department of Psychology Carnegie Mellon University</affiliation>
<address confidence="0.999388">Pittsburgh, PA 15213</address>
<email confidence="0.998264">macw@cmu.edu</email>
<author confidence="0.926202">Shuly</author>
<affiliation confidence="0.999821">Department of Computer University of</affiliation>
<address confidence="0.997878">31905 Haifa,</address>
<email confidence="0.999457">shuly@cs.haifa.ac.il</email>
<abstract confidence="0.997257166666667">Corpora of child language are essential for psycholinguistic research. Linguistic annotation of the corpora provides researchers with better means for exploring the development of grammatical constructions and their usage. We describe an ongoing project that aims to annotate the English section of the CHILDES database with grammatical relations in the form of labeled dependency structures. To date, we have produced a corpus of over 65,000 words with manually curated gold-standard grammatical relation annotations. Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data. The parser and the manually annotated data are freely available for research purposes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
</authors>
<title>A first language: the early stages.</title>
<date>1973</date>
<publisher>George Allen &amp; Unwin Ltd.,</publisher>
<location>London.</location>
<contexts>
<context position="10689" citStr="Brown, 1973" startWordPosition="1705" endWordPosition="1706">’m not sure. Again, we 1|2|DET 2|0|ROOT 3|2|PUNCT specifically mark Clausal PREDicates (This is how I drink my coffee) and XPREDicates (My goal is to win the competition). the first word a is a DETerminer of word 2 (cookie), which is itself the ROOT of the utterance. Adjuncts (denoted by JCT) are optional modi- 2.3 Manual annotation of the corpus fiers of verbs, adjectives or adverbs, and we dis- We focused our manual annotation on a set of tinguish among non-clausal ones (That’s much bet- CHILDES transcripts for a particular child, Eve ter; sit on the stool), finite clausal ones (CJCT, Mary (Brown, 1973), and we refer to these transcripts, left after she saw John) and non-finite clausal ones distributed in a set of 20 files, as the Eve corpus. (XJCT, Mary left after seeing John). We hand-annotated (including correcting POS tags) MODifiers, which modify or complement nouns, the first 15 files of the Eve corpus following the again come in three flavors: MOD (That’s a nice GR scheme outlined above. The annotation probox); CMOD (the movie that I saw was good); and cess started with purely manual annotation of 5,000 XMOD (the student reading a book is tall). words. This initial annotated corpus wa</context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Roger Brown. 1973. A first language: the early stages. George Allen &amp; Unwin Ltd., London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<pages>149--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="13581" citStr="Buchholz and Marsi, 2006" startWordPosition="2169" endWordPosition="2172">xisting JCT rela- Lavie (2006). Because CHILDES syntactic annotation could not capture this requirement. tions are represented as labeled dependencies, using In addition to adding new GRs, we also faced a dependency parsing approach allows us to work challenges with telegraphic child utterances lack- with that representation directly. ing verbs or other content words. For instance, This dependency parser has been shown to have Mommy telephone could have one of several mean- state-of-the-art accuracy in the CoNLL shared tasks ings: Mommy this is a telephone, Mommy I want on dependency parsing (Buchholz and Marsi, 2006; the telephone, that is Mommy’s telephone, etc. We Nivre, 2007)3. Sagae and Tsujii (2007) present a tried to be as consistent as possible in annotating detailed description of the parsing approach used in such utterances and determined their GRs from con- our work, including the parsing algorithm. In sumtext. It was often possible to determine the VOC mary, the parser uses an algorithm similar to the LR reading vs.the MOD (Mommy’s telephone) reading parsing algorithm (Knuth, 1965), keeping a stack of by looking at context. If it was not possible to deter- partially built syntactic structures,</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149–164, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Edward Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<location>Granada,</location>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Edward Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proceedings of the 1st International Conference on Language Resources and Evaluation, pages 447–454, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="20217" citStr="Charniak (2000)" startWordPosition="3282" endWordPosition="3283">ng corpus is relatively small (about 60,000 words). The results are listed in Table 1. LAS UAS Eve cross-validation 92.0 Table 1: Average cross-validation results, Eve The labeled dependency error rate is about 8% and the unlabeled error rate is slightly over 6%. Performance in individual files ranged between the best labeled error rate of 6.2% and labeled error rate of 4.4% for the fifth file, and the worst error rates of 8.9% and 7.8% for labeled and unlabeled respectively in the fifteenth file. For comparison, Sagae et al. (2005) report 86.9% LAS on about 2,000 words of Eve data, using the Charniak (2000) parser with a separate dependency-labeling step. Part of the reason we obtain levels of accuracy higher than usually reported for dependency parsers is that the average sentence length in CHILDES transcripts is much lower than in, for example, newspaper text. The average sentence length for adult utterances in the Eve corpus is 6.1 tokens, and 4.3 tokens for child utterances5. Certain GRs are easily identifiable, such as DET, AUX, and INF. The parser has precision and recall of nearly 1.00 for those. For all GRs that occur more than 1,000 times in the Eve corpus (which contrains more than 60,</context>
<context position="27995" citStr="Charniak, 2000" startWordPosition="4613" endWordPosition="4614">s trained on the annotated corpus and produces highly accurate grammatical relations for both child and adult utterances. These resources are now freely available to the research community, and we expect them to be instrumental in psycholinguistic investigations of language acquisition and child language. Syntactic analysis of child language transcripts using a GR scheme of this kind has already been shown to be effective in a practical setting, namely in automatic measurement of syntactic development in children (Sagae et al., 2005). That work relied on a phrase-structure statistical parser (Charniak, 2000) trained on the Penn Treebank, and the output of that parser had to be converted into CHILDES grammatical relations. Despite the obvious disadvantage of using a parser trained on a completely different language genre, Sagae et al. (2005) demonstrated how current natural language processing techniques can be used effectively in child language work, achieving results that are close to those obtained by manual computation of syntactic development scores for child transcripts. Still, the use of tools not tailored for child language and extra effort necessary to make them work with community standa</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132–139, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Knuth</author>
</authors>
<title>On the translation of languages from left to right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<volume>8</volume>
<issue>6</issue>
<contexts>
<context position="14067" citStr="Knuth, 1965" startWordPosition="2249" endWordPosition="2250">curacy in the CoNLL shared tasks ings: Mommy this is a telephone, Mommy I want on dependency parsing (Buchholz and Marsi, 2006; the telephone, that is Mommy’s telephone, etc. We Nivre, 2007)3. Sagae and Tsujii (2007) present a tried to be as consistent as possible in annotating detailed description of the parsing approach used in such utterances and determined their GRs from con- our work, including the parsing algorithm. In sumtext. It was often possible to determine the VOC mary, the parser uses an algorithm similar to the LR reading vs.the MOD (Mommy’s telephone) reading parsing algorithm (Knuth, 1965), keeping a stack of by looking at context. If it was not possible to deter- partially built syntactic structures, and a queue of mine the correct annotation from context, we anno- remaining input tokens. At each step in the parstated such utterances as VOC relations. ing process, the parser can apply a shift action (reAfter annotating the 15 Eve files, we had 18,863 move a token from the front of the queue and place fully hand-annotated utterances, 10,280 adult it on top of the stack), or a reduce action (pop the and 8,563 child. The utterances consist of 84,226 two topmost stack items, and p</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>D. Knuth. 1965. On the translation of languages from left to right. Information and Control, 8(6):607–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates, Mahwah, NJ, third edition.</title>
<date>2000</date>
<contexts>
<context position="1519" citStr="MacWhinney, 2000" startWordPosition="205" endWordPosition="206">English section of the CHILDES database with grammatical relations in the form of labeled dependency structures. To date, we have produced a corpus of over 65,000 words with manually curated gold-standard grammatical relation annotations. Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data. The parser and the manually annotated data are freely available for research purposes. 1 Introduction In order to investigate the development of child language, corpora which document linguistic interactions involving children are needed. The CHILDES database (MacWhinney, 2000), containing transcripts of spoken interactions between children at various stages of language development with their parents, provides vast amounts of useful data for linguistic, psychological, and sociological studies of child language development. The raw information in CHILDES corpora was gradually enriched by pro25 viding a layer of morphological information. In particular, the English section of the database is augmented by part of speech (POS) tags for each word. However, this information is usually insufficient for investigations dealing with the syntactic, semantic or pragmatic aspect</context>
<context position="4277" citStr="MacWhinney, 2000" startWordPosition="627" endWordPosition="629">orpora and the parser are freely available from http://childes.psy.cmu.edu/. We describe the annotation scheme in the next section, along with issues we faced during the process of manual annotation. Section 3 describes the parser, and an evaluation of the parser is presented in section 4. We analyze the remaining parsing errors in section 5 and conclude with some applications of the parser and directions for future research in section 6. 2 Syntactic annotation The English section of the CHILDES database is augmented with automatically produced ambiguous part-of-speech and morphological tags (MacWhinney, 2000). Some of these data have been manually disambiguated, but we found that some annotation decisions had to be revised to facilitate syntactic annotation. We discuss below some of the revisions we introduced, as well as some details of the syntactic constructions that we account for. 2.1 The morphological annotation scheme The English morphological analyzer incorporated in CHILDES produces various part-of-speech tags (there are 31 distinct POS tags in the CHILDES tagset), including ADJective, ADVerb, COmmunicator, CONJunction, DETerminer, FILler, Noun, NUMeral, ONomatopoeia, PREPosition, PROnoun</context>
<context position="17929" citStr="MacWhinney, 2000" startWordPosition="2901" endWordPosition="2902">word utterances (excluding punctuation) were ignored, since their analysis is trivial and their inclusion would artificially inflate parser accuracy measurements. The size of the Eve evaluation corpus (with single-word utterances removed) was 64,558 words (or 59,873 words excluding punctuation). Of these, 41,369 words come from utterances spoken by adults, and 18,504 come from utterances spoken by the child. To evaluate the parser’s portability to other CHILDES corpora, we also tested the parser (trained only on the entire Eve set) on two additional sets, one taken from the MacWhinney corpus (MacWhinney, 2000) (5,658 total words, 3,896 words in adult utterances and 1,762 words in child utterances), and one taken from the Seth corpus (Peters, 1987; Wilson and Peters, 1988) (1,749 words, 1,059 adult and 690 child). The parser is highly efficient: training on the entire Eve corpus takes less that 20 minutes on standard hardware, and once trained, parsing the Eve corpus takes 18 seconds, or over 3,500 words per second. Following recent work on dependency parsing (Nivre, 2007), we report two evaluation measures: labeled accuracy score (LAS) and unlabeled accuracy score (UAS). LAS is the percentage of to</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>Brian MacWhinney. 2000. The CHILDES Project: Tools for Analyzing Talk. Lawrence Erlbaum Associates, Mahwah, NJ, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gulsen Eryigit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="14981" citStr="Nivre et al. (2006)" startWordPosition="2407" endWordPosition="2410">n apply a shift action (reAfter annotating the 15 Eve files, we had 18,863 move a token from the front of the queue and place fully hand-annotated utterances, 10,280 adult it on top of the stack), or a reduce action (pop the and 8,563 child. The utterances consist of 84,226 two topmost stack items, and push a new item comGRs (including punctuation) and 65,363 words. posed of the two popped items combined in a sinThe average utterance length is 5.3 words (in- gle structure). This parsing approach is very similar cluding punctuation) for adult utterances, 3.6 for to the one used successfully by Nivre et al. (2006), child, 4.5 overall. The annotated Eve corpus but we use a maximum entropy classifier (Berger et is available at http://childes.psy.cmu. al., 1996) to determine parser actions, which makes edu/data/Eng-USA/brown.zip. It was used parsing extremely fast. In addition, our parsing apfor the Domain adaptation task at the CoNLL-2007 proach performs a search over the space of possible dependency parsing shared task (Nivre, 2007). parser actions, while Nivre et al.’s approach is de3 Parsing terministic. See Sagae and Tsujii (2007) for more Although the CHILDES annotation scheme pro- information on th</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Svetoslav Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of the Tenth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<date>2007</date>
<booktitle>CoNLL-XI Shared Task on Multilingual Dependency Parsing,</booktitle>
<editor>Joakim Nivre, editor.</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Prague,</location>
<contexts>
<context position="13671" citStr="(2007)" startWordPosition="2186" endWordPosition="2186">ns are represented as labeled dependencies, using In addition to adding new GRs, we also faced a dependency parsing approach allows us to work challenges with telegraphic child utterances lack- with that representation directly. ing verbs or other content words. For instance, This dependency parser has been shown to have Mommy telephone could have one of several mean- state-of-the-art accuracy in the CoNLL shared tasks ings: Mommy this is a telephone, Mommy I want on dependency parsing (Buchholz and Marsi, 2006; the telephone, that is Mommy’s telephone, etc. We Nivre, 2007)3. Sagae and Tsujii (2007) present a tried to be as consistent as possible in annotating detailed description of the parsing approach used in such utterances and determined their GRs from con- our work, including the parsing algorithm. In sumtext. It was often possible to determine the VOC mary, the parser uses an algorithm similar to the LR reading vs.the MOD (Mommy’s telephone) reading parsing algorithm (Knuth, 1965), keeping a stack of by looking at context. If it was not possible to deter- partially built syntactic structures, and a queue of mine the correct annotation from context, we anno- remaining input tokens.</context>
<context position="15510" citStr="(2007)" startWordPosition="2490" endWordPosition="2490">adult utterances, 3.6 for to the one used successfully by Nivre et al. (2006), child, 4.5 overall. The annotated Eve corpus but we use a maximum entropy classifier (Berger et is available at http://childes.psy.cmu. al., 1996) to determine parser actions, which makes edu/data/Eng-USA/brown.zip. It was used parsing extremely fast. In addition, our parsing apfor the Domain adaptation task at the CoNLL-2007 proach performs a search over the space of possible dependency parsing shared task (Nivre, 2007). parser actions, while Nivre et al.’s approach is de3 Parsing terministic. See Sagae and Tsujii (2007) for more Although the CHILDES annotation scheme pro- information on the parser. posed by Sagae et al. (2004) has been used in prac- Features used in classification to determine tice for automatic parsing of child language tran- whether the parser takes a shift or a reduce action scripts (Sagae et al., 2004; Sagae et al., 2005), such at any point during parsing are derived from the work relied mainly on a statistical parser (Char- parser’s current configuration (contents of the stack niak, 2000) trained on the Wall Street Journal por- and queue) at that point. The specific features used tion o</context>
</contexts>
<marker>2007</marker>
<rawString>Joakim Nivre, editor. 2007. CoNLL-XI Shared Task on Multilingual Dependency Parsing, Prague, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann M Peters</author>
</authors>
<title>The Units of Language Acquisition. Monographs in Applied Psycholinguistics.</title>
<date>1983</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="26493" citStr="Peters (1983)" startWordPosition="4373" endWordPosition="4374">ED GRs with get, e.g., don’t let your hands get dirty (69 errors), and coordination of prepositional objects, as in a birthday cake with Cathy and Becky (154 errors). The performance drop on the Seth corpus can be explained by a number of factors. First and foremost, Seth is widely considered in the literature to be the child who is most likely to invalidate any theory (Wilson and Peters, 1988). He exhibits false starts and filler syllables extensively, and his syntax violates many “universal” principles. This is reflected in the annotation scheme: the Seth corpus, following the annotation of Peters (1983), is abundant with filler syllables. Because there was no appropriate GR label for representing the syntactic relationships involving the filler syllables, we annotated those with a special GR (not used during parser training), which the parser is understandably not able to produce. Filler syllables usually occur near the start of the sentence, and once the parser failed to label them, it could not accurately label the remaining GRs. Other difficulties in the Seth corpus include the usage of dates, of which there were no instances in the Eve corpus. The parser had not been trained on the new D</context>
</contexts>
<marker>Peters, 1983</marker>
<rawString>Ann M. Peters. 1983. The Units of Language Acquisition. Monographs in Applied Psycholinguistics. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann M Peters</author>
</authors>
<title>The role of immitation in the developing syntax of a blind child.</title>
<date>1987</date>
<tech>Text, 7:289–311.</tech>
<contexts>
<context position="18068" citStr="Peters, 1987" startWordPosition="2924" endWordPosition="2926">racy measurements. The size of the Eve evaluation corpus (with single-word utterances removed) was 64,558 words (or 59,873 words excluding punctuation). Of these, 41,369 words come from utterances spoken by adults, and 18,504 come from utterances spoken by the child. To evaluate the parser’s portability to other CHILDES corpora, we also tested the parser (trained only on the entire Eve set) on two additional sets, one taken from the MacWhinney corpus (MacWhinney, 2000) (5,658 total words, 3,896 words in adult utterances and 1,762 words in child utterances), and one taken from the Seth corpus (Peters, 1987; Wilson and Peters, 1988) (1,749 words, 1,059 adult and 690 child). The parser is highly efficient: training on the entire Eve corpus takes less that 20 minutes on standard hardware, and once trained, parsing the Eve corpus takes 18 seconds, or over 3,500 words per second. Following recent work on dependency parsing (Nivre, 2007), we report two evaluation measures: labeled accuracy score (LAS) and unlabeled accuracy score (UAS). LAS is the percentage of tokens for which the parser predicts the correct head-word and dependency label. UAS ignores the dependency labels, and therefore corresponds</context>
</contexts>
<marker>Peters, 1987</marker>
<rawString>Ann M. Peters. 1987. The role of immitation in the developing syntax of a blind child. Text, 7:289–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>691--698</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 691–698, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="13671" citStr="Sagae and Tsujii (2007)" startWordPosition="2183" endWordPosition="2186"> requirement. tions are represented as labeled dependencies, using In addition to adding new GRs, we also faced a dependency parsing approach allows us to work challenges with telegraphic child utterances lack- with that representation directly. ing verbs or other content words. For instance, This dependency parser has been shown to have Mommy telephone could have one of several mean- state-of-the-art accuracy in the CoNLL shared tasks ings: Mommy this is a telephone, Mommy I want on dependency parsing (Buchholz and Marsi, 2006; the telephone, that is Mommy’s telephone, etc. We Nivre, 2007)3. Sagae and Tsujii (2007) present a tried to be as consistent as possible in annotating detailed description of the parsing approach used in such utterances and determined their GRs from con- our work, including the parsing algorithm. In sumtext. It was often possible to determine the VOC mary, the parser uses an algorithm similar to the LR reading vs.the MOD (Mommy’s telephone) reading parsing algorithm (Knuth, 1965), keeping a stack of by looking at context. If it was not possible to deter- partially built syntactic structures, and a queue of mine the correct annotation from context, we anno- remaining input tokens.</context>
<context position="15510" citStr="Sagae and Tsujii (2007)" startWordPosition="2487" endWordPosition="2490">punctuation) for adult utterances, 3.6 for to the one used successfully by Nivre et al. (2006), child, 4.5 overall. The annotated Eve corpus but we use a maximum entropy classifier (Berger et is available at http://childes.psy.cmu. al., 1996) to determine parser actions, which makes edu/data/Eng-USA/brown.zip. It was used parsing extremely fast. In addition, our parsing apfor the Domain adaptation task at the CoNLL-2007 proach performs a search over the space of possible dependency parsing shared task (Nivre, 2007). parser actions, while Nivre et al.’s approach is de3 Parsing terministic. See Sagae and Tsujii (2007) for more Although the CHILDES annotation scheme pro- information on the parser. posed by Sagae et al. (2004) has been used in prac- Features used in classification to determine tice for automatic parsing of child language tran- whether the parser takes a shift or a reduce action scripts (Sagae et al., 2004; Sagae et al., 2005), such at any point during parsing are derived from the work relied mainly on a statistical parser (Char- parser’s current configuration (contents of the stack niak, 2000) trained on the Wall Street Journal por- and queue) at that point. The specific features used tion o</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Proceedings of the Eleventh Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
<author>Brian MacWhinney</author>
</authors>
<title>Adding syntactic annotations to transcripts of parentchild dialogs.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2455" citStr="Sagae et al., 2004" startWordPosition="346" endWordPosition="349">ro25 viding a layer of morphological information. In particular, the English section of the database is augmented by part of speech (POS) tags for each word. However, this information is usually insufficient for investigations dealing with the syntactic, semantic or pragmatic aspects of the data. In this paper we describe an ongoing effort aiming to annotate the English portion of the CHILDES database with syntactic information based on grammatical relations represented as labeled dependency structures. Although an annotation scheme for syntactic information in CHILDES data has been proposed (Sagae et al., 2004), until now no significant amount of annotated data had been made publicly available. In the process of manually annotating several thousands of words, we updated the annotation scheme, mostly by extending it to cover syntactic phenomena that occur in real data but were unaccounted for in the original annotation scheme. The contributions of this work fall into three main categories: revision and extension of the annotation scheme for representing syntactic information in CHILDES data; creation of a manually annotated 65,000 word corpus with gold-standard syntactic analyses; and implementation </context>
<context position="7902" citStr="Sagae et al. (2004)" startWordPosition="1235" endWordPosition="1238">fy an entire sentence, so if a word appears by itself, it cannot be a CO. For example, utterances like here or there are labeled as ADVerb. However, if these words appear at the beginning of a sentence, are followed by a break or pause, and do not clearly express a location, then they are labeled CO. Additionally, in here/there you are/go, here and there are labeled CO. 2.2 The syntactic annotation scheme Our annotation scheme for representing grammatical relations, or GRs (such as subjects, objects and adjuncts), in CHILDES transcripts is a slightly extended version of the scheme proposed by Sagae et al. (2004), which was inspired by a general annota26 tion scheme for grammatical relations (Carroll et al., Finally, we added some specific relations for han1998), but adapted specifically for CHILDES data. dling problematic issues. For example, we use Our scheme contains 37 distinct GR types. Sagae ENUMeration for constructions such as one, two, et al. reported 96.5% interannotator agreement, and three, go or a, b, c. In COORDination construcwe do not believe our minor updates to the annota- tions, each conjunct is marked as a dependent of the tion scheme should affect interannotator agreement conjunct</context>
<context position="15619" citStr="Sagae et al. (2004)" startWordPosition="2505" endWordPosition="2508">l. The annotated Eve corpus but we use a maximum entropy classifier (Berger et is available at http://childes.psy.cmu. al., 1996) to determine parser actions, which makes edu/data/Eng-USA/brown.zip. It was used parsing extremely fast. In addition, our parsing apfor the Domain adaptation task at the CoNLL-2007 proach performs a search over the space of possible dependency parsing shared task (Nivre, 2007). parser actions, while Nivre et al.’s approach is de3 Parsing terministic. See Sagae and Tsujii (2007) for more Although the CHILDES annotation scheme pro- information on the parser. posed by Sagae et al. (2004) has been used in prac- Features used in classification to determine tice for automatic parsing of child language tran- whether the parser takes a shift or a reduce action scripts (Sagae et al., 2004; Sagae et al., 2005), such at any point during parsing are derived from the work relied mainly on a statistical parser (Char- parser’s current configuration (contents of the stack niak, 2000) trained on the Wall Street Journal por- and queue) at that point. The specific features used tion of the Penn Treebank, since a large enough cor- are:4 pus of annotated CHILDES data was not available • Word a</context>
</contexts>
<marker>Sagae, Lavie, MacWhinney, 2004</marker>
<rawString>Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2004. Adding syntactic annotations to transcripts of parentchild dialogs. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
<author>Brian MacWhinney</author>
</authors>
<title>Automatic measurement of syntactic development in child language.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>197--204</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="15839" citStr="Sagae et al., 2005" startWordPosition="2543" endWordPosition="2546">xtremely fast. In addition, our parsing apfor the Domain adaptation task at the CoNLL-2007 proach performs a search over the space of possible dependency parsing shared task (Nivre, 2007). parser actions, while Nivre et al.’s approach is de3 Parsing terministic. See Sagae and Tsujii (2007) for more Although the CHILDES annotation scheme pro- information on the parser. posed by Sagae et al. (2004) has been used in prac- Features used in classification to determine tice for automatic parsing of child language tran- whether the parser takes a shift or a reduce action scripts (Sagae et al., 2004; Sagae et al., 2005), such at any point during parsing are derived from the work relied mainly on a statistical parser (Char- parser’s current configuration (contents of the stack niak, 2000) trained on the Wall Street Journal por- and queue) at that point. The specific features used tion of the Penn Treebank, since a large enough cor- are:4 pus of annotated CHILDES data was not available • Word and its POS tag: s(1), q(2), and q(1). to train a domain-specific parser. Having a corpus • POS: s(3) and q(2). of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allo</context>
<context position="20140" citStr="Sagae et al. (2005)" startWordPosition="3266" endWordPosition="3269">specific data, the parser performed well on held-out data, even though the training corpus is relatively small (about 60,000 words). The results are listed in Table 1. LAS UAS Eve cross-validation 92.0 Table 1: Average cross-validation results, Eve The labeled dependency error rate is about 8% and the unlabeled error rate is slightly over 6%. Performance in individual files ranged between the best labeled error rate of 6.2% and labeled error rate of 4.4% for the fifth file, and the worst error rates of 8.9% and 7.8% for labeled and unlabeled respectively in the fifteenth file. For comparison, Sagae et al. (2005) report 86.9% LAS on about 2,000 words of Eve data, using the Charniak (2000) parser with a separate dependency-labeling step. Part of the reason we obtain levels of accuracy higher than usually reported for dependency parsers is that the average sentence length in CHILDES transcripts is much lower than in, for example, newspaper text. The average sentence length for adult utterances in the Eve corpus is 6.1 tokens, and 4.3 tokens for child utterances5. Certain GRs are easily identifiable, such as DET, AUX, and INF. The parser has precision and recall of nearly 1.00 for those. For all GRs that</context>
<context position="27919" citStr="Sagae et al., 2005" startWordPosition="4601" endWordPosition="4604">rpus of 65,000 words annotated according to this GR scheme, and a parser that was trained on the annotated corpus and produces highly accurate grammatical relations for both child and adult utterances. These resources are now freely available to the research community, and we expect them to be instrumental in psycholinguistic investigations of language acquisition and child language. Syntactic analysis of child language transcripts using a GR scheme of this kind has already been shown to be effective in a practical setting, namely in automatic measurement of syntactic development in children (Sagae et al., 2005). That work relied on a phrase-structure statistical parser (Charniak, 2000) trained on the Penn Treebank, and the output of that parser had to be converted into CHILDES grammatical relations. Despite the obvious disadvantage of using a parser trained on a completely different language genre, Sagae et al. (2005) demonstrated how current natural language processing techniques can be used effectively in child language work, achieving results that are close to those obtained by manual computation of syntactic development scores for child transcripts. Still, the use of tools not tailored for child</context>
</contexts>
<marker>Sagae, Lavie, MacWhinney, 2005</marker>
<rawString>Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005. Automatic measurement of syntactic development in child language. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 197–204, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wilson</author>
<author>Ann M Peters</author>
</authors>
<title>What are you cookin’ on a hot?: A three-year-old blind child’s ‘violation’ of universal constraints on constituent movement.</title>
<date>1988</date>
<journal>Language,</journal>
<pages>64--249</pages>
<contexts>
<context position="18094" citStr="Wilson and Peters, 1988" startWordPosition="2927" endWordPosition="2930">nts. The size of the Eve evaluation corpus (with single-word utterances removed) was 64,558 words (or 59,873 words excluding punctuation). Of these, 41,369 words come from utterances spoken by adults, and 18,504 come from utterances spoken by the child. To evaluate the parser’s portability to other CHILDES corpora, we also tested the parser (trained only on the entire Eve set) on two additional sets, one taken from the MacWhinney corpus (MacWhinney, 2000) (5,658 total words, 3,896 words in adult utterances and 1,762 words in child utterances), and one taken from the Seth corpus (Peters, 1987; Wilson and Peters, 1988) (1,749 words, 1,059 adult and 690 child). The parser is highly efficient: training on the entire Eve corpus takes less that 20 minutes on standard hardware, and once trained, parsing the Eve corpus takes 18 seconds, or over 3,500 words per second. Following recent work on dependency parsing (Nivre, 2007), we report two evaluation measures: labeled accuracy score (LAS) and unlabeled accuracy score (UAS). LAS is the percentage of tokens for which the parser predicts the correct head-word and dependency label. UAS ignores the dependency labels, and therefore corresponds to the percentage of word</context>
<context position="26277" citStr="Wilson and Peters, 1988" startWordPosition="4338" endWordPosition="4341">ROOT of an utterance. COORD and PRED errors would both benefit from more data as well. The parser performs admirably on simple coordination and predicate constructions, but has troubles with less common constructions such as PRED GRs with get, e.g., don’t let your hands get dirty (69 errors), and coordination of prepositional objects, as in a birthday cake with Cathy and Becky (154 errors). The performance drop on the Seth corpus can be explained by a number of factors. First and foremost, Seth is widely considered in the literature to be the child who is most likely to invalidate any theory (Wilson and Peters, 1988). He exhibits false starts and filler syllables extensively, and his syntax violates many “universal” principles. This is reflected in the annotation scheme: the Seth corpus, following the annotation of Peters (1983), is abundant with filler syllables. Because there was no appropriate GR label for representing the syntactic relationships involving the filler syllables, we annotated those with a special GR (not used during parser training), which the parser is understandably not able to produce. Filler syllables usually occur near the start of the sentence, and once the parser failed to label t</context>
</contexts>
<marker>Wilson, Peters, 1988</marker>
<rawString>B. Wilson and Ann M. Peters. 1988. What are you cookin’ on a hot?: A three-year-old blind child’s ‘violation’ of universal constraints on constituent movement. Language, 64:249–273.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>