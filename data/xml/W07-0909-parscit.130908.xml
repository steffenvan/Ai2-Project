<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036328">
<title confidence="0.967886">
Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation
</title>
<author confidence="0.995618">
Idan Szpektor, Ido Dagan Alon Lavie Danny Shacham, Shuly Wintner
</author>
<affiliation confidence="0.980096">
Dept. of Computer Science Language Technologies Inst. Dept. of Computer Science
Bar Ilan University Carnegie Mellon University University of Haifa
</affiliation>
<email confidence="0.997297">
szpekti@cs.biu.ac.il alavie+@cs.cmu.edu shuly@cs.haifa.ac.il
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993425">
We describe a system which enhances the
experience of museum visits by providing
users with language-technology-based in-
formation retrieval capabilities. The sys-
tem consists of a cross-lingual search en-
gine, augmented by state of the art semantic
expansion technology, specifically designed
for the domain of the museum (history and
archaeology of Israel). We discuss the tech-
nology incorporated in the system, its adap-
tation to the specific domain and its contri-
bution to cultural heritage appreciation.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999889">
Museum visits are enriching experiences: they pro-
vide stimulation to the senses, and through them to
the mind. But the experience does not have to end
when the visit ends: further exploration of the ar-
tifacts and their influence on the visitor is possible
after the visit, either on location or elsewhere. One
common means of exploration is Information Re-
trieval (IR) via a Search Engine. For example, a mu-
seum could implement a search engine over a col-
lection of documents relating to the topics exhibited
in the museum.
However, such document collections are usually
much smaller than general collections, in particular
the World Wide Web. Consequently, phenomena in-
herent to natural languages may severely hamper the
performance of human language technology when
applied to small collections. One such phenomenon
is the semantic variability of natural languages, the
ability to express a specific meaning in many dif-
ferent ways. For example, the expression “Archae-
ologists found a new tomb” can be expressed also
by “Archaeologists discovered a tomb” or “A sar-
cophagus was dug up by Egyptian Researchers”. On
top of monolingual variability, the same information
can also be expressed in different languages. Ignor-
ing natural language variability may result in lower
recall of relevant documents for a given query, espe-
cially in small document collections.
This paper describes a system that attempts to
cope with semantic variability through the use of
state of the art human language technology. The
system provides both semantic expansion and cross
lingual IR (and presentation of information) in the
domain of archaeology and history of Israel. It
was specifically developed for the Hecht Museum
in Haifa, Israel, which contains a small but unique
collection of artifacts in this domain. The system
provides different users with different capabilities,
bridging over language divides; it addresses seman-
tic variation in novel ways; and it thereby comple-
ments the visit to the museum with long-lasting in-
stillation of information.
The main component of the system is a domain-
specific search engine that enables users to specify
queries and retrieve information pertaining to the do-
main of the museum. The engine is enriched by lin-
guistic capabilities which embody an array of means
for addressing semantic variation. Queries are ex-
panded using two main techniques: semantic expan-
sion based on textual entailment; and cross-lingual
expansion based on translation of Hebrew queries
to English and vice versa. Retrieved documents are
presented as links with associated snippets; the sys-
tem also translates snippets from Hebrew to English.
The main contribution of this work is, of course,
the system itself, which was recently demonstrated
</bodyText>
<page confidence="0.998267">
65
</page>
<note confidence="0.9918805">
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 65–72,
Prague, 28 June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99985425">
successfully at the museum and which we believe
could be useful to a variety of museum visitor types,
from children to experts. For example, the system
provides Hebrew speakers access to English doc-
uments pertaining to the domain of the museum,
and vice versa, thereby expanding the availability
of multilingual material to museum visitors. More
generally, it is an instance of adaptation of state of
the art human language technology to the domain
of cultural heritage appreciation, demonstrating how
general resources and tools are adapted to a specific
domain, thereby improving their accuracy and us-
ability. Finally, it provides a test-bed for evaluating
the contribution of language technology in general,
as well as specific components and resources, to a
large-scale natural language processing system.
</bodyText>
<sectionHeader confidence="0.77476" genericHeader="introduction">
2 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999733176470588">
Internet search is hampered by the complexity of
natural languages. The two main characteristics of
this complexity are ambiguity and variability: the
former refers to the fact that a given text can be
interpreted in more than one way; the latter indi-
cates that the same meaning can be linguistically ex-
pressed in several ways. The two phenomena make
simple search techniques too weak for unsophisti-
cated users, as existing search engines perform only
direct keyword matching, with very limited linguis-
tic processing of the texts they retrieve.
Specifically, IR systems that do not address the
variability in languages may suffer from lower re-
call, especially in restricted domains and small doc-
ument locations. We next describe two prominent
types of variability that we think should be ad-
dressed in IR systems.
</bodyText>
<subsectionHeader confidence="0.999248">
2.1 Textual Entailment and Entailment Rules
</subsectionHeader>
<bodyText confidence="0.999653846153846">
In many NLP applications, such as Question An-
swering (QA), Information Extraction (IE) and In-
formation Retrieval (IR), it is crucial to recognize
that a specific target meaning can be inferred from
different text variants. For example, a QA system
needs to induce that “Mendelssohn wrote inciden-
tal music” can be inferred from “Mendelssohn com-
posed incidental music” in order to answer the ques-
tion “Who wrote incidental music?”. This type of
reasoning has been identified as a core semantic in-
ference task by the generic textual entailment frame-
work (Dagan et al., 2006; Bar-Haim et al., 2006).
The typical way to address variability in IR is to
use lexical query expansion (Lytinen et al., 2000;
Zukerman and Raskutti, 2002). However, there are
variability patterns that cannot be described using
just constant phrase to phrase entailment. Another
important type of knowledge representation is en-
tailment rules and paraphrases. An entailment rule
is a directional relation between two templates, text
patterns with variables, e.g., ‘X compose Y —*
X write Y ’. The left hand side is assumed to en-
tail the right hand side in certain contexts, under
the same variable instantiation. Paraphrases can be
viewed as bidirectional entailment rules. Such rules
capture basic inferences in the language, and are
used as building blocks for more complex entail-
ment inference. For example, given the above en-
tailment rule, a QA system can identify the answer
“Mendelssohn” in the above example. This need
sparked intensive research on automatic acquisition
of paraphrase and entailment rules.
Although knowledge-bases of entailment-rules
and paraphrases learned by acquisition algorithms
were used in other NLP applications, such as QA
(Lin and Pantel, 2001; Ravichandran and Hovy,
2002) and IE (Sudo et al., 2003; Romano et al.,
2006), to the best of our knowledge the output of
such algorithms was never applied to IR before.
</bodyText>
<subsectionHeader confidence="0.999908">
2.2 Cross Lingual Information Retrieval
</subsectionHeader>
<bodyText confidence="0.999990588235294">
The difficulties caused by variability are amplified
when the user is not a native speaker of the language
in which the retrieved texts are written. For exam-
ple, while most Israelis can read English documents,
fewer are comfortable with the specification of Eng-
lish queries. In a museum setting, some visitors may
be able to read Hebrew documents but still be rel-
atively poor at searching for them. Other visitors
may be unable to read Hebrew texts, but still benefit
from non-textual information that are contained in
Hebrew documents (e.g., pictures, maps, audio and
video files, external links, etc.)
This problem is addressed by the paradigm of
Cross-Lingual Information Retrieval (CLIR). This
paradigm has become a very active research area
in recent years, addressing the needs of multilingual
and non-English speaking communities, such as the
</bodyText>
<page confidence="0.946486">
66
</page>
<bodyText confidence="0.999951545454545">
European Union, East-Asian nations and Spanish
speaking communities in the US (Hull and Grefen-
stette, 1996; Ballesteros and Croft, 1997; Carbonell
et al., 1997). The common approach for CLIR is
to translate a query in a source language to another
target language and then issue the translated query
to retrieve target language documents. As explained
above, CLIR research has to address various generic
problems caused by the variability and ambiguity of
natural languages, as well as specific problems re-
lated to the particular languages being addressed.
</bodyText>
<subsectionHeader confidence="0.539989">
3 Coping with Semantic Variability in IR
</subsectionHeader>
<bodyText confidence="0.999827416666667">
We describe a search engine that is capable of per-
forming: (a) semantic English information retrieval;
and (b) cross-lingual (Hebrew-English and English-
Hebrew) information retrieval, allowing users to
pose queries in either of the two languages and re-
trieve documents in both. This is achieved by two
sub-processes of the search engine: first, the en-
gine performs shallow semantic linguistic inference
and supports the retrieval of documents which con-
tain phrases that imply the meaning of the translated
query, even when no exact match of the translated
keywords is found. This is enabled by automatic ac-
quisition of semantic variability patterns that are fre-
quent in the language, which extend traditional lexi-
cal query expansion techniques. Second, the engine
translates the original or expanded query to the tar-
get language, based on several linguistic processes
and a machine readable bilingual dictionary. The re-
sult is a semantic expansion of a given query to a va-
riety of alternative wordings in which an answer to
this query may be expressed in the target language
of the retrieved documents.
These enhancements are facilitated via a speci-
fication of the domain. As our system is specifi-
cally designed to work in the domain of the history
and archaeology, we could focus our attention on re-
sources and tools that are dedicated to this domain.
Thus, for example, lexicons and dictionaries, whose
preparation is always costly and time consuming,
were developed with the specific domain in mind;
and textual entailment and paraphrase patterns were
extracted for the specific domain. While the result-
ing system is focused on visiting the Hecht Museum,
the methodology which we used and discuss here
can be adapted to other areas of cultural heritage, as
well as to other narrow domains, in the same way.
</bodyText>
<subsectionHeader confidence="0.999664">
3.1 Setting Up a Basic Retrieval Application
</subsectionHeader>
<bodyText confidence="0.999989909090909">
We created a basic retrieval system in two steps:
first, we collected relevant documents; then, we im-
plemented a search engine over the collected docu-
ments.
In order to construct a local corpus, an archae-
ology expert searched the Web for relevant sites
and pages. We then downloaded all the documents
linked from those pages using a crawler. The expert
looked for documents in both English and Hebrew.
In total, we collected a non-comparable bilingual
corpus for Archaeology containing several thousand
documents in English and Hebrew.
We implemented our enhanced retrieval modules
on top of the basic Jakarta Lucene indexing and
search engine1. All documents were indexed using
Lucene, but instead of inflected words, we indexed
the lemma of each word (see detailed description of
our Hebrew lemmatization in Section 3.3). In order
to match the indexed terms, query terms (either He-
brew or English) were also lemmatized before the
index was searched, in a manner similar to lemma-
tizing the documents.
</bodyText>
<subsectionHeader confidence="0.999167">
3.2 Query Expansion Using Entailment Rules
</subsectionHeader>
<bodyText confidence="0.958427777777778">
As described in Section 2.1, entailment rules had not
been used as a knowledge resource for expanding IR
queries, prior to our work. In this paper we use this
resource instead of the typical lexical expansion in
order to test its benefit. Most entailment rules cap-
ture relations between different predicates. We thus
focus on documents retrieved for queries that con-
tain a predicate over one or two entities, which we
term here Relational IR. We would like to retrieve
only documents that describe an occurrence of that
predicate, but possibly in words different than the
ones used in the query. In this section we describe
in detail how we learn entailment rules and how we
apply them in query expansion.
Automatically Learning Entailment Rules from
the Web Many algorithms for automatically learn-
ing paraphrases and entailment rules have been
explored in recent years (Lin and Pantel, 2001;
</bodyText>
<footnote confidence="0.988895">
1http://jakarta.apache.org/lucene/docs/index.html
</footnote>
<page confidence="0.999002">
67
</page>
<bodyText confidence="0.998049430379747">
Ravichandran and Hovy, 2002; Shinyama et al.,
2002; Barzilay and Lee, 2003; Sudo et al., 2003;
Szpektor et al., 2004; Satoshi, 2005). In this pa-
per we use TEASE (Szpektor et al., 2004), a state-
of-the-art unsupervised acquisition algorithm for
lexical-syntactic entailment rules.
TEASE acquires entailment relations for a given
input template from the Web. It first retrieves from
the Web sentences that match the input template.
From these sentences it extracts the variable instan-
tiations, termed anchor-sets, which are identified as
being characteristic for the input template based on
statistical criteria.
Next, TEASE retrieves from the Web sentences
that contain the extracted anchor-sets. The retrieved
sentences are parsed and the anchors found in each
sentence are replaced with their corresponding vari-
ables. Finally, from this retrieved corpus of parsed
sentences, templates that are assumed to entail or
be entailed by the input template are learned. The
learned templates are ranked by the number of oc-
currences they were learned from.
Entailment Rules for Domain Specific Query Ex-
pansion Our goal is to use the knowledge-base of
entailment rules learned by TEASE in order to per-
form query expansion. The two subtasks that arise
are: (a) acquiring an appropriate knowledge-base
of rules; and (b) expanding a query given such a
knowledge-base.
TEASE learns entailment rules for a given input
template. As our document collection is domain
specific, a list of such relevant input templates can
be prepared. In our case, we used an archaeology
expert to generate a list of verbs and verb phrases
that relate to archaeology, such as: ‘excavate’, ‘in-
vade’, ‘build’, ‘reconstruct’, ‘grow’ and ‘be located
in’. We then executed TEASE on each of the tem-
plates representing these verbs in order to learn from
the Web rules in which the input templates partici-
pate. An example for such rules is presented in Ta-
ble 1. We learned approximately 3900 rules for 80
input templates.
Since TEASE learns lexical-syntactic rules, we
need a syntactic representation of the query. We
parse each query using the Minipar dependency
parser (Lin, 1998). We next try to match the left
hand side template of every rule in the learned
knowledge-base. Since TEASE does not identify
the direction of the relation learned between two
templates, we try both directional rules that are in-
duced from a learned relation. Whenever a match
is found, a new query is generated, in which the
constant terms of the matched left hand side tem-
plate are replaced with the constant terms of the right
hand side template. For example, given the query
“excavations of Jerusalem by archaeologists” and a
learned rule ‘excavation of Y by X —* X dig in Y ’,
a new query is generated, containing the terms ‘ar-
chaeologists dig in Jerusalem’. Finally, we retrieve
all the documents that contain all the terms of at least
one of the expanded queries (including the original
query). The basic search engine provides a score for
each document. We re-score each document as the
sum of scores it obtained from the different queries
that it matched. Figure 1 shows an example of our
query expansion, where the first retrieved documents
do not contain the words used to describe the predi-
cate in the query, but other ways to describe it.
All the templates learned by TEASE contain two
variables, and thus the rules that are learned can only
be applied to queries that contain predicates over
two terms. In order to broaden the coverage of the
learned rules, we automatically generate also all the
partial templates of a learned template. These are
templates that contain just one of variables in the
original template. We then generate rules between
these partial templates that correspond to the origi-
nal rules. With partial templates/rules, expansion for
the query in Figure 1 becomes possible.
</bodyText>
<subsectionHeader confidence="0.984187">
3.3 Cross-lingual IR
</subsectionHeader>
<bodyText confidence="0.999804">
Until very recently, linguistic resources for Hebrew
were few and far between (Wintner, 2004). The last
few years, however, have seen a proliferation of re-
sources and tools for this language. In this work we
utilize a relatively large-scale lexicon of over 22,000
entries (Itai et al., 2006); a finite-state based mor-
phological analyzer of Hebrew that is directly linked
to the lexicon (Vona and Wintner, 2007); a medium-
size bilingual dictionary of some 24,000 word pairs;
and a rudimentary Hebrew to English machine trans-
lation system (Lavie et al., 2004). All these re-
sources had to be adapted to the domain of the Hecht
museum.
Cross-lingual language technology is utilized in
</bodyText>
<page confidence="0.999115">
68
</page>
<figureCaption confidence="0.9919625">
Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two
retrieved texts (listed under ‘matched query’) do not contain the original query.
</figureCaption>
<bodyText confidence="0.9990455">
three different components of the system: Hebrew
documents are morphologically processed to pro-
vide better indexing; query terms in English are
translated to Hebrew and vice versa; and Hebrew
snippets are translated to English. We discuss each
of these components in this section.
Linguistically-aware indexing The correct level
of indexing for morphologically-rich language has
been a matter of some debate in the information re-
trieval literature. When Arabic is concerned, Dar-
wish and Oard (2002) conclude that “Character n-
grams or lightly stemmed words were found to
typically yield near-optimal retrieval effectiveness”.
Since Hebrew is even more morphologically (and
orthographically) ambiguous than Arabic, and espe-
cially in light of the various prefix particles which
can be attached to Hebrew words, we opted for full
morphological analysis of Hebrew documents be-
fore they are indexed, followed by indexing on the
lexeme.
We use the HAMSAH morphological analyzer
(Vona and Wintner, 2007), which was recently re-
written in Java and is therefore more portable and
efficient (Wintner, 2007). We processed the entire
domain specific corpus described above and used
the resulting lexemes to index documents. This pre-
processing brought to the foreground several omis-
sions of the analyzer, mostly due to domain-specific
terms missing in the lexicon. We selected the one
thousand most frequent words with no morphologi-
cal analysis and added their lexemes to the lexicon.
While we do not have quantitative evaluation met-
rics, the coverage of the system improved in a very
evident way.
Query translation When users submit a query in
one language they are provided with the option to re-
quest a translation of the query to the other language,
thereby retrieving documents in the other language.
The motivation behind this capability is that users
who may be able to read documents in a language
may find the specification of queries in that language
too challenging; also, retrieving documents in a for-
eign language may be useful due to the non-textual
information in the retrieved documents, especially in
a museum environment.
In order to support cross-lingual query specifica-
tion we capitalized on a medium-size bilingual dic-
tionary that was already used for Hebrew to Eng-
lish machine translation. Since the coverage of the
dictionary was rather limited, and many domain-
specific items were missing, we chose the one thou-
sand most frequent lexemes which had no transla-
</bodyText>
<page confidence="0.997531">
69
</page>
<table confidence="0.99995375">
Input Template Learned Template
X excavate Y X discover Y , X find Y ,
X uncover Y , X examine Y ,
X unearth Y , X explore Y
X construct Y X build Y , X develop Y ,
X create Y , X establish Y
X contribute to Y X cause Y , X linked to Y ,
X involve in Y
date X to Y X built in Y , X began in Y ,
X go back to Y
X cover Y X bury Y ,
X provide coverage for Y
X invade Y X occupy Y , X attack Y ,
X raid Y , X move into Y
X restore Y X protect Y , X preserve Y ,
X save Y , X conserve Y
</table>
<tableCaption confidence="0.853568">
Table 1: Examples for correct templates that were
learned by TEASE for input templates.
</tableCaption>
<bodyText confidence="0.999710311475411">
tions and translated them manually, augmenting the
lexicon with missing Hebrew lexemes where neces-
sary and expanding the bilingual dictionary to cover
this domain.
In order to translate query terms we use the He-
brew English dictionary also as an English-Hebrew
dictionary. While this is known to be sub-optimal,
our current results support such an adaptation in lieu
of dedicated directional bilingual dictionaries.
Translating a query from one language to another
may introduce ambiguity where none exists. For
example, the query term spinh ‘vessel’ is unam-
biguous in Hebrew, but once translated into English
will result in retrieving documents on both senses
of the English word. Usually, this problem is over-
come since users tend to specify multi-term queries,
and the terms disambiguate each other. However,
a more systematic solution can be offered since we
have access to semantic expansion capabilities (in a
single language). That is, expanding the query in
the source language will result in more query terms
which, when translated, are more likely to disam-
biguate the context. We leave such an extension for
future work.
Snippet translation When Hebrew documents are
retrieved, we augment the (Hebrew) snippet which
the system produces by an English translation. We
use an extended, improved version of a rudimentary
Hebrew to English MT system developed by Lavie
et al. (2004). Extensions include an improved mor-
phological analysis of the input, an extended bilin-
gual dictionary and a revised set of transfer rules,
as well as a more modern transfer engine and a
much larger language model for generating the tar-
get (English) sentences.
The MT system is transfer based: it performs lin-
guistic pre-processing of the source language (in our
case, morphological analysis) and post-processing
of the target (generation of English word forms), and
uses a small set of transfer rules to translate local
structures from the source to the target and create
translation hypotheses, which are stored in a lattice.
A statistical language model is used to decode the
lattice and select the best hypotheses.
The benefit of this architecture is that domain spe-
cific adaptation of the system is relatively easy, and
does not require a domain specific parallel corpus
(which we do not have). The system has access
to our domain-specific lexicon and bilingual dictio-
nary, and we even refined some transfer rules due to
peculiarities of the domain. One advantage of the
transfer-based approach is that it enables us to treat
out-of-lexicon items in a unique way. We consider
such items proper names, and transfer rules process
them as such. As an example, Figure 2 depicts the
translation of a Hebrew snippet meaning A jar from
the early bronze period with seashells from the Nile.
The word nilws ‘Nile’ is missing from the lexicon,
but this does not prevent the system from producing
a legible translation, using the transliterated form
where an English equivalent is unavailable.
</bodyText>
<sectionHeader confidence="0.99711" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999939">
We described a system for cross-lingual and
semantically-enhanced retrieval of information in
the cultural heritage domain, obtained by adapting
existing state-of-the-art tools and resources to the
domain. The system enhances the experience of mu-
seum visits, using language technology as a vehi-
cle for long-lasting instillation of information. Due
to the novelty of this application and the dearth of
available multilingual annotated resources in this
domain, we are unable to provide a robust, quan-
</bodyText>
<page confidence="0.995318">
70
</page>
<figureCaption confidence="0.996677">
Figure 2: Translation example
</figureCaption>
<table confidence="0.99885675">
Query Without Expansion With Expansion
Relevant Total Relevant Total
in Top 10 Retrieved in Top 10 Retrieved
discovering boats 2 2 5 86
growing vineyards 0 0 6 8
Persian invasions 5 5 8 22
excavations of the Byzantine period 10 37 10 100
restoring mosaics 0 0 3 69
</table>
<tableCaption confidence="0.749069">
Table 2: Analysis of the number of relevant documents out of the top 10 and the total number of retrieved
documents (up to 100) for a sample of queries.
</tableCaption>
<bodyText confidence="0.999904272727273">
titative evaluation of the approach. A preliminary
analysis of a sample of queries is presented in Ta-
ble 2. It illustrates the potential of expansion for
document collections of narrow domain. In what
follows we provide some qualitative impressions.
We observed that the system was able to learn
many expansion rules that cannot be induced from
manually constructed lexical resources, such as the-
sauri or WordNet (Fellbaum, 1998). This is espe-
cially true for rules that are specific for a narrow do-
main, e.g. ‘X restore Y —* X preserve Y ’. Fur-
thermore, the system learned lexical syntactic rules
that cannot be expressed by a mere lexical substitu-
tion, but include also a syntactic transformation. For
example, ‘date X to Y H X go back to Y ’.
In addition, since rules are acquired by searching
the Web, they are not necessarily restricted to learn-
ing from the target domain, but can be learned from
similar terminology in other domains. For example,
the rule ‘X discover Y H X find Y ’ was learned
from contexts such as {X=‘astronomers’;Y =‘new
planets’} and {X=‘zoologists’;Y =‘new species’}.
The quality of the rules that were automatically
acquired is mediocre. We found that although many
rules were useful for expansion, they had to be
manually filtered in order to retain only rules that
achieved high precision.
Finally, we note that applying semantic query ex-
pansion (using entailment rules), followed by Eng-
lish to Hebrew query translation, results in query ex-
pansion for Hebrew using techniques that were so
far applicable only to resource-rich languages, such
as English.
</bodyText>
<sectionHeader confidence="0.994957" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.979625888888889">
This research was supported by the Israel Internet
Association; by THE ISRAEL SCIENCE FOUN-
DATION (grant No. 137/06 and grant No. 1095/05);
by the Caesarea Rothschild Institute for Interdisci-
plinary Application of Computer Science at the Uni-
versity of Haifa; by the ITC-irst/University of Haifa
collaboration; and by the US National Science Foun-
dation (grants IIS-0121631, IIS-0534217, and the
Office of International Science and Engineering).
</bodyText>
<page confidence="0.996068">
71
</page>
<bodyText confidence="0.999940166666667">
We wish to thank the Hebrew Knowledge Center
at the Technion for providing resources for Hebrew.
We are grateful to Oliviero Stock, Martin Golumbic,
Alon Itai, Dalia Bojan, Erik Peterson, Nurit Mel-
nik, Yaniv Eytani and Noam Ordan for their help
and support.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999487377777778">
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 84–91.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Second PASCAL Challenge Work-
shop for Recognizing Textual Entailment.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings ofHLT-NAACL.
Jaime G. Carbonell, Yiming Yang, Robert E. Frederk-
ing, Ralf D. Brown, Yibing Geng, and Danny Lee.
1997. Translingual information retrieval: A compar-
ative evaluation. In IJCAI (1), pages 708–715.
Ido Dagan, Oren Glickman, and Bernardo. Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In Lecture Notes in Computer Science, Volume
3944, volume 3944, pages 177–190.
Kareem Darwish and Douglas W. Oard. 2002. Term se-
lection for searching printed Arabic. In SIGIR ’02:
Proceedings of the 25th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 261–268, New York, NY,
USA. ACM Press.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and Com-
munication. MIT Press.
D. A. Hull and G. Grefenstette. 1996. Querying across
languages. a dictionary-based approach to multilingual
information retrieval. In Proceedings of the 19th ACM
SIGIR Conference, pages 49–57.
Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A
computational lexicon of contemporary Hebrew. In
Proceedings of The fifth international conference on
Language Resources and Evaluation (LREC-2006).
Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson,
and Katharina Probst. 2004. Rapid prototyping of a
transfer-based Hebrew-to-English machine translation
system. In Proceedings of TMI-2004: The 10th Inter-
national Conference on Theoretical and Methodolog-
ical Issues in Machine Translation, Baltimore, MD,
October.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Natural Lan-
guage Engineering, volume 7(4), pages 343–360.
Dekang Lin. 1998. Dependency-based evaluation of
minipar. In Proceedings of the Workshop on Evalu-
ation of Parsing Systems at LREC.
S. Lytinen, N. Tomuro, and T. Repede. 2000. The use of
wordnet sense tagging in faqfinder. In Proceedings of
the AAAI00 Workshop on AI and Web Search.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings ofACL.
Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido
Dagan, and Alberto Lavelli. 2006. Investigating a
generic paraphrase-based approach for relation extrac-
tion. In Proceedings of EACL.
Sekine Satoshi. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings ofIWP.
Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings ofHLT.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings ofACL.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP.
Shuly Wintner. 2004. Hebrew computational linguis-
tics: Past and future. Artificial Intelligence Review,
21(2):113–138.
Shuly Wintner. 2007. Finite-state technology as a pro-
gramming environment. In Alexander Gelbukh, edi-
tor, Proceedings of the Conference on Computational
Linguistics and Intelligent Text Processing (CICLing-
2007), volume 4394 of Lecture Notes in Computer Sci-
ence, pages 97–106, Berlin and Heidelberg, February.
Springer.
Shlomo Yona and Shuly Wintner. 2007. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering. To appear.
Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical
query paraphrasing for document retrieval. In Pro-
ceedings ofACL.
</reference>
<page confidence="0.998719">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.488794">
<title confidence="0.9982">Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation</title>
<author confidence="0.975056">Idan Szpektor</author>
<author confidence="0.975056">Ido Dagan Alon Lavie Danny Shacham</author>
<author confidence="0.975056">Shuly Wintner</author>
<affiliation confidence="0.783537">Dept. of Computer Science Language Technologies Inst. Dept. of Computer Science Bar Ilan University Carnegie Mellon University University of</affiliation>
<email confidence="0.953308">szpekti@cs.biu.ac.ilalavie+@cs.cmu.edushuly@cs.haifa.ac.il</email>
<abstract confidence="0.991744461538462">We describe a system which enhances the experience of museum visits by providing users with language-technology-based information retrieval capabilities. The system consists of a cross-lingual search engine, augmented by state of the art semantic expansion technology, specifically designed for the domain of the museum (history and archaeology of Israel). We discuss the technology incorporated in the system, its adaptation to the specific domain and its contribution to cultural heritage appreciation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lisa Ballesteros</author>
<author>W Bruce Croft</author>
</authors>
<title>Phrasal translation and query expansion techniques for crosslanguage information retrieval.</title>
<date>1997</date>
<booktitle>In ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>84--91</pages>
<contexts>
<context position="8438" citStr="Ballesteros and Croft, 1997" startWordPosition="1309" endWordPosition="1312">elatively poor at searching for them. Other visitors may be unable to read Hebrew texts, but still benefit from non-textual information that are contained in Hebrew documents (e.g., pictures, maps, audio and video files, external links, etc.) This problem is addressed by the paradigm of Cross-Lingual Information Retrieval (CLIR). This paradigm has become a very active research area in recent years, addressing the needs of multilingual and non-English speaking communities, such as the 66 European Union, East-Asian nations and Spanish speaking communities in the US (Hull and Grefenstette, 1996; Ballesteros and Croft, 1997; Carbonell et al., 1997). The common approach for CLIR is to translate a query in a source language to another target language and then issue the translated query to retrieve target language documents. As explained above, CLIR research has to address various generic problems caused by the variability and ambiguity of natural languages, as well as specific problems related to the particular languages being addressed. 3 Coping with Semantic Variability in IR We describe a search engine that is capable of performing: (a) semantic English information retrieval; and (b) cross-lingual (Hebrew-Engli</context>
</contexts>
<marker>Ballesteros, Croft, 1997</marker>
<rawString>Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal translation and query expansion techniques for crosslanguage information retrieval. In ACM SIGIR Conference on Research and Development in Information Retrieval, pages 84–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Second PASCAL Challenge Workshop for Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="6085" citStr="Bar-Haim et al., 2006" startWordPosition="939" endWordPosition="942">ilment and Entailment Rules In many NLP applications, such as Question Answering (QA), Information Extraction (IE) and Information Retrieval (IR), it is crucial to recognize that a specific target meaning can be inferred from different text variants. For example, a QA system needs to induce that “Mendelssohn wrote incidental music” can be inferred from “Mendelssohn composed incidental music” in order to answer the question “Who wrote incidental music?”. This type of reasoning has been identified as a core semantic inference task by the generic textual entailment framework (Dagan et al., 2006; Bar-Haim et al., 2006). The typical way to address variability in IR is to use lexical query expansion (Lytinen et al., 2000; Zukerman and Raskutti, 2002). However, there are variability patterns that cannot be described using just constant phrase to phrase entailment. Another important type of knowledge representation is entailment rules and paraphrases. An entailment rule is a directional relation between two templates, text patterns with variables, e.g., ‘X compose Y —* X write Y ’. The left hand side is assumed to entail the right hand side in certain contexts, under the same variable instantiation. Paraphrases</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Second PASCAL Challenge Workshop for Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="12819" citStr="Barzilay and Lee, 2003" startWordPosition="2014" endWordPosition="2017"> entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the ex</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiplesequence alignment. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime G Carbonell</author>
<author>Yiming Yang</author>
<author>Robert E Frederking</author>
<author>Ralf D Brown</author>
<author>Yibing Geng</author>
<author>Danny Lee</author>
</authors>
<title>Translingual information retrieval: A comparative evaluation.</title>
<date>1997</date>
<booktitle>In IJCAI (1),</booktitle>
<pages>708--715</pages>
<contexts>
<context position="8463" citStr="Carbonell et al., 1997" startWordPosition="1313" endWordPosition="1316">or them. Other visitors may be unable to read Hebrew texts, but still benefit from non-textual information that are contained in Hebrew documents (e.g., pictures, maps, audio and video files, external links, etc.) This problem is addressed by the paradigm of Cross-Lingual Information Retrieval (CLIR). This paradigm has become a very active research area in recent years, addressing the needs of multilingual and non-English speaking communities, such as the 66 European Union, East-Asian nations and Spanish speaking communities in the US (Hull and Grefenstette, 1996; Ballesteros and Croft, 1997; Carbonell et al., 1997). The common approach for CLIR is to translate a query in a source language to another target language and then issue the translated query to retrieve target language documents. As explained above, CLIR research has to address various generic problems caused by the variability and ambiguity of natural languages, as well as specific problems related to the particular languages being addressed. 3 Coping with Semantic Variability in IR We describe a search engine that is capable of performing: (a) semantic English information retrieval; and (b) cross-lingual (Hebrew-English and EnglishHebrew) inf</context>
</contexts>
<marker>Carbonell, Yang, Frederking, Brown, Geng, Lee, 1997</marker>
<rawString>Jaime G. Carbonell, Yiming Yang, Robert E. Frederking, Ralf D. Brown, Yibing Geng, and Danny Lee. 1997. Translingual information retrieval: A comparative evaluation. In IJCAI (1), pages 708–715.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<marker>Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo. Magnini. 2006. The pascal recognising textual entailment challenge. In Lecture Notes in Computer Science, Volume 3944, volume 3944, pages 177–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kareem Darwish</author>
<author>Douglas W Oard</author>
</authors>
<title>Term selection for searching printed Arabic. In</title>
<date>2002</date>
<booktitle>SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>261--268</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="17984" citStr="Darwish and Oard (2002)" startWordPosition="2855" endWordPosition="2859">eries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different components of the system: Hebrew documents are morphologically processed to provide better indexing; query terms in English are translated to Hebrew and vice versa; and Hebrew snippets are translated to English. We discuss each of these components in this section. Linguistically-aware indexing The correct level of indexing for morphologically-rich language has been a matter of some debate in the information retrieval literature. When Arabic is concerned, Darwish and Oard (2002) conclude that “Character ngrams or lightly stemmed words were found to typically yield near-optimal retrieval effectiveness”. Since Hebrew is even more morphologically (and orthographically) ambiguous than Arabic, and especially in light of the various prefix particles which can be attached to Hebrew words, we opted for full morphological analysis of Hebrew documents before they are indexed, followed by indexing on the lexeme. We use the HAMSAH morphological analyzer (Vona and Wintner, 2007), which was recently rewritten in Java and is therefore more portable and efficient (Wintner, 2007). We</context>
</contexts>
<marker>Darwish, Oard, 2002</marker>
<rawString>Kareem Darwish and Douglas W. Oard. 2002. Term selection for searching printed Arabic. In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 261–268, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database. Language, Speech and Communication.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. Language, Speech and Communication. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Hull</author>
<author>G Grefenstette</author>
</authors>
<title>Querying across languages. a dictionary-based approach to multilingual information retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of the 19th ACM SIGIR Conference,</booktitle>
<pages>49--57</pages>
<contexts>
<context position="8409" citStr="Hull and Grefenstette, 1996" startWordPosition="1304" endWordPosition="1308">brew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew texts, but still benefit from non-textual information that are contained in Hebrew documents (e.g., pictures, maps, audio and video files, external links, etc.) This problem is addressed by the paradigm of Cross-Lingual Information Retrieval (CLIR). This paradigm has become a very active research area in recent years, addressing the needs of multilingual and non-English speaking communities, such as the 66 European Union, East-Asian nations and Spanish speaking communities in the US (Hull and Grefenstette, 1996; Ballesteros and Croft, 1997; Carbonell et al., 1997). The common approach for CLIR is to translate a query in a source language to another target language and then issue the translated query to retrieve target language documents. As explained above, CLIR research has to address various generic problems caused by the variability and ambiguity of natural languages, as well as specific problems related to the particular languages being addressed. 3 Coping with Semantic Variability in IR We describe a search engine that is capable of performing: (a) semantic English information retrieval; and (b</context>
</contexts>
<marker>Hull, Grefenstette, 1996</marker>
<rawString>D. A. Hull and G. Grefenstette. 1996. Querying across languages. a dictionary-based approach to multilingual information retrieval. In Proceedings of the 19th ACM SIGIR Conference, pages 49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Itai</author>
<author>Shuly Wintner</author>
<author>Shlomo Yona</author>
</authors>
<title>A computational lexicon of contemporary Hebrew.</title>
<date>2006</date>
<booktitle>In Proceedings of The fifth international conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<contexts>
<context position="16906" citStr="Itai et al., 2006" startWordPosition="2687" endWordPosition="2690">l the partial templates of a learned template. These are templates that contain just one of variables in the original template. We then generate rules between these partial templates that correspond to the original rules. With partial templates/rules, expansion for the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Vona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in 68 Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different comp</context>
</contexts>
<marker>Itai, Wintner, Yona, 2006</marker>
<rawString>Alon Itai, Shuly Wintner, and Shlomo Yona. 2006. A computational lexicon of contemporary Hebrew. In Proceedings of The fifth international conference on Language Resources and Evaluation (LREC-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Shuly Wintner</author>
<author>Yaniv Eytani</author>
<author>Erik Peterson</author>
<author>Katharina Probst</author>
</authors>
<title>Rapid prototyping of a transfer-based Hebrew-to-English machine translation system.</title>
<date>2004</date>
<booktitle>In Proceedings of TMI-2004: The 10th International Conference on Theoretical and Methodological Issues in Machine Translation,</booktitle>
<location>Baltimore, MD,</location>
<contexts>
<context position="17171" citStr="Lavie et al., 2004" startWordPosition="2730" endWordPosition="2733">r the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Vona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in 68 Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different components of the system: Hebrew documents are morphologically processed to provide better indexing; query terms in English are translated to Hebrew and vice versa; and Hebrew snippets are translated to English. We discuss each of these components in this section. Ling</context>
<context position="21912" citStr="Lavie et al. (2004)" startWordPosition="3538" endWordPosition="3541">the terms disambiguate each other. However, a more systematic solution can be offered since we have access to semantic expansion capabilities (in a single language). That is, expanding the query in the source language will result in more query terms which, when translated, are more likely to disambiguate the context. We leave such an extension for future work. Snippet translation When Hebrew documents are retrieved, we augment the (Hebrew) snippet which the system produces by an English translation. We use an extended, improved version of a rudimentary Hebrew to English MT system developed by Lavie et al. (2004). Extensions include an improved morphological analysis of the input, an extended bilingual dictionary and a revised set of transfer rules, as well as a more modern transfer engine and a much larger language model for generating the target (English) sentences. The MT system is transfer based: it performs linguistic pre-processing of the source language (in our case, morphological analysis) and post-processing of the target (generation of English word forms), and uses a small set of transfer rules to translate local structures from the source to the target and create translation hypotheses, whi</context>
</contexts>
<marker>Lavie, Wintner, Eytani, Peterson, Probst, 2004</marker>
<rawString>Alon Lavie, Shuly Wintner, Yaniv Eytani, Erik Peterson, and Katharina Probst. 2004. Rapid prototyping of a transfer-based Hebrew-to-English machine translation system. In Proceedings of TMI-2004: The 10th International Conference on Theoretical and Methodological Issues in Machine Translation, Baltimore, MD, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for question answering.</title>
<date>2001</date>
<journal>In Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<pages>343--360</pages>
<contexts>
<context position="7239" citStr="Lin and Pantel, 2001" startWordPosition="1117" endWordPosition="1120">rtain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching </context>
<context position="12690" citStr="Lin and Pantel, 2001" startWordPosition="2000" endWordPosition="2003">lations between different predicates. We thus focus on documents retrieved for queries that contain a predicate over one or two entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being char</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question answering. In Natural Language Engineering, volume 7(4), pages 343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC.</booktitle>
<contexts>
<context position="14884" citStr="Lin, 1998" startWordPosition="2346" endWordPosition="2347"> we used an archaeology expert to generate a list of verbs and verb phrases that relate to archaeology, such as: ‘excavate’, ‘invade’, ‘build’, ‘reconstruct’, ‘grow’ and ‘be located in’. We then executed TEASE on each of the templates representing these verbs in order to learn from the Web rules in which the input templates participate. An example for such rules is presented in Table 1. We learned approximately 3900 rules for 80 input templates. Since TEASE learns lexical-syntactic rules, we need a syntactic representation of the query. We parse each query using the Minipar dependency parser (Lin, 1998). We next try to match the left hand side template of every rule in the learned knowledge-base. Since TEASE does not identify the direction of the relation learned between two templates, we try both directional rules that are induced from a learned relation. Whenever a match is found, a new query is generated, in which the constant terms of the matched left hand side template are replaced with the constant terms of the right hand side template. For example, given the query “excavations of Jerusalem by archaeologists” and a learned rule ‘excavation of Y by X —* X dig in Y ’, a new query is gene</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lytinen</author>
<author>N Tomuro</author>
<author>T Repede</author>
</authors>
<title>The use of wordnet sense tagging in faqfinder.</title>
<date>2000</date>
<booktitle>In Proceedings of the AAAI00 Workshop on AI and</booktitle>
<contexts>
<context position="6187" citStr="Lytinen et al., 2000" startWordPosition="957" endWordPosition="960">ction (IE) and Information Retrieval (IR), it is crucial to recognize that a specific target meaning can be inferred from different text variants. For example, a QA system needs to induce that “Mendelssohn wrote incidental music” can be inferred from “Mendelssohn composed incidental music” in order to answer the question “Who wrote incidental music?”. This type of reasoning has been identified as a core semantic inference task by the generic textual entailment framework (Dagan et al., 2006; Bar-Haim et al., 2006). The typical way to address variability in IR is to use lexical query expansion (Lytinen et al., 2000; Zukerman and Raskutti, 2002). However, there are variability patterns that cannot be described using just constant phrase to phrase entailment. Another important type of knowledge representation is entailment rules and paraphrases. An entailment rule is a directional relation between two templates, text patterns with variables, e.g., ‘X compose Y —* X write Y ’. The left hand side is assumed to entail the right hand side in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language,</context>
</contexts>
<marker>Lytinen, Tomuro, Repede, 2000</marker>
<rawString>S. Lytinen, N. Tomuro, and T. Repede. 2000. The use of wordnet sense tagging in faqfinder. In Proceedings of the AAAI00 Workshop on AI and Web Search.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7269" citStr="Ravichandran and Hovy, 2002" startWordPosition="1121" endWordPosition="1124">the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may b</context>
<context position="12772" citStr="Ravichandran and Hovy, 2002" startWordPosition="2006" endWordPosition="2009">for queries that contain a predicate over one or two entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retri</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenza Romano</author>
<author>Milen Kouylekov</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
<author>Alberto Lavelli</author>
</authors>
<title>Investigating a generic paraphrase-based approach for relation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="7317" citStr="Romano et al., 2006" startWordPosition="1131" endWordPosition="1134">ed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew texts, but still benefit</context>
</contexts>
<marker>Romano, Kouylekov, Szpektor, Dagan, Lavelli, 2006</marker>
<rawString>Lorenza Romano, Milen Kouylekov, Idan Szpektor, Ido Dagan, and Alberto Lavelli. 2006. Investigating a generic paraphrase-based approach for relation extraction. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sekine Satoshi</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between ne pairs.</title>
<date>2005</date>
<booktitle>In Proceedings ofIWP.</booktitle>
<contexts>
<context position="12877" citStr="Satoshi, 2005" startWordPosition="2026" endWordPosition="2027">ieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentences are parsed an</context>
</contexts>
<marker>Satoshi, 2005</marker>
<rawString>Sekine Satoshi. 2005. Automatic paraphrase discovery based on context and keywords between ne pairs. In Proceedings ofIWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
<author>Sudo Kiyoshi</author>
<author>Ralph Grishman</author>
</authors>
<title>Automatic paraphrase acquisition from news articles.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT.</booktitle>
<contexts>
<context position="12795" citStr="Shinyama et al., 2002" startWordPosition="2010" endWordPosition="2013">edicate over one or two entities, which we term here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sente</context>
</contexts>
<marker>Shinyama, Sekine, Kiyoshi, Grishman, 2002</marker>
<rawString>Yusuke Shinyama, Satoshi Sekine, Sudo Kiyoshi, and Ralph Grishman. 2002. Automatic paraphrase acquisition from news articles. In Proceedings ofHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyoshi Sudo</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>An improved extraction pattern representation model for automatic ie pattern acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7295" citStr="Sudo et al., 2003" startWordPosition="1127" endWordPosition="1130">phrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building blocks for more complex entailment inference. For example, given the above entailment rule, a QA system can identify the answer “Mendelssohn” in the above example. This need sparked intensive research on automatic acquisition of paraphrase and entailment rules. Although knowledge-bases of entailment-rules and paraphrases learned by acquisition algorithms were used in other NLP applications, such as QA (Lin and Pantel, 2001; Ravichandran and Hovy, 2002) and IE (Sudo et al., 2003; Romano et al., 2006), to the best of our knowledge the output of such algorithms was never applied to IR before. 2.2 Cross Lingual Information Retrieval The difficulties caused by variability are amplified when the user is not a native speaker of the language in which the retrieved texts are written. For example, while most Israelis can read English documents, fewer are comfortable with the specification of English queries. In a museum setting, some visitors may be able to read Hebrew documents but still be relatively poor at searching for them. Other visitors may be unable to read Hebrew te</context>
<context position="12838" citStr="Sudo et al., 2003" startWordPosition="2018" endWordPosition="2021"> here Relational IR. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman. 2003. An improved extraction pattern representation model for automatic ie pattern acquisition. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Ido Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="12861" citStr="Szpektor et al., 2004" startWordPosition="2022" endWordPosition="2025">. We would like to retrieve only documents that describe an occurrence of that predicate, but possibly in words different than the ones used in the query. In this section we describe in detail how we learn entailment rules and how we apply them in query expansion. Automatically Learning Entailment Rules from the Web Many algorithms for automatically learning paraphrases and entailment rules have been explored in recent years (Lin and Pantel, 2001; 1http://jakarta.apache.org/lucene/docs/index.html 67 Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Sudo et al., 2003; Szpektor et al., 2004; Satoshi, 2005). In this paper we use TEASE (Szpektor et al., 2004), a stateof-the-art unsupervised acquisition algorithm for lexical-syntactic entailment rules. TEASE acquires entailment relations for a given input template from the Web. It first retrieves from the Web sentences that match the input template. From these sentences it extracts the variable instantiations, termed anchor-sets, which are identified as being characteristic for the input template based on statistical criteria. Next, TEASE retrieves from the Web sentences that contain the extracted anchor-sets. The retrieved sentenc</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Coppola. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
</authors>
<title>Hebrew computational linguistics: Past and future.</title>
<date>2004</date>
<journal>Artificial Intelligence Review,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="16708" citStr="Wintner, 2004" startWordPosition="2655" endWordPosition="2656">nd thus the rules that are learned can only be applied to queries that contain predicates over two terms. In order to broaden the coverage of the learned rules, we automatically generate also all the partial templates of a learned template. These are templates that contain just one of variables in the original template. We then generate rules between these partial templates that correspond to the original rules. With partial templates/rules, expansion for the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Vona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in 68 Figure 1: </context>
</contexts>
<marker>Wintner, 2004</marker>
<rawString>Shuly Wintner. 2004. Hebrew computational linguistics: Past and future. Artificial Intelligence Review, 21(2):113–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
</authors>
<title>Finite-state technology as a programming environment.</title>
<date>2007</date>
<booktitle>Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing (CICLing2007),</booktitle>
<volume>4394</volume>
<pages>97--106</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer.</publisher>
<location>Berlin and Heidelberg,</location>
<contexts>
<context position="17025" citStr="Wintner, 2007" startWordPosition="2708" endWordPosition="2709">ate. We then generate rules between these partial templates that correspond to the original rules. With partial templates/rules, expansion for the query in Figure 1 becomes possible. 3.3 Cross-lingual IR Until very recently, linguistic resources for Hebrew were few and far between (Wintner, 2004). The last few years, however, have seen a proliferation of resources and tools for this language. In this work we utilize a relatively large-scale lexicon of over 22,000 entries (Itai et al., 2006); a finite-state based morphological analyzer of Hebrew that is directly linked to the lexicon (Vona and Wintner, 2007); a mediumsize bilingual dictionary of some 24,000 word pairs; and a rudimentary Hebrew to English machine translation system (Lavie et al., 2004). All these resources had to be adapted to the domain of the Hecht museum. Cross-lingual language technology is utilized in 68 Figure 1: Semantic expansion example. Note that the expanded queries that were generated in the first two retrieved texts (listed under ‘matched query’) do not contain the original query. three different components of the system: Hebrew documents are morphologically processed to provide better indexing; query terms in English</context>
<context position="18481" citStr="Wintner, 2007" startWordPosition="2933" endWordPosition="2934">en a matter of some debate in the information retrieval literature. When Arabic is concerned, Darwish and Oard (2002) conclude that “Character ngrams or lightly stemmed words were found to typically yield near-optimal retrieval effectiveness”. Since Hebrew is even more morphologically (and orthographically) ambiguous than Arabic, and especially in light of the various prefix particles which can be attached to Hebrew words, we opted for full morphological analysis of Hebrew documents before they are indexed, followed by indexing on the lexeme. We use the HAMSAH morphological analyzer (Vona and Wintner, 2007), which was recently rewritten in Java and is therefore more portable and efficient (Wintner, 2007). We processed the entire domain specific corpus described above and used the resulting lexemes to index documents. This preprocessing brought to the foreground several omissions of the analyzer, mostly due to domain-specific terms missing in the lexicon. We selected the one thousand most frequent words with no morphological analysis and added their lexemes to the lexicon. While we do not have quantitative evaluation metrics, the coverage of the system improved in a very evident way. Query transl</context>
</contexts>
<marker>Wintner, 2007</marker>
<rawString>Shuly Wintner. 2007. Finite-state technology as a programming environment. In Alexander Gelbukh, editor, Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing (CICLing2007), volume 4394 of Lecture Notes in Computer Science, pages 97–106, Berlin and Heidelberg, February. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shlomo Yona</author>
<author>Shuly Wintner</author>
</authors>
<title>A finite-state morphological grammar of Hebrew. Natural Language Engineering.</title>
<date>2007</date>
<note>To appear.</note>
<marker>Yona, Wintner, 2007</marker>
<rawString>Shlomo Yona and Shuly Wintner. 2007. A finite-state morphological grammar of Hebrew. Natural Language Engineering. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Bhavani Raskutti</author>
</authors>
<title>Lexical query paraphrasing for document retrieval.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6217" citStr="Zukerman and Raskutti, 2002" startWordPosition="961" endWordPosition="964">tion Retrieval (IR), it is crucial to recognize that a specific target meaning can be inferred from different text variants. For example, a QA system needs to induce that “Mendelssohn wrote incidental music” can be inferred from “Mendelssohn composed incidental music” in order to answer the question “Who wrote incidental music?”. This type of reasoning has been identified as a core semantic inference task by the generic textual entailment framework (Dagan et al., 2006; Bar-Haim et al., 2006). The typical way to address variability in IR is to use lexical query expansion (Lytinen et al., 2000; Zukerman and Raskutti, 2002). However, there are variability patterns that cannot be described using just constant phrase to phrase entailment. Another important type of knowledge representation is entailment rules and paraphrases. An entailment rule is a directional relation between two templates, text patterns with variables, e.g., ‘X compose Y —* X write Y ’. The left hand side is assumed to entail the right hand side in certain contexts, under the same variable instantiation. Paraphrases can be viewed as bidirectional entailment rules. Such rules capture basic inferences in the language, and are used as building bloc</context>
</contexts>
<marker>Zukerman, Raskutti, 2002</marker>
<rawString>Ingrid Zukerman and Bhavani Raskutti. 2002. Lexical query paraphrasing for document retrieval. In Proceedings ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>