<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002466">
<title confidence="0.994195">
Cross-Lingual Information to the Rescue in Keyword Extraction
</title>
<author confidence="0.992404">
1Chung-Chi Huang 2Maxine Eskenazi 3Jaime Carbonell 4Lun-Wei Ku 5Ping-Che Yang
</author>
<affiliation confidence="0.984421333333333">
1,2,3Language Technologies Institute, CMU, United States
4Institute of Information Science, Academia Sinica, Taiwan
5Institute for Information Industry, Taipei, Taiwan
</affiliation>
<email confidence="0.983164">
{1u901571,4lunwei.jennifer.ku}@gmail.com
{2max+,3jgc}@cs.cmu.edu 5maciaclark@iii.org.tw
</email>
<sectionHeader confidence="0.995589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986352941177">
We introduce a method that extracts keywords
in a language with the help of the other. In our
approach, we bridge and fuse conventionally
irrelevant word statistics in languages. The
method involves estimating preferences for
keywords w.r.t. domain topics and generating
cross-lingual bridges for word statistics
integration. At run-time, we transform parallel
articles into word graphs, build cross-lingual
edges, and exploit PageRank with word
keyness information for keyword extraction.
We present the system, BiKEA, that applies
the method to keyword analysis. Experiments
show that keyword extraction benefits from
PageRank, globally learned keyword
preferences, and cross-lingual word statistics
interaction which respects language diversity.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996812962963">
Recently, an increasing number of Web services
target extracting keywords in articles for content
understanding, event tracking, or opinion mining.
Existing keyword extraction algorithm (KEA)
typically looks at articles monolingually and
calculate word significance in certain language.
However, the calculation in another language
may tell the story differently since languages
differ in grammar, phrase structure, and word
usage, thus word statistics on keyword analysis.
Consider the English article in Figure 1. Based
on the English content alone, monolingual KEA
may not derive the best keyword set. A better set
might be obtained by referring to the article and
its counterpart in another language (e.g.,
Chinese). Different word statistics in articles of
different languages may help, due to language
divergence such as phrasal structure (i.e., word
order) and word usage and repetition (resulting
from word translation or word sense) and so on.
For example, bilingual phrases “social
reintegration” and “重返社會” in Figure 1 have
inverse word orders (“social” translates into “社
會 ” and “reintegration” into “ 重 返 ”), both
“prosthesis” and “artificial limbs” translate into
“義肢”, and “physical” can be associated with “物
理 ” and “ 身 體 ” in “physical therapist” and
“physical rehabilitation” respectively. Intuitively,
using cross-lingual statistics (implicitly
leveraging language divergence) can help look at
articles from different perspectives and extract
keywords more accurately.
We present a system, BiKEA, that learns to
identify keywords in a language with the help of
the other. The cross-language information is
expected to reinforce language similarities and
value language dissimilarities, and better
understand articles in terms of keywords. An
example keyword analysis of an English article
is shown in Figure 1. BiKEA has aligned the
parallel articles at word level and determined the
scores of topical keyword preferences for words.
BiKEA learns these topic-related scores during
training by analyzing a collection of articles. We
will describe the BiKEA training process in more
detail in Section 3.
At run-time, BiKEA transforms an article in a
language (e.g., English) into PageRank word
graph where vertices are words in the article and
edges between vertices indicate the words’ co-
occurrences. To hear another side of the story,
BiKEA also constructs graph from its counterpart
in another language (e.g., Chinese). These two
independent graphs are then bridged over nodes
</bodyText>
<page confidence="0.812116">
1
</page>
<bodyText confidence="0.578552">
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1–6,
Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.892964">
The English Article:
</subsectionHeader>
<bodyText confidence="0.9966402">
I&apos;ve been in Afghanistan for 21 years. I work for the Red Cross and I&apos;m a physical therapist. My job is to
make arms and legs -- well it&apos;s not completely true. We do more than that. We provide the patients, the
Afghan disabled, first with the physical rehabilitation then with the social reintegration. It&apos;s a very logical
plan, but it was not always like this. For many years, we were just providing them with artificial limbs. It
took quite many years for the program to become what it is now. ...
</bodyText>
<subsectionHeader confidence="0.863614">
Its Chinese Counterpart:
</subsectionHeader>
<construct confidence="0.6289195">
我在阿富汗已經 21 年。 我為紅十字會工作, 我是一名物理治療師。 我的工作是製作胳膊和腿 --
恩,這不完全是事實。 我們做的還不止這些。 我們提供給患者, 阿富汗的殘疾人, 首先是身體康
復, 然後重返社會。 這是一個非常合理的計劃, 但它並不是總是這樣。 多年來,我們只是給他們
提供義肢。 花了很多年的程序 才讓這計劃成為現在的模樣。...
</construct>
<subsectionHeader confidence="0.981463">
Word Alignment Information:
</subsectionHeader>
<bodyText confidence="0.972523">
physical (物理), therapist (治療師), social (社會), reintegration (重返), physical (身體), rehabilitation (康
復), prosthesis (義肢), ...
</bodyText>
<subsectionHeader confidence="0.998175">
Scores of Topical Keyword Preferences for Words:
</subsectionHeader>
<bodyText confidence="0.943532">
(English) prosthesis: 0.32; artificial leg: 0.21; physical therapist: 0.15; rehabilitation: 0.08; ...
(Chinese) 義肢: 0.41; 物理治療師: 0.15; 康復:0.10; 阿富汗: 0.08, ...
</bodyText>
<subsectionHeader confidence="0.588638">
English Keywords from Bilingual Perspectives:
</subsectionHeader>
<bodyText confidence="0.48573">
prosthesis, artificial, leg, rehabilitation, orthopedic, ...
</bodyText>
<figureCaption confidence="0.995639">
Figure 1. An example BiKEA keyword analysis for an article.
</figureCaption>
<bodyText confidence="0.99979925">
that are bilingually equivalent or aligned. The
bridging is to take language divergence into
account and to allow for language-wise
interaction over word statistics. BiKEA, then in
bilingual context, iterates with learned word
keyness scores to find keywords. In our
prototype, BiKEA returns keyword candidates of
the article for keyword evaluation (see Figure 1);
alternatively, the keywords returned by BiKEA
can be used as candidates for social tagging the
article or used as input to an article
recommendation system.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998507953488372">
Keyword extraction has been an area of active
research and applied to NLP tasks such as
document categorization (Manning and Schutze,
2000), indexing (Li et al., 2004), and text mining
on social networking services ((Li et al., 2010);
(Zhao et al., 2011); (Wu et al., 2010)).
The body of KEA focuses on learning word
statistics in document collection. Approaches
such as tfidf and entropy, using local document
and/or across-document information, pose strong
baselines. On the other hand, Mihalcea and
Tarau (2004) apply PageRank, connecting words
locally, to extract essential words. In our work,
we leverage globally learned keyword
preferences in PageRank to identify keywords.
Recent work has been done on incorporating
semantics into PageRank. For example, Liu et al.
(2010) construct PageRank synonym graph to
accommodate words with similar meaning. And
Huang and Ku (2013) weigh PageRank edges
based on nodes’ degrees of reference. In contrast,
we bridge PageRank graphs of parallel articles to
facilitate statistics re-distribution or interaction
between the involved languages.
In studies more closely related to our work,
Liu et al. (2010) and Zhao et al. (2011) present
PageRank algorithms leveraging article topic
information for keyword identification. The main
differences from our current work are that the
article topics we exploit are specified by humans
not by automated systems, and that our
PageRank graphs are built and connected
bilingually.
In contrast to the previous research in keyword
extraction, we present a system that
automatically learns topical keyword preferences
and constructs and inter-connects PageRank
graphs in bilingual context, expected to yield
better and more accurate keyword lists for
articles. To the best of our knowledge, we are the
first to exploit cross-lingual information and take
advantage of language divergence in keyword
extraction.
</bodyText>
<sectionHeader confidence="0.978612" genericHeader="method">
3 The BiKEA System
</sectionHeader>
<bodyText confidence="0.9997834">
Submitting natural language articles to keyword
extraction systems may not work very well.
Keyword extractors typically look at articles
from monolingual points of view. Unfortunately,
word statistics derived based on a language may
</bodyText>
<page confidence="0.962556">
2
</page>
<bodyText confidence="0.999978625">
be biased due to the language’s grammar, phrase
structure, word usage and repetition and so on.
To identify keyword lists from natural language
articles, a promising approach is to automatically
bridge the original monolingual framework with
bilingual parallel information expected to respect
language similarities and diversities at the same
time.
</bodyText>
<subsectionHeader confidence="0.99966">
3.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.999138974358974">
We focus on the first step of the article
recommendation process: identifying a set of
words likely to be essential to a given article.
These keyword candidates are then returned as
the output of the system. The returned keyword
list can be examined by human users directly, or
passed on to article recommendation systems for
article retrieval (in terms of the extracted
keywords). Thus, it is crucial that keywords be
present in the candidate list and that the list not
be too large to overwhelm users or the
subsequent (typically computationally expensive)
article recommendation systems. Therefore, our
goal is to return reasonable-sized set of keyword
candidates that, at the same time, must contain
essential terms in the article. We now formally
state the problem that we are addressing.
Problem Statement: We are given a bilingual
parallel article collection of various topics from
social media (e.g., TED), an article ARTe in
language e, and its counterpart ARTc in language
c. Our goal is to determine a set of words that are
likely to contain important words of ARTe. For
this, we bridge language-specific statistics of
ARTe and ARTc via bilingual information (e.g.,
word alignments) and consider word keyness
w.r.t. ARTe’s topic such that cross-lingual
diversities are valued in extracting keywords in e.
In the rest of this section, we describe our
solution to this problem. First, we define
strategies for estimating keyword preferences for
words under different article topics (Section 3.2).
These strategies rely on a set of article-topic
pairs collected from the Web (Section 4.1), and
are monolingual, language-dependent
estimations. Finally, we show how BiKEA
generates keyword lists for articles leveraging
PageRank algorithm with word keyness and
cross-lingual information (Section 3.3).
</bodyText>
<subsectionHeader confidence="0.999036">
3.2 Topical Keyword Preferences
</subsectionHeader>
<bodyText confidence="0.9886524">
We attempt to estimate keyword preferences
with respect to a wide range of article topics.
Basically, the estimation is to calculate word
significance in a domain topic. Our learning
process is shown in Figure 2.
</bodyText>
<listItem confidence="0.998325">
(1) Generate article-word pairs in training data
(2) Generate topic-word pairs in training data
(3) Estimate keyword preferences for words w.r.t.
article topic based on various strategies
(4) Output word-and-keyword-preference-score
pairs for various strategies
</listItem>
<figureCaption confidence="0.998599">
Figure 2. Outline of the process used
to train BiKEA.
</figureCaption>
<bodyText confidence="0.999583269230769">
In the first two stages of the learning process, we
generate two sets of article and word information.
The input to these stages is a set of articles and
their domain topics. The output is a set of pairs
of article ID and word in the article, e.g.,
(ARTe=1, we=“prosthesis”) in language e or
(ARTc=1, wc=“義肢”) in language c, and a set of
pairs of article topic and word in the article, e.g.,
(tpe=“disability”, we=“prosthesis”) in e and
(tpe=“disability”, wc=“義肢”) in c. Note that the
topic information is shared between the involved
languages, and that we confine the calculation of
such word statistics in their specific language to
respect language diversities and the language-
specific word statistics will later interact in
PageRank at run-time (See Section 3.3).
The third stage estimates keyword preferences
for words across articles and domain topics using
aforementioned (ART,w) and (tp,w) sets. In our
paper, two popular estimation strategies in
Information Retrieval are explored. They are as
follows.
tfidf. tfidf(w)=freq(ART,w)/appr(ART’,w) where
term frequency in an article is divided by its
appearance in the article collection to distinguish
important words from common words.
</bodyText>
<equation confidence="0.930886">
ent. entropy(w)= -∑tp’ Pr(tp’|w)×log(Pr(tp’|w))
</equation>
<bodyText confidence="0.999969142857143">
where a word’s uncertainty in topics is used to
estimate its associations with domain topics.
These strategies take global information (i.e.,
article collection) into account, and will be used
as keyword preference models, bilingually
intertwined, in PageRank at run-time which
locally connects words (i.e., within articles).
</bodyText>
<subsectionHeader confidence="0.997854">
3.3 Run-Time Keyword Extraction
</subsectionHeader>
<bodyText confidence="0.999137">
Once language-specific keyword preference
scores for words are automatically learned, they
are stored for run-time reference. BiKEA then
uses the procedure in Figure 3 to fuse the
originally language-independent word statistics
</bodyText>
<page confidence="0.993548">
3
</page>
<bodyText confidence="0.994977666666667">
to determine keyword list for a given article. In
this procedure a machine translation technique
(i.e., IBM word aligner) is exploited to glue
statistics in the involved languages and make
bilingually motivated random-walk algorithm
(i.e., PageRank) possible.
</bodyText>
<figure confidence="0.995568826086957">
procedure PredictKW(ARTe,ARTc,KeyPrefs,WA,α,N)
//Construct language-specific word graph for PageRank
(1) EWe=constructPRwordGraph(ARTe)
(2) EWc=constructPRwordGraph(ARTc)
//Construct inter-language bridges
(3) EW=αx EWe+(1-α)x EWc
for each word alignment (wic, wje) in WA
if IsContWord(wic) and IsContWord(wje)
(4a) EW[i,j]+=1x BiWeightcont
else
(4b) EW[i,j]+=1x BiWeightnoncont
(5) normalize each row of EW to sum to 1
//Iterate for PageRank
(6) set KP1 xv to
[KeyPrefs(w1), KeyPrefs(w2), ...,KeyPrefs(wv)]
(7) initialize KI1 xv to [1/v,1/ v, ...,1/v]
repeat
(8a) KI’=λx KIx EW+(1-λ)x KP
(8b) normalize KI’ to sum to 1
(8c) update KI with KI’ after the check of KI and KI’
until maxIter or avgDifference(KI,KI’) &lt;_ smallDiff
(9) rankedKeywords=Sort words in decreasing order of KI
return the I rankedKeywords in e with highest
</figure>
<figureCaption confidence="0.999869">
Figure 3. Extracting keywords at run-time.
</figureCaption>
<bodyText confidence="0.999687214285714">
Once language-specific keyword preference
scores for words are automatically learned, they
are stored for run-time reference. BiKEA then
uses the procedure in Figure 3 to fuse the
originally language-independent word statistics
to determine keyword list for a given article. In
this procedure a machine translation technique
(i.e., IBM word aligner) is exploited to glue
statistics in the involved languages and make
bilingually motivated random-walk algorithm
(i.e., PageRank) possible.
In Steps (1) and (2) we construct PageRank
word graphs for the article ARTe in language e
and its counterpart ARTc in language c. They are
built individually to respect language properties
(such as subject-verb-object or subject-object-
verb structure). Figure 4 shows the algorithm. In
this algorithm, EW stores normalized edge
weights for word wi and wj (Step (2)). And EW
is a v by v matrix where v is the vocabulary size
of ARTe and ARTc. Note that the graph is directed
(from words to words that follow) and edge
weights are words’ co-occurrences within
window size WS. Additionally we incorporate
edge weight multiplier m&gt;1 to propagate more
PageRank scores to content words, with the
intuition that content words are more likely to be
keywords (Step (2)).
</bodyText>
<equation confidence="0.7830718">
procedure constructPRwordGraph(ART)
(1) EWv xv=0v xv
for each sentence st in ART
for each word wi in st
for each word wj in st where i&lt;j and j-i &lt;_ WS
</equation>
<figure confidence="0.787529222222222">
if not IsContWord(wi) and IsContWord(wj)
(2a) EW[i,jj+=1x m
elif not IsContWord(wi) and not IsContWord(wj)
(2b) EW[i,jj+=1x (1/m)
elif IsContWord(wi) and not IsContWord(wj)
(2c) EW[i,jj+=1x (1/m)
elif IsContWord(wi) and IsContWord(wj)
(2d) EW[i,jj+=1x m
return EW
</figure>
<figureCaption confidence="0.999758">
Figure 4. Constructing PageRank word graph.
</figureCaption>
<bodyText confidence="0.999766333333333">
Step (3) in Figure 3 linearly combines word
graphs EWe and EWc using α. We use α to
balance language properties or statistics, and
BiKEA backs off to monolingual KEA if α is one.
In Step (4) of Figure 3 for each word
alignment (wic, wje), we construct a link between
the word nodes with the weight BiWeight. The
inter-language link is to reinforce language
similarities and respect language divergence
while the weight aims to elevate the cross-
language statistics interaction. Word alignments
are derived using IBM models 1-5 (Och and Ney,
2003). The inter-language link is directed from
wic to wje, basically from language c to e based on
the directional word-aligning entry (wic, wje). The
bridging is expected to help keyword extraction
in language e with the statistics in language c.
Although alternative approach can be used for
bridging, our approach is intuitive, and most
importantly in compliance with the directional
spirit of PageRank.
Step (6) sets KP of keyword preference model
using topical preference scores learned from
Section 3.2, while Step (7) initializes KI of
PageRank scores or, in our case, word keyness
scores. Then we distribute keyness scores until
the number of iteration or the average score
differences of two consecutive iterations reach
their respective limits. In each iteration, a word’s
keyness score is the linear combination of its
keyword preference score and the sum of the
propagation of its inbound words’ previous
PageRank scores. For the word wje in ARTe, any
edge (wie,wje) in ARTe, and any edge (wkc,wje) in
WA, its new PageRank score is computed as
below.
</bodyText>
<page confidence="0.972687">
4
</page>
<table confidence="0.664104">
α X Y KN[1, i] X EWe[i, j] +
� �∈v
KN′[1, j] = λ X �
(1 − α) X Y KN[1, k] X EW[k, j]
� k∈v
+ (1 − λ) X K [1, j]
</table>
<bodyText confidence="0.999367363636364">
Once the iterative process stops, we rank
words according to their final keyness scores and
return top I ranked words in language a as
keyword candidates of the given article ARTe. An
example keyword analysis for an English article
on our working prototype is shown in Figure 1.
Note that language similarities and dissimilarities
lead to different word statistics in articles of
difference languages, and combining such word
statistics helps to generate more promising
keyword lists.
</bodyText>
<sectionHeader confidence="0.999402" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999969333333333">
BiKEA was designed to identify words of
importance in an article that are likely to cover
the keywords of the article. As such, BiKEA will
be trained and evaluated over articles.
Furthermore, since the goal of BiKEA is to
determine a good (representative) set of
keywords with the help of cross-lingual
information, we evaluate BiKEA on bilingual
parallel articles. In this section, we first present
the data sets for training BiKEA (Section 4.1).
Then, Section 4.2 reports the experimental
results under different system settings.
</bodyText>
<subsectionHeader confidence="0.998234">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9984905">
We collected approximately 1,500 English
transcripts (3.8M word tokens and 63K word
types) along with their Chinese counterparts
(3.4M and 73K) from TED (www.ted.com) for
our experiments. The GENIA tagger (Tsuruoka
and Tsujii, 2005) was used to lemmatize and
part-of-speech tag the English transcripts while
the CKIP segmenter (Ma and Chen, 2003)
segment the Chinese.
30 parallel articles were randomly chosen and
manually annotated for keywords on the English
side to examine the effectiveness of BiKEA in
English keyword extraction with the help of
Chinese.
</bodyText>
<subsectionHeader confidence="0.948149">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9911888">
Table 1 summarizes the performance of the
baseline tfidf and our best systems on the test set.
The evaluation metrics are nDCG (Jarvelin and
Kekalainen, 2002), precision, and mean
reciprocal rank.
</bodyText>
<table confidence="0.9983415">
(a) @I=5 nDCG P MRR
tfidf .509 .213 .469
PR+tfidf .676 .400 .621
BiKEA+tfidf .703 .406 .655
(b) @I=7 nDCG P MRR
tfidf .517 .180 .475
PR+tfidf .688 .323 .626
BiKEA+tfidf .720 .338 .660
(c) @I=10 nDCG P MRR
tfidf .527 .133 .479
PR+tfidf .686 .273 .626
BiKEA+tfidf .717 .304 .663
</table>
<tableCaption confidence="0.9503495">
Table 1. System performance at
(a) I=5 (b) I=7 (c) I=10.
</tableCaption>
<bodyText confidence="0.999983764705882">
As we can see, monolingual PageRank (i.e.,
PR) and bilingual PageRank (BiKEA), using
global information tfidf, outperform tfidf. They
relatively boost nDCG by 32% and P by 87%.
The MRR scores also indicate their superiority:
their top-two candidates are often keywords vs.
the 2nd place candidates from tfidf.
Encouragingly, BiKEA+tfidf achieves better
performance than the strong monolingual
PR+tfidf across I’s. Specifically, it further
improves nDCG relatively by 4.6% and MRR
relatively by 5.4%.
Overall, the topical keyword preferences, and
the inter-language bridging and the bilingual
score propagation in PageRank are simple yet
effective. And respecting language statistics and
properties helps keyword extraction.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="evaluation">
5 Summary
</sectionHeader>
<bodyText confidence="0.999889769230769">
We have introduced a method for extracting
keywords in bilingual context. The method
involves estimating keyword preferences, word-
aligning parallel articles, and bridging language-
specific word statistics using PageRank.
Evaluation has shown that the method can
identify more keywords and rank them higher in
the candidate list than monolingual KEAs. As for
future work, we would like to explore the
possibility of incorporating the articles’ reader
feedback into keyword extraction. We would
also like to examine the proposed methodology
in a multi-lingual setting.
</bodyText>
<page confidence="0.990077">
5
</page>
<sectionHeader confidence="0.980854" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9998938">
This study is conducted under the “Online and
Offline integrated Smart Commerce Platform
(1/4)” of the Institute for Information Industry
which is subsidized by the Ministry of Economy
Affairs of the Republic of China.
</bodyText>
<sectionHeader confidence="0.998386" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999526194805195">
Scott A. Golder and Bernardo A. Huberman.
2006. Usage patterns of collaborative tagging
systems. Information Science, 32(2): 198-208.
Harry Halpin, Valentin Robu, and Hana
Shepherd. 2007. The complex dynamics of
collaborative tagging. In Proceedings of the
WWW, pages 211-220.
Chung-chi Huang and Lun-wei Ku. 2013.
Interest analysis using semantic PageRank and
social interaction content. In Proceedings of
the ICDM Workshop on Sentiment Elicitation
from Natural Text for Information Retrieval
and Extraction, pages 929-936.
Kalervo Jarvelin and Jaana Kekalainen. 2002.
Cumulated gain-based evaluation of IR
technologies. ACM Transactions on
Information Systems, 20(4): 422-446.
Philipp Koehn, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the North
American Chapter of the Association for
Computational Linguistics, pages 48-54.
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin
Chen. 2004. Incorporating document
keyphrases in search results. In Proceedings of
the Americas Conference on Information
Systems.
Zhenhui Li, Ging Zhou, Yun-Fang Juan, and
Jiawei Han. 2010. Keyword extraction for
social snippets. In Proceedings of the WWW,
pages 1143-1144.
Marina Litvak and Mark Last. 2008. Graph-
based keyword extraction for single-document
summarization. In Proceedings of the ACL
Workshop on Multi-Source Multilingual
Information Extraction and Summarization,
pages 17-24.
Zhengyang Liu, Jianyi Liu, Wenbin Yao, Cong
Wang. 2010. Keyword extraction using
PageRank on synonym networks. In
Proceedings of the ICEEE, pages 1-4.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase
extraction via topic decomposition. In
Proceedings of the EMNLP, pages 366-376.
Wei-Yun Ma and Keh-Jiann Chen. 2003.
Introduction to CKIP Chinese word
segmentation system for the first international
Chinese word segmentation bakeoff. In
Proceedings of the ACL Workshop on Chinese
Language Processing.
Chris D. Manning and Hinrich Schutze. 2000.
Foundations of statistical natural language
processing. MIT Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing orders into texts. In Proceedings of
the EMNLP, pages 404-411.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1): 19-51.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005.
Bidirectional inference with the easiest-first
strategy for tagging sequence data. In
Proceedings of the EMNLP, pages 467-474.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval,
2(4): 303-336.
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010.
Automatic generation of personalized
annotation tags for Twitter users. In
Proceedings of the NAACL, pages 689-692.
Wayne Xin Zhao, Jing Jiang, Jing He, Yang
Song, Palakorn Achananuparp, Ee-Peng Lim,
and Xiaoming Li. 2011. Topical keyword
extraction from Twitter. In Proceedings of the
ACL, pages 379-388.
</reference>
<page confidence="0.998784">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.487588">
<title confidence="0.999102">Cross-Lingual Information to the Rescue in Keyword Extraction</title>
<author confidence="0.948858">Huang Eskenazi Carbonell Ku Yang</author>
<affiliation confidence="0.7026015">Technologies Institute, CMU, United of Information Science, Academia Sinica,</affiliation>
<address confidence="0.552344">for Information Industry, Taipei, Taiwan</address>
<abstract confidence="0.998957555555556">We introduce a method that extracts keywords in a language with the help of the other. In our approach, we bridge and fuse conventionally irrelevant word statistics in languages. The method involves estimating preferences for keywords w.r.t. domain topics and generating cross-lingual bridges for word statistics integration. At run-time, we transform parallel articles into word graphs, build cross-lingual edges, and exploit PageRank with word keyness information for keyword extraction. present the system, that applies the method to keyword analysis. Experiments show that keyword extraction benefits from PageRank, globally learned keyword preferences, and cross-lingual word statistics interaction which respects language diversity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Scott A Golder</author>
<author>Bernardo A Huberman</author>
</authors>
<title>Usage patterns of collaborative tagging systems.</title>
<date>2006</date>
<journal>Information Science,</journal>
<volume>32</volume>
<issue>2</issue>
<pages>198--208</pages>
<marker>Golder, Huberman, 2006</marker>
<rawString>Scott A. Golder and Bernardo A. Huberman. 2006. Usage patterns of collaborative tagging systems. Information Science, 32(2): 198-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harry Halpin</author>
<author>Valentin Robu</author>
<author>Hana Shepherd</author>
</authors>
<title>The complex dynamics of collaborative tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the WWW,</booktitle>
<pages>211--220</pages>
<marker>Halpin, Robu, Shepherd, 2007</marker>
<rawString>Harry Halpin, Valentin Robu, and Hana Shepherd. 2007. The complex dynamics of collaborative tagging. In Proceedings of the WWW, pages 211-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-chi Huang</author>
<author>Lun-wei Ku</author>
</authors>
<title>Interest analysis using semantic PageRank and social interaction content.</title>
<date>2013</date>
<booktitle>In Proceedings of the ICDM Workshop on Sentiment Elicitation from Natural Text for Information Retrieval and Extraction,</booktitle>
<pages>929--936</pages>
<contexts>
<context position="6544" citStr="Huang and Ku (2013)" startWordPosition="947" endWordPosition="950">et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by automated systems, and that our PageRank graphs are built and connected bilingually. In contrast to </context>
</contexts>
<marker>Huang, Ku, 2013</marker>
<rawString>Chung-chi Huang and Lun-wei Ku. 2013. Interest analysis using semantic PageRank and social interaction content. In Proceedings of the ICDM Workshop on Sentiment Elicitation from Natural Text for Information Retrieval and Extraction, pages 929-936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo Jarvelin</author>
<author>Jaana Kekalainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR technologies.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<pages>422--446</pages>
<contexts>
<context position="18723" citStr="Jarvelin and Kekalainen, 2002" startWordPosition="2827" endWordPosition="2830">se counterparts (3.4M and 73K) from TED (www.ted.com) for our experiments. The GENIA tagger (Tsuruoka and Tsujii, 2005) was used to lemmatize and part-of-speech tag the English transcripts while the CKIP segmenter (Ma and Chen, 2003) segment the Chinese. 30 parallel articles were randomly chosen and manually annotated for keywords on the English side to examine the effectiveness of BiKEA in English keyword extraction with the help of Chinese. 4.2 Experimental Results Table 1 summarizes the performance of the baseline tfidf and our best systems on the test set. The evaluation metrics are nDCG (Jarvelin and Kekalainen, 2002), precision, and mean reciprocal rank. (a) @I=5 nDCG P MRR tfidf .509 .213 .469 PR+tfidf .676 .400 .621 BiKEA+tfidf .703 .406 .655 (b) @I=7 nDCG P MRR tfidf .517 .180 .475 PR+tfidf .688 .323 .626 BiKEA+tfidf .720 .338 .660 (c) @I=10 nDCG P MRR tfidf .527 .133 .479 PR+tfidf .686 .273 .626 BiKEA+tfidf .717 .304 .663 Table 1. System performance at (a) I=5 (b) I=7 (c) I=10. As we can see, monolingual PageRank (i.e., PR) and bilingual PageRank (BiKEA), using global information tfidf, outperform tfidf. They relatively boost nDCG by 32% and P by 87%. The MRR scores also indicate their superiority: th</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2002</marker>
<rawString>Kalervo Jarvelin and Jaana Kekalainen. 2002. Cumulated gain-based evaluation of IR technologies. ACM Transactions on Information Systems, 20(4): 422-446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quanzhi Li</author>
<author>Yi-Fang Wu</author>
<author>Razvan Bot</author>
<author>Xin Chen</author>
</authors>
<title>Incorporating document keyphrases in search results.</title>
<date>2004</date>
<booktitle>In Proceedings of the Americas Conference on Information Systems.</booktitle>
<contexts>
<context position="5832" citStr="Li et al., 2004" startWordPosition="839" endWordPosition="842">into account and to allow for language-wise interaction over word statistics. BiKEA, then in bilingual context, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu e</context>
</contexts>
<marker>Li, Wu, Bot, Chen, 2004</marker>
<rawString>Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen. 2004. Incorporating document keyphrases in search results. In Proceedings of the Americas Conference on Information Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenhui Li</author>
<author>Ging Zhou</author>
<author>Yun-Fang Juan</author>
<author>Jiawei Han</author>
</authors>
<title>Keyword extraction for social snippets.</title>
<date>2010</date>
<booktitle>In Proceedings of the WWW,</booktitle>
<pages>1143--1144</pages>
<contexts>
<context position="5898" citStr="Li et al., 2010" startWordPosition="850" endWordPosition="853">statistics. BiKEA, then in bilingual context, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to accommodate words</context>
</contexts>
<marker>Li, Zhou, Juan, Han, 2010</marker>
<rawString>Zhenhui Li, Ging Zhou, Yun-Fang Juan, and Jiawei Han. 2010. Keyword extraction for social snippets. In Proceedings of the WWW, pages 1143-1144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Litvak</author>
<author>Mark Last</author>
</authors>
<title>Graphbased keyword extraction for single-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Multi-Source Multilingual Information Extraction and Summarization,</booktitle>
<pages>17--24</pages>
<marker>Litvak, Last, 2008</marker>
<rawString>Marina Litvak and Mark Last. 2008. Graphbased keyword extraction for single-document summarization. In Proceedings of the ACL Workshop on Multi-Source Multilingual Information Extraction and Summarization, pages 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengyang Liu</author>
<author>Jianyi Liu</author>
<author>Wenbin Yao</author>
<author>Cong Wang</author>
</authors>
<title>Keyword extraction using PageRank on synonym networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the ICEEE,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="6444" citStr="Liu et al. (2010)" startWordPosition="932" endWordPosition="935">2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by </context>
</contexts>
<marker>Liu, Liu, Yao, Wang, 2010</marker>
<rawString>Zhengyang Liu, Jianyi Liu, Wenbin Yao, Cong Wang. 2010. Keyword extraction using PageRank on synonym networks. In Proceedings of the ICEEE, pages 1-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Liu</author>
<author>Wenyi Huang</author>
<author>Yabin Zheng</author>
<author>Maosong Sun</author>
</authors>
<title>Automatic keyphrase extraction via topic decomposition.</title>
<date>2010</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>366--376</pages>
<contexts>
<context position="6444" citStr="Liu et al. (2010)" startWordPosition="932" endWordPosition="935">2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely related to our work, Liu et al. (2010) and Zhao et al. (2011) present PageRank algorithms leveraging article topic information for keyword identification. The main differences from our current work are that the article topics we exploit are specified by humans not by </context>
</contexts>
<marker>Liu, Huang, Zheng, Sun, 2010</marker>
<rawString>Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong Sun. 2010. Automatic keyphrase extraction via topic decomposition. In Proceedings of the EMNLP, pages 366-376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Yun Ma</author>
<author>Keh-Jiann Chen</author>
</authors>
<title>Introduction to CKIP Chinese word segmentation system for the first international Chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="18326" citStr="Ma and Chen, 2003" startWordPosition="2765" endWordPosition="2768">th the help of cross-lingual information, we evaluate BiKEA on bilingual parallel articles. In this section, we first present the data sets for training BiKEA (Section 4.1). Then, Section 4.2 reports the experimental results under different system settings. 4.1 Data Sets We collected approximately 1,500 English transcripts (3.8M word tokens and 63K word types) along with their Chinese counterparts (3.4M and 73K) from TED (www.ted.com) for our experiments. The GENIA tagger (Tsuruoka and Tsujii, 2005) was used to lemmatize and part-of-speech tag the English transcripts while the CKIP segmenter (Ma and Chen, 2003) segment the Chinese. 30 parallel articles were randomly chosen and manually annotated for keywords on the English side to examine the effectiveness of BiKEA in English keyword extraction with the help of Chinese. 4.2 Experimental Results Table 1 summarizes the performance of the baseline tfidf and our best systems on the test set. The evaluation metrics are nDCG (Jarvelin and Kekalainen, 2002), precision, and mean reciprocal rank. (a) @I=5 nDCG P MRR tfidf .509 .213 .469 PR+tfidf .676 .400 .621 BiKEA+tfidf .703 .406 .655 (b) @I=7 nDCG P MRR tfidf .517 .180 .475 PR+tfidf .688 .323 .626 BiKEA+t</context>
</contexts>
<marker>Ma, Chen, 2003</marker>
<rawString>Wei-Yun Ma and Keh-Jiann Chen. 2003. Introduction to CKIP Chinese word segmentation system for the first international Chinese word segmentation bakeoff. In Proceedings of the ACL Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Manning</author>
<author>Hinrich Schutze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5804" citStr="Manning and Schutze, 2000" startWordPosition="834" endWordPosition="837">idging is to take language divergence into account and to allow for language-wise interaction over word statistics. BiKEA, then in bilingual context, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into </context>
</contexts>
<marker>Manning, Schutze, 2000</marker>
<rawString>Chris D. Manning and Hinrich Schutze. 2000. Foundations of statistical natural language processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing orders into texts.</title>
<date>2004</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="6179" citStr="Mihalcea and Tarau (2004)" startWordPosition="893" endWordPosition="896">candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to accommodate words with similar meaning. And Huang and Ku (2013) weigh PageRank edges based on nodes’ degrees of reference. In contrast, we bridge PageRank graphs of parallel articles to facilitate statistics re-distribution or interaction between the involved languages. In studies more closely rel</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing orders into texts. In Proceedings of the EMNLP, pages 404-411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="15762" citStr="Och and Ney, 2003" startWordPosition="2339" endWordPosition="2342">(2d) EW[i,jj+=1x m return EW Figure 4. Constructing PageRank word graph. Step (3) in Figure 3 linearly combines word graphs EWe and EWc using α. We use α to balance language properties or statistics, and BiKEA backs off to monolingual KEA if α is one. In Step (4) of Figure 3 for each word alignment (wic, wje), we construct a link between the word nodes with the weight BiWeight. The inter-language link is to reinforce language similarities and respect language divergence while the weight aims to elevate the crosslanguage statistics interaction. Word alignments are derived using IBM models 1-5 (Och and Ney, 2003). The inter-language link is directed from wic to wje, basically from language c to e based on the directional word-aligning entry (wic, wje). The bridging is expected to help keyword extraction in language e with the statistics in language c. Although alternative approach can be used for bridging, our approach is intuitive, and most importantly in compliance with the directional spirit of PageRank. Step (6) sets KP of keyword preference model using topical preference scores learned from Section 3.2, while Step (7) initializes KI of PageRank scores or, in our case, word keyness scores. Then we</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>467--474</pages>
<contexts>
<context position="18212" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2747" endWordPosition="2750">d evaluated over articles. Furthermore, since the goal of BiKEA is to determine a good (representative) set of keywords with the help of cross-lingual information, we evaluate BiKEA on bilingual parallel articles. In this section, we first present the data sets for training BiKEA (Section 4.1). Then, Section 4.2 reports the experimental results under different system settings. 4.1 Data Sets We collected approximately 1,500 English transcripts (3.8M word tokens and 63K word types) along with their Chinese counterparts (3.4M and 73K) from TED (www.ted.com) for our experiments. The GENIA tagger (Tsuruoka and Tsujii, 2005) was used to lemmatize and part-of-speech tag the English transcripts while the CKIP segmenter (Ma and Chen, 2003) segment the Chinese. 30 parallel articles were randomly chosen and manually annotated for keywords on the English side to examine the effectiveness of BiKEA in English keyword extraction with the help of Chinese. 4.2 Experimental Results Table 1 summarizes the performance of the baseline tfidf and our best systems on the test set. The evaluation metrics are nDCG (Jarvelin and Kekalainen, 2002), precision, and mean reciprocal rank. (a) @I=5 nDCG P MRR tfidf .509 .213 .469 PR+tfidf </context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of the EMNLP, pages 467-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Learning algorithms for keyphrase extraction.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>2</volume>
<issue>4</issue>
<pages>303--336</pages>
<marker>Turney, 2000</marker>
<rawString>Peter D. Turney. 2000. Learning algorithms for keyphrase extraction. Information Retrieval, 2(4): 303-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wu</author>
<author>Bin Zhang</author>
<author>Mari Ostendorf</author>
</authors>
<title>Automatic generation of personalized annotation tags for Twitter users.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL,</booktitle>
<pages>689--692</pages>
<contexts>
<context position="5938" citStr="Wu et al., 2010" startWordPosition="858" endWordPosition="861">text, iterates with learned word keyness scores to find keywords. In our prototype, BiKEA returns keyword candidates of the article for keyword evaluation (see Figure 1); alternatively, the keywords returned by BiKEA can be used as candidates for social tagging the article or used as input to an article recommendation system. 2 Related Work Keyword extraction has been an area of active research and applied to NLP tasks such as document categorization (Manning and Schutze, 2000), indexing (Li et al., 2004), and text mining on social networking services ((Li et al., 2010); (Zhao et al., 2011); (Wu et al., 2010)). The body of KEA focuses on learning word statistics in document collection. Approaches such as tfidf and entropy, using local document and/or across-document information, pose strong baselines. On the other hand, Mihalcea and Tarau (2004) apply PageRank, connecting words locally, to extract essential words. In our work, we leverage globally learned keyword preferences in PageRank to identify keywords. Recent work has been done on incorporating semantics into PageRank. For example, Liu et al. (2010) construct PageRank synonym graph to accommodate words with similar meaning. And Huang and Ku </context>
</contexts>
<marker>Wu, Zhang, Ostendorf, 2010</marker>
<rawString>Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. Automatic generation of personalized annotation tags for Twitter users. In Proceedings of the NAACL, pages 689-692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
</authors>
<title>Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>379--388</pages>
<marker>Zhao, Jiang, 2011</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li. 2011. Topical keyword extraction from Twitter. In Proceedings of the ACL, pages 379-388.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>