<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003991">
<title confidence="0.99247">
Improving Document Clustering by Utilizing Meta-Data*
</title>
<author confidence="0.982147">
Kam-Fai Wong
</author>
<affiliation confidence="0.973201">
Department of Systems
</affiliation>
<author confidence="0.493291">
Engineering and
Engineering Management,
</author>
<affiliation confidence="0.945379">
The Chinese University of
</affiliation>
<address confidence="0.535178">
Hong Kong
</address>
<email confidence="0.980007">
kfwong@se.cuhk.edu.hk
</email>
<author confidence="0.898906">
Nam-Kiu Chan
</author>
<affiliation confidence="0.846046">
Centre for Innovation and
Technology,
The Chinese University of
</affiliation>
<address confidence="0.523249">
Hong Kong
</address>
<email confidence="0.979833">
jussie@cintec.cuhk.edu.hk
</email>
<author confidence="0.958322">
Kam-Lai Wong
</author>
<affiliation confidence="0.882502666666667">
Centre for Innovation and
Technology,
The Chinese University of
</affiliation>
<address confidence="0.536375">
Hong Kong
</address>
<email confidence="0.989222">
klwong@cintec.cuhk.edu.hk
</email>
<sectionHeader confidence="0.998556" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998085">
In this paper, we examine how to im-
prove the precision and recall of docu-
ment clustering by utilizing meta-data.
We use meta-data through NewsML tags
to assist clustering and show that this ap-
proach is effective through experiments
on sample news data. Experimental result
shows that clustering using NewsML
could improve average recall and preci-
sion over the same without using
NewsML by about 10%. Our algorithm
facilitates effective e-business for the
news media and publishing industry to
empower e-business.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934051282051">
Nowadays, people have great demand on
knowledge and information, while information
overload becoming one serious problem. News
media and publishing industry therefore try to suit
customers’ need by using electronic information
management system. Document clustering algo-
rithm has been introduced to group similar docu-
ments together for easier searching and reading.
Document clustering algorithm has been
widely used in news media and publishing indus-
try, which ensured it effectiveness over manual
clustering. With labor cost reduced and time saved,
document clustering algorithm provides conven-
ient clustered-news for users.
To improve the accuracy of document
clustering algorithm, we suggest to provide more
flexible information for each document. Under the
hypothesis that document clustering algorithm can
get better result with more information about data
used, we suggest that using additional meta-data
contained in NewsML standard could enhance the
performance of document clustering algorithms.
We evaluated the effectiveness of using
meta-data in the proposed clustering algorithm.
We used Chinese electronic news sources for the
evaluation. The experiment showed that using the
meta-data provided by NewsML achieved better
document clustering.
The remaining of the paper is organized as fol-
lows: In Section 2, we give an overview of current
document clustering approaches. Section 3, analy-
ses existing problems in document clustering and
suggests a solution using NewsML. We show the
tags that could be used in the algorithm and how
they are handled. The performance of algorithm
through experiments, and we will present the ex-
perimental results regarding measures in precision
and recall. In Section 5, we include a brief sum-
mary and discussion on future work.
</bodyText>
<sectionHeader confidence="0.94027" genericHeader="method">
2 Current Approaches
</sectionHeader>
<bodyText confidence="0.998939666666667">
Different document clustering methods have
been examined. These conventional clustering
methods mainly consist of two parts: construction
of a similarity matrix between documents and
formulation of clustering algorithm to generate
clusters.
</bodyText>
<subsectionHeader confidence="0.993607">
2.1 Similarity Matrix
</subsectionHeader>
<bodyText confidence="0.999744833333333">
The first step of conventional clustering
method is to construct a similarity matrix between
these documents so as to understand how docu-
ments are similar to one another. The constructed
similarity matrix will later be used by the cluster-
ing algorithm for generating clusters.
</bodyText>
<subsectionHeader confidence="0.577972">
* Corresponding Author: Kam-Lai Wong (klwong@cintec.cuhk.edu.hk)
</subsectionHeader>
<bodyText confidence="0.999583409090909">
Named entity method (Volk and Clematide,
2001) is one of the widely used approaches for
constructing a similarity matrix. Named Entities
form the major components in a document. When
fundamental entities like Person, Company and
Geographical names are detected, the algorithm
could understand the content of a document to a
certain extent.
Named entity method has also been investi-
gated together with keywords to perform cluster-
ing (Lam, Meng, Wong and Yen, 2001). The
similarity score is calculated based on the named
entity vectors and the keyword vector with
weighting parameters to control the degree of em-
phasis on the corresponding vectors.
Concept terms method (Wong, Lam and Yen,
1999) has been proposed in order to deal with the
problem of vocabulary switching. The potentially
concept terms are basically the keywords derived
from a separated concept generation corpus. Con-
cept terms are selected based on the co-occurrence
between a query and a document.
However, named entities approach and con-
cept terms approach contain some limitations: The
accuracy of the clustering algorithm would be di-
rectly proportional to the accuracy of algorithm.
Thus, any error from identifications of named en-
tities or concept terms will adversely affect the
quality of the clustering algorithm as well.
N-gram Algorithm (Lee, Cho and Park, 1999)
has been introduced in order to avoid the afore-
mentioned limitations. An N-gram is a character
sequence of length N extracted from a document.
The main idea of the N-gram approach is that the
character structure of a term can be used to find
semantically similar terms. The approach assumes
no prior linguistic knowledge about the text being
processed. Moreover, there is no language-
specific information used in the N-grams ap-
proach, which qualifies this method as a language-
independent approach. By using N-grams, fre-
quently appeared terms of each document can be
extracted and compared to make the similarity
measure.
</bodyText>
<subsectionHeader confidence="0.999592">
2.2 Clustering Algorithm
</subsectionHeader>
<bodyText confidence="0.999857784313725">
Probabilistic method is one of the com-
monly used methods in document clustering. The
aim of probabilistic method is to minimize the
heterogeneity in each group with respect to the
group representative based on statistical ap-
proaches (Estivill-Castro and Yang, 2001). Neural
network is also used to perform a cyclic learning
process for clustering (Grothkopf, Andernach and
Stevens-Rayburn, 1998).
Hierarchical clustering methods include
group-average clustering algorithm and single-
link clustering algorithm (Johnson and Kargupta,
2000; Tombros, Villa and Van Rijsbergen, 2002).
Group average clustering is based on creating a
hierarchical tree by initially creating a singleton
cluster for each document. The clusters are
merged to the parent node until the algorithm goal
is achieved. The algorithm merges document pairs
in the resulting clusters by merging clusters in a
greedy, bottom-up fashion. A divide-and-conquer
strategy can be used to balance the cluster quality
and computational efficiency.
The basic steps of group-average clustering
algorithm are like this: On each iteration, it first
divides the current pool of clusters into evenly
sized buckets. Group-average clustering is then
applied to each bucket locally, merging smaller
clusters into larger ones. The time complexity for
the algorithm is O(kn), where k is bucket size and
n is the number of documents.
Single link clustering (Dunlop, 2000), on the
other hand, is based on creating a hierarchical tree
by continually inserting an additional node that
satisfies the following criteria:
- The new node is currently outside the hierar-
chy
- Of all similarities between nodes inside and
outside the hierarchy, the new node which has
the strongest similarity is selected. It is then
added to the hierarchy at a level based on how
strong the similarity is.
The approach is fairly fast and result in hier-
archies where the closest nearest neighbours are at
lower levels of the hierarchy. However, it leads to
non-balanced clusters, and many node-node com-
parisons can have the same strength of similarity
thus many documents can be linked at the same
level in the hierarchy.
The accuracy of the above conventional clus-
tering method, however, is generally low. There-
fore a new approach is proposed.
</bodyText>
<figureCaption confidence="0.998566">
Figure 1: A sample document in NewsML format containing keywords and subject labels
</figureCaption>
<sectionHeader confidence="0.993112" genericHeader="method">
3 Our Proposed Method
</sectionHeader>
<bodyText confidence="0.987853">
In our proposal, we suggest the following ap-
proaches for generating clusters.
</bodyText>
<listItem confidence="0.99826675">
1. Use Bi-gram to extract terms from
documents
2. Use &lt;KeywordLine&gt; Tag to look up
keyword terms for documents
3. Compare terms betweens documents to
construct similarity matrix
4. Use &lt;SubjectCode&gt; Tag to group docu-
ments to different subjects
5. Adjust similarity matrix by data pro-
vided by step 4
6. Apply group-average clustering algo-
rithm to generate clusters
</listItem>
<bodyText confidence="0.999943657534247">
We have chosen to use Bi-gram algorithm to
extract terms from documents. The idea of bi-
gram Algorithm is similar to N-grams’. The rea-
son of using bi-gram instead of N-gram is that our
experiment mainly deals with Chinese (Big5)
news. Since Chinese terms are typically formed
by two Chinese characters, bi-gram approach is
sufficient for this application. Using bi-gram,
moreover, would be a more effective approach
when handling other two-byte code like Japanese
and Korean languages (Lee, Cho and Park, 1999).
Using bi-grams instead of N-grams can re-
duce system resources. Since hundreds of docu-
ments are handled each time, using N-gram would
be impractical. For instance, using N-grams ap-
proach for a document with M Chinese characters
would extract (M-1) + (M-2)+...+[M- (N-1)]
terms. This would require more time for compari-
son than only M-1 terms for bi-grams.
We suggest using NewsML
(http://www.newsml.org) in our project. NewsML,
which is released by International Press and Tele-
communications Council (IPTC) (www.iptc.org ),
is an XML-based data format for news that is in-
tended to use for the creation, transfer and deliv-
ery of news. All news is created based on
NewsML data type definition (DTD) file.
NewsML has been widely recognized globally.
Noticeable users include Reuters, AFP, and Kyo-
donews.
A sample document in NewsML format is
shown in Figure 1. By using NewsML, a helpful
tag called &lt;KeywordLine&gt; can be used in order to
look up existing keywords from a document. The
keywords, which could highly reflect the main
concept of the document, would usually be given
by the author of the document and stored in the
&lt;KeywordLine&gt; tag under NewsML. Thus, the
&lt;KeywordLine&gt; Tag is a useful indicator for
keyword extraction.
We used NewsML tagged keywords for
comparison. A score is computed for each key-
word based on term frequency to reflect the level
of importance to the document. A given keyword,
however, may exist in more than two Chinese
characters. But in order to make easy and accurate
comparison, we extracted the keywords in two
Chinese characters each and compare them with
the terms extracted by the bi-gram algorithm.
These keywords would be more representative
than those extracted by bi-gram. Thus, they would
be very useful for identifying the news content.
We therefore applied higher weighting to these
terms. In view of this, we assign the weighting of
NewsML tagged keywords ten times more than
those extracted using bi-grams.
Besides using only the keywords and terms
from a news document, subject of the news is also
crucial. For example, news referring to entertain-
ment should not be in the same cluster with one
talking about business news. Understanding news
semantically to a certain extent can help improve
clustering accuracy. &lt;SubjectCode&gt; Tag in
NewsML enabled the clustering algorithm to dis-
tinguish which subject a particular news is refer-
ring to. Although the &lt;SubjectCode&gt; only gives a
rough concept of the news, it can significantly
improve accuracy.
In our experiment, we ensured that news with
different &lt;SubjectCode&gt; would not be put in the
same cluster. The similarity measure of two pieces
of news with different &lt;SubjectCode&gt; would,
therefore, be zero.
</bodyText>
<sectionHeader confidence="0.997868" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.982744333333333">
We assessed the effectiveness of using meta-
data for document clustering empirically. We used
NewsML meta-data and Hong Kong Chinese
news articles for this purpose.
Performance metrics are based on Recall and
Precision, which are defined as follows:
In event Not in event
In cluster A B
Not in cluster C D
</bodyText>
<equation confidence="0.957679">
Recall = A / (A+C) if A+C &gt; 0
Precision = A / (A+B) if A+B &gt; 0
</equation>
<bodyText confidence="0.991503625">
To study the effect of NewsML, we calcu-
lated the percentage of recall and precision under
different threshold values. The threshold value (0-
1) is a user-defined variable for clustering. The
threshold is the value where documents within a
cluster should have a similarity greater than it, and
similarity between two documents is calculated by
Jaccard’s coefficient:
</bodyText>
<equation confidence="0.988943">
wa
wa + wb + wc
</equation>
<bodyText confidence="0.985057416666667">
where
a is the set of terms present in both docu-
ments
b is the set of terms present in document 1
but absent in document 2
c is the set of terms present in document 2
but absent in document 1
wa , wb , wc are the sum of weights of the
related set.
A higher threshold represent higher similarity
is needed for documents to be put in the same
cluster.
Prior to the experiment, we matched each
piece of news with specific Subject Codes defined
by IPTC (http://www.iptc.org/site/subject-
codes/index.html), as well as adding keywords
with respect to their contents. After the prepara-
tion process, we started our experiment and find
out the recall and precision measures as above
stated
The main difference between the algorithms
with and without NewsML is on the use of the
KeywordLine and SubjectCode tags. The evalua-
tion steps are as follows:
</bodyText>
<listItem confidence="0.9984379">
1. Non-overlapping clusters are generated by
our clustering system
2. Each generated cluster is matched to the most
similar sample event using a one-to-one
matching method.
3. If the number of clusters generated is less
than the number of events in the data set, dis-
card the excessive sample events after match-
ing.
4. If the number of clusters generated is greater
than the number of events in the data set,
empty events are added to the sample and
they are matched with the generated clusters.
5. Calculate recall and precision for each cluster
pairs
6. Use the calculated recall and precision, to
obtain the macro-average (Yang, Pierce and
Carbonell, 1998).
7. Repeat the above steps on different threshold
values.
</listItem>
<figure confidence="0.998744875">
100.00%
40.00%
20.00%
90.00%
80.00%
70.00%
60.00%
50.00%
30.00%
10.00%
0.00%
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Clustering Evaluation Result
Recall
Without
NewsML
Precision
With
NewsML
Precision
Without
NewsML
Recall With
NewsML
</figure>
<figureCaption confidence="0.994993">
Figure 2: Clustering evaluation result for 158 news articles from three different publications
</figureCaption>
<subsectionHeader confidence="0.9813055">
4.1 Experiment A: News Articles from Mul- 4.2
tiple Publications
</subsectionHeader>
<bodyText confidence="0.989220732142857">
We began our evaluation with a set of data
from three different publications. We used 158
news articles containing 31 distinct events (which
was equivalent to 31 non-overlapping clusters) as
our training set. The events belonged to different
sector including local news, international news,
sports news, entertainment news and business
news. Based on the IPTC Subject Reference Sys-
tem (www.iptc.org), these news articles were clas-
sified into 7 different subjects.
The result is shown in Figure 2 where x-axis
represents the threshold and y-axis represents the
percentage of the measure.
The graph suggests that both recall and preci-
sion values decrease with increasing threshold.
The performance of using NewsML is better when
the threshold is smaller. The greatest difference
occurs when threshold value = 0.2
Moreover, we observe that the variation of
the performance is greater when using NewsML.
The precision and recall can drop about 40% by
changing the threshold. In the contrast, without
using NewsML, the result is more steady.
On average, recall and precision NewsML-
based algorithm gives 16.15% and 12.09% respec-
tively improvement of its non-NewsML counter-
part.
Experiment B: News Articles from the
Same Publication
We then carried out another experiment us-
ing 503 news articles from one single publication.
We tried to classify these news articles manually
to events of related news, and then carried out the
same approach as the first experiment. These 503
news articles contained 126 distinct events and
classified to 12 different subjects with reference to
the IPTC Subject Reference System.
We find out how the system performs when
dealing with one single publication. As we predict,
news from only one single publication often uses
same keywords or terms to express the same con-
cept. This simplified clustering.
The result is shown in Figure 3 where x-axis
represents the threshold and y-axis represents the
percentage of the measure.
The performance of the system decrease
when the threshold value increases, which is con-
sistent with the first experiment. From the graph,
we observe that the performance is best when the
threshold is between 0.2 and 0.4, which means
those values could best match human expectation.
The graph also suggests that both recall and
precision gives a better result towards using meta-
data. On average 8.02% (recall) and 8.92% (preci-
sion) improvement over the approach without us-
ing NewsML meta-data are achieved.
</bodyText>
<figure confidence="0.99912292">
100.00%
90.00%
70.00%
40.00%
20.00%
80.00%
60.00%
50.00%
30.00%
10.00%
0.00%
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold value
Clustering Evaluation Result
Recall With
NewsML
Recall
Without
NewsML
Precision
With
NewsML
Precision
Without
NewsML
</figure>
<figureCaption confidence="0.999435">
Figure 3: Clustering evaluation result for 503 news articles from the same publication
</figureCaption>
<bodyText confidence="0.9997214">
From the results of the two experiments, we
have found that using NewsML could improve
both the recall and precision of the document clus-
tering algorithm by about 10%, over those without
using NewsML.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999808">
In this paper, we demonstrate that the effec-
tiveness of document clustering algorithm could
be improved by utilizing meta-data in additional
to the original data content. In our experiment, we
chose NewsML as a representation of news con-
tent with added meta-data. We proposed to use the
&lt;KeywordLine&gt; tags and &lt;SubjectCode&gt; tags in
NewsML for clustering.
Our proposed document clustering algorithm
is a refinement of conventional group-average
clustering algorithm and bi-gram algorithm. It has
been shown that NewsML could help conven-
tional clustering methods improve both the recall
and precision by about 10% on average.
In order to demonstrate the practicality of
NewsML for e-business, we have deployed
NewsML in developing an application called
NewsFocus1. News Focus consists of a clustering
function. The function is mainly for clustering
similar news from three different news sources in
Hong Kong. News articles under NewsFocus are
clustered by news events.
In the future, we try to use different methods
to further improve the clustering performance.
Inverted document frequency and cosine-angle
formula, for example, have been widely used in
terms score calculation and matrix similarity cal-
culation. Top relevance terms can be used as key-
words in case of any insufficiency of metadata.
We will also try to use other tags in NewsML like
&lt;Headline&gt; in supplement with &lt;KeywordLine&gt;
and &lt;SubjectCode&gt; to give more metadata infor-
mation for the document. Weighting parameters
may also be applied to show the degree of empha-
sis on using those metadata.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994066217391305">
M. D. Dunlop, Development and evaluation of cluste-
ing techniques for finding people, Proceedings of the
Third International Conference Basel, Volume 34,
2000
V. Estivill-Castro and J. Yang, Non-crisp Clustering by
Fast, Convergent, and Robust Algorithms, Principles
of Data Mining and Knowledge Discovery, Volume
2168, 2001, pp. 103-114
U. Grothkopf, H. Andernach, S. Stevens-Rayburn, and
M. Gomez, Comparison of Two “Document Similarity
Search Engines”, Library and Information Services in
Astronomy III, ASP Conference Series, Volume 153,
1998, pp. 85-92
E. L. Johnson and H. Kargupta, Collective, Hierarchi-
cal Clustering from Distributed, Heterogeneous Data,
Large-Scale Parallel Data Mining, Lecture Notes in
Artificial Intelligence, Volume 1759, 2000, pp. 221-
244
Lam, W., Meng, H., Wong, K.L. and Yen, J., Using
Contextual Analysis for News Event Detection,
International Journal of Intelligent Systems, 16(4),
2001, pp.525-546
1 Please visit http://www.cnewsml.org/clustering/jsp/index2.html for demonstration
J. H. Lee, H. Y. Cho and H. R. Park, N-gram-based
indexing for Korean text retrieval, Information Proc-
essing and Management, Volume 35 Number 4, 1999,
pp. 427-441
A. Tombros, R. Villa, C. J. Van Rijsbergen, The effec-
tiveness of query-specific hierarchic clustering in in-
formation retrieval, Information processing &amp;
management, Volume 38, 2002, pp. 559-582
M. Volk and S. Clematide: Learn-Filter-Apply-Forget.
Mixed Approaches to Named Entity Recognition. Pro-
ceedings of 6th International Workshop on Applica-
tions of Natural Language for Information Systems.
GI-Edition. Lecture Notes in Informatics. vol. 3. Ma-
drid: 2001.
K.L. Wong, W. Lam, J. Yen, Interactive Chinese News
Event Detection and Tracking, Proceedings of The
Second Asian Digital Library Conference, 1999,
pp.30-43.
Y. Yang, T. Pierce, J. Carbonell, A Study on Retrospec-
tive and On-Line Event Detection, Proceedings of
SIGIR-98, 21st ACM International Conference on Re-
search and Development in Information Retrieval,
1998, pp.28-36.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.039126">
<title confidence="0.999362">Improving Document Clustering by Utilizing Meta-Data*</title>
<author confidence="0.898968">Kam-Fai</author>
<affiliation confidence="0.87287125">Department of Engineering Engineering The Chinese University</affiliation>
<author confidence="0.755764">Hong Kong</author>
<email confidence="0.6707935">kfwong@se.cuhk.edu.hkNam-Kiu</email>
<affiliation confidence="0.762181666666667">Centre for Innovation and Technology, The Chinese University of</affiliation>
<author confidence="0.702164">Hong Kong</author>
<email confidence="0.963078">jussie@cintec.cuhk.edu.hk</email>
<author confidence="0.996769">Kam-Lai Wong</author>
<affiliation confidence="0.984306333333333">Centre for Innovation and Technology, The Chinese University</affiliation>
<author confidence="0.733945">Hong Kong</author>
<email confidence="0.994702">klwong@cintec.cuhk.edu.hk</email>
<abstract confidence="0.997396066666667">In this paper, we examine how to improve the precision and recall of document clustering by utilizing meta-data. We use meta-data through NewsML tags to assist clustering and show that this approach is effective through experiments on sample news data. Experimental result shows that clustering using NewsML could improve average recall and precision over the same without using NewsML by about 10%. Our algorithm facilitates effective e-business for the news media and publishing industry to empower e-business.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M D Dunlop</author>
</authors>
<title>Development and evaluation of clusteing techniques for finding people,</title>
<date>2000</date>
<booktitle>Proceedings of the Third International Conference Basel,</booktitle>
<volume>34</volume>
<contexts>
<context position="6751" citStr="Dunlop, 2000" startWordPosition="1007" endWordPosition="1008">erges document pairs in the resulting clusters by merging clusters in a greedy, bottom-up fashion. A divide-and-conquer strategy can be used to balance the cluster quality and computational efficiency. The basic steps of group-average clustering algorithm are like this: On each iteration, it first divides the current pool of clusters into evenly sized buckets. Group-average clustering is then applied to each bucket locally, merging smaller clusters into larger ones. The time complexity for the algorithm is O(kn), where k is bucket size and n is the number of documents. Single link clustering (Dunlop, 2000), on the other hand, is based on creating a hierarchical tree by continually inserting an additional node that satisfies the following criteria: - The new node is currently outside the hierarchy - Of all similarities between nodes inside and outside the hierarchy, the new node which has the strongest similarity is selected. It is then added to the hierarchy at a level based on how strong the similarity is. The approach is fairly fast and result in hierarchies where the closest nearest neighbours are at lower levels of the hierarchy. However, it leads to non-balanced clusters, and many node-nod</context>
</contexts>
<marker>Dunlop, 2000</marker>
<rawString>M. D. Dunlop, Development and evaluation of clusteing techniques for finding people, Proceedings of the Third International Conference Basel, Volume 34, 2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Estivill-Castro</author>
<author>J Yang</author>
</authors>
<title>Non-crisp Clustering by Fast, Convergent, and Robust Algorithms,</title>
<date>2001</date>
<journal>Principles of Data Mining and Knowledge Discovery, Volume</journal>
<volume>2168</volume>
<pages>103--114</pages>
<contexts>
<context position="5601" citStr="Estivill-Castro and Yang, 2001" startWordPosition="836" endWordPosition="839">ach assumes no prior linguistic knowledge about the text being processed. Moreover, there is no languagespecific information used in the N-grams approach, which qualifies this method as a languageindependent approach. By using N-grams, frequently appeared terms of each document can be extracted and compared to make the similarity measure. 2.2 Clustering Algorithm Probabilistic method is one of the commonly used methods in document clustering. The aim of probabilistic method is to minimize the heterogeneity in each group with respect to the group representative based on statistical approaches (Estivill-Castro and Yang, 2001). Neural network is also used to perform a cyclic learning process for clustering (Grothkopf, Andernach and Stevens-Rayburn, 1998). Hierarchical clustering methods include group-average clustering algorithm and singlelink clustering algorithm (Johnson and Kargupta, 2000; Tombros, Villa and Van Rijsbergen, 2002). Group average clustering is based on creating a hierarchical tree by initially creating a singleton cluster for each document. The clusters are merged to the parent node until the algorithm goal is achieved. The algorithm merges document pairs in the resulting clusters by merging clust</context>
</contexts>
<marker>Estivill-Castro, Yang, 2001</marker>
<rawString>V. Estivill-Castro and J. Yang, Non-crisp Clustering by Fast, Convergent, and Robust Algorithms, Principles of Data Mining and Knowledge Discovery, Volume 2168, 2001, pp. 103-114</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Grothkopf</author>
<author>H Andernach</author>
<author>S Stevens-Rayburn</author>
<author>M Gomez</author>
</authors>
<title>Comparison of Two “Document Similarity Search Engines”,</title>
<date>1998</date>
<booktitle>Library and Information Services in Astronomy III, ASP Conference Series, Volume</booktitle>
<volume>153</volume>
<pages>85--92</pages>
<marker>Grothkopf, Andernach, Stevens-Rayburn, Gomez, 1998</marker>
<rawString>U. Grothkopf, H. Andernach, S. Stevens-Rayburn, and M. Gomez, Comparison of Two “Document Similarity Search Engines”, Library and Information Services in Astronomy III, ASP Conference Series, Volume 153, 1998, pp. 85-92</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Johnson</author>
<author>H Kargupta</author>
</authors>
<title>Collective, Hierarchical Clustering from Distributed, Heterogeneous Data, Large-Scale Parallel Data Mining,</title>
<date>2000</date>
<journal>Lecture Notes in Artificial Intelligence,</journal>
<volume>1759</volume>
<pages>221--244</pages>
<contexts>
<context position="5871" citStr="Johnson and Kargupta, 2000" startWordPosition="870" endWordPosition="873">ent can be extracted and compared to make the similarity measure. 2.2 Clustering Algorithm Probabilistic method is one of the commonly used methods in document clustering. The aim of probabilistic method is to minimize the heterogeneity in each group with respect to the group representative based on statistical approaches (Estivill-Castro and Yang, 2001). Neural network is also used to perform a cyclic learning process for clustering (Grothkopf, Andernach and Stevens-Rayburn, 1998). Hierarchical clustering methods include group-average clustering algorithm and singlelink clustering algorithm (Johnson and Kargupta, 2000; Tombros, Villa and Van Rijsbergen, 2002). Group average clustering is based on creating a hierarchical tree by initially creating a singleton cluster for each document. The clusters are merged to the parent node until the algorithm goal is achieved. The algorithm merges document pairs in the resulting clusters by merging clusters in a greedy, bottom-up fashion. A divide-and-conquer strategy can be used to balance the cluster quality and computational efficiency. The basic steps of group-average clustering algorithm are like this: On each iteration, it first divides the current pool of cluste</context>
</contexts>
<marker>Johnson, Kargupta, 2000</marker>
<rawString>E. L. Johnson and H. Kargupta, Collective, Hierarchical Clustering from Distributed, Heterogeneous Data, Large-Scale Parallel Data Mining, Lecture Notes in Artificial Intelligence, Volume 1759, 2000, pp. 221-244</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lam</author>
<author>H Meng</author>
<author>K L Wong</author>
<author>J Yen</author>
</authors>
<title>Using Contextual Analysis for News Event Detection,</title>
<date>2001</date>
<journal>International Journal of Intelligent Systems,</journal>
<volume>16</volume>
<issue>4</issue>
<pages>525--546</pages>
<marker>Lam, Meng, Wong, Yen, 2001</marker>
<rawString>Lam, W., Meng, H., Wong, K.L. and Yen, J., Using Contextual Analysis for News Event Detection, International Journal of Intelligent Systems, 16(4), 2001, pp.525-546</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lee</author>
<author>H Y Cho</author>
<author>H R Park</author>
</authors>
<title>1 Please visit http://www.cnewsml.org/clustering/jsp/index2.html for demonstration</title>
<date>1999</date>
<booktitle>Information Processing and Management, Volume</booktitle>
<volume>35</volume>
<pages>427--441</pages>
<marker>Lee, Cho, Park, 1999</marker>
<rawString>1 Please visit http://www.cnewsml.org/clustering/jsp/index2.html for demonstration J. H. Lee, H. Y. Cho and H. R. Park, N-gram-based indexing for Korean text retrieval, Information Processing and Management, Volume 35 Number 4, 1999, pp. 427-441</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Tombros</author>
<author>R Villa</author>
<author>C J Van</author>
</authors>
<title>Rijsbergen, The effectiveness of query-specific hierarchic clustering in information retrieval,</title>
<date>2002</date>
<journal>Information processing &amp; management, Volume</journal>
<booktitle>Proceedings of 6th International Workshop on Applications of Natural Language for Information Systems. GI-Edition. Lecture Notes in Informatics.</booktitle>
<volume>38</volume>
<pages>559--582</pages>
<location>Madrid:</location>
<marker>Tombros, Villa, Van, 2002</marker>
<rawString>A. Tombros, R. Villa, C. J. Van Rijsbergen, The effectiveness of query-specific hierarchic clustering in information retrieval, Information processing &amp; management, Volume 38, 2002, pp. 559-582 M. Volk and S. Clematide: Learn-Filter-Apply-Forget. Mixed Approaches to Named Entity Recognition. Proceedings of 6th International Workshop on Applications of Natural Language for Information Systems. GI-Edition. Lecture Notes in Informatics. vol. 3. Madrid: 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Wong</author>
<author>W Lam</author>
<author>J Yen</author>
</authors>
<title>Interactive Chinese News Event Detection and Tracking,</title>
<date>1999</date>
<booktitle>Proceedings of The Second Asian Digital Library Conference,</booktitle>
<pages>30--43</pages>
<marker>Wong, Lam, Yen, 1999</marker>
<rawString>K.L. Wong, W. Lam, J. Yen, Interactive Chinese News Event Detection and Tracking, Proceedings of The Second Asian Digital Library Conference, 1999, pp.30-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>T Pierce</author>
<author>J Carbonell</author>
</authors>
<title>A Study on Retrospective and On-Line Event Detection,</title>
<date>1998</date>
<booktitle>Proceedings of SIGIR-98, 21st ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>28--36</pages>
<marker>Yang, Pierce, Carbonell, 1998</marker>
<rawString>Y. Yang, T. Pierce, J. Carbonell, A Study on Retrospective and On-Line Event Detection, Proceedings of SIGIR-98, 21st ACM International Conference on Research and Development in Information Retrieval, 1998, pp.28-36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>