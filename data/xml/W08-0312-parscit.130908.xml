<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003252">
<title confidence="0.957394333333333">
METEOR, M-BLEU and M-TER: Evaluation Metrics for
High-Correlation with Human Rankings of Machine Translation
Output
</title>
<author confidence="0.981563">
Abhaya Agarwal and Alon Lavie
</author>
<affiliation confidence="0.9821555">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.570646">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.999648">
{abhayaa,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.995673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998508">
This paper describes our submissions to the
machine translation evaluation shared task in
ACL WMT-08. Our primary submission is the
METEOR metric tuned for optimizing correla-
tion with human rankings of translation hy-
potheses. We show significant improvement
in correlation as compared to the earlier ver-
sion of metric which was tuned to optimized
correlation with traditional adequacy and flu-
ency judgments. We also describe M-BLEU and
M-TER, enhanced versions of two other widely
used metrics BLEU and TER respectively, which
extend the exact word matching used in these
metrics with the flexible matching based on
stemming and Wordnet in METEOR.
</bodyText>
<sectionHeader confidence="0.998489" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999782288888889">
Automatic Metrics for MT evaluation have been re-
ceiving significant attention in recent years. Evalu-
ating an MT system using such automatic metrics is
much faster, easier and cheaper compared to human
evaluations, which require trained bilingual evalua-
tors. The most commonly used MT evaluation met-
ric in recent years has been IBM’s BLEU metric (Pa-
pineni et al., 2002). BLEU is fast and easy to run,
and it can be used as a target function in parameter
optimization training procedures that are commonly
used in state-of-the-art statistical MT systems (Och,
2003). Various researchers have noted, however, var-
ious weaknesses in the metric. Most notably, BLEU
does not produce very reliable sentence-level scores.
METEOR , as well as several other proposed metrics
such as GTM (Melamed et al., 2003), TER (Snover
et al., 2006) and CDER (Leusch et al., 2006) aim to
address some of these weaknesses.
METEOR , initially proposed and released in 2004
(Lavie et al., 2004) was explicitly designed to im-
prove correlation with human judgments of MT qual-
ity at the segment level. Previous publications on
METEOR (Lavie et al., 2004; Banerjee and Lavie,
2005; Lavie and Agarwal, 2007) have described the
details underlying the metric and have extensively
compared its performance with BLEU and several
other MT evaluation metrics. In (Lavie and Agar-
wal, 2007), we described the process of tuning free
parameters within the metric to optimize the corre-
lation with human judgments and the extension of
the metric for evaluating translations in languages
other than English.
This paper provides a brief technical description of
METEOR and describes our experiments in re-tuning
the metric for improving correlation with the human
rankings of translation hypotheses corresponding to
a single source sentence. Our experiments show sig-
nificant improvement in correlation as a result of re-
tuning which shows the importance of having a met-
ric tunable to different testing conditions. Also, in
order to establish the usefulness of the flexible match-
ing based on stemming and Wordnet, we extend two
other widely used metrics BLEU and TER which use
exact word matching, with the matcher module of
METEOR.
</bodyText>
<sectionHeader confidence="0.981698" genericHeader="method">
2 The METEOR Metric
</sectionHeader>
<bodyText confidence="0.998802">
METEOR evaluates a translation by computing a
score based on explicit word-to-word matches be-
tween the translation and a given reference trans-
lation. If more than one reference translation is
available, the translation is scored against each refer-
ence independently, and the best scoring pair is used.
Given a pair of strings to be compared, METEOR cre-
ates a word alignment between the two strings. An
alignment is mapping between words, such that ev-
ery word in each string maps to at most one word
in the other string. This alignment is incrementally
produced by a sequence of word-mapping modules.
The “exact” module maps two words if they are ex-
actly the same. The “porter stem” module maps two
words if they are the same after they are stemmed us-
</bodyText>
<page confidence="0.99018">
115
</page>
<note confidence="0.401418">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 115–118,
</note>
<page confidence="0.479329">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999162888888889">
ing the Porter stemmer. The “WN synonymy” mod-
ule maps two words if they are considered synonyms,
based on the fact that they both belong to the same
“synset&amp;quot; in WordNet.
The word-mapping modules initially identify all
possible word matches between the pair of strings.
We then identify the largest subset of these word
mappings such that the resulting set constitutes an
alignment as defined above. If more than one maxi-
mal cardinality alignment is found, METEOR selects
the alignment for which the word order in the two
strings is most similar (the mapping that has the
least number of “crossing” unigram mappings). The
order in which the modules are run reflects word-
matching preferences. The default ordering is to
first apply the “exact” mapping module, followed by
“porter stemming” and then “WN synonymy”.
Once a final alignment has been produced between
the system translation and the reference translation,
the METEOR score for this pairing is computed as
follows. Based on the number of mapped unigrams
found between the two strings (m), the total num-
ber of unigrams in the translation (t) and the total
number of unigrams in the reference (r), we calcu-
late unigram precision P = m/t and unigram recall
R = m/r. We then compute a parametrized har-
monic mean of P and R (van Rijsbergen, 1979):
</bodyText>
<equation confidence="0.996494">
P · R
Fmean = a · P + (1 − a) · R
</equation>
<bodyText confidence="0.999205538461538">
Precision, recall and Fmean are based on single-
word matches. To take into account the extent to
which the matched unigrams in the two strings are
in the same word order, METEOR computes a penalty
for a given alignment as follows. First, the sequence
of matched unigrams between the two strings is di-
vided into the fewest possible number of “chunks”
such that the matched unigrams in each chunk are
adjacent (in both strings) and in identical word or-
der. The number of chunks (ch) and the number of
matches (m) is then used to calculate a fragmenta-
tion fraction: frag = ch/m. The penalty is then
computed as:
</bodyText>
<equation confidence="0.937584">
Pen = y · frage
</equation>
<bodyText confidence="0.9718475">
The value of y determines the maximum penalty
(0 &lt; y &lt; 1). The value of 0 determines the
functional relation between fragmentation and the
penalty. Finally, the METEOR score for the align-
ment between the two strings is calculated as:
score = (1 − Pen) · Fmean
The free parameters in the metric, a, 0 and y are
tuned to achieve maximum correlation with the hu-
man judgments as described in (Lavie and Agarwal,
2007).
</bodyText>
<sectionHeader confidence="0.786141" genericHeader="method">
3 Extending BLEU and TER with
</sectionHeader>
<subsectionHeader confidence="0.642708">
Flexible Matching
</subsectionHeader>
<bodyText confidence="0.999740590909091">
Many widely used metrics like BLEU (Papineni et al.,
2002) and TER (Snover et al., 2006) are based on
measuring string level similarity between the refer-
ence translation and translation hypothesis, just like
METEOR . Most of them, however, depend on find-
ing exact matches between the words in two strings.
Many researchers (Banerjee and Lavie, 2005; Liu and
Gildea, 2006), have observed consistent gains by us-
ing more flexible matching criteria. In the following
experiments, we extend the BLEU and TER metrics
to use the stemming and Wordnet based word map-
ping modules from METEOR.
Given a translation hypothesis and reference pair,
we first align them using the word mapping modules
from METEOR. We then rewrite the reference trans-
lation by replacing the matched words with the cor-
responding words in the translation hypothesis. We
now compute BLEU and TER with these new refer-
ences without changing anything inside the metrics.
To get meaningful BLEU scores at segment level,
we compute smoothed BLEU as described in (Lin and
Och, 2004).
</bodyText>
<sectionHeader confidence="0.996241" genericHeader="method">
4 Re-tuning METEOR for Rankings
</sectionHeader>
<bodyText confidence="0.999765666666667">
(Callison-Burch et al., 2007) reported that the inter-
coder agreement on the task of assigning ranks to
a given set of candidate hypotheses is much better
than the intercoder agreement on the task of assign-
ing a score to a hypothesis in isolation. Based on
that finding, in WMT-08, only ranking judgments
are being collected from the human judges.
The current version of METEOR uses parameters
optimized towards maximizing the Pearson&apos;s corre-
lation with human judgments of adequacy scores. It
is not clear that the same parameters would be op-
timal for correlation with human rankings. So we
would like to re-tune the parameters in the metric
for maximizing the correlation with ranking judg-
ments instead. This requires computing full rankings
according to the metric and the humans and then
computing a suitable correlation measure on those
rankings.
</bodyText>
<subsectionHeader confidence="0.982494">
4.1 Computing Full Rankings
</subsectionHeader>
<bodyText confidence="0.9477185">
METEOR assigns a score between 0 and 1 to every
translation hypothesis. This score can be converted
</bodyText>
<page confidence="0.989072">
116
</page>
<table confidence="0.9981848">
Language Judgments
Binary Sentences
English 3978 365
German 2971 334
French 1903 208
Spanish 2588 284
English German French Spanish
α 0.95 0.9 0.9 0.9
0 0.5 3 0.5 0.5
y 0.45 0.15 0.55 0.55
</table>
<tableCaption confidence="0.762921">
Table 2: Optimal Values of Tuned Parameters for Various
Languages
Table 1: Corpus Statistics for Various Languages
</tableCaption>
<bodyText confidence="0.9987315">
to rankings trivially by assuming that a higher score
indicates a better hypothesis.
In development data, human rankings are avail-
able as binary judgments indicating the preferred hy-
pothesis between a given pair. There are also cases
where both the hypotheses in the pair are judged to
be equal. In order to convert these binary judgments
into full rankings, we do the following:
</bodyText>
<listItem confidence="0.993547222222222">
1. Throw out all the equal judgments.
2. Construct a directed graph where nodes corre-
spond to the translation hypotheses and every
binary judgment is represented by a directed
edge between the corresponding nodes.
3. Do a topological sort on the resulting graph and
assign ranks in the sort order. The cycles in the
graph are broken by assigning same rank to all
the nodes in the cycle.
</listItem>
<subsectionHeader confidence="0.990379">
4.2 Measuring Correlation
</subsectionHeader>
<bodyText confidence="0.999949428571429">
Following (Ye et al., 2007), we first compute the
Spearman correlation between the human rankings
and METEOR rankings of the translation hypotheses
corresponding to a single source sentence. Let N be
the number of translation hypotheses and D be the
difference in ranks assigned to a hypothesis by two
rankings, then Spearman correlation is given by:
</bodyText>
<equation confidence="0.997609">
6ED2
r = 1 − N(N2 − 1)
</equation>
<bodyText confidence="0.9995275">
The final score for the metric is the average of the
Spearman correlations for individual sentences.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.88211">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.998296444444444">
We use the human judgment data from WMT-07
which was released as development data for the eval-
uation shared task. Amount of data available for
various languages is shown in Table 1. Development
data contains the majority judgments (not every hy-
potheses pair was judged by same number of judges)
which means that in the cases where multiple judges
judged the same pair of hypotheses, the judgment
given by majority of the judges was considered.
</bodyText>
<table confidence="0.9970134">
Original Re-tuned
English 0.3813 0.4020
German 0.2166 0.2838
French 0.2992 0.3640
Spanish 0.2021 0.2186
</table>
<tableCaption confidence="0.996978">
Table 3: Average Spearman Correlation with Human
Rankings for METEOR on Development Data
</tableCaption>
<subsectionHeader confidence="0.988918">
5.2 Methodology
</subsectionHeader>
<bodyText confidence="0.999993857142857">
We do an exhaustive grid search in the feasible ranges
of parameter values, looking for parameters that
maximize the average Spearman correlation over the
training data. To get a fair estimate of performance,
we use 3-fold cross validation on the development
data. Final parameter values are chosen as the best
performing set on the data pooled from all the folds.
</bodyText>
<subsectionHeader confidence="0.6060345">
5.3 Results
5.3.1 Re-tuning METEOR for Rankings
</subsectionHeader>
<bodyText confidence="0.999971916666667">
The re-tuned parameter values are shown in Ta-
ble 2 while the average Spearman correlations for
various languages with original and re-tuned param-
eters are shown in Table 3. We get significant im-
provements for all the languages. Gains are specially
pronounced for German and French.
Interestingly, weight for recall becomes even higher
than earlier parameters where it was already high.
So it seems that ranking judgments are almost en-
tirely driven by the recall in all the languages. Also
the re-tuned parameters for all the languages except
German are quite similar.
</bodyText>
<subsubsectionHeader confidence="0.76133">
5.3.2 M-BLEU and M-TER
</subsubsectionHeader>
<bodyText confidence="0.999798166666667">
Table 4 shows the average Spearman correlations
of M-BLEU and M-TER with human rankings. For
English, both M-BLEU and M-TER show considerable
improvements. For other languages, improvements
in M-TER are smaller but consistent. M-BLEU , how-
ever, doesn’t shows any improvements in this case.
A possible reason for this behavior can be the lack of
a “WN synonymy” module for languages other than
English which results in fewer extra matches over the
exact matching baseline. Additionally, French, Ger-
man and Spanish have a richer morphology as com-
pared to English. The morphemes in these languages
</bodyText>
<page confidence="0.993353">
117
</page>
<table confidence="0.999907888888889">
Exact Match Flexible Match
English: BLEU 0.2486 0.2747
TER 0.1598 0.2033
French: BLEU 0.2906 0.2889
TER 0.2472 0.2604
German: BLEU 0.1829 0.1806
TER 0.1509 0.1668
Spanish: BLEU 0.1804 0.1847
TER 0.1787 0.1839
</table>
<tableCaption confidence="0.9189295">
Table 4: Average Spearman Correlation with Human
Rankings for M-BLEU and M-TER
</tableCaption>
<bodyText confidence="0.9998916">
carry much more information and different forms of
the same word may not be as freely replaceable as in
English. A more fine grained strategy for matching
words in these languages remains an area of further
investigation.
</bodyText>
<sectionHeader confidence="0.999308" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999974866666667">
In this paper, we described the re-tuning of ME-
TEOR parameters to better correlate with human
rankings of translation hypotheses. Results on the
development data indicate that the re-tuned ver-
sion is significantly better at predicting ranking than
the earlier version. We also presented enhanced
BLEU and TER that use the flexible word match-
ing module from Meteor and show that this re-
sults in better correlations as compared to the de-
fault exact matching versions. The new version of
METEOR will be soon available on our website at:
http://www.cs.cmu.edu/~alavie/METEOR/ . This
release will also include the flexible word matcher
module which can be used to extend any metric with
the flexible matching.
</bodyText>
<sectionHeader confidence="0.996928" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.965845">
The work reported in this paper was supported by
NSF Grant IIS-0534932.
</bodyText>
<sectionHeader confidence="0.996629" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998758457142857">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65–72, Ann Arbor,
Michigan, June.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136–158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Levels
of Correlation with Human Judgments. In Proceedings
of the Second ACL Workshop on Statistical Machine
Translation, pages 228–231, Prague, Czech Republic,
June.
Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman.
2004. The Significance of Recall in Automatic Metrics
for MT Evaluation. In Proceedings of the 6th Confer-
ence of the Association for Machine Translation in the
Americas (AMTA-2004), pages 134–143, Washington,
DC, September.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of the Thirteenth Conference of
the European Chapter of the Association for Compu-
tational Linguistics.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics
for machine translation. In COLING &apos;04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Ding Liu and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 539–546, Morristown, NJ, USA.
Association for Computational Linguistics.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003.
Precision and Recall of Machine Translation. In Pro-
ceedings of the HLT-NAACL 2003 Conference: Short
Papers, pages 61–63, Edmonton, Alberta.
Franz Josef Och. 2003. Minimum Error Rate Training
for Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311–318, Philadelphia, PA,
July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), pages 223–231, Cambridge, MA, Au-
gust.
C. van Rijsbergen, 1979. Information Retrieval. Butter-
worths, London, UK, 2nd edition.
Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sen-
tence level machine translation evaluation as a rank-
ing. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 240–247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
</reference>
<page confidence="0.996164">
118
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.363187">
<title confidence="0.892889666666667">M-BLEU Evaluation Metrics High-Correlation with Human Rankings of Machine Translation Output</title>
<author confidence="0.647969">Agarwal</author>
<affiliation confidence="0.996486">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.99966">Pittsburgh, PA, 15213,</address>
<abstract confidence="0.999880133333333">This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08. Our primary submission is the tuned for optimizing correlation with human rankings of translation hypotheses. We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and flujudgments. We also describe versions of two other widely metrics which extend the exact word matching used in these metrics with the flexible matching based on</abstract>
<intro confidence="0.629767">and Wordnet in</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2081" citStr="Banerjee and Lavie, 2005" startWordPosition="320" endWordPosition="323">art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English. This paper provides a brief technical description of METEOR and describes our experiments in re-tuning the metric for improving correlation with the human rankings of translation hypothe</context>
<context position="6827" citStr="Banerjee and Lavie, 2005" startWordPosition="1129" endWordPosition="1132">ment between the two strings is calculated as: score = (1 − Pen) · Fmean The free parameters in the metric, a, 0 and y are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007). 3 Extending BLEU and TER with Flexible Matching Many widely used metrics like BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like METEOR . Most of them, however, depend on finding exact matches between the words in two strings. Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria. In the following experiments, we extend the BLEU and TER metrics to use the stemming and Wordnet based word mapping modules from METEOR. Given a translation hypothesis and reference pair, we first align them using the word mapping modules from METEOR. We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute BLEU and TER with these new references without changing anything inside the metrics. To get meaningful</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7581" citStr="Callison-Burch et al., 2007" startWordPosition="1252" endWordPosition="1255">, we extend the BLEU and TER metrics to use the stemming and Wordnet based word mapping modules from METEOR. Given a translation hypothesis and reference pair, we first align them using the word mapping modules from METEOR. We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute BLEU and TER with these new references without changing anything inside the metrics. To get meaningful BLEU scores at segment level, we compute smoothed BLEU as described in (Lin and Och, 2004). 4 Re-tuning METEOR for Rankings (Callison-Burch et al., 2007) reported that the intercoder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation. Based on that finding, in WMT-08, only ranking judgments are being collected from the human judges. The current version of METEOR uses parameters optimized towards maximizing the Pearson&apos;s correlation with human judgments of adequacy scores. It is not clear that the same parameters would be optimal for correlation with human rankings. So we would like to re-tune the parameters in</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. (meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 136–158, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>228--231</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2107" citStr="Lavie and Agarwal, 2007" startWordPosition="324" endWordPosition="327"> (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English. This paper provides a brief technical description of METEOR and describes our experiments in re-tuning the metric for improving correlation with the human rankings of translation hypotheses corresponding to a sin</context>
<context position="6429" citStr="Lavie and Agarwal, 2007" startWordPosition="1064" endWordPosition="1067">th strings) and in identical word order. The number of chunks (ch) and the number of matches (m) is then used to calculate a fragmentation fraction: frag = ch/m. The penalty is then computed as: Pen = y · frage The value of y determines the maximum penalty (0 &lt; y &lt; 1). The value of 0 determines the functional relation between fragmentation and the penalty. Finally, the METEOR score for the alignment between the two strings is calculated as: score = (1 − Pen) · Fmean The free parameters in the metric, a, 0 and y are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007). 3 Extending BLEU and TER with Flexible Matching Many widely used metrics like BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like METEOR . Most of them, however, depend on finding exact matches between the words in two strings. Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria. In the following experiments, we extend the BLEU and TER metrics to use the stemming and Wordnet based w</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In Proceedings of the Second ACL Workshop on Statistical Machine Translation, pages 228–231, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Kenji Sagae</author>
<author>Shyamsundar Jayaraman</author>
</authors>
<title>The Significance of Recall in Automatic Metrics for MT Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004),</booktitle>
<pages>134--143</pages>
<location>Washington, DC,</location>
<contexts>
<context position="1899" citStr="Lavie et al., 2004" startWordPosition="290" endWordPosition="293">pineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with human judgments and the extension of the metric for evaluating translations in languages other than English. This</context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. 2004. The Significance of Recall in Automatic Metrics for MT Evaluation. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004), pages 134–143, Washington, DC, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDER: Efficient MT Evaluation Using Block Movements.</title>
<date>2006</date>
<booktitle>In Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1788" citStr="Leusch et al., 2006" startWordPosition="271" endWordPosition="274">bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to optimize the correlation with hu</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006. CDER: Efficient MT Evaluation Using Block Movements. In Proceedings of the Thirteenth Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In COLING &apos;04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>501</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="7518" citStr="Lin and Och, 2004" startWordPosition="1243" endWordPosition="1246">xible matching criteria. In the following experiments, we extend the BLEU and TER metrics to use the stemming and Wordnet based word mapping modules from METEOR. Given a translation hypothesis and reference pair, we first align them using the word mapping modules from METEOR. We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute BLEU and TER with these new references without changing anything inside the metrics. To get meaningful BLEU scores at segment level, we compute smoothed BLEU as described in (Lin and Och, 2004). 4 Re-tuning METEOR for Rankings (Callison-Burch et al., 2007) reported that the intercoder agreement on the task of assigning ranks to a given set of candidate hypotheses is much better than the intercoder agreement on the task of assigning a score to a hypothesis in isolation. Based on that finding, in WMT-08, only ranking judgments are being collected from the human judges. The current version of METEOR uses parameters optimized towards maximizing the Pearson&apos;s correlation with human judgments of adequacy scores. It is not clear that the same parameters would be optimal for correlation wit</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In COLING &apos;04: Proceedings of the 20th international conference on Computational Linguistics, page 501, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic iterative alignment for machine translation evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>539--546</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6850" citStr="Liu and Gildea, 2006" startWordPosition="1133" endWordPosition="1136">gs is calculated as: score = (1 − Pen) · Fmean The free parameters in the metric, a, 0 and y are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007). 3 Extending BLEU and TER with Flexible Matching Many widely used metrics like BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like METEOR . Most of them, however, depend on finding exact matches between the words in two strings. Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria. In the following experiments, we extend the BLEU and TER metrics to use the stemming and Wordnet based word mapping modules from METEOR. Given a translation hypothesis and reference pair, we first align them using the word mapping modules from METEOR. We then rewrite the reference translation by replacing the matched words with the corresponding words in the translation hypothesis. We now compute BLEU and TER with these new references without changing anything inside the metrics. To get meaningful BLEU scores at segment</context>
</contexts>
<marker>Liu, Gildea, 2006</marker>
<rawString>Ding Liu and Daniel Gildea. 2006. Stochastic iterative alignment for machine translation evaluation. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 539–546, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Conference: Short Papers,</booktitle>
<pages>61--63</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="1730" citStr="Melamed et al., 2003" startWordPosition="260" endWordPosition="263">eaper compared to human evaluations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parame</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the HLT-NAACL 2003 Conference: Short Papers, pages 61–63, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1495" citStr="Och, 2003" startWordPosition="225" endWordPosition="226">stemming and Wordnet in METEOR. 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years. Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Ag</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1300" citStr="Papineni et al., 2002" startWordPosition="190" endWordPosition="194">e also describe M-BLEU and M-TER, enhanced versions of two other widely used metrics BLEU and TER respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in METEOR. 1 Introduction Automatic Metrics for MT evaluation have been receiving significant attention in recent years. Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) </context>
<context position="6537" citStr="Papineni et al., 2002" startWordPosition="1082" endWordPosition="1085">to calculate a fragmentation fraction: frag = ch/m. The penalty is then computed as: Pen = y · frage The value of y determines the maximum penalty (0 &lt; y &lt; 1). The value of 0 determines the functional relation between fragmentation and the penalty. Finally, the METEOR score for the alignment between the two strings is calculated as: score = (1 − Pen) · Fmean The free parameters in the metric, a, 0 and y are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007). 3 Extending BLEU and TER with Flexible Matching Many widely used metrics like BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like METEOR . Most of them, however, depend on finding exact matches between the words in two strings. Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria. In the following experiments, we extend the BLEU and TER metrics to use the stemming and Wordnet based word mapping modules from METEOR. Given a translation hypothesis and reference pair, we first align them usin</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-2006),</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="1757" citStr="Snover et al., 2006" startWordPosition="265" endWordPosition="268">uations, which require trained bilingual evaluators. The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002). BLEU is fast and easy to run, and it can be used as a target function in parameter optimization training procedures that are commonly used in state-of-the-art statistical MT systems (Och, 2003). Various researchers have noted, however, various weaknesses in the metric. Most notably, BLEU does not produce very reliable sentence-level scores. METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses. METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level. Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005; Lavie and Agarwal, 2007) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics. In (Lavie and Agarwal, 2007), we described the process of tuning free parameters within the metric to o</context>
<context position="6567" citStr="Snover et al., 2006" startWordPosition="1088" endWordPosition="1091">ction: frag = ch/m. The penalty is then computed as: Pen = y · frage The value of y determines the maximum penalty (0 &lt; y &lt; 1). The value of 0 determines the functional relation between fragmentation and the penalty. Finally, the METEOR score for the alignment between the two strings is calculated as: score = (1 − Pen) · Fmean The free parameters in the metric, a, 0 and y are tuned to achieve maximum correlation with the human judgments as described in (Lavie and Agarwal, 2007). 3 Extending BLEU and TER with Flexible Matching Many widely used metrics like BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like METEOR . Most of them, however, depend on finding exact matches between the words in two strings. Many researchers (Banerjee and Lavie, 2005; Liu and Gildea, 2006), have observed consistent gains by using more flexible matching criteria. In the following experiments, we extend the BLEU and TER metrics to use the stemming and Wordnet based word mapping modules from METEOR. Given a translation hypothesis and reference pair, we first align them using the word mapping modules fro</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA-2006), pages 223–231, Cambridge, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London, UK,</location>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. van Rijsbergen, 1979. Information Retrieval. Butterworths, London, UK, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Ye</author>
<author>Ming Zhou</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Sentence level machine translation evaluation as a ranking.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>240--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9658" citStr="Ye et al., 2007" startWordPosition="1599" endWordPosition="1602">en pair. There are also cases where both the hypotheses in the pair are judged to be equal. In order to convert these binary judgments into full rankings, we do the following: 1. Throw out all the equal judgments. 2. Construct a directed graph where nodes correspond to the translation hypotheses and every binary judgment is represented by a directed edge between the corresponding nodes. 3. Do a topological sort on the resulting graph and assign ranks in the sort order. The cycles in the graph are broken by assigning same rank to all the nodes in the cycle. 4.2 Measuring Correlation Following (Ye et al., 2007), we first compute the Spearman correlation between the human rankings and METEOR rankings of the translation hypotheses corresponding to a single source sentence. Let N be the number of translation hypotheses and D be the difference in ranks assigned to a hypothesis by two rankings, then Spearman correlation is given by: 6ED2 r = 1 − N(N2 − 1) The final score for the metric is the average of the Spearman correlations for individual sentences. 5 Experiments 5.1 Data We use the human judgment data from WMT-07 which was released as development data for the evaluation shared task. Amount of data </context>
</contexts>
<marker>Ye, Zhou, Lin, 2007</marker>
<rawString>Yang Ye, Ming Zhou, and Chin-Yew Lin. 2007. Sentence level machine translation evaluation as a ranking. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 240–247, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>