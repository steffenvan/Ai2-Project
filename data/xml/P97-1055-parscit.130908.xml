<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002738">
<title confidence="0.9941875">
Paradigmatic Cascades: a Linguistically Sound Model of
Pronunciation by Analogy
</title>
<author confidence="0.931587">
Francois Yvon
</author>
<affiliation confidence="0.682086">
ENST and CNRS, URA 820
Computer Science Department
</affiliation>
<address confidence="0.958995">
46 rue Barrault - F 75 013 Paris
</address>
<email confidence="0.948719">
yvonOinf.enst.fr
</email>
<sectionHeader confidence="0.993921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999915">
We present and experimentally evaluate a
new model of pronunciation by analogy:
the paradigmatic cascades model. Given a
pronunciation lexicon, this algorithm first
extracts the most productive paradigmatic
mappings in the graphemic domain, and
pairs them statistically with their cone-
late(s) in the phonemic domain. These
mappings are used to search and retrieve
in the lexical database the most promising
analog of unseen words. We finally apply
to the analogs pronunciation the correlated
series of mappings in the phonemic domain
to get the desired pronunciation.
</bodyText>
<sectionHeader confidence="0.984588" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999960870967742">
Psychological models of reading aloud traditionally
assume the existence of two separate routes for con-
verting print to sound: a direct lexical route, which
is used to read familiar words, and a dual route rely-
ing upon abstract letter-to-sound rules to pronounce
previously unseen words (Coltheart, 1978; Coltheart
et al., 1993). This view has been challenged by
a number of authors (e.g. (Glushsko, 1981)), who
claim that the pronunciation process of every word,
familiar or unknown, could be accounted for in a
unified framework. These single-route models cru-
cially suggest that the pronunciation of unknown
words results from the parallel activation of similar
lexical items (the lexical neighbours). This idea has
been tentatively implemented both into various sym-
bolic analogy-based algorithms (e.g. (Dedina and
Nusbaum, 1991; Sullivan and Damper, 1992)) and
into connectionist pronunciation devices (e.g. (Sei-
denberg and McClelland, 1989)).
The basic idea of these analogy-based models is
to pronounce an unknown word x by recombin-
ing pronunciations of lexical items sharing common
subparts with x. To illustrate this strategy, Ded-
ina and Nussbaum show how the pronunciation of
the sequence lop in the pseudo-word blope is analo-
gized with the pronunciation of the same sequence
in sloping. As there exists more than one way to re-
combine segments of lexical items, Dedina and Nuss-
baum&apos;s algorithm favors recombinations including
large substrings of existing words. In this model,
the similarity between two words is thus implicitely
defined as a function of the length of their common
subparts: the longer the common part, the better
the analogy.
This conception of analogical processes has an im-
portant consequence: it offers, as Damper and East-
mond ((Damper and Eastmond, 1996)) state it, &amp;quot;no
principled way of deciding the orthographic neigh-
bours of a novel word which are deemed to influ-
ence its pronunciation (...)&amp;quot; . For example, in the
model proposed by Dedina and Nusbaum, any word
having a common orthographic substring with the
unknown word is likely to contribute to its pronun-
ciation, which increases the number of lexical neigh-
bours far beyond acceptable limits (in the case of
blope, this neighbourhood would contain every En-
glish word starting in bl, or ending in ope, etc).
From a computational standpoint, implement-
ing the recombination strategy requires a one-to-
one alignment between the lexical graphemic and
phonemic representations, where each grapheme is
matched with the corresponding phoneme (a null
symbol is used to account for the cases where the
lengths of these representations differ). This align-
ment makes it possible to retrieve, for any graphemic
substring of a given lexical item, the corresponding
phonemic string, at the cost however of an unmoti-
vated complexification of lexical representations.
In comparison, the paradigmatic cascades model
(PCP for short) promotes an alternative view of
analogical processes, which relies upon a linguisti-
cally motivated similarity measure between words.
</bodyText>
<page confidence="0.997991">
428
</page>
<bodyText confidence="0.999974842105263">
The basic idea of our model is to take advantage
of the internal structure of &amp;quot;natural&amp;quot; lexicons. In
fact, a lexicon is a very complex object, whose ele-
ments are intimately tied together by a number of
fine-grained relationships (typically induced by mor-
phological processes), and whose content is severely
restricted, on a language-dependant basis, by a com-
plex of graphotactic, phonotactic and morphotac-
tic constraints. Following e.g. (Pirrelli and Fed-
erici, 1994), we assume that these constraints sur-
face simultaneously in the orthographical and in
the phonological domain in the recurring pattern
of paradigmatically alterning pairs of lexical items.
Extending the idea originally proposed in (Federici,
Pirrelli, and Yvon, 1995), we show that it is possible
to extract these alternation patterns, to associate
alternations in one domain with the related alterna-
tion in the other domain, and to construct, using this
pairing, a fairly reliable pronunciation procedure.
</bodyText>
<sectionHeader confidence="0.893326" genericHeader="introduction">
2 The Paradigmatic Cascades Model
</sectionHeader>
<bodyText confidence="0.999953428571429">
In this section, we introduce the paradigmatic cas-
cades model. We first formalize the concept of a
paradigmatic relationship. We then go through the
details of the learning procedure, which essentially
consists in an extensive search for such relationships.
We finally explain how these patterns are used in the
pronunciation procedure.
</bodyText>
<subsectionHeader confidence="0.975851">
2.1 Paradigmatic Relationships and
Alternations
</subsectionHeader>
<bodyText confidence="0.999927666666667">
The paradigmatic cascades model crucially relies
upon the existence of numerous paradigmatic rela-
tionships in lexical databases. A paradigmatic re-
lationship involves four lexical entries a, b, c, d, and
expresses that these forms are involved in an ana-
logical (in the Saussurian (de Saussure, 1916) sense)
proportion: a is to b as c is to d (further along ab-
breviated as a:b=c:d, see also (Lepage and
Shin-Ichi, 1996) for another utilization of this kind
of proportions). Morphologically related pairs pro-
vide us with numerous examples of orthographical
proportions, as in:
</bodyText>
<equation confidence="0.996002">
reactor : reaction = factor : faction (1)
</equation>
<bodyText confidence="0.9993316">
Considering these proportions in terms of ortho-
graphical alternations, that is in terms of partial
functions in the graphemic domain, we can see that
each proportion involves two alternations. The first
one transforms reactor into reaction (and factor
into faction), and consists in exchanging the suffixes
or and ion. The second one transforms reactor into
factor (and reaction into faction), and consists in
exchanging the prefixes re and f. These alternations
are represented on figure 1.
</bodyText>
<figure confidence="0.819058333333333">
reaction
9
faction
</figure>
<figureCaption confidence="0.999328">
Figure 1: An Analogical Proportion
</figureCaption>
<bodyText confidence="0.999607857142857">
Formally, we define the notion of a paradigmatic
relationship as follows. Given E, a finite alphabet,
and ,C, a finite subset of E*, we say that (a, b) Erxr
is paradigmatically related to (c, d) E x if there
exits two partial functions f and g from E* to E*,
where f exchanges prefixes and g exchanges suffixes,
and:
</bodyText>
<equation confidence="0.999811">
1(a) = c and f (b) = d (2)
g (a) = b and g (c) = d (3)
</equation>
<bodyText confidence="0.997335">
f and g are termed the paradigmatic alternations
associated with the relationship a : b =f,g C : d.
The domain of an alternation f will be denoted by
dom(f).
</bodyText>
<subsectionHeader confidence="0.99909">
2.2 The Learning Procedure
</subsectionHeader>
<bodyText confidence="0.999936038461538">
The main purpose of the learning procedure is to
extract from a pronunciation lexicon, presumably
structured by multiple paradigmatic relationships,
the most productive paradigmatic alternations.
Let us start with some notations: Given G a
graphemic alphabet and P a phonetic alphabet, a
pronunciation lexicon ,C is a subset of G* x P. The
restriction of ,C on G* (respectively .135) will be noted
LG (resp. Lp). Given two strings x and y, pre f (x,
(resp. su f f (x , y)) denotes their longest common pre-
fix (resp. suffix). For two strings x and y having a
non-empty common prefix (resp. suffix) u, f, (resp,
g4) denotes the function which transforms x into y:
as x = uv, and as y = ut, ft&apos;y substitutes a final v
with a final t. A denotes the empty string.
Given ,C, the learning procedure searches LG for
any for every 4-uples (a, b, c, d) of graphemic strings
such that a : b =f,g c : d. Each match increments
the productivity of the related alternations f and
g. This search is performed using using a slightly
modified version of the algorithm presented in (Fed-
erici, Pirrelli, and Yvon, 1995), which applies to ev-
ery word x in LG the procedure detailled in table 1.
In fact, the properties of paradigmatic relation-
ships, notably their symetry, allow to reduce dra-
matically the cost of this procedure, since not all
</bodyText>
<equation confidence="0.580824333333333">
reactor
9
factor
</equation>
<page confidence="0.861737">
429
</page>
<figure confidence="0.939834444444445">
GET ALTERNATIONS (X)
1 D(x) 4- { y E .CGAt = pre f(x,y)) A}
2 for y E D(x)
3 do
4 P(x,y) {(z,t) E LG X LG/z = f;y(t)}
5 if P(x,y) 0
6 then
7 IncrementCount
8 IncrementCount(fft)
</figure>
<tableCaption confidence="0.948147">
Table 1: The Learning Procedure
</tableCaption>
<bodyText confidence="0.978691333333333">
4-uple of strings in LG need to be examined during
that stage.
For each graphemic alternation, we also record
their correlated alternation(s) in the phonological
domain, and accordingly increment their productiv-
ity. For instance, assuming that factor and reactor
respectively receive the pronunciations /fwktar/ and
/ri:aektar/, the discovery of the relationship ex-
pressed in (1) will lead our algorithm to record that
the graphemic alternation f -4 re correlates in the
phonemic domain with the alternation /f/ -4 /ri:/.
Note that the discovery of phonemic correlates does
not require any sort of alignment between the or-
thographic and the phonemic representations: the
procedure simply records the changes in the phone-
mic domain when the alternation applies in the
graphemic domain.
At the end of the learning stage, we have in hand
a set A = {Ai} of functions exchanging suffixes or
prefixes in the graphemic domain, and for each Ai
in A:
(i) a statistical measure pi of its productivity, de-
fined as the likelihood that the transform of a
lexical item be another lexical item:
</bodyText>
<equation confidence="0.767677333333333">
Ilx E dom(Ai) and A(x) E L}
vi =I dorn(Ai) I
(4)
</equation>
<bodyText confidence="0.990402076923077">
(ii) a set {Bi,j}, j E {1 of correlated func-
tions in the phonemic domain, and a statistical
measure Pi,j of their conditional productivity,
i.e. of the likelihood that the phonetic alterna-
tion B1, correlates with A.
Table 2 gives the list of the phonological correlates
of the alternation which consists in adding the suffix
/y, corresponding to a productive rule for deriving
adverbs from adjectives in English. If the first lines
of table 2 are indeed &amp;quot;true&amp;quot; phonemic correlates of
the derivation, corresponding to various classes of
adjectives, a careful examination of the last lines re-
veals that the extraction procedure is easily fooled
</bodyText>
<table confidence="0.999298384615384">
alternation Example
x -4 x-Ili:I good
x-It1 -4 x-/adli:/ marked
x- loll -4 x- I oli:I equal
-4 x-/li:/ capable
x -4 x-/i:/ cool
x-/in/ -4 x-/enli:/ clean
x-11(11 x-/aidli:/ id
x-/Iv/ -4 x-/aili:/ live
x-I01 -- x-/ali:/ loath
S —4 s-/ail imp
x-/Ir/ -4 x-/3:1i:/ ear
-4 x-/onli:/ on
</table>
<tableCaption confidence="0.999415">
Table 2: Phonemic correlates of x x — ly
</tableCaption>
<bodyText confidence="0.999875736842105">
by accidental pairs like imp-imply, on-only or ear-
early. A simple pruning rule was used to get rid of
these alternations on the basis of their productivity,
and only alternations which were observed at least
twice were retained.
It is important to realize that A allows to specifiy
lexical neighbourhoods in LG: given a lexical entry
x, its nearest neighbour is simply f(x), where f is
the most productive alternation applying to x. Lex-
ical neighbourhoods in the paradigmatic cascades
model are thus defined with respect to the locally
most productive alternations. As a consequence,
the definition of neighbourhoods implicitely incorpo-
rates a great deal of linguistic knowledge extracted
from the lexicon, especially regarding morphological
processes and phonotactic constraints, which makes
it much for relevant for grounding the notion of anal-
ogy between lexical items than, say, any neighbour-
hood based on the string edition metric.
</bodyText>
<subsectionHeader confidence="0.922332">
2.3 The Pronunciation of Unknown Words
</subsectionHeader>
<bodyText confidence="0.999933375">
Supose now that we wish to infer the pronunciation
of a word x, which does not appear in the lexicon.
This goal is achieved by exploring the neighbour-
hood of x defined by A, in order to find one or several
analogous lexical entry(ies) y. The second stage of
the pronunciation procedure is to adapt the known
pronunciation of y, and derive a suitable pronuncia-
tion for x: the idea here is to mirror in the phonemic
domain the series of alternations which transform x
into y in the graphemic domain, using the statistical
pairing between alternations that is extracted dur-
ing the learning stage. The complete pronunciation
procedure is represented on figure 2.
Let us examine carefully how these two aspects of
the pronunciation procedure are implemented. The
first stage is to find a lexical entry in the neighbour-
</bodyText>
<page confidence="0.988341">
430
</page>
<figure confidence="0.865979">
Graphemic domain Phonemic domain
</figure>
<figureCaption confidence="0.999863">
Figure 2: The pronunciation of an unknown word
</figureCaption>
<bodyText confidence="0.999694914285714">
hood of x defined by ,C.
The basic idea is to generate A (x), defined
as {Ai (x), forAi E A, x E dornain(Ai)}, which con-
tains all the words that can be derived from x us-
ing a function in A. This set, better viewed as a
stack, is ordered according to the productivity of the
Ai: the topmost element in the stack is the nearest
neighbour of x, etc. The first lexical item found in
A (x) is the analog of x. If A (x) does not contain
any known word, we iterate the procedure, using
x&apos;, the top-ranked element of A (x), instead of x.
This expands the set of possible analogs, which is
accordingly reordered, etc. This basic search strat-
egy, which amounts to the exploration of a deriva-
tion tree, is extremely ressource consuming (every
expension stage typically adds about a hundred of
new virtual analogs), and is, in theory, not guar-
anted to terminate. In fact, the search problem is
equivalent to the problem of parsing with an unre-
stricted Phrase Structure Grammar, which is known
to be undecidable.
We have evaluated two different search strategies,
which implement various ways to alternate between
expansion stages (the stack is expanded by gener-
ating the derivatives of the topmost element) and
matching stages (elements in the stack are looked
for in the lexicon). The first strategy implements a
depth-first search of the analog set: each time the
topmost element of the stack is searched, but not
found, in the lexicon, its derivatives are immediately
generated, and added to the stack. In this approach,
the position of an analog in the stack is assessed as a
function of the &amp;quot;distance&amp;quot; between the original word
x and the analog y = Ai, (Ai„_, (... Ai, (x))), accord-
ing to:
</bodyText>
<equation confidence="0.9664095">
I.k
d(x,y) = (5)
</equation>
<page confidence="0.648668">
1.1
</page>
<bodyText confidence="0.9999094">
The search procedure is stopped as soon an ana-
log is found in LG, or else, when the distance be-
tween x and the topmost element of the stack, which
monotonously decreases (Vi, pi &lt; I), falls below a
pre-defined theshold.
The second strategy implements a kind of com-
promise between depth-first and breadth-first explo-
ration of the derivation tree, and is best understood
if we first look at a concrete example. Most alter-
nations substituting one initial consonant are very
productive, in English like in many other languages.
Therefore, a word starting with say, a p, is very likely
to have a very close derivative where the initial p
has been replaced by say, a r. Now suppose that
this word starts with pl: the alternation will de-
rive an analog starting with rl, and will assess it
with a very high score. This analog will, in turn,
derive many more virtual analogs starting with rl,
once its suffixes will have been substituted during
another expansion phase. This should be avoided,
since there are in fact very few words starting with
the prefix rl: we would therefore like these words to
be very poorly ranked. The second search strategy
has been devised precisely to cope with this problem.
The idea is to rank the stack of analogs according
to the expectation of the number of lexical deriva-
tives a given analog may have. This expectation is
computed by summing up the productivities of all
the alternations that can be applied to an analog y
according to:
</bodyText>
<equation confidence="0.7434335">
E Pi (6)
i/yEdompio
</equation>
<bodyText confidence="0.999942636363636">
This ranking will necessarily assess any analog start-
ing in rl with a low score, as very few alternations
will substitute its prefix. However, the computation
of (6) is much more complex than (5), since it re-
quires to examine a given derivative before it can be
positioned in the stack. This led us to bring for-
ward the lexical matching stage: during the expan-
sion of the topmost stack element, all its derivatives
are looked for in the lexicon. If several derivatives
are simultaneously found, the search procedure halts
and returns more than one analog.
The expectation (6) does not decrease as more
derivatives are added to the stack; consequently,
it cannot be used to define a stopping criterion.
The search procedure is therefore stopped when
all derivatives up to a given depth (2 in our ex-
periments) have been generated, and unsuccessfully
looked for in the lexicon. This termination criterion
is very restrictive, in comparison to the one imple-
mented in the depth-first strategy, since it makes it
impossible to pronounce very long derivatives, for
which a significant number of alternations need to
</bodyText>
<page confidence="0.995688">
431
</page>
<bodyText confidence="0.995386209302325">
be applied before an analog is found. An example is
the word synergistically, for which the &amp;quot;breadth-
first&amp;quot; search terminates uncessfully, whereas the
depth-first search manages to retrieve the &amp;quot;analog&amp;quot;
energy. Nonetheless, the results reported hereafter
have been obtained using this &amp;quot;breadth-first&amp;quot; strat-
egy, mainly because this search was associated with
a more efficient procedure for reconstructing pronun-
ciations (see below).
Various pruning procedures have also been imple-
mented in order to control the exponential growth of
the stack. For example, one pruning procedure de-
tects the most obvious derivation cycles, which gen-
erate in loops the same derivatives; another prun-
ing procedure tries to detect commutating alterna-
tions: substituting the prefix p, and then the suffix
s often produces the same analog than when alter-
nations apply in the reverse order, etc. More de-
tails regarding implementational aspects are given
in (Yvon, 1996b).
If the search procedure returns an analog y
Ai, k_l(... A1 (x))) in E, we can build a pronun-
ciation for x, using the known pronunciation 0(y)
of y. For this purpose, we will use our knowledge
of the Bi,j, for i E {i1 ...ik}, and generate ev-
ery possible transforms of 0(y) in the phonological
domain: 1/3713. k(B71 (... (q5(y))))}, with jk in
11 ... nik 1, and order this set using some function of
the pi,j. The top-ranked element in this set is the
pronunciation of x. Of course, when the search fails,
this procedure fails to propose any pronunciation.
In fact, the results reported hereafter use a slightly
extended version of this procedure, where the pro-
nunciations of more than one analog are used for
generating and selecting the pronunciation of the un-
known word. The reason for using multiple analogs
is twofold: first, it obviates the risk of being wrongly
influenced by one very exceptional analog; second,
it enables us to model conspiracy effects more accu-
rately. Psychological models of reading aloud indeed
assume that the pronunciation of an unknown word
is not influenced by just one analog, but rather by
its entire lexical neighbourhood.
</bodyText>
<sectionHeader confidence="0.997189" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.958293">
3.1 Experimental Design
</subsectionHeader>
<bodyText confidence="0.999942263157895">
We have evaluated this algorithm on two different
pronunciation tasks. The first experiment consists
in infering the pronunciation of the 70 pseudo-words
originally used in Glushko&apos;s experiments, which have
been used as a test-bed for various other pronun-
ciation algorithms, and allow for a fair head-to-
head comparison between the paradigmatic cascades
model and other analogy-based procedures. For
this experiment, we have used the entire nettalk
(Sejnowski and Rosenberg, 1987) database (about
20 000 words) as the learning set.
The second series of experiments is intended to
provide a more realistic evaluation of our model in
the task of pronouncing unknown words. We have
used the following experimental design: 10 pairs of
disjoint (learning set, test set) are randomly selected
from the nettalk database and evaluated. In each
experiment, the test set contains about the tenth
of the available data. A transcription is judged to
be correct when it matches exactly the pronuncia-
tion listed in the database at the segmental level.
The number of correct phonemes in a transcription
is computed on the basis of the string-to-string edit
distance with the target pronunciation. For each
experiment, we measure the percentage of phoneme
and words that are correctly predicted (referred to
as correctness), and two additional figures, which are
usually not significant in context of the evaluation
of transcription systems. Recall that our algorithm,
unlike many other pronunciation algorithms, is likely
to remain silent. In order to take this aspect into ac-
count, we measure in each experiment the number
of words that can not be pronounced at all (the si-
lence), and the percentage of phonemes and words
that are correctly transcribed amongst those words
that have been pronounced at all (the precision). The
average values for these measures are reported here-
after.
</bodyText>
<subsectionHeader confidence="0.998775">
3.2 Pseudo-words
</subsectionHeader>
<bodyText confidence="0.992773571428571">
All but one pseudo-words of Glushko&apos;s test set could
be pronounced by the paradigmatic cascades algo-
rithm, and amongst the 69 pronunciation suggested
by our program, only 9 were uncorrect (that is, were
not proposed by human subjects in Glushko&apos;s ex-
periments), yielding an overall correctness of 85.7%,
and a precision of 87.3%.
An important property of our algortihm is that it
allows to precisely identify, for each pseudo-word,
the lexical entries that have been analogized, i.e.
whose pronunciation was used in the inferential pro-
cess. Looking at these analogs, it appears that three
of our errors are grounded on very sensible analo-
gies, and provide us with pronunciations that seem
at least plausible, even if they were not suggested in
Glushko&apos;s experiments. These were pild and bild,
analogized with wild, and pomb, analogized with
tomb.
These results compare favorably well with the per-
formances reported for other pronunciation by anal-
ogy algorithms ((Damper and Eastmond, 1996) re-
</bodyText>
<page confidence="0.997376">
432
</page>
<bodyText confidence="0.9981252">
ports very similat correctness figures), especially if
one remembers that our results have been obtained,
without resorting to any kind of pre-alignment be-
tween the graphemic and phonemic strings in the
lexicons.
</bodyText>
<subsectionHeader confidence="0.999776">
3.3 Lexical Entries
</subsectionHeader>
<bodyText confidence="0.998962864864865">
This second series of experiment is intended to
provide us with more realistic evaluations of the
paradigmatic cascade model: Glushko&apos;s pseudo-
words have been built by substitutin.g the initial
consonant of existing monosyllabic words, and con-
stitute therefore an over-simplistic test-bed. The
nettalk dataset contains plurisyllabic words, com-
plex derivatives, loan words, etc, and allows to test
the ability of our model to learn complex morpho-
phonological phenomenas, notably vocalic alterna-
tions and other kinds of phonologically conditioned
root allomorphy, that are very difficult to learn.
With this new test set, the overall performances
of our algorithm averages at about 54.5% of en-
tirely correct words, corresponding to a 76% per
phoneme correctness. If we keep the words that
could not be pronounced at all (about 15% of the
test set) apart from the evaluation, the per word and
per phoneme precision improve considerably, reach-
ing respectively 65% and 93%. Again, these pre-
cision results compare relatively well with the re-
sults achieved on the same corpus using other self-
learning algorithms for grapheme-to-phoneme tran-
scription (e.g. (van den Bosch and Da.elemans, 1993;
Yvon, 1996a)), which, unlike ours, benefit from
the knowledge of the alignment between graphemic
and phonemic strings. Table 3 summaries the per-
formance (in terms of per word correctness, si-
lence, and precision) of various other pronunciation
systems, namely PRONOUNCE (Dedina and Nus-
baum, 1991), DEC (TorkoIla, 1993), SMPA (Yvon,
I 996a). All these models have been tested using ex-
actly the same evaluation procedure and data (see
(Yvon, 1996b), which also contains an evolution per-
Ibrmed with a. French database suggesting that this
learning strategy effectively applies to other lan-
guages).
</bodyText>
<table confidence="0.9948976">
Systei n corr. prec. silence
DEC 56.67 56.67 0
S M PA 63.96 64.24 0.42
PRONOUNCE 56.56 56.75 0.32
PCP 54.49 63.95 14.80
</table>
<tableCaption confidence="0.590113">
&apos;fable 3: A Comparative Evaluation
</tableCaption>
<bodyText confidence="0.961178689655173">
&apos;Fable 3 pinpoints the main weakness of our model,
that is, its significant silence rate. The careful ex-
amination of the words that cannot be pronounced
reveals that they are either loan words, which are
very isolated in an English lexicon, and for which
no analog can be found; or complex morphological
derivatives for which the search procedure is stopped
before the existing analog(s) can be reached. Typical
examples are: synergistically, timpani, hangdog,
oasis, pemmican, to list just a few. This suggests
that the words which were not pronounced are not
randomly distributed. Instead, they mostly belong
to a linguistically homogeneous group, the group of
foreign words, which, for lack of better evidence,
should better be left silent, or processed by another
pronunciation procedure (for example a rule-based
system (Coker, Church, and Liberman, 1990)), than
uncorrectly analogized.
Some complementary results finally need to be
mentioned here, in relation to the size of lexical
neighbourhoods. In fact, one of our main goal was
to define in a sensible way the concept of a lexical
neighbourhood: it is therefore important to check
that our model manages to keep this neighbourhood
relatively small. Indeed, if this neighbourhood can
be quite large (typically 50 analogs) for short words,
the number of analogs used in a pronunciation aver-
ages at about 9.5, which proves that our definition
of a lexical neighbourhood is sufficiently restrictive.
</bodyText>
<sectionHeader confidence="0.978118" genericHeader="discussions">
4 Discussion and Perspectives
</sectionHeader>
<subsectionHeader confidence="0.986345">
4.1 Related works
</subsectionHeader>
<bodyText confidence="0.997887272727272">
A large number of procedures aiming at the auto-
matic discovery of pronunciation &amp;quot;rules&amp;quot; have been
proposed over the past few years: connectionist
models (e.g. (Sejnowski and Rosenberg, 1987)), tra-
ditional symbolic machine learning techniques (in-
duction of decision trees, k-nearest neighbours) e.g.
(Torkolla, 1993; van den Bosch and Daelemans,
1993), as well as various recombination techniques
(Yvon, 1996a). In these models, orthographical cor-
respondances are primarily viewed as resulting from
a. strict underlying phonographical system, where
each grapheme encodes exactly one phoneme. This
assumption is reflected by the possibility of align-
ing on a one-to-one basis graphemic and phonemic
strings, and these models indeed use this kind of
alignment to initiate learning. Under this view, the
orthographical representation of individual words is
strongly subject, to their phonological forms on an
word per word basis. The main task of a machine-
learning algorithm is thus mainly to retrieve, on
a. statistical basis, these grapheme-phoneme corre-
spondances, which are, in languages like French or
</bodyText>
<page confidence="0.998153">
433
</page>
<bodyText confidence="0.9999883125">
English, accidentally obscured by a multitude of ex-
ceptional and idiosyncratic correspondances. There
exists undoubtly strong historical evidences support-
ing the view that the orthographical system of most
european languages developped from a such phono-
graphical system, and languages like Spanish or Ital-
ian still offer examples of that kind of very regular
organization.
Our model, which extends the proposals of (Coker,
Church, and Liberman, 1990), and more recently,
of (Federici, Pirrelli, and Yvon, 1995), entertains a
different view of orthographical systems. Even we
if acknowledge the mostly phonographical organiza-
tion of say, French orthography, we believe that the
multiple deviations from a strict grapheme-phoneme
correspondance are best captured in a model which
weakens somehow the assumption of a strong de-
pendancy between orthographical and phonological
representations. In our model, each domain has its
own organization, which is represented in the form
of systematic (paradigmatic) set of oppositions and
alternations. In both domain however, this orga-
nization is subject to the same paradigmatic prin-
ciple, which makes it possible to represent the re-
lationships between orthographical and phonologi-
cal representations in the form of a statistical pair-
ing between alternations. Using this model, it be-
comes possible to predict correctly the outcome in
the phonological domain of a given derivation in the
orthographic domain, including patterns of vocalic
alternations, which are notoriously difficult to model
using a &amp;quot;rule-based&amp;quot; approach.
</bodyText>
<subsectionHeader confidence="0.902259">
4.2 Achievements
</subsectionHeader>
<bodyText confidence="0.998701604651163">
The paradigmatic cascades model offers an origi-
nal and new framework for extracting information
from large corpora. In the particular context of
grapheme-to-phoneme transcription, it provides us
with a more satisfying model of pronunciation by
analogy, which:
• gives a principled way to automatically learn
local similarities that implicitely incorporate a
substantial knowledge of the morphological pro-
cesses and of the phonotactic constraints, both
in the graphemic and the phonemic domain.
This has allowed us to precisely define and iden-
tify the content of lexical neighbourhoods;
• achieves a very high precision without resorting
to pre-aligned data, and detects automatically
those words that are potentially the most dif-
ficult to pronounce (especially foreign words).
Interestingly, the ability of our model to pro-
cess data which are not aligned makes it directly
applicable to the reverse problem, i.e. phoneme-
to-grapheme conversion.
• is computationally tractable, even if extremely
ressource-consuming in the current version of
our algorithm. The main trouble here comes
from isolated words: for these words, the search
procedure wastes a lot of time examining a very
large number of very unlikely analogs, before re-
alizing that there is no acceptable lexical neigh-
bour. This aspect definitely needs to be im-
proved. We intend to explore several directions
to improve this search: one possibility is to use
a graphotactical model (e.g. a fl-gram model) in
order to make the pruning of the derivation tree
more effective. We expect such a model to bias
the search in favor of short words, which are
more represented than very long derivatives.
Another possibility is to tag, during the learning
stage, alternations with one or several morpho-
syntactic labels expressing morphotactical re-
strictions: this would restrict the domain of an
alternation to a certain class of words, and ac-
cordingly reduce the expansion of the analog
set.
</bodyText>
<subsectionHeader confidence="0.977465">
4.3 Perspectives
</subsectionHeader>
<bodyText confidence="0.999933407407407">
The paradigmatic cascades model achieves quite sat-
isfactory generalization performances when evalu-
ated in the task of pronouncing unknown words.
Moreover, this model provides us with an effective
way to define the lexical neighbourhood of a given
word, on the basis of &amp;quot;surface&amp;quot; (orthographical) local
similarities. It remains however to be seen how this
model can be extended to take into account other
factors which have been proven to influence analogi-
cal processes. For instance, frequency effects, which
tend to favor the more frequent lexical neighbours,
need to be properly model, if we wish to make a
more realistic account of the human performance in
the pronunciation task.
In a more general perspective, the notion of simi-
larity between linguistic objects plays a central role
in many corpus-based natural language processing
applications. This is especially obvious in the con-
text of example-based learning techniques, where the
inference of some unknown linguistics property of a
new object is performed on the basis of the most
similar available example(s). The use of some kind
of similarity measure has also demonstrated its effec-
tiveness to circumvent the problem of data sparse-
ness in the context of statistical language modeling.
In this context, we believe that our model, which
is precisely capable of detecting local similarities in
</bodyText>
<page confidence="0.996972">
434
</page>
<bodyText confidence="0.999694555555556">
lexicons, and to perform, on the basis of these simi-
larities, a global inferential transfer of knowledge, is
especially well suited for a large range of NLP tasks.
Encouraging results on the task of learning the En-
glish past-tense forms have already been reported in
(Yvon, 1996b), and we intend to continue to test this
model on various other potentially relevant applica-
tions, such as morpho-syntactical &amp;quot;guessing&amp;quot;, part-
of-speech tagging, etc.
</bodyText>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997601948717948">
Coker, Cecil H., Kenneth W. Church, and Mark Y.
Liberman. 1990. Morphology and rhyming: two
powerful alternatives to letter-to-sound rules. In
Proceedings of the ESCA Conference on Speech
Synthesis, Autrans, France.
Coltheart, Max. 1978. Lexical access in simple read-
ing tasks. In G. Underwood, editor, Strategies
of information processing. Academic Press, New
York, pages 151-216.
Coltheart, Max, Brent Curtis, Paul Atkins, and
Michael Haller. 1993. Models of reading aloud:
dual route and parallel distributed processing ap-
proaches. Psychological Review, 100:589-608.
Damper, Robert I. and John F. G. Eastmond. 1996.
Pronuncing text by analogy. In Proceedings of
the seventeenth International Conference on Com-
putational Linguistics (COLING&apos;96), pages 268-
273, Copenhagen, Denmark.
de Saussure, Ferdinand. 1916. Cours de Linguis-
tique Generale. Payot, Paris.
Dedina, Michael J. and Howard C. Nusbaum. 1991.
PRONOUNCE: a program for pronunciation by
analogy. Computer Speech and Langage, 5:55-64.
Federici, Stefano, Vito Pirrelli, and Francois Yvon.
1995. Advances in analogy-based learning: false
friends and exceptional items in pronunciation by
paradigm-driven analogy. In Proceedings of IJ-
CA I&apos;95 workshop on &apos;New Approaches to Learning
for Natural Language Processing&apos;, pages 158-163,
Montreal.
Glushsko, J. R. 1981. Principles for pronouncing
print: the psychology of phonography. In A. M.
Lesgold and C. A. Perfetti, editors, Interactive
Processes in Reading, pages 61-84, Hillsdale, New
Jersey. Erlbaum.
Lepage, Yves and Ando 1996. Saussurian
analogy : A theoretical account and its applica-
tion. In Proceedings of the seventeenth Interna-
tional Conference on Computational Linguistics
(COLING &apos;96), pages 717-722, Copenhagen, Den-
mark.
Pirrelli, Vito and Stefano Federici. 1994. &amp;quot;Deriva-
tional&amp;quot; paradigms in morphonology. In Proceed-
ings of the sixteenth International Conference on
Computational Linguistics (COLING &apos;94), Kyoto,
Japan.
Seidenberg, M. S. and James. L. McClelland. 1989.
A distributed, developmental model of word
recognition and naming. Psychological review,
96:523-568.
Sejnowski, Terrence J. and Charles R. Rosenberg.
1987. Parrallel network that learn to pronounce
English text. Complex Systems, 1:145-168.
Sullivan, K.P.H and Robert I. Damper. 1992. Novel-
word pronunciation within a text-to-speech sys-
tem. In Gerard Bailly and Christian Benoit, edi-
tors, Talking Machines, pages 183-195. North Hol-
land.
Torkolla, Kari. 1993. An efficient way to learn
English grapheme-to-phoneme rules automati-
cally. In Proceedings of the International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), volume 2, pages 199-202, Minneapo-
lis, Apr.
van den Bosch, Antal and Walter Daelemans. 1993.
Data-oriented methods for grapheme-to-phoneme
conversion. In Proceedings of the European Chap-
ter of the Association for Computational Linguis-
tics (PA CL), pages 45-53, Utrecht.
Yvon, Francois. 1996a. Grapheme-to-phoneme
conversion using multiple unbounded overlapping
chunks. In Proceedings of the conference on New
Methods in Natural Language Processing (NeM-
LaP II), pages 218-228, Ankara, Turkey.
Yvon, Francois. 1996b. Prononcer par analogie :
motivations, formalisations et evaluations. Ph.D.
thesis, Ecole Nationale Superieure des Telecom-
munications, Paris.
</reference>
<page confidence="0.99921">
435
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000622">
<title confidence="0.9989105">Paradigmatic Cascades: a Linguistically Sound Model of Pronunciation by Analogy</title>
<author confidence="0.998665">Francois Yvon</author>
<affiliation confidence="0.824454">ENST and CNRS, URA 820 Computer Science Department</affiliation>
<address confidence="0.945593">46 rue Barrault - F 75 013 Paris</address>
<email confidence="0.993576">yvonOinf.enst.fr</email>
<abstract confidence="0.997930364028778">We present and experimentally evaluate a new model of pronunciation by analogy: the paradigmatic cascades model. Given a pronunciation lexicon, this algorithm first extracts the most productive paradigmatic mappings in the graphemic domain, and pairs them statistically with their conelate(s) in the phonemic domain. These mappings are used to search and retrieve in the lexical database the most promising analog of unseen words. We finally apply to the analogs pronunciation the correlated series of mappings in the phonemic domain to get the desired pronunciation. 1 Motivation Psychological models of reading aloud traditionally assume the existence of two separate routes for converting print to sound: a direct lexical route, which is used to read familiar words, and a dual route relying upon abstract letter-to-sound rules to pronounce previously unseen words (Coltheart, 1978; Coltheart et al., 1993). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar items (the neighbours). idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation devices (e.g. (Seidenberg and McClelland, 1989)). The basic idea of these analogy-based models is pronounce an unknown word recombining pronunciations of lexical items sharing common with illustrate this strategy, Dedina and Nussbaum show how the pronunciation of sequence the pseudo-word analogized with the pronunciation of the same sequence there exists more than one way to recombine segments of lexical items, Dedina and Nussbaum&apos;s algorithm favors recombinations including of existing words. In this model, the similarity between two words is thus implicitely defined as a function of the length of their common subparts: the longer the common part, the better the analogy. This conception of analogical processes has an important consequence: it offers, as Damper and Eastmond ((Damper and Eastmond, 1996)) state it, &amp;quot;no principled way of deciding the orthographic neighbours of a novel word which are deemed to influence its pronunciation (...)&amp;quot; . For example, in the model proposed by Dedina and Nusbaum, any word having a common orthographic substring with the unknown word is likely to contribute to its pronunciation, which increases the number of lexical neighbours far beyond acceptable limits (in the case of neighbourhood would contain every Enword starting in ending in From a computational standpoint, implementing the recombination strategy requires a one-toone alignment between the lexical graphemic and phonemic representations, where each grapheme is matched with the corresponding phoneme (a null symbol is used to account for the cases where the lengths of these representations differ). This alignment makes it possible to retrieve, for any graphemic substring of a given lexical item, the corresponding phonemic string, at the cost however of an unmotivated complexification of lexical representations. In comparison, the paradigmatic cascades model (PCP for short) promotes an alternative view of analogical processes, which relies upon a linguistically motivated similarity measure between words. 428 The basic idea of our model is to take advantage of the internal structure of &amp;quot;natural&amp;quot; lexicons. In fact, a lexicon is a very complex object, whose elements are intimately tied together by a number of fine-grained relationships (typically induced by morphological processes), and whose content is severely restricted, on a language-dependant basis, by a complex of graphotactic, phonotactic and morphotactic constraints. Following e.g. (Pirrelli and Federici, 1994), we assume that these constraints surface simultaneously in the orthographical and in the phonological domain in the recurring pattern alterning of lexical items. Extending the idea originally proposed in (Federici, Pirrelli, and Yvon, 1995), we show that it is possible to extract these alternation patterns, to associate alternations in one domain with the related alternation in the other domain, and to construct, using this pairing, a fairly reliable pronunciation procedure. 2 The Paradigmatic Cascades Model In this section, we introduce the paradigmatic cascades model. We first formalize the concept of a paradigmatic relationship. We then go through the details of the learning procedure, which essentially consists in an extensive search for such relationships. We finally explain how these patterns are used in the pronunciation procedure. 2.1 Paradigmatic Relationships and Alternations The paradigmatic cascades model crucially relies the existence of numerous relalexical databases. A paradigmatic reinvolves four lexical entries b, c, d, expresses that these forms are involved in an analogical (in the Saussurian (de Saussure, 1916) sense) a is to is along abas also (Lepage and Shin-Ichi, 1996) for another utilization of this kind of proportions). Morphologically related pairs provide us with numerous examples of orthographical proportions, as in: : reaction = factor : faction these proportions in terms of orthoalternations, is in terms of partial functions in the graphemic domain, we can see that each proportion involves two alternations. The first transforms consists in exchanging the suffixes second one transforms consists in the prefixes f.These alternations are represented on figure 1. reaction 9 faction Figure 1: An Analogical Proportion Formally, we define the notion of a paradigmatic relationship as follows. Given E, a finite alphabet, finite subset of E*, we say that (a, Erxr paradigmatically related to d) there two partial functions E* to E*, prefixes and suffixes, and: = c (b) = d (a) = b (c) = d termed the alternations with the relationship a : C : domain of an alternation be denoted by dom(f). 2.2 The Learning Procedure The main purpose of the learning procedure is to extract from a pronunciation lexicon, presumably structured by multiple paradigmatic relationships, the most productive paradigmatic alternations. us start with some notations: Given alphabet and phonetic alphabet, a lexicon a subset of of be noted two strings pre f (x, f f (x , y)) their longest common pre- (resp. suffix). For two strings y having a common prefix (resp. suffix) u, (resp, denotes the function which transforms uv, and as = ut, a final v a final denotes the empty string. learning procedure searches for every 4-uples b, c, d) graphemic strings that a : c : d. match increments productivity of the related alternations search is performed using using a slightly modified version of the algorithm presented in (Federici, Pirrelli, and Yvon, 1995), which applies to evword procedure detailled in table 1. In fact, the properties of paradigmatic relationships, notably their symetry, allow to reduce dramatically the cost of this procedure, since not all reactor 9 factor 429 GET ALTERNATIONS (X) D(x) { y E f(x,y)) for E D(x) 3 do {(z,t) if 6 then Table 1: The Learning Procedure of strings in to be examined during that stage. For each graphemic alternation, we also record their correlated alternation(s) in the phonological domain, and accordingly increment their productiv- For instance, assuming that respectively receive the pronunciations /fwktar/ and /ri:aektar/, the discovery of the relationship expressed in (1) will lead our algorithm to record that graphemic alternation -4 re in the phonemic domain with the alternation /f/ -4 /ri:/. Note that the discovery of phonemic correlates does not require any sort of alignment between the orthographic and the phonemic representations: the procedure simply records the changes in the phonemic domain when the alternation applies in the graphemic domain. At the end of the learning stage, we have in hand set = of functions exchanging suffixes or in the graphemic domain, and for each a statistical measure its productivity, defined as the likelihood that the transform of a lexical item be another lexical item: E L} vi =I dorn(Ai) I (4) a set {Bi,j}, {1 correlated tions in the phonemic domain, and a statistical of their conditional productivity, i.e. of the likelihood that the phonetic alternawith A. Table 2 gives the list of the phonological correlates of the alternation which consists in adding the suffix /y, corresponding to a productive rule for deriving adverbs from adjectives in English. If the first lines of table 2 are indeed &amp;quot;true&amp;quot; phonemic correlates of the derivation, corresponding to various classes of adjectives, a careful examination of the last lines reveals that the extraction procedure is easily fooled alternation Example x -4 x-Ili:I good x-It1 -4 x-/adli:/ marked xloll -4 x- I oli:I equal -4 x-/li:/ capable x -4 x-/i:/ cool x-/in/ -4 x-/enli:/ clean x-11(11 x-/aidli:/ id x-/Iv/ -4 x-/aili:/ live x-I01 -x-/ali:/ loath S —4 s-/ail imp x-/Ir/ -4 x-/3:1i:/ ear -4 x-/onli:/ on 2: Phonemic correlates of x — ly accidental pairs like on-only earsimple pruning rule was used to get rid of these alternations on the basis of their productivity, and only alternations which were observed at least twice were retained. is important to realize that to specifiy neighbourhoods in a lexical entry nearest neighbour is simply most productive alternation applying to Lexical neighbourhoods in the paradigmatic cascades model are thus defined with respect to the locally most productive alternations. As a consequence, the definition of neighbourhoods implicitely incorporates a great deal of linguistic knowledge extracted from the lexicon, especially regarding morphological processes and phonotactic constraints, which makes much for relevant grounding the notion of anallexical items than, say, any neighbourhood based on the string edition metric. 2.3 The Pronunciation of Unknown Words Supose now that we wish to infer the pronunciation a word does not appear in the lexicon. This goal is achieved by exploring the neighbourof by order to find one or several lexical entry(ies) second stage of the pronunciation procedure is to adapt the known of derive a suitable pronunciafor idea here is to mirror in the phonemic the series of alternations which transform the graphemic domain, using the statistical pairing between alternations that is extracted during the learning stage. The complete pronunciation procedure is represented on figure 2. Let us examine carefully how these two aspects of the pronunciation procedure are implemented. The stage is to find a lexical entry in the neighbour- 430 Graphemic domain Phonemic domain Figure 2: The pronunciation of an unknown word of by basic idea is to generate (x), {Ai forAi x conall the words that can be derived from usa function in set, better viewed as a stack, is ordered according to the productivity of the Ai: the topmost element in the stack is the nearest of The first lexical item found in (x) the analog of (x) not contain any known word, we iterate the procedure, using top-ranked element of (x), of This expands the set of possible analogs, which is accordingly reordered, etc. This basic search strategy, which amounts to the exploration of a derivation tree, is extremely ressource consuming (every expension stage typically adds about a hundred of new virtual analogs), and is, in theory, not guaranted to terminate. In fact, the search problem is equivalent to the problem of parsing with an unrestricted Phrase Structure Grammar, which is known to be undecidable. We have evaluated two different search strategies, which implement various ways to alternate between stages stack is expanded by generating the derivatives of the topmost element) and stages in the stack are looked for in the lexicon). The first strategy implements a depth-first search of the analog set: each time the topmost element of the stack is searched, but not found, in the lexicon, its derivatives are immediately generated, and added to the stack. In this approach, the position of an analog in the stack is assessed as a function of the &amp;quot;distance&amp;quot; between the original word the analog = (Ai„_, (... Ai, according to: I.k d(x,y) = (5) 1.1 The search procedure is stopped as soon an anais found in else, when the distance bethe topmost element of the stack, which monotonously decreases (Vi, pi &lt; I), falls below a pre-defined theshold. The second strategy implements a kind of compromise between depth-first and breadth-first exploration of the derivation tree, and is best understood if we first look at a concrete example. Most alternations substituting one initial consonant are very productive, in English like in many other languages. a word starting with say, a very likely to have a very close derivative where the initial p has been replaced by say, a r. Now suppose that word starts with alternation will dean analog starting with will assess it with a very high score. This analog will, in turn, many more virtual analogs starting with once its suffixes will have been substituted during another expansion phase. This should be avoided, since there are in fact very few words starting with prefix would therefore like these words to be very poorly ranked. The second search strategy has been devised precisely to cope with this problem. The idea is to rank the stack of analogs according to the expectation of the number of lexical derivatives a given analog may have. This expectation is computed by summing up the productivities of all alternations that can be applied to an analog according to: (6) i/yEdompio This ranking will necessarily assess any analog startin a low score, as very few alternations will substitute its prefix. However, the computation of (6) is much more complex than (5), since it reto examine a given derivative can be positioned in the stack. This led us to bring forward the lexical matching stage: during the expansion of the topmost stack element, all its derivatives are looked for in the lexicon. If several derivatives are simultaneously found, the search procedure halts more than one analog. The expectation (6) does not decrease as more derivatives are added to the stack; consequently, it cannot be used to define a stopping criterion. The search procedure is therefore stopped when all derivatives up to a given depth (2 in our experiments) have been generated, and unsuccessfully looked for in the lexicon. This termination criterion is very restrictive, in comparison to the one implemented in the depth-first strategy, since it makes it impossible to pronounce very long derivatives, for which a significant number of alternations need to 431 be applied before an analog is found. An example is word which the &amp;quot;breadthfirst&amp;quot; search terminates uncessfully, whereas the depth-first search manages to retrieve the &amp;quot;analog&amp;quot; the results reported hereafter have been obtained using this &amp;quot;breadth-first&amp;quot; strategy, mainly because this search was associated with a more efficient procedure for reconstructing pronunciations (see below). Various pruning procedures have also been implemented in order to control the exponential growth of stack. For example, one pruning procedure detects the most obvious derivation cycles, which genin loops the same derivatives; another prunprocedure tries to detect commutating alternasubstituting the prefix then the suffix produces the same analog than when alternations apply in the reverse order, etc. More details regarding implementational aspects are given in (Yvon, 1996b). If the search procedure returns an analog y (x))) can build a pronunfor the known pronunciation 0(y) of y. For this purpose, we will use our knowledge the ...ik}, and generate every possible transforms of 0(y) in the phonological (... (q5(y))))}, with ... 1, and order this set using some function of the pi,j. The top-ranked element in this set is the of course, when the search fails, this procedure fails to propose any pronunciation. In fact, the results reported hereafter use a slightly version of this procedure, where the pronunciations of more than one analog are used for generating and selecting the pronunciation of the unknown word. The reason for using multiple analogs is twofold: first, it obviates the risk of being wrongly influenced by one very exceptional analog; second, enables us to model effects accurately. Psychological models of reading aloud indeed assume that the pronunciation of an unknown word is not influenced by just one analog, but rather by its entire lexical neighbourhood. 3 Experimental Results 3.1 Experimental Design We have evaluated this algorithm on two different pronunciation tasks. The first experiment consists in infering the pronunciation of the 70 pseudo-words originally used in Glushko&apos;s experiments, which have been used as a test-bed for various other pronunciation algorithms, and allow for a fair head-tohead comparison between the paradigmatic cascades model and other analogy-based procedures. For this experiment, we have used the entire nettalk (Sejnowski and Rosenberg, 1987) database (about 20 000 words) as the learning set. The second series of experiments is intended to provide a more realistic evaluation of our model in the task of pronouncing unknown words. We have used the following experimental design: 10 pairs of disjoint (learning set, test set) are randomly selected from the nettalk database and evaluated. In each experiment, the test set contains about the tenth of the available data. A transcription is judged to be correct when it matches exactly the pronunciation listed in the database at the segmental level. The number of correct phonemes in a transcription is computed on the basis of the string-to-string edit distance with the target pronunciation. For each experiment, we measure the percentage of phoneme and words that are correctly predicted (referred to as correctness), and two additional figures, which are usually not significant in context of the evaluation of transcription systems. Recall that our algorithm, unlike many other pronunciation algorithms, is likely to remain silent. In order to take this aspect into account, we measure in each experiment the number of words that can not be pronounced at all (the silence), and the percentage of phonemes and words are correctly transcribed those words have been pronounced at all precision). The average values for these measures are reported hereafter. 3.2 Pseudo-words All but one pseudo-words of Glushko&apos;s test set could be pronounced by the paradigmatic cascades algorithm, and amongst the 69 pronunciation suggested by our program, only 9 were uncorrect (that is, were not proposed by human subjects in Glushko&apos;s experiments), yielding an overall correctness of 85.7%, and a precision of 87.3%. An important property of our algortihm is that it allows to precisely identify, for each pseudo-word, the lexical entries that have been analogized, i.e. whose pronunciation was used in the inferential process. Looking at these analogs, it appears that three of our errors are grounded on very sensible analogies, and provide us with pronunciations that seem at least plausible, even if they were not suggested in experiments. These were with with tomb. These results compare favorably well with the performances reported for other pronunciation by analalgorithms ((Damper and Eastmond, 1996) re- 432 ports very similat correctness figures), especially if one remembers that our results have been obtained, without resorting to any kind of pre-alignment between the graphemic and phonemic strings in the lexicons. 3.3 Lexical Entries This second series of experiment is intended to provide us with more realistic evaluations of the paradigmatic cascade model: Glushko&apos;s pseudowords have been built by substitutin.g the initial consonant of existing monosyllabic words, and constitute therefore an over-simplistic test-bed. The nettalk dataset contains plurisyllabic words, complex derivatives, loan words, etc, and allows to test the ability of our model to learn complex morphophonological phenomenas, notably vocalic alternations and other kinds of phonologically conditioned root allomorphy, that are very difficult to learn. With this new test set, the overall performances of our algorithm averages at about 54.5% of entirely correct words, corresponding to a 76% per phoneme correctness. If we keep the words that could not be pronounced at all (about 15% of the test set) apart from the evaluation, the per word and per phoneme precision improve considerably, reaching respectively 65% and 93%. Again, these precision results compare relatively well with the results achieved on the same corpus using other selflearning algorithms for grapheme-to-phoneme transcription (e.g. (van den Bosch and Da.elemans, 1993; Yvon, 1996a)), which, unlike ours, benefit from the knowledge of the alignment between graphemic and phonemic strings. Table 3 summaries the performance (in terms of per word correctness, silence, and precision) of various other pronunciation systems, namely PRONOUNCE (Dedina and Nusbaum, 1991), DEC (TorkoIla, 1993), SMPA (Yvon, I 996a). All these models have been tested using exactly the same evaluation procedure and data (see (Yvon, 1996b), which also contains an evolution per- Ibrmed with a. French database suggesting that this learning strategy effectively applies to other languages). Systei n corr. prec. silence DEC 56.67 56.67 0 S M PA 63.96 64.24 0.42 PRONOUNCE 56.56 56.75 0.32 PCP 54.49 63.95 14.80 3: Comparative Evaluation &apos;Fable 3 pinpoints the main weakness of our model, that is, its significant silence rate. The careful examination of the words that cannot be pronounced reveals that they are either loan words, which are very isolated in an English lexicon, and for which no analog can be found; or complex morphological derivatives for which the search procedure is stopped before the existing analog(s) can be reached. Typical are: timpani, hangdog, pemmican, list just a few. This suggests that the words which were not pronounced are not randomly distributed. Instead, they mostly belong to a linguistically homogeneous group, the group of foreign words, which, for lack of better evidence, should better be left silent, or processed by another pronunciation procedure (for example a rule-based system (Coker, Church, and Liberman, 1990)), than uncorrectly analogized. Some complementary results finally need to be mentioned here, in relation to the size of lexical neighbourhoods. In fact, one of our main goal was to define in a sensible way the concept of a lexical neighbourhood: it is therefore important to check that our model manages to keep this neighbourhood relatively small. Indeed, if this neighbourhood can be quite large (typically 50 analogs) for short words, the number of analogs used in a pronunciation averages at about 9.5, which proves that our definition of a lexical neighbourhood is sufficiently restrictive. 4 Discussion and Perspectives 4.1 Related works A large number of procedures aiming at the automatic discovery of pronunciation &amp;quot;rules&amp;quot; have been proposed over the past few years: connectionist models (e.g. (Sejnowski and Rosenberg, 1987)), traditional symbolic machine learning techniques (induction of decision trees, k-nearest neighbours) e.g. (Torkolla, 1993; van den Bosch and Daelemans, 1993), as well as various recombination techniques (Yvon, 1996a). In these models, orthographical correspondances are primarily viewed as resulting from strict underlying system, each grapheme encodes exactly one phoneme. This assumption is reflected by the possibility of aligning on a one-to-one basis graphemic and phonemic strings, and these models indeed use this kind of alignment to initiate learning. Under this view, the orthographical representation of individual words is strongly subject, to their phonological forms on an word per word basis. The main task of a machinelearning algorithm is thus mainly to retrieve, on a. statistical basis, these grapheme-phoneme correspondances, which are, in languages like French or 433 English, accidentally obscured by a multitude of exceptional and idiosyncratic correspondances. There exists undoubtly strong historical evidences supporting the view that the orthographical system of most european languages developped from a such phonographical system, and languages like Spanish or Italian still offer examples of that kind of very regular organization. Our model, which extends the proposals of (Coker, Church, and Liberman, 1990), and more recently, of (Federici, Pirrelli, and Yvon, 1995), entertains a different view of orthographical systems. Even we if acknowledge the mostly phonographical organization of say, French orthography, we believe that the multiple deviations from a strict grapheme-phoneme correspondance are best captured in a model which weakens somehow the assumption of a strong dependancy between orthographical and phonological representations. In our model, each domain has its own organization, which is represented in the form of systematic (paradigmatic) set of oppositions and alternations. In both domain however, this orgais to the same paradigmatic prinmakes it possible to represent the relationships between orthographical and phonological representations in the form of a statistical pairing between alternations. Using this model, it becomes possible to predict correctly the outcome in the phonological domain of a given derivation in the orthographic domain, including patterns of vocalic alternations, which are notoriously difficult to model using a &amp;quot;rule-based&amp;quot; approach. 4.2 Achievements The paradigmatic cascades model offers an original and new framework for extracting information from large corpora. In the particular context of grapheme-to-phoneme transcription, it provides us with a more satisfying model of pronunciation by analogy, which: • gives a principled way to automatically learn local similarities that implicitely incorporate a substantial knowledge of the morphological processes and of the phonotactic constraints, both in the graphemic and the phonemic domain. This has allowed us to precisely define and identify the content of lexical neighbourhoods; • achieves a very high precision without resorting to pre-aligned data, and detects automatically those words that are potentially the most difficult to pronounce (especially foreign words). Interestingly, the ability of our model to process data which are not aligned makes it directly applicable to the reverse problem, i.e. phonemeto-grapheme conversion. • is computationally tractable, even if extremely ressource-consuming in the current version of our algorithm. The main trouble here comes from isolated words: for these words, the search procedure wastes a lot of time examining a very large number of very unlikely analogs, before realizing that there is no acceptable lexical neighbour. This aspect definitely needs to be improved. We intend to explore several directions to improve this search: one possibility is to use a graphotactical model (e.g. a fl-gram model) in order to make the pruning of the derivation tree more effective. We expect such a model to bias the search in favor of short words, which are more represented than very long derivatives. Another possibility is to tag, during the learning stage, alternations with one or several morphosyntactic labels expressing morphotactical restrictions: this would restrict the domain of an alternation to a certain class of words, and accordingly reduce the expansion of the analog set. 4.3 Perspectives The paradigmatic cascades model achieves quite satisfactory generalization performances when evaluated in the task of pronouncing unknown words. Moreover, this model provides us with an effective way to define the lexical neighbourhood of a given word, on the basis of &amp;quot;surface&amp;quot; (orthographical) local similarities. It remains however to be seen how this model can be extended to take into account other factors which have been proven to influence analogical processes. For instance, frequency effects, which tend to favor the more frequent lexical neighbours, need to be properly model, if we wish to make a more realistic account of the human performance in the pronunciation task. a more general perspective, the notion of similinguistic objects plays a central role in many corpus-based natural language processing applications. This is especially obvious in the context of example-based learning techniques, where the inference of some unknown linguistics property of a new object is performed on the basis of the most similar available example(s). The use of some kind of similarity measure has also demonstrated its effectiveness to circumvent the problem of data sparseness in the context of statistical language modeling. In this context, we believe that our model, which is precisely capable of detecting local similarities in 434 lexicons, and to perform, on the basis of these similarities, a global inferential transfer of knowledge, is especially well suited for a large range of NLP tasks. Encouraging results on the task of learning the English past-tense forms have already been reported in (Yvon, 1996b), and we intend to continue to test this model on various other potentially relevant applications, such as morpho-syntactical &amp;quot;guessing&amp;quot;, partof-speech tagging, etc.</abstract>
<note confidence="0.832355644736842">References Coker, Cecil H., Kenneth W. Church, and Mark Y. Liberman. 1990. Morphology and rhyming: two powerful alternatives to letter-to-sound rules. In Proceedings of the ESCA Conference on Speech France. Coltheart, Max. 1978. Lexical access in simple readtasks. In G. Underwood, editor, information processing. Press, New York, pages 151-216. Coltheart, Max, Brent Curtis, Paul Atkins, and Michael Haller. 1993. Models of reading aloud: dual route and parallel distributed processing ap- Review, Damper, Robert I. and John F. G. Eastmond. 1996. text by analogy. In of the seventeenth International Conference on Com- Linguistics (COLING&apos;96), 268- 273, Copenhagen, Denmark. Saussure, Ferdinand. 1916. de Linguis- Generale. Paris. Dedina, Michael J. and Howard C. Nusbaum. 1991. PRONOUNCE: a program for pronunciation by Speech and Langage, Federici, Stefano, Vito Pirrelli, and Francois Yvon. 1995. Advances in analogy-based learning: false friends and exceptional items in pronunciation by analogy. In of IJ- CA I&apos;95 workshop on &apos;New Approaches to Learning Natural Language Processing&apos;, 158-163, Montreal. Glushsko, J. R. 1981. Principles for pronouncing print: the psychology of phonography. In A. M. and C. A. Perfetti, editors, in Reading, 61-84, Hillsdale, New Jersey. Erlbaum. Lepage, Yves and Ando 1996. Saussurian analogy : A theoretical account and its applica- In of the seventeenth International Conference on Computational Linguistics &apos;96), 717-722, Copenhagen, Denmark. Pirrelli, Vito and Stefano Federici. 1994. &amp;quot;Derivaparadigms in morphonology. In Proceedings of the sixteenth International Conference on Linguistics (COLING &apos;94), Japan. Seidenberg, M. S. and James. L. McClelland. 1989. A distributed, developmental model of word and naming. review, 96:523-568. Sejnowski, Terrence J. and Charles R. Rosenberg. 1987. Parrallel network that learn to pronounce text. Systems, Sullivan, K.P.H and Robert I. Damper. 1992. Novelpronunciation within a text-to-speech sys- Gerard Bailly and Christian Benoit, edi- Machines, 183-195. North Holland. Torkolla, Kari. 1993. An efficient way to learn English grapheme-to-phoneme rules automati- In of the International Conference on Acoustics, Speech and Signal Processing 2, pages 199-202, Minneapolis, Apr. van den Bosch, Antal and Walter Daelemans. 1993. Data-oriented methods for grapheme-to-phoneme In of the European Chapter of the Association for Computational Linguis- (PA CL), 45-53, Utrecht. Yvon, Francois. 1996a. Grapheme-to-phoneme conversion using multiple unbounded overlapping In of the conference on New Methods in Natural Language Processing (NeM- II), 218-228, Ankara, Turkey. Francois. 1996b. par analogie :</note>
<abstract confidence="0.684709333333333">formalisations et evaluations. thesis, Ecole Nationale Superieure des Telecommunications, Paris.</abstract>
<intro confidence="0.364011">435</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cecil H Coker</author>
<author>Kenneth W Church</author>
<author>Mark Y Liberman</author>
</authors>
<title>Morphology and rhyming: two powerful alternatives to letter-to-sound rules.</title>
<date>1990</date>
<booktitle>In Proceedings of the ESCA Conference on Speech Synthesis,</booktitle>
<location>Autrans, France.</location>
<contexts>
<context position="24814" citStr="Coker, Church, and Liberman, 1990" startWordPosition="4062" endWordPosition="4066">glish lexicon, and for which no analog can be found; or complex morphological derivatives for which the search procedure is stopped before the existing analog(s) can be reached. Typical examples are: synergistically, timpani, hangdog, oasis, pemmican, to list just a few. This suggests that the words which were not pronounced are not randomly distributed. Instead, they mostly belong to a linguistically homogeneous group, the group of foreign words, which, for lack of better evidence, should better be left silent, or processed by another pronunciation procedure (for example a rule-based system (Coker, Church, and Liberman, 1990)), than uncorrectly analogized. Some complementary results finally need to be mentioned here, in relation to the size of lexical neighbourhoods. In fact, one of our main goal was to define in a sensible way the concept of a lexical neighbourhood: it is therefore important to check that our model manages to keep this neighbourhood relatively small. Indeed, if this neighbourhood can be quite large (typically 50 analogs) for short words, the number of analogs used in a pronunciation averages at about 9.5, which proves that our definition of a lexical neighbourhood is sufficiently restrictive. 4 </context>
<context position="27015" citStr="Coker, Church, and Liberman, 1990" startWordPosition="4394" endWordPosition="4398">s. The main task of a machinelearning algorithm is thus mainly to retrieve, on a. statistical basis, these grapheme-phoneme correspondances, which are, in languages like French or 433 English, accidentally obscured by a multitude of exceptional and idiosyncratic correspondances. There exists undoubtly strong historical evidences supporting the view that the orthographical system of most european languages developped from a such phonographical system, and languages like Spanish or Italian still offer examples of that kind of very regular organization. Our model, which extends the proposals of (Coker, Church, and Liberman, 1990), and more recently, of (Federici, Pirrelli, and Yvon, 1995), entertains a different view of orthographical systems. Even we if acknowledge the mostly phonographical organization of say, French orthography, we believe that the multiple deviations from a strict grapheme-phoneme correspondance are best captured in a model which weakens somehow the assumption of a strong dependancy between orthographical and phonological representations. In our model, each domain has its own organization, which is represented in the form of systematic (paradigmatic) set of oppositions and alternations. In both d</context>
</contexts>
<marker>Coker, Church, Liberman, 1990</marker>
<rawString>Coker, Cecil H., Kenneth W. Church, and Mark Y. Liberman. 1990. Morphology and rhyming: two powerful alternatives to letter-to-sound rules. In Proceedings of the ESCA Conference on Speech Synthesis, Autrans, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Coltheart</author>
</authors>
<title>Lexical access in simple reading tasks.</title>
<date>1978</date>
<booktitle>Strategies of information processing.</booktitle>
<pages>151--216</pages>
<editor>In G. Underwood, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="1088" citStr="Coltheart, 1978" startWordPosition="162" endWordPosition="163"> with their conelate(s) in the phonemic domain. These mappings are used to search and retrieve in the lexical database the most promising analog of unseen words. We finally apply to the analogs pronunciation the correlated series of mappings in the phonemic domain to get the desired pronunciation. 1 Motivation Psychological models of reading aloud traditionally assume the existence of two separate routes for converting print to sound: a direct lexical route, which is used to read familiar words, and a dual route relying upon abstract letter-to-sound rules to pronounce previously unseen words (Coltheart, 1978; Coltheart et al., 1993). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). This idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation device</context>
</contexts>
<marker>Coltheart, 1978</marker>
<rawString>Coltheart, Max. 1978. Lexical access in simple reading tasks. In G. Underwood, editor, Strategies of information processing. Academic Press, New York, pages 151-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Coltheart</author>
<author>Brent Curtis</author>
<author>Paul Atkins</author>
<author>Michael Haller</author>
</authors>
<title>Models of reading aloud: dual route and parallel distributed processing approaches.</title>
<date>1993</date>
<journal>Psychological Review,</journal>
<pages>100--589</pages>
<contexts>
<context position="1113" citStr="Coltheart et al., 1993" startWordPosition="164" endWordPosition="167">ate(s) in the phonemic domain. These mappings are used to search and retrieve in the lexical database the most promising analog of unseen words. We finally apply to the analogs pronunciation the correlated series of mappings in the phonemic domain to get the desired pronunciation. 1 Motivation Psychological models of reading aloud traditionally assume the existence of two separate routes for converting print to sound: a direct lexical route, which is used to read familiar words, and a dual route relying upon abstract letter-to-sound rules to pronounce previously unseen words (Coltheart, 1978; Coltheart et al., 1993). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). This idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation devices (e.g. (Seidenberg and M</context>
</contexts>
<marker>Coltheart, Curtis, Atkins, Haller, 1993</marker>
<rawString>Coltheart, Max, Brent Curtis, Paul Atkins, and Michael Haller. 1993. Models of reading aloud: dual route and parallel distributed processing approaches. Psychological Review, 100:589-608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert I Damper</author>
<author>John F G Eastmond</author>
</authors>
<title>Pronuncing text by analogy.</title>
<date>1996</date>
<booktitle>In Proceedings of the seventeenth International Conference on Computational Linguistics (COLING&apos;96),</booktitle>
<pages>268--273</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2569" citStr="Damper and Eastmond, 1996" startWordPosition="391" endWordPosition="394"> how the pronunciation of the sequence lop in the pseudo-word blope is analogized with the pronunciation of the same sequence in sloping. As there exists more than one way to recombine segments of lexical items, Dedina and Nussbaum&apos;s algorithm favors recombinations including large substrings of existing words. In this model, the similarity between two words is thus implicitely defined as a function of the length of their common subparts: the longer the common part, the better the analogy. This conception of analogical processes has an important consequence: it offers, as Damper and Eastmond ((Damper and Eastmond, 1996)) state it, &amp;quot;no principled way of deciding the orthographic neighbours of a novel word which are deemed to influence its pronunciation (...)&amp;quot; . For example, in the model proposed by Dedina and Nusbaum, any word having a common orthographic substring with the unknown word is likely to contribute to its pronunciation, which increases the number of lexical neighbours far beyond acceptable limits (in the case of blope, this neighbourhood would contain every English word starting in bl, or ending in ope, etc). From a computational standpoint, implementing the recombination strategy requires a one-t</context>
<context position="21781" citStr="Damper and Eastmond, 1996" startWordPosition="3589" endWordPosition="3592">rtihm is that it allows to precisely identify, for each pseudo-word, the lexical entries that have been analogized, i.e. whose pronunciation was used in the inferential process. Looking at these analogs, it appears that three of our errors are grounded on very sensible analogies, and provide us with pronunciations that seem at least plausible, even if they were not suggested in Glushko&apos;s experiments. These were pild and bild, analogized with wild, and pomb, analogized with tomb. These results compare favorably well with the performances reported for other pronunciation by analogy algorithms ((Damper and Eastmond, 1996) re432 ports very similat correctness figures), especially if one remembers that our results have been obtained, without resorting to any kind of pre-alignment between the graphemic and phonemic strings in the lexicons. 3.3 Lexical Entries This second series of experiment is intended to provide us with more realistic evaluations of the paradigmatic cascade model: Glushko&apos;s pseudowords have been built by substitutin.g the initial consonant of existing monosyllabic words, and constitute therefore an over-simplistic test-bed. The nettalk dataset contains plurisyllabic words, complex derivatives, </context>
</contexts>
<marker>Damper, Eastmond, 1996</marker>
<rawString>Damper, Robert I. and John F. G. Eastmond. 1996. Pronuncing text by analogy. In Proceedings of the seventeenth International Conference on Computational Linguistics (COLING&apos;96), pages 268-273, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferdinand de Saussure</author>
</authors>
<date>1916</date>
<booktitle>Cours de Linguistique Generale. Payot,</booktitle>
<location>Paris.</location>
<marker>de Saussure, 1916</marker>
<rawString>de Saussure, Ferdinand. 1916. Cours de Linguistique Generale. Payot, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Dedina</author>
<author>Howard C Nusbaum</author>
</authors>
<title>PRONOUNCE: a program for pronunciation by analogy.</title>
<date>1991</date>
<journal>Computer Speech and Langage,</journal>
<pages>5--55</pages>
<contexts>
<context position="1615" citStr="Dedina and Nusbaum, 1991" startWordPosition="239" endWordPosition="242">relying upon abstract letter-to-sound rules to pronounce previously unseen words (Coltheart, 1978; Coltheart et al., 1993). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). This idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation devices (e.g. (Seidenberg and McClelland, 1989)). The basic idea of these analogy-based models is to pronounce an unknown word x by recombining pronunciations of lexical items sharing common subparts with x. To illustrate this strategy, Dedina and Nussbaum show how the pronunciation of the sequence lop in the pseudo-word blope is analogized with the pronunciation of the same sequence in sloping. As there exists more than one way to recombine segments of lexical items, Dedina and Nussbaum&apos;s algorithm favors recombinations includ</context>
<context position="23501" citStr="Dedina and Nusbaum, 1991" startWordPosition="3853" endWordPosition="3857"> the evaluation, the per word and per phoneme precision improve considerably, reaching respectively 65% and 93%. Again, these precision results compare relatively well with the results achieved on the same corpus using other selflearning algorithms for grapheme-to-phoneme transcription (e.g. (van den Bosch and Da.elemans, 1993; Yvon, 1996a)), which, unlike ours, benefit from the knowledge of the alignment between graphemic and phonemic strings. Table 3 summaries the performance (in terms of per word correctness, silence, and precision) of various other pronunciation systems, namely PRONOUNCE (Dedina and Nusbaum, 1991), DEC (TorkoIla, 1993), SMPA (Yvon, I 996a). All these models have been tested using exactly the same evaluation procedure and data (see (Yvon, 1996b), which also contains an evolution perIbrmed with a. French database suggesting that this learning strategy effectively applies to other languages). Systei n corr. prec. silence DEC 56.67 56.67 0 S M PA 63.96 64.24 0.42 PRONOUNCE 56.56 56.75 0.32 PCP 54.49 63.95 14.80 &apos;fable 3: A Comparative Evaluation &apos;Fable 3 pinpoints the main weakness of our model, that is, its significant silence rate. The careful examination of the words that cannot be pron</context>
</contexts>
<marker>Dedina, Nusbaum, 1991</marker>
<rawString>Dedina, Michael J. and Howard C. Nusbaum. 1991. PRONOUNCE: a program for pronunciation by analogy. Computer Speech and Langage, 5:55-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Federici</author>
<author>Vito Pirrelli</author>
<author>Francois Yvon</author>
</authors>
<title>Advances in analogy-based learning: false friends and exceptional items in pronunciation by paradigm-driven analogy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCA I&apos;95 workshop on &apos;New Approaches to Learning for Natural Language Processing&apos;,</booktitle>
<pages>158--163</pages>
<location>Montreal.</location>
<contexts>
<context position="4556" citStr="Federici, Pirrelli, and Yvon, 1995" startWordPosition="696" endWordPosition="700">n fact, a lexicon is a very complex object, whose elements are intimately tied together by a number of fine-grained relationships (typically induced by morphological processes), and whose content is severely restricted, on a language-dependant basis, by a complex of graphotactic, phonotactic and morphotactic constraints. Following e.g. (Pirrelli and Federici, 1994), we assume that these constraints surface simultaneously in the orthographical and in the phonological domain in the recurring pattern of paradigmatically alterning pairs of lexical items. Extending the idea originally proposed in (Federici, Pirrelli, and Yvon, 1995), we show that it is possible to extract these alternation patterns, to associate alternations in one domain with the related alternation in the other domain, and to construct, using this pairing, a fairly reliable pronunciation procedure. 2 The Paradigmatic Cascades Model In this section, we introduce the paradigmatic cascades model. We first formalize the concept of a paradigmatic relationship. We then go through the details of the learning procedure, which essentially consists in an extensive search for such relationships. We finally explain how these patterns are used in the pronunciation</context>
<context position="8033" citStr="Federici, Pirrelli, and Yvon, 1995" startWordPosition="1287" endWordPosition="1292">(x , y)) denotes their longest common prefix (resp. suffix). For two strings x and y having a non-empty common prefix (resp. suffix) u, f, (resp, g4) denotes the function which transforms x into y: as x = uv, and as y = ut, ft&apos;y substitutes a final v with a final t. A denotes the empty string. Given ,C, the learning procedure searches LG for any for every 4-uples (a, b, c, d) of graphemic strings such that a : b =f,g c : d. Each match increments the productivity of the related alternations f and g. This search is performed using using a slightly modified version of the algorithm presented in (Federici, Pirrelli, and Yvon, 1995), which applies to every word x in LG the procedure detailled in table 1. In fact, the properties of paradigmatic relationships, notably their symetry, allow to reduce dramatically the cost of this procedure, since not all reactor 9 factor 429 GET ALTERNATIONS (X) 1 D(x) 4- { y E .CGAt = pre f(x,y)) A} 2 for y E D(x) 3 do 4 P(x,y) {(z,t) E LG X LG/z = f;y(t)} 5 if P(x,y) 0 6 then 7 IncrementCount 8 IncrementCount(fft) Table 1: The Learning Procedure 4-uple of strings in LG need to be examined during that stage. For each graphemic alternation, we also record their correlated alternation(s) in </context>
<context position="27075" citStr="Federici, Pirrelli, and Yvon, 1995" startWordPosition="4403" endWordPosition="4407">nly to retrieve, on a. statistical basis, these grapheme-phoneme correspondances, which are, in languages like French or 433 English, accidentally obscured by a multitude of exceptional and idiosyncratic correspondances. There exists undoubtly strong historical evidences supporting the view that the orthographical system of most european languages developped from a such phonographical system, and languages like Spanish or Italian still offer examples of that kind of very regular organization. Our model, which extends the proposals of (Coker, Church, and Liberman, 1990), and more recently, of (Federici, Pirrelli, and Yvon, 1995), entertains a different view of orthographical systems. Even we if acknowledge the mostly phonographical organization of say, French orthography, we believe that the multiple deviations from a strict grapheme-phoneme correspondance are best captured in a model which weakens somehow the assumption of a strong dependancy between orthographical and phonological representations. In our model, each domain has its own organization, which is represented in the form of systematic (paradigmatic) set of oppositions and alternations. In both domain however, this organization is subject to the same para</context>
</contexts>
<marker>Federici, Pirrelli, Yvon, 1995</marker>
<rawString>Federici, Stefano, Vito Pirrelli, and Francois Yvon. 1995. Advances in analogy-based learning: false friends and exceptional items in pronunciation by paradigm-driven analogy. In Proceedings of IJCA I&apos;95 workshop on &apos;New Approaches to Learning for Natural Language Processing&apos;, pages 158-163, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Glushsko</author>
</authors>
<title>Principles for pronouncing print: the psychology of phonography.</title>
<date>1981</date>
<booktitle>Interactive Processes in Reading,</booktitle>
<pages>61--84</pages>
<editor>In A. M. Lesgold and C. A. Perfetti, editors,</editor>
<publisher>Erlbaum.</publisher>
<location>Hillsdale, New Jersey.</location>
<contexts>
<context position="1190" citStr="Glushsko, 1981" startWordPosition="179" endWordPosition="180">lexical database the most promising analog of unseen words. We finally apply to the analogs pronunciation the correlated series of mappings in the phonemic domain to get the desired pronunciation. 1 Motivation Psychological models of reading aloud traditionally assume the existence of two separate routes for converting print to sound: a direct lexical route, which is used to read familiar words, and a dual route relying upon abstract letter-to-sound rules to pronounce previously unseen words (Coltheart, 1978; Coltheart et al., 1993). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). This idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation devices (e.g. (Seidenberg and McClelland, 1989)). The basic idea of these analogy-based models is to pronoun</context>
</contexts>
<marker>Glushsko, 1981</marker>
<rawString>Glushsko, J. R. 1981. Principles for pronouncing print: the psychology of phonography. In A. M. Lesgold and C. A. Perfetti, editors, Interactive Processes in Reading, pages 61-84, Hillsdale, New Jersey. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
<author>Ando</author>
</authors>
<title>Saussurian analogy : A theoretical account and its application.</title>
<date>1996</date>
<booktitle>In Proceedings of the seventeenth International Conference on Computational Linguistics (COLING &apos;96),</booktitle>
<pages>717--722</pages>
<location>Copenhagen, Denmark.</location>
<marker>Lepage, Ando, 1996</marker>
<rawString>Lepage, Yves and Ando 1996. Saussurian analogy : A theoretical account and its application. In Proceedings of the seventeenth International Conference on Computational Linguistics (COLING &apos;96), pages 717-722, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vito Pirrelli</author>
<author>Stefano Federici</author>
</authors>
<title>Derivational&amp;quot; paradigms in morphonology.</title>
<date>1994</date>
<booktitle>In Proceedings of the sixteenth International Conference on Computational Linguistics (COLING &apos;94),</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="4289" citStr="Pirrelli and Federici, 1994" startWordPosition="658" endWordPosition="662">es model (PCP for short) promotes an alternative view of analogical processes, which relies upon a linguistically motivated similarity measure between words. 428 The basic idea of our model is to take advantage of the internal structure of &amp;quot;natural&amp;quot; lexicons. In fact, a lexicon is a very complex object, whose elements are intimately tied together by a number of fine-grained relationships (typically induced by morphological processes), and whose content is severely restricted, on a language-dependant basis, by a complex of graphotactic, phonotactic and morphotactic constraints. Following e.g. (Pirrelli and Federici, 1994), we assume that these constraints surface simultaneously in the orthographical and in the phonological domain in the recurring pattern of paradigmatically alterning pairs of lexical items. Extending the idea originally proposed in (Federici, Pirrelli, and Yvon, 1995), we show that it is possible to extract these alternation patterns, to associate alternations in one domain with the related alternation in the other domain, and to construct, using this pairing, a fairly reliable pronunciation procedure. 2 The Paradigmatic Cascades Model In this section, we introduce the paradigmatic cascades mo</context>
</contexts>
<marker>Pirrelli, Federici, 1994</marker>
<rawString>Pirrelli, Vito and Stefano Federici. 1994. &amp;quot;Derivational&amp;quot; paradigms in morphonology. In Proceedings of the sixteenth International Conference on Computational Linguistics (COLING &apos;94), Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L McClelland</author>
</authors>
<title>A distributed, developmental model of word recognition and naming. Psychological review,</title>
<date>1989</date>
<pages>96--523</pages>
<contexts>
<context position="1729" citStr="McClelland, 1989" startWordPosition="256" endWordPosition="257">). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). This idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation devices (e.g. (Seidenberg and McClelland, 1989)). The basic idea of these analogy-based models is to pronounce an unknown word x by recombining pronunciations of lexical items sharing common subparts with x. To illustrate this strategy, Dedina and Nussbaum show how the pronunciation of the sequence lop in the pseudo-word blope is analogized with the pronunciation of the same sequence in sloping. As there exists more than one way to recombine segments of lexical items, Dedina and Nussbaum&apos;s algorithm favors recombinations including large substrings of existing words. In this model, the similarity between two words is thus implicitely define</context>
</contexts>
<marker>McClelland, 1989</marker>
<rawString>Seidenberg, M. S. and James. L. McClelland. 1989. A distributed, developmental model of word recognition and naming. Psychological review, 96:523-568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terrence J Sejnowski</author>
<author>Charles R Rosenberg</author>
</authors>
<title>Parrallel network that learn to pronounce English text.</title>
<date>1987</date>
<journal>Complex Systems,</journal>
<pages>1--145</pages>
<contexts>
<context position="19385" citStr="Sejnowski and Rosenberg, 1987" startWordPosition="3206" endWordPosition="3209">f an unknown word is not influenced by just one analog, but rather by its entire lexical neighbourhood. 3 Experimental Results 3.1 Experimental Design We have evaluated this algorithm on two different pronunciation tasks. The first experiment consists in infering the pronunciation of the 70 pseudo-words originally used in Glushko&apos;s experiments, which have been used as a test-bed for various other pronunciation algorithms, and allow for a fair head-tohead comparison between the paradigmatic cascades model and other analogy-based procedures. For this experiment, we have used the entire nettalk (Sejnowski and Rosenberg, 1987) database (about 20 000 words) as the learning set. The second series of experiments is intended to provide a more realistic evaluation of our model in the task of pronouncing unknown words. We have used the following experimental design: 10 pairs of disjoint (learning set, test set) are randomly selected from the nettalk database and evaluated. In each experiment, the test set contains about the tenth of the available data. A transcription is judged to be correct when it matches exactly the pronunciation listed in the database at the segmental level. The number of correct phonemes in a transc</context>
<context position="25650" citStr="Sejnowski and Rosenberg, 1987" startWordPosition="4194" endWordPosition="4197">y the concept of a lexical neighbourhood: it is therefore important to check that our model manages to keep this neighbourhood relatively small. Indeed, if this neighbourhood can be quite large (typically 50 analogs) for short words, the number of analogs used in a pronunciation averages at about 9.5, which proves that our definition of a lexical neighbourhood is sufficiently restrictive. 4 Discussion and Perspectives 4.1 Related works A large number of procedures aiming at the automatic discovery of pronunciation &amp;quot;rules&amp;quot; have been proposed over the past few years: connectionist models (e.g. (Sejnowski and Rosenberg, 1987)), traditional symbolic machine learning techniques (induction of decision trees, k-nearest neighbours) e.g. (Torkolla, 1993; van den Bosch and Daelemans, 1993), as well as various recombination techniques (Yvon, 1996a). In these models, orthographical correspondances are primarily viewed as resulting from a. strict underlying phonographical system, where each grapheme encodes exactly one phoneme. This assumption is reflected by the possibility of aligning on a one-to-one basis graphemic and phonemic strings, and these models indeed use this kind of alignment to initiate learning. Under this v</context>
</contexts>
<marker>Sejnowski, Rosenberg, 1987</marker>
<rawString>Sejnowski, Terrence J. and Charles R. Rosenberg. 1987. Parrallel network that learn to pronounce English text. Complex Systems, 1:145-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K P H Sullivan</author>
<author>Robert I Damper</author>
</authors>
<title>Novelword pronunciation within a text-to-speech system.</title>
<date>1992</date>
<booktitle>Talking Machines,</booktitle>
<pages>183--195</pages>
<editor>In Gerard Bailly and Christian Benoit, editors,</editor>
<publisher>North Holland.</publisher>
<contexts>
<context position="1643" citStr="Sullivan and Damper, 1992" startWordPosition="243" endWordPosition="246">er-to-sound rules to pronounce previously unseen words (Coltheart, 1978; Coltheart et al., 1993). This view has been challenged by a number of authors (e.g. (Glushsko, 1981)), who claim that the pronunciation process of every word, familiar or unknown, could be accounted for in a unified framework. These single-route models crucially suggest that the pronunciation of unknown words results from the parallel activation of similar lexical items (the lexical neighbours). This idea has been tentatively implemented both into various symbolic analogy-based algorithms (e.g. (Dedina and Nusbaum, 1991; Sullivan and Damper, 1992)) and into connectionist pronunciation devices (e.g. (Seidenberg and McClelland, 1989)). The basic idea of these analogy-based models is to pronounce an unknown word x by recombining pronunciations of lexical items sharing common subparts with x. To illustrate this strategy, Dedina and Nussbaum show how the pronunciation of the sequence lop in the pseudo-word blope is analogized with the pronunciation of the same sequence in sloping. As there exists more than one way to recombine segments of lexical items, Dedina and Nussbaum&apos;s algorithm favors recombinations including large substrings of exis</context>
</contexts>
<marker>Sullivan, Damper, 1992</marker>
<rawString>Sullivan, K.P.H and Robert I. Damper. 1992. Novelword pronunciation within a text-to-speech system. In Gerard Bailly and Christian Benoit, editors, Talking Machines, pages 183-195. North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kari Torkolla</author>
</authors>
<title>An efficient way to learn English grapheme-to-phoneme rules automatically.</title>
<date>1993</date>
<booktitle>In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<volume>2</volume>
<pages>199--202</pages>
<location>Minneapolis,</location>
<contexts>
<context position="25774" citStr="Torkolla, 1993" startWordPosition="4212" endWordPosition="4213">all. Indeed, if this neighbourhood can be quite large (typically 50 analogs) for short words, the number of analogs used in a pronunciation averages at about 9.5, which proves that our definition of a lexical neighbourhood is sufficiently restrictive. 4 Discussion and Perspectives 4.1 Related works A large number of procedures aiming at the automatic discovery of pronunciation &amp;quot;rules&amp;quot; have been proposed over the past few years: connectionist models (e.g. (Sejnowski and Rosenberg, 1987)), traditional symbolic machine learning techniques (induction of decision trees, k-nearest neighbours) e.g. (Torkolla, 1993; van den Bosch and Daelemans, 1993), as well as various recombination techniques (Yvon, 1996a). In these models, orthographical correspondances are primarily viewed as resulting from a. strict underlying phonographical system, where each grapheme encodes exactly one phoneme. This assumption is reflected by the possibility of aligning on a one-to-one basis graphemic and phonemic strings, and these models indeed use this kind of alignment to initiate learning. Under this view, the orthographical representation of individual words is strongly subject, to their phonological forms on an word per w</context>
</contexts>
<marker>Torkolla, 1993</marker>
<rawString>Torkolla, Kari. 1993. An efficient way to learn English grapheme-to-phoneme rules automatically. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume 2, pages 199-202, Minneapolis, Apr.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
<author>Walter Daelemans</author>
</authors>
<title>Data-oriented methods for grapheme-to-phoneme conversion.</title>
<date>1993</date>
<booktitle>In Proceedings of the European Chapter of the Association for Computational Linguistics (PA CL),</booktitle>
<pages>45--53</pages>
<location>Utrecht.</location>
<marker>van den Bosch, Daelemans, 1993</marker>
<rawString>van den Bosch, Antal and Walter Daelemans. 1993. Data-oriented methods for grapheme-to-phoneme conversion. In Proceedings of the European Chapter of the Association for Computational Linguistics (PA CL), pages 45-53, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Yvon</author>
</authors>
<title>Grapheme-to-phoneme conversion using multiple unbounded overlapping chunks.</title>
<date>1996</date>
<booktitle>In Proceedings of the conference on New Methods in Natural Language Processing (NeMLaP II),</booktitle>
<pages>218--228</pages>
<location>Ankara, Turkey.</location>
<contexts>
<context position="17698" citStr="Yvon, 1996" startWordPosition="2933" endWordPosition="2934">rch was associated with a more efficient procedure for reconstructing pronunciations (see below). Various pruning procedures have also been implemented in order to control the exponential growth of the stack. For example, one pruning procedure detects the most obvious derivation cycles, which generate in loops the same derivatives; another pruning procedure tries to detect commutating alternations: substituting the prefix p, and then the suffix s often produces the same analog than when alternations apply in the reverse order, etc. More details regarding implementational aspects are given in (Yvon, 1996b). If the search procedure returns an analog y Ai, k_l(... A1 (x))) in E, we can build a pronunciation for x, using the known pronunciation 0(y) of y. For this purpose, we will use our knowledge of the Bi,j, for i E {i1 ...ik}, and generate every possible transforms of 0(y) in the phonological domain: 1/3713. k(B71 (... (q5(y))))}, with jk in 11 ... nik 1, and order this set using some function of the pi,j. The top-ranked element in this set is the pronunciation of x. Of course, when the search fails, this procedure fails to propose any pronunciation. In fact, the results reported hereafter u</context>
<context position="23216" citStr="Yvon, 1996" startWordPosition="3813" endWordPosition="3814">to learn. With this new test set, the overall performances of our algorithm averages at about 54.5% of entirely correct words, corresponding to a 76% per phoneme correctness. If we keep the words that could not be pronounced at all (about 15% of the test set) apart from the evaluation, the per word and per phoneme precision improve considerably, reaching respectively 65% and 93%. Again, these precision results compare relatively well with the results achieved on the same corpus using other selflearning algorithms for grapheme-to-phoneme transcription (e.g. (van den Bosch and Da.elemans, 1993; Yvon, 1996a)), which, unlike ours, benefit from the knowledge of the alignment between graphemic and phonemic strings. Table 3 summaries the performance (in terms of per word correctness, silence, and precision) of various other pronunciation systems, namely PRONOUNCE (Dedina and Nusbaum, 1991), DEC (TorkoIla, 1993), SMPA (Yvon, I 996a). All these models have been tested using exactly the same evaluation procedure and data (see (Yvon, 1996b), which also contains an evolution perIbrmed with a. French database suggesting that this learning strategy effectively applies to other languages). Systei n corr. p</context>
<context position="25867" citStr="Yvon, 1996" startWordPosition="4226" endWordPosition="4227"> number of analogs used in a pronunciation averages at about 9.5, which proves that our definition of a lexical neighbourhood is sufficiently restrictive. 4 Discussion and Perspectives 4.1 Related works A large number of procedures aiming at the automatic discovery of pronunciation &amp;quot;rules&amp;quot; have been proposed over the past few years: connectionist models (e.g. (Sejnowski and Rosenberg, 1987)), traditional symbolic machine learning techniques (induction of decision trees, k-nearest neighbours) e.g. (Torkolla, 1993; van den Bosch and Daelemans, 1993), as well as various recombination techniques (Yvon, 1996a). In these models, orthographical correspondances are primarily viewed as resulting from a. strict underlying phonographical system, where each grapheme encodes exactly one phoneme. This assumption is reflected by the possibility of aligning on a one-to-one basis graphemic and phonemic strings, and these models indeed use this kind of alignment to initiate learning. Under this view, the orthographical representation of individual words is strongly subject, to their phonological forms on an word per word basis. The main task of a machinelearning algorithm is thus mainly to retrieve, on a. sta</context>
</contexts>
<marker>Yvon, 1996</marker>
<rawString>Yvon, Francois. 1996a. Grapheme-to-phoneme conversion using multiple unbounded overlapping chunks. In Proceedings of the conference on New Methods in Natural Language Processing (NeMLaP II), pages 218-228, Ankara, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francois Yvon</author>
</authors>
<title>Prononcer par analogie : motivations, formalisations et evaluations.</title>
<date>1996</date>
<booktitle>Ph.D. thesis, Ecole Nationale Superieure des Telecommunications,</booktitle>
<location>Paris.</location>
<contexts>
<context position="17698" citStr="Yvon, 1996" startWordPosition="2933" endWordPosition="2934">rch was associated with a more efficient procedure for reconstructing pronunciations (see below). Various pruning procedures have also been implemented in order to control the exponential growth of the stack. For example, one pruning procedure detects the most obvious derivation cycles, which generate in loops the same derivatives; another pruning procedure tries to detect commutating alternations: substituting the prefix p, and then the suffix s often produces the same analog than when alternations apply in the reverse order, etc. More details regarding implementational aspects are given in (Yvon, 1996b). If the search procedure returns an analog y Ai, k_l(... A1 (x))) in E, we can build a pronunciation for x, using the known pronunciation 0(y) of y. For this purpose, we will use our knowledge of the Bi,j, for i E {i1 ...ik}, and generate every possible transforms of 0(y) in the phonological domain: 1/3713. k(B71 (... (q5(y))))}, with jk in 11 ... nik 1, and order this set using some function of the pi,j. The top-ranked element in this set is the pronunciation of x. Of course, when the search fails, this procedure fails to propose any pronunciation. In fact, the results reported hereafter u</context>
<context position="23216" citStr="Yvon, 1996" startWordPosition="3813" endWordPosition="3814">to learn. With this new test set, the overall performances of our algorithm averages at about 54.5% of entirely correct words, corresponding to a 76% per phoneme correctness. If we keep the words that could not be pronounced at all (about 15% of the test set) apart from the evaluation, the per word and per phoneme precision improve considerably, reaching respectively 65% and 93%. Again, these precision results compare relatively well with the results achieved on the same corpus using other selflearning algorithms for grapheme-to-phoneme transcription (e.g. (van den Bosch and Da.elemans, 1993; Yvon, 1996a)), which, unlike ours, benefit from the knowledge of the alignment between graphemic and phonemic strings. Table 3 summaries the performance (in terms of per word correctness, silence, and precision) of various other pronunciation systems, namely PRONOUNCE (Dedina and Nusbaum, 1991), DEC (TorkoIla, 1993), SMPA (Yvon, I 996a). All these models have been tested using exactly the same evaluation procedure and data (see (Yvon, 1996b), which also contains an evolution perIbrmed with a. French database suggesting that this learning strategy effectively applies to other languages). Systei n corr. p</context>
<context position="25867" citStr="Yvon, 1996" startWordPosition="4226" endWordPosition="4227"> number of analogs used in a pronunciation averages at about 9.5, which proves that our definition of a lexical neighbourhood is sufficiently restrictive. 4 Discussion and Perspectives 4.1 Related works A large number of procedures aiming at the automatic discovery of pronunciation &amp;quot;rules&amp;quot; have been proposed over the past few years: connectionist models (e.g. (Sejnowski and Rosenberg, 1987)), traditional symbolic machine learning techniques (induction of decision trees, k-nearest neighbours) e.g. (Torkolla, 1993; van den Bosch and Daelemans, 1993), as well as various recombination techniques (Yvon, 1996a). In these models, orthographical correspondances are primarily viewed as resulting from a. strict underlying phonographical system, where each grapheme encodes exactly one phoneme. This assumption is reflected by the possibility of aligning on a one-to-one basis graphemic and phonemic strings, and these models indeed use this kind of alignment to initiate learning. Under this view, the orthographical representation of individual words is strongly subject, to their phonological forms on an word per word basis. The main task of a machinelearning algorithm is thus mainly to retrieve, on a. sta</context>
</contexts>
<marker>Yvon, 1996</marker>
<rawString>Yvon, Francois. 1996b. Prononcer par analogie : motivations, formalisations et evaluations. Ph.D. thesis, Ecole Nationale Superieure des Telecommunications, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>