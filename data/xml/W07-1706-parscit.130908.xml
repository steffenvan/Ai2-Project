<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.993411">
Towards the Automatic Extraction of Definitions in Slavic
</title>
<author confidence="0.845163">
&apos;Adam Przepi´orkowski
&apos;Łukasz Deg´orski
&apos;Beata W´ojtowicz
</author>
<affiliation confidence="0.998252">
Institute of Computer Science PAS
</affiliation>
<address confidence="0.868406">
Ordona 21, Warsaw, Poland
</address>
<email confidence="0.985715666666667">
adamp@ipipan.waw.pl
ldegorski@bach.ipipan.waw.pl
beataw@bach.ipipan.waw.pl
</email>
<author confidence="0.886705">
&apos;Kiril Simov
</author>
<affiliation confidence="0.865753">
5Petya Osenova
Institute for Parallel Processing BAS
Bonchev St. 25A, Sofia, Bulgaria
</affiliation>
<email confidence="0.8529645">
kivs@bultreebank.org
petya@bultreebank.org
</email>
<note confidence="0.3986205">
3Miroslav Spousta
&apos;Vladislav Kuboˇn
Charles University
Malostransk´e n´amˇest´ı 25
</note>
<address confidence="0.548052">
Prague, Czech Republic
</address>
<email confidence="0.979808">
spousta@ufal.ms.mff.cuni.cz
vk@ufal.ms.mff.cuni.cz
</email>
<author confidence="0.866498">
&apos;Lothar Lemnitzer
</author>
<affiliation confidence="0.991118">
University of T¨ubingen
</affiliation>
<address confidence="0.48918">
Wilhelmstr. 19, T¨ubingen, Germany
</address>
<email confidence="0.986789">
lothar@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.99289" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999665">
This paper presents the results of the prelim-
inary experiments in the automatic extrac-
tion of definitions (for semi-automatic glos-
sary construction) from usually unstructured
or only weakly structured e-learning texts
in Bulgarian, Czech and Polish. The ex-
traction is performed by regular grammars
over XML-encoded morphosyntactically-
annotated documents. The results are less
than satisfying and we claim that the rea-
son for that is the intrinsic difficulty of the
task, as measured by the low interannota-
tor agreement, which calls for more sophis-
ticated deeper linguistic processing, as well
as for the use of machine learning classifica-
tion techniques.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9482732">
The aim of this paper is to report on the preliminary
results of a subtask of the European Project Lan-
guage Technology for eLearning (http://www.
lt4el.eu/) consisting in the identification of
term definitions in eLearning materials (Learning
</bodyText>
<page confidence="0.996817">
43
</page>
<bodyText confidence="0.999900074074074">
Objects; henceforth: LOs), where definitions are
understood pragmatically, as those text fragments
which may, after perhaps some minor editing, be
put into a glossary. Such automatically extracted
term definitions are to be presented to the author or
the maintainer of the LO and, thus, significantly fa-
cilitate and accelerate the creation of a glossary for
a given LO. From this specification of the task it fol-
lows that good recall is much more important than
good precision, as it is easier to reject wrong glos-
sary candidates than to browse the LO for term def-
initions which were not automatically spotted.
The project involves 9 European languages in-
cluding 3 Slavic (and, regrettably, no Baltic) lan-
guages: one South Slavic, i.e., Bulgarian, and two
West Slavic, i.e., Czech and Polish. For all lan-
guages, shallow grammars identifying definitions
have been constructed; after mentioning some previ-
ous work on Information Extraction (IE) for Slavic
languages and on extraction of definitions in sec-
tion 2, we briefly describe the three Slavic grammars
developed within this project in section 3. Section 4
presents the results of the application of these gram-
mars to LOs in respective languages. These results
are evaluated in section 5, where main problems, as
well as some possible solutions, are discussed. Fi-
nally, section 6 concludes the paper.
</bodyText>
<subsubsectionHeader confidence="0.719678">
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50,
</subsubsectionHeader>
<bodyText confidence="0.716546">
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999948515151515">
Definition extraction is an important NLP task,
most frequently a subtask of terminology extraction
(Pearson, 1996), the automatic creation of glossaries
(Klavans and Muresan, 2000; Klavans and Muresan,
2001), question answering (Miliaraki and Androut-
sopoulos, 2004; Fahmi and Bouma, 2006), learning
lexical semantic relations (Malais´e et al., 2004; Stor-
rer and Wellinghoff, 2006) and automatic construc-
tion of ontologies (Walter and Pinkal, 2006). Tools
for definition extraction are invariably language-
specific and involve shallow or deep processing,
with most work done for English (Pearson, 1996;
Klavans and Muresan, 2000; Klavans and Muresan,
2001) and other Germanic languages (Fahmi and
Bouma, 2006; Storrer and Wellinghoff, 2006; Wal-
ter and Pinkal, 2006), as well as French (Malais´e et
al., 2004). To the best of our knowledge, no previ-
ous attempts at definition extraction have been made
for Slavic, with the exception of some work on Bul-
garian (Tanev, 2004; Simov and Osenova, 2005).
Other work on Slavic information extraction has
been carried out mainly for the last 5 years. Prob-
ably the first forum where such work was compre-
hensively presented was the International Workshop
on Information Extraction for Slavonic and Other
Central and Eastern European Languages (IESL),
RANLP, Borovets, 2003, Bulgaria. One of the pa-
pers presented there, (Dro˙zd˙zy´nski et al., 2003), dis-
cusses shallow SProUT (Becker et al., 2002) gram-
mars for Czech, Polish and Lithuanian. SProUT has
subsequently been extensively used for the informa-
tion extraction from Polish medical texts (Piskorski
et al., 2004; Marciniak et al., 2005).1
</bodyText>
<sectionHeader confidence="0.962936" genericHeader="method">
3 Shallow Grammars for Definition
Extraction
</sectionHeader>
<bodyText confidence="0.946134666666667">
The input to the task of definition extraction is
XML-encoded morphosyntactically-annotated text,
possibly with some keywords already marked by an
</bodyText>
<footnote confidence="0.823828875">
1SProUT has not been seriously considered for the task at
hand for two reasons: first, it was decided that only open source
tools will be used in the current project, if only available, sec-
ond, the input format to the current task is morphosyntactically-
annotated XML-encoded text, rather than raw text, as normally
expected by SProUT. The second obstacle could be removed by
converting input texts to the SProUT-internal XML representa-
tion.
</footnote>
<bodyText confidence="0.99966525">
independent process. For example, the representa-
tion of a Polish sentence starting as Konstruktywizm
kładzie nacisk na (Eng. “Constructivism puts em-
phasis on”) may be as follows:2
</bodyText>
<equation confidence="0.991665">
&lt;s id=&amp;quot;s9&amp;quot;&gt;
&lt;markedTerm id=&amp;quot;mt7&amp;quot; kw=&amp;quot;y&amp;quot;&gt;
&lt;tok base=&amp;quot;konstruktywizm&amp;quot; ctag=&amp;quot;subst&amp;quot;
id=&amp;quot;t253&amp;quot;
msd=&amp;quot;sg:nom:m3&amp;quot;&gt;Konstruktywizm&lt;/tok&gt;
&lt;/markedTerm&gt;
&lt;tok base=&amp;quot;klasc&amp;quot; ctag=&amp;quot;fin&amp;quot; id=&amp;quot;t254&amp;quot;
msd=&amp;quot;sg:ter:imperf&amp;quot;&gt;kladzie&lt;/tok&gt;
&lt;tok base=&amp;quot;nacisk&amp;quot; ctag=&amp;quot;subst&amp;quot; id=&amp;quot;t255&amp;quot;
msd=&amp;quot;sg:acc:m3&amp;quot;&gt;nacisk&lt;/tok&gt;
&lt;tok base=&amp;quot;na&amp;quot; ctag=&amp;quot;prep&amp;quot; id=&amp;quot;t256&amp;quot;
msd=&amp;quot;acc&amp;quot;&gt;na&lt;/tok&gt;
[...]
&lt;tok base=&amp;quot;.&amp;quot; ctag=&amp;quot;interp&amp;quot; id=&amp;quot;t273&amp;quot;&gt;.
&lt;/tok&gt;
&lt;/s&gt;
</equation>
<bodyText confidence="0.993026636363636">
For each language, definitions were manually
marked in two batches of texts: the first batch, con-
sulted during the process of grammar development,
contained at least 300 definitions, and the second
batch, held out for evaluation, contained about 150
definitions. All grammars are regular grammars im-
plemented with the use of the lxtransduce tool
(Tobin, 2005), a component of the LTXML2 toolset
developed at the University of Edinburgh.3 An ex-
ample of a simple rule for prepositional phrases is
given below:
</bodyText>
<equation confidence="0.928512125">
&lt;rule name=&amp;quot;PP&amp;quot;&gt;
&lt;seq&gt;
&lt;query match=&amp;quot;tok[@ctag = ’prep’]&amp;quot;/&gt;
&lt;ref name=&amp;quot;NP1&amp;quot;&gt;
&lt;with-param name=&amp;quot;case&amp;quot; value=&amp;quot;”&amp;quot;/&gt;
&lt;/ref&gt;
&lt;/seq&gt;
&lt;/rule&gt;
</equation>
<bodyText confidence="0.999742142857143">
This rule identifies a sequence whose first element
is a token tagged as a preposition and whose subse-
quent elements are identified by a rule called NP1.
This latter rule (not shown here for brevity) is a pa-
rameterised rule which finds a nominal phrase of a
given case, but the way it is called above ensures that
it will find an NP of any case.
</bodyText>
<footnote confidence="0.994828857142857">
2Part of the representation has been replaced by ‘[...]’.
3Among the tools considered here were also CLaRK (Simov
et al., 2001), ultimately rejected because it currently does not
work in batch mode, and GATE / JAPE (Cunningham et al.,
2002), not used here because we found GATE’s handling of
previously XML-annotated texts rather cumbersome and ill-
documented. Cf. also fn. 1.
</footnote>
<page confidence="0.999531">
44
</page>
<bodyText confidence="0.999963">
Currently the grammars show varying degrees of
sophistication, with a small Bulgarian grammar (8
rules in a 2.5-kilobyte file), a larger Polish grammar
(34 rules in a 11 KiB file) and a sophisticated Czech
grammar most developed (147 rules in a 28 KiB
file). The patterns defined by these three grammars
are similar, but sufficiently different to defy an at-
tempt to write a single parameterised grammar.4 The
remainder of this section briefly describes the gram-
mars.
</bodyText>
<subsectionHeader confidence="0.998637">
3.1 Bulgarian
</subsectionHeader>
<bodyText confidence="0.999223">
The Bulgarian grammar is manually constructed af-
ter examination of the manually annotated defini-
tions. Here is a list of the rule schemata, together
with the number and percentage of matching defini-
tions:
</bodyText>
<equation confidence="0.7336875">
Pattern
NP is NP
NP verb NP
NP - NP
</equation>
<bodyText confidence="0.436079">
This is NP
It represents NP
other patterns
</bodyText>
<tableCaption confidence="0.993594">
Table 1: Bulgarian definition types
</tableCaption>
<bodyText confidence="0.996682894736842">
In the second schema above, “verb” is a verb or
a verb phrase (not necessarily a constituent) which
is one of the following: ‘представлява’ (to repre-
sent), ‘показва’ (to show), ‘означава’ (to mean),
‘описва’ (to describe), ‘се използва’ (to be used),
‘позволява’ (to allow), ‘дава възможност да’
(to give opportunity), ‘се нарича’ (is called),
‘подобрява’ (to improve), ‘осигурява’ (to ensure),
‘служи за’ (to serve as), ‘се разбира’ (to be under-
stood as), ‘обозначава’ (to denote), ‘съдържа’ (to
contain), ‘определя’ (to determine), ‘включва’
(to include), ‘се дефинира като’ (is defined as),
‘се основава на’ (is based on).
We classify the rules in five types: copula defi-
nitions, copula definitions with anaphoric relation,
copula definitions with ellipsis of the copula, defi-
nitions with a verb phrase, definitions with a verb
4Because of this relative language-dependence of definition
patters, which includes, e.g., idiosyncratic case information,
we have not seriously considered re-using rules for other, non-
Slavic, languages.
phrase and anaphoric relation. Each of these types of
definitions defines an NP (sometimes via anaphoric
relation) by another one. There are some variations
of the models where some parenthetical expressions
are presented in the definition.
The grammar contains several most important
rules for each type. The different verb patterns are
encoded as a lexicon. For some of the rules, variants
with parenthetical phrases are also encoded. The rest
of the grammar is devoted to the recognition of noun
phrases and parenthetical phrases. For parentheti-
cal phrases, we have encoded a list of such possible
phrases, extracted on the basis of a bigger corpus.
The NP grammar in our view is the crucial grammar
for recognition of the definitions. Most work now
has to be invested into developing the more complex
and recursive NPs.
</bodyText>
<subsectionHeader confidence="0.998689">
3.2 Czech
</subsectionHeader>
<bodyText confidence="0.999933307692308">
The Czech grammar for definition context extraction
is constructed to follow both linguistic intuition and
observation of common patterns in manually anno-
tated data.
We adapted a grammar5 based mainly on the ob-
servation of Czech Wikipedia entries. Encyclopedia
definitions are usually clear and very well structured,
but it is quite difficult to find such well-formed defi-
nitions in common texts, including learning objects.
The rules were extended using part of our manually
annotated texts, evaluated and adjusted in several it-
erations, based on the observation of the annotated
data.
</bodyText>
<equation confidence="0.815092">
Pattern
NP is/are NP
NP verb NP
structural
NP (NP)
NP -/:/= NP
other patterns
</equation>
<tableCaption confidence="0.927319">
Table 2: Czech definition types
</tableCaption>
<bodyText confidence="0.999891">
There are 21 top level rules, divided into five cate-
gories. Most of the correctly marked definitions fall
into the copula verb (‘is/are’) category. The sec-
</bodyText>
<footnote confidence="0.7533555">
5The grammar was originally developed by Nguyen Thu
Trang.
</footnote>
<figure confidence="0.999494714285714">
# %
140 34.2
18 29.8
21 5.0
15 3.7
4 1.0
107 26.2
# %
52 21.2
45 18.4
39 15.9
30 12.2
20 8.2
59 24.1
</figure>
<page confidence="0.995135">
45
</page>
<bodyText confidence="0.999964285714286">
ond most successful rule is the one using selected
verbs like ‘definuje’ (defines), ‘znamená’ (means),
‘vymezuje’ (delimits), ‘pˇredstavuje’ (presents) and
several others. The remaining categories make use
of the typical patterns of characters (dash, colon,
equal sign and brackets) or additional structural in-
formation (e.g., HTML tags).
</bodyText>
<subsectionHeader confidence="0.995341">
3.3 Polish
</subsectionHeader>
<bodyText confidence="0.998114857142857">
The Polish grammar rules are divided into three lay-
ers. Similarly to the Czech grammar, each layer only
refers to itself or lower layers. This allows for ex-
pressing top level rules in a clear and easily man-
ageable way.
The top level layer consists of rules representing
typical patterns found in Polish documents:
</bodyText>
<table confidence="0.99945525">
Pattern # %
NP (...) are/is NP-INS 40 15.6
NP -/: NP 39 15.2
NP (are/is) to NP-NOM 27 10.6
NP VP-3PERS 25 9.8
NP - i.e./or WH-question 11 4.3
N ADJ - PPAS 8 3.1
NP, i.e./or NP 7 2.7
NP-ACC one may 5 2.0
describe/define as NP-ACC
other patterns 94 36.7
(not in the grammar)
</table>
<tableCaption confidence="0.999503">
Table 3: Polish definition types
</tableCaption>
<bodyText confidence="0.999528428571429">
The middle layer consists of rules catching pat-
terns such as “simple NP in given case, followed by
a sequence of non-punctuation elements” or “cop-
ula”.
The bottom layer rules basically only refer to
POS markup in the input files (or other bottom layer
rules).
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.997335480769231">
As mentioned above, the testing corpus for each lan-
guage consists of about 150 definitions, unseen dur-
ing the construction of the grammar.6
6Obviously, three different corpora had to be used to eval-
uate the grammars for the three languages, but the corpora are
similar in size and character, so any differences in results stem
mostly from the differences in the three grammars.
The Bulgarian test corpus, containing around
76,800 tokens, consists of the third part of the
Calimera guidelines (http://www.calimera.
org/). We view this document as appropriate for
testing because it reflects the chosen domain and it
combines definitions from otherwise different sub-
domains, such as XML language, Internet usage,
etc. There are 203 manually annotated definitions
in this corpus: 129 definitions contained in one sen-
tence, 69 definitions split across 2 sentences, 4 def-
initions in 3 sentences and one definition in 4 sen-
tences. Note that the real test part is the set of the
129 definitions in one sentence, since the Bulgar-
ian grammar does not consider cross-sentence def-
initions in any way.
Czech data used for evaluation consist of several
chapters of the Calimera guidelines and Microsoft
Excel tutorial. The tutorial is a typical text used
in e-learning, consisting of five chapters describing
sheets, tables, formating, graphs and lists. The cor-
pus consists of over 90,000 tokens and contains 162
definitions, out of which 153 are contained in a sin-
gle sentence, 6 span 2 sentences, and 3 definitions
span 3 sentences.
Polish test corpus consists of over 83,200 tokens
containing 157 definitions: 148 definitions are con-
tained within one sentence, while 9 span 2 sen-
tences. The corpus is made up of 10 chapters of a
popular introduction to and history of computer sci-
ence and computer hardware.
Each grammar was quantitatively evaluated by
comparing manually annotated files with the same
files annotated automatically by the grammar. After
considering various ways of quantitative evaluation,
we decided to do the comparison at token level: pre-
cision was calculated as the ratio of the number of
those tokens which were parts of both a manually
marked definition and an automatically discovered
definition to the number of all tokens in automati-
cally discovered definitions, while recall was taken
to be the ratio of the number of tokens simultane-
ously in both kinds of definitions to the number of
tokens in all manually annotated definitions. Since,
for this task, recall is more important than precision,
we used the F2-measure for the combined result.7
</bodyText>
<footnote confidence="0.982061">
7In general, F. = (1 + α) · (precision · recall)/(α ·
precision+recall). Perhaps α larger than 2 could be used, but it
is currently not clear to us what criteria should be assumed when
</footnote>
<page confidence="0.999714">
46
</page>
<tableCaption confidence="0.981166">
The results for the three grammars are given in
Table 4. Note that the processing model for Czech
</tableCaption>
<table confidence="0.99996325">
precision recall F2
Bulgarian 20.5% 2.2% 3.1
Czech 18.3% 40.7% 28.9
Polish 14.8% 22.2% 19.0
</table>
<tableCaption confidence="0.9471795">
Table 4: Token-based evaluation of shallow gram-
mars
</tableCaption>
<bodyText confidence="0.99172525">
differs from the other two languages, as the input
text is converted to a flat format, as described in sec-
tion 5.3, and grammar rules are sensitive to sentence
boundaries (and may operate over them).
</bodyText>
<sectionHeader confidence="0.992186" genericHeader="evaluation">
5 Evaluation and Possible Improvements
</sectionHeader>
<subsectionHeader confidence="0.641029">
5.1 Interannotator Agreement
</subsectionHeader>
<bodyText confidence="0.998804">
We calculated Cohen’s kappa statistic (1) for the cur-
rent task, where both the relative observed agree-
ment among raters Pr(a) and the probability that
agreement is due to chance Pr(e) where calculated
at token level.
</bodyText>
<equation confidence="0.9988755">
Pr(a) − Pr(e) � = (1)
1 − Pr(e)
</equation>
<bodyText confidence="0.999986636363636">
More specifically, we assumed that two annotators
agree on a token if the token belongs to a definition
either according to both annotations or according to
neither. In order to estimate the probability of agree-
ment due to chance Pr(e), we measured, separately
for each annotator, the proportion of tokens found in
definitions to all tokens in text, which resulted in two
probability estimates p1 and p2, and treated Pr(e) as
the probability that the two annotators agree if they
randomly, with their own probability, classify a to-
ken as belonging to a definition, i.e.:
</bodyText>
<equation confidence="0.999936">
Pr(e) = p1 · p2 + (1 − p1) · (1 − p2) (2)
</equation>
<bodyText confidence="0.9982742">
The interannotator agreement (IAA) was mea-
sured this way for Czech and Polish, where — for
each language — the respective test corpus was an-
notated by two annotators. The results are 0.44 for
Czech and 0.31 for Polish. Such results are very low
for any classification task, and especially low for a
deciding on the exact value of α. Note that it would not make
sense to use recall alone, as it is trivial to write all-accepting
grammars with 100% recall.
binary classification task. They show that the task of
identifying definitions in running texts and agreeing
on which parts of text count as a definition is intrin-
sically very difficult. They also call for the recon-
sideration of the evaluation and IAA measurement
methodology based on token classification.8
</bodyText>
<subsectionHeader confidence="0.996542">
5.2 Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.9999461875">
To the best of our knowledge, there is no estab-
lished evaluation methodology for the task of def-
inition extraction, where definitions may span sev-
eral sentences.9 For this reason we evaluated the re-
sults again, in a different way: we treated an auto-
matically discovered definition as correct, if it over-
lapped with a manually annotated definition. We
calculated precision as the number of automatic defi-
nitions overlapping with manual definitions, divided
by the number of automatic definitions, while re-
call — as the number of manual definitions overlap-
ping automatic definitions, divided by the number of
manual definitions.10
The results for the three grammars, given in Ta-
ble 5, are much higher than those in Table 4 above,
although still less than satisfactory.
</bodyText>
<table confidence="0.9997135">
precision recall F2
Bulgarian 22.5% 8.9% 11.1
Czech 22.3% 46% 33.9
Polish 23.3% 32% 28.4
</table>
<tableCaption confidence="0.9493475">
Table 5: Definition-based evaluation of shallow
grammars
</tableCaption>
<subsectionHeader confidence="0.996232">
5.3 Definitions and Sentence Boundaries
</subsectionHeader>
<bodyText confidence="0.8615129375">
Regardless of the inherent difficulties of the task and
difficulties with the evaluation of the results, there
is clear room for improvement; one possible path
8A better approximation would be to measure IAA on the
basis of sentence or (as suggested by an anonymous reviewer)
NP classification; we intend to pursue this idea in future work.
9With the assumption that definitions are no longer than
a sentence, usually the task is treated as a classification task,
where sentences are classified as definitional or not, and ap-
propriate precision and recall measures are applied at sentence
level.
10At this stage definition fragments distributed across a num-
ber of different sentences were treated as different definitions,
which negatively affects the evaluation of the Bulgarian gram-
mar, as the Bulgarian test corpus contains a large number of
multi-sentence definitions.
</bodyText>
<page confidence="0.998588">
47
</page>
<bodyText confidence="0.999468875">
to explore concerns multi-sentence definitions. As
noted above, for all languages considered here, there
were definitions which were spanning 2 or more sen-
tences; this turned out to be a problem especially for
Bulgarian, were 36% of definitions crossed a sen-
tence boundary.11
Such multi-sentence definitions are a problem be-
cause in the DTD adopted in this project definitions
are subelements of sentences rather than the other
way round. In case of a multi-sentence definition,
for each sentence there is a separate element en-
capsulating the part of the definition contained in
this sentence. Although these are linked via spe-
cial attributes and the information that they are part
of the same definition can subsequently be recov-
ered, it is difficult to construct an lxtransduce
grammar which would be able to automatically mark
such multi-sentence definitions: an lxtransduce
grammar expects to find a sequence of elements and
wrap them in a single larger element.
A solution to this technical problem has been im-
plemented in the Czech grammar, where first the in-
put text is flattened (via an XSLT script), so that,
e.g.:
</bodyText>
<figure confidence="0.559545904761905">
&lt;par id=&amp;quot;d1p2&amp;quot;&gt;
&lt;s id=&amp;quot;d1p2s1&amp;quot;&gt;
&lt;tok id=&amp;quot;d1p2s1t1&amp;quot; base=&amp;quot;Pavel&amp;quot;
ctag=&amp;quot;N&amp;quot; msd=&amp;quot;NMS1 A----&amp;quot;&gt;
Pavel&lt;/tok&gt;
&lt;tok id=&amp;quot;d1p2s1t2&amp;quot; base=&amp;quot;satrapa&amp;quot;
ctag=&amp;quot;N&amp;quot; msd=&amp;quot;NMS1 A----&amp;quot;&gt;
Satrapa&lt;/tok&gt;
&lt;/s&gt;
&lt;/par&gt;
becomes:
&lt;par id=&amp;quot;Sd1p2&amp;quot;/&gt;
&lt;s id=&amp;quot;Sd1p2s1&amp;quot;/&gt;
&lt;tok id=&amp;quot;d1p2s1t1&amp;quot; base=&amp;quot;Pavel&amp;quot;
ctag=&amp;quot;N&amp;quot; msd=&amp;quot;NMS1 A----&amp;quot;&gt;
Pavel&lt;/tok&gt;
&lt;tok id=&amp;quot;d1p2s1t2&amp;quot; base=&amp;quot;satrapa&amp;quot;
ctag=&amp;quot;N&amp;quot; msd=&amp;quot;NMS1 A----&amp;quot;&gt;
Satrapa&lt;/tok&gt;
&lt;s id=&amp;quot;Ed1p2s1&amp;quot;/&gt;
&lt;par id=&amp;quot;Ed1p2&amp;quot;/&gt;
</figure>
<bodyText confidence="0.9706724">
11An example of a Polish manually annotated multi-sentence
definition is: ... opracowano techniki antyspamowe. Tech-
niki te drastycznie zani˙zaja˛ warto´s´c strony albo ja˛ banuj ˛a...
(Eng. “...anti-spam techniques were developed. Such tech-
niques drastically lower the value of the page or they ban it... ”).
The definition is split into two fragments fully contained in re-
spective sentences: techniki antyspamowe and Techniki te....
No attempt at anaphora resolution is made.
This flattened representation is an input to a gram-
mar which is sensitive to the empty s and par ele-
ments and may discover definitions containing such
elements; in such a case, the postprocessing script,
which restores the hierarchical paragraph and sen-
tence structure, splits such definitions into smaller
elements, fully contained in respective sentences.
</bodyText>
<subsectionHeader confidence="0.997648">
5.4 Problems Specific to Slavic
</subsectionHeader>
<bodyText confidence="0.999976068965517">
At least in case of the two West Slavic languages
considered here, the task of writing a definition
grammar is intrinsically more difficult than for Ger-
manic or Romance languages, mainly for the follow-
ing two reasons.
First, Czech and Polish have very rich nominal
inflection with a large number of paradigm-internal
syncretisms. These syncretisms are a common cause
of tagger errors, which percolate to further stages of
processing. Moreover, the number of cases makes it
more difficult to encode patterns like “NP verb NP”,
as different verbs may combine with NPs of different
case. In fact, even two different copulas in Polish
take different cases!
Second, the relatively free word order increases
the number of rules that must be encoded, and makes
the grammar writing task more labour-intensive and
error-prone. The current version of the Polish gram-
mar, with 34 rules, is rather basic, and even the 147
rules of the Czech grammar do not take into consid-
eration all possible patterns of grammar definitions.
As Tables 4 and 5 show, there is a positive corre-
lation between the grammar size and the value of
F2, and the Bulgarian and Polish grammars certainly
have room to grow. Moreover, a path that is well
worth exploring is to drastically increase the num-
ber of rules and, hence, the recall, and then deal with
precision via Machine Learning methods (cf. sec-
tion 5.6).
</bodyText>
<subsectionHeader confidence="0.999062">
5.5 Levels of Linguistic Processing
</subsectionHeader>
<bodyText confidence="0.99990525">
The work reported here has been an excercise in
definition extraction using shallow parsing methods.
However, the poor results suggest that this is one
of the tasks that require a much more sophisticated
and deeper approach to language analysis. In fact,
in turns out that virtually all successful attempts at
definition extraction that we are aware of build on
worked-out deep linguistic approaches (Klavans and
</bodyText>
<page confidence="0.997262">
48
</page>
<bodyText confidence="0.997926434782608">
Muresan, 2000; Fahmi and Bouma, 2006; Walter
and Pinkal, 2006), some of them combining syn-
tactic and semantic information (Miliaraki and An-
droutsopoulos, 2004; Walter and Pinkal, 2006).
Unfortunately, for most Baltic and Slavic lan-
guages, such deep parsers are unavailable or have
not yet been extensively tested on real texts. One
exception is Czech, where a number of parsers were
already described and evaluated (on the Prague De-
pendency Treebank) in (Zeman, 2004, § 14.2); the
best of these parsers reach 80–85% accuracy.
For Polish, apart from a number of linguistically
motivated toy parsers, there is a possibly wide cov-
erage deep parser (Woli´nski, 2004), but it has not yet
been evaluated on naturally occurring texts. The sit-
uation is probably most dire for Bulgarian, although
there have been attempts at the induction of a depen-
dency parser from the BulTreeBank (Marinov and
Nivre, 2005; Chanev et al., 2006).
Nevertheless, if other possible paths of improve-
ment suggested in this section do not bring satisfac-
tory results, we plan to make an attempt at adapting
these parsers to the task at hand.
</bodyText>
<subsectionHeader confidence="0.672855">
5.6 Postprocessing: Machine Learning and
Keyword Identification
</subsectionHeader>
<bodyText confidence="0.953817333333333">
Various approaches to the machine learning treat-
ment of the task of classifying sentences or snippets
as definitions or non-definitions can be found, e.g.,
in (Miliaraki and Androutsopoulos, 2004; Fahmi
and Bouma, 2006) and references therein. In the
context of the present work, such methods may be
used to postprocess apparent definitions found at
earlier processing stages and decide which of them
are genuine definitions. For example, (Fahmi and
Bouma, 2006) report that a system trained on 2299
sentences, including 1366 definition sentences, may
increase the accuracy of a definition extraction tool
from 59% to around 90%.12
Another possible improvement may consist in,
again, aiming at very high recall and then using
an independent keyword detector to mark keywords
(and key phrases) in text and classifying as genuine
definitions those definitions, whose defined term has
been marked as a keyword.
12The numbers are so high “probably due to the fact that the
current corpus consists of encyclopedic material only” (Fahmi
and Bouma, 2006, fn. 4).
Whatever postprocessing technique or combina-
tion of techniques proves most efficient, it seems that
the linguistic processing should aim at high recall
rather than high precision, which further justifies the
use of the F2 measure for evaluation.13
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999936818181818">
To the best of our knowledge, this paper is the first
report on the task of definition extraction for a num-
ber of Slavic languages. It shows that the task is
intrinsically very difficult, which partially explains
the relatively low results obtained. It also calls atten-
tion to the fact that there is no established evaluation
methodology where possibly multi-sentence defini-
tions are involved and suggests what such method-
ology could amount to. Finally, the paper suggests
ways of improving the results, which we hope to fol-
low and report in the future.
</bodyText>
<sectionHeader confidence="0.996698" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993032111111111">
Markus Becker et al. 2002. SProUT — shallow process-
ing with typed feature structures and unification. In
Proceedings of the International Conference on NLP
(ICON 2002), Mumbai, India.
Sharon A. Caraballo. 2001. Automatic Construction of a
Hypernym-Labeled Noun Hierarchy from Text. Ph. D.
dissertation, Brown University.
Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-
toslav Marinov. 2006. Dependency conversion and
parsing of the BulTreeBank. In proceedings of the
LREC workshop Merging and Layering Linguistic In-
formation, Genoa, Italy.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Witold Droidiy´nski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkeviˇcius. 2003. Adapting SProUT to
processing Baltic and Slavonic languages. In Infor-
mation Extraction for Slavonic and Other Central and
Eastern European Languages, pp. 18–25.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications.
</reference>
<bodyText confidence="0.79178525">
13Note that the situation here is different than in the task of
acquiring hyponymic relations from texts, where high-precision
manual rules (Hearst, 1992) must be augmented with statistical
clustering methods to increase recall (Caraballo, 2001).
</bodyText>
<page confidence="0.998356">
49
</page>
<reference confidence="0.999887092105263">
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, Nantes, France.
Judith L. Klavans and Smaranda Muresan. 2000.
DEFINDER: Rule-based methods for the extraction of
medical terminology and their associated definitions
from on-line text. In Proceedings of the Annual Fall
Symposium of the American Medical Informatics As-
sociation.
Judith L. Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of AMIA Sym-
posium 2001.
Véronique Malaisé, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In S. Ananadiou and
P. Zweigenbaum, editors, COLING 2004 CompuTerm
2004: 3rd International Workshop on Computational
Terminology, pp. 55–62, Geneva, Switzerland. COL-
ING.
Małgorzata Marciniak, Agnieszka Mykowiecka, Anna
Kup´s´c, and Jakub Piskorski. 2005. Intelligent con-
tent extraction from Polish medical texts. In L. Bolc
et al., editors, Intelligent Media Technology for Com-
municative Intelligence, Second International Work-
shop, IMTCI2004, Warsaw, Poland, September 13-14,
2004, Revised Selected Papers, volume 3490 of Lec-
ture Notes in Computer Science, pp. 68–78. Springer-
Verlag.
Svetoslav Marinov and Joakim Nivre. 2005. A data-
driven parser for Bulgarian. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, pp. 89–100, Barcelona.
Spyridoula Miliaraki and Ion Androutsopoulos. 2004.
Learning to identify single-snippet answers to defi-
nition questions. In Proceedings of COLING 2004,
pp. 1360–1366, Geneva, Switzerland. COLING.
Jennifer Pearson. 1996. The expression of defini-
tions in specialised texts: a corpus-based analysis.
In M. Gellerstam et al., editors, Proceedings of the
Seventh Euralex International Congress, pp. 817–824,
Göteborg.
Jakub Piskorski et al. 2004. Information extraction for
Polish using the SProUT platform. In M. A. Kłopotek
et al., editors, Intelligent Information Processing and
Web Mining, pp. 227–236. Springer-Verlag, Berlin.
Kiril Simov and Petya Osenova. 2005. BulQA:
Bulgarian-Bulgarian Question Answering at CLEF
2005. In CLEF, pp. 517–526.
Kiril Simov et al. 2001. CLaRK — an XML-based sys-
tem for corpora development. In P. Rayson et al., edi-
tors, Proceedings of the Corpus Linguistics 2001 Con-
ference, pp. 558–560, Lancaster.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
German text corpora. In Proceedings of LREC 2006.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377–386. John Benjamins.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Stephan Walter and Manfred Pinkal. 2006. Automatic
extraction of definitions from German court decisions.
In Proceedings of the Workshop on Information Ex-
traction Beyond The Document, pp. 20–28, Sydney,
Australia. Association for Computational Linguistics.
Marcin Woli´nski. 2004. Komputerowa weryfikacja gra-
matyki ´Swidzi´nskiego. Ph. D. dissertation, ICS PAS,
Warsaw.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph. D. dissertation, Charles University,
Prague.
</reference>
<page confidence="0.997677">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.053267">
<title confidence="0.95702">Towards the Automatic Extraction of Definitions in Slavic</title>
<affiliation confidence="0.983859">Institute of Computer Science</affiliation>
<address confidence="0.94013">Ordona 21, Warsaw,</address>
<email confidence="0.996509">beataw@bach.ipipan.waw.pl</email>
<affiliation confidence="0.847554">Institute for Parallel Processing</affiliation>
<address confidence="0.870598">Bonchev St. 25A, Sofia,</address>
<email confidence="0.990399">petya@bultreebank.org</email>
<author confidence="0.283632">Charles</author>
<affiliation confidence="0.373819">Malostransk´e n´amˇest´ı</affiliation>
<address confidence="0.385556">Prague, Czech</address>
<email confidence="0.479896">vk@ufal.ms.mff.cuni.cz</email>
<affiliation confidence="0.934658">University of</affiliation>
<address confidence="0.780928">Wilhelmstr. 19, T¨ubingen,</address>
<email confidence="0.992355">lothar@sfs.uni-tuebingen.de</email>
<abstract confidence="0.998801470588235">This paper presents the results of the preliminary experiments in the automatic extraction of definitions (for semi-automatic glossary construction) from usually unstructured or only weakly structured e-learning texts in Bulgarian, Czech and Polish. The extraction is performed by regular grammars over XML-encoded morphosyntacticallyannotated documents. The results are less than satisfying and we claim that the reason for that is the intrinsic difficulty of the task, as measured by the low interannotator agreement, which calls for more sophisticated deeper linguistic processing, as well as for the use of machine learning classification techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Markus Becker</author>
</authors>
<title>SProUT — shallow processing with typed feature structures and unification.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on NLP (ICON 2002),</booktitle>
<location>Mumbai, India.</location>
<marker>Becker, 2002</marker>
<rawString>Markus Becker et al. 2002. SProUT — shallow processing with typed feature structures and unification. In Proceedings of the International Conference on NLP (ICON 2002), Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic Construction of a Hypernym-Labeled Noun Hierarchy from Text.</title>
<date>2001</date>
<tech>Ph. D. dissertation,</tech>
<institution>Brown University.</institution>
<marker>Caraballo, 2001</marker>
<rawString>Sharon A. Caraballo. 2001. Automatic Construction of a Hypernym-Labeled Noun Hierarchy from Text. Ph. D. dissertation, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atanas Chanev</author>
<author>Kiril Simov</author>
<author>Petya Osenova</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Dependency conversion and parsing of the BulTreeBank.</title>
<date>2006</date>
<booktitle>In proceedings of the LREC workshop Merging and Layering Linguistic Information,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="24130" citStr="Chanev et al., 2006" startWordPosition="3805" endWordPosition="3808">ively tested on real texts. One exception is Czech, where a number of parsers were already described and evaluated (on the Prague Dependency Treebank) in (Zeman, 2004, § 14.2); the best of these parsers reach 80–85% accuracy. For Polish, apart from a number of linguistically motivated toy parsers, there is a possibly wide coverage deep parser (Woli´nski, 2004), but it has not yet been evaluated on naturally occurring texts. The situation is probably most dire for Bulgarian, although there have been attempts at the induction of a dependency parser from the BulTreeBank (Marinov and Nivre, 2005; Chanev et al., 2006). Nevertheless, if other possible paths of improvement suggested in this section do not bring satisfactory results, we plan to make an attempt at adapting these parsers to the task at hand. 5.6 Postprocessing: Machine Learning and Keyword Identification Various approaches to the machine learning treatment of the task of classifying sentences or snippets as definitions or non-definitions can be found, e.g., in (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006) and references therein. In the context of the present work, such methods may be used to postprocess apparent definitions found</context>
</contexts>
<marker>Chanev, Simov, Osenova, Marinov, 2006</marker>
<rawString>Atanas Chanev, Kiril Simov, Petya Osenova, and Svetoslav Marinov. 2006. Dependency conversion and parsing of the BulTreeBank. In proceedings of the LREC workshop Merging and Layering Linguistic Information, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7169" citStr="Cunningham et al., 2002" startWordPosition="1052" endWordPosition="1055">se&amp;quot; value=&amp;quot;”&amp;quot;/&gt; &lt;/ref&gt; &lt;/seq&gt; &lt;/rule&gt; This rule identifies a sequence whose first element is a token tagged as a preposition and whose subsequent elements are identified by a rule called NP1. This latter rule (not shown here for brevity) is a parameterised rule which finds a nominal phrase of a given case, but the way it is called above ensures that it will find an NP of any case. 2Part of the representation has been replaced by ‘[...]’. 3Among the tools considered here were also CLaRK (Simov et al., 2001), ultimately rejected because it currently does not work in batch mode, and GATE / JAPE (Cunningham et al., 2002), not used here because we found GATE’s handling of previously XML-annotated texts rather cumbersome and illdocumented. Cf. also fn. 1. 44 Currently the grammars show varying degrees of sophistication, with a small Bulgarian grammar (8 rules in a 2.5-kilobyte file), a larger Polish grammar (34 rules in a 11 KiB file) and a sophisticated Czech grammar most developed (147 rules in a 28 KiB file). The patterns defined by these three grammars are similar, but sufficiently different to defy an attempt to write a single parameterised grammar.4 The remainder of this section briefly describes the gram</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Witold Droidiy´nski</author>
<author>Petr Homola</author>
<author>Jakub Piskorski</author>
<author>Vytautas Zinkeviˇcius</author>
</authors>
<title>Adapting SProUT to processing Baltic and Slavonic languages.</title>
<date>2003</date>
<booktitle>In Information Extraction for Slavonic and Other Central and Eastern European Languages,</booktitle>
<pages>18--25</pages>
<marker>Droidiy´nski, Homola, Piskorski, Zinkeviˇcius, 2003</marker>
<rawString>Witold Droidiy´nski, Petr Homola, Jakub Piskorski, and Vytautas Zinkeviˇcius. 2003. Adapting SProUT to processing Baltic and Slavonic languages. In Information Extraction for Slavonic and Other Central and Eastern European Languages, pp. 18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismail Fahmi</author>
<author>Gosse Bouma</author>
</authors>
<title>Learning to identify definitions using syntactic features.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 workshop on Learning Structured Information in Natural Language Applications.</booktitle>
<contexts>
<context position="3384" citStr="Fahmi and Bouma, 2006" startWordPosition="483" endWordPosition="486">pective languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have</context>
<context position="23246" citStr="Fahmi and Bouma, 2006" startWordPosition="3661" endWordPosition="3664">ly increase the number of rules and, hence, the recall, and then deal with precision via Machine Learning methods (cf. section 5.6). 5.5 Levels of Linguistic Processing The work reported here has been an excercise in definition extraction using shallow parsing methods. However, the poor results suggest that this is one of the tasks that require a much more sophisticated and deeper approach to language analysis. In fact, in turns out that virtually all successful attempts at definition extraction that we are aware of build on worked-out deep linguistic approaches (Klavans and 48 Muresan, 2000; Fahmi and Bouma, 2006; Walter and Pinkal, 2006), some of them combining syntactic and semantic information (Miliaraki and Androutsopoulos, 2004; Walter and Pinkal, 2006). Unfortunately, for most Baltic and Slavic languages, such deep parsers are unavailable or have not yet been extensively tested on real texts. One exception is Czech, where a number of parsers were already described and evaluated (on the Prague Dependency Treebank) in (Zeman, 2004, § 14.2); the best of these parsers reach 80–85% accuracy. For Polish, apart from a number of linguistically motivated toy parsers, there is a possibly wide coverage dee</context>
<context position="24603" citStr="Fahmi and Bouma, 2006" startWordPosition="3878" endWordPosition="3881">garian, although there have been attempts at the induction of a dependency parser from the BulTreeBank (Marinov and Nivre, 2005; Chanev et al., 2006). Nevertheless, if other possible paths of improvement suggested in this section do not bring satisfactory results, we plan to make an attempt at adapting these parsers to the task at hand. 5.6 Postprocessing: Machine Learning and Keyword Identification Various approaches to the machine learning treatment of the task of classifying sentences or snippets as definitions or non-definitions can be found, e.g., in (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006) and references therein. In the context of the present work, such methods may be used to postprocess apparent definitions found at earlier processing stages and decide which of them are genuine definitions. For example, (Fahmi and Bouma, 2006) report that a system trained on 2299 sentences, including 1366 definition sentences, may increase the accuracy of a definition extraction tool from 59% to around 90%.12 Another possible improvement may consist in, again, aiming at very high recall and then using an independent keyword detector to mark keywords (and key phrases) in text and classifying as</context>
</contexts>
<marker>Fahmi, Bouma, 2006</marker>
<rawString>Ismail Fahmi and Gosse Bouma. 2006. Learning to identify definitions using syntactic features. In Proceedings of the EACL 2006 workshop on Learning Structured Information in Natural Language Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Computational Linguistics,</booktitle>
<location>Nantes, France.</location>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the Fourteenth International Conference on Computational Linguistics, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>Smaranda Muresan</author>
</authors>
<title>DEFINDER: Rule-based methods for the extraction of medical terminology and their associated definitions from on-line text.</title>
<date>2000</date>
<booktitle>In Proceedings of the Annual Fall Symposium of the American Medical Informatics Association.</booktitle>
<contexts>
<context position="3275" citStr="Klavans and Muresan, 2000" startWordPosition="468" endWordPosition="471">hin this project in section 3. Section 4 presents the results of the application of these grammars to LOs in respective languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as Fre</context>
</contexts>
<marker>Klavans, Muresan, 2000</marker>
<rawString>Judith L. Klavans and Smaranda Muresan. 2000. DEFINDER: Rule-based methods for the extraction of medical terminology and their associated definitions from on-line text. In Proceedings of the Annual Fall Symposium of the American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith L Klavans</author>
<author>Smaranda Muresan</author>
</authors>
<title>Evaluation of the DEFINDER system for fully automatic glossary construction.</title>
<date>2001</date>
<booktitle>In Proceedings of AMIA Symposium</booktitle>
<contexts>
<context position="3303" citStr="Klavans and Muresan, 2001" startWordPosition="472" endWordPosition="475"> 3. Section 4 presents the results of the application of these grammars to LOs in respective languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004).</context>
</contexts>
<marker>Klavans, Muresan, 2001</marker>
<rawString>Judith L. Klavans and Smaranda Muresan. 2001. Evaluation of the DEFINDER system for fully automatic glossary construction. In Proceedings of AMIA Symposium 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Véronique Malaisé</author>
<author>Pierre Zweigenbaum</author>
<author>Bruno Bachimont</author>
</authors>
<title>Detecting semantic relations between terms in definitions.</title>
<date>2004</date>
<booktitle>COLING 2004 CompuTerm 2004: 3rd International Workshop on Computational Terminology,</booktitle>
<pages>55--62</pages>
<editor>In S. Ananadiou and P. Zweigenbaum, editors,</editor>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland.</location>
<marker>Malaisé, Zweigenbaum, Bachimont, 2004</marker>
<rawString>Véronique Malaisé, Pierre Zweigenbaum, and Bruno Bachimont. 2004. Detecting semantic relations between terms in definitions. In S. Ananadiou and P. Zweigenbaum, editors, COLING 2004 CompuTerm 2004: 3rd International Workshop on Computational Terminology, pp. 55–62, Geneva, Switzerland. COLING.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Małgorzata Marciniak</author>
<author>Agnieszka Mykowiecka</author>
<author>Anna Kup´s´c</author>
<author>Jakub Piskorski</author>
</authors>
<title>Intelligent content extraction from Polish medical texts.</title>
<date>2005</date>
<booktitle>Intelligent Media Technology for Communicative Intelligence, Second International Workshop, IMTCI2004,</booktitle>
<volume>3490</volume>
<pages>68--78</pages>
<editor>In L. Bolc et al., editors,</editor>
<publisher>SpringerVerlag.</publisher>
<location>Warsaw, Poland,</location>
<marker>Marciniak, Mykowiecka, Kup´s´c, Piskorski, 2005</marker>
<rawString>Małgorzata Marciniak, Agnieszka Mykowiecka, Anna Kup´s´c, and Jakub Piskorski. 2005. Intelligent content extraction from Polish medical texts. In L. Bolc et al., editors, Intelligent Media Technology for Communicative Intelligence, Second International Workshop, IMTCI2004, Warsaw, Poland, September 13-14, 2004, Revised Selected Papers, volume 3490 of Lecture Notes in Computer Science, pp. 68–78. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetoslav Marinov</author>
<author>Joakim Nivre</author>
</authors>
<title>A datadriven parser for Bulgarian.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>89--100</pages>
<location>Barcelona.</location>
<contexts>
<context position="24108" citStr="Marinov and Nivre, 2005" startWordPosition="3801" endWordPosition="3804"> have not yet been extensively tested on real texts. One exception is Czech, where a number of parsers were already described and evaluated (on the Prague Dependency Treebank) in (Zeman, 2004, § 14.2); the best of these parsers reach 80–85% accuracy. For Polish, apart from a number of linguistically motivated toy parsers, there is a possibly wide coverage deep parser (Woli´nski, 2004), but it has not yet been evaluated on naturally occurring texts. The situation is probably most dire for Bulgarian, although there have been attempts at the induction of a dependency parser from the BulTreeBank (Marinov and Nivre, 2005; Chanev et al., 2006). Nevertheless, if other possible paths of improvement suggested in this section do not bring satisfactory results, we plan to make an attempt at adapting these parsers to the task at hand. 5.6 Postprocessing: Machine Learning and Keyword Identification Various approaches to the machine learning treatment of the task of classifying sentences or snippets as definitions or non-definitions can be found, e.g., in (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006) and references therein. In the context of the present work, such methods may be used to postprocess appa</context>
</contexts>
<marker>Marinov, Nivre, 2005</marker>
<rawString>Svetoslav Marinov and Joakim Nivre. 2005. A datadriven parser for Bulgarian. In Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories, pp. 89–100, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyridoula Miliaraki</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Learning to identify single-snippet answers to definition questions.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>1360--1366</pages>
<location>Geneva, Switzerland. COLING.</location>
<contexts>
<context position="3360" citStr="Miliaraki and Androutsopoulos, 2004" startWordPosition="478" endWordPosition="482">ation of these grammars to LOs in respective languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at de</context>
<context position="23368" citStr="Miliaraki and Androutsopoulos, 2004" startWordPosition="3678" endWordPosition="3682">thods (cf. section 5.6). 5.5 Levels of Linguistic Processing The work reported here has been an excercise in definition extraction using shallow parsing methods. However, the poor results suggest that this is one of the tasks that require a much more sophisticated and deeper approach to language analysis. In fact, in turns out that virtually all successful attempts at definition extraction that we are aware of build on worked-out deep linguistic approaches (Klavans and 48 Muresan, 2000; Fahmi and Bouma, 2006; Walter and Pinkal, 2006), some of them combining syntactic and semantic information (Miliaraki and Androutsopoulos, 2004; Walter and Pinkal, 2006). Unfortunately, for most Baltic and Slavic languages, such deep parsers are unavailable or have not yet been extensively tested on real texts. One exception is Czech, where a number of parsers were already described and evaluated (on the Prague Dependency Treebank) in (Zeman, 2004, § 14.2); the best of these parsers reach 80–85% accuracy. For Polish, apart from a number of linguistically motivated toy parsers, there is a possibly wide coverage deep parser (Woli´nski, 2004), but it has not yet been evaluated on naturally occurring texts. The situation is probably most</context>
</contexts>
<marker>Miliaraki, Androutsopoulos, 2004</marker>
<rawString>Spyridoula Miliaraki and Ion Androutsopoulos. 2004. Learning to identify single-snippet answers to definition questions. In Proceedings of COLING 2004, pp. 1360–1366, Geneva, Switzerland. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Pearson</author>
</authors>
<title>The expression of definitions in specialised texts: a corpus-based analysis.</title>
<date>1996</date>
<booktitle>Proceedings of the Seventh Euralex International Congress,</booktitle>
<pages>817--824</pages>
<editor>In M. Gellerstam et al., editors,</editor>
<location>Göteborg.</location>
<contexts>
<context position="3210" citStr="Pearson, 1996" startWordPosition="461" endWordPosition="462">iefly describe the three Slavic grammars developed within this project in section 3. Section 4 presents the results of the application of these grammars to LOs in respective languages. These results are evaluated in section 5, where main problems, as well as some possible solutions, are discussed. Finally, section 6 concludes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storre</context>
</contexts>
<marker>Pearson, 1996</marker>
<rawString>Jennifer Pearson. 1996. The expression of definitions in specialised texts: a corpus-based analysis. In M. Gellerstam et al., editors, Proceedings of the Seventh Euralex International Congress, pp. 817–824, Göteborg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Piskorski</author>
</authors>
<title>Information extraction for Polish using the SProUT platform.</title>
<date>2004</date>
<booktitle>Intelligent Information Processing and Web Mining,</booktitle>
<pages>227--236</pages>
<editor>In M. A. Kłopotek et al., editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Piskorski, 2004</marker>
<rawString>Jakub Piskorski et al. 2004. Information extraction for Polish using the SProUT platform. In M. A. Kłopotek et al., editors, Intelligent Information Processing and Web Mining, pp. 227–236. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Simov</author>
<author>Petya Osenova</author>
</authors>
<title>BulQA: Bulgarian-Bulgarian Question Answering at CLEF</title>
<date>2005</date>
<booktitle>In CLEF,</booktitle>
<pages>517--526</pages>
<contexts>
<context position="4090" citStr="Simov and Osenova, 2005" startWordPosition="594" endWordPosition="597">off, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and Osenova, 2005). Other work on Slavic information extraction has been carried out mainly for the last 5 years. Probably the first forum where such work was comprehensively presented was the International Workshop on Information Extraction for Slavonic and Other Central and Eastern European Languages (IESL), RANLP, Borovets, 2003, Bulgaria. One of the papers presented there, (Dro˙zd˙zy´nski et al., 2003), discusses shallow SProUT (Becker et al., 2002) grammars for Czech, Polish and Lithuanian. SProUT has subsequently been extensively used for the information extraction from Polish medical texts (Piskorski et </context>
</contexts>
<marker>Simov, Osenova, 2005</marker>
<rawString>Kiril Simov and Petya Osenova. 2005. BulQA: Bulgarian-Bulgarian Question Answering at CLEF 2005. In CLEF, pp. 517–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiril Simov</author>
</authors>
<title>CLaRK — an XML-based system for corpora development.</title>
<date>2001</date>
<booktitle>Proceedings of the Corpus Linguistics 2001 Conference,</booktitle>
<pages>558--560</pages>
<editor>In P. Rayson et al., editors,</editor>
<location>Lancaster.</location>
<marker>Simov, 2001</marker>
<rawString>Kiril Simov et al. 2001. CLaRK — an XML-based system for corpora development. In P. Rayson et al., editors, Proceedings of the Corpus Linguistics 2001 Conference, pp. 558–560, Lancaster.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Storrer</author>
<author>Sandra Wellinghoff</author>
</authors>
<title>Automated detection and annotation of term definitions in German text corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="3476" citStr="Storrer and Wellinghoff, 2006" startWordPosition="495" endWordPosition="499"> well as some possible solutions, are discussed. Finally, section 6 concludes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and </context>
</contexts>
<marker>Storrer, Wellinghoff, 2006</marker>
<rawString>Angelika Storrer and Sandra Wellinghoff. 2006. Automated detection and annotation of term definitions in German text corpora. In Proceedings of LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hristo Tanev</author>
</authors>
<title>Socrates: A question answering prototype for Bulgarian.</title>
<date>2004</date>
<booktitle>In Recent Advances in Natural Language Processing III, Selected Papers from RANLP 2003,</booktitle>
<pages>377--386</pages>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="4064" citStr="Tanev, 2004" startWordPosition="592" endWordPosition="593"> and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and Osenova, 2005). Other work on Slavic information extraction has been carried out mainly for the last 5 years. Probably the first forum where such work was comprehensively presented was the International Workshop on Information Extraction for Slavonic and Other Central and Eastern European Languages (IESL), RANLP, Borovets, 2003, Bulgaria. One of the papers presented there, (Dro˙zd˙zy´nski et al., 2003), discusses shallow SProUT (Becker et al., 2002) grammars for Czech, Polish and Lithuanian. SProUT has subsequently been extensively used for the information extraction from Polish me</context>
</contexts>
<marker>Tanev, 2004</marker>
<rawString>Hristo Tanev. 2004. Socrates: A question answering prototype for Bulgarian. In Recent Advances in Natural Language Processing III, Selected Papers from RANLP 2003, pages 377–386. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Tobin</author>
</authors>
<title>Lxtransduce, a replacement for fsgmatch.</title>
<date>2005</date>
<institution>University of Edinburgh.</institution>
<note>http://www.cogsci.ed.ac.uk/~richard/ ltxml2/lxtransduce-manual.html.</note>
<contexts>
<context position="6299" citStr="Tobin, 2005" startWordPosition="908" endWordPosition="909">base=&amp;quot;klasc&amp;quot; ctag=&amp;quot;fin&amp;quot; id=&amp;quot;t254&amp;quot; msd=&amp;quot;sg:ter:imperf&amp;quot;&gt;kladzie&lt;/tok&gt; &lt;tok base=&amp;quot;nacisk&amp;quot; ctag=&amp;quot;subst&amp;quot; id=&amp;quot;t255&amp;quot; msd=&amp;quot;sg:acc:m3&amp;quot;&gt;nacisk&lt;/tok&gt; &lt;tok base=&amp;quot;na&amp;quot; ctag=&amp;quot;prep&amp;quot; id=&amp;quot;t256&amp;quot; msd=&amp;quot;acc&amp;quot;&gt;na&lt;/tok&gt; [...] &lt;tok base=&amp;quot;.&amp;quot; ctag=&amp;quot;interp&amp;quot; id=&amp;quot;t273&amp;quot;&gt;. &lt;/tok&gt; &lt;/s&gt; For each language, definitions were manually marked in two batches of texts: the first batch, consulted during the process of grammar development, contained at least 300 definitions, and the second batch, held out for evaluation, contained about 150 definitions. All grammars are regular grammars implemented with the use of the lxtransduce tool (Tobin, 2005), a component of the LTXML2 toolset developed at the University of Edinburgh.3 An example of a simple rule for prepositional phrases is given below: &lt;rule name=&amp;quot;PP&amp;quot;&gt; &lt;seq&gt; &lt;query match=&amp;quot;tok[@ctag = ’prep’]&amp;quot;/&gt; &lt;ref name=&amp;quot;NP1&amp;quot;&gt; &lt;with-param name=&amp;quot;case&amp;quot; value=&amp;quot;”&amp;quot;/&gt; &lt;/ref&gt; &lt;/seq&gt; &lt;/rule&gt; This rule identifies a sequence whose first element is a token tagged as a preposition and whose subsequent elements are identified by a rule called NP1. This latter rule (not shown here for brevity) is a parameterised rule which finds a nominal phrase of a given case, but the way it is called above ensures that it</context>
</contexts>
<marker>Tobin, 2005</marker>
<rawString>Richard Tobin, 2005. Lxtransduce, a replacement for fsgmatch. University of Edinburgh. http://www.cogsci.ed.ac.uk/~richard/ ltxml2/lxtransduce-manual.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Walter</author>
<author>Manfred Pinkal</author>
</authors>
<title>Automatic extraction of definitions from German court decisions.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Information Extraction Beyond The Document,</booktitle>
<pages>20--28</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="3543" citStr="Walter and Pinkal, 2006" startWordPosition="506" endWordPosition="509">udes the paper. Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43–50, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Related Work Definition extraction is an important NLP task, most frequently a subtask of terminology extraction (Pearson, 1996), the automatic creation of glossaries (Klavans and Muresan, 2000; Klavans and Muresan, 2001), question answering (Miliaraki and Androutsopoulos, 2004; Fahmi and Bouma, 2006), learning lexical semantic relations (Malais´e et al., 2004; Storrer and Wellinghoff, 2006) and automatic construction of ontologies (Walter and Pinkal, 2006). Tools for definition extraction are invariably languagespecific and involve shallow or deep processing, with most work done for English (Pearson, 1996; Klavans and Muresan, 2000; Klavans and Muresan, 2001) and other Germanic languages (Fahmi and Bouma, 2006; Storrer and Wellinghoff, 2006; Walter and Pinkal, 2006), as well as French (Malais´e et al., 2004). To the best of our knowledge, no previous attempts at definition extraction have been made for Slavic, with the exception of some work on Bulgarian (Tanev, 2004; Simov and Osenova, 2005). Other work on Slavic information extraction has bee</context>
<context position="23272" citStr="Walter and Pinkal, 2006" startWordPosition="3665" endWordPosition="3668">of rules and, hence, the recall, and then deal with precision via Machine Learning methods (cf. section 5.6). 5.5 Levels of Linguistic Processing The work reported here has been an excercise in definition extraction using shallow parsing methods. However, the poor results suggest that this is one of the tasks that require a much more sophisticated and deeper approach to language analysis. In fact, in turns out that virtually all successful attempts at definition extraction that we are aware of build on worked-out deep linguistic approaches (Klavans and 48 Muresan, 2000; Fahmi and Bouma, 2006; Walter and Pinkal, 2006), some of them combining syntactic and semantic information (Miliaraki and Androutsopoulos, 2004; Walter and Pinkal, 2006). Unfortunately, for most Baltic and Slavic languages, such deep parsers are unavailable or have not yet been extensively tested on real texts. One exception is Czech, where a number of parsers were already described and evaluated (on the Prague Dependency Treebank) in (Zeman, 2004, § 14.2); the best of these parsers reach 80–85% accuracy. For Polish, apart from a number of linguistically motivated toy parsers, there is a possibly wide coverage deep parser (Woli´nski, 2004)</context>
</contexts>
<marker>Walter, Pinkal, 2006</marker>
<rawString>Stephan Walter and Manfred Pinkal. 2006. Automatic extraction of definitions from German court decisions. In Proceedings of the Workshop on Information Extraction Beyond The Document, pp. 20–28, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Woli´nski</author>
</authors>
<date>2004</date>
<booktitle>Komputerowa weryfikacja gramatyki ´Swidzi´nskiego. Ph. D. dissertation, ICS PAS,</booktitle>
<location>Warsaw.</location>
<marker>Woli´nski, 2004</marker>
<rawString>Marcin Woli´nski. 2004. Komputerowa weryfikacja gramatyki ´Swidzi´nskiego. Ph. D. dissertation, ICS PAS, Warsaw.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Parsing with a Statistical Dependency Model.</title>
<date>2004</date>
<tech>Ph. D. dissertation,</tech>
<institution>Charles University,</institution>
<location>Prague.</location>
<contexts>
<context position="23676" citStr="Zeman, 2004" startWordPosition="3731" endWordPosition="3732">t virtually all successful attempts at definition extraction that we are aware of build on worked-out deep linguistic approaches (Klavans and 48 Muresan, 2000; Fahmi and Bouma, 2006; Walter and Pinkal, 2006), some of them combining syntactic and semantic information (Miliaraki and Androutsopoulos, 2004; Walter and Pinkal, 2006). Unfortunately, for most Baltic and Slavic languages, such deep parsers are unavailable or have not yet been extensively tested on real texts. One exception is Czech, where a number of parsers were already described and evaluated (on the Prague Dependency Treebank) in (Zeman, 2004, § 14.2); the best of these parsers reach 80–85% accuracy. For Polish, apart from a number of linguistically motivated toy parsers, there is a possibly wide coverage deep parser (Woli´nski, 2004), but it has not yet been evaluated on naturally occurring texts. The situation is probably most dire for Bulgarian, although there have been attempts at the induction of a dependency parser from the BulTreeBank (Marinov and Nivre, 2005; Chanev et al., 2006). Nevertheless, if other possible paths of improvement suggested in this section do not bring satisfactory results, we plan to make an attempt at </context>
</contexts>
<marker>Zeman, 2004</marker>
<rawString>Daniel Zeman. 2004. Parsing with a Statistical Dependency Model. Ph. D. dissertation, Charles University, Prague.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>