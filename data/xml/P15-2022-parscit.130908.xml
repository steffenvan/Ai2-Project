<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9983795">
Recurrent Neural Network based Rule Sequence Model
for Statistical Machine Translation
</title>
<author confidence="0.99006">
Heng Yu, Xuan Zhu
</author>
<affiliation confidence="0.97318">
Samsung R&amp;D Institute of China, Beijing, China
</affiliation>
<email confidence="0.957994">
Jh0517.yu, xuan.zhul@samsung.com
</email>
<sectionHeader confidence="0.981651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999687117647059">
The inability to model long-distance depen-
dency has been handicapping SMT for years.
Specifically, the context independence as-
sumption makes it hard to capture the depen-
dency between translation rules. In this paper,
we introduce a novel recurrent neural network
based rule sequence model to incorporate arbi-
trary long contextual information during esti-
mating probabilities of rule sequences. More-
over, our model frees the translation model
from keeping huge and redundant grammars,
resulting in more efficient training and de-
coding. Experimental results show that our
method achieves a 0.9 point BLEU gain over
the baseline, and a significant reduction in rule
table size for both phrase-based and hierarchi-
cal phrase-based systems.
</bodyText>
<sectionHeader confidence="0.995161" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980368421053">
Modeling long-distance dependency has always
been a bottleneck for statistical machine translation
(SMT). While lots of efforts have been made in solv-
ing long-distance reordering (Xiong et al., 2006;
Zens and Ney, 2006; Kumar and Byrne, 2005), long-
span n-gram matching (Charniak et al., 2003; Shen
et al., 2008; Yu et al., 2014), much less attention has
been concentrated on capturing translation rule de-
pendency, which is not explicitly modeled in most
translation systems (Wu et al., 2014).
SMT systems typically model the translation pro-
cess as a sequence of translation steps, each of which
uses a translation rule. These rules are usually ap-
plied independently of each other, which violates the
conventional wisdom that translation should be done
in context (Gim´enez and M`arquez, 2007). However,
it is not an easy task to capture the rule dependency,
which entails much longer context and more severe
data sparsity. There are two major solutions: the
first one is breaking the rules into bilingual word-
pairs and use a n-gram translation model to incorpo-
rate lexical dependencies that span rule boundaries
(Marino et al., 2006; Durrani et al., 2013). These n-
gram models (also known as tuple sequence model)
could help phrase-based translation models to over-
come the phrasal independence assumption, but they
rely on word alignment to extract bilingual tuples,
which brings in additional alignment error (Wu et
al., 2014). The other direction lies in utilizing the
rule Markov model (Vaswani et al., 2011; Quirk
and Menezes, 2006), which directly explores depen-
dencies in rule derivation history and achieves both
good performance and slimmer translation model in
syntax-based SMT systems. However, the sparsity
of translation rules entails aggressive pruning of the
training data and constrains the model from scaling
to high order grams, significantly limiting the ability
of the model.
In this paper we follow the second line and pro-
pose a novel recurrent neural network based rule
sequence model (RNN-RSM), which utilizes the
representational power of recurrent neural network
(RNN) to capture arbitrary distance of contextual in-
formation in estimating the probability of rule se-
quences, rather than constrained to n-gram local
context limited by Markov assumption. Compared
with previous studies, our contributions are as fol-
lows:
First, we lift the Markov assumption in rule se-
quence model and use RNN to capture arbitrary-
length of contextual information, which is proven to
be more accurate in estimating sequential probabili-
ties (Mikolov et al., 2010).
Second, to alleviate the sparsity of translation
rules, we extend our model to factorized RNN-RSM,
which incorporates both the source and target side
phrase embedding in addition to the translation rule
</bodyText>
<page confidence="0.470922">
132
</page>
<note confidence="0.792684333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 132–138,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.6271791">
Distribution on Distribution on
source phrases classes of source phrases
delayed copy
W
U
Hidden layer, hn
Input layer, xn
Output layer, yn
hn-1 rn-1 Sn-1 tn-1
history.
</figure>
<bodyText confidence="0.9993954">
Lastly, we apply our model to both phrase-
based and hierarchical phrase-based (HPB) systems
and achieve an average improvement of 0.9 BLEU
points with much slimmer translation models in hy-
pergraph reranking task (Huang, 2008).
</bodyText>
<sectionHeader confidence="0.671103" genericHeader="method">
2 Rule Sequence Model
</sectionHeader>
<bodyText confidence="0.959765">
We will first brief our rule sequence model with an
example from phrase-based system (Koehn et al.,
2007). Consider the following translation from Chi-
nese to English:
</bodyText>
<figure confidence="0.995879076923077">
B`ush(Bush
yˇu Sh¯aling
with Sharon
jˇux´ıng
hold
le hu`ıt´an
-ed meeting
‘Bush held a meeting with Sharon’
So one possible rule derivation of the above ex-
ample could be:
(0 ) : (s0, “”)
r1
r2
</figure>
<figureCaption confidence="0.9788">
Figure 1: Factorized recurrent neural network with
source and target side phrase embeddings.
</figureCaption>
<bodyText confidence="0.997914333333333">
So the rule sequence model does not make any con-
text independence assumption and generate a rule by
looking at a context of previous rules.
</bodyText>
<subsectionHeader confidence="0.895789">
2.1 Training
</subsectionHeader>
<equation confidence="0.878643333333333">
�
1 if 1j, s.t. p = yj and q = yj+1
Pforced(q  |p) = 0 otherwise
</equation>
<bodyText confidence="0.994836166666667">
For each hypothesis, we keep the bourndary words
as its signiture (only right side for phrase-based
model and both sides for HPB). If a boundary word
does not occur in the reference, its language model
score will be set to −oc; if a boundary word occurs
more than once in the reference, the hypothesis is
split into multiple hypotheses, one for each index of
occurance.
According to the definition, we can see that the
rule sequence [r1, r2, r3] in the example could pro-
duce the exact reference translation, which is ideal
for the training of rule sequence model.
</bodyText>
<sectionHeader confidence="0.856438" genericHeader="method">
3 Recurrent Neural Network based Rule
</sectionHeader>
<subsectionHeader confidence="0.78643">
Sequence Model
</subsectionHeader>
<bodyText confidence="0.998065">
In order to capture long-span context, we introduce
recurrent neural network based rule sequence model
</bodyText>
<equation confidence="0.935333">
(0 0006) : (s2, “Bush held talks”)
(01 ) : (s1, “Bush”)
(0003000) : (s3, “Bush held talks with Sharon”)
r1: B`ush(—* Bush
r2: jˇux(ng le hu`ıt´an —*held talks
r3: yˇu Sh¯aling —* with Sharon
</equation>
<bodyText confidence="0.9859614">
Each row is a derivation step, where sn denotes
a hypothesis with a coverage vector capturing the
source language words translated so far, and a 0 in
the coverage vector indicates the source word at this
position is “covered”. Each hypothesis sn−1 can
be extended into a longer hypothesis sn by a rule
rn translating an uncovered segment. Note that in
phrase-based translation we need to set a distortion
limit to prohibit long distance reordering, so the end-
ing position of last phrase is maintained (e.g., 1 and
6 in the coverage vector).
In our example, translation rules r1, r2, r3 form a
derivation T which leads to a complete translation.
So for rule sequence model, the probability of rn
depends on its derivation history H(rn):
</bodyText>
<equation confidence="0.999794">
P(rn) = P(rn|H(rn)) (1)
</equation>
<bodyText confidence="0.576615">
and the probability of a rule derivation T is
</bodyText>
<equation confidence="0.9935165">
P(T) = � P(ri|H(ri)) (2)
ri∈T
</equation>
<bodyText confidence="0.99927875">
The rule sequence model can then be trained on the
path set of rule derivations. To obtain golden deriva-
tions of translation rules for each sentence pair, We
r3 follow Yu et al. (2013) to utilize force decoding to
get golden rule derivations. Specifically, we define
a new forced decoding LM which only accepts two
consecutive words (denote as p, q) in the reference
translation (yi):
</bodyText>
<page confidence="0.771089">
133
</page>
<bodyText confidence="0.999989916666667">
to estimate the probability P(rn|H(rn)). Our RNN-
RSM can potentially capture arbitrary long context
rather than n-1 previous rules limited by Markov
assumption. Following Mikolov et al. (2010), we
adopt the standard RNN architecture: the input layer
encodes previous translation rule using one-hot cod-
ing, the output layer produces a probability distribu-
tion over all translation rules, and the hidden layer
maintains a representation of rule derivation history.
However, the standard implementation has severe
data sparsity problem due to the large size of rule
table couple with the limited training data.
</bodyText>
<subsectionHeader confidence="0.985173">
3.1 Factorized RNN-RSM
</subsectionHeader>
<bodyText confidence="0.999912074074074">
To solve the sparsity problem, we extend the
RNN-RSM model with factorizing rules in the
input layer, as shown in Figure 1. It con-
sists of an input layer x, a hidden layer h (state
layer), and an output layer y. The connection
weights among layers are denoted by matrixes
U and W respectively. Unlike the RNN-RSM,
which predicts probability P(rn|rn−1, H(rn−1)),
the factorized RNN-RSM predicts probability
P(rn|rn−1, H(rn−1), ¯sn−1, ¯tn−1) to generate fol-
lowing rule rn, where ¯sn−1/tn−1 are the source/tar-
get side of rn−1, However, ¯sn−1 and tn−1 are still
too sparse considering the huge vocabulary size and
the diversity in forming phrases, so here we use re-
cursive auto-encoder (Socher et al., 2011; Li et al.,
2013) to learn phrase embeddings on both source
and target side in an unsupervised mannner, mini-
mizing the reconstruction error.
For those rules that are not contained in the train-
ing data, the factorized RNN-RSM backs off to the
source/target side embedding Esi_1/Eti_1. In the
special case that Esi_1 and Eti_1 are dropped, the
factorized RNN-RSM goes back to RNN-RSM. Fi-
nally, the input layer xn is formed by concatenating
the input vectors and hidden layer hn−1 at the pre-
ceding time step, as shown in the following equa-
tion.
</bodyText>
<equation confidence="0.944606125">
[ u ¯s t ]
xn = vn−1, vn−1, vn−1, hn−1 (3)
The neurons in the hidden and output layers are
computed as follows:
hn = f(U × xn), yn = g(w × hn) (4)
1 ezM
f(z) = 1 + e−z ,g(z) =
Ek ezk (5)
</equation>
<subsectionHeader confidence="0.747338">
3.2 Factorized RNN-RSM on source and target
phrases
</subsectionHeader>
<bodyText confidence="0.999956">
The above factorized RNN-RSM is conditioned on
the previous context during computing the probabil-
ity of rule rn. Since rn may still suffer from sparsity,
we further factorize rn into its source side phrase ¯sn
and target side phrase ¯tn. So the probability formula
could be rewrite as:
</bodyText>
<equation confidence="0.9994225">
P(rn|H(rn)) = P(sn, tn|H(rn))
= P(sn|H(rn)) × P(tn|sn, H(rn)) (6)
</equation>
<bodyText confidence="0.999982375">
The first sub-model P(sn, |H(rn)) computes the
probability distribution over source phrases. Then
the second sub-model P(tn|sn, H(rn)) computes
the probability distribution over tn that are trans-
lated from sn. The two sub-models are computed
with the similar recurrent network shown in Figure
1 except adding the source side information sn of
the current rule rn into the input layer. This method
share the same spirit with the RNN-based translation
model (Sundermeyer et al., 2014; Cho et al., 2014),
except that we focus on capturing rule dependencies
which has a much small search space. Noted that
this new factorize model provides richer information
for prediction, and actually is faster to train since the
vocabulary of source/target phrases are much small
than that of the translation rules.
</bodyText>
<sectionHeader confidence="0.999244" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.94786">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999743470588235">
The training corpus consists of 1M sentence pairs
with 25M/21M words of Chinese/English respec-
tively. Our development and test set are NIST 2006
and 2008 (newswire portion) respectively.
We obtained alignments by running GIZA++
(Och and Ney, 2004) and used the SRILM toolkit
(Stolcke, 2002) to train a 4-gram language model
with KN-smoothing on the English side of the train-
ing data. Case-insensitive BLEU (Papineni et al.,
2002) and MERT (Och, 2003) were used for evalua-
tion and tuning.
We test our method on both phrase-based and
hierarchical phrase-based translation models. For
phrase-based system, we use Moses with standard
features (Koehn et al., 2007). While for hierarchical
phrase-based model, we use a in-house implemen-
tation of Hiero (Chiang, 2005). We set phrase-limit
</bodyText>
<table confidence="0.95318375">
134
System Moses Hiero
dev-set test-set dev-set test-set
Baseline 28.4 27.7 30.4 30.0
+RMM 28.7 28.3 30.7 30.2
+fRNN-RSM (1) 28.9 28.6 30.9 30.6
+fRNN-RSMst (2) 29.3 28.5 31.2 30.7
+(1)+(2) 29.6 28.7 31.4 30.8
</table>
<tableCaption confidence="0.986005333333333">
Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized
RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section
3.2. Results in bold mean that the improvements over “Baseline” are statistically significant (p &lt; 0.05) (Koehn, 2004).
</tableCaption>
<bodyText confidence="0.999985">
to 5 for the extraction of both phrase-based rule and
SCFG rule, as well as beam size to 100 and distor-
tion limit to 7 in decoding.
Since the rule sequence model belongs to the fam-
ily of non-local feature (Huang, 2008), traditional
testing methods like nbest reranking are not suit-
able for our experiments. So we adopt hypergraph
reranking (Huang and Chiang, 2007; Huang, 2008),
which proves to be effective for integrating nonlo-
cal features into dynamic programming. The de-
coding process is divided into two passes. In the
first pass, only standard features (i.e., standard fea-
tures for phrase-based or HPB model) are used to
produce a hypergraph. In the second pass, we use
the hypergraph reranking algorithm (Huang, 2008)
to find promising translations using additional rule
sequence feature.
For RNN training, we set the hidden layer size
to 512 and classes in the output layer to 256. To
obtain phrase-embedding, we use open source tool
str2vec1 (Li et al., 2013) to train two autoencoders
on the source and target side of rule-table respec-
tively.
</bodyText>
<sectionHeader confidence="0.515748" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999848222222222">
Table 1 presents the main results of our paper. To
show the merits of our RNN-RSM, we also re-
implement Vaswani et al. (2011)’s work, denote as
rule Markov model (RMM). It utilize tri-gram rule
derivation history for prediction, whereas our RNN-
RSM could capture arbitrary length of contextual in-
formation. We can see that RMM provides a mod-
est improvement over the baseline, 0.6/0.2 points
over Moses/Hiero, thanks to the positive guidance
</bodyText>
<footnote confidence="0.793174">
1https://github.com/pengli09/str2vec
</footnote>
<table confidence="0.999558666666667">
System w/o monotone Full
Moses Hiero Moses Hiero
Baseline 27.4 29.8 27.7 30.0
+RMM 27.6 29.9 28.3 30.2
+fRNN-RSM 28.0 30.4 28.6 30.6
+fRNN-RSMst 28.2 30.6 28.5 30.7
</table>
<tableCaption confidence="0.997365">
Table 2: BLEU score comparison on different rule-set,
</tableCaption>
<bodyText confidence="0.9486145">
“w/o monotone” denotes we filter out monotone com-
posed rules in both rule table and our RNN-RSM, full
denotes we use the total rule-set.
of short-span rule dependency. On the other hand,
our factorized RNN-RSM with phrase embeddings
(fRNN-RSM) provides a more significant BLEU
score improvement (0.9 for Moses, 0.6 for Hiero),
which exemplifies that the long-span rule depen-
dency captured by RNN could provides additional
boost in translation quality. At the same time, fac-
torized RNN-RSM on source and target phrases
(fRNN-RSMst) alleviate the data sparse problem
in RNN training, resulting in slightly better per-
formance. Finally, when we combine both factor-
ized model, we get the best performance at 28.7 for
Moses and 30.8 for Hiero, both significantly better
than baseline systems.
Also, we conduct an interesting experiment to see
if our fRNN-RSM could somehow replace the role
of composed rules (rules that can be formed out of
smaller rules in the grammar) and guides more fine-
grained rule-set to produce better translation results.
We re-implement He et al. (2009)’s work to filter
out monotone composed rules for both Hiero and
Moses. We are able to filter out a large number of
monotone composed rules, about 50% rules for Hi-
</bodyText>
<page confidence="0.864972">
135
</page>
<bodyText confidence="0.998659545454545">
ero and 31% for Moses. The results are shown in
Table 2. Interestingly the performance of slimmer
translation model with fRNN-RSM exceeds baseline
with full rule-table, and catches up with the orig-
inal fRNN-RSM. The reason is two-folded: first,
deleting monotone composed rules doesn’t effect
the overall coverage of the rule-set, making limited
harm to the system. Second, with less rules, the data
sparse problem of RNN training is further alleviated,
resulting in a better fRNN-RSM for probability pre-
diction.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99997">
Besides the work of Vaswani et al. (2011) discussed
in Section 1, there are several other works using a
rule bigram or trigram model in machine translation,
Ding and Palmer (2005) use n-gram rule Markov
model in the dependency treelet model, Liu and
Gildea (2008) applies the same method in a tree-to-
string model. Our work is different from theirs in
that we lift the Markov assumption and use recur-
rent neural network to capture much longer contex-
tual information to help probability prediction.
Our work is also in the same spirit with tuple se-
quence models (Marino et al., 2006; Durrani et al.,
2013; Hui Zhang, 2013; Wu et al., 2014), which
break the translation sequence into bilingual tuples
and use a Markov model to capture the dependency
of tuples. Comparing to them, we take a more di-
rect approach to use translation rule dependency to
guide translation process, rather than rely on tuples
which will be significant affected by word alignment
errors.
Outside of machine translation, the idea of weak-
ening independence assumption by modeling the
derivation history is also found in parsing (Johnson,
1998), where rule probabilities are conditioned on
parent and grand-parent nonterminals. Inspired by
it, we successfully find a solution for the translation
field.
</bodyText>
<sectionHeader confidence="0.992577" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999979571428572">
In this paper, we have presented a novel recurrent
neural network based rule sequence model to esti-
mate the probability of translation rule sequences.
One of the major advantages of our model is its po-
tential to capture long-span dependency compared
with n-gram Markov models. In addition, our factor-
ized model with phrase embedding could further al-
leviate the data sparse problem in RNN training. Fi-
nally we conduct experiments on both phrase-based
and hierarchical phrase-based models and get an av-
erage improvement of 0.9 BLEU points over the
baseline. In the future we will investigate stronger
network structure such as LSTM to further improve
the prediction power of our model.
</bodyText>
<sectionHeader confidence="0.957739" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998608684210526">
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proceedings of MT Summit IX,
pages 40–46. Citeseer.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd ACL, Ann Arbor, MI.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Fethi Bougares, Holger Schwenk, and Yoshua Ben-
gio. 2014. Learning phrase representations using
rnn encoder-decoder for statistical machine transla-
tion. arXiv preprint arXiv:1406.1078.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 541–548. Association for Computational
Linguistics.
Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu
Hoang, and Philipp Koehn. 2013. Can markov models
over minimal translation units help phrase-based smt?
In ACL (2), pages 399–405.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proceedings of the Second Workshop on
Statistical Machine Translation, pages 159–166. Asso-
ciation for Computational Linguistics.
Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding
monotone composed rule for hierarchical phrase-based
statistical machine translation. In Proceedings of the
3rd International Universal Communication Sympo-
sium, pages 25–29. ACM.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
</reference>
<page confidence="0.788894">
136
</page>
<reference confidence="0.99241856">
Chris Quirk Jianfeng Gao Hui Zhang,
Kristina Toutanova. 2013. Beyond left-to-right:
Multiple decomposition structures for smt. In
NAACL.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613–
632.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings ofACL:
Demonstrations.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388–395. Citeseer.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, pages 161–168. Association for
Computational Linguistics.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive
autoencoders for itg-based translation. In EMNLP,
pages 567–577.
Ding Liu and Daniel Gildea. 2008. Improved tree-
to-string transducer for machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 62–69. Association for Computa-
tional Linguistics.
Jos´e B Marino, Rafael E Banchs, Josep M Crego, Adria
de Gispert, Patrik Lambert, Jos´e AR Fonollosa, and
Marta R Costa-Juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527–
549.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Re-
current neural network based language model. In
INTERSPEECH 2010, 11th Annual Conference of
the International Speech Communication Association,
Makuhari, Chiba, Japan, September 26-30, 2010,
pages 1045–1048.
Franz Joseph Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417–449.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings ofACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311–318, Philadephia, USA, July.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases?: challenging the conventional wisdom in sta-
tistical machine translation. In Proceedings of the
main conference on human language technology con-
ference of the north american chapter of the associa-
tion of computational linguistics, pages 9–16. Associ-
ation for Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008.
A new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
ACL, pages 577–585.
Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sen-
timent distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 151–161. Association for Computational
Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901–904.
Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker,
and Hermann Ney. 2014. Translation modeling with
bidirectional recurrent neural networks. In Proceed-
ings of the Conference on Empirical Methods on Nat-
ural Language Processing, October.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of ACL 2011, Port-
land, OR.
Youzheng Wu, Taro Watanabe, and Chiori Hori. 2014.
Recurrent neural network-based tuple sequence model
for machine translation. In Proc. COLING, pages
1908–1917.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 521–528. Associa-
tion for Computational Linguistics.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable mt training. In EMNLP, pages 1112–
1123.
Heng Yu, Haitao Mi, Liang Huang, and Qun Liu. 2014.
A structured language model for incremental tree-to-
string translation.
</reference>
<figure confidence="0.4320372">
137
Richard Zens and Hermann Ney. 2006. Discriminative Translation, pages 55–63. Association for Computa-
reordering models for statistical machine translation. tional Linguistics.
In Proceedings of the Workshop on Statistical Machine
138
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.508307">
<title confidence="0.9986665">Recurrent Neural Network based Rule Sequence for Statistical Machine Translation</title>
<author confidence="0.756625">Heng Yu</author>
<author confidence="0.756625">Xuan Zhu Samsung R&amp;D Institute of China</author>
<author confidence="0.756625">China Beijing</author>
<abstract confidence="0.999199055555556">The inability to model long-distance dependency has been handicapping SMT for years. Specifically, the context independence assumption makes it hard to capture the dependency between translation rules. In this paper, we introduce a novel recurrent neural network based rule sequence model to incorporate arbitrary long contextual information during estimating probabilities of rule sequences. Moreover, our model frees the translation model from keeping huge and redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Kevin Knight</author>
<author>Kenji Yamada</author>
</authors>
<title>Syntax-based language models for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX,</booktitle>
<pages>40--46</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1234" citStr="Charniak et al., 2003" startWordPosition="179" endWordPosition="182">the translation model from keeping huge and redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 1 Introduction Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solving long-distance reordering (Xiong et al., 2006; Zens and Ney, 2006; Kumar and Byrne, 2005), longspan n-gram matching (Charniak et al., 2003; Shen et al., 2008; Yu et al., 2014), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled in most translation systems (Wu et al., 2014). SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). However, it is not an easy task to capture the rule dependency, which entails much longer context</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Eugene Charniak, Kevin Knight, and Kenji Yamada. 2003. Syntax-based language models for statistical machine translation. In Proceedings of MT Summit IX, pages 40–46. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="11265" citStr="Chiang, 2005" startWordPosition="1828" endWordPosition="1829">d 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model with KN-smoothing on the English side of the training data. Case-insensitive BLEU (Papineni et al., 2002) and MERT (Och, 2003) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30.2 +fRNN-RSM (1) 28.9 28.6 30.9 30.6 +fRNN-RSMst (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section 3.2. Results in bold mean that the improvements over “Baseline” are statistically significant (p &lt; 0.05) (Koehn, 2004). to 5 for the extraction of bot</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd ACL, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>541--548</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15542" citStr="Ding and Palmer (2005)" startWordPosition="2520" endWordPosition="2523">e of slimmer translation model with fRNN-RSM exceeds baseline with full rule-table, and catches up with the original fRNN-RSM. The reason is two-folded: first, deleting monotone composed rules doesn’t effect the overall coverage of the rule-set, making limited harm to the system. Second, with less rules, the data sparse problem of RNN training is further alleviated, resulting in a better fRNN-RSM for probability prediction. 5 Related Work Besides the work of Vaswani et al. (2011) discussed in Section 1, there are several other works using a rule bigram or trigram model in machine translation, Ding and Palmer (2005) use n-gram rule Markov model in the dependency treelet model, Liu and Gildea (2008) applies the same method in a tree-tostring model. Our work is different from theirs in that we lift the Markov assumption and use recurrent neural network to capture much longer contextual information to help probability prediction. Our work is also in the same spirit with tuple sequence models (Marino et al., 2006; Durrani et al., 2013; Hui Zhang, 2013; Wu et al., 2014), which break the translation sequence into bilingual tuples and use a Markov model to capture the dependency of tuples. Comparing to them, we</context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 541–548. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Can markov models over minimal translation units help phrase-based smt?</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>399--405</pages>
<contexts>
<context position="2098" citStr="Durrani et al., 2013" startWordPosition="320" endWordPosition="323">on process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). However, it is not an easy task to capture the rule dependency, which entails much longer context and more severe data sparsity. There are two major solutions: the first one is breaking the rules into bilingual wordpairs and use a n-gram translation model to incorporate lexical dependencies that span rule boundaries (Marino et al., 2006; Durrani et al., 2013). These ngram models (also known as tuple sequence model) could help phrase-based translation models to overcome the phrasal independence assumption, but they rely on word alignment to extract bilingual tuples, which brings in additional alignment error (Wu et al., 2014). The other direction lies in utilizing the rule Markov model (Vaswani et al., 2011; Quirk and Menezes, 2006), which directly explores dependencies in rule derivation history and achieves both good performance and slimmer translation model in syntax-based SMT systems. However, the sparsity of translation rules entails aggressiv</context>
<context position="15965" citStr="Durrani et al., 2013" startWordPosition="2594" endWordPosition="2597">n. 5 Related Work Besides the work of Vaswani et al. (2011) discussed in Section 1, there are several other works using a rule bigram or trigram model in machine translation, Ding and Palmer (2005) use n-gram rule Markov model in the dependency treelet model, Liu and Gildea (2008) applies the same method in a tree-tostring model. Our work is different from theirs in that we lift the Markov assumption and use recurrent neural network to capture much longer contextual information to help probability prediction. Our work is also in the same spirit with tuple sequence models (Marino et al., 2006; Durrani et al., 2013; Hui Zhang, 2013; Wu et al., 2014), which break the translation sequence into bilingual tuples and use a Markov model to capture the dependency of tuples. Comparing to them, we take a more direct approach to use translation rule dependency to guide translation process, rather than rely on tuples which will be significant affected by word alignment errors. Outside of machine translation, the idea of weakening independence assumption by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. Insp</context>
</contexts>
<marker>Durrani, Fraser, Schmid, Hoang, Koehn, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013. Can markov models over minimal translation units help phrase-based smt? In ACL (2), pages 399–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Context-aware discriminative phrase selection for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>159--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Context-aware discriminative phrase selection for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 159–166. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Yao Meng</author>
<author>Hao Yu</author>
</authors>
<title>Discarding monotone composed rule for hierarchical phrase-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International Universal Communication Symposium,</booktitle>
<pages>25--29</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="14668" citStr="He et al. (2009)" startWordPosition="2375" endWordPosition="2378">e same time, factorized RNN-RSM on source and target phrases (fRNN-RSMst) alleviate the data sparse problem in RNN training, resulting in slightly better performance. Finally, when we combine both factorized model, we get the best performance at 28.7 for Moses and 30.8 for Hiero, both significantly better than baseline systems. Also, we conduct an interesting experiment to see if our fRNN-RSM could somehow replace the role of composed rules (rules that can be formed out of smaller rules in the grammar) and guides more finegrained rule-set to produce better translation results. We re-implement He et al. (2009)’s work to filter out monotone composed rules for both Hiero and Moses. We are able to filter out a large number of monotone composed rules, about 50% rules for Hi135 ero and 31% for Moses. The results are shown in Table 2. Interestingly the performance of slimmer translation model with fRNN-RSM exceeds baseline with full rule-table, and catches up with the original fRNN-RSM. The reason is two-folded: first, deleting monotone composed rules doesn’t effect the overall coverage of the rule-set, making limited harm to the system. Second, with less rules, the data sparse problem of RNN training is</context>
</contexts>
<marker>He, Meng, Yu, 2009</marker>
<rawString>Zhongjun He, Yao Meng, and Hao Yu. 2009. Discarding monotone composed rule for hierarchical phrase-based statistical machine translation. In Proceedings of the 3rd International Universal Communication Symposium, pages 25–29. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest rescoring: Fast decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Prague, Czech Rep.,</location>
<contexts>
<context position="12198" citStr="Huang and Chiang, 2007" startWordPosition="1978" endWordPosition="1981">RNN-RSM denotes for factorized RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section 3.2. Results in bold mean that the improvements over “Baseline” are statistically significant (p &lt; 0.05) (Koehn, 2004). to 5 for the extraction of both phrase-based rule and SCFG rule, as well as beam size to 100 and distortion limit to 7 in decoding. Since the rule sequence model belongs to the family of non-local feature (Huang, 2008), traditional testing methods like nbest reranking are not suitable for our experiments. So we adopt hypergraph reranking (Huang and Chiang, 2007; Huang, 2008), which proves to be effective for integrating nonlocal features into dynamic programming. The decoding process is divided into two passes. In the first pass, only standard features (i.e., standard features for phrase-based or HPB model) are used to produce a hypergraph. In the second pass, we use the hypergraph reranking algorithm (Huang, 2008) to find promising translations using additional rule sequence feature. For RNN training, we set the hidden layer size to 512 and classes in the output layer to 256. To obtain phrase-embedding, we use open source tool str2vec1 (Li et al., </context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest rescoring: Fast decoding with integrated language models. In Proceedings of ACL, Prague, Czech Rep., June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL: HLT,</booktitle>
<location>Columbus, OH,</location>
<contexts>
<context position="4385" citStr="Huang, 2008" startWordPosition="670" endWordPosition="671"> for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 132–138, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Distribution on Distribution on source phrases classes of source phrases delayed copy W U Hidden layer, hn Input layer, xn Output layer, yn hn-1 rn-1 Sn-1 tn-1 history. Lastly, we apply our model to both phrasebased and hierarchical phrase-based (HPB) systems and achieve an average improvement of 0.9 BLEU points with much slimmer translation models in hypergraph reranking task (Huang, 2008). 2 Rule Sequence Model We will first brief our rule sequence model with an example from phrase-based system (Koehn et al., 2007). Consider the following translation from Chinese to English: B`ush(Bush yˇu Sh¯aling with Sharon jˇux´ıng hold le hu`ıt´an -ed meeting ‘Bush held a meeting with Sharon’ So one possible rule derivation of the above example could be: (0 ) : (s0, “”) r1 r2 Figure 1: Factorized recurrent neural network with source and target side phrase embeddings. So the rule sequence model does not make any context independence assumption and generate a rule by looking at a context of</context>
<context position="12053" citStr="Huang, 2008" startWordPosition="1958" endWordPosition="1959">st (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section 3.2. Results in bold mean that the improvements over “Baseline” are statistically significant (p &lt; 0.05) (Koehn, 2004). to 5 for the extraction of both phrase-based rule and SCFG rule, as well as beam size to 100 and distortion limit to 7 in decoding. Since the rule sequence model belongs to the family of non-local feature (Huang, 2008), traditional testing methods like nbest reranking are not suitable for our experiments. So we adopt hypergraph reranking (Huang and Chiang, 2007; Huang, 2008), which proves to be effective for integrating nonlocal features into dynamic programming. The decoding process is divided into two passes. In the first pass, only standard features (i.e., standard features for phrase-based or HPB model) are used to produce a hypergraph. In the second pass, we use the hypergraph reranking algorithm (Huang, 2008) to find promising translations using additional rule sequence feature. For RNN training, we s</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the ACL: HLT, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
</authors>
<title>Jianfeng Gao Hui Zhang, Kristina Toutanova.</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<marker>Quirk, 2013</marker>
<rawString>Chris Quirk Jianfeng Gao Hui Zhang, Kristina Toutanova. 2013. Beyond left-to-right: Multiple decomposition structures for smt. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfg models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<pages>632</pages>
<contexts>
<context position="16477" citStr="Johnson, 1998" startWordPosition="2678" endWordPosition="2679"> work is also in the same spirit with tuple sequence models (Marino et al., 2006; Durrani et al., 2013; Hui Zhang, 2013; Wu et al., 2014), which break the translation sequence into bilingual tuples and use a Markov model to capture the dependency of tuples. Comparing to them, we take a more direct approach to use translation rule dependency to guide translation process, rather than rely on tuples which will be significant affected by word alignment errors. Outside of machine translation, the idea of weakening independence assumption by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. Inspired by it, we successfully find a solution for the translation field. 6 Conclusion In this paper, we have presented a novel recurrent neural network based rule sequence model to estimate the probability of translation rule sequences. One of the major advantages of our model is its potential to capture long-span dependency compared with n-gram Markov models. In addition, our factorized model with phrase embedding could further alleviate the data sparse problem in RNN training. Finally we conduct experiments</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24(4):613– 632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL: Demonstrations.</booktitle>
<contexts>
<context position="4514" citStr="Koehn et al., 2007" startWordPosition="690" endWordPosition="693">ages 132–138, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics Distribution on Distribution on source phrases classes of source phrases delayed copy W U Hidden layer, hn Input layer, xn Output layer, yn hn-1 rn-1 Sn-1 tn-1 history. Lastly, we apply our model to both phrasebased and hierarchical phrase-based (HPB) systems and achieve an average improvement of 0.9 BLEU points with much slimmer translation models in hypergraph reranking task (Huang, 2008). 2 Rule Sequence Model We will first brief our rule sequence model with an example from phrase-based system (Koehn et al., 2007). Consider the following translation from Chinese to English: B`ush(Bush yˇu Sh¯aling with Sharon jˇux´ıng hold le hu`ıt´an -ed meeting ‘Bush held a meeting with Sharon’ So one possible rule derivation of the above example could be: (0 ) : (s0, “”) r1 r2 Figure 1: Factorized recurrent neural network with source and target side phrase embeddings. So the rule sequence model does not make any context independence assumption and generate a rule by looking at a context of previous rules. 2.1 Training � 1 if 1j, s.t. p = yj and q = yj+1 Pforced(q |p) = 0 otherwise For each hypothesis, we keep the bo</context>
<context position="11164" citStr="Koehn et al., 2007" startWordPosition="1811" endWordPosition="1814">nce pairs with 25M/21M words of Chinese/English respectively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model with KN-smoothing on the English side of the training data. Case-insensitive BLEU (Papineni et al., 2002) and MERT (Och, 2003) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30.2 +fRNN-RSM (1) 28.9 28.6 30.9 30.6 +fRNN-RSMst (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section 3.2. Results in bold mean that the improvements o</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings ofACL: Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<pages>388--395</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="11833" citStr="Koehn, 2004" startWordPosition="1916" endWordPosition="1917">house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30.2 +fRNN-RSM (1) 28.9 28.6 30.9 30.6 +fRNN-RSMst (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section 3.2. Results in bold mean that the improvements over “Baseline” are statistically significant (p &lt; 0.05) (Koehn, 2004). to 5 for the extraction of both phrase-based rule and SCFG rule, as well as beam size to 100 and distortion limit to 7 in decoding. Since the rule sequence model belongs to the family of non-local feature (Huang, 2008), traditional testing methods like nbest reranking are not suitable for our experiments. So we adopt hypergraph reranking (Huang and Chiang, 2007; Huang, 2008), which proves to be effective for integrating nonlocal features into dynamic programming. The decoding process is divided into two passes. In the first pass, only standard features (i.e., standard features for phrase-bas</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In EMNLP, pages 388–395. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Local phrase reordering models for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>161--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1185" citStr="Kumar and Byrne, 2005" startWordPosition="171" endWordPosition="174">ties of rule sequences. Moreover, our model frees the translation model from keeping huge and redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 1 Introduction Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solving long-distance reordering (Xiong et al., 2006; Zens and Ney, 2006; Kumar and Byrne, 2005), longspan n-gram matching (Charniak et al., 2003; Shen et al., 2008; Yu et al., 2014), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled in most translation systems (Wu et al., 2014). SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). However, it is not an easy task to capture the r</context>
</contexts>
<marker>Kumar, Byrne, 2005</marker>
<rawString>Shankar Kumar and William Byrne. 2005. Local phrase reordering models for statistical machine translation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 161–168. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for itg-based translation. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>567--577</pages>
<contexts>
<context position="8565" citStr="Li et al., 2013" startWordPosition="1375" endWordPosition="1378"> shown in Figure 1. It consists of an input layer x, a hidden layer h (state layer), and an output layer y. The connection weights among layers are denoted by matrixes U and W respectively. Unlike the RNN-RSM, which predicts probability P(rn|rn−1, H(rn−1)), the factorized RNN-RSM predicts probability P(rn|rn−1, H(rn−1), ¯sn−1, ¯tn−1) to generate following rule rn, where ¯sn−1/tn−1 are the source/target side of rn−1, However, ¯sn−1 and tn−1 are still too sparse considering the huge vocabulary size and the diversity in forming phrases, so here we use recursive auto-encoder (Socher et al., 2011; Li et al., 2013) to learn phrase embeddings on both source and target side in an unsupervised mannner, minimizing the reconstruction error. For those rules that are not contained in the training data, the factorized RNN-RSM backs off to the source/target side embedding Esi_1/Eti_1. In the special case that Esi_1 and Eti_1 are dropped, the factorized RNN-RSM goes back to RNN-RSM. Finally, the input layer xn is formed by concatenating the input vectors and hidden layer hn−1 at the preceding time step, as shown in the following equation. [ u ¯s t ] xn = vn−1, vn−1, vn−1, hn−1 (3) The neurons in the hidden and ou</context>
<context position="12803" citStr="Li et al., 2013" startWordPosition="2077" endWordPosition="2080">hiang, 2007; Huang, 2008), which proves to be effective for integrating nonlocal features into dynamic programming. The decoding process is divided into two passes. In the first pass, only standard features (i.e., standard features for phrase-based or HPB model) are used to produce a hypergraph. In the second pass, we use the hypergraph reranking algorithm (Huang, 2008) to find promising translations using additional rule sequence feature. For RNN training, we set the hidden layer size to 512 and classes in the output layer to 256. To obtain phrase-embedding, we use open source tool str2vec1 (Li et al., 2013) to train two autoencoders on the source and target side of rule-table respectively. 4.2 Results Table 1 presents the main results of our paper. To show the merits of our RNN-RSM, we also reimplement Vaswani et al. (2011)’s work, denote as rule Markov model (RMM). It utilize tri-gram rule derivation history for prediction, whereas our RNNRSM could capture arbitrary length of contextual information. We can see that RMM provides a modest improvement over the baseline, 0.6/0.2 points over Moses/Hiero, thanks to the positive guidance 1https://github.com/pengli09/str2vec System w/o monotone Full Mo</context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for itg-based translation. In EMNLP, pages 567–577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Improved treeto-string transducer for machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>62--69</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15626" citStr="Liu and Gildea (2008)" startWordPosition="2534" endWordPosition="2537">nd catches up with the original fRNN-RSM. The reason is two-folded: first, deleting monotone composed rules doesn’t effect the overall coverage of the rule-set, making limited harm to the system. Second, with less rules, the data sparse problem of RNN training is further alleviated, resulting in a better fRNN-RSM for probability prediction. 5 Related Work Besides the work of Vaswani et al. (2011) discussed in Section 1, there are several other works using a rule bigram or trigram model in machine translation, Ding and Palmer (2005) use n-gram rule Markov model in the dependency treelet model, Liu and Gildea (2008) applies the same method in a tree-tostring model. Our work is different from theirs in that we lift the Markov assumption and use recurrent neural network to capture much longer contextual information to help probability prediction. Our work is also in the same spirit with tuple sequence models (Marino et al., 2006; Durrani et al., 2013; Hui Zhang, 2013; Wu et al., 2014), which break the translation sequence into bilingual tuples and use a Markov model to capture the dependency of tuples. Comparing to them, we take a more direct approach to use translation rule dependency to guide translation</context>
</contexts>
<marker>Liu, Gildea, 2008</marker>
<rawString>Ding Liu and Daniel Gildea. 2008. Improved treeto-string transducer for machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 62–69. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Marino</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adria de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e AR Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>549</pages>
<marker>Marino, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B Marino, Rafael E Banchs, Josep M Crego, Adria de Gispert, Patrik Lambert, Jos´e AR Fonollosa, and Marta R Costa-Juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527– 549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<location>Makuhari, Chiba, Japan,</location>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="10752" citStr="Och and Ney, 2004" startWordPosition="1745" endWordPosition="1748">el (Sundermeyer et al., 2014; Cho et al., 2014), except that we focus on capturing rule dependencies which has a much small search space. Noted that this new factorize model provides richer information for prediction, and actually is faster to train since the vocabulary of source/target phrases are much small than that of the translation rules. 4 Experiments 4.1 Setup The training corpus consists of 1M sentence pairs with 25M/21M words of Chinese/English respectively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model with KN-smoothing on the English side of the training data. Case-insensitive BLEU (Papineni et al., 2002) and MERT (Och, 2003) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz Joseph Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="10955" citStr="Och, 2003" startWordPosition="1781" endWordPosition="1782">ction, and actually is faster to train since the vocabulary of source/target phrases are much small than that of the translation rules. 4 Experiments 4.1 Setup The training corpus consists of 1M sentence pairs with 25M/21M words of Chinese/English respectively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model with KN-smoothing on the English side of the training data. Case-insensitive BLEU (Papineni et al., 2002) and MERT (Och, 2003) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30.2 +fRNN-RSM (1) 28.9 28.6 30.9 30.6 +fRNN-RSMst (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-implementation of Vas</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadephia, USA,</location>
<contexts>
<context position="10934" citStr="Papineni et al., 2002" startWordPosition="1775" endWordPosition="1778">ides richer information for prediction, and actually is faster to train since the vocabulary of source/target phrases are much small than that of the translation rules. 4 Experiments 4.1 Setup The training corpus consists of 1M sentence pairs with 25M/21M words of Chinese/English respectively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model with KN-smoothing on the English side of the training data. Case-insensitive BLEU (Papineni et al., 2002) and MERT (Och, 2003) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30.2 +fRNN-RSM (1) 28.9 28.6 30.9 30.6 +fRNN-RSMst (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318, Philadephia, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
</authors>
<title>Do we need phrases?: challenging the conventional wisdom in statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on human language technology conference of the north american chapter of the association of computational linguistics,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2478" citStr="Quirk and Menezes, 2006" startWordPosition="380" endWordPosition="383">a sparsity. There are two major solutions: the first one is breaking the rules into bilingual wordpairs and use a n-gram translation model to incorporate lexical dependencies that span rule boundaries (Marino et al., 2006; Durrani et al., 2013). These ngram models (also known as tuple sequence model) could help phrase-based translation models to overcome the phrasal independence assumption, but they rely on word alignment to extract bilingual tuples, which brings in additional alignment error (Wu et al., 2014). The other direction lies in utilizing the rule Markov model (Vaswani et al., 2011; Quirk and Menezes, 2006), which directly explores dependencies in rule derivation history and achieves both good performance and slimmer translation model in syntax-based SMT systems. However, the sparsity of translation rules entails aggressive pruning of the training data and constrains the model from scaling to high order grams, significantly limiting the ability of the model. In this paper we follow the second line and propose a novel recurrent neural network based rule sequence model (RNN-RSM), which utilizes the representational power of recurrent neural network (RNN) to capture arbitrary distance of contextual</context>
</contexts>
<marker>Quirk, Menezes, 2006</marker>
<rawString>Chris Quirk and Arul Menezes. 2006. Do we need phrases?: challenging the conventional wisdom in statistical machine translation. In Proceedings of the main conference on human language technology conference of the north american chapter of the association of computational linguistics, pages 9–16. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph M Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>577--585</pages>
<contexts>
<context position="1253" citStr="Shen et al., 2008" startWordPosition="183" endWordPosition="186">rom keeping huge and redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 1 Introduction Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solving long-distance reordering (Xiong et al., 2006; Zens and Ney, 2006; Kumar and Byrne, 2005), longspan n-gram matching (Charniak et al., 2003; Shen et al., 2008; Yu et al., 2014), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled in most translation systems (Wu et al., 2014). SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). However, it is not an easy task to capture the rule dependency, which entails much longer context and more severe da</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph M Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In ACL, pages 577–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semisupervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8547" citStr="Socher et al., 2011" startWordPosition="1371" endWordPosition="1374">n the input layer, as shown in Figure 1. It consists of an input layer x, a hidden layer h (state layer), and an output layer y. The connection weights among layers are denoted by matrixes U and W respectively. Unlike the RNN-RSM, which predicts probability P(rn|rn−1, H(rn−1)), the factorized RNN-RSM predicts probability P(rn|rn−1, H(rn−1), ¯sn−1, ¯tn−1) to generate following rule rn, where ¯sn−1/tn−1 are the source/target side of rn−1, However, ¯sn−1 and tn−1 are still too sparse considering the huge vocabulary size and the diversity in forming phrases, so here we use recursive auto-encoder (Socher et al., 2011; Li et al., 2013) to learn phrase embeddings on both source and target side in an unsupervised mannner, minimizing the reconstruction error. For those rules that are not contained in the training data, the factorized RNN-RSM backs off to the source/target side embedding Esi_1/Eti_1. In the special case that Esi_1 and Eti_1 are dropped, the factorized RNN-RSM goes back to RNN-RSM. Finally, the input layer xn is formed by concatenating the input vectors and hidden layer hn−1 at the preceding time step, as shown in the following equation. [ u ¯s t ] xn = vn−1, vn−1, vn−1, hn−1 (3) The neurons in</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semisupervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLP,</booktitle>
<volume>30</volume>
<pages>901--904</pages>
<contexts>
<context position="10795" citStr="Stolcke, 2002" startWordPosition="1754" endWordPosition="1755">, except that we focus on capturing rule dependencies which has a much small search space. Noted that this new factorize model provides richer information for prediction, and actually is faster to train since the vocabulary of source/target phrases are much small than that of the translation rules. 4 Experiments 4.1 Setup The training corpus consists of 1M sentence pairs with 25M/21M words of Chinese/English respectively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used the SRILM toolkit (Stolcke, 2002) to train a 4-gram language model with KN-smoothing on the English side of the training data. Case-insensitive BLEU (Papineni et al., 2002) and MERT (Och, 2003) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Tamer Alkhouli</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Translation modeling with bidirectional recurrent neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing,</booktitle>
<contexts>
<context position="10162" citStr="Sundermeyer et al., 2014" startWordPosition="1652" endWordPosition="1655"> and target side phrase ¯tn. So the probability formula could be rewrite as: P(rn|H(rn)) = P(sn, tn|H(rn)) = P(sn|H(rn)) × P(tn|sn, H(rn)) (6) The first sub-model P(sn, |H(rn)) computes the probability distribution over source phrases. Then the second sub-model P(tn|sn, H(rn)) computes the probability distribution over tn that are translated from sn. The two sub-models are computed with the similar recurrent network shown in Figure 1 except adding the source side information sn of the current rule rn into the input layer. This method share the same spirit with the RNN-based translation model (Sundermeyer et al., 2014; Cho et al., 2014), except that we focus on capturing rule dependencies which has a much small search space. Noted that this new factorize model provides richer information for prediction, and actually is faster to train since the vocabulary of source/target phrases are much small than that of the translation rules. 4 Experiments 4.1 Setup The training corpus consists of 1M sentence pairs with 25M/21M words of Chinese/English respectively. Our development and test set are NIST 2006 and 2008 (newswire portion) respectively. We obtained alignments by running GIZA++ (Och and Ney, 2004) and used </context>
</contexts>
<marker>Sundermeyer, Alkhouli, Wuebker, Ney, 2014</marker>
<rawString>Martin Sundermeyer, Tamer Alkhouli, Joern Wuebker, and Hermann Ney. 2014. Translation modeling with bidirectional recurrent neural networks. In Proceedings of the Conference on Empirical Methods on Natural Language Processing, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Rule markov models for fast tree-tostring translation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="2452" citStr="Vaswani et al., 2011" startWordPosition="376" endWordPosition="379">xt and more severe data sparsity. There are two major solutions: the first one is breaking the rules into bilingual wordpairs and use a n-gram translation model to incorporate lexical dependencies that span rule boundaries (Marino et al., 2006; Durrani et al., 2013). These ngram models (also known as tuple sequence model) could help phrase-based translation models to overcome the phrasal independence assumption, but they rely on word alignment to extract bilingual tuples, which brings in additional alignment error (Wu et al., 2014). The other direction lies in utilizing the rule Markov model (Vaswani et al., 2011; Quirk and Menezes, 2006), which directly explores dependencies in rule derivation history and achieves both good performance and slimmer translation model in syntax-based SMT systems. However, the sparsity of translation rules entails aggressive pruning of the training data and constrains the model from scaling to high order grams, significantly limiting the ability of the model. In this paper we follow the second line and propose a novel recurrent neural network based rule sequence model (RNN-RSM), which utilizes the representational power of recurrent neural network (RNN) to capture arbitr</context>
<context position="11573" citStr="Vaswani et al. (2011)" startWordPosition="1877" endWordPosition="1880">03) were used for evaluation and tuning. We test our method on both phrase-based and hierarchical phrase-based translation models. For phrase-based system, we use Moses with standard features (Koehn et al., 2007). While for hierarchical phrase-based model, we use a in-house implementation of Hiero (Chiang, 2005). We set phrase-limit 134 System Moses Hiero dev-set test-set dev-set test-set Baseline 28.4 27.7 30.4 30.0 +RMM 28.7 28.3 30.7 30.2 +fRNN-RSM (1) 28.9 28.6 30.9 30.6 +fRNN-RSMst (2) 29.3 28.5 31.2 30.7 +(1)+(2) 29.6 28.7 31.4 30.8 Table 1: Main results. RMM is the re-implementation of Vaswani et al. (2011), fRNN-RSM denotes for factorized RNN-RSM describe in Section 3.1, fRNN-RSMst denotes for RNN-RSM factorized by source/target side in Section 3.2. Results in bold mean that the improvements over “Baseline” are statistically significant (p &lt; 0.05) (Koehn, 2004). to 5 for the extraction of both phrase-based rule and SCFG rule, as well as beam size to 100 and distortion limit to 7 in decoding. Since the rule sequence model belongs to the family of non-local feature (Huang, 2008), traditional testing methods like nbest reranking are not suitable for our experiments. So we adopt hypergraph rerankin</context>
<context position="13024" citStr="Vaswani et al. (2011)" startWordPosition="2117" endWordPosition="2120">andard features for phrase-based or HPB model) are used to produce a hypergraph. In the second pass, we use the hypergraph reranking algorithm (Huang, 2008) to find promising translations using additional rule sequence feature. For RNN training, we set the hidden layer size to 512 and classes in the output layer to 256. To obtain phrase-embedding, we use open source tool str2vec1 (Li et al., 2013) to train two autoencoders on the source and target side of rule-table respectively. 4.2 Results Table 1 presents the main results of our paper. To show the merits of our RNN-RSM, we also reimplement Vaswani et al. (2011)’s work, denote as rule Markov model (RMM). It utilize tri-gram rule derivation history for prediction, whereas our RNNRSM could capture arbitrary length of contextual information. We can see that RMM provides a modest improvement over the baseline, 0.6/0.2 points over Moses/Hiero, thanks to the positive guidance 1https://github.com/pengli09/str2vec System w/o monotone Full Moses Hiero Moses Hiero Baseline 27.4 29.8 27.7 30.0 +RMM 27.6 29.9 28.3 30.2 +fRNN-RSM 28.0 30.4 28.6 30.6 +fRNN-RSMst 28.2 30.6 28.5 30.7 Table 2: BLEU score comparison on different rule-set, “w/o monotone” denotes we fil</context>
<context position="15404" citStr="Vaswani et al. (2011)" startWordPosition="2497" endWordPosition="2500"> monotone composed rules, about 50% rules for Hi135 ero and 31% for Moses. The results are shown in Table 2. Interestingly the performance of slimmer translation model with fRNN-RSM exceeds baseline with full rule-table, and catches up with the original fRNN-RSM. The reason is two-folded: first, deleting monotone composed rules doesn’t effect the overall coverage of the rule-set, making limited harm to the system. Second, with less rules, the data sparse problem of RNN training is further alleviated, resulting in a better fRNN-RSM for probability prediction. 5 Related Work Besides the work of Vaswani et al. (2011) discussed in Section 1, there are several other works using a rule bigram or trigram model in machine translation, Ding and Palmer (2005) use n-gram rule Markov model in the dependency treelet model, Liu and Gildea (2008) applies the same method in a tree-tostring model. Our work is different from theirs in that we lift the Markov assumption and use recurrent neural network to capture much longer contextual information to help probability prediction. Our work is also in the same spirit with tuple sequence models (Marino et al., 2006; Durrani et al., 2013; Hui Zhang, 2013; Wu et al., 2014), wh</context>
</contexts>
<marker>Vaswani, Mi, Huang, Chiang, 2011</marker>
<rawString>Ashish Vaswani, Haitao Mi, Liang Huang, and David Chiang. 2011. Rule markov models for fast tree-tostring translation. In Proceedings of ACL 2011, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youzheng Wu</author>
<author>Taro Watanabe</author>
<author>Chiori Hori</author>
</authors>
<title>Recurrent neural network-based tuple sequence model for machine translation.</title>
<date>2014</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>1908--1917</pages>
<contexts>
<context position="1434" citStr="Wu et al., 2014" startWordPosition="212" endWordPosition="215">, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 1 Introduction Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solving long-distance reordering (Xiong et al., 2006; Zens and Ney, 2006; Kumar and Byrne, 2005), longspan n-gram matching (Charniak et al., 2003; Shen et al., 2008; Yu et al., 2014), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled in most translation systems (Wu et al., 2014). SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). However, it is not an easy task to capture the rule dependency, which entails much longer context and more severe data sparsity. There are two major solutions: the first one is breaking the rules into bilingual wordpairs and use a n-gram translation model to incorporate lexical dependencies that </context>
<context position="16000" citStr="Wu et al., 2014" startWordPosition="2601" endWordPosition="2604">aswani et al. (2011) discussed in Section 1, there are several other works using a rule bigram or trigram model in machine translation, Ding and Palmer (2005) use n-gram rule Markov model in the dependency treelet model, Liu and Gildea (2008) applies the same method in a tree-tostring model. Our work is different from theirs in that we lift the Markov assumption and use recurrent neural network to capture much longer contextual information to help probability prediction. Our work is also in the same spirit with tuple sequence models (Marino et al., 2006; Durrani et al., 2013; Hui Zhang, 2013; Wu et al., 2014), which break the translation sequence into bilingual tuples and use a Markov model to capture the dependency of tuples. Comparing to them, we take a more direct approach to use translation rule dependency to guide translation process, rather than rely on tuples which will be significant affected by word alignment errors. Outside of machine translation, the idea of weakening independence assumption by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. Inspired by it, we successfully find a </context>
</contexts>
<marker>Wu, Watanabe, Hori, 2014</marker>
<rawString>Youzheng Wu, Taro Watanabe, and Chiori Hori. 2014. Recurrent neural network-based tuple sequence model for machine translation. In Proc. COLING, pages 1908–1917.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>521--528</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1141" citStr="Xiong et al., 2006" startWordPosition="163" endWordPosition="166"> information during estimating probabilities of rule sequences. Moreover, our model frees the translation model from keeping huge and redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 1 Introduction Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solving long-distance reordering (Xiong et al., 2006; Zens and Ney, 2006; Kumar and Byrne, 2005), longspan n-gram matching (Charniak et al., 2003; Shen et al., 2008; Yu et al., 2014), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled in most translation systems (Wu et al., 2014). SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). Howe</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 521–528. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Liang Huang</author>
<author>Haitao Mi</author>
<author>Kai Zhao</author>
</authors>
<title>Max-violation perceptron and forced decoding for scalable mt training.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1112--1123</pages>
<contexts>
<context position="7010" citStr="Yu et al. (2013)" startWordPosition="1128" endWordPosition="1131"> limit to prohibit long distance reordering, so the ending position of last phrase is maintained (e.g., 1 and 6 in the coverage vector). In our example, translation rules r1, r2, r3 form a derivation T which leads to a complete translation. So for rule sequence model, the probability of rn depends on its derivation history H(rn): P(rn) = P(rn|H(rn)) (1) and the probability of a rule derivation T is P(T) = � P(ri|H(ri)) (2) ri∈T The rule sequence model can then be trained on the path set of rule derivations. To obtain golden derivations of translation rules for each sentence pair, We r3 follow Yu et al. (2013) to utilize force decoding to get golden rule derivations. Specifically, we define a new forced decoding LM which only accepts two consecutive words (denote as p, q) in the reference translation (yi): 133 to estimate the probability P(rn|H(rn)). Our RNNRSM can potentially capture arbitrary long context rather than n-1 previous rules limited by Markov assumption. Following Mikolov et al. (2010), we adopt the standard RNN architecture: the input layer encodes previous translation rule using one-hot coding, the output layer produces a probability distribution over all translation rules, and the h</context>
</contexts>
<marker>Yu, Huang, Mi, Zhao, 2013</marker>
<rawString>Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao. 2013. Max-violation perceptron and forced decoding for scalable mt training. In EMNLP, pages 1112– 1123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Yu</author>
<author>Haitao Mi</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>A structured language model for incremental tree-tostring translation.</title>
<date>2014</date>
<contexts>
<context position="1271" citStr="Yu et al., 2014" startWordPosition="187" endWordPosition="190">d redundant grammars, resulting in more efficient training and decoding. Experimental results show that our method achieves a 0.9 point BLEU gain over the baseline, and a significant reduction in rule table size for both phrase-based and hierarchical phrase-based systems. 1 Introduction Modeling long-distance dependency has always been a bottleneck for statistical machine translation (SMT). While lots of efforts have been made in solving long-distance reordering (Xiong et al., 2006; Zens and Ney, 2006; Kumar and Byrne, 2005), longspan n-gram matching (Charniak et al., 2003; Shen et al., 2008; Yu et al., 2014), much less attention has been concentrated on capturing translation rule dependency, which is not explicitly modeled in most translation systems (Wu et al., 2014). SMT systems typically model the translation process as a sequence of translation steps, each of which uses a translation rule. These rules are usually applied independently of each other, which violates the conventional wisdom that translation should be done in context (Gim´enez and M`arquez, 2007). However, it is not an easy task to capture the rule dependency, which entails much longer context and more severe data sparsity. There</context>
</contexts>
<marker>Yu, Mi, Huang, Liu, 2014</marker>
<rawString>Heng Yu, Haitao Mi, Liang Huang, and Qun Liu. 2014. A structured language model for incremental tree-tostring translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>