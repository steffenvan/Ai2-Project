<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002757">
<title confidence="0.982203">
Learning Stochastic Bracketing Inversion Transduction Grammars
with a Cubic Time Biparsing Algorithm
</title>
<author confidence="0.979094">
Markus SAERS Joakim NIVRE
</author>
<affiliation confidence="0.933712333333333">
Dept. of Linguistics and Philology
Uppsala University
Sweden
</affiliation>
<email confidence="0.968766">
first.last@lingfil.uu.se
</email>
<author confidence="0.973236">
Dekai WU
</author>
<affiliation confidence="0.830802666666667">
Human Language Technology Center
Dept. of Computer Science and Engineering
HKUST
</affiliation>
<address confidence="0.347879">
Hong Kong
</address>
<email confidence="0.989787">
dekai@cs.ust.hk
</email>
<sectionHeader confidence="0.997286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920928571429">
We present a biparsing algorithm for
Stochastic Bracketing Inversion Transduc-
tion Grammars that runs in O(bn3) time
instead of O(n6). Transduction gram-
mars learned via an EM estimation proce-
dure based on this biparsing algorithm are
evaluated directly on the translation task,
by building a phrase-based statistical MT
system on top of the alignments dictated
by Viterbi parses under the induced bi-
grammars. Translation quality at different
levels of pruning are compared, showing
improvements over a conventional word
aligner even at heavy pruning levels.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981588235294">
As demonstrated by Saers &amp; Wu (2009) there
is something to be gained by applying structural
models such as Inversion Transduction Grammars
(ITG) to the problem of word alignment. One is-
sue is that naive methods for inducing ITGs from
parallel data can be very time consuming. We in-
troduce a parsing algorithm for inducing Stochas-
tic Bracketing ITGs from parallel data in O(bn3)
time instead of O(n6), where b is a pruning param-
eter (lower = tighter pruning). We try out different
values for b, and evaluate the results on a transla-
tion tasks.
In section 2 we summarize the ITG framework;
in section 3 we present our algorithm, whose time
complexity is analyzed in section 4. In section 5
we describe how the algorithm is evaluated, and in
section 6, the empirical results are given.
</bodyText>
<sectionHeader confidence="0.998203" genericHeader="method">
2 Inversion Transduction Grammars
</sectionHeader>
<bodyText confidence="0.98842255">
Inversion transductions are a theoretically inter-
esting and empirically useful equivalence class of
transductions, with expressiveness and computa-
tional complexity characteristics lying intermedi-
ate between finite-state transductions and syntax-
directed transductions. An Inversion Transduc-
tion Grammar (ITG) can be used to synchronously
generate sentence pairs, synchronously parse sen-
tence pairs, or transduce from a sentence in one
language to a sentence in another.1
The equivalence class of inversion transduc-
tions can be described by restricting Syntax-
Directed Transduction Grammars (SDTG)2 in var-
ious equivalent ways to the special cases of (a) bi-
nary SDTGs, (b) ternary SDTGs, or (c) SDTGs
whose transduction rules are restricted to straight
and inverted permutations only.
Thus on one hand, any binary or ternary SDTG
is an ITG. Conversely, any ITG can be stated in
binary two-normal form (Wu, 1997). Only three
kinds of rules are present in the normal form:
A [BC]
A (BC)
A e/f
On the other hand, under characterization (c),
what distinguishes ITGs is that the permutation of
constituents is restricted in such a way that all chil-
dren of a node must be read either left-to-right, or
right-to-left. The movement only applies to one of
the languages, the other is fixed. Formally, an ITG
is a tuple (N, V, A, 5), where N is a set of nonter-
minal symbols, A is a set of rewrite rules, 5 E N
is the start symbol and V C_ VE x VF is a set of
biterminal symbols, where VE is the vocabulary of
E and VF is the vocabulary of F. We will write a
biterminal as e/f, where e E VE and f E VF. A
sentence pair will be written as e/f, and a bispan
as es..t/fu..v.
Each rule δ E A is a tuple (X, γ, θ) where
X E N is the right hand side of the rule, γ E
</bodyText>
<footnote confidence="0.9950682">
1All transduction grammars (a.k.a. synchronous gram-
mars, or simply bigrammars) can be interpreted as models
for generation, recognition, or transduction.
2SDTGs (Lewis &amp; Stearns (1968); Aho &amp; Ullman (1969),
(1972)) are also recently called synchronous CFGs.
</footnote>
<page confidence="0.993956">
29
</page>
<bodyText confidence="0.968412176470588">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 29–32,
Paris, October 2009. c�2009 Association for Computational Linguistics
{N ∪ V }∗ is a series of nonterminal and biter-
minal symbols representing the production of the
rule and θ ∈ {∅, [], hi} denotes the orientation (ax-
iomatic, straight or inverted) of the rule. Straight
rules are read left-to-right in both languages, while
inverted rules are read left-to-right in E and right-
to-left in F. The direction of the axiomatic rules is
undefined, as they must be completely made up of
terminals. For notational convenience, the orien-
tation of the rule is written as surrounding the pro-
duction, like so: X → &apos;y, X → [&apos;y] and X → h&apos;yi.
The vocabularies of the languages may both in-
clude the empty token E, allowing for deletions
and insertions. The empty biterminal, E/E is not
allowed.
</bodyText>
<subsectionHeader confidence="0.982283">
2.1 Stochastic ITGs
</subsectionHeader>
<bodyText confidence="0.9996145">
In a Stochastic ITG (SITG), each rule is also asso-
ciated with a probability, such that
</bodyText>
<equation confidence="0.8927915">
� Pr(X → &apos;y) = 1
γ
</equation>
<bodyText confidence="0.9909535">
for all X ∈ N. The probability of a deriva-
tion S ⇒∗ e/f is defined as the production of
the probabilities of all rules used. As shown by
Wu (1995), it is possible to fit the parameters of
a SITG to a parallel corpus via EM (expectation-
maximization) estimation.
</bodyText>
<subsectionHeader confidence="0.999621">
2.2 Bracketing ITGs
</subsectionHeader>
<bodyText confidence="0.999935666666667">
An ITG where there is only one nonterminal (other
than the start symbol) is called a bracketing ITG
(BITG). Since the one nonterminal is devoid of
information, it can only be used to group its chil-
dren together, imposing a bracketing on the sen-
tence pairs.
</bodyText>
<sectionHeader confidence="0.905575" genericHeader="method">
3 Parsing SBITGs
</sectionHeader>
<bodyText confidence="0.999983463414634">
In this section we present a biparsing algorithm
for Stochastic Bracketing Inversion Transduction
Grammars (SBITGs) in normal form which incor-
porates a pruning parameter b. The algorithm is
basically an agenda-based bottom-up chart parser,
where the pruning parameter controls the number
of active items of a given length.
To parse a sentence pair e/f, the parser needs
a chart C and a series of T + V agendas
A1, A2, ... , AT+V , where T = |e |and V = |f|.
An item is defined as a nonterminal symbol (we
use X to denote the anonymous nonterminal sym-
bol of the bracketing ITG) and one span in each
language, written as Xstuv where 0 ≤ s ≤ t ≤ T
corresponds to the span es..t and 0 ≤ u ≤ v ≤ V
corresponds to the span fu..v. The length of an
item is defined as |Xstuv |= (t−s)+(v−u). Since
items are grouped by their length, highly skewed
links (eg. 6:1) will be competing with very even
links (eg. 4:3). Skewed links are generally bad
(and should be pruned), or have a high probability
(which means they are likely to survive pruning).
An item may be active or passive, the active items
are present in the agendas and the chart, whereas
the passive items are only present in the chart.
The parser starts by asserting items from all lex-
ical rules (X → e/f), and placing them on their
respective agendas. After the initial seeding, the
agendas are processed in order. When an agenda
is processed, it is first pruned, so that only the b
best items are kept active. After pruning, the re-
maining active items are allowed to be extended.
When extended, the item combines with an adja-
cent item in the chart to form a larger item. The
newly created item is considered active, and added
to both the chart and the appropriate agenda. Once
an item has been processed it goes from being ac-
tive to being passive. The process is halted when
the goal item S0T0V is reached, or when no active
items remain. To build the forest corresponding to
the parse process, back-pointers are used.
</bodyText>
<subsectionHeader confidence="0.972614">
3.1 Initialization
</subsectionHeader>
<bodyText confidence="0.999916666666667">
In the initial step, the set of lexical items L is built.
All lexical items i ∈ L are then activated by plac-
ing them on their corresponding agenda A|i|.
</bodyText>
<equation confidence="0.7286758">
0≤s≤t≤T,
0≤u≤v≤V,
X → es..t/fu..v ∈ A ⎫
⎭
⎬
</equation>
<bodyText confidence="0.99989275">
By limiting the length of phrasal terminals to some
threshold µ, the variables t and v can be limited to
s+µ and u+µ respectively, limiting the complexity
of the initialization step from O(n4) to O(n2).
</bodyText>
<subsectionHeader confidence="0.993474">
3.2 Recursion
</subsectionHeader>
<bodyText confidence="0.9997085">
In the recursive step we build a set of extensions
E(i) for all active items i. All items in E(i)
are then activated by placing them on their cor-
responding agenda (i ∈ A|i|).
</bodyText>
<equation confidence="0.71695675">
E(Xstuv) =
{XStUv|0≤S≤s,0≤U ≤u,XSsUu ∈ C} ∪
{XsSuU|t≤S≤T,v≤U ≤V,XtSvU ∈ C} ∪
{XsSUv|t≤S≤T,0≤U ≤u,XtSUu ∈ C} ∪
{XStuU|0≤S≤s,v≤U ≤V,XSsvU ∈ C}
L= ⎧ Xstuv ������
⎨
⎩
</equation>
<page confidence="0.976844">
30
</page>
<bodyText confidence="0.9999934">
Since we are processing the agendas in order, any
item in the chart will be as long as or shorter than
the item being extended. This fact can be exploited
to limit the number of possible siblings explored,
but has no impact on time complexity.
</bodyText>
<subsectionHeader confidence="0.999354">
3.3 Viterbi parsing
</subsectionHeader>
<bodyText confidence="0.999389142857143">
When doing Viterbi parsing, all derivations but
the most probable are discarded. This gives an
unambiguous parse, which dictates exactly one
alignment between e and f. The alignment of
the Viterbi parse can be used to substitute that of
other word aligners (Saers and Wu, 2009) such as
GIZA++ (Och and Ney, 2003).
</bodyText>
<sectionHeader confidence="0.997356" genericHeader="method">
4 Analysis
</sectionHeader>
<bodyText confidence="0.999933291666667">
Looking at the algorithm, it is clear that there will
be a total of T + V = O(n) agendas, each con-
taining items of a certain length. The items in an
agenda can start anywhere in the alignment space:
O(n2) possible starting points, but once the end
point in one language is set, the end point in the
other follows from that, adding a factor O(n).
This means that each agenda contains O(n3) ac-
tive items. Each active item has to go through all
possible siblings in the recursive step. Since the
start point of the sibling is determined by the item
itself (it has to be adjacent), only the O(n2) pos-
sible end points have to be explored. This means
that each active item takes O(n2) time to process.
The total time is thus O(n6): O(n) agendas,
containing O(n3) active items, requiring O(n2)
time to process. This is also the time complex-
ity reported for ITGs in previous work (Wu, 1995;
Wu, 1997).
The pruning works by limiting the number of
active items in an agenda to a constant b, meaning
that there are O(n) agendas, containing O(b) ac-
tive items, requiring O(n2) time to process. This
gives a total time complexity of O(bn3).
</bodyText>
<sectionHeader confidence="0.998854" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.9999415">
We evaluate the parser on a translation task
(WMT’08 shared task3). In order to evaluate on
a translation task, a translation system has to be
built. We use the alignments from the Viterbi
parses of the training corpus to substitute the
alignments of GIZA++. This is the same approach
as taken in Saers &amp; Wu (2009). We will evalu-
ate the resulting translations with two automatic
</bodyText>
<footnote confidence="0.965687">
3http://www.statmt.org/wmt08/
</footnote>
<note confidence="0.9481">
metrics: BLEU (Papineni et al., 2002) and NIST
(Doddington, 2002).
</note>
<sectionHeader confidence="0.985588" genericHeader="method">
6 Empirical results
</sectionHeader>
<bodyText confidence="0.999936">
In this section we describe the experimental setup
as well as the outcomes.
</bodyText>
<subsectionHeader confidence="0.99754">
6.1 Setup
</subsectionHeader>
<bodyText confidence="0.999991777777778">
We use the Moses Toolkit (Koehn et al., 2007) to
train our phrase-based SMT models. The toolkit
also includes scripts for applying GIZA++ (Och
and Ney, 2003) as a word aligner. We have
trained several systems, one using GIZA++ (our
baseline system), one with no pruning at all, and
6 different values of b (1, 10, 25, 50, 75 and
100). We used the grow-diag-final-and
method to extract phrases from the word align-
ment, and MERT (Och, 2003) to optimize the re-
sulting model. We trained a 5-gram SRI language
model (Stolcke, 2002) using the corpus supplied
for this purpose by the shared task organizers. All
of the above is consistent with the guidelines for
building a baseline system for the WMT’08 shared
task.
The translation tasks we applied the above
procedure to are all taken from the Europarl
corpus (Koehn, 2005). We selected the tasks
German-English, French-English and Spanish-
English. Furthermore, we restricted the training
sentence pairs so that none of the sentences ex-
ceeded length 10. This was necessary to be able to
carry out exhaustive search. The total amount of
training data was roughly 100,000 sentence pairs
in each language pair, which is a relatively small
corpus, but by no means a toy example.
</bodyText>
<subsectionHeader confidence="0.998298">
6.2 Grammar induction
</subsectionHeader>
<bodyText confidence="0.998088">
It is possible to set the parameters of a SBITG
by applying EM to an initial guess (Wu, 1995).
As our initial guess, we used word co-occurrence
counts, assuming that there was one empty token
in each sentence. This gave an estimate of the lex-
ical rules. The probability mass was divided so
that the lexical rules could share half of it, while
the other half was shared equally by the two struc-
tural rules (X → [XX] and X → (XX)).
Several training runs were made with different
pruning parameters. The EM process was halted
when a relative improvement in log-likelihood of
10−3 was no longer achieved over the previous it-
eration.
</bodyText>
<page confidence="0.999732">
31
</page>
<table confidence="0.9999495">
Baseline Different values of b for SBITGs
Metric (GIZA++) ∞ 100 75 50 25 10 1
Spanish-English
BLEU 0.2597 0.2663 0.2671 0.2661 0.2653 0.2655 0.2608 0.1234
NIST 6.6352 6.7407 6.7445 6.7329 6.7101 6.7312 6.6439 3.9705
time 03:20:00 02:40:00 02:00:00 01:20:00 00:38:00 00:17:00 00:03:10
German-English
BLEU 0.2059 0.2113 0.2094 0.2091 0.2090 0.2091 0.2050 0.0926
NIST 5.8668 5.9380 5.9086 5.8955 5.8947 5.9292 5.8743 3.4297
time 03:40:00 02:45:00 02:10:00 01:25:00 00:41:00 00:17:00 00:03:20
French-English
BLEU 0.2603 0.2663 0.2655 0.2668 0.2669 0.2654 0.2632 0.1268
NIST 6.6907 6.8151 6.8068 6.8068 6.8065 6.7013 6.7136 4.0849
time 03:10:00 02:45:00 02:10:00 01:25:00 00:42:00 00:17:00 00:03:25
</table>
<tableCaption confidence="0.999887">
Table 1: Results. Time measures are approximate time per iteration.
</tableCaption>
<bodyText confidence="0.999127">
Once the EM process terminated, Viterbi parses
were calculated for the training corpus, and the
alignments from them outputted in the same for-
mat produced by GIZA++.
</bodyText>
<subsectionHeader confidence="0.55391">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999986222222222">
The results are presented in Table 1. GIZA++
generally terminates within minutes (6–7) on the
training corpora used, making it faster than any
of the SBITGs (they generally required 4–6 iter-
ations to terminate, making even the fastest ones
slower than GIZA++). To put the times in per-
spective, about 6 iterations were needed to get
the ITGs to converge, making the longest training
time about 16–17 hours. The time it takes to ex-
tract the phrases and tune the model using MERT
is about 14 hours for these data sets.
Looking at translation quality, we see a sharp
initial rise as b grows to 10. At this point the
SBITG system is on par with GIZA++. It con-
tinues to rise up to b = 25, but after that is more or
less levels out. From this we conclude that the pos-
itive results reported in Saers &amp; Wu (2009) hold
under harsh pruning.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999926857142857">
We have presented a SBITG biparsing algorithm
that uses a novel form of pruning to cut the com-
plexity of EM-estimation from O(n6) to O(bn3).
Translation quality using the resulting learned
SBITG models is improved over using conven-
tional word alignments, even under harsh levels of
pruning.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.894396888888889">
The authors are grateful for the comments made by the two anonymous review-
ers. This work was funded by the Swedish National Graduate School of Lan-
guage Technology, the Defense Advanced Research Projects Agency (DARPA)
under GALE Contract No. HR0011-06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants GRF621008, DAG03/04.EG09,
RGC6256/00E, and RGC6083/99E. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the authors and do
not necessarily reflect the views of the Defense Advanced Research Projects
Agency.
</bodyText>
<sectionHeader confidence="0.994506" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999599512195122">
Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax-directed translations
and the pushdown assembler. Journal of Computer and System Sciences,
3(1):37–56.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Transla-
tion, and Compiling (Volumes 1 and 2). Prentice-Halll, Englewood Cliffs,
NJ.
George Doddington. 2002. Automatic evaluation of machine translation qual-
ity using n-gram co-occurrence statistics. In Human Language Technology
conference (HLT-2002), San Diego, CA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for statistical machine trans-
lation. In ACL-2007 Demo and Poster Sessions, pages 177–180, Prague,
Jun.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine trans-
lation. In Machine Translation Summit X, Phuket, Thailand, September.
Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction.
Journal of the Association for Computing Machinery, 15(3):465–488.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1):19–52.
Franz Josef Och. 2003. Minimum error rate training in statistical machine
translation. In 41st Annual Meeting of the Association for Computational
Linguistics, pages 160–167, Sapporo, Japan, Jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:
A method for automatic evaluation of machine translations. In 40th Annual
Meeting of the Association for Computational Linguistics (ACL-2002),
pages 311–318, Philadelphia, Jul.
Markus Saers and Dekai Wu. 2009. Improving phrase-based translation via
word alignments from Stochastic Inversion Transduction Grammars. In
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statis-
tical Translation (at NAACL HLT 2009), pages 28–36, Boulder, CO, Jun.
Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit.
In International Conference on Spoken Language Processing, Denver, CO,
Sep.
Dekai Wu. 1995. Trainable coarse bilingual grammars for parallel text brack-
eting. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages
69–81, Cambridge, MA, Jun.
Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual
parsing of parallel corpora. Computational Linguistics, 23(3):377–404,
Sep.
</reference>
<page confidence="0.999299">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.312986">
<title confidence="0.9856705">Learning Stochastic Bracketing Inversion Transduction with a Cubic Time Biparsing Algorithm</title>
<affiliation confidence="0.97193125">Dept. of Linguistics and Uppsala Human Language Technology Dept. of Computer Science and</affiliation>
<address confidence="0.360754">Hong</address>
<email confidence="0.954275">dekai@cs.ust.hk</email>
<abstract confidence="0.9982392">We present a biparsing algorithm for Stochastic Bracketing Inversion Transduc- Grammars that runs in of Transduction grammars learned via an EM estimation procedure based on this biparsing algorithm are evaluated directly on the translation task, by building a phrase-based statistical MT system on top of the alignments dictated by Viterbi parses under the induced bigrammars. Translation quality at different levels of pruning are compared, showing improvements over a conventional word aligner even at heavy pruning levels.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Syntax-directed translations and the pushdown assembler.</title>
<date>1969</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="3678" citStr="Aho &amp; Ullman (1969)" startWordPosition="607" endWordPosition="610"> set of nonterminal symbols, A is a set of rewrite rules, 5 E N is the start symbol and V C_ VE x VF is a set of biterminal symbols, where VE is the vocabulary of E and VF is the vocabulary of F. We will write a biterminal as e/f, where e E VE and f E VF. A sentence pair will be written as e/f, and a bispan as es..t/fu..v. Each rule δ E A is a tuple (X, γ, θ) where X E N is the right hand side of the rule, γ E 1All transduction grammars (a.k.a. synchronous grammars, or simply bigrammars) can be interpreted as models for generation, recognition, or transduction. 2SDTGs (Lewis &amp; Stearns (1968); Aho &amp; Ullman (1969), (1972)) are also recently called synchronous CFGs. 29 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 29–32, Paris, October 2009. c�2009 Association for Computational Linguistics {N ∪ V }∗ is a series of nonterminal and biterminal symbols representing the production of the rule and θ ∈ {∅, [], hi} denotes the orientation (axiomatic, straight or inverted) of the rule. Straight rules are read left-to-right in both languages, while inverted rules are read left-to-right in E and rightto-left in F. The direction of the axiomatic rules is undefined, as they m</context>
</contexts>
<marker>Aho, Ullman, 1969</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1969. Syntax-directed translations and the pushdown assembler. Journal of Computer and System Sciences, 3(1):37–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>The Theory of Parsing,</title>
<date>1972</date>
<journal>Translation, and Compiling (Volumes</journal>
<volume>1</volume>
<publisher>Prentice-Halll,</publisher>
<location>Englewood Cliffs, NJ.</location>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling (Volumes 1 and 2). Prentice-Halll, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Human Language Technology conference (HLT-2002),</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="10236" citStr="Doddington, 2002" startWordPosition="1783" endWordPosition="1784">re are O(n) agendas, containing O(b) active items, requiring O(n2) time to process. This gives a total time complexity of O(bn3). 5 Evaluation We evaluate the parser on a translation task (WMT’08 shared task3). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resulting translations with two automatic 3http://www.statmt.org/wmt08/ metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 Empirical results In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Human Language Technology conference (HLT-2002), San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL-2007 Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="10389" citStr="Koehn et al., 2007" startWordPosition="1808" endWordPosition="1811">ate the parser on a translation task (WMT’08 shared task3). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resulting translations with two automatic 3http://www.statmt.org/wmt08/ metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 Empirical results In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guid</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL-2007 Demo and Poster Sessions, pages 177–180, Prague, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="11163" citStr="Koehn, 2005" startWordPosition="1943" endWordPosition="1944">s, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry out exhaustive search. The total amount of training data was roughly 100,000 sentence pairs in each language pair, which is a relatively small corpus, but by no means a toy example. 6.2 Grammar induction It is possible to set the parameters of a SBITG by applying EM to an initial guess (Wu, 1995). As our initial guess, we used word co-occurrence counts, assuming that there was o</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine Translation Summit X, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M Lewis</author>
<author>Richard E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="3657" citStr="Lewis &amp; Stearns (1968)" startWordPosition="603" endWordPosition="606">, V, A, 5), where N is a set of nonterminal symbols, A is a set of rewrite rules, 5 E N is the start symbol and V C_ VE x VF is a set of biterminal symbols, where VE is the vocabulary of E and VF is the vocabulary of F. We will write a biterminal as e/f, where e E VE and f E VF. A sentence pair will be written as e/f, and a bispan as es..t/fu..v. Each rule δ E A is a tuple (X, γ, θ) where X E N is the right hand side of the rule, γ E 1All transduction grammars (a.k.a. synchronous grammars, or simply bigrammars) can be interpreted as models for generation, recognition, or transduction. 2SDTGs (Lewis &amp; Stearns (1968); Aho &amp; Ullman (1969), (1972)) are also recently called synchronous CFGs. 29 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 29–32, Paris, October 2009. c�2009 Association for Computational Linguistics {N ∪ V }∗ is a series of nonterminal and biterminal symbols representing the production of the rule and θ ∈ {∅, [], hi} denotes the orientation (axiomatic, straight or inverted) of the rule. Straight rules are read left-to-right in both languages, while inverted rules are read left-to-right in E and rightto-left in F. The direction of the axiomatic rules is</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Philip M. Lewis and Richard E. Stearns. 1968. Syntax-directed transduction. Journal of the Association for Computing Machinery, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8609" citStr="Och and Ney, 2003" startWordPosition="1494" endWordPosition="1497">≤U ≤V,XSsvU ∈ C} L= ⎧ Xstuv ������ ⎨ ⎩ 30 Since we are processing the agendas in order, any item in the chart will be as long as or shorter than the item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f. The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 Analysis Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda can start anywhere in the alignment space: O(n2) possible starting points, but once the end point in one language is set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (it has to be adjacent), only the </context>
<context position="10501" citStr="Och and Ney, 2003" startWordPosition="1826" endWordPosition="1829">tion system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resulting translations with two automatic 3http://www.statmt.org/wmt08/ metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 Empirical results In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above pro</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="10782" citStr="Och, 2003" startWordPosition="1879" endWordPosition="1880">etrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 Empirical results In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translations.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<pages>311--318</pages>
<location>Philadelphia,</location>
<contexts>
<context position="10208" citStr="Papineni et al., 2002" startWordPosition="1777" endWordPosition="1780">to a constant b, meaning that there are O(n) agendas, containing O(b) active items, requiring O(n2) time to process. This gives a total time complexity of O(bn3). 5 Evaluation We evaluate the parser on a translation task (WMT’08 shared task3). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resulting translations with two automatic 3http://www.statmt.org/wmt08/ metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 Empirical results In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translations. In 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), pages 311–318, Philadelphia, Jul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrase-based translation via word alignments from Stochastic Inversion Transduction Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation (at NAACL HLT</booktitle>
<pages>28--36</pages>
<location>Boulder, CO,</location>
<contexts>
<context position="8574" citStr="Saers and Wu, 2009" startWordPosition="1487" endWordPosition="1490">T,0≤U ≤u,XtSUu ∈ C} ∪ {XStuU|0≤S≤s,v≤U ≤V,XSsvU ∈ C} L= ⎧ Xstuv ������ ⎨ ⎩ 30 Since we are processing the agendas in order, any item in the chart will be as long as or shorter than the item being extended. This fact can be exploited to limit the number of possible siblings explored, but has no impact on time complexity. 3.3 Viterbi parsing When doing Viterbi parsing, all derivations but the most probable are discarded. This gives an unambiguous parse, which dictates exactly one alignment between e and f. The alignment of the Viterbi parse can be used to substitute that of other word aligners (Saers and Wu, 2009) such as GIZA++ (Och and Ney, 2003). 4 Analysis Looking at the algorithm, it is clear that there will be a total of T + V = O(n) agendas, each containing items of a certain length. The items in an agenda can start anywhere in the alignment space: O(n2) possible starting points, but once the end point in one language is set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself</context>
<context position="946" citStr="Saers &amp; Wu (2009)" startWordPosition="131" endWordPosition="134">Abstract We present a biparsing algorithm for Stochastic Bracketing Inversion Transduction Grammars that runs in O(bn3) time instead of O(n6). Transduction grammars learned via an EM estimation procedure based on this biparsing algorithm are evaluated directly on the translation task, by building a phrase-based statistical MT system on top of the alignments dictated by Viterbi parses under the induced bigrammars. Translation quality at different levels of pruning are compared, showing improvements over a conventional word aligner even at heavy pruning levels. 1 Introduction As demonstrated by Saers &amp; Wu (2009) there is something to be gained by applying structural models such as Inversion Transduction Grammars (ITG) to the problem of word alignment. One issue is that naive methods for inducing ITGs from parallel data can be very time consuming. We introduce a parsing algorithm for inducing Stochastic Bracketing ITGs from parallel data in O(bn3) time instead of O(n6), where b is a pruning parameter (lower = tighter pruning). We try out different values for b, and evaluate the results on a translation tasks. In section 2 we summarize the ITG framework; in section 3 we present our algorithm, whose tim</context>
<context position="10076" citStr="Saers &amp; Wu (2009)" startWordPosition="1760" endWordPosition="1763">reported for ITGs in previous work (Wu, 1995; Wu, 1997). The pruning works by limiting the number of active items in an agenda to a constant b, meaning that there are O(n) agendas, containing O(b) active items, requiring O(n2) time to process. This gives a total time complexity of O(bn3). 5 Evaluation We evaluate the parser on a translation task (WMT’08 shared task3). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resulting translations with two automatic 3http://www.statmt.org/wmt08/ metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 6 Empirical results In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100</context>
<context position="13961" citStr="Saers &amp; Wu (2009)" startWordPosition="2410" endWordPosition="2413">iterations to terminate, making even the fastest ones slower than GIZA++). To put the times in perspective, about 6 iterations were needed to get the ITGs to converge, making the longest training time about 16–17 hours. The time it takes to extract the phrases and tune the model using MERT is about 14 hours for these data sets. Looking at translation quality, we see a sharp initial rise as b grows to 10. At this point the SBITG system is on par with GIZA++. It continues to rise up to b = 25, but after that is more or less levels out. From this we conclude that the positive results reported in Saers &amp; Wu (2009) hold under harsh pruning. 7 Conclusions We have presented a SBITG biparsing algorithm that uses a novel form of pruning to cut the complexity of EM-estimation from O(n6) to O(bn3). Translation quality using the resulting learned SBITG models is improved over using conventional word alignments, even under harsh levels of pruning. Acknowledgments The authors are grateful for the comments made by the two anonymous reviewers. This work was funded by the Swedish National Graduate School of Language Technology, the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>Markus Saers and Dekai Wu. 2009. Improving phrase-based translation via word alignments from Stochastic Inversion Transduction Grammars. In Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation (at NAACL HLT 2009), pages 28–36, Boulder, CO, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="10870" citStr="Stolcke, 2002" startWordPosition="1894" endWordPosition="1895">s In this section we describe the experimental setup as well as the outcomes. 6.1 Setup We use the Moses Toolkit (Koehn et al., 2007) to train our phrase-based SMT models. The toolkit also includes scripts for applying GIZA++ (Och and Ney, 2003) as a word aligner. We have trained several systems, one using GIZA++ (our baseline system), one with no pruning at all, and 6 different values of b (1, 10, 25, 50, 75 and 100). We used the grow-diag-final-and method to extract phrases from the word alignment, and MERT (Och, 2003) to optimize the resulting model. We trained a 5-gram SRI language model (Stolcke, 2002) using the corpus supplied for this purpose by the shared task organizers. All of the above is consistent with the guidelines for building a baseline system for the WMT’08 shared task. The translation tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry out exhaustive search. The total amount of training data was roughly 100,000 sentence pa</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In International Conference on Spoken Language Processing, Denver, CO, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Trainable coarse bilingual grammars for parallel text bracketing.</title>
<date>1995</date>
<booktitle>In Third Annual Workshop on Very Large Corpora (WVLC-3),</booktitle>
<pages>69--81</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="4879" citStr="Wu (1995)" startWordPosition="821" endWordPosition="822">s they must be completely made up of terminals. For notational convenience, the orientation of the rule is written as surrounding the production, like so: X → &apos;y, X → [&apos;y] and X → h&apos;yi. The vocabularies of the languages may both include the empty token E, allowing for deletions and insertions. The empty biterminal, E/E is not allowed. 2.1 Stochastic ITGs In a Stochastic ITG (SITG), each rule is also associated with a probability, such that � Pr(X → &apos;y) = 1 γ for all X ∈ N. The probability of a derivation S ⇒∗ e/f is defined as the production of the probabilities of all rules used. As shown by Wu (1995), it is possible to fit the parameters of a SITG to a parallel corpus via EM (expectationmaximization) estimation. 2.2 Bracketing ITGs An ITG where there is only one nonterminal (other than the start symbol) is called a bracketing ITG (BITG). Since the one nonterminal is devoid of information, it can only be used to group its children together, imposing a bracketing on the sentence pairs. 3 Parsing SBITGs In this section we present a biparsing algorithm for Stochastic Bracketing Inversion Transduction Grammars (SBITGs) in normal form which incorporates a pruning parameter b. The algorithm is b</context>
<context position="9503" citStr="Wu, 1995" startWordPosition="1661" endWordPosition="1662">set, the end point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (it has to be adjacent), only the O(n2) possible end points have to be explored. This means that each active item takes O(n2) time to process. The total time is thus O(n6): O(n) agendas, containing O(n3) active items, requiring O(n2) time to process. This is also the time complexity reported for ITGs in previous work (Wu, 1995; Wu, 1997). The pruning works by limiting the number of active items in an agenda to a constant b, meaning that there are O(n) agendas, containing O(b) active items, requiring O(n2) time to process. This gives a total time complexity of O(bn3). 5 Evaluation We evaluate the parser on a translation task (WMT’08 shared task3). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resu</context>
<context position="11679" citStr="Wu, 1995" startWordPosition="2030" endWordPosition="2031"> tasks we applied the above procedure to are all taken from the Europarl corpus (Koehn, 2005). We selected the tasks German-English, French-English and SpanishEnglish. Furthermore, we restricted the training sentence pairs so that none of the sentences exceeded length 10. This was necessary to be able to carry out exhaustive search. The total amount of training data was roughly 100,000 sentence pairs in each language pair, which is a relatively small corpus, but by no means a toy example. 6.2 Grammar induction It is possible to set the parameters of a SBITG by applying EM to an initial guess (Wu, 1995). As our initial guess, we used word co-occurrence counts, assuming that there was one empty token in each sentence. This gave an estimate of the lexical rules. The probability mass was divided so that the lexical rules could share half of it, while the other half was shared equally by the two structural rules (X → [XX] and X → (XX)). Several training runs were made with different pruning parameters. The EM process was halted when a relative improvement in log-likelihood of 10−3 was no longer achieved over the previous iteration. 31 Baseline Different values of b for SBITGs Metric (GIZA++) ∞ 1</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Trainable coarse bilingual grammars for parallel text bracketing. In Third Annual Workshop on Very Large Corpora (WVLC-3), pages 69–81, Cambridge, MA, Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2634" citStr="Wu, 1997" startWordPosition="402" endWordPosition="403">mar (ITG) can be used to synchronously generate sentence pairs, synchronously parse sentence pairs, or transduce from a sentence in one language to a sentence in another.1 The equivalence class of inversion transductions can be described by restricting SyntaxDirected Transduction Grammars (SDTG)2 in various equivalent ways to the special cases of (a) binary SDTGs, (b) ternary SDTGs, or (c) SDTGs whose transduction rules are restricted to straight and inverted permutations only. Thus on one hand, any binary or ternary SDTG is an ITG. Conversely, any ITG can be stated in binary two-normal form (Wu, 1997). Only three kinds of rules are present in the normal form: A [BC] A (BC) A e/f On the other hand, under characterization (c), what distinguishes ITGs is that the permutation of constituents is restricted in such a way that all children of a node must be read either left-to-right, or right-to-left. The movement only applies to one of the languages, the other is fixed. Formally, an ITG is a tuple (N, V, A, 5), where N is a set of nonterminal symbols, A is a set of rewrite rules, 5 E N is the start symbol and V C_ VE x VF is a set of biterminal symbols, where VE is the vocabulary of E and VF is </context>
<context position="9514" citStr="Wu, 1997" startWordPosition="1663" endWordPosition="1664">nd point in the other follows from that, adding a factor O(n). This means that each agenda contains O(n3) active items. Each active item has to go through all possible siblings in the recursive step. Since the start point of the sibling is determined by the item itself (it has to be adjacent), only the O(n2) possible end points have to be explored. This means that each active item takes O(n2) time to process. The total time is thus O(n6): O(n) agendas, containing O(n3) active items, requiring O(n2) time to process. This is also the time complexity reported for ITGs in previous work (Wu, 1995; Wu, 1997). The pruning works by limiting the number of active items in an agenda to a constant b, meaning that there are O(n) agendas, containing O(b) active items, requiring O(n2) time to process. This gives a total time complexity of O(bn3). 5 Evaluation We evaluate the parser on a translation task (WMT’08 shared task3). In order to evaluate on a translation task, a translation system has to be built. We use the alignments from the Viterbi parses of the training corpus to substitute the alignments of GIZA++. This is the same approach as taken in Saers &amp; Wu (2009). We will evaluate the resulting trans</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404, Sep.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>