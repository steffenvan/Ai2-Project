<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003904">
<title confidence="0.961924">
UCB: System Description for SemEval Task #4
</title>
<author confidence="0.595582">
Preslav I. Nakov
</author>
<affiliation confidence="0.7317225">
EECS, CS division
University of California at Berkeley
</affiliation>
<address confidence="0.77102">
Berkeley, CA 94720
</address>
<email confidence="0.995897">
nakov@cs.berkeley.edu
</email>
<sectionHeader confidence="0.995536" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999858285714286">
The UC Berkeley team participated in the
SemEval 2007 Task #4, with an approach
that leverages the vast size of the Web in or-
der to build lexically-specific features. The
idea is to determine which verbs, preposi-
tions, and conjunctions are used in sentences
containing a target word pair, and to com-
pare those to features extracted for other
word pairs in order to determine which are
most similar. By combining these Web fea-
tures with words from the sentence context,
our team was able to achieve the best results
for systems of category C and third best for
systems of category A.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999628176470588">
Semantic relation classification is an important but
understudied language problem arising in many
NLP applications, including question answering, in-
formation retrieval, machine translation, word sense
disambiguation, information extraction, etc. This
year’s SemEval (previously SensEval) competition
has included a task targeting the important special
case of Classification of Semantic Relations between
Nominals. In the present paper we describe the UCB
system which took part in that competition.
The SemEval dataset contains a total of 7 se-
mantic relations (not exhaustive and possibly over-
lapping), with 140 training and about 70 testing
sentences per relation. Sentence classes are ap-
proximately 50% negative and 50% positive (“near
misses”). Table 1 lists the 7 relations together with
some examples.
</bodyText>
<note confidence="0.413338">
Marti A. Hearst
School of Information
University of California at Berkeley
Berkeley, CA 94720
</note>
<email confidence="0.892868">
hearst@ischool.berkeley.edu
</email>
<sectionHeader confidence="0.485857" genericHeader="introduction">
# Relation Name Examples
</sectionHeader>
<figure confidence="0.918568857142857">
1 Cause-Effect hormone-growth, laugh-wrinkle
2 Instrument-Agency laser-printer, ax-murderer
3 Product-Producer honey-bee, philosopher-theory
4 Origin-Entity grain-alcohol, desert-storm
5 Theme-Tool work-force, copyright-law
6 Part-Whole leg-table, door-car
7 Content-Container apple-basket, plane-cargo
</figure>
<tableCaption confidence="0.8957965">
Table 1: SemEval dataset: Relations with examples
(context sentences are not shown).
</tableCaption>
<bodyText confidence="0.99566">
Each example consists of a sentence, two nomi-
nals to be judged on whether they are in the target
semantic relation, manually annotated WordNet 3.0
sense keys for these nominals, and the Web query
used to obtain that example:
&amp;quot;Among the contents of the &lt;e1&gt;vessel&lt;/e1&gt;
were a set of carpenters &lt;e2&gt;tools&lt;/e2&gt;,
several large storage jars, ceramic
utensils, ropes and remnants of food, as
well as a heavy load of ballast stones.&amp;quot;
</bodyText>
<equation confidence="0.99615275">
WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;,
WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;,
Content-Container(e2, e1) = &amp;quot;true&amp;quot;,
Query = &amp;quot;contents of the * were a&amp;quot;
</equation>
<sectionHeader confidence="0.999118" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999813222222222">
Lauer (1995) proposes that eight prepositions are
enough to characterize the relation between nouns
in a noun-noun compound: of, for, in, at, on, from,
with or about. Lapata and Keller (2005) improve
on his results by using Web statistics. Rosario et al.
(2002) use a “descent of hierarchy”, which charac-
terizes the relation based on the semantic category of
the two nouns. Girju et al. (2005) apply SVM, deci-
sion trees, semantic scattering and iterative seman-
</bodyText>
<page confidence="0.986374">
366
</page>
<bodyText confidence="0.9848805">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369,
Prague, June 2007. c�2007 Association for Computational Linguistics
tic specialization, using WordNet, word sense dis-
ambiguation, and linguistic features. Barker and Sz-
pakowicz (1998) propose a two-level hierarchy with
5 classes at the upper level and 30 at the lower level.
Turney (2005) introduces latent relational analysis,
which uses the Web, synonyms, patterns like “X for
Y ”, “X such as Y ”, etc., and singular value decom-
position to smooth the frequencies. Turney (2006)
induces patterns from the Web, e.g. CAUSE is best
characterized by “Y * causes X”, and “Y in * early
X” is the best pattern for TEMPORAL. Kim and Bald-
win (2006) propose to use a predefined set of seed
verbs and multiple resources: WordNet, CoreLex,
and Moby’s thesaurus. Finally, in a previous publi-
cation (Nakov and Hearst, 2006), we make the claim
that the relation between the nouns in a noun-noun
compound can be characterized by the set of inter-
vening verbs extracted from the Web.
</bodyText>
<sectionHeader confidence="0.990978" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999929384615385">
Given an entity-annotated example sentence, we re-
duce the target entities e1 and e2 to single nouns
noun1 and noun2, by keeping their last nouns
only, which we assume to be the heads. We then
mine the Web for sentences containing both noun1
and noun2, from which we extract features, con-
sisting of word(s), part of speech (verb, preposi-
tion, verb+preposition, coordinating conjunction),
and whether noun1 precedes noun2. Table 2 shows
some example features and their frequencies.
We start with a set of exact phrase queries
against Google: “infl1 THAT * infl2”, “infl2
THAT * infl1”, “infl1 * infl2”, and “infl2 *
infl1”, where infl1 and infl2 are inflectional vari-
ants of noun1 and noun2, generated using WordNet
(Fellbaum, 1998); THAT can be that, which, or who;
and * stands for 0 or more (up to 8) stars separated
by spaces, representing the Google * single-word
wildcard match operator. For each query, we collect
the text snippets from the result set (up to 1000 per
query), split them into sentences, assign POS tags
using the OpenNLP tagger1, and extract features:
Verb: If one of the nouns is the subject, and the
other one is a direct or indirect object of that verb,
we extract it and we lemmatize it using WordNet
(Fellbaum, 1998). We ignore modals and auxil-
</bodyText>
<footnote confidence="0.954886">
1OpenNLP: http://opennlp.sourceforge.net
</footnote>
<table confidence="0.9996768">
Freq. Feature POS Direction
2205 of P 2 - -+1
1923 be V 1 - -+2
771 include V 1 - -+2
382 serve on V 2 - -+1
189 chair V 2 - -+1
189 have V 1 - -+2
169 consist of V 1 - -+2
148 comprise V 1 - -+2
106 sit on V 2 - -+1
81 be chaired by V 1 - -+2
78 appoint V 1 - -+2
77 on P 2 - -+1
66 and C 1 - -+2
� � � � � � � � � � � �
</table>
<tableCaption confidence="0.940596666666667">
Table 2: Most frequent features for committee
member. V stands for verb, P for preposition, and
C for coordinating conjunction.
</tableCaption>
<bodyText confidence="0.996887">
iaries, but retain the passive be, verb particles and
prepositions (in case of indirect object).
Preposition: If one of the nouns is the head of
an NP which contains a PP, inside which there is an
NP headed by the other noun (or an inflectional form
thereof), we extract the preposition heading that PP.
Coordination: If the two nouns are the heads of
two coordinated NPs, we extract the coordinating
conjunction.
In addition, we include some non-Web features2:
Sentence word: We use as features the words
from the context sentence, after stop words removal
and stemming with the Porter stemmer.
Entity word: We also use the lemmas of the
words that are part of ei (i = 1, 2).
Query word: Finally, we use the individual
words that are part of the query string. This feature
is used for category C runs only (see below).
Once extracted, the features are used to calculate
the similarity between two noun pairs. Each feature
triplet is assigned a weight. We wish to downweight
very common features, such as “of” used as a prepo-
sition in the 2 —* 1 direction, so we apply tf.idf
weighting to each feature. We then use the following
variant of the Dice coefficient to compare the weight
vectors A = (a1, ... , an) and B = (b1, ... , bn):
</bodyText>
<equation confidence="0.893468">
2 � �n i=1 min(ai, bi)
Dice(A, B) = (1)
�ni=1 ai + bi
This vector representation is similar to that of
</equation>
<footnote confidence="0.872294">
2Features have type prefix to prevent them from mixing.
</footnote>
<page confidence="0.95949">
367
</page>
<table confidence="0.999952823529412">
System Relation P R F Acc
UCB-A1 Cause-Effect 58.2 78.0 66.7 60.0
Instrument-Agency 62.5 78.9 69.8 66.7
Product-Producer 77.3 54.8 64.2 59.1
Origin-Entity 67.9 52.8 59.4 67.9
Theme-Tool 50.0 31.0 38.3 59.2
Part-Whole 51.9 53.8 52.8 65.3
Content-Container 62.2 60.5 61.3 60.8
average 61.4 58.6 58.9 62.7
UCB-A2 Cause-Effect 58.0 70.7 63.7 58.8
Instrument-Agency 65.9 71.1 68.4 67.9
Product-Producer 80.0 77.4 78.7 72.0
Origin-Entity 60.6 55.6 58.0 64.2
Theme-Tool 45.0 31.0 36.7 56.3
Part-Whole 41.7 38.5 40.0 58.3
Content-Container 56.4 57.9 57.1 55.4
average 58.2 57.5 57.5 61.9
UCB-A3 Cause-Effect 62.5 73.2 67.4 63.8
Instrument-Agency 65.9 76.3 70.7 69.2
Product-Producer 75.0 67.7 71.2 63.4
Origin-Entity 48.4 41.7 44.8 54.3
Theme-Tool 62.5 51.7 56.6 67.6
Part-Whole 50.0 46.2 48.0 63.9
Content-Container 64.9 63.2 64.0 63.5
average 61.3 60.0 60.4 63.7
UCB-A4 Cause-Effect 63.5 80.5 71.0 66.2
Instrument-Agency 70.0 73.7 71.8 71.8
Product-Producer 76.3 72.6 74.4 66.7
Origin-Entity 50.0 47.2 48.6 55.6
Theme-Tool 61.5 55.2 58.2 67.6
Part-Whole 52.2 46.2 49.0 65.3
Content-Container 65.8 65.8 65.8 64.9
average 62.7 63.0 62.7 65.4
Baseline (majority) 81.3 42.9 30.8 57.0
</table>
<tableCaption confidence="0.996489">
Table 3: Task 4 results. UCB systems A1-A4.
</tableCaption>
<table confidence="0.999919441176471">
System Relation P R F Acc
UCB-C1 Cause-Effect 58.5 75.6 66.0 60.0
Instrument-Agency 65.2 78.9 71.4 69.2
Product-Producer 81.4 56.5 66.7 62.4
Origin-Entity 67.9 52.8 59.4 67.9
Theme-Tool 50.0 31.0 38.3 59.2
Part-Whole 51.9 53.8 52.8 65.3
Content-Container 62.2 60.5 61.3 60.8
Average 62.4 58.5 59.4 63.5
UCB-C2 Cause-Effect 58.0 70.7 63.7 58.8
Instrument-Agency 67.5 71.1 69.2 69.2
Product-Producer 80.3 79.0 79.7 73.1
Origin-Entity 60.6 55.6 58.0 64.2
Theme-Tool 50.0 37.9 43.1 59.2
Part-Whole 43.5 38.5 40.8 59.7
Content-Container 56.4 57.9 57.1 55.4
Average 59.5 58.7 58.8 62.8
UCB-C3 Cause-Effect 62.5 73.2 67.4 63.8
Instrument-Agency 68.2 78.9 73.2 71.8
Product-Producer 74.1 69.4 71.7 63.4
Origin-Entity 56.8 58.3 57.5 61.7
Theme-Tool 62.5 51.7 56.6 67.6
Part-Whole 50.0 42.3 45.8 63.9
Content-Container 64.9 63.2 64.0 63.5
Average 62.7 62.4 62.3 65.1
UCB-C4 Cause-Effect 63.5 80.5 71.0 66.2
Instrument-Agency 70.7 76.3 73.4 73.1
Product-Producer 76.7 74.2 75.4 67.7
Origin-Entity 59.0 63.9 61.3 64.2
Theme-Tool 63.0 58.6 60.7 69.0
Part-Whole 52.2 46.2 49.0 65.3
Content-Container 64.1 65.8 64.9 63.5
Average 64.2 66.5 65.1 67.0
Baseline (majority) 81.3 42.9 30.8 57.0
</table>
<tableCaption confidence="0.999871">
Table 4: Task 4 results. UCB systems C1-C4.
</tableCaption>
<bodyText confidence="0.9999084">
Lin (1998), who measures word similarity by using
triples extracted from a dependency parser. In par-
ticular, given a noun, he finds all verbs that have it
as a subject or object, and all adjectives that modify
it, together with the corresponding frequencies.
</bodyText>
<sectionHeader confidence="0.994566" genericHeader="conclusions">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999565">
Participants were asked to classify their systems
into categories depending on whether they used the
WordNet sense (WN) and/or the Google query (GC).
Our team submitted runs for categories A (WN=no,
QC=no) and C (WN=no, QC=yes) only, since we
believe that having the target entities annotated with
the correct WordNet senses is an unrealistic assump-
tion for a real-world application.
Following Turney and Littman (2005) and Barker
and Szpakowicz (1998), we used a 1-nearest-
neighbor classifier. Given a test example, we calcu-
lated the Dice coefficient between its feature vector
and the vector of each of the training examples. If
there was a single highest-scoring training example,
we predicted its class for that test example. Oth-
erwise, if there were ties for first, we assumed the
class predicted by the majority of the tied examples.
If there was no majority, we predicted the class that
was most likely on the training data. Regardless of
the classifier’s prediction, if the head words of the
two entities e1 and e2 had the same lemma, we clas-
sified that example as negative.
Table 3 and 4 show the results for our A and C
runs for different amounts of training data: 45 (A1,
C1), 90 (A2, C2), 105 (A3, C3) and 140 (A4, C4).
All results are above the baseline: always propose
the majority label (“true”/“false”) in the test set. In
fact, our category C system is the best-performing
(in terms of F and Acc) among the participating
systems, and we achieved the third best results for
category A. Our category C results are slightly but
</bodyText>
<page confidence="0.997162">
368
</page>
<bodyText confidence="0.997890785714286">
consistently better than for A for all measures (P, R,
F, Acc), which suggests that knowing the query is
helpful. Interestingly, systems UCB-A2 and UCB-
C2 performed worse than UCB-A1 and UCB-C1,
which means that having more training data does not
necessarily help with a 1NN classifier.
Table 5 shows additional analysis for A4 and C4.
We study the effect of adding extra Google contexts
(using up to 10 stars, rather than 8), and using differ-
ent subsets of features. We show the results for: (a)
leave-one-out cross-validation on the training data,
(b) on the test data, and (c) our official UCB runs.
Acknowledgements: This work is supported in
part by NSF DBI-0317510.
</bodyText>
<sectionHeader confidence="0.998748" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997876264705882">
Ken Barker and Stan Szpakowicz. 1998. Semi-automatic
recognition of noun modifier relationships. In Proceedings
of COLING-ACL’98, pages 96–102.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe.
2005. On the semantics of noun compounds. Computer
Speech and Language, 19(4):479–496.
Su Nam Kim and Timothy Baldwin. 2006. Interpreting seman-
tic relations in noun compounds via verb semantics. In Pro-
ceedings of COLING/ACL 2006. (poster), pages 491–498.
Mirella Lapata and Frank Keller. 2005. Web-based models for
natural language processing. ACM Transactions on Speech
and Language Processing, 2:1–31.
Mark Lauer. 1995. Designing Statistical Language Learners:
Experiments on Noun Compounds. Ph.D. thesis, Department
of Computing Macquarie University NSW 2109 Australia.
Dekang Lin. 1998. An information-theoretic definition of sim-
ilarity. In Proceedings of International Conference on Ma-
chine Learning, pages 296–304.
Preslav Nakov and Marti Hearst. 2006. Using verbs to charac-
terize noun-noun relations. In Proceedings of AIMSA, pages
233–244.
Barbara Rosario, Marti Hearst, and Charles Fillmore. 2002.
The descent of hierarchy, and selection in relational seman-
tics. In ACL, pages 247–254.
Peter Turney and Michael Littman. 2005. Corpus-based learn-
ing of analogies and semantic relations. Machine Learning
Journal, 60(1-3):251–278.
Peter Turney. 2005. Measuring semantic similarity by latent
relational analysis. In Proceedings IJCAI, pages 1136–1141.
Peter Turney. 2006. Expressing implicit semantic relations
without supervision. In Proceedings of COLING-ACL,
pages 313–320.
</reference>
<figure confidence="0.985560372881356">
Features Used Leave-1-out Test UCB
Cause-Effect
sent 45.7 50.0
p 55.0 53.8
v 59.3 68.8
v + p 57.1 63.7
v + p + c 70.5 67.5
v+p+c+sent 58.5 66.2 66.2
v + p + c + sent + query 59.3 66.2 66.2
Instrument-Agency
sent 63.6 59.0
p 62.1 70.5
v 71.4 69.2
v + p 70.7 70.5
v + p + c 70.0 70.5
v+p+c+sent 68.6 71.8 71.8
v + p + c + sent + query 70.0 73.1 73.1
Product-Producer
sent 47.9 59.1
p 55.7 58.1
v 70.0 61.3
v + p 66.4 65.6
v + p + c 67.1 65.6
v + p + c + sent 66.4 69.9 66.7
v + p + c + sent + query 67.9 69.9 67.7
Origin-Entity
sent 64.3 72.8
p 63.6 56.8
v 69.3 71.6
v + p 67.9 69.1
v + p + c 66.4 70.4
v + p + c + sent 68.6 72.8 55.6
v + p + c + sent + query 67.9 72.8 64.2
Theme-Tool
sent 66.4 69.0
p 56.4 56.3
v 61.4 70.4
v + p 56.4 67.6
v + p + c 57.1 69.0
v+p+c+sent 52.1 62.0 67.6
v + p + c + sent + query 52.9 62.0 69.0
Part-Whole
sent 47.1 51.4
p 57.1 54.1
v 60.0 66.7
v + p 62.1 63.9
v + p + c 61.4 63.9
v+p+c+sent 60.0 61.1 65.3
v + p + c + sent + query 60.0 61.1 65.3
Content-Container
sent 56.4 54.1
p 57.9 59.5
v 71.4 67.6
v + p 72.1 67.6
v + p + c 72.9 67.6
v+p+c+sent 69.3 67.6 64.9
v + p + c + sent + query 71.4 71.6 63.5
Average A4 67.3 65.4
Average C4 68.1 67.0
</figure>
<tableCaption confidence="0.866761">
Table 5: Accuracy for different features and extra
</tableCaption>
<bodyText confidence="0.8985515">
Web contexts: on leave-one-out cross-validation,
on testing data, and in the official UCB runs.
</bodyText>
<page confidence="0.998627">
369
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459238">
<title confidence="0.924359">UCB: System Description for SemEval Task #4</title>
<author confidence="0.997102">Preslav I Nakov</author>
<affiliation confidence="0.998413">EECS, CS division University of California at Berkeley</affiliation>
<address confidence="0.99971">Berkeley, CA 94720</address>
<email confidence="0.998401">nakov@cs.berkeley.edu</email>
<abstract confidence="0.9661444">The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in order to build lexically-specific features. The idea is to determine which verbs, prepositions, and conjunctions are used in sentences containing a target word pair, and to compare those to features extracted for other word pairs in order to determine which are most similar. By combining these Web features with words from the sentence context, our team was able to achieve the best results systems of category third best for of category</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ken Barker</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semi-automatic recognition of noun modifier relationships.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>96--102</pages>
<contexts>
<context position="3435" citStr="Barker and Szpakowicz (1998)" startWordPosition="502" endWordPosition="506">und: of, for, in, at, on, from, with or about. Lapata and Keller (2005) improve on his results by using Web statistics. Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publicati</context>
<context position="10559" citStr="Barker and Szpakowicz (1998)" startWordPosition="1722" endWordPosition="1725">given a noun, he finds all verbs that have it as a subject or object, and all adjectives that modify it, together with the corresponding frequencies. 4 Experiments and Results Participants were asked to classify their systems into categories depending on whether they used the WordNet sense (WN) and/or the Google query (GC). Our team submitted runs for categories A (WN=no, QC=no) and C (WN=no, QC=yes) only, since we believe that having the target entities annotated with the correct WordNet senses is an unrealistic assumption for a real-world application. Following Turney and Littman (2005) and Barker and Szpakowicz (1998), we used a 1-nearestneighbor classifier. Given a test example, we calculated the Dice coefficient between its feature vector and the vector of each of the training examples. If there was a single highest-scoring training example, we predicted its class for that test example. Otherwise, if there were ties for first, we assumed the class predicted by the majority of the tied examples. If there was no majority, we predicted the class that was most likely on the training data. Regardless of the classifier’s prediction, if the head words of the two entities e1 and e2 had the same lemma, we classif</context>
</contexts>
<marker>Barker, Szpakowicz, 1998</marker>
<rawString>Ken Barker and Stan Szpakowicz. 1998. Semi-automatic recognition of noun modifier relationships. In Proceedings of COLING-ACL’98, pages 96–102.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3435" citStr="(1998)" startWordPosition="506" endWordPosition="506">on, from, with or about. Lapata and Keller (2005) improve on his results by using Web statistics. Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publicati</context>
<context position="9832" citStr="(1998)" startWordPosition="1610" endWordPosition="1610">strument-Agency 68.2 78.9 73.2 71.8 Product-Producer 74.1 69.4 71.7 63.4 Origin-Entity 56.8 58.3 57.5 61.7 Theme-Tool 62.5 51.7 56.6 67.6 Part-Whole 50.0 42.3 45.8 63.9 Content-Container 64.9 63.2 64.0 63.5 Average 62.7 62.4 62.3 65.1 UCB-C4 Cause-Effect 63.5 80.5 71.0 66.2 Instrument-Agency 70.7 76.3 73.4 73.1 Product-Producer 76.7 74.2 75.4 67.7 Origin-Entity 59.0 63.9 61.3 64.2 Theme-Tool 63.0 58.6 60.7 69.0 Part-Whole 52.2 46.2 49.0 65.3 Content-Container 64.1 65.8 64.9 63.5 Average 64.2 66.5 65.1 67.0 Baseline (majority) 81.3 42.9 30.8 57.0 Table 4: Task 4 results. UCB systems C1-C4. Lin (1998), who measures word similarity by using triples extracted from a dependency parser. In particular, given a noun, he finds all verbs that have it as a subject or object, and all adjectives that modify it, together with the corresponding frequencies. 4 Experiments and Results Participants were asked to classify their systems into categories depending on whether they used the WordNet sense (WN) and/or the Google query (GC). Our team submitted runs for categories A (WN=no, QC=no) and C (WN=no, QC=yes) only, since we believe that having the target entities annotated with the correct WordNet senses </context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="3080" citStr="Girju et al. (2005)" startWordPosition="456" endWordPosition="459">, ropes and remnants of food, as well as a heavy load of ballast stones.&amp;quot; WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;, WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;, Content-Container(e2, e1) = &amp;quot;true&amp;quot;, Query = &amp;quot;contents of the * were a&amp;quot; 2 Related Work Lauer (1995) proposes that eight prepositions are enough to characterize the relation between nouns in a noun-noun compound: of, for, in, at, on, from, with or about. Lapata and Keller (2005) improve on his results by using Web statistics. Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value dec</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19(4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Timothy Baldwin</author>
</authors>
<title>Interpreting semantic relations in noun compounds via verb semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<volume>(poster),</volume>
<pages>491--498</pages>
<contexts>
<context position="3892" citStr="Kim and Baldwin (2006)" startWordPosition="585" endWordPosition="589"> c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns noun1 and noun2, by keeping their last nouns only, which we assume to be the heads. We then mine the Web for sentences containing both noun1 and noun2, from which w</context>
</contexts>
<marker>Kim, Baldwin, 2006</marker>
<rawString>Su Nam Kim and Timothy Baldwin. 2006. Interpreting semantic relations in noun compounds via verb semantics. In Proceedings of COLING/ACL 2006. (poster), pages 491–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<pages>2--1</pages>
<contexts>
<context position="2878" citStr="Lapata and Keller (2005)" startWordPosition="421" endWordPosition="424"> sense keys for these nominals, and the Web query used to obtain that example: &amp;quot;Among the contents of the &lt;e1&gt;vessel&lt;/e1&gt; were a set of carpenters &lt;e2&gt;tools&lt;/e2&gt;, several large storage jars, ceramic utensils, ropes and remnants of food, as well as a heavy load of ballast stones.&amp;quot; WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;, WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;, Content-Container(e2, e1) = &amp;quot;true&amp;quot;, Query = &amp;quot;contents of the * were a&amp;quot; 2 Related Work Lauer (1995) proposes that eight prepositions are enough to characterize the relation between nouns in a noun-noun compound: of, for, in, at, on, from, with or about. Lapata and Keller (2005) improve on his results by using Web statistics. Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 class</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-based models for natural language processing. ACM Transactions on Speech and Language Processing, 2:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Noun Compounds.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<pages>2109</pages>
<institution>Department of Computing Macquarie University NSW</institution>
<contexts>
<context position="2699" citStr="Lauer (1995)" startWordPosition="394" endWordPosition="395">nces are not shown). Each example consists of a sentence, two nominals to be judged on whether they are in the target semantic relation, manually annotated WordNet 3.0 sense keys for these nominals, and the Web query used to obtain that example: &amp;quot;Among the contents of the &lt;e1&gt;vessel&lt;/e1&gt; were a set of carpenters &lt;e2&gt;tools&lt;/e2&gt;, several large storage jars, ceramic utensils, ropes and remnants of food, as well as a heavy load of ballast stones.&amp;quot; WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;, WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;, Content-Container(e2, e1) = &amp;quot;true&amp;quot;, Query = &amp;quot;contents of the * were a&amp;quot; 2 Related Work Lauer (1995) proposes that eight prepositions are enough to characterize the relation between nouns in a noun-noun compound: of, for, in, at, on, from, with or about. Lapata and Keller (2005) improve on his results by using Web statistics. Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Compu</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. Ph.D. thesis, Department of Computing Macquarie University NSW 2109 Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="9832" citStr="Lin (1998)" startWordPosition="1609" endWordPosition="1610">8 Instrument-Agency 68.2 78.9 73.2 71.8 Product-Producer 74.1 69.4 71.7 63.4 Origin-Entity 56.8 58.3 57.5 61.7 Theme-Tool 62.5 51.7 56.6 67.6 Part-Whole 50.0 42.3 45.8 63.9 Content-Container 64.9 63.2 64.0 63.5 Average 62.7 62.4 62.3 65.1 UCB-C4 Cause-Effect 63.5 80.5 71.0 66.2 Instrument-Agency 70.7 76.3 73.4 73.1 Product-Producer 76.7 74.2 75.4 67.7 Origin-Entity 59.0 63.9 61.3 64.2 Theme-Tool 63.0 58.6 60.7 69.0 Part-Whole 52.2 46.2 49.0 65.3 Content-Container 64.1 65.8 64.9 63.5 Average 64.2 66.5 65.1 67.0 Baseline (majority) 81.3 42.9 30.8 57.0 Table 4: Task 4 results. UCB systems C1-C4. Lin (1998), who measures word similarity by using triples extracted from a dependency parser. In particular, given a noun, he finds all verbs that have it as a subject or object, and all adjectives that modify it, together with the corresponding frequencies. 4 Experiments and Results Participants were asked to classify their systems into categories depending on whether they used the WordNet sense (WN) and/or the Google query (GC). Our team submitted runs for categories A (WN=no, QC=no) and C (WN=no, QC=yes) only, since we believe that having the target entities annotated with the correct WordNet senses </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of International Conference on Machine Learning, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using verbs to characterize noun-noun relations.</title>
<date>2006</date>
<booktitle>In Proceedings of AIMSA,</booktitle>
<pages>233--244</pages>
<contexts>
<context position="4062" citStr="Nakov and Hearst, 2006" startWordPosition="613" endWordPosition="616">pose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns noun1 and noun2, by keeping their last nouns only, which we assume to be the heads. We then mine the Web for sentences containing both noun1 and noun2, from which we extract features, consisting of word(s), part of speech (verb, preposition, verb+preposition, coordinating conjunction), and whether noun1 precedes noun2. Table 2 shows</context>
</contexts>
<marker>Nakov, Hearst, 2006</marker>
<rawString>Preslav Nakov and Marti Hearst. 2006. Using verbs to characterize noun-noun relations. In Proceedings of AIMSA, pages 233–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
<author>Charles Fillmore</author>
</authors>
<title>The descent of hierarchy, and selection in relational semantics.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>247--254</pages>
<contexts>
<context position="2948" citStr="Rosario et al. (2002)" startWordPosition="433" endWordPosition="436">mple: &amp;quot;Among the contents of the &lt;e1&gt;vessel&lt;/e1&gt; were a set of carpenters &lt;e2&gt;tools&lt;/e2&gt;, several large storage jars, ceramic utensils, ropes and remnants of food, as well as a heavy load of ballast stones.&amp;quot; WordNet(e1) = &amp;quot;vessel%1:06:00::&amp;quot;, WordNet(e2) = &amp;quot;tool%1:06:00::&amp;quot;, Content-Container(e2, e1) = &amp;quot;true&amp;quot;, Query = &amp;quot;contents of the * were a&amp;quot; 2 Related Work Lauer (1995) proposes that eight prepositions are enough to characterize the relation between nouns in a noun-noun compound: of, for, in, at, on, from, with or about. Lapata and Keller (2005) improve on his results by using Web statistics. Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introdu</context>
</contexts>
<marker>Rosario, Hearst, Fillmore, 2002</marker>
<rawString>Barbara Rosario, Marti Hearst, and Charles Fillmore. 2002. The descent of hierarchy, and selection in relational semantics. In ACL, pages 247–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Corpus-based learning of analogies and semantic relations.</title>
<date>2005</date>
<journal>Machine Learning Journal,</journal>
<pages>60--1</pages>
<contexts>
<context position="10526" citStr="Turney and Littman (2005)" startWordPosition="1717" endWordPosition="1720">ndency parser. In particular, given a noun, he finds all verbs that have it as a subject or object, and all adjectives that modify it, together with the corresponding frequencies. 4 Experiments and Results Participants were asked to classify their systems into categories depending on whether they used the WordNet sense (WN) and/or the Google query (GC). Our team submitted runs for categories A (WN=no, QC=no) and C (WN=no, QC=yes) only, since we believe that having the target entities annotated with the correct WordNet senses is an unrealistic assumption for a real-world application. Following Turney and Littman (2005) and Barker and Szpakowicz (1998), we used a 1-nearestneighbor classifier. Given a test example, we calculated the Dice coefficient between its feature vector and the vector of each of the training examples. If there was a single highest-scoring training example, we predicted its class for that test example. Otherwise, if there were ties for first, we assumed the class predicted by the majority of the tied examples. If there was no majority, we predicted the class that was most likely on the training data. Regardless of the classifier’s prediction, if the head words of the two entities e1 and </context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter Turney and Michael Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning Journal, 60(1-3):251–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Measuring semantic similarity by latent relational analysis.</title>
<date>2005</date>
<booktitle>In Proceedings IJCAI,</booktitle>
<pages>1136--1141</pages>
<contexts>
<context position="3540" citStr="Turney (2005)" startWordPosition="524" endWordPosition="525">Rosario et al. (2002) use a “descent of hierarchy”, which characterizes the relation based on the semantic category of the two nouns. Girju et al. (2005) apply SVM, decision trees, semantic scattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compoun</context>
</contexts>
<marker>Turney, 2005</marker>
<rawString>Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings IJCAI, pages 1136–1141.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="3731" citStr="Turney (2006)" startWordPosition="556" endWordPosition="557">cattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, Prague, June 2007. c�2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns nou</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter Turney. 2006. Expressing implicit semantic relations without supervision. In Proceedings of COLING-ACL, pages 313–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>