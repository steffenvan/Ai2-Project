<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001137">
<title confidence="0.991678">
Exact Maximum Inference for the Fertility Hidden Markov Model
</title>
<author confidence="0.984326">
Chris Quirk
</author>
<affiliation confidence="0.953414">
Microsoft Research
</affiliation>
<address confidence="0.9427705">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.999303">
chrisq@microsoft.com
</email>
<sectionHeader confidence="0.993905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952">
The notion of fertility in word alignment
(the number of words emitted by a sin-
gle state) is useful but difficult to model.
Initial attempts at modeling fertility used
heuristic search methods. Recent ap-
proaches instead use more principled ap-
proximate inference techniques such as
Gibbs sampling for parameter estimation.
Yet in practice we also need the single best
alignment, which is difficult to find us-
ing Gibbs. Building on recent advances in
dual decomposition, this paper introduces
an exact algorithm for finding the sin-
gle best alignment with a fertility HMM.
Finding the best alignment appears impor-
tant, as this model leads to a substantial
improvement in alignment quality.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999134">
Word-based translation models intended to model
the translation process have found new uses iden-
tifying word correspondences in sentence pairs.
These word alignments are a crucial training com-
ponent in most machine translation systems. Fur-
thermore, they are useful in other NLP applica-
tions, such as entailment identification.
The simplest models may use lexical infor-
mation alone. The seminal Model 1 (Brown
et al., 1993) has proved very powerful, per-
forming nearly as well as more complicated
models in some phrasal systems (Koehn et al.,
2003). With minor improvements to initializa-
tion (Moore, 2004) (which may be important
(Toutanova and Galley, 2011)), it can be quite
competitive. Subsequent IBM models include
more detailed information about context. Models
2 and 3 incorporate a positional model based on
the absolute position of the word; Models 4 and
5 use a relative position model instead (an English
word tends to align to a French word that is nearby
the French word aligned to the previous English
word). Models 3, 4, and 5 all incorporate a no-
tion of “fertility”: the number of French words that
align to any English word.
Although these latter models covered a broad
range of phenomena, estimation techniques and
MAP inference were challenging. The au-
thors originally recommended heuristic proce-
dures based on local search for both. Such meth-
ods work reasonably well, but can be computation-
ally inefficient and have few guarantees. Thus,
many researchers have switched to the HMM
model (Vogel et al., 1996) and variants with more
parameters (He, 2007). This captures the posi-
tional information in the IBM models in a frame-
work that admits exact parameter estimation infer-
ence, though the objective function is not concave:
local maxima are a concern.
Modeling fertility is challenging in the HMM
framework as it violates the Markov assump-
tion. Where the HMM jump model considers only
the prior state, fertility requires looking across
the whole state space. Therefore, the standard
forward-backward and Viterbi algorithms do not
apply. Recent work (Zhao and Gildea, 2010) de-
scribed an extension to the HMM with a fertility
model, using MCMC techniques for parameter es-
timation. However, they do not have a efficient
means of MAP inference, which is necessary in
many applications such as machine translation.
This paper introduces a method for exact MAP
inference with the fertility HMM using dual de-
composition. The resulting model leads to sub-
stantial improvements in alignment quality.
</bodyText>
<page confidence="0.992521">
7
</page>
<note confidence="0.5275675">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–11,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.946171" genericHeader="method">
2 HMM alignment
</sectionHeader>
<bodyText confidence="0.999803333333333">
Let us briefly review the HMM translation model
as a starting point. We are given a sequence of
English words e = e1, ... , eI. This model pro-
duces distributions over French word sequences
f = f1, ... , fJ and word alignment vectors a =
a1, ... , aJ, where aj E [0..J] indicates the En-
glish word generating the jth French word, 0 rep-
resenting a special NULL state to handle systemat-
ically unaligned words.
</bodyText>
<equation confidence="0.994468666666667">
J
Pr(f, a|e) = p(J|I) Y p(aj|aj−1) pfj ��eaj �
j=1
</equation>
<bodyText confidence="0.998965090909091">
The generative story begins by predicting the num-
ber of words in the French sentence (hence the
number of elements in the alignment vector). Then
for each French word position, first the alignment
variable (English word index used to generate the
current French word) is selected based on only the
prior alignment variable. Next the French word is
predicted based on its aligned English word.
Following prior work (Zhao and Gildea, 2010),
we augment the standard HMM with a fertility dis-
tribution.
</bodyText>
<equation confidence="0.997542">
Pr(f, a|e) =p(J|I) YI p(φi|ei)
J i=1 (1)
Y p(aj|aj−1) pfj ��eaj �
j=1
</equation>
<bodyText confidence="0.999775">
where φi = PJj=1 δ(i, aj) indicates the number of
times that state j is visited. This deficient model
wastes some probability mass on inconsistent con-
figurations where the number of times that a state
i is visited does not match its fertility φi. Follow-
ing in the footsteps of older, richer, and wiser col-
leagues (Brown et al., 1993),we forge ahead un-
concerned by this complication.
</bodyText>
<subsectionHeader confidence="0.983767">
2.1 Parameter estimation
</subsectionHeader>
<bodyText confidence="0.985281769230769">
Of greater concern is the exponential complex-
ity of inference in this model. For the standard
HMM, there is a dynamic programming algorithm
to compute the posterior probability over word
alignments Pr(a|e, f). These are the sufficient
statistics gathered in the E step of EM.
The structure of the fertility model violates the
Markov assumptions used in this dynamic pro-
gramming method. However, we may empirically
estimate the posterior distribution using Markov
chain Monte Carlo methods such as Gibbs sam-
pling (Zhao and Gildea, 2010). In this case,
we make some initial estimate of the a vector,
potentially randomly. We then repeatedly re-
sample each element of that vector conditioned
on all other positions according to the distribu-
tion Pr(aj|a−j, e, f). Given a complete assign-
ment of the alignment for all words except the cur-
rent, computing the complete probability includ-
ing transition, emission, and jump, is straightfor-
ward. This estimate comes with a computational
cost: we must cycle through all positions of the
vector repeatedly to gather a good estimate. In
practice, a small number of samples will suffice.
2.2 MAP inference with dual decomposition
Dual decomposition, also known as Lagrangian
relaxation, is a method for solving complex
combinatorial optimization problems (Rush and
Collins, 2012). These complex problems are sepa-
rated into distinct components with tractable MAP
inference procedures. The subproblems are re-
peatedly solved with some communication over
consistency until a consistent and globally optimal
solution is found.
Here we are interested in the problem of find-
ing the most likely alignment of a sentence pair
e, f. Thus, we need to solve the combinatorial op-
timization problem arg maxa Pr(f, a|e). Let us
rewrite the objective function as follows:
</bodyText>
<equation confidence="0.998144625">
h(a) = XI ⎛ ⎞
i=1 X log p(fj|ei)
⎝log p(φi|ei) + ⎠
2
j,aj=i
J � eaj )
+X log p(aj|aj−1) + log p(fj 2
j=1
</equation>
<bodyText confidence="0.9991661">
Because f is fixed, the p(J|I) term is constant and
may be omitted. Note how we’ve split the opti-
mization into two portions. The first captures fer-
tility as well as some component of the translation
distribution, and the second captures the jump dis-
tribution and the remainder of the translation dis-
tribution.
Our dual decomposition method follows this
segmentation. Define ya as ya(i, j) = 1 if aj = i,
and 0 otherwise. Let z E 10,1}I×J be a binary
</bodyText>
<page confidence="0.790981">
8
</page>
<equation confidence="0.70758175">
u(0)(i, j) := 0 `di E 1..I, j E 1..J
for k = 1 to K (
)a(k) := arg maxa f(a) + Ei,j u(k−1)(i, j)ya(i, j)
( )
z(k) := arg maxz g(z) − Ei,j u(k−1)(i, j)z(i, j)
if ya = z
return a(k)
end if
( )
u(k)(i, j) := u(k)(i, j) + δk ya(k)(i, j) − z(k)(i, j)
end for
return a(K)
</equation>
<figureCaption confidence="0.994626">
Figure 1: The dual decomposition algorithm for
</figureCaption>
<bodyText confidence="0.846818">
the fertility HMM, where Sk is the step size at the
kth iteration for 1 G k G K, and K is the max
number of iterations.
matrix. Define the functions f and g as
</bodyText>
<equation confidence="0.990546">
J
(logp(aj|aj−1)+ 21logp(fjleaj))
j=1
</equation>
<bodyText confidence="0.782921">
Then we want to find
</bodyText>
<equation confidence="0.909388">
f(a) + g(z)
</equation>
<bodyText confidence="0.995493142857143">
subject to the constraints ya(i, j) = z(i, j)Vi, j.
Note how this recovers the original objective func-
tion when matching variables are found.
We use the dual decomposition algorithm
from Rush and Collins (2012), reproduced
here in Figure 1. Note how the langrangian
adds one additional term word, scaled by a
value indicating whether that word is aligned
in the current position. Because it is only
added for those words that are aligned, we
can merge this with the log p(fj leaj) terms
in both f and g. Therefore, we can solve
arg maxa(f(a) + Ei,j u(k−1)(i,j)ya(i,j)) us-
ing the standard Viterbi algorithm.
The g function, on the other hand, does not have
a commonly used decomposition structure. Luck-
ily we can factor this maximization into pieces that
allow for efficient computation. Note that g sums
over arbitrary binary matrices. Unlike the HMM,
where each French word must have exactly one
English generator, this maximization allows each
</bodyText>
<equation confidence="0.921902666666667">
z(i, j) := 0 `d(i, j) E [1..I] x [1..J]
v := 0
for i = 1 to I
for j = 1 to J
x(j) := (log p(fj|ei) , j)
end for
sort x in descending order by first component
max := logp(O = 0|ei) , arg := 0, sum := 0
for f = 1 to J
sum := sum + x[f, 1]
if sum + log p(O = f|ei) &gt; max
max := sum + log p(O = f|ei)
arg := f
end if
end for
v := v + max
for f = 1 to arg
z(i,x[f,2]) := 1
end for
end for
return z, v
</equation>
<figureCaption confidence="0.749586">
Figure 2: Algorithm for finding the arg max and
max of g, the fertility-related component of the
dual decomposition objective.
</figureCaption>
<bodyText confidence="0.999749925925926">
French word to have zero or many generators. Be-
cause assignments that are in accordance between
this model and the HMM will meet the HMM’s
constraints, the overall dual decomposition algo-
rithm will return valid assignments, even though
individual selections for this model may fail to
meet the requirements.
As the scoring function g can be decomposed
into a sum of scores for each row Ei gi (i.e., there
are no interactions between distinct rows of the
matrix) we can maximize each row independently:
Within each row, we seek the best of all 2J pos-
sible configurations. These configurations may
be grouped into equivalence classes based on the
number of non-zero entries. In each class, the
max assignment is the one using words with the
highest log probabilities; the total score of this as-
signment is the sum those log probabilities and
the log probability of that fertility. Sorting the
scores of each cell in the row in descending or-
der by log probability allows for linear time com-
putation of the max for each row. The algorithm
described in Figure 2 finds this maximal assign-
ment in O(IJ log J) time, generally faster than
the O(I2J) time used by Viterbi.
We note in passing that this maximizer is pick-
ing from an unconstrained set of binary matri-
</bodyText>
<equation confidence="0.758681071428572">
I
i=1
gi(zi) =
gi(zi)
max
z
max
z
I
i=1
f(a) =
g(z) =
I
i=1 ( log p(φ (zi)|ei) +
</equation>
<figure confidence="0.54644175">
J z(i, j) �log p(fj|ei)
j=1 2
arg max
a,z
</figure>
<page confidence="0.97869">
9
</page>
<bodyText confidence="0.999982933333333">
ces. Since each English word may generate as
many French words as it likes, regardless of all
other words in the sentence, the underlying ma-
trix have many more or many fewer non-zero en-
tries than there are French words. A straightfor-
ward extension to the algorithm of Figure 2 returns
only z matrices with exactly J nonzero entries.
Rather than maximizing each row totally indepen-
dently, we keep track of the best configurations
for each number of words generated in each row,
and then pick the best combination that sums to J:
another straightforward exercise in dynamic pro-
gramming. This refinement does not change the
correctness of the dual decomposition algorithm;
rather it speeds the convergence.
</bodyText>
<sectionHeader confidence="0.970369" genericHeader="method">
3 Fertility distribution parameters
</sectionHeader>
<bodyText confidence="0.995368222222222">
Original IBM models used a categorical distribu-
tion of fertility, one such distribution for each En-
glish word. This gives EM a great amount of free-
dom in parameter estimation, with no smoothing
or parameter tying of even rare words. Prior work
addressed this by using the single parameter Pois-
son distribution, forcing infrequent words to share
a global parameter estimated from the fertility of
all words in the corpus (Zhao and Gildea, 2010).
We explore instead a feature-rich approach to
address this issue. Prior work has explored
feature-rich approaches to modeling the transla-
tion distribution (Berg-Kirkpatrick et al., 2010);
we use the same technique, but only for the fertil-
ity model. The fertility distribution is modeled as
a log-linear distribution of F, a binary feature set:
p(φ|e) a exp (θ · F(e, φ)). We include a simple
set of features:
</bodyText>
<listItem confidence="0.972246333333333">
• A binary indicator for each fertility φ. This
feature is present for all words, acting as
smoothing.
• A binary indicator for each word id and fer-
tility, if the word occurs more than 10 times.
• A binary indicator for each word length (in
letters) and fertility.
• A binary indicator for each four letter word
prefix and fertility.
</listItem>
<bodyText confidence="0.9967745">
Together these produce a distribution that can
learn a reasonable distribution not only for com-
mon words, but also for rare words. Including
word length information aids in for languages with
compounding: long words in one language may
correspond to multiple words in the other.
</bodyText>
<table confidence="0.9947">
Algorithm AER (G→E) AER (E→G)
HMM 24.0 21.8
FHMM Viterbi 19.7 19.6
FHMM Dual-dec 18.0 17.4
</table>
<tableCaption confidence="0.98985">
Table 1: Experimental results over the 120 evalu-
ation sentences. Alignment error rates in both di-
rections are provided here.
</tableCaption>
<sectionHeader confidence="0.998051" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999981714285714">
We explore the impact of this improved MAP in-
ference procedure on a task in German-English
word alignment. For training data we use the news
commentary data from the WMT 2012 translation
task.1 120 of the training sentences were manually
annotated with word alignments.
The results in Table 1 compare several differ-
ent algorithms on this same data. The first line is
a baseline HMM using exact posterior computa-
tion and inference with the standard dynamic pro-
gramming algorithms. The next line shows the fer-
tility HMM with approximate posterior computa-
tion from Gibbs sampling but with final alignment
selected by the Viterbi algorithm. Clearly fertil-
ity modeling is improving alignment quality. The
prior work compared Viterbi with a form of local
search (sampling repeatedly and keeping the max),
finding little difference between the two (Zhao and
Gildea, 2010). Here, however, the difference be-
tween a dual decomposition and Viterbi is signifi-
cant: their results were likely due to search error.
</bodyText>
<sectionHeader confidence="0.993848" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999982533333334">
We have introduced a dual decomposition ap-
proach to alignment inference that substantially
reduces alignment error. Unfortunately the algo-
rithm is rather slow to converge: after 40 iterations
of the dual decomposition, still only 55 percent
of the test sentences have converged. We are ex-
ploring improvements to the simple sub-gradient
method applied here in hopes of finding faster con-
vergence, fast enough to make this algorithm prac-
tical. Alternate parameter estimation techniques
appear promising given the improvements of dual
decomposition over sampling. Once the perfor-
mance issues of this algorithm are improved, ex-
ploring hard EM or some variant thereof might
lead to more substantial improvements.
</bodyText>
<footnote confidence="0.967921">
1www.statmt.org/wmt12/translation-task.html
</footnote>
<page confidence="0.998794">
10
</page>
<sectionHeader confidence="0.995834" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915510204081">
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 582–590, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311.
Xiaodong He. 2007. Using word-dependent transition
models in HMM-based word alignment for statisti-
cal machine translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 80–87, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics.
Robert C. Moore. 2004. Improving ibm word align-
ment model 1. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL’04), Main Volume, pages 518–525, Barcelona,
Spain, July.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305–362.
Kristina Toutanova and Michel Galley. 2011. Why
initialization matters for ibm model 1: Multiple op-
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 461–466, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical
translation. In COLING.
Shaojun Zhao and Daniel Gildea. 2010. A fast fertil-
ity hidden markov model for word alignment using
MCMC. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, pages 596–605, Cambridge, MA, October. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999486">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621806">
<title confidence="0.984255">Exact Maximum Inference for the Fertility Hidden Markov Model</title>
<author confidence="0.875159">Chris</author>
<affiliation confidence="0.853478">Microsoft</affiliation>
<address confidence="0.870059">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.99967">chrisq@microsoft.com</email>
<abstract confidence="0.998766277777778">The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Painless unsupervised learning with features.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>582--590</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Berg-Kirkpatrick, Bouchard-Cˆot´e, DeNero, Klein, 2010</marker>
<rawString>Taylor Berg-Kirkpatrick, Alexandre Bouchard-Cˆot´e, John DeNero, and Dan Klein. 2010. Painless unsupervised learning with features. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 582–590, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1288" citStr="Brown et al., 1993" startWordPosition="195" endWordPosition="198">ithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality. 1 Introduction Word-based translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word). M</context>
<context position="4933" citStr="Brown et al., 1993" startWordPosition="806" endWordPosition="809">sed on only the prior alignment variable. Next the French word is predicted based on its aligned English word. Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution. Pr(f, a|e) =p(J|I) YI p(φi|ei) J i=1 (1) Y p(aj|aj−1) pfj ��eaj � j=1 where φi = PJj=1 δ(i, aj) indicates the number of times that state j is visited. This deficient model wastes some probability mass on inconsistent configurations where the number of times that a state i is visited does not match its fertility φi. Following in the footsteps of older, richer, and wiser colleagues (Brown et al., 1993),we forge ahead unconcerned by this complication. 2.1 Parameter estimation Of greater concern is the exponential complexity of inference in this model. For the standard HMM, there is a dynamic programming algorithm to compute the posterior probability over word alignments Pr(a|e, f). These are the sufficient statistics gathered in the E step of EM. The structure of the fertility model violates the Markov assumptions used in this dynamic programming method. However, we may empirically estimate the posterior distribution using Markov chain Monte Carlo methods such as Gibbs sampling (Zhao and Gil</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
</authors>
<title>Using word-dependent transition models in HMM-based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>80--87</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2435" citStr="He, 2007" startWordPosition="386" endWordPosition="387">earby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challenging. The authors originally recommended heuristic procedures based on local search for both. Such methods work reasonably well, but can be computationally inefficient and have few guarantees. Thus, many researchers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007). This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima are a concern. Modeling fertility is challenging in the HMM framework as it violates the Markov assumption. Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space. Therefore, the standard forward-backward and Viterbi algorithms do not apply. Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques</context>
</contexts>
<marker>He, 2007</marker>
<rawString>Xiaodong He. 2007. Using word-dependent transition models in HMM-based word alignment for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 80–87, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1412" citStr="Koehn et al., 2003" startWordPosition="216" endWordPosition="219"> leads to a substantial improvement in alignment quality. 1 Introduction Word-based translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Althou</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Improving ibm word alignment model 1.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>518--525</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="1469" citStr="Moore, 2004" startWordPosition="226" endWordPosition="227">oduction Word-based translation models intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Although these latter models covered a broad range of phenomena</context>
</contexts>
<marker>Moore, 2004</marker>
<rawString>Robert C. Moore. 2004. Improving ibm word alignment model 1. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 518–525, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>45--305</pages>
<contexts>
<context position="6322" citStr="Rush and Collins, 2012" startWordPosition="1023" endWordPosition="1026">all other positions according to the distribution Pr(aj|a−j, e, f). Given a complete assignment of the alignment for all words except the current, computing the complete probability including transition, emission, and jump, is straightforward. This estimate comes with a computational cost: we must cycle through all positions of the vector repeatedly to gather a good estimate. In practice, a small number of samples will suffice. 2.2 MAP inference with dual decomposition Dual decomposition, also known as Lagrangian relaxation, is a method for solving complex combinatorial optimization problems (Rush and Collins, 2012). These complex problems are separated into distinct components with tractable MAP inference procedures. The subproblems are repeatedly solved with some communication over consistency until a consistent and globally optimal solution is found. Here we are interested in the problem of finding the most likely alignment of a sentence pair e, f. Thus, we need to solve the combinatorial optimization problem arg maxa Pr(f, a|e). Let us rewrite the objective function as follows: h(a) = XI ⎛ ⎞ i=1 X log p(fj|ei) ⎝log p(φi|ei) + ⎠ 2 j,aj=i J � eaj ) +X log p(aj|aj−1) + log p(fj 2 j=1 Because f is fixed,</context>
<context position="8109" citStr="Rush and Collins (2012)" startWordPosition="1362" endWordPosition="1365">− Ei,j u(k−1)(i, j)z(i, j) if ya = z return a(k) end if ( ) u(k)(i, j) := u(k)(i, j) + δk ya(k)(i, j) − z(k)(i, j) end for return a(K) Figure 1: The dual decomposition algorithm for the fertility HMM, where Sk is the step size at the kth iteration for 1 G k G K, and K is the max number of iterations. matrix. Define the functions f and g as J (logp(aj|aj−1)+ 21logp(fjleaj)) j=1 Then we want to find f(a) + g(z) subject to the constraints ya(i, j) = z(i, j)Vi, j. Note how this recovers the original objective function when matching variables are found. We use the dual decomposition algorithm from Rush and Collins (2012), reproduced here in Figure 1. Note how the langrangian adds one additional term word, scaled by a value indicating whether that word is aligned in the current position. Because it is only added for those words that are aligned, we can merge this with the log p(fj leaj) terms in both f and g. Therefore, we can solve arg maxa(f(a) + Ei,j u(k−1)(i,j)ya(i,j)) using the standard Viterbi algorithm. The g function, on the other hand, does not have a commonly used decomposition structure. Luckily we can factor this maximization into pieces that allow for efficient computation. Note that g sums over a</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M Rush and Michael Collins. 2012. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. Journal of Artificial Intelligence Research, 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Michel Galley</author>
</authors>
<title>Why initialization matters for ibm model 1: Multiple optima and non-strict convexity.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>461--466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1522" citStr="Toutanova and Galley, 2011" startWordPosition="232" endWordPosition="235"> intended to model the translation process have found new uses identifying word correspondences in sentence pairs. These word alignments are a crucial training component in most machine translation systems. Furthermore, they are useful in other NLP applications, such as entailment identification. The simplest models may use lexical information alone. The seminal Model 1 (Brown et al., 1993) has proved very powerful, performing nearly as well as more complicated models in some phrasal systems (Koehn et al., 2003). With minor improvements to initialization (Moore, 2004) (which may be important (Toutanova and Galley, 2011)), it can be quite competitive. Subsequent IBM models include more detailed information about context. Models 2 and 3 incorporate a positional model based on the absolute position of the word; Models 4 and 5 use a relative position model instead (an English word tends to align to a French word that is nearby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challe</context>
</contexts>
<marker>Toutanova, Galley, 2011</marker>
<rawString>Kristina Toutanova and Michel Galley. 2011. Why initialization matters for ibm model 1: Multiple optima and non-strict convexity. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 461–466, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2390" citStr="Vogel et al., 1996" startWordPosition="377" endWordPosition="380"> English word tends to align to a French word that is nearby the French word aligned to the previous English word). Models 3, 4, and 5 all incorporate a notion of “fertility”: the number of French words that align to any English word. Although these latter models covered a broad range of phenomena, estimation techniques and MAP inference were challenging. The authors originally recommended heuristic procedures based on local search for both. Such methods work reasonably well, but can be computationally inefficient and have few guarantees. Thus, many researchers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007). This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima are a concern. Modeling fertility is challenging in the HMM framework as it violates the Markov assumption. Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space. Therefore, the standard forward-backward and Viterbi algorithms do not apply. Recent work (Zhao and Gildea, 2010) described an extension to the HMM </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaojun Zhao</author>
<author>Daniel Gildea</author>
</authors>
<title>A fast fertility hidden markov model for word alignment using MCMC.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>596--605</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2955" citStr="Zhao and Gildea, 2010" startWordPosition="466" endWordPosition="469">chers have switched to the HMM model (Vogel et al., 1996) and variants with more parameters (He, 2007). This captures the positional information in the IBM models in a framework that admits exact parameter estimation inference, though the objective function is not concave: local maxima are a concern. Modeling fertility is challenging in the HMM framework as it violates the Markov assumption. Where the HMM jump model considers only the prior state, fertility requires looking across the whole state space. Therefore, the standard forward-backward and Viterbi algorithms do not apply. Recent work (Zhao and Gildea, 2010) described an extension to the HMM with a fertility model, using MCMC techniques for parameter estimation. However, they do not have a efficient means of MAP inference, which is necessary in many applications such as machine translation. This paper introduces a method for exact MAP inference with the fertility HMM using dual decomposition. The resulting model leads to substantial improvements in alignment quality. 7 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 7–11, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguist</context>
<context position="4469" citStr="Zhao and Gildea, 2010" startWordPosition="722" endWordPosition="725">icates the English word generating the jth French word, 0 representing a special NULL state to handle systematically unaligned words. J Pr(f, a|e) = p(J|I) Y p(aj|aj−1) pfj ��eaj � j=1 The generative story begins by predicting the number of words in the French sentence (hence the number of elements in the alignment vector). Then for each French word position, first the alignment variable (English word index used to generate the current French word) is selected based on only the prior alignment variable. Next the French word is predicted based on its aligned English word. Following prior work (Zhao and Gildea, 2010), we augment the standard HMM with a fertility distribution. Pr(f, a|e) =p(J|I) YI p(φi|ei) J i=1 (1) Y p(aj|aj−1) pfj ��eaj � j=1 where φi = PJj=1 δ(i, aj) indicates the number of times that state j is visited. This deficient model wastes some probability mass on inconsistent configurations where the number of times that a state i is visited does not match its fertility φi. Following in the footsteps of older, richer, and wiser colleagues (Brown et al., 1993),we forge ahead unconcerned by this complication. 2.1 Parameter estimation Of greater concern is the exponential complexity of inference</context>
<context position="11930" citStr="Zhao and Gildea, 2010" startWordPosition="2061" endWordPosition="2064">in dynamic programming. This refinement does not change the correctness of the dual decomposition algorithm; rather it speeds the convergence. 3 Fertility distribution parameters Original IBM models used a categorical distribution of fertility, one such distribution for each English word. This gives EM a great amount of freedom in parameter estimation, with no smoothing or parameter tying of even rare words. Prior work addressed this by using the single parameter Poisson distribution, forcing infrequent words to share a global parameter estimated from the fertility of all words in the corpus (Zhao and Gildea, 2010). We explore instead a feature-rich approach to address this issue. Prior work has explored feature-rich approaches to modeling the translation distribution (Berg-Kirkpatrick et al., 2010); we use the same technique, but only for the fertility model. The fertility distribution is modeled as a log-linear distribution of F, a binary feature set: p(φ|e) a exp (θ · F(e, φ)). We include a simple set of features: • A binary indicator for each fertility φ. This feature is present for all words, acting as smoothing. • A binary indicator for each word id and fertility, if the word occurs more than 10 t</context>
<context position="14047" citStr="Zhao and Gildea, 2010" startWordPosition="2411" endWordPosition="2414">tated with word alignments. The results in Table 1 compare several different algorithms on this same data. The first line is a baseline HMM using exact posterior computation and inference with the standard dynamic programming algorithms. The next line shows the fertility HMM with approximate posterior computation from Gibbs sampling but with final alignment selected by the Viterbi algorithm. Clearly fertility modeling is improving alignment quality. The prior work compared Viterbi with a form of local search (sampling repeatedly and keeping the max), finding little difference between the two (Zhao and Gildea, 2010). Here, however, the difference between a dual decomposition and Viterbi is significant: their results were likely due to search error. 5 Conclusions and future work We have introduced a dual decomposition approach to alignment inference that substantially reduces alignment error. Unfortunately the algorithm is rather slow to converge: after 40 iterations of the dual decomposition, still only 55 percent of the test sentences have converged. We are exploring improvements to the simple sub-gradient method applied here in hopes of finding faster convergence, fast enough to make this algorithm pra</context>
</contexts>
<marker>Zhao, Gildea, 2010</marker>
<rawString>Shaojun Zhao and Daniel Gildea. 2010. A fast fertility hidden markov model for word alignment using MCMC. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>