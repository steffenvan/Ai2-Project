<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.988562333333333">
Morphological and Phonological Learning: Proceedings of the 6th Workshop of the
ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,
July 2002, pp. 31-40. Association for Computational Linguistics.
</note>
<title confidence="0.983473">
Unsupervised Learning of Morphology Without Morphemes
</title>
<author confidence="0.9969">
Sylvain Neuvel Sean A. Fulop
</author>
<affiliation confidence="0.8952495">
Dept. of Linguistics Depts. of Linguistics
sneuvel@uchicago.edu and Computer Science
</affiliation>
<email confidence="0.750131">
sfulop@uchicago.edu
</email>
<affiliation confidence="0.923573">
The University of Chicago
</affiliation>
<sectionHeader confidence="0.993626" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999898486486486">
The first morphological learner based
upon the theory of Whole Word Mor-
phology (Ford et al., 1997) is outlined,
and preliminary evaluation results are pre-
sented. The program, Whole Word Mor-
phologizer, takes a POS-tagged lexicon
as input, induces morphological relation-
ships without attempting to discover or
identify morphemes, and is then able to
generate new words beyond the learning
sample. The accuracy (precision) of the
generated new words is as high as 80% us-
ing the pure Whole Word theory, and 92%
after a post-hoc adjustment is added to the
routine.
The aim of this project is to develop a computa-
tional model employing the theory of whole word
morphology (Ford et al., 1997) capable on the one
hand of identifying morphological relations within a
list of words from any one of a wide variety of lan-
guages and, on the other, of putting that knowledge
to use in creating previously unseen word forms.
A small application called Whole Word Morpholo-
gizer which does just this is outlined and discussed.
In particular, this approach is set against the liter-
ature on computational morphology as an entirely
different way of doing things which has the potential
to be generalized to all known varieties of morphol-
ogy in the world’s languages, a feature not shared by
previous methods. As it is based on a model of the
mental lexicon in which all entries are entire, fully
fledged words, this project also serves as an empiri-
cal demonstration that a word-based morphological
theory that rejects the notion of morpheme as mini-
mal unit of form and meaning (and/or grammatical
properties) is viable from the point of view of acqui-
sition as well as generation.
</bodyText>
<sectionHeader confidence="0.978028" genericHeader="method">
1 Morphological learning
</sectionHeader>
<bodyText confidence="0.999874923076923">
Since its inception in the mid 1950s, the field of
computational morphology has been characterized
by a paucity of procedures for generation. Notwith-
standing the impressive body of literature on the
shortcomings of traditional Paninian morphology,
most computational research projects also rely on a
traditional notion of the morpheme and ignore all
non-compositional aspects of morphology. These
observations are obviously not unrelated and are in
part inherited from the field of computational syntax
where applications traditionally were designed to as-
sign a syntactic structure to a given string of words,
though this is less true today.
</bodyText>
<subsectionHeader confidence="0.997256">
1.1 Segmentation and morpheme identification
</subsectionHeader>
<bodyText confidence="0.999988338983051">
Word formation and the population of the lexicon,
while central to morphological theory, are notice-
ably absent from the field of computational mor-
phology. Most computational work in the field
of morphology has focused on the identification of
morphemes or morphological parsing while paying
little or no attention to generation. While these ap-
plications find a common goal in the automatic ac-
quisition of morphology, it is helpful to distinguish
between two types of analysis in light of the often
very different results sought by various morphologi-
cal learners.
On the one hand, some applications focus ex-
clusively on the segmentation of words or longer
strings into smaller units. In other words, their
function is to identify morpheme boundaries within
words and, as such, they only indirectly identify
morphemes as linguistic units. Zellig Harris’s (Har-
ris, 1955; Harris, 1967) pioneering work suggests
that morpheme boundaries can be determined by
counting the number of letters that follow a given
substring within a corpus (v. (Hafer and Weiss,
1974) for a further development of Harris’s ideas).
Janssen (1992) and Flenner (1994; 1995) also work
towards segmenting words but use training corpora
in which morpheme boundaries have been manually
inserted. Recent work by Kazakov and Manand-
har (1998) combines unsupervised and supervised
learning techniques to generate a set of segmenta-
tion rules that can further be applied to previously
unseen words.
On the other hand, some computational morpho-
logical applications are designed solely to identify
morphemes based on a training corpus and not to
provide a morphological analysis for each word of
that corpus. Brent (1993), for example, aims at find-
ing the right set of suffixes from a corpus, but the
algorithm cannot double as a morphological parser.
More recently, efforts have been developing
which identify morphemes and perform some sort of
analysis. Schone and Jurafsky (2001) employ a great
many sophisticated post-hoc adjustments to obtain
the right conflation sets for words by pure corpus
analysis without annotations. Their procedure uses
a morpheme-based model, provides an analysis of
the words, and does in a sense discover morphologi-
cal relations. Goldsmith (2001b; 2001a), inspired by
de Marcken’s (1995) thesis on minimum description
length, attempts to provide both a list of morphemes
and an analysis of each word in a corpus. Also, Ba-
roni (2000) aims at finding a set of prefixes from a
corpus, together with an affix-stem parse of each of
the words.
While they might differ in their methods or ob-
jectives, all of the above morphological applications
share a common characteristic in that they are learn-
ers designed exclusively for the acquisition of mor-
phological facts from corpora and do not generate
new words based on the information they acquire.
</bodyText>
<subsectionHeader confidence="0.997575">
1.2 Parsing and generation
</subsectionHeader>
<bodyText confidence="0.999977566666667">
Only a handful of programs can both parse and gen-
erate words. Once again, these programs fall into
two very distinct categories. In view of the dispar-
ity between these programs, it is useful to distin-
guish between genuine morphological learners able
to generate from acquired knowledge and genera-
tors/parsers that implement a man-made analysis.
The latter group is perhaps the most well known, so
let us begin with them.
Kimmo-type applications of two-level morphol-
ogy (Koskenniemi, 1983; Antworth, 1990; Kart-
tunen et al., 1992; Karttunen, 1993; Karttunen,
1994) can provide a morphological analysis of the
words in a corpus and generate new words based on
a set of rules; but these programs must first be pro-
vided with that set of rules and a lexicon contain-
ing morphemes by the user. Similar work in one-
and two-level morphology has been done using the
Attribute-Logic Engine (Carpenter, 1992). Some of
these systems (e.g. (Karttunen et al., 1987)) have
a front-end that compiles more traditional linearly
ordered morphological rules into the finite-state au-
tomata of two-level morphology. Once again, these
applications require a set of man-made lexical rules
to function. While the practical uses of such applica-
tions as PC-Kimmo are incontestable, it is clear that
they are part of a different endeavour, and should not
be confused with genuine morphological learners.
The other relevant group of computational appli-
cations can, as mentioned, both acquire morpho-
logical knowledge from corpora and generate new
words based on that knowledge. Albright and Hayes
(2001a; 2001b) tackle the wider task of acquir-
ing morphology and (morpho)phonology based on
a small paradigm list and their learner is able to gen-
erate particular inflected forms given a related word.
Dˇzeroski and Erjavec (1997) work towards learning
morphological rules for forming particular inflec-
tional forms given a lemma (a set of related words).
Their learner produces a set of rules relating all the
members of a paradigm to a base form. The program
can then produce a member of that paradigm on
command given the base form. While the methods
used by Albright and Hayes and Dˇzeroski and Er-
javec radically differ, both use a form of supervised
learning which significantly reduces the amount of
information their learner has to acquire. Albright
and Hayes train their program using a paradigm list
in which each entry contains, for example, both the
present and past tense forms of an English verb.
Similarly, the training data used by Dˇzeroski and Er-
javec similarly has a base form, or lexeme, associ-
ated to each and every word so that all the words
of a given paradigm share a common label. The
distinctions between the two methods are immate-
rial, what matters is that both learners are being told
which words are related to which and are left with
the task of describing that relation in the form a rule.
In other words, the algorithms they use cannot dis-
cover that words are morphologically related.
</bodyText>
<subsectionHeader confidence="0.962019">
1.3 What’s morphology?
</subsectionHeader>
<bodyText confidence="0.999987951219512">
In the above algorithms, the task of determining
whether one word is related to another in a morpho-
logical sense is most frequently left to the linguist,
as this information has to be encoded in the train-
ing data for these algorithms. (Some of the most
recent work such as (Schone and Jurafsky, 2001)
and (Goldsmith, 2001b) are notable exceptions to
this paradigm.) This is perhaps not surprising, since
no serious attempt at defining a morphological rela-
tion has been made in the last few decades. Amer-
ican structuralists of the forties and fifties proposed
what have been referred to as discovery procedures
(v. (Nida, 1949), for example) for the identification
of morphemes but since the mid fifties (Chomsky,
1955), it has been customary for morphological the-
ory to ignore this aspect of morphology and relegate
it to studies on language acquisition. But, since a
morphological learner like that presented here is de-
signed to model the acquisition of morphology, it
seems that it should above all be able to determine
for itself whether two words are morphologically re-
lated or not, whether there is anything morphologi-
cal to acquire at all.
Another important thing to note about the vast
majority of computational morphology learners is
their reliance on a traditional notion of the mor-
pheme as a lexical unit and their exclusive fo-
cus on concatenative morphology. There is a
panoply of recent publications devoted to the em-
pirical shortcomings of traditional so-called “Item-
and-Arrangement” morphology (Hockett, 1954;
Bochner, 1993; Ford and Singh, 1991; Anderson,
1992; Ford et al., 1997), and the list of phenomena
that fall out of reach of a compositional approach
is rather impressive: zero-morphs, ablaut-like pro-
cesses, templatic morphology, class markers, partial
suppletion, etc. Still, seemingly every documented
morphological learner relies on a Bloomfieldian no-
tion of the morpheme and produces an Item-and-
Arrangement analysis; this description applies to all
of the computational papers cited above.
</bodyText>
<sectionHeader confidence="0.782889" genericHeader="method">
2 An alternative theory
</sectionHeader>
<bodyText confidence="0.998972740740741">
Whole Word Morphologizer (henceforth WWM) is
the first implementation of the theory of Whole
Word Morphology. The theory, developed by Alan
Ford and Rajendra Singh at Universit´e de Montr´eal,
seeks to account for morphological relations in a
minimalist fashion. Ford and Singh published a se-
ries of papers dealing with various aspects of the the-
ory between 1983 and 1990. Drawing on these pa-
pers, they published a full outline of it in 1991 (Ford
and Singh, 1991) and an even fuller defense of it
in 1997 (Ford et al., 1997). Since then, aspects of it
have been taken up in a series of publications by Ag-
nihotri, Dasgupta, Ford, Neuvel, Singh, and various
combinations of these authors. The central mech-
anism of the theory, the Word Formation Strategy
(WFS), is a sort of non-decomposable morpholog-
ical transformation that relates full words with full
words (or helps one fashion a full word from an-
other full word) and parses any complex word into
a variable and a non-variable component. Neuvel
and Singh (In press) offer a strict definition of mor-
phological relatedness and, based on this definition,
suggest guidelines for the acquisition of Word For-
mation Strategies.
In Whole-Word Morphology, any morphological
relation can be represented by a rule of the following
form:
</bodyText>
<listItem confidence="0.658092">
(1) |X|α ↔ |X0|β
</listItem>
<bodyText confidence="0.6899245">
in which the following conditions and notations are
employed:
</bodyText>
<listItem confidence="0.99976225">
1. |X|α and |X0|β are statements that words of the
form X and X0 are possible in the language,
and X and X0 are abbreviations of the forms of
classes of words belonging to categories α and
β (with which specific words belonging to the
right category can be unified in form);
2. 0 represents all the form-related differences be-
tween X and X0;
3. α and b are categories that may be represented
as feature-bundles;
4. ↔ represents a bi-directional implication;
5. X&apos; and X are semantically related.
</listItem>
<bodyText confidence="0.99995935">
There are several ramifications of (1). First, there
is only one morphology; no distinction, other than
a functional one, is made between inflection and
derivation. Second, morphology is relational and not
compositional. The program thus makes no refer-
ence to theoretical constructs such as ‘root’, ‘stem’,
and ‘morpheme’, or devices such as ‘levels’ and
‘strata’ and relies exclusively on the notion of mor-
phological relatedness. And since its objective is
not to assign a probability to a given word or string,
it must rely on a strict formal definition of a mor-
phological relation. Ultimately, the theory takes the
Saussurean view that words are defined by the differ-
ences amongst them and argues that some of these
differences, namely those that are found between
two or more pairs of words, constitute the domain
of morphology. In other words, two words of a lexi-
con are morphologically related if and only if all the
differences between them are found in at least one
other pair of words of the same lexicon.
</bodyText>
<sectionHeader confidence="0.919397" genericHeader="method">
3 Overview of the method
</sectionHeader>
<bodyText confidence="0.999953818181818">
Under the assumption that the morphology of a lan-
guage resides exclusively in differences that are ex-
ploited in more than one pair of words within its lex-
icon, WWM (Algorithm 1 in the next section) com-
pares every word of a small lexicon and determines
the segmental differences found between them. The
input to the current version of the program is a small
text file that contains anywhere from 1000 to 5000
words. Each word appears in orthographic form and
is followed by its syntactic and morphological cate-
gories, as in the example below:
</bodyText>
<equation confidence="0.4484804">
(2) cat, Ns (Noun, singular)
catch, V
catches, V3s (Verb, (pres.) 3rd pers.
sing.)
decided, Vp (Verb, past)
</equation>
<bodyText confidence="0.999941896551724">
The algorithm simply compares each letter from
word A to the corresponding one from word B to
produce a comparison record, which can be viewed
as a data structure. Currently, it works on ortho-
graphic representations. This means it would as eas-
ily work on phonemic transcriptions, but it will re-
quire empirical evaluation to see whether the results
from these can improve upon those obtained using
spellings, and we have not yet gone through such an
exercise. It starts on either the left or right edge of
the words if the two words share their first (few) seg-
ments or their last (few) segments, respectively (the
forward version is presented in Algorithm 2 in the
next section). This is just a simple-minded way of
aligning the similar parts of the words for the com-
parison; a more sophisticated implementation in the
future could use a more general sequence alignment
procedure. The segments are placed in one of two
lists in the comparison structure (differences or sim-
ilarities) based on whether or not they are identical.
Each comparison structure also contains the cate-
gories of both words, and is kept in a large list of all
comparison structures found from analyzing the en-
tire corpus. The example below shows the informa-
tion in the comparison structure produced from the
English words receive and reception. It includes the
differences and similarities between the two words,
from the perspective of each word in turn, as well as
the lexical categories of the words.
</bodyText>
<figure confidence="0.849522333333333">
(3) Differences
First word Second word
####iveV ####ptionNs
Similarities
First Second
rece### rece#####
</figure>
<bodyText confidence="0.996368428571428">
Matching character sequences in the difference
section are replaced with a variable. The re-
sult is then set against comparisons generated by
other pairs of words and duplicate differences are
recognized. In the example below, the compar-
isons produced by the pairs receive/reception, con-
ceive/conception and deceive/deception are shown.
</bodyText>
<listItem confidence="0.856128">
(4) Differences
</listItem>
<table confidence="0.839337">
First word Second word
X iveV X ptionNs
X iveV X ptionNs
X iveV X ptionNs
Similarities
First Second
rece### rece#####
conce### conce#####
dece### dece#####
</table>
<bodyText confidence="0.994718571428571">
The three comparisons in (4) share the same for-
mal and grammatical differences, and so the theory
indicates they should be merged into one morpho-
logical strategy. Since the differences are the same,
it is only the similarities that are actually merged.
Each new morphological strategy is also restricted
to apply in as narrow an environment as possible.
Neuvel and Singh (Neuvel and Singh, In press) sug-
gest that any morphological strategy must be maxi-
mally restricted at all times; this is accomplished by
specifying as constant all the similarities found, not
between words, but between the similarities found
between words. In (4), all three sets of similarities
end with the sequence of letters “ce.” These similar-
ities between similarities are specified as constant in
each strategy and the length of each word is also fac-
tored in. The merge routine called in Algorithm 2
carries out this procedure; we don’t show it because
it is tedious but not especially interesting. The re-
stricted morphological strategy relating the words in
(4) is as follows:
</bodyText>
<figure confidence="0.394214333333333">
(5) Differences
First word Second word
X iveV X ptionNs
Similarities
First Second
*##ce### *##ce#####
</figure>
<bodyText confidence="0.9476042">
For the sake of clarity, we can represent the infor-
mation contained in (5) in a more familiar fashion
using the formalism described in (1). The vertical
brackets ‘ · ’ are used for orthographic forms so as
not to confuse them with phonemic representations.
</bodyText>
<listItem confidence="0.901334">
(6) *##ceive V H *##ception Ns
</listItem>
<bodyText confidence="0.999398744680851">
The ‘#’ signs in the above representations stand
for letters that must be instantiated but are not spec-
ified; the ‘*’ symbol stands for a letter that is not
specified and that may or may not be instantiated.
Strategy (6) can therefore be interpreted as follows:
(6) If there is a verb that ends with the sequence
“ceive” preceded by no less than two and
no more than three characters, there should
also be a singular noun that ends with the se-
quence “ception” preceded by the same two
or three characters.
After performing the comparisons and merging,
WWM extracts a list of morphological strategies,
which are those comparison structures whose count
is more than some fixed threshold. Table 1 con-
tains a few strategies found from the first few
chapters of Moby Dick. These strategies result
from merging comparison structures which have the
same differences—merging the similarities of sev-
eral unifiable word pairs, and so many have no spec-
ified letters at all.
WWM then goes through the lexicon word by
word and attempts to unify each word in form and
category with the left or right side of this strategy.
If it succeeds, WWM replaces all the segments fully
specified on the side of the strategy the word is uni-
fied with, with the segments fully specified on the
other side. For example, given the noun perception
in the corpus and strategy (6), WWM will map the
word onto the right hand side of (6), take out the se-
quence “ception” from the end and replace it with
the sequence “ceive” to produce the new word per-
ceive. The category of the word will also be changed
from singular noun to verb. New words can thus be
generated in a rather obvious fashion by taking each
word in the original lexicon and applying any strate-
gies that can be applied, i.e. whose orthographic
form and part of speech can be unified with the word
at hand. Algorithm 3 shows the basic generation
procedure; once again the routines called unify
and create which implement the nitty-gritty de-
tails of the above description are not given because
they are more tedious than interesting, and will cer-
tainly need to be changed in more general future
versions of WWM. Table 2 gives some of the new
words WWM creates using text from Le petit prince
as its base lexicon.
</bodyText>
<tableCaption confidence="0.999026">
Table 1: Word-formation strategies discovered from Moby Dick
</tableCaption>
<table confidence="0.9999195">
Differences Similarities Examples
1st word 2nd word 1st word 2nd word
XdPP XV ****####e# ****####e baked/bake, charged/charge
XedPP XV *######## *###### directed/direct
XsNp XNs ******##### ******#### helmets/helmet, rabbits/rabbit
XingGER XedPP ******####### ******###### walking/walked, talking/talked
XingGER XsV3s *****####### *****##### walking/walks, talking/talks
XnessNs XADJ ****######### ********##### short/shortness
XlyADV XADJ ******###### ******#### easy/easily, quick/quickly
XestADJ XADJ *####### *#### hardest/hard, shortest/short
XsV3s XV ***##### ***#### jumps/jump, plays/play
XerADJ XADJ *###### *#### harder/hard, louder/loud
XlessADJ XNs *######## *#### painless/pain, childless/child
XingGER XyADJ *####### *##### raining/rainy, running/runny
XedPP XsV3s **###### **##### played/plays
XingsNp XV ***######### ***##### paintings/paint
</table>
<tableCaption confidence="0.973642">
Table 2: Words generated from Le petit prince
</tableCaption>
<table confidence="0.995034363636364">
drames Np droitement ADV
dress´ee PF drˆoles AIP
dresser INF drˆolement ADV
dressa Vp3 dunes Np
dressais Vi2 durerait Vc3
dresse V3 d´ecid´ee PF
dressent V6 d´ecider INF
dressez V5 d´ecida Vp3
dressait Vi3 d´ecide V3
droits AMP d´ecoiff´e AM
droites AFP d´econcentr´es AMP
</table>
<bodyText confidence="0.99686745">
The output from the algorithm is a list of words,1
much as in Table 2, which are generated from the in-
put corpus using the morphological relations (strate-
gies) discovered. The method described above will
clearly force WWM to create words that were al-
ready part of its original lexicon; in fact, each and
every word involved in licensing the discovery of
a morphological strategy will be duplicated by the
program. Generated words that were not part of
WWM’s original lexicon are then added to a sepa-
1By word we mean an orthographic form together with the
part of speech. Further work in this vein would add meanings
as well.
rate word list containing only new words. If desired,
this new word list can be merged with the original
lexicon for another round of discovery to formu-
late new strategies based on a larger dataset. Ad-
ditionally, each of the new words can simply be put
through another cycle of word creation by applying
the same strategies as before a second time.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.999878105263158">
This section contains some pseudocode showing
several basic components of the Whole Word Mor-
phologizer. Algorithm 1 shows the main procedure,
which takes a POS-tagged lexicon as input and out-
puts a list of all words that are possible given the
morphological relations present in the lexicon.
The two procedures compforward and comp-
backward are symmetrical, so Algorithm 2 shows
just the first of these. This algorithm provides the
data structure which includes the differences and
similarities between each pair of words in the lexi-
con, in similar fashion to the examples in the preced-
ing section. In practice, only those pairs of words
which are by some heuristic sufficiently similar in
the first place are compared. Additionally, the two
similarities sequences for each word pair are actu-
ally represented as one sequence which encodes the
information found in the two sequences of the exam-
ples in the preceding; this is just for convenience of
</bodyText>
<figure confidence="0.817254318181818">
Algorithm 1 WWM(lexicon)
Require: lexicon to be a list of POS-tagged
words.
Ensure: a list newwords is generated
for all tagged words wi do
for all tagged words wj do
if wi and wj share a beginning sequence
then
compforward(wi,wj)
else if wi and wj share an ending sequence
then
compbackward(wi,wj)
end if
end for
end for
for all comparison structures in the list do
if count(comparison) &gt; Threshold then
append comparison to the list strate-
gies
generate(lexicon, strategies)
end if
end for
</figure>
<bodyText confidence="0.997010714285714">
storage and computation.
Algorithm 3 shows the outline of the final stage,
which generates an output list of words from the in-
put lexicon and the morphological strategies. The
strategy list is simply a list of all comparison struc-
tures that occurred more frequently than some arbi-
trary threshold number.
</bodyText>
<sectionHeader confidence="0.97265" genericHeader="conclusions">
5 Accomplishments and prospects
</sectionHeader>
<subsectionHeader confidence="0.941488">
5.1 Initial results
</subsectionHeader>
<bodyText confidence="0.972151375">
Whole Word Morphologizer has been tested on a
limited basis using English and French lexicons of
approximately 3000 entries, garnered from the POS-
tagged versions of Le petit prince and Moby Dick.
The program initially, without any post-hoc correc-
tions, achieved between 70% and 82% accuracy in
generation; these figures measure the percentage of
the new words beyond the original lexicon that are
possible words of the language. The figures thus
measure a kind of precision value, in terms of the
precision/recall tradeoff, and are fair values in that
they do not include the generated words that are al-
ready in the lexicon.
Algorithm 2 compforward(w1,w2)
Require: w1 and w2 to be (word, category) pairs.
Ensure: a data structure comparison document-
ing the different and similar letters between w1
and w2 is merged into the global list of com-
parisons. comparison is a structure of 5 lists
w1dif, w1cat, w2dif, w2cat, sim.
for x = 1 to length(w2) do
if characters w1(x) = w2(x) then
append w1(x) to list sim
if list w1dif does not end with ‘X’ then
</bodyText>
<figure confidence="0.690215">
append ‘X’ to both lists w1dif and w2dif
else
append w1(x) to w1dif,
append w2(x) to w2dif, append ‘#’ to sim
end if
end if
end for
for x = length(w2) + 1 to length(w1) do
append w1(x) to w1dif
</figure>
<subsectionHeader confidence="0.645877">
end for
</subsectionHeader>
<bodyText confidence="0.929509833333333">
if dif lists and categories match a comparison al-
ready in the list comps then
merge comparisons and increment
count(comparison)
else
append comparison to comps
</bodyText>
<equation confidence="0.7431815">
count(comparison) ← 1
end if
</equation>
<bodyText confidence="0.944128233333333">
A satisfactory recall metric seems impossible to
think of in its usual sense here. First of all, there are
generally an indefinite number of possible words in a
language. One therefore cannot give a precise set of
words that we wish the system could generate from
a specific lexicon, so there seems to be no way to
measure the percentage of “desired words” that are
in fact generated. Even if we were to make such a
list by hand from the current small corpora to use as
a gold standard (which has been suggested by a ref-
eree), it must also be remembered that WWM dis-
covers strategies (morphological relations) for cre-
ating new words from given ones. It cannot be ex-
pected to discover strategies that are not evident in a
corpus. Indeed, WWM will never discover that, for
example, ‘am’ and ‘be’ are related, because accord-
ing to the theory of morphology being applied these
Algorithm 3 generate(lexicon, strategies)
Ensure: a list newwords is generated using lex-
icon and strategies
for all words in lexicon do
for all strategies do
if unify(lexicon[x], strategies[x])
says the word and strategy match with either
left or right alignment then
newword ← create(lexicon[x],
strategies[x])
if newword is not in the lexicon or the list
newwords then
append newword to newwords list
</bodyText>
<tableCaption confidence="0.492789">
end if
end if
end for
end for
</tableCaption>
<bodyText confidence="0.99958275">
words are only related by convention, not by mor-
phology. “Nonproductive morphology” is not really
morphology.
The real point is that we do not want to hold
WWM’s performance up against our own ideas
about morphological relations among words, since
it would be practically impossible to determine not
merely a large set of possible words that linguists
think are related to those in the corpus, but rather a
set of possible words that WWM ought to generate
according to its theory. This would amount to try-
ing to beat WWM at its own game in pursuit of a
gold standard, which could only be obtained using a
better implementation of WWM’s theory. A perfect
implementation of Whole Word Morphology would
have perfect recall, in view of our eventual goal of
using this theory to inform us about the morphology
of a language—about what ought to be recalled. We
are not trying to learn something that we feel is al-
ready known.
</bodyText>
<subsectionHeader confidence="0.995734">
5.2 What’s learning?
</subsectionHeader>
<bodyText confidence="0.999989912280702">
It is worth considering the endeavor of learning mor-
phology in terms of formal learning theory, as pre-
sented in Osherson et al. (1986) or Kanazawa (1998)
for example. In the classical framework, the prob-
lem of learning a language from positive exam-
ple data is approached by considering the succes-
sive guesses at the target language that a purported
learner makes when presented with some sequen-
tially increasing learning sample drawn from that
language. Considering just morphology, it seems
that the target language is the set of all possible
words of the natural language at hand, a possibly
infinite (or at least indefinite) set. WWM’s output
is a list of generated words subsuming the corpus,
which are supposed to be all the words creatable by
applying its idea of morphology to that corpus. It
can thus be viewed as making a guess about the tar-
get language, given a certain learning sample. If the
learning sample is increased, its guess increases in
size also. The errors in precision of course mean
that at the current corpus sizes its guesses are for the
moment not even subsets of the target language.
According to one classic paradigm, a system
would be held to be a successful learner if it could
be proven to home in on the target language as the
learning sample increased in size indefinitely. This
is Gold’s (1967) criterion of identification in the
limit. In this framework, an empirical analysis can-
not be used to decide the adequacy of a learner, and
we would like to deemphasize the importance of the
empirical results for this purpose. That said, the em-
pirical results are for now all we have to show, but
eventually we hope to produce a mathematical proof
of just what WWM can learn, and just what kinds of
lexicons are learnable in Gold’s sense.
To our knowledge, it has never been proven
whether the total lexicon of a natural language is
identifiable in the limit from the sort of data we pro-
vide (i.e. POS-tagged words), using in particular the
theory of Whole Word Morphology in a perfect fash-
ion. Still, it is interesting that nothing about this lan-
guage learning paradigm says anything about mor-
phological analysis. The current crop of true mor-
phological learners, e.g. (Goldsmith, 2001b), en-
deavor to learn to analyze the morphology of the
language at hand in the manner of a linguist. Gold-
smith has even called his Linguistica system a “lin-
guist in a box.” This is perhaps an interesting and
worthwhile endeavor, but it is not one that is un-
dertaken here. WWM is instead attempting to learn
the target language in a more direct way from the
data, without first constructing the intermediary of
a traditional morphological analysis. We are thus
not learning the linguist’s notion of morphology but
rather the result of morphology, i.e. the word forms
of the language together with the other information
that goes into a word.2
</bodyText>
<subsectionHeader confidence="0.997759">
5.3 Post-hoc fixes and future developments
</subsectionHeader>
<bodyText confidence="0.986145157142857">
A significant proportion of errors in generation re-
sult from the application of competing ambiguous
morphological strategies. For example, when us-
ing the (French) text of Le petit prince as its base
lexicon, WWM produces two strategies relating 2nd
person verb forms to their infinitives. Given the verb
conjugues ‘conjugate,’ pres. 2nd sing., one strategy
produces the correct -er class infinitive conjuguer
while the other creates the non-word *conjuguere,
based on the relation among -re verb forms like
fais/faire ‘do’ and vends/vendre ‘sell.’ This is be-
cause of an inherent ambiguity among various word
pairs which do not fully indicate the paradigms of
which they are a part. WWM then adds to its lex-
icon, not only the correct form, but all the outputs
warranted by its grammar.
To try to correct this problem, a form of lexical
blocking has been implemented in the current ver-
sion of the program. WWM creates every possible
word, including different strategies giving the same
one, and lets lexical lookup take precedence over
productive morphology. The knowledge WWM pos-
sesses about its lexicon increases considerably dur-
ing the creation of morphological strategies. The
program learns not only which strategies are li-
censed by a given lexicon, but also which words
of its lexicon are related to one another. WWM
can assign a number to every lexical entry and give
the same “paradigm” number to related words. Be-
fore adding a newly created word to its lexicon, the
program looks for an existing word with the same
paradigm number and category. For example, if
WWM maps the word decoction, which was as-
signed to, say, paradigm 489 onto a strategy creating
plural nouns, it will look for a plural noun belonging
to paradigm 489 in its lexicon before it adds decoc-
tions to the list of new words.
Preliminary results are encouraging, with WWM
reaching up to 92% accuracy in generation after
2In this theory, a word’s form cannot be usefully divorced
from the other information that allows its proper use, and in our
implementation the POS tags (poor substitutes for what should
be a richer database of information) are crucial to the discovery
of the strategies.
the blocking modification. Obviously the program
needs to be systematically tested on multiple lexica
from different languages, but these results strongly
suggest that it is possible to model the acquisition
of morphology as a component of learning to gen-
erate language directly, rather than to treat computa-
tional learning as the acquisition of linguistic theory
as several current approaches do, e.g. (Goldsmith,
2001b).
Although the principles of whole word morphol-
ogy allow one to contemplate versions of WWM that
would work on templatic morphologies, polysyn-
thetic languages, and a host of other recalcitrant phe-
nomena, the current instantiation of the program is
not so ambitious. The comparison algorithm de-
tailed in the previous section compares words letter
by letter, either from left to right or from right to
left. No other possible alignments between words
are considered and WWM is in its current state only
capable of grasping prefixal and suffixal morphol-
ogy. We are currently developing a more sophis-
ticated sequence alignment routine which will al-
low the program to handle infixing, circumfixing,
and templatic morphologies of the Semitic type, as
well as word-internal changes typified by Germanic
strong verb ablaut.
</bodyText>
<sectionHeader confidence="0.996862" genericHeader="references">
References
</sectionHeader>
<subsectionHeader confidence="0.733011">
Adam Albright and Bruce Hayes. 2001a. An
</subsectionHeader>
<bodyText confidence="0.773417666666667">
automated learner for phonology and morphol-
ogy. http://www.linguistics.ucla.edu/people/hayes/
learning/index.htm.
</bodyText>
<reference confidence="0.997574236363636">
Adam Albright and Bruce Hayes. 2001b.
Burnt and splang: Some problems
of generality in phonological learning.
http://www.linguistics.ucla.edu/people/hayes/learning/
index.htm.
Stephen R. Anderson. 1992. A-Morphous Morphology.
Cambridge University Press.
Evan L. Antworth. 1990. PC-KIMMO: a two-level pro-
cessor for morphological analysis. Occasional Publi-
cations in Academic Computing 16, Summer Institute
of Linguistics, Dallas, TX.
Marco Baroni. 2000. An automated distribution-driven
prefix learner. Presented at the 9th International Mor-
phology Meeting, Vienna, Austria, February.
H. Bochner. 1993. Simplicity in Generative Morphology.
Mouton de Gruyter, Berlin.
Michael Brent. 1993. Minimal generative models: A
middle ground between neurons and triggers. In Pro-
ceedings of the 15th Annual Conference of the Cogni-
tive Science Society, pages 28–36. Lawrence Erlbaum
Associates.
Bob Carpenter. 1992. The Logic of Typed Feature Struc-
tures, volume 32 of Cambridge Tracts in Theoretical
Computer Science. Cambridge University Press.
Noam Chomsky. 1955. The logical structure of linguistic
theory. Unpublished manuscript. Published as a book
with a new introduction in 1975 by Plenum Press.
Carl de Marcken. 1995. Unsupervised Language Acqui-
sition. Ph.D. thesis, MIT.
Saˇso Dˇzeroski and Tomaˇz Erjavec. 1997. Induction
of Slovene nominal paradigms. In Nada Lavrac and
Saˇso Dˇzeroski, editors, Inductive Logic Programming,
7th International Workshop, volume 1297 of Lecture
Notes in Computer Science. Springer.
Gudrun Flenner. 1994. Ein quantitatives Morphsegmen-
tierungssystem f¨ur spanische Wortformen. In Ursula
Klenk, editor, Computatio Linguae II, pages 31–62.
Steiner Verlag, Stuttgart.
Gudrun Flenner. 1995. Quantitative Morphseg-
mentierung im Spanischen auf phonologisher Basis.
Sprache und Datenverarbeitung, 19(2):63–79.
A. Ford and R. Singh. 1991. Prop´edeutique mor-
phologique. Folia Linguistica, 25(3–4):549–575.
A. Ford, R. Singh, and G. Martohardjono. 1997. Pace
Panini. Peter Lang, New York.
E. M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447–474.
John A. Goldsmith. 2001a. Linguistica: An automatic
morphological analyzer. In Arika Okrent and John
Boyle, editors, CLS 36: The Main Session, volume 36-
1. Chicago Linguistic Society, Chicago.
John A. Goldsmith. 2001b. Unsupervised learning of
the morphology of a natural language. Computational
Linguistics, 27(2):153–198.
M. A. Hafer and S. F. Weiss. 1974. Word segmentation
by letter successsor varieties. Information Storage and
Retrieval, 10(371–385).
Zellig Harris. 1955. From phoneme to morpheme. Lan-
guage, 31:190–222.
Zellig Harris. 1967. Morpheme boundaries within
words: Report on a computer test. In Transformations
and Discourse Analysis Papers, volume 73.
Charles Hockett. 1954. Two models of grammatical de-
scription. Word, 10:210–231.
Axel Janssen. 1992. Segmentierung franz¨osischer Wort-
formen in Morphe ohne Verwendung eines Lexikons.
In Ursula Klenk, editor, Computatio Linguae, pages
74–95. Steiner Verlag, Stuttgart.
Makoto Kanazawa. 1998. Learnable Classes of Catego-
rial Grammars. Studies in Logic, Language and Infor-
mation. CSLI Publications and the European Associa-
tion for Logic, Language and Information.
Lauri Karttunen, Kimmo Koskenniemi, and Ronald M.
Kaplan. 1987. A compiler for two-level phonological
rules. Technical Report CSLI-87-108, Center for the
Study of Language and Information, Palo Alto.
Lauri Karttunen, Ronald M. Kaplan, and Annie Zaenen.
1992. Two-level morphology with composition. In
Proceedings of the 15th International Conference on
Computational Linguistics, volume I, pages 141–148,
Nantes, France.
Lauri Karttunen. 1993. Finite state constraints. In
John A. Goldsmith, editor, The Last Phonological
Rule, pages 173–194. University of Chicago Press.
Lauri Karttunen. 1994. Constructing lexical transducers.
In Proceedings of the 15th International Conference
on Computational Linguistics, volume I, pages 406–
411.
Dimitar Kazakov and Suresh Manandhar. 1998. A hy-
brid approach to word segmentation. In David Page,
editor, Proceedings of Inductive Logic Programming-
98, volume 1446 of Lecture Notes in Computer Sci-
ence. Springer.
Kimmo Koskenniemi. 1983. Two-level morphology: a
general computational model for word-form recogni-
tion and production. Technical Report 11, Dept. of
General Linguistics, University of Helsinki.
S. Neuvel and R. Singh. In press. Vive la diff´erence!
What morphology is about. Folia Linguistica.
Eugene Nida. 1949. Morphology. The descriptive anal-
ysis of words. University of Michigan Press, Ann Ar-
bor, MI.
Daniel N. Osherson, Michael Stob, and Scott Weinstein.
1986. Systems that Learn. The MIT Press, Cam-
bridge, MA.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
free induction of inflectional morphologies. In 2nd
Meeting of the North American Chapter of the ACL,
pages 183–191. Association for Computational Lin-
guistics, Morgan Kaufman.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003220">
<note confidence="0.946354333333333">Morphological and Phonological Learning: Proceedings of the 6th Workshop of the ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia, July 2002, pp. 31-40. Association for Computational Linguistics.</note>
<title confidence="0.975074">Unsupervised Learning of Morphology Without Morphemes</title>
<author confidence="0.998246">Sylvain Neuvel Sean A Fulop</author>
<affiliation confidence="0.9855945">Dept. of Linguistics Depts. of Linguistics sneuvel@uchicago.edu and Computer Science</affiliation>
<email confidence="0.991404">sfulop@uchicago.edu</email>
<affiliation confidence="0.973074">The University of Chicago</affiliation>
<abstract confidence="0.99479614774282">The first morphological learner based upon the theory of Whole Word Morphology (Ford et al., 1997) is outlined, and preliminary evaluation results are presented. The program, Whole Word Morphologizer, takes a POS-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes, and is then able to generate new words beyond the learning sample. The accuracy (precision) of the generated new words is as high as 80% using the pure Whole Word theory, and 92% after a post-hoc adjustment is added to the routine. The aim of this project is to develop a computamodel employing the theory of word et al., 1997) capable on the one hand of identifying morphological relations within a list of words from any one of a wide variety of languages and, on the other, of putting that knowledge to use in creating previously unseen word forms. A small application called Whole Word Morphologizer which does just this is outlined and discussed. In particular, this approach is set against the literature on computational morphology as an entirely different way of doing things which has the potential to be generalized to all known varieties of morphology in the world’s languages, a feature not shared by previous methods. As it is based on a model of the mental lexicon in which all entries are entire, fully fledged words, this project also serves as an empirical demonstration that a word-based morphological theory that rejects the notion of morpheme as minimal unit of form and meaning (and/or grammatical properties) is viable from the point of view of acquisition as well as generation. 1 Morphological learning Since its inception in the mid 1950s, the field of computational morphology has been characterized by a paucity of procedures for generation. Notwithstanding the impressive body of literature on the shortcomings of traditional Paninian morphology, most computational research projects also rely on a traditional notion of the morpheme and ignore all non-compositional aspects of morphology. These observations are obviously not unrelated and are in part inherited from the field of computational syntax where applications traditionally were designed to assign a syntactic structure to a given string of words, though this is less true today. 1.1 Segmentation and morpheme identification Word formation and the population of the lexicon, while central to morphological theory, are noticeably absent from the field of computational morphology. Most computational work in the field of morphology has focused on the identification of morphemes or morphological parsing while paying little or no attention to generation. While these applications find a common goal in the automatic acquisition of morphology, it is helpful to distinguish between two types of analysis in light of the often very different results sought by various morphological learners. On the one hand, some applications focus exon the words or longer strings into smaller units. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphoapplications are designed solely to on a training corpus and not to provide a morphological analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the algorithm cannot double as a morphological parser. More recently, efforts have been developing identify morphemes some sort of analysis. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Their procedure uses a morpheme-based model, provides an analysis of the words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by de Marcken’s (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. Also, Baroni (2000) aims at finding a set of prefixes from a corpus, together with an affix-stem parse of each of the words. While they might differ in their methods or objectives, all of the above morphological applications share a common characteristic in that they are learners designed exclusively for the acquisition of morcorpora and do not generate new words based on the information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able to generate from acquired knowledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications require a set of man-made lexical rules to function. While the practical uses of such applications as PC-Kimmo are incontestable, it is clear that they are part of a different endeavour, and should not be confused with genuine morphological learners. The other relevant group of computational applications can, as mentioned, both acquire morphological knowledge from corpora and generate new words based on that knowledge. Albright and Hayes (2001a; 2001b) tackle the wider task of acquiring morphology and (morpho)phonology based on a small paradigm list and their learner is able to generate particular inflected forms given a related word. Dˇzeroski and Erjavec (1997) work towards learning morphological rules for forming particular inflectional forms given a lemma (a set of related words). Their learner produces a set of rules relating all the members of a paradigm to a base form. The program can then produce a member of that paradigm on command given the base form. While the methods used by Albright and Hayes and Dˇzeroski and Erjavec radically differ, both use a form of supervised learning which significantly reduces the amount of information their learner has to acquire. Albright and Hayes train their program using a paradigm list in which each entry contains, for example, both the present and past tense forms of an English verb. Similarly, the training data used by Dˇzeroski and Erjavec similarly has a base form, or lexeme, associated to each and every word so that all the words of a given paradigm share a common label. The distinctions between the two methods are immaterial, what matters is that both learners are being told which words are related to which and are left with the task of describing that relation in the form a rule. In other words, the algorithms they use cannot discover that words are morphologically related. 1.3 What’s morphology? In the above algorithms, the task of determining whether one word is related to another in a morphological sense is most frequently left to the linguist, as this information has to be encoded in the training data for these algorithms. (Some of the most recent work such as (Schone and Jurafsky, 2001) and (Goldsmith, 2001b) are notable exceptions to this paradigm.) This is perhaps not surprising, since no serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed what have been referred to as discovery procedures (v. (Nida, 1949), for example) for the identification of morphemes but since the mid fifties (Chomsky, 1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate it to studies on language acquisition. But, since a morphological learner like that presented here is designed to model the acquisition of morphology, it seems that it should above all be able to determine itself two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a panoply of recent publications devoted to the empirical shortcomings of traditional so-called “Itemand-Arrangement” morphology (Hockett, 1954; Bochner, 1993; Ford and Singh, 1991; Anderson, 1992; Ford et al., 1997), and the list of phenomena that fall out of reach of a compositional approach is rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial suppletion, etc. Still, seemingly every documented morphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-and- Arrangement analysis; this description applies to all of the computational papers cited above. 2 An alternative theory Whole Word Morphologizer (henceforth WWM) is the first implementation of the theory of Whole Word Morphology. The theory, developed by Alan Ford and Rajendra Singh at Universit´e de Montr´eal, seeks to account for morphological relations in a minimalist fashion. Ford and Singh published a series of papers dealing with various aspects of the theory between 1983 and 1990. Drawing on these papers, they published a full outline of it in 1991 (Ford and Singh, 1991) and an even fuller defense of it in 1997 (Ford et al., 1997). Since then, aspects of it have been taken up in a series of publications by Agnihotri, Dasgupta, Ford, Neuvel, Singh, and various combinations of these authors. The central mechanism of the theory, the Word Formation Strategy (WFS), is a sort of non-decomposable morphological transformation that relates full words with full words (or helps one fashion a full word from another full word) and parses any complex word into a variable and a non-variable component. Neuvel and Singh (In press) offer a strict definition of morphological relatedness and, based on this definition, suggest guidelines for the acquisition of Word Formation Strategies. In Whole-Word Morphology, any morphological relation can be represented by a rule of the following form: ↔ in which the following conditions and notations are employed: 1. and statements that words of the are possible in the language, are abbreviations of the forms of of words belonging to categories which specific words belonging to the right category can be unified in form); 2. 0represents all the form-related differences be- α categories that may be represented as feature-bundles; ↔ a bi-directional implication; 5. and semantically related. There are several ramifications of (1). First, there is only one morphology; no distinction, other than a functional one, is made between inflection and derivation. Second, morphology is relational and not compositional. The program thus makes no reference to theoretical constructs such as ‘root’, ‘stem’, and ‘morpheme’, or devices such as ‘levels’ and ‘strata’ and relies exclusively on the notion of morphological relatedness. And since its objective is not to assign a probability to a given word or string, it must rely on a strict formal definition of a morphological relation. Ultimately, the theory takes the Saussurean view that words are defined by the differences amongst them and argues that some of these differences, namely those that are found between two or more pairs of words, constitute the domain of morphology. In other words, two words of a lexicon are morphologically related if and only if all the differences between them are found in at least one other pair of words of the same lexicon. 3 Overview of the method Under the assumption that the morphology of a language resides exclusively in differences that are exploited in more than one pair of words within its lexicon, WWM (Algorithm 1 in the next section) compares every word of a small lexicon and determines the segmental differences found between them. The input to the current version of the program is a small text file that contains anywhere from 1000 to 5000 words. Each word appears in orthographic form and is followed by its syntactic and morphological categories, as in the example below: (2) cat, Ns (Noun, singular) catch, V catches, V3s (Verb, (pres.) 3rd pers. sing.) decided, Vp (Verb, past) The algorithm simply compares each letter from word A to the corresponding one from word B to produce a comparison record, which can be viewed as a data structure. Currently, it works on orthographic representations. This means it would as easily work on phonemic transcriptions, but it will require empirical evaluation to see whether the results from these can improve upon those obtained using spellings, and we have not yet gone through such an exercise. It starts on either the left or right edge of the words if the two words share their first (few) segments or their last (few) segments, respectively (the forward version is presented in Algorithm 2 in the next section). This is just a simple-minded way of aligning the similar parts of the words for the comparison; a more sophisticated implementation in the future could use a more general sequence alignment procedure. The segments are placed in one of two lists in the comparison structure (differences or similarities) based on whether or not they are identical. Each comparison structure also contains the categories of both words, and is kept in a large list of all comparison structures found from analyzing the entire corpus. The example below shows the information in the comparison structure produced from the words includes the differences and similarities between the two words, from the perspective of each word in turn, as well as the lexical categories of the words. (3) Differences First word Second word Similarities First Second rece### rece##### Matching character sequences in the difference section are replaced with a variable. The result is then set against comparisons generated by other pairs of words and duplicate differences are recognized. In the example below, the comparproduced by the pairs conshown. (4) Differences First word Second word Similarities First Second rece### rece##### conce### conce##### dece### dece##### three comparisons in (4) share the same forand grammatical and so the theory indicates they should be merged into one morphological strategy. Since the differences are the same, it is only the similarities that are actually merged. Each new morphological strategy is also restricted to apply in as narrow an environment as possible. Neuvel and Singh (Neuvel and Singh, In press) suggest that any morphological strategy must be maximally restricted at all times; this is accomplished by specifying as constant all the similarities found, not between words, but between the similarities found between words. In (4), all three sets of similarities end with the sequence of letters “ce.” These similarities between similarities are specified as constant in each strategy and the length of each word is also facin. The called in Algorithm 2 carries out this procedure; we don’t show it because it is tedious but not especially interesting. The restricted morphological strategy relating the words in (4) is as follows: (5) Differences First word Second word Similarities First Second For the sake of clarity, we can represent the information contained in (5) in a more familiar fashion using the formalism described in (1). The vertical ‘ are used for orthographic forms so as not to confuse them with phonemic representations. (6) VH The ‘#’ signs in the above representations stand for letters that must be instantiated but are not specthe symbol stands for a letter that is not specified and that may or may not be instantiated. Strategy (6) can therefore be interpreted as follows: there is a verb that ends with the sequence “ceive” preceded by no less than two and no more than three characters, there should also be a singular noun that ends with the sequence “ception” preceded by the same two or three characters. After performing the comparisons and merging, WWM extracts a list of morphological strategies, which are those comparison structures whose count is more than some fixed threshold. Table 1 contains a few strategies found from the first few of Dick. strategies result from merging comparison structures which have the same differences—merging the similarities of several unifiable word pairs, and so many have no specified letters at all. WWM then goes through the lexicon word by word and attempts to unify each word in form and category with the left or right side of this strategy. If it succeeds, WWM replaces all the segments fully specified on the side of the strategy the word is unified with, with the segments fully specified on the side. For example, given the noun in the corpus and strategy (6), WWM will map the word onto the right hand side of (6), take out the sequence “ception” from the end and replace it with sequence “ceive” to produce the new word percategory of the word will also be changed from singular noun to verb. New words can thus be generated in a rather obvious fashion by taking each word in the original lexicon and applying any strategies that can be applied, i.e. whose orthographic form and part of speech can be unified with the word at hand. Algorithm 3 shows the basic generation once again the routines called implement the nitty-gritty details of the above description are not given because they are more tedious than interesting, and will certainly need to be changed in more general future versions of WWM. Table 2 gives some of the new WWM creates using text from petit prince as its base lexicon. 1: Word-formation strategies discovered from Dick Differences Similarities Examples 1st word 2nd word 1st word 2nd word baked/bake, charged/charge directed/direct helmets/helmet, rabbits/rabbit walking/walked, talking/talked walking/walks, talking/talks short/shortness easy/easily, quick/quickly hardest/hard, shortest/short jumps/jump, plays/play harder/hard, louder/loud painless/pain, childless/child raining/rainy, running/runny played/plays paintings/paint 2: Words generated from petit prince drames Np droitement ADV dress´ee PF drˆoles AIP dresser INF drˆolement ADV dressa Vp3 dunes Np dressais Vi2 durerait Vc3 dresse V3 d´ecid´ee PF dressent V6 d´ecider INF dressez V5 d´ecida Vp3 dressait Vi3 d´ecide V3 droits AMP d´ecoiff´e AM droites AFP d´econcentr´es AMP output from the algorithm is a list of much as in Table 2, which are generated from the input corpus using the morphological relations (strategies) discovered. The method described above will clearly force WWM to create words that were already part of its original lexicon; in fact, each and every word involved in licensing the discovery of a morphological strategy will be duplicated by the Generated words that were of original lexicon are then added to a sepamean an orthographic form together with the part of speech. Further work in this vein would add meanings as well. rate word list containing only new words. If desired, this new word list can be merged with the original lexicon for another round of discovery to formulate new strategies based on a larger dataset. Additionally, each of the new words can simply be put through another cycle of word creation by applying the same strategies as before a second time. 4 Implementation This section contains some pseudocode showing several basic components of the Whole Word Morphologizer. Algorithm 1 shows the main procedure, which takes a POS-tagged lexicon as input and outputs a list of all words that are possible given the morphological relations present in the lexicon. two procedures compsymmetrical, so Algorithm 2 shows just the first of these. This algorithm provides the data structure which includes the differences and similarities between each pair of words in the lexicon, in similar fashion to the examples in the preceding section. In practice, only those pairs of words which are by some heuristic sufficiently similar in the first place are compared. Additionally, the two similarities sequences for each word pair are actually represented as one sequence which encodes the information found in the two sequences of the examples in the preceding; this is just for convenience of 1 be a list of POS-tagged words. list generated all words all words a beginning sequence then if an ending sequence then end if end for end for all in the list the list strategies end if end for storage and computation. Algorithm 3 shows the outline of the final stage, which generates an output list of words from the input lexicon and the morphological strategies. The strategy list is simply a list of all comparison structures that occurred more frequently than some arbitrary threshold number. 5 Accomplishments and prospects 5.1 Initial results Whole Word Morphologizer has been tested on a limited basis using English and French lexicons of approximately 3000 entries, garnered from the POStagged versions of Le petit prince and Moby Dick. The program initially, without any post-hoc corrections, achieved between 70% and 82% accuracy in generation; these figures measure the percentage of the new words beyond the original lexicon that are possible words of the language. The figures thus a kind of in terms of the precision/recall tradeoff, and are fair values in that they do not include the generated words that are already in the lexicon. 2 be (word, category) pairs. data structure documentthe different and similar letters between merged into the global list of coma structure of 5 lists sim. to = list sim does not end with ‘X’ ‘X’ to both lists and else append ‘#’ to sim end if end if end for + to end for lists and categories match a comparison alin the list and increment else end if satisfactory seems impossible to think of in its usual sense here. First of all, there are generally an indefinite number of possible words in a language. One therefore cannot give a precise set of words that we wish the system could generate from a specific lexicon, so there seems to be no way to measure the percentage of “desired words” that are in fact generated. Even if we were to make such a list by hand from the current small corpora to use as a gold standard (which has been suggested by a referee), it must also be remembered that WWM discovers strategies (morphological relations) for creating new words from given ones. It cannot be expected to discover strategies that are not evident in a Indeed, WWM will that, for example, ‘am’ and ‘be’ are related, because according to the theory of morphology being applied these 3 list generated using lexall in all says the word and strategy match with either or right alignment is not in the lexicon or the list newword to end if end if end for end for words are only related by convention, not by morphology. “Nonproductive morphology” is not really morphology. The real point is that we do not want to hold WWM’s performance up against our own ideas about morphological relations among words, since it would be practically impossible to determine not a large set of possible words that think are related to those in the corpus, but rather a of possible words that WWM to generate according to its theory. This would amount to trying to beat WWM at its own game in pursuit of a gold standard, which could only be obtained using a better implementation of WWM’s theory. A perfect implementation of Whole Word Morphology would have perfect recall, in view of our eventual goal of using this theory to inform us about the morphology of a language—about what ought to be recalled. We are not trying to learn something that we feel is already known. 5.2 What’s learning? It is worth considering the endeavor of learning morphology in terms of formal learning theory, as presented in Osherson et al. (1986) or Kanazawa (1998) for example. In the classical framework, the problem of learning a language from positive example data is approached by considering the successive guesses at the target language that a purported learner makes when presented with some sequentially increasing learning sample drawn from that language. Considering just morphology, it seems that the target language is the set of all possible words of the natural language at hand, a possibly infinite (or at least indefinite) set. WWM’s output is a list of generated words subsuming the corpus, which are supposed to be all the words creatable by applying its idea of morphology to that corpus. It can thus be viewed as making a guess about the target language, given a certain learning sample. If the learning sample is increased, its guess increases in size also. The errors in precision of course mean that at the current corpus sizes its guesses are for the moment not even subsets of the target language. According to one classic paradigm, a system would be held to be a successful learner if it could be proven to home in on the target language as the learning sample increased in size indefinitely. This Gold’s (1967) criterion of in the this framework, an empirical analysis cannot be used to decide the adequacy of a learner, and we would like to deemphasize the importance of the empirical results for this purpose. That said, the empirical results are for now all we have to show, but eventually we hope to produce a mathematical proof of just what WWM can learn, and just what kinds of lexicons are learnable in Gold’s sense. To our knowledge, it has never been proven whether the total lexicon of a natural language is identifiable in the limit from the sort of data we provide (i.e. POS-tagged words), using in particular the theory of Whole Word Morphology in a perfect fashion. Still, it is interesting that nothing about this language learning paradigm says anything about morphological analysis. The current crop of true morphological learners, e.g. (Goldsmith, 2001b), endeavor to learn to analyze the morphology of the language at hand in the manner of a linguist. Goldsmith has even called his Linguistica system a “linguist in a box.” This is perhaps an interesting and worthwhile endeavor, but it is not one that is undertaken here. WWM is instead attempting to learn the target language in a more direct way from the data, without first constructing the intermediary of a traditional morphological analysis. We are thus not learning the linguist’s notion of morphology but the morphology, i.e. the word forms of the language together with the other information goes into a 5.3 Post-hoc fixes and future developments A significant proportion of errors in generation result from the application of competing ambiguous morphological strategies. For example, when usthe (French) text of petit prince its base lexicon, WWM produces two strategies relating 2nd person verb forms to their infinitives. Given the verb pres. 2nd sing., one strategy the correct infinitive the other creates the non-word on the relation among forms like and This is because of an inherent ambiguity among various word pairs which do not fully indicate the paradigms of which they are a part. WWM then adds to its lexicon, not only the correct form, but all the outputs warranted by its grammar. To try to correct this problem, a form of lexical blocking has been implemented in the current version of the program. WWM creates every possible word, including different strategies giving the same one, and lets lexical lookup take precedence over productive morphology. The knowledge WWM possesses about its lexicon increases considerably during the creation of morphological strategies. The program learns not only which strategies are licensed by a given lexicon, but also which words of its lexicon are related to one another. WWM can assign a number to every lexical entry and give the same “paradigm” number to related words. Before adding a newly created word to its lexicon, the program looks for an existing word with the same paradigm number and category. For example, if maps the word was assigned to, say, paradigm 489 onto a strategy creating plural nouns, it will look for a plural noun belonging paradigm 489 in its lexicon before it adds decocthe list of new words. Preliminary results are encouraging, with WWM reaching up to 92% accuracy in generation after this theory, a word’s form cannot be usefully divorced from the other information that allows its proper use, and in our implementation the POS tags (poor substitutes for what should be a richer database of information) are crucial to the discovery of the strategies. the blocking modification. Obviously the program needs to be systematically tested on multiple lexica from different languages, but these results strongly suggest that it is possible to model the acquisition of morphology as a component of learning to generate language directly, rather than to treat computational learning as the acquisition of linguistic theory as several current approaches do, e.g. (Goldsmith, 2001b). Although the principles of whole word morphology allow one to contemplate versions of WWM that would work on templatic morphologies, polysynthetic languages, and a host of other recalcitrant phenomena, the current instantiation of the program is not so ambitious. The comparison algorithm detailed in the previous section compares words letter by letter, either from left to right or from right to left. No other possible alignments between words are considered and WWM is in its current state only capable of grasping prefixal and suffixal morphology. We are currently developing a more sophisticated sequence alignment routine which will allow the program to handle infixing, circumfixing, and templatic morphologies of the Semitic type, as well as word-internal changes typified by Germanic strong verb ablaut.</abstract>
<title confidence="0.683409">References</title>
<author confidence="0.694096">a An</author>
<abstract confidence="0.945523333333333">automated learner for phonology and morphology. http://www.linguistics.ucla.edu/people/hayes/ learning/index.htm. Adam Albright and Bruce Hayes. 2001b. Some problems of generality in phonological learning.</abstract>
<web confidence="0.976628">http://www.linguistics.ucla.edu/people/hayes/learning/</web>
<note confidence="0.821044909090909">index.htm. R. Anderson. 1992. Cambridge University Press. Evan L. Antworth. 1990. PC-KIMMO: a two-level processor for morphological analysis. Occasional Publications in Academic Computing 16, Summer Institute of Linguistics, Dallas, TX. Marco Baroni. 2000. An automated distribution-driven prefix learner. Presented at the 9th International Morphology Meeting, Vienna, Austria, February. Bochner. 1993. in Generative Mouton de Gruyter, Berlin. Michael Brent. 1993. Minimal generative models: A ground between neurons and triggers. In Proceedings of the 15th Annual Conference of the Cogni- Science pages 28–36. Lawrence Erlbaum Associates. Carpenter. 1992. Logic of Typed Feature Strucvolume 32 of Tracts in Theoretical Cambridge University Press. Noam Chomsky. 1955. The logical structure of linguistic theory. Unpublished manuscript. Published as a book</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Albright</author>
<author>Bruce Hayes</author>
</authors>
<title>Burnt and splang: Some problems of generality in phonological learning.</title>
<date>2001</date>
<note>http://www.linguistics.ucla.edu/people/hayes/learning/ index.htm.</note>
<contexts>
<context position="7246" citStr="Albright and Hayes (2001" startWordPosition="1142" endWordPosition="1145">87)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications require a set of man-made lexical rules to function. While the practical uses of such applications as PC-Kimmo are incontestable, it is clear that they are part of a different endeavour, and should not be confused with genuine morphological learners. The other relevant group of computational applications can, as mentioned, both acquire morphological knowledge from corpora and generate new words based on that knowledge. Albright and Hayes (2001a; 2001b) tackle the wider task of acquiring morphology and (morpho)phonology based on a small paradigm list and their learner is able to generate particular inflected forms given a related word. Dˇzeroski and Erjavec (1997) work towards learning morphological rules for forming particular inflectional forms given a lemma (a set of related words). Their learner produces a set of rules relating all the members of a paradigm to a base form. The program can then produce a member of that paradigm on command given the base form. While the methods used by Albright and Hayes and Dˇzeroski and Erjavec </context>
</contexts>
<marker>Albright, Hayes, 2001</marker>
<rawString>Adam Albright and Bruce Hayes. 2001b. Burnt and splang: Some problems of generality in phonological learning. http://www.linguistics.ucla.edu/people/hayes/learning/ index.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen R Anderson</author>
</authors>
<title>A-Morphous Morphology.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="10253" citStr="Anderson, 1992" startWordPosition="1646" endWordPosition="1647">logy, it seems that it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a panoply of recent publications devoted to the empirical shortcomings of traditional so-called “Itemand-Arrangement” morphology (Hockett, 1954; Bochner, 1993; Ford and Singh, 1991; Anderson, 1992; Ford et al., 1997), and the list of phenomena that fall out of reach of a compositional approach is rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial suppletion, etc. Still, seemingly every documented morphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-andArrangement analysis; this description applies to all of the computational papers cited above. 2 An alternative theory Whole Word Morphologizer (henceforth WWM) is the first implementation of the theory of Whole Word Morphology. The theory, developed b</context>
</contexts>
<marker>Anderson, 1992</marker>
<rawString>Stephen R. Anderson. 1992. A-Morphous Morphology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan L Antworth</author>
</authors>
<title>PC-KIMMO: a two-level processor for morphological analysis.</title>
<date>1990</date>
<booktitle>Occasional Publications in Academic Computing 16, Summer Institute of Linguistics,</booktitle>
<location>Dallas, TX.</location>
<contexts>
<context position="6180" citStr="Antworth, 1990" startWordPosition="973" endWordPosition="974">s from corpora and do not generate new words based on the information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able to generate from acquired knowledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, </context>
</contexts>
<marker>Antworth, 1990</marker>
<rawString>Evan L. Antworth. 1990. PC-KIMMO: a two-level processor for morphological analysis. Occasional Publications in Academic Computing 16, Summer Institute of Linguistics, Dallas, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
</authors>
<title>An automated distribution-driven prefix learner.</title>
<date>2000</date>
<booktitle>Presented at the 9th International Morphology Meeting,</booktitle>
<location>Vienna, Austria,</location>
<contexts>
<context position="5242" citStr="Baroni (2000)" startWordPosition="819" endWordPosition="821">More recently, efforts have been developing which identify morphemes and perform some sort of analysis. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Their procedure uses a morpheme-based model, provides an analysis of the words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by de Marcken’s (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. Also, Baroni (2000) aims at finding a set of prefixes from a corpus, together with an affix-stem parse of each of the words. While they might differ in their methods or objectives, all of the above morphological applications share a common characteristic in that they are learners designed exclusively for the acquisition of morphological facts from corpora and do not generate new words based on the information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between the</context>
</contexts>
<marker>Baroni, 2000</marker>
<rawString>Marco Baroni. 2000. An automated distribution-driven prefix learner. Presented at the 9th International Morphology Meeting, Vienna, Austria, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bochner</author>
</authors>
<title>Simplicity in Generative Morphology. Mouton de Gruyter,</title>
<date>1993</date>
<location>Berlin.</location>
<contexts>
<context position="10215" citStr="Bochner, 1993" startWordPosition="1640" endWordPosition="1641">ed to model the acquisition of morphology, it seems that it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a panoply of recent publications devoted to the empirical shortcomings of traditional so-called “Itemand-Arrangement” morphology (Hockett, 1954; Bochner, 1993; Ford and Singh, 1991; Anderson, 1992; Ford et al., 1997), and the list of phenomena that fall out of reach of a compositional approach is rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial suppletion, etc. Still, seemingly every documented morphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-andArrangement analysis; this description applies to all of the computational papers cited above. 2 An alternative theory Whole Word Morphologizer (henceforth WWM) is the first implementation of the theory of Whole Wo</context>
</contexts>
<marker>Bochner, 1993</marker>
<rawString>H. Bochner. 1993. Simplicity in Generative Morphology. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>Minimal generative models: A middle ground between neurons and triggers.</title>
<date>1993</date>
<booktitle>In Proceedings of the 15th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="4498" citStr="Brent (1993)" startWordPosition="702" endWordPosition="703">ther development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphological applications are designed solely to identify morphemes based on a training corpus and not to provide a morphological analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the algorithm cannot double as a morphological parser. More recently, efforts have been developing which identify morphemes and perform some sort of analysis. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Their procedure uses a morpheme-based model, provides an analysis of the words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by de Marcken’s (1995) </context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Michael Brent. 1993. Minimal generative models: A middle ground between neurons and triggers. In Proceedings of the 15th Annual Conference of the Cognitive Science Society, pages 28–36. Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures,</title>
<date>1992</date>
<volume>32</volume>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="6571" citStr="Carpenter, 1992" startWordPosition="1041" endWordPosition="1042">owledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications require a set of man-made lexical rules to function. While the practical uses of such applications as PC-Kimmo are incontestable, it is clear that they are part of a different endeavour, and should not be confused with genuine morphological learners. The other relevant group of computational applications can, as mentioned, both acquire morphological knowledge from corpo</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. The Logic of Typed Feature Structures, volume 32 of Cambridge Tracts in Theoretical Computer Science. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>The logical structure of linguistic theory. Unpublished manuscript. Published as a book with a new introduction in</title>
<date>1955</date>
<publisher>by Plenum Press.</publisher>
<contexts>
<context position="9396" citStr="Chomsky, 1955" startWordPosition="1509" endWordPosition="1510">al sense is most frequently left to the linguist, as this information has to be encoded in the training data for these algorithms. (Some of the most recent work such as (Schone and Jurafsky, 2001) and (Goldsmith, 2001b) are notable exceptions to this paradigm.) This is perhaps not surprising, since no serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed what have been referred to as discovery procedures (v. (Nida, 1949), for example) for the identification of morphemes but since the mid fifties (Chomsky, 1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate it to studies on language acquisition. But, since a morphological learner like that presented here is designed to model the acquisition of morphology, it seems that it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and</context>
</contexts>
<marker>Chomsky, 1955</marker>
<rawString>Noam Chomsky. 1955. The logical structure of linguistic theory. Unpublished manuscript. Published as a book with a new introduction in 1975 by Plenum Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>Unsupervised Language Acquisition.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>MIT.</institution>
<marker>de Marcken, 1995</marker>
<rawString>Carl de Marcken. 1995. Unsupervised Language Acquisition. Ph.D. thesis, MIT.</rawString>
</citation>
<citation valid="true">
<title>Saˇso Dˇzeroski and Tomaˇz Erjavec.</title>
<date>1997</date>
<booktitle>In Nada Lavrac and Saˇso Dˇzeroski, editors, Inductive Logic Programming, 7th International Workshop,</booktitle>
<volume>1297</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="7470" citStr="(1997)" startWordPosition="1181" endWordPosition="1181">actical uses of such applications as PC-Kimmo are incontestable, it is clear that they are part of a different endeavour, and should not be confused with genuine morphological learners. The other relevant group of computational applications can, as mentioned, both acquire morphological knowledge from corpora and generate new words based on that knowledge. Albright and Hayes (2001a; 2001b) tackle the wider task of acquiring morphology and (morpho)phonology based on a small paradigm list and their learner is able to generate particular inflected forms given a related word. Dˇzeroski and Erjavec (1997) work towards learning morphological rules for forming particular inflectional forms given a lemma (a set of related words). Their learner produces a set of rules relating all the members of a paradigm to a base form. The program can then produce a member of that paradigm on command given the base form. While the methods used by Albright and Hayes and Dˇzeroski and Erjavec radically differ, both use a form of supervised learning which significantly reduces the amount of information their learner has to acquire. Albright and Hayes train their program using a paradigm list in which each entry co</context>
</contexts>
<marker>1997</marker>
<rawString>Saˇso Dˇzeroski and Tomaˇz Erjavec. 1997. Induction of Slovene nominal paradigms. In Nada Lavrac and Saˇso Dˇzeroski, editors, Inductive Logic Programming, 7th International Workshop, volume 1297 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gudrun Flenner</author>
</authors>
<title>Ein quantitatives Morphsegmentierungssystem f¨ur spanische Wortformen.</title>
<date>1994</date>
<booktitle>Computatio Linguae II,</booktitle>
<pages>31--62</pages>
<editor>In Ursula Klenk, editor,</editor>
<publisher>Steiner Verlag,</publisher>
<location>Stuttgart.</location>
<contexts>
<context position="3955" citStr="Flenner (1994" startWordPosition="618" endWordPosition="619">ught by various morphological learners. On the one hand, some applications focus exclusively on the segmentation of words or longer strings into smaller units. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphological applications are designed solely to identify morphemes based on a training corpus and not to provide a morphological analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes </context>
</contexts>
<marker>Flenner, 1994</marker>
<rawString>Gudrun Flenner. 1994. Ein quantitatives Morphsegmentierungssystem f¨ur spanische Wortformen. In Ursula Klenk, editor, Computatio Linguae II, pages 31–62. Steiner Verlag, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gudrun Flenner</author>
</authors>
<title>Quantitative Morphsegmentierung im Spanischen auf phonologisher Basis. Sprache und Datenverarbeitung,</title>
<date>1995</date>
<marker>Flenner, 1995</marker>
<rawString>Gudrun Flenner. 1995. Quantitative Morphsegmentierung im Spanischen auf phonologisher Basis. Sprache und Datenverarbeitung, 19(2):63–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ford</author>
<author>R Singh</author>
</authors>
<date>1991</date>
<booktitle>Prop´edeutique morphologique. Folia Linguistica,</booktitle>
<pages>25--3</pages>
<contexts>
<context position="10237" citStr="Ford and Singh, 1991" startWordPosition="1642" endWordPosition="1645"> acquisition of morphology, it seems that it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a panoply of recent publications devoted to the empirical shortcomings of traditional so-called “Itemand-Arrangement” morphology (Hockett, 1954; Bochner, 1993; Ford and Singh, 1991; Anderson, 1992; Ford et al., 1997), and the list of phenomena that fall out of reach of a compositional approach is rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial suppletion, etc. Still, seemingly every documented morphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-andArrangement analysis; this description applies to all of the computational papers cited above. 2 An alternative theory Whole Word Morphologizer (henceforth WWM) is the first implementation of the theory of Whole Word Morphology. The the</context>
</contexts>
<marker>Ford, Singh, 1991</marker>
<rawString>A. Ford and R. Singh. 1991. Prop´edeutique morphologique. Folia Linguistica, 25(3–4):549–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ford</author>
<author>R Singh</author>
<author>G Martohardjono</author>
</authors>
<title>Pace Panini. Peter Lang,</title>
<date>1997</date>
<location>New York.</location>
<contexts>
<context position="1132" citStr="Ford et al., 1997" startWordPosition="166" endWordPosition="169"> Morphology (Ford et al., 1997) is outlined, and preliminary evaluation results are presented. The program, Whole Word Morphologizer, takes a POS-tagged lexicon as input, induces morphological relationships without attempting to discover or identify morphemes, and is then able to generate new words beyond the learning sample. The accuracy (precision) of the generated new words is as high as 80% using the pure Whole Word theory, and 92% after a post-hoc adjustment is added to the routine. The aim of this project is to develop a computational model employing the theory of whole word morphology (Ford et al., 1997) capable on the one hand of identifying morphological relations within a list of words from any one of a wide variety of languages and, on the other, of putting that knowledge to use in creating previously unseen word forms. A small application called Whole Word Morphologizer which does just this is outlined and discussed. In particular, this approach is set against the literature on computational morphology as an entirely different way of doing things which has the potential to be generalized to all known varieties of morphology in the world’s languages, a feature not shared by previous metho</context>
<context position="10273" citStr="Ford et al., 1997" startWordPosition="1648" endWordPosition="1651">hat it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a panoply of recent publications devoted to the empirical shortcomings of traditional so-called “Itemand-Arrangement” morphology (Hockett, 1954; Bochner, 1993; Ford and Singh, 1991; Anderson, 1992; Ford et al., 1997), and the list of phenomena that fall out of reach of a compositional approach is rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial suppletion, etc. Still, seemingly every documented morphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-andArrangement analysis; this description applies to all of the computational papers cited above. 2 An alternative theory Whole Word Morphologizer (henceforth WWM) is the first implementation of the theory of Whole Word Morphology. The theory, developed by Alan Ford and Raje</context>
</contexts>
<marker>Ford, Singh, Martohardjono, 1997</marker>
<rawString>A. Ford, R. Singh, and G. Martohardjono. 1997. Pace Panini. Peter Lang, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<date>1967</date>
<booktitle>Language identification in the limit. Information and Control,</booktitle>
<pages>10--447</pages>
<marker>Gold, 1967</marker>
<rawString>E. M. Gold. 1967. Language identification in the limit. Information and Control, 10:447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>Linguistica: An automatic morphological analyzer.</title>
<date>2001</date>
<booktitle>In Arika Okrent and</booktitle>
<volume>volume</volume>
<pages>36--1</pages>
<editor>John Boyle, editors,</editor>
<publisher>Chicago Linguistic Society,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5055" citStr="Goldsmith (2001" startWordPosition="788" endWordPosition="789">ogical analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the algorithm cannot double as a morphological parser. More recently, efforts have been developing which identify morphemes and perform some sort of analysis. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Their procedure uses a morpheme-based model, provides an analysis of the words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by de Marcken’s (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. Also, Baroni (2000) aims at finding a set of prefixes from a corpus, together with an affix-stem parse of each of the words. While they might differ in their methods or objectives, all of the above morphological applications share a common characteristic in that they are learners designed exclusively for the acquisition of morphological facts from corpora and do not generate new words based on the information they acquire. 1.2 P</context>
<context position="8999" citStr="Goldsmith, 2001" startWordPosition="1446" endWordPosition="1447"> are immaterial, what matters is that both learners are being told which words are related to which and are left with the task of describing that relation in the form a rule. In other words, the algorithms they use cannot discover that words are morphologically related. 1.3 What’s morphology? In the above algorithms, the task of determining whether one word is related to another in a morphological sense is most frequently left to the linguist, as this information has to be encoded in the training data for these algorithms. (Some of the most recent work such as (Schone and Jurafsky, 2001) and (Goldsmith, 2001b) are notable exceptions to this paradigm.) This is perhaps not surprising, since no serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed what have been referred to as discovery procedures (v. (Nida, 1949), for example) for the identification of morphemes but since the mid fifties (Chomsky, 1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate it to studies on language acquisition. But, since a morphological learner like that presented here is des</context>
<context position="29961" citStr="Goldsmith, 2001" startWordPosition="4940" endWordPosition="4941">w all we have to show, but eventually we hope to produce a mathematical proof of just what WWM can learn, and just what kinds of lexicons are learnable in Gold’s sense. To our knowledge, it has never been proven whether the total lexicon of a natural language is identifiable in the limit from the sort of data we provide (i.e. POS-tagged words), using in particular the theory of Whole Word Morphology in a perfect fashion. Still, it is interesting that nothing about this language learning paradigm says anything about morphological analysis. The current crop of true morphological learners, e.g. (Goldsmith, 2001b), endeavor to learn to analyze the morphology of the language at hand in the manner of a linguist. Goldsmith has even called his Linguistica system a “linguist in a box.” This is perhaps an interesting and worthwhile endeavor, but it is not one that is undertaken here. WWM is instead attempting to learn the target language in a more direct way from the data, without first constructing the intermediary of a traditional morphological analysis. We are thus not learning the linguist’s notion of morphology but rather the result of morphology, i.e. the word forms of the language together with the </context>
<context position="33232" citStr="Goldsmith, 2001" startWordPosition="5482" endWordPosition="5483">formation that allows its proper use, and in our implementation the POS tags (poor substitutes for what should be a richer database of information) are crucial to the discovery of the strategies. the blocking modification. Obviously the program needs to be systematically tested on multiple lexica from different languages, but these results strongly suggest that it is possible to model the acquisition of morphology as a component of learning to generate language directly, rather than to treat computational learning as the acquisition of linguistic theory as several current approaches do, e.g. (Goldsmith, 2001b). Although the principles of whole word morphology allow one to contemplate versions of WWM that would work on templatic morphologies, polysynthetic languages, and a host of other recalcitrant phenomena, the current instantiation of the program is not so ambitious. The comparison algorithm detailed in the previous section compares words letter by letter, either from left to right or from right to left. No other possible alignments between words are considered and WWM is in its current state only capable of grasping prefixal and suffixal morphology. We are currently developing a more sophisti</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John A. Goldsmith. 2001a. Linguistica: An automatic morphological analyzer. In Arika Okrent and John Boyle, editors, CLS 36: The Main Session, volume 36-1. Chicago Linguistic Society, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="5055" citStr="Goldsmith (2001" startWordPosition="788" endWordPosition="789">ogical analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the algorithm cannot double as a morphological parser. More recently, efforts have been developing which identify morphemes and perform some sort of analysis. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Their procedure uses a morpheme-based model, provides an analysis of the words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by de Marcken’s (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. Also, Baroni (2000) aims at finding a set of prefixes from a corpus, together with an affix-stem parse of each of the words. While they might differ in their methods or objectives, all of the above morphological applications share a common characteristic in that they are learners designed exclusively for the acquisition of morphological facts from corpora and do not generate new words based on the information they acquire. 1.2 P</context>
<context position="8999" citStr="Goldsmith, 2001" startWordPosition="1446" endWordPosition="1447"> are immaterial, what matters is that both learners are being told which words are related to which and are left with the task of describing that relation in the form a rule. In other words, the algorithms they use cannot discover that words are morphologically related. 1.3 What’s morphology? In the above algorithms, the task of determining whether one word is related to another in a morphological sense is most frequently left to the linguist, as this information has to be encoded in the training data for these algorithms. (Some of the most recent work such as (Schone and Jurafsky, 2001) and (Goldsmith, 2001b) are notable exceptions to this paradigm.) This is perhaps not surprising, since no serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed what have been referred to as discovery procedures (v. (Nida, 1949), for example) for the identification of morphemes but since the mid fifties (Chomsky, 1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate it to studies on language acquisition. But, since a morphological learner like that presented here is des</context>
<context position="29961" citStr="Goldsmith, 2001" startWordPosition="4940" endWordPosition="4941">w all we have to show, but eventually we hope to produce a mathematical proof of just what WWM can learn, and just what kinds of lexicons are learnable in Gold’s sense. To our knowledge, it has never been proven whether the total lexicon of a natural language is identifiable in the limit from the sort of data we provide (i.e. POS-tagged words), using in particular the theory of Whole Word Morphology in a perfect fashion. Still, it is interesting that nothing about this language learning paradigm says anything about morphological analysis. The current crop of true morphological learners, e.g. (Goldsmith, 2001b), endeavor to learn to analyze the morphology of the language at hand in the manner of a linguist. Goldsmith has even called his Linguistica system a “linguist in a box.” This is perhaps an interesting and worthwhile endeavor, but it is not one that is undertaken here. WWM is instead attempting to learn the target language in a more direct way from the data, without first constructing the intermediary of a traditional morphological analysis. We are thus not learning the linguist’s notion of morphology but rather the result of morphology, i.e. the word forms of the language together with the </context>
<context position="33232" citStr="Goldsmith, 2001" startWordPosition="5482" endWordPosition="5483">formation that allows its proper use, and in our implementation the POS tags (poor substitutes for what should be a richer database of information) are crucial to the discovery of the strategies. the blocking modification. Obviously the program needs to be systematically tested on multiple lexica from different languages, but these results strongly suggest that it is possible to model the acquisition of morphology as a component of learning to generate language directly, rather than to treat computational learning as the acquisition of linguistic theory as several current approaches do, e.g. (Goldsmith, 2001b). Although the principles of whole word morphology allow one to contemplate versions of WWM that would work on templatic morphologies, polysynthetic languages, and a host of other recalcitrant phenomena, the current instantiation of the program is not so ambitious. The comparison algorithm detailed in the previous section compares words letter by letter, either from left to right or from right to left. No other possible alignments between words are considered and WWM is in its current state only capable of grasping prefixal and suffixal morphology. We are currently developing a more sophisti</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John A. Goldsmith. 2001b. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hafer</author>
<author>S F Weiss</author>
</authors>
<title>Word segmentation by letter successsor varieties. Information Storage and Retrieval,</title>
<date>1974</date>
<pages>10--371</pages>
<contexts>
<context position="3876" citStr="Hafer and Weiss, 1974" startWordPosition="604" endWordPosition="607">istinguish between two types of analysis in light of the often very different results sought by various morphological learners. On the one hand, some applications focus exclusively on the segmentation of words or longer strings into smaller units. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphological applications are designed solely to identify morphemes based on a training corpus and not to provide a morphological analysis for each word of tha</context>
</contexts>
<marker>Hafer, Weiss, 1974</marker>
<rawString>M. A. Hafer and S. F. Weiss. 1974. Word segmentation by letter successsor varieties. Information Storage and Retrieval, 10(371–385).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>From phoneme to morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<pages>31--190</pages>
<contexts>
<context position="3685" citStr="Harris, 1955" startWordPosition="575" endWordPosition="577">r morphological parsing while paying little or no attention to generation. While these applications find a common goal in the automatic acquisition of morphology, it is helpful to distinguish between two types of analysis in light of the often very different results sought by various morphological learners. On the one hand, some applications focus exclusively on the segmentation of words or longer strings into smaller units. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the </context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Zellig Harris. 1955. From phoneme to morpheme. Language, 31:190–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Morpheme boundaries within words: Report on a computer test.</title>
<date>1967</date>
<booktitle>In Transformations and Discourse Analysis Papers,</booktitle>
<volume>73</volume>
<contexts>
<context position="3700" citStr="Harris, 1967" startWordPosition="578" endWordPosition="579">l parsing while paying little or no attention to generation. While these applications find a common goal in the automatic acquisition of morphology, it is helpful to distinguish between two types of analysis in light of the often very different results sought by various morphological learners. On the one hand, some applications focus exclusively on the segmentation of words or longer strings into smaller units. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, som</context>
</contexts>
<marker>Harris, 1967</marker>
<rawString>Zellig Harris. 1967. Morpheme boundaries within words: Report on a computer test. In Transformations and Discourse Analysis Papers, volume 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Hockett</author>
</authors>
<title>Two models of grammatical description.</title>
<date>1954</date>
<journal>Word,</journal>
<pages>10--210</pages>
<contexts>
<context position="10200" citStr="Hockett, 1954" startWordPosition="1638" endWordPosition="1639"> here is designed to model the acquisition of morphology, it seems that it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphology learners is their reliance on a traditional notion of the morpheme as a lexical unit and their exclusive focus on concatenative morphology. There is a panoply of recent publications devoted to the empirical shortcomings of traditional so-called “Itemand-Arrangement” morphology (Hockett, 1954; Bochner, 1993; Ford and Singh, 1991; Anderson, 1992; Ford et al., 1997), and the list of phenomena that fall out of reach of a compositional approach is rather impressive: zero-morphs, ablaut-like processes, templatic morphology, class markers, partial suppletion, etc. Still, seemingly every documented morphological learner relies on a Bloomfieldian notion of the morpheme and produces an Item-andArrangement analysis; this description applies to all of the computational papers cited above. 2 An alternative theory Whole Word Morphologizer (henceforth WWM) is the first implementation of the the</context>
</contexts>
<marker>Hockett, 1954</marker>
<rawString>Charles Hockett. 1954. Two models of grammatical description. Word, 10:210–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Axel Janssen</author>
</authors>
<title>Segmentierung franz¨osischer Wortformen in Morphe ohne Verwendung eines Lexikons.</title>
<date>1992</date>
<booktitle>Computatio Linguae,</booktitle>
<pages>74--95</pages>
<editor>In Ursula Klenk, editor,</editor>
<publisher>Steiner Verlag,</publisher>
<location>Stuttgart.</location>
<contexts>
<context position="3937" citStr="Janssen (1992)" startWordPosition="615" endWordPosition="616">ifferent results sought by various morphological learners. On the one hand, some applications focus exclusively on the segmentation of words or longer strings into smaller units. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphological applications are designed solely to identify morphemes based on a training corpus and not to provide a morphological analysis for each word of that corpus. Brent (1993), for example, aims at finding the righ</context>
</contexts>
<marker>Janssen, 1992</marker>
<rawString>Axel Janssen. 1992. Segmentierung franz¨osischer Wortformen in Morphe ohne Verwendung eines Lexikons. In Ursula Klenk, editor, Computatio Linguae, pages 74–95. Steiner Verlag, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Kanazawa</author>
</authors>
<title>Learnable Classes of Categorial Grammars. Studies in Logic, Language and Information.</title>
<date>1998</date>
<publisher>CSLI Publications</publisher>
<contexts>
<context position="27900" citStr="Kanazawa (1998)" startWordPosition="4584" endWordPosition="4585">. This would amount to trying to beat WWM at its own game in pursuit of a gold standard, which could only be obtained using a better implementation of WWM’s theory. A perfect implementation of Whole Word Morphology would have perfect recall, in view of our eventual goal of using this theory to inform us about the morphology of a language—about what ought to be recalled. We are not trying to learn something that we feel is already known. 5.2 What’s learning? It is worth considering the endeavor of learning morphology in terms of formal learning theory, as presented in Osherson et al. (1986) or Kanazawa (1998) for example. In the classical framework, the problem of learning a language from positive example data is approached by considering the successive guesses at the target language that a purported learner makes when presented with some sequentially increasing learning sample drawn from that language. Considering just morphology, it seems that the target language is the set of all possible words of the natural language at hand, a possibly infinite (or at least indefinite) set. WWM’s output is a list of generated words subsuming the corpus, which are supposed to be all the words creatable by appl</context>
</contexts>
<marker>Kanazawa, 1998</marker>
<rawString>Makoto Kanazawa. 1998. Learnable Classes of Categorial Grammars. Studies in Logic, Language and Information. CSLI Publications and the European Association for Logic, Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Kimmo Koskenniemi</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A compiler for two-level phonological rules.</title>
<date>1987</date>
<tech>Technical Report CSLI-87-108,</tech>
<institution>Center for</institution>
<location>Palo Alto.</location>
<contexts>
<context position="6625" citStr="Karttunen et al., 1987" startWordPosition="1048" endWordPosition="1051"> man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications require a set of man-made lexical rules to function. While the practical uses of such applications as PC-Kimmo are incontestable, it is clear that they are part of a different endeavour, and should not be confused with genuine morphological learners. The other relevant group of computational applications can, as mentioned, both acquire morphological knowledge from corpora and generate new words based on that knowledge. Alb</context>
</contexts>
<marker>Karttunen, Koskenniemi, Kaplan, 1987</marker>
<rawString>Lauri Karttunen, Kimmo Koskenniemi, and Ronald M. Kaplan. 1987. A compiler for two-level phonological rules. Technical Report CSLI-87-108, Center for the Study of Language and Information, Palo Alto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Ronald M Kaplan</author>
<author>Annie Zaenen</author>
</authors>
<title>Two-level morphology with composition.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics, volume I,</booktitle>
<pages>141--148</pages>
<location>Nantes, France.</location>
<contexts>
<context position="6204" citStr="Karttunen et al., 1992" startWordPosition="975" endWordPosition="979">nd do not generate new words based on the information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able to generate from acquired knowledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications requi</context>
</contexts>
<marker>Karttunen, Kaplan, Zaenen, 1992</marker>
<rawString>Lauri Karttunen, Ronald M. Kaplan, and Annie Zaenen. 1992. Two-level morphology with composition. In Proceedings of the 15th International Conference on Computational Linguistics, volume I, pages 141–148, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Finite state constraints. In</title>
<date>1993</date>
<booktitle>The Last Phonological Rule,</booktitle>
<pages>173--194</pages>
<editor>John A. Goldsmith, editor,</editor>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6221" citStr="Karttunen, 1993" startWordPosition="980" endWordPosition="981">ords based on the information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able to generate from acquired knowledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications require a set of man-m</context>
</contexts>
<marker>Karttunen, 1993</marker>
<rawString>Lauri Karttunen. 1993. Finite state constraints. In John A. Goldsmith, editor, The Last Phonological Rule, pages 173–194. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Constructing lexical transducers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics, volume I,</booktitle>
<pages>406--411</pages>
<contexts>
<context position="6239" citStr="Karttunen, 1994" startWordPosition="982" endWordPosition="983"> information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able to generate from acquired knowledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morphology. Once again, these applications require a set of man-made lexical rules </context>
</contexts>
<marker>Karttunen, 1994</marker>
<rawString>Lauri Karttunen. 1994. Constructing lexical transducers. In Proceedings of the 15th International Conference on Computational Linguistics, volume I, pages 406– 411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
<author>Suresh Manandhar</author>
</authors>
<title>A hybrid approach to word segmentation.</title>
<date>1998</date>
<booktitle>Proceedings of Inductive Logic Programming98,</booktitle>
<volume>1446</volume>
<editor>In David Page, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4124" citStr="Kazakov and Manandhar (1998)" startWordPosition="641" endWordPosition="645">nits. In other words, their function is to identify morpheme boundaries within words and, as such, they only indirectly identify morphemes as linguistic units. Zellig Harris’s (Harris, 1955; Harris, 1967) pioneering work suggests that morpheme boundaries can be determined by counting the number of letters that follow a given substring within a corpus (v. (Hafer and Weiss, 1974) for a further development of Harris’s ideas). Janssen (1992) and Flenner (1994; 1995) also work towards segmenting words but use training corpora in which morpheme boundaries have been manually inserted. Recent work by Kazakov and Manandhar (1998) combines unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphological applications are designed solely to identify morphemes based on a training corpus and not to provide a morphological analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the algorithm cannot double as a morphological parser. More recently, efforts have been developing which identify morphemes and perform some sort of a</context>
</contexts>
<marker>Kazakov, Manandhar, 1998</marker>
<rawString>Dimitar Kazakov and Suresh Manandhar. 1998. A hybrid approach to word segmentation. In David Page, editor, Proceedings of Inductive Logic Programming98, volume 1446 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level morphology: a general computational model for word-form recognition and production.</title>
<date>1983</date>
<tech>Technical Report 11,</tech>
<institution>Dept. of General Linguistics, University of Helsinki.</institution>
<contexts>
<context position="6164" citStr="Koskenniemi, 1983" startWordPosition="971" endWordPosition="972"> morphological facts from corpora and do not generate new words based on the information they acquire. 1.2 Parsing and generation Only a handful of programs can both parse and generate words. Once again, these programs fall into two very distinct categories. In view of the disparity between these programs, it is useful to distinguish between genuine morphological learners able to generate from acquired knowledge and generators/parsers that implement a man-made analysis. The latter group is perhaps the most well known, so let us begin with them. Kimmo-type applications of two-level morphology (Koskenniemi, 1983; Antworth, 1990; Karttunen et al., 1992; Karttunen, 1993; Karttunen, 1994) can provide a morphological analysis of the words in a corpus and generate new words based on a set of rules; but these programs must first be provided with that set of rules and a lexicon containing morphemes by the user. Similar work in oneand two-level morphology has been done using the Attribute-Logic Engine (Carpenter, 1992). Some of these systems (e.g. (Karttunen et al., 1987)) have a front-end that compiles more traditional linearly ordered morphological rules into the finite-state automata of two-level morpholo</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-level morphology: a general computational model for word-form recognition and production. Technical Report 11, Dept. of General Linguistics, University of Helsinki.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Neuvel</author>
<author>R Singh</author>
</authors>
<title>In press. Vive la diff´erence! What morphology is about. Folia Linguistica.</title>
<marker>Neuvel, Singh, </marker>
<rawString>S. Neuvel and R. Singh. In press. Vive la diff´erence! What morphology is about. Folia Linguistica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Nida</author>
</authors>
<title>Morphology. The descriptive analysis of words.</title>
<date>1949</date>
<publisher>University of Michigan Press,</publisher>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="9304" citStr="Nida, 1949" startWordPosition="1495" endWordPosition="1496">gorithms, the task of determining whether one word is related to another in a morphological sense is most frequently left to the linguist, as this information has to be encoded in the training data for these algorithms. (Some of the most recent work such as (Schone and Jurafsky, 2001) and (Goldsmith, 2001b) are notable exceptions to this paradigm.) This is perhaps not surprising, since no serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed what have been referred to as discovery procedures (v. (Nida, 1949), for example) for the identification of morphemes but since the mid fifties (Chomsky, 1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate it to studies on language acquisition. But, since a morphological learner like that presented here is designed to model the acquisition of morphology, it seems that it should above all be able to determine for itself whether two words are morphologically related or not, whether there is anything morphological to acquire at all. Another important thing to note about the vast majority of computational morphol</context>
</contexts>
<marker>Nida, 1949</marker>
<rawString>Eugene Nida. 1949. Morphology. The descriptive analysis of words. University of Michigan Press, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel N Osherson</author>
<author>Michael Stob</author>
<author>Scott Weinstein</author>
</authors>
<title>Systems that Learn.</title>
<date>1986</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="27881" citStr="Osherson et al. (1986)" startWordPosition="4579" endWordPosition="4582">te according to its theory. This would amount to trying to beat WWM at its own game in pursuit of a gold standard, which could only be obtained using a better implementation of WWM’s theory. A perfect implementation of Whole Word Morphology would have perfect recall, in view of our eventual goal of using this theory to inform us about the morphology of a language—about what ought to be recalled. We are not trying to learn something that we feel is already known. 5.2 What’s learning? It is worth considering the endeavor of learning morphology in terms of formal learning theory, as presented in Osherson et al. (1986) or Kanazawa (1998) for example. In the classical framework, the problem of learning a language from positive example data is approached by considering the successive guesses at the target language that a purported learner makes when presented with some sequentially increasing learning sample drawn from that language. Considering just morphology, it seems that the target language is the set of all possible words of the natural language at hand, a possibly infinite (or at least indefinite) set. WWM’s output is a list of generated words subsuming the corpus, which are supposed to be all the word</context>
</contexts>
<marker>Osherson, Stob, Weinstein, 1986</marker>
<rawString>Daniel N. Osherson, Michael Stob, and Scott Weinstein. 1986. Systems that Learn. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledgefree induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>In 2nd Meeting of the North American Chapter of the ACL,</booktitle>
<pages>183--191</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics, Morgan Kaufman.</institution>
<contexts>
<context position="4759" citStr="Schone and Jurafsky (2001)" startWordPosition="742" endWordPosition="745"> unsupervised and supervised learning techniques to generate a set of segmentation rules that can further be applied to previously unseen words. On the other hand, some computational morphological applications are designed solely to identify morphemes based on a training corpus and not to provide a morphological analysis for each word of that corpus. Brent (1993), for example, aims at finding the right set of suffixes from a corpus, but the algorithm cannot double as a morphological parser. More recently, efforts have been developing which identify morphemes and perform some sort of analysis. Schone and Jurafsky (2001) employ a great many sophisticated post-hoc adjustments to obtain the right conflation sets for words by pure corpus analysis without annotations. Their procedure uses a morpheme-based model, provides an analysis of the words, and does in a sense discover morphological relations. Goldsmith (2001b; 2001a), inspired by de Marcken’s (1995) thesis on minimum description length, attempts to provide both a list of morphemes and an analysis of each word in a corpus. Also, Baroni (2000) aims at finding a set of prefixes from a corpus, together with an affix-stem parse of each of the words. While they </context>
<context position="8978" citStr="Schone and Jurafsky, 2001" startWordPosition="1441" endWordPosition="1444">inctions between the two methods are immaterial, what matters is that both learners are being told which words are related to which and are left with the task of describing that relation in the form a rule. In other words, the algorithms they use cannot discover that words are morphologically related. 1.3 What’s morphology? In the above algorithms, the task of determining whether one word is related to another in a morphological sense is most frequently left to the linguist, as this information has to be encoded in the training data for these algorithms. (Some of the most recent work such as (Schone and Jurafsky, 2001) and (Goldsmith, 2001b) are notable exceptions to this paradigm.) This is perhaps not surprising, since no serious attempt at defining a morphological relation has been made in the last few decades. American structuralists of the forties and fifties proposed what have been referred to as discovery procedures (v. (Nida, 1949), for example) for the identification of morphemes but since the mid fifties (Chomsky, 1955), it has been customary for morphological theory to ignore this aspect of morphology and relegate it to studies on language acquisition. But, since a morphological learner like that </context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2001. Knowledgefree induction of inflectional morphologies. In 2nd Meeting of the North American Chapter of the ACL, pages 183–191. Association for Computational Linguistics, Morgan Kaufman.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>