<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.445105">
A PROPER TREATMENT OF SYNTAX AND SEMANTICS IN MACHINE TRANSLATION
</title>
<author confidence="0.8140095">
Yoshihiko Nitta, Atsushi Okajima, Hiroyuki Kaji,
Youichi Hidano, Koichiro Ishihara
</author>
<affiliation confidence="0.966247">
Systems Development Laboratory, Hitachi, Ltd.
</affiliation>
<address confidence="0.725223">
1099 Ohzenji Asao-ku, Kawasaki-shi, 215 JAPAN
</address>
<email confidence="0.62387">
ABSTRACT
</email>
<bodyText confidence="0.999946625">
A proper treatment of syntax and semantics in
machine translation is introduced and discussed
from the empirical viewpoint. For English-
Japanese machine translation, the syntax directed
approach is effective where the Heuristic Parsing
Model (HPM) and the Syntactic Role System play
important roles. For Japanese-English
translation, the semantics directed approach is
powerful where the Conceptual Dependency Diagram
(CDD) and the Augmented Case Marker System (which
is a kind of Semantic Role System) play essential
roles. Some examples of the difference between
Japanese sentence structure and English sentence
structure, which is vital to machine translation,
are also discussed together with various
interesting ambiguities.
</bodyText>
<sectionHeader confidence="0.929668" genericHeader="method">
I INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999857714285714">
We have been studying machine translation
between Japanese and English for several years.
Experiences gained in systems development and in
linguistic data investigation suggest that the
essential point in constructing a practical
machine translation system is in the appropriate
blending of syntax directed processing and the
semantics directed processing.
In order to clarify the above-mentioned
suggestion, let us compare the characteristics of
the syntax directed approach with those of the
semantics directed approach.
The advantages of the syntax directed approach
are as follows:
</bodyText>
<listItem confidence="0.993254230769231">
(1) It is not so difficult to construct the
necessary linguistic data for syntax directed
processors because the majority of these data can
be reconstructed from already established and
well-structured lexical items such as verb pattern
codes and parts of speech codes, which are
overflowingly abundant in popular lexicons.
(2) The total number of grammatical rules
necessary for syntactic processing usually stays
within a controllable range.
(3) The essential aspects of syntactic
processing are already well-known, apart from
efficiency problems.
</listItem>
<bodyText confidence="0.9982048">
The disadvantage of the syntax directed
approach is its insufficient ability to resolve
various ambiguities inherent in natural languages.
On the other hand, the advantages of the
semantics directed approach are as follows:
</bodyText>
<listItem confidence="0.996072363636364">
(1) The meaning of sentences or texts can be
grasped in a unified form without being affected
by the syntactic variety.
(2) Semantic representation can play a pivotal
role for language transformation and can provide
a basis for constructing a transparent machine
translation system, because semantic representa-
tion is fairly independent of the differences in
language classes.
(3) Consequently, semantics directed internal
representation can produce accurate translations.
</listItem>
<bodyText confidence="0.967929">
The disadvantages of the semantics directed
approach are as follows:
</bodyText>
<listItem confidence="0.663536727272727">
(1) It is not easy to construct a semantic
lexicon which covers real world phenomena of a
reasonably wide range. The main reason for this
difficulty is that a well-established and
widely-accepted method of describing semantics
does not exist. (For strongly restricted
statements or topics, of course, there exist
well-elaborated methods such as Montague grammar
[2], Script and MOP (Memory Organization Packet)
theory [13], Procedural Semantics [14], and
Semantic Interlingual Representation [15].)
</listItem>
<bodyText confidence="0.963299">
(2) The second but intractable problem is that,
even if you could devise a fairly acceptable
method to describe semantics, the total number of
semantic rule descriptions may expand beyond all
manageable limits.
Therefore, we think that it is necessary to
seek proper combinations of syntactic processing
and semantic processing so as to compensate for
the disadvantages of each.
The purpose of this paper is to propose a
proper treatment of syntax and semantics in
machine translation systems from a heuristic
viewpoint, together with persuasive examples
obtained through operating experiences. A
sub-language approach which would put some
moderate restrictions on the syntax and semantics
of source language is also discussed.
</bodyText>
<page confidence="0.998552">
159
</page>
<sectionHeader confidence="0.955826" genericHeader="method">
II SYNTAX AND SEMANTICS
</sectionHeader>
<bodyText confidence="0.99998521875">
It is not entirely possible to distinguish a
syntax directed approach from a semantics
directed approach, because syntax and semantics
are always performing their linguistic functions
reciprocally.
As Wilks [16] points out, it is plausible but a
great mistake to identify syntactic processing
with superficial processing, or to identify
semantic processing with deep processing. The
term &amp;quot;superficial&amp;quot; or &amp;quot;deep&amp;quot; only reflects the
intuitive distance from the language represen-
tation in (superficial) character strings or from
the language representation in our (deep) minds.
Needless to say, machine translation inevitably
has something to do with superficial processing.
In various aspects of natural language
processing, it is quite common to segment a
superficial sentence into a collection of phrases.
A phrase itself is a collection of words. In
order to restructure the collection of phrases,
the processor must first of all attach some sorts
of labels to the phrases. If these labels are
something like subject, object, complement, etc.,
then we will call this processor a syntax directed
processor, and if these labels are something like
agent, object, instrument, etc., or animate,
inanimate, concrete, abstract, human, etc., then
we will call this processor a semantics directed
processor.
The above definition is oversimplified and of
course incomplete, but it is still enough for the
arguments in this paper.
</bodyText>
<sectionHeader confidence="0.992084666666667" genericHeader="method">
III SYNTAX DIRECTED APPROACH:
A PROTOTYPE ENGLISH-JAPANESE
MACHINE TRANSLATION SYSTEM
</sectionHeader>
<bodyText confidence="0.999728">
So far we have developed two prototype machine
translation systems; one is for English-Japanese
translation [6] and the other is for Japanese-
English translation.
The prototype model system for English-
Japanese translation (Figure 1) is constructed as
a syntax directed processor using a phrase
structure type internal representation called HPM
(Heuristic Parsing Model), where the semantics is
utilized to disambiguate dependency relationships.
The somewhat new name HPM (Heuristic Parsing
Model) reflects the parsing strategy by which the
machine translation tries to simultate the
heuristic way of actual human of language
translation. The essential features of heuristic
translation are summarized in the following three
steps:
</bodyText>
<listItem confidence="0.99889">
(1) To segment an input sentence into phrasal
elements (PE) and clausal elements (CE).
(2) To assign syntactic roles to PE&apos;s and CE&apos;s,
and restructure the segmented elements into
tree-forms by governing relation, and into
link-forms by modifying relation.
(3) To permute the segmented elements, and to
assign appropriate Japanese equivalents with
necessary case suffixes and postpositions.
</listItem>
<subsectionHeader confidence="0.737976">
Noteworthy findings from operational
</subsectionHeader>
<bodyText confidence="0.998776">
experience and efforts to improve the prototype
model are as follows:
</bodyText>
<sectionHeader confidence="0.479412" genericHeader="method">
Input English Sentence
</sectionHeader>
<subsectionHeader confidence="0.373274">
Output Japanese Sentence
</subsectionHeader>
<figureCaption confidence="0.991089">
Figure 1 Configuration of Machine Translation System: ATHENE [6]
</figureCaption>
<figure confidence="0.828696882352941">
Lexicons PI
&apos;entry:
• word
• phrase
• idiom
etc.
description:
• attribute
• Japanese equivalents
• controlling marks
for analysis,
transformation and
generation
• etc.
Lexicon Retrieval
Morphological Analysis
....dSyntactic Analysis
[based on HPM]
Internal Language
Representation
[based on HPM]
Grammatical
Data
[Parsed Tree/Link]
Post-editing Support
[&apos;solution to manifold]
meanings
Tree/Link Transformation
Sentence Generation
Morphological Synthesis
[&apos;adjustment of tense and
mode
&apos;assignment of
postpositions
</figure>
<page confidence="0.965806">
160
</page>
<listItem confidence="0.999683">
• WE: Word Element
• PE; Phrasal Element
• CP: Clausal Element
• SE: Sentence
• This sample English sentence is taken from Datamation Jan. 1982.
</listItem>
<figureCaption confidence="0.955008">
Figure 2 An Example of Phrase Structure Type Representation
</figureCaption>
<figure confidence="0.965001">
7With some hel;Tfrom oversea:T:rthe Japanesjare beginningL 10-year R&amp;D effort71ntendedTto yieldL fifth generation systemT.T
I \ \ \ \
AUX V7 KIRA ai N6 N6 ILIZO&lt;I GE‘gp gya lam 1,61/ mg PROD
IIIM M IMECOiMMMEMIN = MI 1WE
MOM
IMO
CE
} SE
}.PE
COMM
(1) The essential structure of English sentences
</figure>
<bodyText confidence="0.999151338235294">
should be grasped by phrase structure type
representations.
An example of phrase strucure type
representation, which we call HPM (Heuristic
Parsing Model), is illustrated in Figure 2. In
Figure 2, a parsed tree is composed of two
substructures. One is &amp;quot;tree ( \i/ ),&amp;quot;
representing a compulsory dependency relation,
and the other is &amp;quot;link (14.,,/),&amp;quot; representing an
optional dependency relation. Each node
corresponds to a certain constituent of the
sentence.
The most important constituent is a &amp;quot;phrasal
element (PE)&amp;quot; which is composed of one or more
word element(s) and carries a part of the
sentential meaning in the smallest possible
form. PE&apos;s are mutually exclusive. In Figure 2,
PE&apos;s are shown by using the &amp;quot;segmenting marker
(T)&amp;quot;, such as
TWith some help (ADVL)T,
Tfrom overseas (ADJV)T.
T,(COMM)T,
Tthe Japanese (SUBJ)1
and
Tare beginning (GOV)T,
where the terminologies in parentheses are the
syntactic roles which will be discussed later.
A &amp;quot;clausal element (CE)&amp;quot; is composed of one or
more PE(&apos;s) which carries a part of sentential
meaning in a nexus-like form. A CE roughly
corresponds to a Japanese simple sentence such
as: &amp;quot;.1,{wa/ga/wo/no/ni} {suru/dearu} [koto].&amp;quot;
CE&apos;s allow mutual intersection. Typical examples
are the underlined parts in the following:
&amp;quot;It is important for you to do so.&amp;quot;
&amp;quot;... intended to yield a fifth generation system.&amp;quot;
One interesting example in Figure 2 may be the
part
&amp;quot;With some help from overseas&amp;quot;,
which is treated as only two consecutive phrasal
elements. This is the typical result of a syntax
directed parser. In the case of a semantics
directed parser, the above-mentioned part will be
treated as a clausal element. This is because
the meaning of this part is &amp;quot;(by) getting some
help from overseas&amp;quot; or the like, which is rather
clausal than phrasal.
(2) Syntax directed processors are effective and
powerful to get phrase structure type parsed
trees.
Our HPM parser operates both in a top-down way
globally and in a bottom-up way locally. An
example of top-down operation would be the
segmentation of an input sentence (i.e. the
sequence of word elements (WE&apos;s)) to get phrasal
elements (PE), and an example of bottom-up
operation would be the construction of tree-forms
or link-forms to get clausal elements (CE) or a
sentence (SE). These operations are supported by
syntax directed grammatical data such as
verb dependency type codes (cf. Table 1, which is
a simplified version of Hornby&apos;s classification
[51), syntactic role codes (Table 2) and some
production rule type grammars (Table 3 &amp; Table
4). It may be permissible to say that all these
syntactic data are fairly compact and the kernel
parts are already well-elaborated (cf. [1], [81.
[11], [121).
</bodyText>
<page confidence="0.998975">
161
</page>
<tableCaption confidence="0.997941">
Table 1 Dependency Pattern of Verb
</tableCaption>
<table confidence="0.999672">
Code Verb Pattern Examples
VI Be + ... be
V2 Vi (A. Be) + Complement, get, look
It/There + Vi + ...
V3 Vi [+ Adverbial Modifier] rise, walk
V6 Vt + To-infinitive intend
V7 Vt + Object begin, yield
V8 Vt + that + ... agree, think
V14 Vt + Object (+not] + know, bring
To-infinitive
</table>
<tableCaption confidence="0.920642">
Table 2 Syntactic Roles
</tableCaption>
<table confidence="0.999039833333333">
Code Role
SUBJ Subject
OBJ Object
TOOBJ Object in To-infinitive Form
NAPP Noun in Apposition
GOV Governing Verb
TOGOV Governing Verb in To-infinitive Form
ENGOV Governing Verb in Past Participle Form
ADJV Adjectival
ENADJ Adjectival in Past Participle Form
ADVL Adverbial
SENT Sentence
</table>
<listItem confidence="0.790742">
(3) The weak point of syntax directed processors
is their insufficient ability to disambiguate;
i.e. the ability to identify dependency types of
verb phrases and the ability to determine heads
of prepositional phrase modifiers.
(4) In order to boost the aforementioned
disambiguation power, it is useful to apply
</listItem>
<bodyText confidence="0.786509428571429">
semantic filters that facilitate the selective
restrictions on linking a verb with nominals and
on linking a modifier with its head.
A typical example of the semantic filter is
illustrated in Figure 3. The semantic filter may
operate along with selective restriction rules
such as:
</bodyText>
<table confidence="0.843162833333333">
N22 (Animal) + with + N753 (Accessory)
Plausible
[.: N22 is equipped with N753]
V21 (Watching-Action) + with N541
(Watching Instrument) —I.. OK
P:1121 by using N541 as an instrument]
</table>
<tableCaption confidence="0.7517855">
The semantic filter is not complete,
especially for metaphorical expressions. A bird
could also use binoculars.
Table 3 Rules for Assigning Syntactic Roles to Phrasal Elements
</tableCaption>
<table confidence="0.9962175">
Pattern to be Scanned New Pattern to be Generated
1 1PREI 4. 1N6i 4. IPNAT 4. [COMM} N61 4. IPNAL1* 4. [COMM}
0 [PRE}4. I &apos;......-
-- ADJV
..
2 [COMM] 4. IN1 4. IV71* 4. IN? [COMM] 4. I N I 4. [V7 1* 4. [N i
-- 4&apos; 0 0 — SUBJ GOV OBJ
3 ITOV7]* 4. JN} ITOV7 I* 4. 1 N 1
(I) • TOGOV OBJ
4 {ENT ITOVI 1ENV I* 4. [TOV1
0 4. ENGOV
5 4&apos; + IP. 1* + l!AJADJ4+ {SENT} 0 + IPAZ1* +
{SENT}
*: focus, --.: not mentioned, (I): empty, (...]: optional
</table>
<tableCaption confidence="0.765054">
Table 4 Rules for Constructing Clausal Elements
</tableCaption>
<table confidence="0.993128833333333">
Pattern to be Scanned New Element to be Generated
1 1 — V7 1* 4. I— I *
SUBJ1 + /GOV OBJ I SENT
2 i[EN]V61 4. [ — 1* 4 [—I* 1[ENIV61 4 1 --- 1*
--- TOGOV OBJ --- TOOBJ
3 11 + IEZV1* + f(TWOBJ1* 111,.:/lENADJI
</table>
<page confidence="0.948263">
162
</page>
<figure confidence="0.9864825">
Semantic filter
(a) and (d) are plausible.
</figure>
<figureCaption confidence="0.626261">
* X Y implies that X is modified by Y.
Figure 3 A Typical Operation of Semantic Filter
</figureCaption>
<listItem confidence="0.7891825">
(5) The aforementioned semantic filters are
compatible with syntax directed processors; i.e.
there is no need to reconstruct processors or to
modify internal representations. It is only
necessary to add filtrating programs to the
syntax directed processor.
</listItem>
<bodyText confidence="0.9975715">
One noteworthy point is that the thesaurus for
controlling the semantic fields or semantic
features of words should be constructed in an
appropriate form (such as word hierarchy) so as
to avoid the so-called combinatorial explosion of
the number of selective restriction rules.
</bodyText>
<listItem confidence="0.931038130434782">
(6) For the Japaneses sentence generating
process, it may be necessary to devise a very
complicated semantic processor if a system to
produce natural idiomatic Japanese sentences is
desired. But the majority of Japanese users may
tolerate awkward word-by-word translation and
understand its meaning. Thus we have concluded
that our research efforts should give priority to
the syntax directed analysis of English
sentences. The semantics directed generation of
Japanese sentences might not be an urgent issue;
rather it should be treated as a kind of profound
basic science to be studied without haste.
(7) Even though the output Japanese translation
may be an awkward word-by-word translation, it
should be composed of pertinent function words
and proper equivalents for content words.
Otherwise it could not express the proper meaning
of the input English sentences.
(8) In order to select proper equivalents,
semantic filters can be applied fairly
effectively to test the agreement among the
semantic codes assigned to words (or phrases).
</listItem>
<bodyText confidence="0.8638592">
Again the semantic filter is not always
complete. For example, in Figure 2, the verb
&amp;quot;yield&amp;quot; has at least two different meanings (and
consequently has at least two different Japanese
equivalents):
</bodyText>
<equation confidence="0.986761">
&amp;quot;yield&amp;quot;-Wproduce&amp;quot; (= Umidasu)
t&amp;quot;concede&amp;quot; (= Yuzuru).
</equation>
<bodyText confidence="0.983488818181818">
But it is neither easy nor certain how to
devise a filter to distinguish the above two
meanings mechanically. Thus we need some human
aids such as post-editing and inter-editing.
(9) As for the pertinent selection of function
words such as postpositions, there are no formal
computational rules to perform it. So we must
find and store heuristic rules empirically and
then make proper use of them.
Some heruistic rules to select appropriate
Japanese postpositions are shown in Table 5.
</bodyText>
<tableCaption confidence="0.98532">
Table 5 Heuristic Rules for Selecting
Postpositions for &amp;quot;in + N&amp;quot;
</tableCaption>
<table confidence="0.993488714285714">
Semantic Japanese Post- English Examples
Category of N positions for
ADVL/ADJV
in+N1 (N1=Place) Nl+de/Nl+niokeru in California
in+N3 (N3=Time) N3+ni/N3+no in Spring
in+NISN4 ---/N3E.N4+go-ni in two days
(N4=Quantity)
in+N6 N6+dewa/N6+no in my opinion
(N6=Abstract
Concept)
in+N8 (N8=Means) N8+de/N8+niyoru in Z-method
. No rules. +de/+no (speak) in English
. A kind of
idiom [7] to
be retrieved
directly from
a lexicon.
+wo-kite/ in uniform
+wo-kita
+wo-kakete/ in spectacles
+wo-kaketa
</table>
<listItem confidence="0.72566625">
(10) To get back to the previous findings (1)
and (2), the heuristic approach was also found to
be effective in segmenting the input English
sentence into a sequence of phrasal elements, and
</listItem>
<bodyText confidence="0.9145539">
in structuring them into a tree-like dependency
diagram (cf. Figure 2).
(11) A practical machine translation should be
considered from a kind of heuristic viewpoint
rather than from a purely rigid analytical
linguistic viewpoint. One persuasive reason for
this is the fact that humans, even foreign
language learners, can translate fairly difficult
English sentences without going into the details
of parsing problems.
</bodyText>
<sectionHeader confidence="0.999019666666667" genericHeader="method">
IV SEMANTICS DIRECTED APPROACH:
A PROTOTYPE JAPANESE-ENGLISH
MACHINE TRANSLATION SYSTEM
</sectionHeader>
<bodyText confidence="0.999762571428571">
The prototype model system for Japanese-
English translation is constructed as a semantics
directed processor using a conceptual dependency
diagram as the internal representation.
Noteworthy findings through operational
experience and efforts to improve on the
prototype model are as follows:
</bodyText>
<page confidence="0.996993">
163
</page>
<bodyText confidence="0.800350125">
(1) Considering some of the characteristics of
the Japanese language, such as flexible word
ordering and ambiguous usage of function words,
it is not advantageous to adopt a syntax directed
representation for the internal base of language
transformation.
For example, the following five Japanese
sentences have almost the same meaning except for
word ordering and a subtle nuance. Lowercase
letters represent function words.
Boku wa Fude de Tegami wo Kaku.
(I) (brush)(with)(letter) (write)
Boku wa tegami wo Fude de Kaku.
Fude de Boku wa Tegami wo Kaku.
Tegami wa Boku wa Fude de Kaku.
Boku wa Tegami wa Fude de Kaku.
</bodyText>
<listItem confidence="0.595208882352941">
(2) Therefore we have decided to adopt the
conceptual dependency diagram (CDD) as a compact
and powerful semantics directed internal
representation.
Our idea of the CDD is similar to the
well—known dependency grammar defined by Hays
[4] and Robinson [9] [10], except for the
augmented case markers which play essentially
semantic roles.
(3) The conceptual dependency diagram for
Japanese sentences is composed of predicate
phrase nodes (PPNs in abbreviation) and nominal
phrase nodes (NPNs in abbreviation). Each PPN
governs a few NPNs as its dependants. Even among
PPNs there exist some governor—dependant
relationships.
Examples of formal CDD description are:
</listItem>
<bodyText confidence="0.9195305">
PPN (NPN1, NPN2, NPNn),
Kaku (Boku, Tegami, Fude),
Write (I, Letter, Br7OTY,
where the underlined word &amp;quot;a&amp;quot; represents the
concept code corresponding to the superficial
word &amp;quot;a&amp;quot;, and the augmented case markers are
omitted.
In the avove description, the order of
dependants NI, N2, Nn are to be neglected.
For example,
PPN (NPNn, NPN2, NPN1)
is identical to the above first formula. This
convention may be different from the one defined
by Hays [4]. Our convention was introduced to
cope with the above—mentioned flexible word
ordering in Japanese sentences.
</bodyText>
<listItem confidence="0.90244375">
(4) The aforementioned dependency relationships
can be represented as a linking topology, where
each link has one governor node and one dependant
node as its top and bottom terminal point (Figure
4).
(5) The links are labeled with case markers.
Our case marker system is obtained by augmenting
the traditional case markers such as Fillmore&apos;s
</listItem>
<bodyText confidence="0.949501833333333">
[3] from the standpoint of machine translation.
For the PPN—NPN link, its label usually
represents agent, object, goal, location, topic,
etc. For the PPN—PPN link, its label is usually
represent causality, temporality,
restrictiveness, etc. (cf. Figure 4).
</bodyText>
<table confidence="0.7390132">
C4 117.
PPN Kaku Write
C/VIC&apos;.;\C*, Vi°\\I\
NPN1 NPN2 NPN2 Boku asImL Fade I Letter Brush
Ci: case marker
</table>
<figureCaption confidence="0.864084">
Figure 4 Examples of a Conceptual Dependency
Diagram (CDD)
</figureCaption>
<listItem confidence="0.994207933333333">
(6) As for the total number of case markers, our
current conclusion is that the number of
compulsory case markers to represent predicative
dominance should be small, say around 20; and
that the number of optional case markers to
represent adjective or adverbial modification
should be large, say from 50 to 70 (Table 6).
(7) The reason for the large number of optional
case markers is that the detailed classification
of optional cases is very useful for making an
appropriate selection of prepositions and
participles (Table 7).
(8) Each NPN is to be labeled with some properly
selected semantic features which are under the
control of a thesaurus type lexicon. Semantic
</listItem>
<bodyText confidence="0.91774632">
features are effective to disambiguate
predicative dependency so as to produce an
appropriate English verb phrase.
(9) The essential difference between a Japanese
sentence and the equivalent English sentence can
be grasped as the difference in the mode of PPN
selections, taken from the viewpoint of
conceptual dependency diagram (Figure 5). Once
an appropriate PPN selection is made, it will be
rather simple and mechanical to determine the
rest of the dependency topology.
(10) Thus the essential task of Japanese—English
translation can be reduced to the task of
constructing the rules for transforming the
dependency topology by changing PPNs, while
preserving the meaning of the original dependency
topology (cf. Figure 5).
(11) All the aforementioned findings have
something to do with the semantic directed
approach. Once the English oriented conceptual
dependency diagram is obtained, the rest of the
translation process is rather syntactic. That
is, the phrase structure generation can easily be
handled with somewhat traditional syntax directed
processors.
</bodyText>
<page confidence="0.996717">
164
</page>
<bodyText confidence="0.956205545454545">
(12) As is well known, the Japanese language has
a very high degree of complexity and ambiguity
mainly caused by frequent ellipsis and functional
multiplicity, which creates serious obstacles for
the achievement of a totally automatic treatment
of &amp;quot;raw&amp;quot; Japanese sentences.
(ex 1) &amp;quot;Sakana wa Taberu.&amp;quot;
(fish) (eat)
has at least two different interpretations:
. &amp;quot;[Sombody] can eat a fish.&amp;quot;
. &amp;quot;The fish may eat [something].&amp;quot;
</bodyText>
<tableCaption confidence="0.904832">
Table 6 Case Markers for CDD (subset only)
</tableCaption>
<table confidence="0.998651461538461">
Predicative A Agent
Dominance 0 Object
(Compulsory) C Complement
R Recipient
AC Agent in Causative
T Theme, Topic (Mental Subject)
P Partner
Q Quote
RI Range of Interest
RQ Range of Qualification
RM Range of Mention
I Instrument
E Element
... ...
Adverbial CT Goal in Abstract Collection
Modification CF Source in Abstract Collection
(Optional) TP Point in Time
... ...
Adjective ET Embedding Sentence Type Modifier
Modification EA whose gapping is Theme
(Optional) E0 --- whose gapping is Agent
... --- whose gapping is Object
...
Link and LA Linking through &amp;quot;AND&amp;quot;
Conjunction BT Conjunction through &amp;quot;BUT&amp;quot;
(Optional) ... ...
</table>
<listItem confidence="0.7795755">
• Kasoukioku-Akusesu-Hou niyori, Daiyouryou-Deitasetto
eno Kouritsu no Yoi Nyushutsuryoku ga Kanou ni Naru.
</listItem>
<subsectionHeader confidence="0.831154">
Analysis
</subsectionHeader>
<bodyText confidence="0.996883357142857">
(ex 2) &amp;quot;Kawaii Ningyou wo Motteiru Onnanoko.&amp;quot;
(lovely) (doll) (carry) (girl)
has also two different interpretations:
. &amp;quot;The lovely girl who carries a doll with
her.&amp;quot;
. &amp;quot;The girl who carries a lovely doll with
her.&amp;quot;
(13) Thus we have judged that some sub-Japanese
language should be constructed so as to restrict
the input Japanese sentences within a range of
clear tractable structures. The essential
restrictions given by the sub-language should be
concerned with the usage of function words and
sentential embeddings.
</bodyText>
<tableCaption confidence="0.992317">
Table 7 Detailed Classification of Optional Case
Markers for Modification (subset only)
</tableCaption>
<table confidence="0.997084">
Phase Code Most-Likely Prepositions or Participles
F from
T to, till
D during
P at
I in, inside
0 out, outside
V over, above
U under, below
S beside
B before, in front of
A after, behind
AL along
H through
AB over, superior to
SE apart from
WI within
... ...
</table>
<listItem confidence="0.4191165">
. Case Marker Body Code + Phase Code
. Body Code T (=Time)1S (=Space)IC (=Collection)
• The virtual storage access method enables the efficient
input-output processing to a large capacity data set.
</listItem>
<figure confidence="0.985836692307692">
/1 Generation
4) enable 4)
Naru
Kanou
Kasoukioku-
Akusesu-Hou
Nyushutsuryoku
5)
CT
NT
3)
5)
virtual storage
access method
Daiyouryou-
Deitasetto
5)&amp;quot;
Kouritsu
input-output
processing
Transformation
large capacity
data set
efficient
4)&amp;quot;
Naru(■Become)-type CDD Suru(=Make)-type CDD
</figure>
<figureCaption confidence="0.947784">
Figure 5 Difference between Japanese and English Grasped Through CDD
</figureCaption>
<page confidence="0.99444">
165
</page>
<bodyText confidence="0.964271111111111">
(14) A sub-language approach will not fetter the
users, if a Japanese-English translation system
is used as an English sentence composing aid for
Japanese people.
V CONCLUSION
We have found that there are some proper
approaches to the treatment of syntax and
semantics from the viewpoint of machine
translation. Our conclusions are as follows:
</bodyText>
<listItem confidence="0.902701">
(1) In order to construct a practical
English-Japanese machine translation system, it
is advantageous to take the syntax directed
approach, in which a syntactic role system plays
a central role, together with phrase structure
type internal representation (which we call RPM).
(2) In English-Japanese machine translation,
</listItem>
<bodyText confidence="0.864074025641026">
syntax should be treated in a heuristic manner
based on actual human translation methods.
Semantics plays an assistant role in
disambiguating the dependency among phrases.
(3) In English-Japanese machine translation, an
output Japanese sentence can be obtained directly
from the internal phrase structure representation
(RPM) which is essentially a structured set of
syntactic roles. Output sentences from the above
are, of course, a kind of literal translation of
stilted style, but no doubt they are
understandable enough for practical use.
(4) In order to construct a practical
Japanese-English machine translation system, it
is advantageous to take the approach in which
semantics plays a central role together with
conceptual dependency type internal
representation (which we call CDD).
(5) In Japanese-English machine translation,
augmented case markers play a powerful semantic
role.
(6) In Japanese-English machine translation, the
essential part of language transformation between
Japanese and English can be performed in terms of
changing dependency diagrams (COD) which involves
predicate replacements.
One further problem concerns establishing a
practical method of compensating a machine
translation system for its mistakes or
limitations caused by the intractable
complexities inherent to natural languages. This
problem may be solved through the concept of
sublanguage, pre-editing and post-editing to
modify source/target languages. The sub-Japanese
language approach in particular seems to be
effective for Japanese-English machine
translaton. One of our current interests is in a
proper treatment of syntax and semantics in the
sublanguage approach.
</bodyText>
<sectionHeader confidence="0.967058" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.9997524">
We would like to thank Prof. M. Nagao of Kyoto
University and Prof. H. Tanaka of Tokyo Institute
of Technology, for their kind and stimulative
discussion on various aspects of machine
translation. Thanks are also due to Dr. J.
Kawasaki, Dr. T. Mitsumaki and Dr. S. Mitsumori
of SDL Hitachi Ltd. for their constant
encouragement to this work, and Mr. F. Yamano and
Mr. A. Hirai for their enthusiastic assistance in
programming.
</bodyText>
<sectionHeader confidence="0.997961" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999936283018868">
[1] Chomsky, N., Aspects of the Theory of Syntax
(MIT Press, Cambridge, MA, 1965).
[2] Dowty, D.A. et. al., Introduction to Montague
Semantics (D. Reidel Publishing Company,
Dordrecht: Holland, Boston: U.S.A., London:
England, 1981)
[3] Fillmore, C.J., The Case for Case, in: Bach
and Harms (eds.), Universals in Linguistic
Theory, (Holt, Reinhart and Winston, 1968)
1-90
[4] Hays, D.C., Dependency Theory: A Formalism
and Some Observations, Language, vol.40,
no.4 (1964) 511-525
[5] Hornby, A.S., Guide to Patterns and Usage in
English, second edition (Oxford University
Press, London, 1975).
[6] Nitta, Y., Okajima, A. et. al., A Heuristic
Approach to English-into-Japanese Machine
Translation, COLING-82, Prague (1982) 283-288
[7] Okajima, A., Nitta, Y. et. al., Lexicon
Structure for Machine Translation, ICTP-83,
Tokyo (1983) 252-255
[8] Quirk et. al., A Grammar of Contemporary
English (Longman, London; Seminar Press, New
York, 1972).
[9] Robinson, J.J., Case, Category and
Configuration, Journal of Linguistics, vol.6
no.1 (1970) 57-80
[10] Robinson, J.J., Dependency Structures and
Transformational Rules, Language, vol.46,
no.2 (1970) 259-285
[11] Robinson, J.J., DIAGRAM: A Grammar for
Dialogues, Comm. ACM vol.25, no.1 (1982)
27-47.
[12] Sager, N., Natural Language Information
Processing (Addison Wesley, Reading, MA.,
1981).
[13] Schank, R.C., Reminding and Memory
Organization: An Introduction to MOPs, in:
Lehnert W.G. and Ringle, M.H. (eds.),
Strategies for Natural Language Processing
(Lawrence Erlbaum Associates, Publishers,
Hillsdale, New Jersey, London, 1982) 455-493
[14] Wilks, Y., Some Thoughts on Procedural
Semantics, in: ibid. 495-521
[151 Wilks, Y., An Artificial Intelligence
Approach to Machine Translation, in: Schank,
R.C. and Colby, K.M. (eds.), Computer Models
of Thought and Language (W.H. Freeman and
Company, San Francisco, 1973) 114-151
[16] Wilks, Y., Deep and Superficial Parsing, in:
King, M. (ed.), Parsing Natural Language
(Academic Press, London, 1983) 219-246
</reference>
<page confidence="0.998762">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.721286">
<title confidence="0.999766">A PROPER TREATMENT OF SYNTAX AND SEMANTICS IN MACHINE TRANSLATION</title>
<author confidence="0.873217">Yoshihiko Nitta</author>
<author confidence="0.873217">Atsushi Okajima</author>
<author confidence="0.873217">Hiroyuki Kaji</author>
<author confidence="0.873217">Youichi Hidano</author>
<author confidence="0.873217">Koichiro Ishihara</author>
<affiliation confidence="0.994972">Systems Development Laboratory, Hitachi, Ltd.</affiliation>
<address confidence="0.986154">1099 Ohzenji Asao-ku, Kawasaki-shi, 215 JAPAN</address>
<abstract confidence="0.998476058823529">A proper treatment of syntax and semantics in machine translation is introduced and discussed from the empirical viewpoint. For English- Japanese machine translation, the syntax directed approach is effective where the Heuristic Parsing Model (HPM) and the Syntactic Role System play important roles. For Japanese-English translation, the semantics directed approach is powerful where the Conceptual Dependency Diagram (CDD) and the Augmented Case Marker System (which is a kind of Semantic Role System) play essential roles. Some examples of the difference between Japanese sentence structure and English sentence structure, which is vital to machine translation, are also discussed together with various interesting ambiguities.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax</title>
<date>1965</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="10705" citStr="[1]" startWordPosition="1596" endWordPosition="1596">e of word elements (WE&apos;s)) to get phrasal elements (PE), and an example of bottom-up operation would be the construction of tree-forms or link-forms to get clausal elements (CE) or a sentence (SE). These operations are supported by syntax directed grammatical data such as verb dependency type codes (cf. Table 1, which is a simplified version of Hornby&apos;s classification [51), syntactic role codes (Table 2) and some production rule type grammars (Table 3 &amp; Table 4). It may be permissible to say that all these syntactic data are fairly compact and the kernel parts are already well-elaborated (cf. [1], [81. [11], [121). 161 Table 1 Dependency Pattern of Verb Code Verb Pattern Examples VI Be + ... be V2 Vi (A. Be) + Complement, get, look It/There + Vi + ... V3 Vi [+ Adverbial Modifier] rise, walk V6 Vt + To-infinitive intend V7 Vt + Object begin, yield V8 Vt + that + ... agree, think V14 Vt + Object (+not] + know, bring To-infinitive Table 2 Syntactic Roles Code Role SUBJ Subject OBJ Object TOOBJ Object in To-infinitive Form NAPP Noun in Apposition GOV Governing Verb TOGOV Governing Verb in To-infinitive Form ENGOV Governing Verb in Past Participle Form ADJV Adjectival ENADJ Adjectival in P</context>
</contexts>
<marker>[1]</marker>
<rawString>Chomsky, N., Aspects of the Theory of Syntax (MIT Press, Cambridge, MA, 1965).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Dowty</author>
</authors>
<title>Introduction to Montague Semantics (D.</title>
<date>1981</date>
<publisher>Reidel Publishing Company,</publisher>
<location>Dordrecht: Holland, Boston: U.S.A., London: England,</location>
<contexts>
<context position="3261" citStr="[2]" startWordPosition="464" endWordPosition="464">esentation is fairly independent of the differences in language classes. (3) Consequently, semantics directed internal representation can produce accurate translations. The disadvantages of the semantics directed approach are as follows: (1) It is not easy to construct a semantic lexicon which covers real world phenomena of a reasonably wide range. The main reason for this difficulty is that a well-established and widely-accepted method of describing semantics does not exist. (For strongly restricted statements or topics, of course, there exist well-elaborated methods such as Montague grammar [2], Script and MOP (Memory Organization Packet) theory [13], Procedural Semantics [14], and Semantic Interlingual Representation [15].) (2) The second but intractable problem is that, even if you could devise a fairly acceptable method to describe semantics, the total number of semantic rule descriptions may expand beyond all manageable limits. Therefore, we think that it is necessary to seek proper combinations of syntactic processing and semantic processing so as to compensate for the disadvantages of each. The purpose of this paper is to propose a proper treatment of syntax and semantics in m</context>
</contexts>
<marker>[2]</marker>
<rawString>Dowty, D.A. et. al., Introduction to Montague Semantics (D. Reidel Publishing Company, Dordrecht: Holland, Boston: U.S.A., London: England, 1981)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>The Case for Case,</title>
<date>1968</date>
<booktitle>Universals in Linguistic Theory,</booktitle>
<pages>1--90</pages>
<editor>in: Bach and Harms (eds.),</editor>
<location>Holt, Reinhart and Winston,</location>
<contexts>
<context position="19176" citStr="[3]" startWordPosition="2969" endWordPosition="2969">ted. For example, PPN (NPNn, NPN2, NPN1) is identical to the above first formula. This convention may be different from the one defined by Hays [4]. Our convention was introduced to cope with the above—mentioned flexible word ordering in Japanese sentences. (4) The aforementioned dependency relationships can be represented as a linking topology, where each link has one governor node and one dependant node as its top and bottom terminal point (Figure 4). (5) The links are labeled with case markers. Our case marker system is obtained by augmenting the traditional case markers such as Fillmore&apos;s [3] from the standpoint of machine translation. For the PPN—NPN link, its label usually represents agent, object, goal, location, topic, etc. For the PPN—PPN link, its label is usually represent causality, temporality, restrictiveness, etc. (cf. Figure 4). C4 117. PPN Kaku Write C/VIC&apos;.;\C*, Vi°\\I\ NPN1 NPN2 NPN2 Boku asImL Fade I Letter Brush Ci: case marker Figure 4 Examples of a Conceptual Dependency Diagram (CDD) (6) As for the total number of case markers, our current conclusion is that the number of compulsory case markers to represent predicative dominance should be small, say around 20; </context>
</contexts>
<marker>[3]</marker>
<rawString>Fillmore, C.J., The Case for Case, in: Bach and Harms (eds.), Universals in Linguistic Theory, (Holt, Reinhart and Winston, 1968) 1-90</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Hays</author>
</authors>
<title>Dependency Theory: A Formalism and Some Observations,</title>
<date>1964</date>
<pages>511--525</pages>
<location>Language, vol.40, no.4</location>
<contexts>
<context position="17848" citStr="[4]" startWordPosition="2762" endWordPosition="2762">n. For example, the following five Japanese sentences have almost the same meaning except for word ordering and a subtle nuance. Lowercase letters represent function words. Boku wa Fude de Tegami wo Kaku. (I) (brush)(with)(letter) (write) Boku wa tegami wo Fude de Kaku. Fude de Boku wa Tegami wo Kaku. Tegami wa Boku wa Fude de Kaku. Boku wa Tegami wa Fude de Kaku. (2) Therefore we have decided to adopt the conceptual dependency diagram (CDD) as a compact and powerful semantics directed internal representation. Our idea of the CDD is similar to the well—known dependency grammar defined by Hays [4] and Robinson [9] [10], except for the augmented case markers which play essentially semantic roles. (3) The conceptual dependency diagram for Japanese sentences is composed of predicate phrase nodes (PPNs in abbreviation) and nominal phrase nodes (NPNs in abbreviation). Each PPN governs a few NPNs as its dependants. Even among PPNs there exist some governor—dependant relationships. Examples of formal CDD description are: PPN (NPN1, NPN2, NPNn), Kaku (Boku, Tegami, Fude), Write (I, Letter, Br7OTY, where the underlined word &amp;quot;a&amp;quot; represents the concept code corresponding to the superficial word &amp;quot;</context>
</contexts>
<marker>[4]</marker>
<rawString>Hays, D.C., Dependency Theory: A Formalism and Some Observations, Language, vol.40, no.4 (1964) 511-525</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Hornby</author>
</authors>
<title>Guide to Patterns and Usage in English, second edition</title>
<date>1975</date>
<publisher>Oxford University Press,</publisher>
<location>London,</location>
<marker>[5]</marker>
<rawString>Hornby, A.S., Guide to Patterns and Usage in English, second edition (Oxford University Press, London, 1975).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Nitta</author>
<author>A Okajima</author>
</authors>
<title>A Heuristic Approach to English-into-Japanese Machine Translation, COLING-82,</title>
<date>1982</date>
<pages>283--288</pages>
<location>Prague</location>
<contexts>
<context position="5768" citStr="[6]" startWordPosition="835" endWordPosition="835">ct, object, complement, etc., then we will call this processor a syntax directed processor, and if these labels are something like agent, object, instrument, etc., or animate, inanimate, concrete, abstract, human, etc., then we will call this processor a semantics directed processor. The above definition is oversimplified and of course incomplete, but it is still enough for the arguments in this paper. III SYNTAX DIRECTED APPROACH: A PROTOTYPE ENGLISH-JAPANESE MACHINE TRANSLATION SYSTEM So far we have developed two prototype machine translation systems; one is for English-Japanese translation [6] and the other is for JapaneseEnglish translation. The prototype model system for EnglishJapanese translation (Figure 1) is constructed as a syntax directed processor using a phrase structure type internal representation called HPM (Heuristic Parsing Model), where the semantics is utilized to disambiguate dependency relationships. The somewhat new name HPM (Heuristic Parsing Model) reflects the parsing strategy by which the machine translation tries to simultate the heuristic way of actual human of language translation. The essential features of heuristic translation are summarized in the foll</context>
<context position="6996" citStr="[6]" startWordPosition="1010" endWordPosition="1010"> segment an input sentence into phrasal elements (PE) and clausal elements (CE). (2) To assign syntactic roles to PE&apos;s and CE&apos;s, and restructure the segmented elements into tree-forms by governing relation, and into link-forms by modifying relation. (3) To permute the segmented elements, and to assign appropriate Japanese equivalents with necessary case suffixes and postpositions. Noteworthy findings from operational experience and efforts to improve the prototype model are as follows: Input English Sentence Output Japanese Sentence Figure 1 Configuration of Machine Translation System: ATHENE [6] Lexicons PI &apos;entry: • word • phrase • idiom etc. description: • attribute • Japanese equivalents • controlling marks for analysis, transformation and generation • etc. Lexicon Retrieval Morphological Analysis ....dSyntactic Analysis [based on HPM] Internal Language Representation [based on HPM] Grammatical Data [Parsed Tree/Link] Post-editing Support [&apos;solution to manifold] meanings Tree/Link Transformation Sentence Generation Morphological Synthesis [&apos;adjustment of tense and mode &apos;assignment of postpositions 160 • WE: Word Element • PE; Phrasal Element • CP: Clausal Element • SE: Sentence • </context>
</contexts>
<marker>[6]</marker>
<rawString>Nitta, Y., Okajima, A. et. al., A Heuristic Approach to English-into-Japanese Machine Translation, COLING-82, Prague (1982) 283-288</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Okajima</author>
<author>Y Nitta</author>
</authors>
<date>1983</date>
<booktitle>Lexicon Structure for Machine Translation, ICTP-83,</booktitle>
<pages>252--255</pages>
<location>Tokyo</location>
<contexts>
<context position="15891" citStr="[7]" startWordPosition="2463" endWordPosition="2463">and store heuristic rules empirically and then make proper use of them. Some heruistic rules to select appropriate Japanese postpositions are shown in Table 5. Table 5 Heuristic Rules for Selecting Postpositions for &amp;quot;in + N&amp;quot; Semantic Japanese Post- English Examples Category of N positions for ADVL/ADJV in+N1 (N1=Place) Nl+de/Nl+niokeru in California in+N3 (N3=Time) N3+ni/N3+no in Spring in+NISN4 ---/N3E.N4+go-ni in two days (N4=Quantity) in+N6 N6+dewa/N6+no in my opinion (N6=Abstract Concept) in+N8 (N8=Means) N8+de/N8+niyoru in Z-method . No rules. +de/+no (speak) in English . A kind of idiom [7] to be retrieved directly from a lexicon. +wo-kite/ in uniform +wo-kita +wo-kakete/ in spectacles +wo-kaketa (10) To get back to the previous findings (1) and (2), the heuristic approach was also found to be effective in segmenting the input English sentence into a sequence of phrasal elements, and in structuring them into a tree-like dependency diagram (cf. Figure 2). (11) A practical machine translation should be considered from a kind of heuristic viewpoint rather than from a purely rigid analytical linguistic viewpoint. One persuasive reason for this is the fact that humans, even foreign l</context>
</contexts>
<marker>[7]</marker>
<rawString>Okajima, A., Nitta, Y. et. al., Lexicon Structure for Machine Translation, ICTP-83, Tokyo (1983) 252-255</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quirk</author>
</authors>
<title>A Grammar of Contemporary</title>
<date>1972</date>
<publisher>English (Longman, London; Seminar Press,</publisher>
<location>New York,</location>
<marker>[8]</marker>
<rawString>Quirk et. al., A Grammar of Contemporary English (Longman, London; Seminar Press, New York, 1972).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Robinson</author>
<author>Case</author>
</authors>
<title>Category and Configuration,</title>
<date>1970</date>
<journal>Journal of Linguistics,</journal>
<volume>6</volume>
<pages>57--80</pages>
<contexts>
<context position="17865" citStr="[9]" startWordPosition="2765" endWordPosition="2765">he following five Japanese sentences have almost the same meaning except for word ordering and a subtle nuance. Lowercase letters represent function words. Boku wa Fude de Tegami wo Kaku. (I) (brush)(with)(letter) (write) Boku wa tegami wo Fude de Kaku. Fude de Boku wa Tegami wo Kaku. Tegami wa Boku wa Fude de Kaku. Boku wa Tegami wa Fude de Kaku. (2) Therefore we have decided to adopt the conceptual dependency diagram (CDD) as a compact and powerful semantics directed internal representation. Our idea of the CDD is similar to the well—known dependency grammar defined by Hays [4] and Robinson [9] [10], except for the augmented case markers which play essentially semantic roles. (3) The conceptual dependency diagram for Japanese sentences is composed of predicate phrase nodes (PPNs in abbreviation) and nominal phrase nodes (NPNs in abbreviation). Each PPN governs a few NPNs as its dependants. Even among PPNs there exist some governor—dependant relationships. Examples of formal CDD description are: PPN (NPN1, NPN2, NPNn), Kaku (Boku, Tegami, Fude), Write (I, Letter, Br7OTY, where the underlined word &amp;quot;a&amp;quot; represents the concept code corresponding to the superficial word &amp;quot;a&amp;quot;, and the augme</context>
</contexts>
<marker>[9]</marker>
<rawString>Robinson, J.J., Case, Category and Configuration, Journal of Linguistics, vol.6 no.1 (1970) 57-80</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Robinson</author>
</authors>
<title>Dependency Structures and Transformational Rules,</title>
<date>1970</date>
<journal>Language,</journal>
<volume>46</volume>
<pages>259--285</pages>
<contexts>
<context position="17870" citStr="[10]" startWordPosition="2766" endWordPosition="2766">ollowing five Japanese sentences have almost the same meaning except for word ordering and a subtle nuance. Lowercase letters represent function words. Boku wa Fude de Tegami wo Kaku. (I) (brush)(with)(letter) (write) Boku wa tegami wo Fude de Kaku. Fude de Boku wa Tegami wo Kaku. Tegami wa Boku wa Fude de Kaku. Boku wa Tegami wa Fude de Kaku. (2) Therefore we have decided to adopt the conceptual dependency diagram (CDD) as a compact and powerful semantics directed internal representation. Our idea of the CDD is similar to the well—known dependency grammar defined by Hays [4] and Robinson [9] [10], except for the augmented case markers which play essentially semantic roles. (3) The conceptual dependency diagram for Japanese sentences is composed of predicate phrase nodes (PPNs in abbreviation) and nominal phrase nodes (NPNs in abbreviation). Each PPN governs a few NPNs as its dependants. Even among PPNs there exist some governor—dependant relationships. Examples of formal CDD description are: PPN (NPN1, NPN2, NPNn), Kaku (Boku, Tegami, Fude), Write (I, Letter, Br7OTY, where the underlined word &amp;quot;a&amp;quot; represents the concept code corresponding to the superficial word &amp;quot;a&amp;quot;, and the augmented </context>
</contexts>
<marker>[10]</marker>
<rawString>Robinson, J.J., Dependency Structures and Transformational Rules, Language, vol.46, no.2 (1970) 259-285</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Robinson</author>
</authors>
<title>DIAGRAM: A Grammar for Dialogues,</title>
<date>1982</date>
<journal>Comm. ACM</journal>
<volume>25</volume>
<pages>1</pages>
<contexts>
<context position="10716" citStr="[11]" startWordPosition="1598" endWordPosition="1598">elements (WE&apos;s)) to get phrasal elements (PE), and an example of bottom-up operation would be the construction of tree-forms or link-forms to get clausal elements (CE) or a sentence (SE). These operations are supported by syntax directed grammatical data such as verb dependency type codes (cf. Table 1, which is a simplified version of Hornby&apos;s classification [51), syntactic role codes (Table 2) and some production rule type grammars (Table 3 &amp; Table 4). It may be permissible to say that all these syntactic data are fairly compact and the kernel parts are already well-elaborated (cf. [1], [81. [11], [121). 161 Table 1 Dependency Pattern of Verb Code Verb Pattern Examples VI Be + ... be V2 Vi (A. Be) + Complement, get, look It/There + Vi + ... V3 Vi [+ Adverbial Modifier] rise, walk V6 Vt + To-infinitive intend V7 Vt + Object begin, yield V8 Vt + that + ... agree, think V14 Vt + Object (+not] + know, bring To-infinitive Table 2 Syntactic Roles Code Role SUBJ Subject OBJ Object TOOBJ Object in To-infinitive Form NAPP Noun in Apposition GOV Governing Verb TOGOV Governing Verb in To-infinitive Form ENGOV Governing Verb in Past Participle Form ADJV Adjectival ENADJ Adjectival in Past Partici</context>
</contexts>
<marker>[11]</marker>
<rawString>Robinson, J.J., DIAGRAM: A Grammar for Dialogues, Comm. ACM vol.25, no.1 (1982) 27-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
</authors>
<title>Natural Language Information Processing</title>
<date>1981</date>
<publisher>Addison Wesley,</publisher>
<location>Reading, MA.,</location>
<marker>[12]</marker>
<rawString>Sager, N., Natural Language Information Processing (Addison Wesley, Reading, MA., 1981).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Reminding and Memory Organization: An Introduction to MOPs,</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing (Lawrence Erlbaum Associates, Publishers,</booktitle>
<pages>455--493</pages>
<editor>in: Lehnert W.G. and Ringle, M.H. (eds.),</editor>
<location>Hillsdale, New Jersey, London,</location>
<contexts>
<context position="3318" citStr="[13]" startWordPosition="472" endWordPosition="472">anguage classes. (3) Consequently, semantics directed internal representation can produce accurate translations. The disadvantages of the semantics directed approach are as follows: (1) It is not easy to construct a semantic lexicon which covers real world phenomena of a reasonably wide range. The main reason for this difficulty is that a well-established and widely-accepted method of describing semantics does not exist. (For strongly restricted statements or topics, of course, there exist well-elaborated methods such as Montague grammar [2], Script and MOP (Memory Organization Packet) theory [13], Procedural Semantics [14], and Semantic Interlingual Representation [15].) (2) The second but intractable problem is that, even if you could devise a fairly acceptable method to describe semantics, the total number of semantic rule descriptions may expand beyond all manageable limits. Therefore, we think that it is necessary to seek proper combinations of syntactic processing and semantic processing so as to compensate for the disadvantages of each. The purpose of this paper is to propose a proper treatment of syntax and semantics in machine translation systems from a heuristic viewpoint, to</context>
</contexts>
<marker>[13]</marker>
<rawString>Schank, R.C., Reminding and Memory Organization: An Introduction to MOPs, in: Lehnert W.G. and Ringle, M.H. (eds.), Strategies for Natural Language Processing (Lawrence Erlbaum Associates, Publishers, Hillsdale, New Jersey, London, 1982) 455-493</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Some Thoughts on Procedural Semantics, in: ibid. 495-521 [151 Wilks, Y., An Artificial Intelligence Approach to Machine Translation,</title>
<date>1973</date>
<booktitle>Computer Models of Thought and Language (W.H. Freeman and Company,</booktitle>
<pages>114--151</pages>
<editor>in: Schank, R.C. and Colby, K.M. (eds.),</editor>
<location>San Francisco,</location>
<contexts>
<context position="3345" citStr="[14]" startWordPosition="475" endWordPosition="475">uently, semantics directed internal representation can produce accurate translations. The disadvantages of the semantics directed approach are as follows: (1) It is not easy to construct a semantic lexicon which covers real world phenomena of a reasonably wide range. The main reason for this difficulty is that a well-established and widely-accepted method of describing semantics does not exist. (For strongly restricted statements or topics, of course, there exist well-elaborated methods such as Montague grammar [2], Script and MOP (Memory Organization Packet) theory [13], Procedural Semantics [14], and Semantic Interlingual Representation [15].) (2) The second but intractable problem is that, even if you could devise a fairly acceptable method to describe semantics, the total number of semantic rule descriptions may expand beyond all manageable limits. Therefore, we think that it is necessary to seek proper combinations of syntactic processing and semantic processing so as to compensate for the disadvantages of each. The purpose of this paper is to propose a proper treatment of syntax and semantics in machine translation systems from a heuristic viewpoint, together with persuasive exam</context>
</contexts>
<marker>[14]</marker>
<rawString>Wilks, Y., Some Thoughts on Procedural Semantics, in: ibid. 495-521 [151 Wilks, Y., An Artificial Intelligence Approach to Machine Translation, in: Schank, R.C. and Colby, K.M. (eds.), Computer Models of Thought and Language (W.H. Freeman and Company, San Francisco, 1973) 114-151</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
</authors>
<title>Deep and Superficial Parsing,</title>
<date>1983</date>
<booktitle>Parsing Natural Language</booktitle>
<pages>219--246</pages>
<editor>in: King, M. (ed.),</editor>
<publisher>Academic Press,</publisher>
<location>London,</location>
<contexts>
<context position="4362" citStr="[16]" startWordPosition="625" endWordPosition="625">ntages of each. The purpose of this paper is to propose a proper treatment of syntax and semantics in machine translation systems from a heuristic viewpoint, together with persuasive examples obtained through operating experiences. A sub-language approach which would put some moderate restrictions on the syntax and semantics of source language is also discussed. 159 II SYNTAX AND SEMANTICS It is not entirely possible to distinguish a syntax directed approach from a semantics directed approach, because syntax and semantics are always performing their linguistic functions reciprocally. As Wilks [16] points out, it is plausible but a great mistake to identify syntactic processing with superficial processing, or to identify semantic processing with deep processing. The term &amp;quot;superficial&amp;quot; or &amp;quot;deep&amp;quot; only reflects the intuitive distance from the language representation in (superficial) character strings or from the language representation in our (deep) minds. Needless to say, machine translation inevitably has something to do with superficial processing. In various aspects of natural language processing, it is quite common to segment a superficial sentence into a collection of phrases. A phra</context>
</contexts>
<marker>[16]</marker>
<rawString>Wilks, Y., Deep and Superficial Parsing, in: King, M. (ed.), Parsing Natural Language (Academic Press, London, 1983) 219-246</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>