<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.995166">
Learning Features that Predict Cue Usage
</title>
<author confidence="0.978125">
Barbara Di Eugenio* Johanna D. Mooret Massimo PaolucciI
</author>
<affiliation confidence="0.819191">
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.480359">
{dieugeni,jmoore,paolucci}Ocs.pitt.edu
</email>
<sectionHeader confidence="0.991165" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998655625">
Our goal is to identify the features that pre-
dict the occurrence and placement of dis-
course cues in tutorial explanations in or-
der to aid in the automatic generation of
explanations. Previous attempts to devise
rules for text generation were based on in-
tuition or small numbers of constructed ex-
amples. We apply a machine learning pro-
gram, C4.5, to induce decision trees for cue
occurrence and placement from a corpus of
data coded for a variety of features previ-
ously thought to affect cue usage. Our ex-
periments enable us to identify the features
with most predictive power, and show that
machine learning can be used to induce de-
cision trees useful for text generation.
</bodyText>
<sectionHeader confidence="0.999392" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.961185">
Discourse cues are words or phrases, such as because,
first, and although, that mark structural and seman-
tic relationships between discourse entities. They
play a crucial role in many discourse processing
tasks, including plan recognition (Litman and Allen,
1987), text comprehension (Cohen, 1984; Hobbs,
1985; Mann and Thompson, 1986; Reichman-Adar,
1984), and anaphora resolution (Grosz and Sidner,
1986). Moreover, research in reading comprehension
indicates that felicitous use of cues improves compre-
hension and recall (Goldman, 1988), but that their
indiscriminate use may have detrimental effects on
recall (Mills, Graesser, and Haberlandt, 1993).
Our goal is to identify general strategies for cue us-
age that can be implemented for automatic text gen-
eration. From the generation perspective, cue usage
consists of three distinct, but interrelated problems:
</bodyText>
<listItem confidence="0.689313428571429">
(1) occurrence: whether or not to include a cue in the
generated text, (2) placement: where the cue should
be placed in the text, and (3) selection: what lexical
item(s) should be used.
Prior work in text generation has focused on cue
selection (McKeown and Elhadad, 1991; Elhadad
and McKeown, 1990), or on the relation between
</listItem>
<affiliation confidence="0.8169095">
*Learning Research St Development Center
tComputer Science Department, and Learning Re-
search 8/ Development Center
*Intelligent Systems Program
</affiliation>
<bodyText confidence="0.999888306122449">
cue occurrence and placement and specific rhetori-
cal structures (Rosner and Stede, 1992; Scott and
de Souza, 1990; Vander Linden and Martin, 1995).
Other hypotheses about cue usage derive from work
on discourse coherence and structure. Previous
research (Hobbs, 1985; Grosz and Sidner, 1986;
Schiffrin, 1987; Mann and Thompson, 1988; Elhadad
and McKeown, 1990), which has been largely de-
scriptive, suggests factors such as structural features
of the discourse (e.g., level of embedding and segment
complexity), intentional and informational relations
in that structure, ordering of relata, and syntactic
form of discourse constituents.
Moser and Moore (1995; 1997) coded a corpus
of naturally occurring tutorial explanations for the
range of features identified in prior work. Because
they were also interested in the contrast between oc-
currence and non-occurrence of cues, they exhaus-
tively coded for all of the factors thought to con-
tribute to cue usage in all of the text. From their
study, Moser and Moore identified several interesting
correlations between particular features and specific
aspects of cue usage, and were able to test specific
hypotheses from the literature that were based on
constructed examples.
In this paper, we focus on cue occurrence and
placement, and present an empirical study of the hy-
potheses provided by previous research, which have
never been systematically evaluated with naturally
occurring data. We use a machine learning program,
C4.5 (Quinlan, 1993), on the tagged corpus of Moser
and Moore to induce decision trees. The number of
coded features and their interactions makes the man-
ual construction of rules that predict cue occurrence
and placement an intractable task.
Our results largely confirm the suggestions from
the literature, and clarify them by highlighting the
most influential features for a particular task. Dis-
course structure, in terms of both segment structure
and levels of embedding, affects cue occurrence the
most; intentional relations also play an important
role. For cue placement, the most important factors
are syntactic structure and segment complexity.
The paper is organized as follows. In Section 2 we
discuss previous research in more detail. Section 3
provides an overview of Moser and Moore&apos;s coding
scheme. In Section 4 we present our learning exper-
iments, and in Section 5 we discuss our results and
conclude.
</bodyText>
<page confidence="0.997174">
80
</page>
<sectionHeader confidence="0.999813" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999891709677419">
McKeown and Elhadad (1991; 1990) studied several
connectives (e.g., but, since, because), and include
many insightful hypotheses about cue selection; their
observation that the distinction between but and al-
though depends on the point of the move is related
to the notion of core discussed below. However, they
do not address the problem of cue occurrence.
Other researchers (ROsner and Stede, 1992; Scott
and de Souza, 1990) are concerned with generating
text from &amp;quot;RST trees&amp;quot;, hierarchical structures where
leaf nodes contain content and internal nodes indi-
cate the rhetorical relations, as defined in Rhetori-
cal Structure Theory (RST) (Mann and Thompson,
1988), that exist between subtrees. They proposed
heuristics for including and choosing cues based on
the rhetorical relation between spans of text, the or-
der of the relata, and the complexity of the related
text spans. However, (Scott and de Souza, 1990)
was based on a small number of constructed exam-
ples, and (11,5sner and Stede, 1992) focused on a small
number of RST relations.
(Litman, 1996) and (Siegel and McKeown, 1994)
have applied machine learning to disambiguate be-
tween the discourse and sentential usages of cues;
however, they do not consider the issues of occur-
rence and placement, and approach the problem from
the point of view of interpretation. We closely follow
the approach in (Litman, 1996) in two ways. First,
we use C4.5. Second, we experiment first with each
feature individually, and then with &amp;quot;interesting&amp;quot; sub-
sets of features.
</bodyText>
<sectionHeader confidence="0.997897" genericHeader="method">
3 Relational Discourse Analysis
</sectionHeader>
<bodyText confidence="0.992591714285715">
This section briefly describes Relational Discourse
Analysis (RDA) (Moser, Moore, and Glendening,
1996), the coding scheme used to tag the data for
our machine learning experiments.&apos;
RDA is a scheme devised for analyzing tutorial ex-
planations in the domain of electronics troubleshoot-
ing. It synthesizes ideas from (Grosz and Sidner,
1986) and from RST (Mann and Thompson, 1988).
Coders use RDA to exhaustively analyze each expla-
nation in the corpus, i.e., every word in each expla-
nation belongs to exactly one element in the anal-
ysis. An explanation may consist of multiple seg-
ments. Each segment originates with an intention
of the speaker. Segments are internally structured
and consist of a core, i.e., that element that most di-
rectly expresses the segment purpose, and any num-
ber of contributors, i.e. the remaining constituents.
For each contributor, one analyzes its relation to the
core from an intentional perspective, i.e., how it is
intended to support the core, and from an informa-
tional perspective, i.e., how its content relates to that
For more detail about the RDA coding scheme see
(Moser and Moore, 1995; Moser and Moore, 1997).
of the core. The set of intentional relations in RDA
is a modification of the presentational relations of
RST, while informational relations are similar to the
subject matter relations in RST. Each segment con-
stituent, both core and contributors, may itself be a
segment with a core:contributor structure. In some
cases the core is not explicit. This is often the case
with the whole tutor&apos;s explanation, since its purpose
is to answer the student&apos;s explicit question.
As an example of the application of RDA, consider
the partial tutor explanation in (1)2. The purpose of
this segment is to inform the student that she made
the strategy error of testing inside part3 too soon.
The constituent that makes the purpose obvious, in
this case (1-B), is the core of the segment. The other
constituents help to serve the segment purpose by
contributing to it. (1-C) is an example of subsegment
with its own core:contributor structure; its purpose
is to give a reason for testing part2 first.
The RDA analysis of (1) is shown schematically in
Figure 1. The core is depicted as the mother of all
the relations it participates in. Each relation node is
labeled with both its intentional and informational
relation, with the order of relata in the label indicat-
ing the linear order in the discourse. Each relation
node has up to two daughters: the cue, if any, and
the contributor, in the order they appear in the dis-
course.
Coders analyze each explanation in the corpus and
enter their analyses into a database. The corpus con-
sists of 854 clauses comprising 668 segments, for a
total of 780 relations. Table 1 summarizes the dis-
tribution of different relations, and the number of
cued relations in each category. Joints are segments
comprising more than one core, but no contributor;
clusters are multiunit structures with no recogniz-
able core:contributor relation. (1-B) is a cluster com-
posed of two units (the two clauses), related only at
the informational level by a temporal relation. Both
clauses describe actions, with the first action descrip-
tion embedded in a matrix (&amp;quot;You should&amp;quot;). Cues are
much more likely to occur in clusters, where only in-
formational relations occur, than in core:contributor
structures, where intentional and informational rela-
tions co-occur (x2 = 33.367, p &lt;.001, df = I). In
the following, we will not discuss joints and clusters
any further.
An important result pointed out by (Moser and
Moore, 1995) is that cue placement depends on core
position. When the core is first and a cue is asso-
ciated with the relation, the cue never occurs with
the core. In contrast, when the core is second, if a
cue occurs, it can occur either on the core or on the
contributor.
</bodyText>
<footnote confidence="0.961565666666667">
2To make the example more intelligible, we replaced
references to parts of the circuit with the labels part/,
part2 and part3.
</footnote>
<page confidence="0.998569">
81
</page>
<bodyText confidence="0.996862666666667">
you know that partl is good,
you should eliminate part2
before troubleshooting inside part3.
</bodyText>
<equation confidence="0.753151">
1. part2 is moved frequently
</equation>
<bodyText confidence="0.970998">
and thus 2. is more susceptible to damage than part3.
it is more work to open up part3 for testing
the process of opening drawers and extending cards in part3
may induce problems which did not already exist.
</bodyText>
<figure confidence="0.9959289375">
Although A.
B.
This is
(1) because C.
Also, D.
and E.
B. you should eliminate part2
before troubleshooting inside part3
concede convince convince convince
criterion:act act:reason act:reason act:reason
Although A (This is) C.2 and E
because
convince
cause:effect
C.1 and
thus
</figure>
<figureCaption confidence="0.999975">
Figure 1: The RDA analysis of (1)
</figureCaption>
<sectionHeader confidence="0.878431" genericHeader="method">
4 Learning from the corpus
</sectionHeader>
<subsectionHeader confidence="0.995657">
4.1 The algorithm
</subsectionHeader>
<bodyText confidence="0.999950272727273">
We chose the C4.5 learning algorithm (Quinlan,
1993) because it is well suited to a domain such as
ours with discrete valued attributes. Moreover, C4.5
produces decision trees and rule sets, both often used
in text generation to implement mappings from func-
tion features to forms.3 Finally, C4.5 is both read-
ily available, and is a benchmark learning algorithm
that has been extensively used in NLP applications,
e.g. (Litman, 1996; Mooney, 1996; Vander Linden
and Di Eugenio, 1996).
As our dataset is small, the results we report are
based on cross-validation, which (Weiss and Ku-
likowski, 1991) recommends as the best method to
evaluate decision trees on datasets whose cardinality
is in the hundreds. Data for learning should be di-
vided into training and test sets; however, for small
datasets this has the disadvantage that a sizable por-
tion of the data is not available for learning. Cross-
validation obviates this problem by running the algo-
rithm N times (N=10 is a typical value): in each run,
of the data, randomly chosen, is used as the
training set, and the remaining Ttith used as the test
</bodyText>
<footnote confidence="0.422451">
3We will discuss only decision trees here.
</footnote>
<bodyText confidence="0.999950111111111">
set. The error rate of a tree obtained by using the
whole dataset for training is then assumed to be the
average error rate on the test set over the N runs.
Further, as C4.5 prunes the initial tree it obtains to
avoid overfitting, it computes both actual and esti-
mated error rates for the pruned tree; see (Quinlan,
1993, Ch. 4) for details. Thus, below we will report
the average estimated error rate on the test set, as
computed by 10-fold cross-validation experiments.
</bodyText>
<subsectionHeader confidence="0.979251">
4.2 The features
</subsectionHeader>
<bodyText confidence="0.997003833333333">
Each data point in our dataset corresponds to a
core:contributor relation, and is characterized by the
following features, summarized in Table 2.
Segment Structure. Three features capture the
global structure of the segment in which the current
core:contributor relation appears.
</bodyText>
<listItem confidence="0.61043925">
• (Con)Trib(utor)-pos(ition) captures the posi-
tion of a particular contributor within the larger
segment in which it occurs, and encodes the
structure of the segment in terms of how many
</listItem>
<bodyText confidence="0.9301234">
contributors precede and follow the core. For ex-
ample, contributor (1-D) in Figure 1 is labeled
as B 1A3-2after, as it is the second contributor
following the core in a segment with 1 contrib-
utor before and 3 after the core.
</bodyText>
<page confidence="0.999262">
82
</page>
<tableCaption confidence="0.99886">
Table 1: Distributions of relations and cue occurrences
</tableCaption>
<table confidence="0.9855685">
feature type _ feature description
Segment structure Trib-pos relative position of contrib in segment -I-
Inten-structure number of contribs before and after core
Infor-structure intentional structure of segment
informational structure of segment
Core:contributor Int en-rel enable, convince, concede
relation Info-rel 4 classes of about 30 distinct relations
Syn-rel independent sentences / segments,
Adjacency coordinated clauses, subordinated clauses
are core and contributor adjacent?
Embedding Core-type segment, minimal unit
Trib-type segment, minimal unit
Above / Below number of relations hierarchically
above / below current relation
</table>
<tableCaption confidence="0.94532">
Table 2: Features
</tableCaption>
<figure confidence="0.959156642857143">
Type of relation
Total # of cued relations
Core:Contributor
Joints
Clusters
Total
406
64
310
181
19
276
780
476
</figure>
<listItem confidence="0.9993875">
• Inten(tional)-structure indicates which contrib-
utors in the segment bear the same intentional
relations to the core.
• Infor(mational)-structure. Similar to inten-
</listItem>
<bodyText confidence="0.623756">
tional structure, but applied to informational
relations.
Core:contributor relation. These features more
specifically characterize the current core:contributor
relation.
</bodyText>
<listItem confidence="0.9993825">
• Inten(tional)-rel(ation). One of concede, con-
vince, enable.
• Infor(mational)-rel(ation). About 30 informa-
tional relations have been coded for. However,
as preliminary experiments showed that using
them individually results in overfitting the data,
we classify them according to the four classes
proposed in (Moser, Moore, and Glendening,
1996): causality, similarity, elaboration, tempo-
ral. Temporal relations only appear in clusters,
thus not in the data we discuss in this paper.
• Syn(tactic)-rel(ation). Captures whether the
core and contributor are independent units (seg-
ments or sentences); whether they are coordi-
nated clauses; or which of the two is subordinate
to the other.
• Adjacency. Whether core and contributor are
adjacent in linear order.
Embedding. These features capture segment em-
bedding, Core-type and Trib-type qualitatively, and
Above/Below quantitatively.
• Core-type/(Con)Trib(utor)-type. Whether the
core/the contributor is a segment, or a mini-
mal unit (further subdivided into action, state,
matrix).
• Above/Below encode the number of relations hi-
erarchically above and below the current rela-
tion.
</listItem>
<subsectionHeader confidence="0.996756">
4.3 The experiments
</subsectionHeader>
<bodyText confidence="0.9997068">
Initially, we performed learning on all 406 instances
of core:contributor relations. We quickly determined
that this approach would not lead to useful decision
trees. First, the trees we obtained were extremely
complex (at least 50 nodes). Second, some of the sub-
trees corresponded to clearly identifiable subclasses
of the data, such as relations with an implicit core,
which suggested that we should apply learning to
these independently identifiable subclasses. Thus,
we subdivided the data into three subsets:
</bodyText>
<listItem confidence="0.9727365">
• Corel: core:contributor relations with the core
in first position
• Core2: core:contributor relations with the core
in second position
• Impl(icit)-core: core:contributor relations with
an implicit core
</listItem>
<bodyText confidence="0.999254">
While this has the disadvantage of smaller training
sets, the trees we obtain are more manageable and
more meaningful. Table 3 summarizes the cardinal-
ity of these sets, and the frequencies of cue occur-
rence.
</bodyText>
<page confidence="0.99608">
83
</page>
<table confidence="0.419577">
Total 406 181
</table>
<tableCaption confidence="0.983494">
Table 3: Distributions of relations and cue occurrences
</tableCaption>
<figure confidence="0.970381846153846">
127
52
Corel
Core2
155
100
(on Trib: 43) (on Core: 57)
Impl-core
29
124
Dataset
# of relations
# of cued relations
</figure>
<bodyText confidence="0.8431825">
We ran four sets of experiments. In three of them
we predict cue occurrence and in one cue placement.&apos;
</bodyText>
<subsectionHeader confidence="0.501437">
4.3.1 Cue Occurrence
</subsectionHeader>
<bodyText confidence="0.995268090909091">
Table 4 summarizes our main results concerning
cue occurrence, and includes the error rates asso-
ciated with different feature sets. We adopt Lit-
man&apos;s approach (1996) to determine whether two er-
ror rates El and E2 are significantly different. We
compute 95% confidence intervals for the two error
rates using a t-test. El is significantly better than
C2 if the upper bound of the 95% confidence inter-
val for el is lower than the lower bound of the 95%
confidence interval for e2.,
For each set of experiments, we report the following:
</bodyText>
<listItem confidence="0.805936">
1. A baseline measure obtained by choosing the
majority class. E.g., for Corel 58.9% of the re-
lations are not cued; thus, by deciding to never
include a cue, one would be wrong 41.1% of the
times.
2. The best individual features whose predictive
</listItem>
<bodyText confidence="0.925399114285714">
power is better than the baseline: as Table 4
makes apparent, individual features do not have
much predictive power. For neither Corel nor
Impl-core does any individual feature perform
better than the baseline, and for Core2 only one
feature is sufficiently predictive.
3. (One of) the best induced tree(s). For each tree,
we list the number of nodes, and up to six of the
features that appear highest in the tree, with
their levels of embedding.5 Figure 2 shows the
tree for Core2 (space constraints prevent us from
including figures for each tree). In the figure,
the numbers in parentheses indicate the number
of cases correctly covered by the leaf, and the
number of expected errors at that leaf.
Learning turns out to be most useful for Corel,
where the error reduction (as percentage) from base-
line to the upper bound of the best result is 32%;
4A11 our experiments are run with grouping turned on,
so that C4.5 groups values together rather than creating
a branch per value. The latter choice always results in
trees overfitted to the data in our domain. Using classes
of informational relations, rather than individual infor-
mational relations, constitutes a sort of a priori grouping.
5The trees that C4.5 generates are right-branching, so
this description is fairly adequate.
error reduction is 19% for Core2 and only 3% for
Impl- core.
The best tree was obtained partly by informed
choice, partly by trial and error. Automatically try-
ing out all the 211 = 2048 subsets of features would
be possible, but it would require manual examina-
tion of about 2,000 sets of results, a daunting task.
Thus, for each dataset we considered only the follow-
ing subsets of features.
</bodyText>
<listItem confidence="0.987076857142857">
1. All features. This always results in C4.5 select-
ing a few features (from 3 to 7) for the final tree.
2. Subsets built out of the 2 to 4 attributes appear-
ing highest in the tree obtained by running C4.5
on all features.
3. In Table 2, three features — Trib-pos, Inten-
struct, Infor-struct — concern segment struc-
</listItem>
<bodyText confidence="0.948876">
ture, eight do not. We constructed three subsets
by always including the eight features that do
not concern segment structure, and adding one
of those that does. The trees obtained by includ-
ing Trib-pos, Inten-struct, Infor-struct at the
same time are in general more complex, and not
significantly better than other trees obtained by
including only one of these three features. We
attribute this to the fact that these features en-
code partly overlapping information.
Finally, the best tree was obtained as follows. We
build the set of trees that are statistically equivalent
to the tree with the best error rate (i.e., with the
lowest error rate upper bound). Among these trees,
we choose the one that we deem the most perspicuous
in terms of features and of complexity. Namely, we
pick the simplest tree with Trib-Pos as the root if
one exists, otherwise the simplest tree. Trees that
have nib-Pos as the root are the most useful for
text generation, because, given a complex segment,
Trib-Pos is the only attribute that unambiguously
identifies a specific contributor.
Our results make apparent that the structure of
segments plays a fundamental role in determining
cue occurrence. One of the three features concerning
segment structure (Trib-Pos, mien-Structure, Infor-
Structure) appears as the root or just below the root
in all trees in Table 4; more importantly, this same
configuration occurs in all trees equivalent to the best
tree (even if the specific feature encoding segment
structure may change). The level of embedding in a
</bodyText>
<page confidence="0.997521">
84
</page>
<table confidence="0.9854792">
Corel Core2 Impl-core
Baseline 41.1 35.4 23.5
Best features 0 Info-rel: 33.4±0.94 0
Best tree 25.6±1.24 (15) 27.4±1.28 (18) 22.1±0.57 (10)
0. Trib-pos 0. Trib-Pos 0. Core-type
1. Trib-type 1. Inten-rel 1. Infor-struct
2. Syn-rel 2. Info-rel 2. Inten-rel
3. Core-type 3. Above
4. Above 4. Core-type
5. Inten-rel 5. Below
</table>
<tableCaption confidence="0.9997">
Table 4: Summary of learning results
</tableCaption>
<figure confidence="0.999471791666667">
co 2
(enable)
(causal. elaboration)
No-Cue
( segment )
(matrix. state)
(action)
(1/0.8)
Trib Pos )
1 (132A2-1 pre)
w0.8)
Cue
(131A0- I prc.B IA 1-I pre.B 1A2-1 pre.B I A3-Ipre.
B2A0-1pre.132A0-2pre.
B2A I-I pre,132A1-2pre
B3A0-1pre.133A0-2pre)
(convince. conceeal)
(70/12.7)
Cue
2
(62.3)
Cue
(similarity)
(4/1.2)
( B 1A4-1 pre.
B2A2-2pre.
133A0-3pre
(4/1.2)
No-Cue
( Below )
0
(12/1.3)
/
0
Cue
Cue
No-Cue
(H IA 1-Ipre.13 I A2- I pre.
B1A3- I pre,
B2A0- I pre.k32A0-2pre.
132A I -1pre.132A1-2pre
1F33A0-1pre.133A0-2pre)
(15/3.3)
No-Cue
(131 A0-1 pre,
132A0-2pre )
(19/5.8)
No-Cue Cue
</figure>
<figureCaption confidence="0.942924">
Figure 2: Decision tree for Core2 - occurrence
</figureCaption>
<figure confidence="0.828483285714286">
(BIA0-1pre )
(16/1.8)
(131A1-1pre.BIA2-1pre.
B IA3- I pre.B2A0-1pre.
132A I- I pre./32A1-2pre
B3A0- I pre.133A0-2pre)
(7/3.3)
</figure>
<bodyText confidence="0.9979768">
segment, as encoded by Core-type, Trib-type, Above
and Below also figures prominently.
Inten-rel appears in all trees, confirming the in-
tuition that the speaker&apos;s purpose affects cue occur-
rence. More specifically, in Figure 2, Inten-rel distin-
guishes two different speaker purposes, convince and
enable. The same split occurs in some of the best
trees induced on Corel, with the same outcome: i.e.,
convince directly correlates with the occurrence of a
cue, whereas for enable other features must be taken
into account.&apos; Informational relations do not appear
as often as intentional relations; their discriminatory
power seems more relevant for clusters. Preliminary
6We can&apos;t draw any conclusions concerning concede,
as there are only 24 occurrences of concede out of 406
core:contributor relations.
experiments show that cue occurrence in clusters de-
pends only on informational and syntactic relations.
Finally, Adjacency does not seem to play any sub-
stantial role.
</bodyText>
<subsectionHeader confidence="0.757293">
4.3.2 Cue Placement
</subsectionHeader>
<bodyText confidence="0.999966181818182">
While cue occurrence and placement are interre-
lated problems, we performed learning on them sep-
arately. First, the issue of placement arises only in
the case of Core2; for Corel, cues only occur on the
contributor. Second, we attempted experiments on
Core2 that discriminated between occurrence and
placement at the same time, and the derived trees
were complex and not perspicuous. Thus, we ran an
experiment on the 100 cued relations from Core2 to
investigate which factors affect placing the cue on the
contributor in first position or on the core in second;
</bodyText>
<page confidence="0.999823">
85
</page>
<tableCaption confidence="0.997579">
Table 5: Cue placement on Core2
</tableCaption>
<figureCaption confidence="0.775291333333333">
ic: Core and Tnb are independent clauses
cc.cp,ct: Core and Tnb are coordinated
phrases
</figureCaption>
<figure confidence="0.994982">
Cue-on-Trib Trib-Pon
(131A0-1 pre.13 I AI- I pre.
BIA2-1pre.BIA3-1pre.
82A0-2pre.B2A I -2pre.B2A2-1 pre.
B3A0-1pre I ■01/12.0
Cue-on-Core
</figure>
<figureCaption confidence="0.999967">
Figure 3: Decision tree for Core2 — placement
</figureCaption>
<bodyText confidence="0.999695727272727">
see Table 5.
We ran the same trials discussed above on this
dataset. In this case, the best tree — see Figure 3
— results from combining the two best individual
features, and reduces the error rate by 50%. The
most discriminant feature turns out to be the syn-
tactic relation between the contributor and the core.
However, segment structure still plays an important
role, via Tib-pos.
While the importance of Syn-rel for placement
seems clear, its role concerning occurrence requires
further exploration. It is interesting to note that the
tree induced on Corel — the only case in which Syn-
rel is relevant for occurrence — includes the same dis-
tinction as in Figure 3: namely, if the contributor de-
pends on the core, the contributor must be marked,
otherwise other features have to be taken into ac-
count. Scott and de Souza (1990) point out that
&amp;quot;there is a strong correlation between the syntactic
specification of a complex sentence and its perceived
rhetorical structure.&amp;quot; It seems that certain syntactic
structures function as a cue.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="conclusions">
5 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999989423728814">
We have presented the results of machine learning ex-
periments concerning cue occurrence and placement.
As (Litman, 1996) observes, this sort of empirical
work supports the utility of machine learning tech-
niques applied to coded corpora. As our study shows,
individual features have no predictive power for cue
occurrence. Moreover, it is hard to see how the best
combination of individual features could be found by
manual inspection.
Our results also provide guidance for those build-
ing text generation systems. This study clearly in-
dicates that segment structure, most notably the
ordering of core and contributor, is crucial for de-
termining cue occurrence. Recall that it was only
by considering Corel and Core2 relations in distinct
datasets that we were able to obtain perspicuous de-
cision trees that significantly reduce the error rate.
This indicates that the representations produced
by discourse planners should distinguish those ele-
ments that constitute the core of each discourse seg-
ment, in addition to representing the hierarchical
structure of segments. Note that the notion of core
is related to the notions of nucleus in RST, intended
effect in (Young and Moore, 1994), and of point of
a move in (Elhadad and McKeown, 1990), and that
text generators representing these notions exist.
Moreover, in order to use the decision trees derived
here, decisions about whether or not to make the core
explicit and how to order the core and contributor(s)
must be made before deciding cue occurrence, e.g.,
by exploiting other factors such as focus (McKeown,
1985) and a discourse history.
Once decisions about core:contributor ordering
and cue occurrence have been made, a generator
must still determine where to place cues and se-
lect appropriate lexical items. A major focus of
our future research is to explore the relationship be-
tween the selection and placement decisions. Else-
where, we have found that particular lexical items
tend to have a preferred location, defined in terms of
functional (i.e., core or contributor) and linear (i.e.,
first or second relatum) criteria (Moser and Moore,
1997). Thus, if a generator uses decision trees such
as the one shown in Figure 3 to determine where a
cue should be placed, it can then select an appro-
priate cue from those that can mark the given in-
tentional / informational relations, and are usually
placed in that functional-linear location. To evaluate
this strategy, we must do further work to understand
whether there are important distinctions among cues
(e.g., so, because) apart from their different preferred
locations. The work of Elhadad (1990) and Knott
(1996) will help in answering this question.
Future work comprises further probing into ma-
chine learning techniques, in particular investigating
whether other learning algorithms are more appro-
priate for our problem (Mooney, 1996), especially al-
gorithms that take into account some a priori knowl-
edge about features and their dependencies.
</bodyText>
<sectionHeader confidence="0.996519" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997071444444445">
This research is supported by the Office of Naval
Research, Cognitive and Neural Sciences Division
(Grants N00014-91-J-1694 and N00014-934-0812).
Thanks to Megan Moser for her prior work on this
project and for comments on this paper; to Erin
Glendening and Liina Pylkkanen for their coding ef-
forts; to Haiqin Wang for running many experiments;
to Giuseppe Carenini and Steffi Briininghaus for dis-
cussions about machine learning.
</bodyText>
<figure confidence="0.998903266666667">
Baseline
43%
Best features
Syn-rel: 24.1±0.69
Trib-pos: 40±0.88
20.6±0.97 (5)
0. Syn-rel
1. Trib-pos
Best tree
12d: Trill depends on Core
2Idi Core depends On Trib
B2A0-1pre.B2A1-1pre.
B3A0-2pre)
1315.77
Cue-on-Trite
</figure>
<page confidence="0.978351">
86
</page>
<sectionHeader confidence="0.988214" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989989330188679">
Cohen, Robin. 1984. A computational theory of the
function of clue words in argument understand-
ing. In Proceedings of COLING84, pages 251-258,
Stanford, CA.
Elhadad, Michael and Kathleen McKeown. 1990.
Generating connectives. In Proceedings of COL-
ING90, pages 97-101, Helsinki, Finland.
Goldman, Susan R. 1988. The role of sequence
markers in reading and recall: Comparison of na-
tive and nonnative english speakers. Technical re-
port, University of California, Santa Barbara.
Grosz, Barbara J. and Candace L. Sidner. 1986. At-
tention, intention, and the structure of discourse.
Computational Linguistics, 12(3):175-204.
Hobbs, Jerry R. 1985. On the coherence and struc-
ture of discourse. Technical Report CSLI-85-37,
Center for the Study of Language and Informa-
tion, Stanford University.
Knott, Alistair. 1996. A Data-Driven methodology
for motivating a set of coherence relations. Ph.D.
thesis, University of Edinburgh.
Litman, Diane J. 1996. Cue phrase classification
using machine learning. Journal of Artificial In-
telligence Research, 5:53-94.
Litman, Diane J. and James F. Allen. 1987. A
plan recognition model for subdialogues in conver-
sations. Cognitive Science, 11:163-200.
Mann, William C. and Sandra A. Thompson. 1986.
Relational propositions in discourse. Discourse
Processes, 9:57-90.
Mann, William C. and Sandra A. Thompson.
1988. Rhetorical Structure Theory: Towards a
functional theory of text organization. TEXT,
8(3):243-281.
McKeown, Kathleen R. 1985. Text Generation: Us-
ing Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge Uni-
versity Press, Cambridge, England.
McKeown, Kathleen R. and Michael Elhadad. 1991.
A contrastive evaluation of functional unification
grammar for surface language generation: A case
study in the choice of connectives. In C. L. Paris,
W. R. Swartout, and W. C. Mann, eds., Natu-
ral Language Generation in Artificial Intelligence
and Computational Linguistics. Kluwer Academic
Publishers, Boston, pages 351-396.
Millis, Keith, Arthur Graesser, and Karl Haberlandt.
1993. The impact of connectives on the memory
for expository text. Applied Cognitive Psychology,
7:317-339.
Mooney, Raymond J. 1996. Comparative experi-
ments on disambiguating word senses: An illus-
tration of the role of bias in machine learning. In
Conference on Empirical Methods in Natural Lan-
guage Processing.
Moser, Megan and Johanna D. Moore. 1995. In-
vestigating cue selection and placement in tutorial
discourse. In Proceedings of ACL95, pages 130-
135, Boston, MA.
Moser, Megan and Johanna D. Moore. 1997. A cor-
pus analysis of discourse cues and relational dis-
course structure. Submitted for publication.
Moser, Megan, Johanna D. Moore, and Erin Glen-
dening. 1996. Instructions for Coding Explana-
tions: Identifying Segments, Relations and Mini-
mal Units. Technical Report 96-17, University of
Pittsburgh, Department of Computer Science.
Quinlan, J. Ross. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Reichman-Adar, Rachel. 1984. Extended
person-machine interface. Artificial Intelligence,
22(2):157-218.
Wisner, Dietmar and Manfred Stede. 1992. Cus-
tomizing RST for the automatic production of
technical manuals. In R. Dale, E. Hovy, D. Rosner,
and 0. Stock, eds., 6th International Workshop
on Natural Language Generation, Springer-Verlag,
Berlin, pages 199-215.
Schiffrin, Deborah. 1987. Discourse Markers. Cam-
bridge University Press, New York.
Scott, Donia and Clarisse Sieckenius de Souza. 1990.
Getting the message across in RST-based text gen-
eration. In R. Dale, C. Mellish, and M. Zock,
eds., Current Research in Natural Language Gen-
eration. Academic Press, New York, pages 47-73.
Siegel, Eric V. and Kathleen R. McKeown. 1994.
Emergent linguistic rules from inducing decision
trees: Disambiguating discourse clue words. In
Proceedings of AA AI94, pages 820-826.
Vander Linden, Keith and Barbara Di Eugenio.
1996. Learning micro-planning rules for preven-
tative expressions. In 8th International Workshop
on Natural Language Generation, Sussex, UK.
Vander Linden, Keith and James H. Martin. 1995.
Expressing rhetorical relations in instructional
text: A case study of the purpose relation. Com-
putational Linguistics, 21(1):29-58.
Weiss, Sholom M. and Casimir Kulikowski. 1991.
Computer Systems that learn: classification and
prediction methods from statistics, neural nets,
machine learning, and expert systems. Morgan
Kaufmann.
Young, R. Michael and Johanna D. Moore. 1994.
DPOCL: A Principled Approach to Discourse
Planning. In 7th International Workshop on Natu-
ral Language Generation, Kennebunkport, Maine.
</reference>
<page confidence="0.999474">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.958861">
<title confidence="0.999931">Learning Features that Predict Cue Usage</title>
<author confidence="0.999688">Barbara Di_Eugenio Johanna D Mooret Massimo PaolucciI</author>
<affiliation confidence="0.999985">University of Pittsburgh</affiliation>
<address confidence="0.997436">Pittsburgh, PA 15260, USA</address>
<email confidence="0.999789">dieugeniOcs.pitt.edu</email>
<email confidence="0.999789">jmooreOcs.pitt.edu</email>
<email confidence="0.999789">paolucciOcs.pitt.edu</email>
<abstract confidence="0.997750235294118">Our goal is to identify the features that predict the occurrence and placement of discourse cues in tutorial explanations in order to aid in the automatic generation of explanations. Previous attempts to devise rules for text generation were based on intuition or small numbers of constructed ex- We apply a machine learning proto induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>A computational theory of the function of clue words in argument understanding.</title>
<date>1984</date>
<booktitle>In Proceedings of COLING84,</booktitle>
<pages>251--258</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="1180" citStr="Cohen, 1984" startWordPosition="183" endWordPosition="184">r cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not </context>
</contexts>
<marker>Cohen, 1984</marker>
<rawString>Cohen, Robin. 1984. A computational theory of the function of clue words in argument understanding. In Proceedings of COLING84, pages 251-258, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Generating connectives.</title>
<date>1990</date>
<booktitle>In Proceedings of COLING90,</booktitle>
<pages>97--101</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="2048" citStr="Elhadad and McKeown, 1990" startWordPosition="314" endWordPosition="317">), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research St Development Center tComputer Science Department, and Learning Research 8/ Development Center *Intelligent Systems Program cue occurrence and placement and specific rhetorical structures (Rosner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as stru</context>
<context position="26283" citStr="Elhadad and McKeown, 1990" startWordPosition="4209" endWordPosition="4212">or, is crucial for determining cue occurrence. Recall that it was only by considering Corel and Core2 relations in distinct datasets that we were able to obtain perspicuous decision trees that significantly reduce the error rate. This indicates that the representations produced by discourse planners should distinguish those elements that constitute the core of each discourse segment, in addition to representing the hierarchical structure of segments. Note that the notion of core is related to the notions of nucleus in RST, intended effect in (Young and Moore, 1994), and of point of a move in (Elhadad and McKeown, 1990), and that text generators representing these notions exist. Moreover, in order to use the decision trees derived here, decisions about whether or not to make the core explicit and how to order the core and contributor(s) must be made before deciding cue occurrence, e.g., by exploiting other factors such as focus (McKeown, 1985) and a discourse history. Once decisions about core:contributor ordering and cue occurrence have been made, a generator must still determine where to place cues and select appropriate lexical items. A major focus of our future research is to explore the relationship bet</context>
</contexts>
<marker>Elhadad, McKeown, 1990</marker>
<rawString>Elhadad, Michael and Kathleen McKeown. 1990. Generating connectives. In Proceedings of COLING90, pages 97-101, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan R Goldman</author>
</authors>
<title>The role of sequence markers in reading and recall: Comparison of native and nonnative english speakers.</title>
<date>1988</date>
<tech>Technical report,</tech>
<institution>University of California, Santa Barbara.</institution>
<contexts>
<context position="1423" citStr="Goldman, 1988" startWordPosition="216" endWordPosition="217">e used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; E</context>
</contexts>
<marker>Goldman, 1988</marker>
<rawString>Goldman, Susan R. 1988. The role of sequence markers in reading and recall: Comparison of native and nonnative english speakers. Technical report, University of California, Santa Barbara.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intention, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="1290" citStr="Grosz and Sidner, 1986" startWordPosition="196" endWordPosition="199">ought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) sel</context>
<context position="6511" citStr="Grosz and Sidner, 1986" startWordPosition="1007" endWordPosition="1010">proach the problem from the point of view of interpretation. We closely follow the approach in (Litman, 1996) in two ways. First, we use C4.5. Second, we experiment first with each feature individually, and then with &amp;quot;interesting&amp;quot; subsets of features. 3 Relational Discourse Analysis This section briefly describes Relational Discourse Analysis (RDA) (Moser, Moore, and Glendening, 1996), the coding scheme used to tag the data for our machine learning experiments.&apos; RDA is a scheme devised for analyzing tutorial explanations in the domain of electronics troubleshooting. It synthesizes ideas from (Grosz and Sidner, 1986) and from RST (Mann and Thompson, 1988). Coders use RDA to exhaustively analyze each explanation in the corpus, i.e., every word in each explanation belongs to exactly one element in the analysis. An explanation may consist of multiple segments. Each segment originates with an intention of the speaker. Segments are internally structured and consist of a core, i.e., that element that most directly expresses the segment purpose, and any number of contributors, i.e. the remaining constituents. For each contributor, one analyzes its relation to the core from an intentional perspective, i.e., how i</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, Barbara J. and Candace L. Sidner. 1986. Attention, intention, and the structure of discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>On the coherence and structure of discourse.</title>
<date>1985</date>
<tech>Technical Report CSLI-85-37,</tech>
<institution>Center for the Study of Language and Information, Stanford University.</institution>
<contexts>
<context position="1193" citStr="Hobbs, 1985" startWordPosition="185" endWordPosition="186">nce and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a </context>
<context position="2487" citStr="Hobbs, 1985" startWordPosition="379" endWordPosition="380">t, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research St Development Center tComputer Science Department, and Learning Research 8/ Development Center *Intelligent Systems Program cue occurrence and placement and specific rhetorical structures (Rosner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence o</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Hobbs, Jerry R. 1985. On the coherence and structure of discourse. Technical Report CSLI-85-37, Center for the Study of Language and Information, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven methodology for motivating a set of coherence relations.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="27670" citStr="Knott (1996)" startWordPosition="4435" endWordPosition="4436">e or contributor) and linear (i.e., first or second relatum) criteria (Moser and Moore, 1997). Thus, if a generator uses decision trees such as the one shown in Figure 3 to determine where a cue should be placed, it can then select an appropriate cue from those that can mark the given intentional / informational relations, and are usually placed in that functional-linear location. To evaluate this strategy, we must do further work to understand whether there are important distinctions among cues (e.g., so, because) apart from their different preferred locations. The work of Elhadad (1990) and Knott (1996) will help in answering this question. Future work comprises further probing into machine learning techniques, in particular investigating whether other learning algorithms are more appropriate for our problem (Mooney, 1996), especially algorithms that take into account some a priori knowledge about features and their dependencies. Acknowledgements This research is supported by the Office of Naval Research, Cognitive and Neural Sciences Division (Grants N00014-91-J-1694 and N00014-934-0812). Thanks to Megan Moser for her prior work on this project and for comments on this paper; to Erin Glende</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Knott, Alistair. 1996. A Data-Driven methodology for motivating a set of coherence relations. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
</authors>
<title>Cue phrase classification using machine learning.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>5--53</pages>
<contexts>
<context position="5681" citStr="Litman, 1996" startWordPosition="880" endWordPosition="881">h generating text from &amp;quot;RST trees&amp;quot;, hierarchical structures where leaf nodes contain content and internal nodes indicate the rhetorical relations, as defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), that exist between subtrees. They proposed heuristics for including and choosing cues based on the rhetorical relation between spans of text, the order of the relata, and the complexity of the related text spans. However, (Scott and de Souza, 1990) was based on a small number of constructed examples, and (11,5sner and Stede, 1992) focused on a small number of RST relations. (Litman, 1996) and (Siegel and McKeown, 1994) have applied machine learning to disambiguate between the discourse and sentential usages of cues; however, they do not consider the issues of occurrence and placement, and approach the problem from the point of view of interpretation. We closely follow the approach in (Litman, 1996) in two ways. First, we use C4.5. Second, we experiment first with each feature individually, and then with &amp;quot;interesting&amp;quot; subsets of features. 3 Relational Discourse Analysis This section briefly describes Relational Discourse Analysis (RDA) (Moser, Moore, and Glendening, 1996), the </context>
<context position="11252" citStr="Litman, 1996" startWordPosition="1806" endWordPosition="1807">eason act:reason act:reason Although A (This is) C.2 and E because convince cause:effect C.1 and thus Figure 1: The RDA analysis of (1) 4 Learning from the corpus 4.1 The algorithm We chose the C4.5 learning algorithm (Quinlan, 1993) because it is well suited to a domain such as ours with discrete valued attributes. Moreover, C4.5 produces decision trees and rule sets, both often used in text generation to implement mappings from function features to forms.3 Finally, C4.5 is both readily available, and is a benchmark learning algorithm that has been extensively used in NLP applications, e.g. (Litman, 1996; Mooney, 1996; Vander Linden and Di Eugenio, 1996). As our dataset is small, the results we report are based on cross-validation, which (Weiss and Kulikowski, 1991) recommends as the best method to evaluate decision trees on datasets whose cardinality is in the hundreds. Data for learning should be divided into training and test sets; however, for small datasets this has the disadvantage that a sizable portion of the data is not available for learning. Crossvalidation obviates this problem by running the algorithm N times (N=10 is a typical value): in each run, of the data, randomly chosen, i</context>
<context position="25164" citStr="Litman, 1996" startWordPosition="4031" endWordPosition="4032">ase in which Synrel is relevant for occurrence — includes the same distinction as in Figure 3: namely, if the contributor depends on the core, the contributor must be marked, otherwise other features have to be taken into account. Scott and de Souza (1990) point out that &amp;quot;there is a strong correlation between the syntactic specification of a complex sentence and its perceived rhetorical structure.&amp;quot; It seems that certain syntactic structures function as a cue. 5 Discussion and Conclusions We have presented the results of machine learning experiments concerning cue occurrence and placement. As (Litman, 1996) observes, this sort of empirical work supports the utility of machine learning techniques applied to coded corpora. As our study shows, individual features have no predictive power for cue occurrence. Moreover, it is hard to see how the best combination of individual features could be found by manual inspection. Our results also provide guidance for those building text generation systems. This study clearly indicates that segment structure, most notably the ordering of core and contributor, is crucial for determining cue occurrence. Recall that it was only by considering Corel and Core2 relat</context>
</contexts>
<marker>Litman, 1996</marker>
<rawString>Litman, Diane J. 1996. Cue phrase classification using machine learning. Journal of Artificial Intelligence Research, 5:53-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>James F Allen</author>
</authors>
<title>A plan recognition model for subdialogues in conversations.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<pages>11--163</pages>
<contexts>
<context position="1147" citStr="Litman and Allen, 1987" startWordPosition="177" endWordPosition="180">ng program, C4.5, to induce decision trees for cue occurrence and placement from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems</context>
</contexts>
<marker>Litman, Allen, 1987</marker>
<rawString>Litman, Diane J. and James F. Allen. 1987. A plan recognition model for subdialogues in conversations. Cognitive Science, 11:163-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Relational propositions in discourse.</title>
<date>1986</date>
<booktitle>Discourse Processes,</booktitle>
<pages>9--57</pages>
<contexts>
<context position="1218" citStr="Mann and Thompson, 1986" startWordPosition="187" endWordPosition="190">ment from a corpus of data coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text</context>
</contexts>
<marker>Mann, Thompson, 1986</marker>
<rawString>Mann, William C. and Sandra A. Thompson. 1986. Relational propositions in discourse. Discourse Processes, 9:57-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Towards a functional theory of text organization.</title>
<date>1988</date>
<journal>TEXT,</journal>
<pages>8--3</pages>
<contexts>
<context position="2553" citStr="Mann and Thompson, 1988" startWordPosition="387" endWordPosition="390">sed. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research St Development Center tComputer Science Department, and Learning Research 8/ Development Center *Intelligent Systems Program cue occurrence and placement and specific rhetorical structures (Rosner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all of the factors thought to </context>
<context position="5288" citStr="Mann and Thompson, 1988" startWordPosition="812" endWordPosition="815">d several connectives (e.g., but, since, because), and include many insightful hypotheses about cue selection; their observation that the distinction between but and although depends on the point of the move is related to the notion of core discussed below. However, they do not address the problem of cue occurrence. Other researchers (ROsner and Stede, 1992; Scott and de Souza, 1990) are concerned with generating text from &amp;quot;RST trees&amp;quot;, hierarchical structures where leaf nodes contain content and internal nodes indicate the rhetorical relations, as defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), that exist between subtrees. They proposed heuristics for including and choosing cues based on the rhetorical relation between spans of text, the order of the relata, and the complexity of the related text spans. However, (Scott and de Souza, 1990) was based on a small number of constructed examples, and (11,5sner and Stede, 1992) focused on a small number of RST relations. (Litman, 1996) and (Siegel and McKeown, 1994) have applied machine learning to disambiguate between the discourse and sentential usages of cues; however, they do not consider the issues of occurrence and placement, and ap</context>
<context position="6550" citStr="Mann and Thompson, 1988" startWordPosition="1014" endWordPosition="1017">iew of interpretation. We closely follow the approach in (Litman, 1996) in two ways. First, we use C4.5. Second, we experiment first with each feature individually, and then with &amp;quot;interesting&amp;quot; subsets of features. 3 Relational Discourse Analysis This section briefly describes Relational Discourse Analysis (RDA) (Moser, Moore, and Glendening, 1996), the coding scheme used to tag the data for our machine learning experiments.&apos; RDA is a scheme devised for analyzing tutorial explanations in the domain of electronics troubleshooting. It synthesizes ideas from (Grosz and Sidner, 1986) and from RST (Mann and Thompson, 1988). Coders use RDA to exhaustively analyze each explanation in the corpus, i.e., every word in each explanation belongs to exactly one element in the analysis. An explanation may consist of multiple segments. Each segment originates with an intention of the speaker. Segments are internally structured and consist of a core, i.e., that element that most directly expresses the segment purpose, and any number of contributors, i.e. the remaining constituents. For each contributor, one analyzes its relation to the core from an intentional perspective, i.e., how it is intended to support the core, and </context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, William C. and Sandra A. Thompson. 1988. Rhetorical Structure Theory: Towards a functional theory of text organization. TEXT, 8(3):243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="26613" citStr="McKeown, 1985" startWordPosition="4264" endWordPosition="4265">nstitute the core of each discourse segment, in addition to representing the hierarchical structure of segments. Note that the notion of core is related to the notions of nucleus in RST, intended effect in (Young and Moore, 1994), and of point of a move in (Elhadad and McKeown, 1990), and that text generators representing these notions exist. Moreover, in order to use the decision trees derived here, decisions about whether or not to make the core explicit and how to order the core and contributor(s) must be made before deciding cue occurrence, e.g., by exploiting other factors such as focus (McKeown, 1985) and a discourse history. Once decisions about core:contributor ordering and cue occurrence have been made, a generator must still determine where to place cues and select appropriate lexical items. A major focus of our future research is to explore the relationship between the selection and placement decisions. Elsewhere, we have found that particular lexical items tend to have a preferred location, defined in terms of functional (i.e., core or contributor) and linear (i.e., first or second relatum) criteria (Moser and Moore, 1997). Thus, if a generator uses decision trees such as the one sho</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, Kathleen R. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>A contrastive evaluation of functional unification grammar for surface language generation: A case study in the choice of connectives.</title>
<date>1991</date>
<booktitle>Natural Language Generation in Artificial Intelligence and Computational Linguistics.</booktitle>
<pages>351--396</pages>
<editor>In C. L. Paris, W. R. Swartout, and W. C. Mann, eds.,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston,</location>
<contexts>
<context position="2020" citStr="McKeown and Elhadad, 1991" startWordPosition="310" endWordPosition="313">n and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where the cue should be placed in the text, and (3) selection: what lexical item(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research St Development Center tComputer Science Department, and Learning Research 8/ Development Center *Intelligent Systems Program cue occurrence and placement and specific rhetorical structures (Rosner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, s</context>
<context position="4650" citStr="McKeown and Elhadad (1991" startWordPosition="714" endWordPosition="717">hting the most influential features for a particular task. Discourse structure, in terms of both segment structure and levels of embedding, affects cue occurrence the most; intentional relations also play an important role. For cue placement, the most important factors are syntactic structure and segment complexity. The paper is organized as follows. In Section 2 we discuss previous research in more detail. Section 3 provides an overview of Moser and Moore&apos;s coding scheme. In Section 4 we present our learning experiments, and in Section 5 we discuss our results and conclude. 80 2 Related Work McKeown and Elhadad (1991; 1990) studied several connectives (e.g., but, since, because), and include many insightful hypotheses about cue selection; their observation that the distinction between but and although depends on the point of the move is related to the notion of core discussed below. However, they do not address the problem of cue occurrence. Other researchers (ROsner and Stede, 1992; Scott and de Souza, 1990) are concerned with generating text from &amp;quot;RST trees&amp;quot;, hierarchical structures where leaf nodes contain content and internal nodes indicate the rhetorical relations, as defined in Rhetorical Structure </context>
</contexts>
<marker>McKeown, Elhadad, 1991</marker>
<rawString>McKeown, Kathleen R. and Michael Elhadad. 1991. A contrastive evaluation of functional unification grammar for surface language generation: A case study in the choice of connectives. In C. L. Paris, W. R. Swartout, and W. C. Mann, eds., Natural Language Generation in Artificial Intelligence and Computational Linguistics. Kluwer Academic Publishers, Boston, pages 351-396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Millis</author>
<author>Arthur Graesser</author>
<author>Karl Haberlandt</author>
</authors>
<title>The impact of connectives on the memory for expository text.</title>
<date>1993</date>
<journal>Applied Cognitive Psychology,</journal>
<pages>7--317</pages>
<marker>Millis, Graesser, Haberlandt, 1993</marker>
<rawString>Millis, Keith, Arthur Graesser, and Karl Haberlandt. 1993. The impact of connectives on the memory for expository text. Applied Cognitive Psychology, 7:317-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="11266" citStr="Mooney, 1996" startWordPosition="1808" endWordPosition="1809">on act:reason Although A (This is) C.2 and E because convince cause:effect C.1 and thus Figure 1: The RDA analysis of (1) 4 Learning from the corpus 4.1 The algorithm We chose the C4.5 learning algorithm (Quinlan, 1993) because it is well suited to a domain such as ours with discrete valued attributes. Moreover, C4.5 produces decision trees and rule sets, both often used in text generation to implement mappings from function features to forms.3 Finally, C4.5 is both readily available, and is a benchmark learning algorithm that has been extensively used in NLP applications, e.g. (Litman, 1996; Mooney, 1996; Vander Linden and Di Eugenio, 1996). As our dataset is small, the results we report are based on cross-validation, which (Weiss and Kulikowski, 1991) recommends as the best method to evaluate decision trees on datasets whose cardinality is in the hundreds. Data for learning should be divided into training and test sets; however, for small datasets this has the disadvantage that a sizable portion of the data is not available for learning. Crossvalidation obviates this problem by running the algorithm N times (N=10 is a typical value): in each run, of the data, randomly chosen, is used as the </context>
<context position="27894" citStr="Mooney, 1996" startWordPosition="4467" endWordPosition="4468">n select an appropriate cue from those that can mark the given intentional / informational relations, and are usually placed in that functional-linear location. To evaluate this strategy, we must do further work to understand whether there are important distinctions among cues (e.g., so, because) apart from their different preferred locations. The work of Elhadad (1990) and Knott (1996) will help in answering this question. Future work comprises further probing into machine learning techniques, in particular investigating whether other learning algorithms are more appropriate for our problem (Mooney, 1996), especially algorithms that take into account some a priori knowledge about features and their dependencies. Acknowledgements This research is supported by the Office of Naval Research, Cognitive and Neural Sciences Division (Grants N00014-91-J-1694 and N00014-934-0812). Thanks to Megan Moser for her prior work on this project and for comments on this paper; to Erin Glendening and Liina Pylkkanen for their coding efforts; to Haiqin Wang for running many experiments; to Giuseppe Carenini and Steffi Briininghaus for discussions about machine learning. Baseline 43% Best features Syn-rel: 24.1±0.</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>Mooney, Raymond J. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Moser</author>
<author>Johanna D Moore</author>
</authors>
<title>Investigating cue selection and placement in tutorial discourse.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL95,</booktitle>
<pages>130--135</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2878" citStr="Moser and Moore (1995" startWordPosition="433" endWordPosition="436">cific rhetorical structures (Rosner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all of the factors thought to contribute to cue usage in all of the text. From their study, Moser and Moore identified several interesting correlations between particular features and specific aspects of cue usage, and were able to test specific hypotheses from the literature that were based on constructed examples. In this paper, we focus on cue occurr</context>
<context position="7293" citStr="Moser and Moore, 1995" startWordPosition="1138" endWordPosition="1141">xactly one element in the analysis. An explanation may consist of multiple segments. Each segment originates with an intention of the speaker. Segments are internally structured and consist of a core, i.e., that element that most directly expresses the segment purpose, and any number of contributors, i.e. the remaining constituents. For each contributor, one analyzes its relation to the core from an intentional perspective, i.e., how it is intended to support the core, and from an informational perspective, i.e., how its content relates to that For more detail about the RDA coding scheme see (Moser and Moore, 1995; Moser and Moore, 1997). of the core. The set of intentional relations in RDA is a modification of the presentational relations of RST, while informational relations are similar to the subject matter relations in RST. Each segment constituent, both core and contributors, may itself be a segment with a core:contributor structure. In some cases the core is not explicit. This is often the case with the whole tutor&apos;s explanation, since its purpose is to answer the student&apos;s explicit question. As an example of the application of RDA, consider the partial tutor explanation in (1)2. The purpose of t</context>
<context position="9745" citStr="Moser and Moore, 1995" startWordPosition="1547" endWordPosition="1550">ures with no recognizable core:contributor relation. (1-B) is a cluster composed of two units (the two clauses), related only at the informational level by a temporal relation. Both clauses describe actions, with the first action description embedded in a matrix (&amp;quot;You should&amp;quot;). Cues are much more likely to occur in clusters, where only informational relations occur, than in core:contributor structures, where intentional and informational relations co-occur (x2 = 33.367, p &lt;.001, df = I). In the following, we will not discuss joints and clusters any further. An important result pointed out by (Moser and Moore, 1995) is that cue placement depends on core position. When the core is first and a cue is associated with the relation, the cue never occurs with the core. In contrast, when the core is second, if a cue occurs, it can occur either on the core or on the contributor. 2To make the example more intelligible, we replaced references to parts of the circuit with the labels part/, part2 and part3. 81 you know that partl is good, you should eliminate part2 before troubleshooting inside part3. 1. part2 is moved frequently and thus 2. is more susceptible to damage than part3. it is more work to open up part3 </context>
</contexts>
<marker>Moser, Moore, 1995</marker>
<rawString>Moser, Megan and Johanna D. Moore. 1995. Investigating cue selection and placement in tutorial discourse. In Proceedings of ACL95, pages 130-135, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Moser</author>
<author>Johanna D Moore</author>
</authors>
<title>A corpus analysis of discourse cues and relational discourse structure.</title>
<date>1997</date>
<note>Submitted for publication.</note>
<contexts>
<context position="7317" citStr="Moser and Moore, 1997" startWordPosition="1142" endWordPosition="1145">he analysis. An explanation may consist of multiple segments. Each segment originates with an intention of the speaker. Segments are internally structured and consist of a core, i.e., that element that most directly expresses the segment purpose, and any number of contributors, i.e. the remaining constituents. For each contributor, one analyzes its relation to the core from an intentional perspective, i.e., how it is intended to support the core, and from an informational perspective, i.e., how its content relates to that For more detail about the RDA coding scheme see (Moser and Moore, 1995; Moser and Moore, 1997). of the core. The set of intentional relations in RDA is a modification of the presentational relations of RST, while informational relations are similar to the subject matter relations in RST. Each segment constituent, both core and contributors, may itself be a segment with a core:contributor structure. In some cases the core is not explicit. This is often the case with the whole tutor&apos;s explanation, since its purpose is to answer the student&apos;s explicit question. As an example of the application of RDA, consider the partial tutor explanation in (1)2. The purpose of this segment is to inform</context>
<context position="27151" citStr="Moser and Moore, 1997" startWordPosition="4347" endWordPosition="4350">ding cue occurrence, e.g., by exploiting other factors such as focus (McKeown, 1985) and a discourse history. Once decisions about core:contributor ordering and cue occurrence have been made, a generator must still determine where to place cues and select appropriate lexical items. A major focus of our future research is to explore the relationship between the selection and placement decisions. Elsewhere, we have found that particular lexical items tend to have a preferred location, defined in terms of functional (i.e., core or contributor) and linear (i.e., first or second relatum) criteria (Moser and Moore, 1997). Thus, if a generator uses decision trees such as the one shown in Figure 3 to determine where a cue should be placed, it can then select an appropriate cue from those that can mark the given intentional / informational relations, and are usually placed in that functional-linear location. To evaluate this strategy, we must do further work to understand whether there are important distinctions among cues (e.g., so, because) apart from their different preferred locations. The work of Elhadad (1990) and Knott (1996) will help in answering this question. Future work comprises further probing into</context>
</contexts>
<marker>Moser, Moore, 1997</marker>
<rawString>Moser, Megan and Johanna D. Moore. 1997. A corpus analysis of discourse cues and relational discourse structure. Submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Moser</author>
<author>Johanna D Moore</author>
<author>Erin Glendening</author>
</authors>
<title>Instructions for Coding Explanations: Identifying Segments, Relations and Minimal Units.</title>
<date>1996</date>
<tech>Technical Report 96-17,</tech>
<institution>University of Pittsburgh, Department of Computer Science.</institution>
<contexts>
<context position="6274" citStr="Moser, Moore, and Glendening, 1996" startWordPosition="968" endWordPosition="972">l number of RST relations. (Litman, 1996) and (Siegel and McKeown, 1994) have applied machine learning to disambiguate between the discourse and sentential usages of cues; however, they do not consider the issues of occurrence and placement, and approach the problem from the point of view of interpretation. We closely follow the approach in (Litman, 1996) in two ways. First, we use C4.5. Second, we experiment first with each feature individually, and then with &amp;quot;interesting&amp;quot; subsets of features. 3 Relational Discourse Analysis This section briefly describes Relational Discourse Analysis (RDA) (Moser, Moore, and Glendening, 1996), the coding scheme used to tag the data for our machine learning experiments.&apos; RDA is a scheme devised for analyzing tutorial explanations in the domain of electronics troubleshooting. It synthesizes ideas from (Grosz and Sidner, 1986) and from RST (Mann and Thompson, 1988). Coders use RDA to exhaustively analyze each explanation in the corpus, i.e., every word in each explanation belongs to exactly one element in the analysis. An explanation may consist of multiple segments. Each segment originates with an intention of the speaker. Segments are internally structured and consist of a core, i</context>
<context position="14654" citStr="Moser, Moore, and Glendening, 1996" startWordPosition="2327" endWordPosition="2331">tributors in the segment bear the same intentional relations to the core. • Infor(mational)-structure. Similar to intentional structure, but applied to informational relations. Core:contributor relation. These features more specifically characterize the current core:contributor relation. • Inten(tional)-rel(ation). One of concede, convince, enable. • Infor(mational)-rel(ation). About 30 informational relations have been coded for. However, as preliminary experiments showed that using them individually results in overfitting the data, we classify them according to the four classes proposed in (Moser, Moore, and Glendening, 1996): causality, similarity, elaboration, temporal. Temporal relations only appear in clusters, thus not in the data we discuss in this paper. • Syn(tactic)-rel(ation). Captures whether the core and contributor are independent units (segments or sentences); whether they are coordinated clauses; or which of the two is subordinate to the other. • Adjacency. Whether core and contributor are adjacent in linear order. Embedding. These features capture segment embedding, Core-type and Trib-type qualitatively, and Above/Below quantitatively. • Core-type/(Con)Trib(utor)-type. Whether the core/the contrib</context>
</contexts>
<marker>Moser, Moore, Glendening, 1996</marker>
<rawString>Moser, Megan, Johanna D. Moore, and Erin Glendening. 1996. Instructions for Coding Explanations: Identifying Segments, Relations and Minimal Units. Technical Report 96-17, University of Pittsburgh, Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3711" citStr="Quinlan, 1993" startWordPosition="566" endWordPosition="567">ey exhaustively coded for all of the factors thought to contribute to cue usage in all of the text. From their study, Moser and Moore identified several interesting correlations between particular features and specific aspects of cue usage, and were able to test specific hypotheses from the literature that were based on constructed examples. In this paper, we focus on cue occurrence and placement, and present an empirical study of the hypotheses provided by previous research, which have never been systematically evaluated with naturally occurring data. We use a machine learning program, C4.5 (Quinlan, 1993), on the tagged corpus of Moser and Moore to induce decision trees. The number of coded features and their interactions makes the manual construction of rules that predict cue occurrence and placement an intractable task. Our results largely confirm the suggestions from the literature, and clarify them by highlighting the most influential features for a particular task. Discourse structure, in terms of both segment structure and levels of embedding, affects cue occurrence the most; intentional relations also play an important role. For cue placement, the most important factors are syntactic st</context>
<context position="10873" citStr="Quinlan, 1993" startWordPosition="1744" endWordPosition="1745">nd thus 2. is more susceptible to damage than part3. it is more work to open up part3 for testing the process of opening drawers and extending cards in part3 may induce problems which did not already exist. Although A. B. This is (1) because C. Also, D. and E. B. you should eliminate part2 before troubleshooting inside part3 concede convince convince convince criterion:act act:reason act:reason act:reason Although A (This is) C.2 and E because convince cause:effect C.1 and thus Figure 1: The RDA analysis of (1) 4 Learning from the corpus 4.1 The algorithm We chose the C4.5 learning algorithm (Quinlan, 1993) because it is well suited to a domain such as ours with discrete valued attributes. Moreover, C4.5 produces decision trees and rule sets, both often used in text generation to implement mappings from function features to forms.3 Finally, C4.5 is both readily available, and is a benchmark learning algorithm that has been extensively used in NLP applications, e.g. (Litman, 1996; Mooney, 1996; Vander Linden and Di Eugenio, 1996). As our dataset is small, the results we report are based on cross-validation, which (Weiss and Kulikowski, 1991) recommends as the best method to evaluate decision tree</context>
<context position="12284" citStr="Quinlan, 1993" startWordPosition="1989" endWordPosition="1990">he data is not available for learning. Crossvalidation obviates this problem by running the algorithm N times (N=10 is a typical value): in each run, of the data, randomly chosen, is used as the training set, and the remaining Ttith used as the test 3We will discuss only decision trees here. set. The error rate of a tree obtained by using the whole dataset for training is then assumed to be the average error rate on the test set over the N runs. Further, as C4.5 prunes the initial tree it obtains to avoid overfitting, it computes both actual and estimated error rates for the pruned tree; see (Quinlan, 1993, Ch. 4) for details. Thus, below we will report the average estimated error rate on the test set, as computed by 10-fold cross-validation experiments. 4.2 The features Each data point in our dataset corresponds to a core:contributor relation, and is characterized by the following features, summarized in Table 2. Segment Structure. Three features capture the global structure of the segment in which the current core:contributor relation appears. • (Con)Trib(utor)-pos(ition) captures the position of a particular contributor within the larger segment in which it occurs, and encodes the structure </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J. Ross. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman-Adar</author>
</authors>
<title>Extended person-machine interface.</title>
<date>1984</date>
<journal>Artificial Intelligence,</journal>
<pages>22--2</pages>
<contexts>
<context position="1240" citStr="Reichman-Adar, 1984" startWordPosition="191" endWordPosition="192">a coded for a variety of features previously thought to affect cue usage. Our experiments enable us to identify the features with most predictive power, and show that machine learning can be used to induce decision trees useful for text generation. 1 Introduction Discourse cues are words or phrases, such as because, first, and although, that mark structural and semantic relationships between discourse entities. They play a crucial role in many discourse processing tasks, including plan recognition (Litman and Allen, 1987), text comprehension (Cohen, 1984; Hobbs, 1985; Mann and Thompson, 1986; Reichman-Adar, 1984), and anaphora resolution (Grosz and Sidner, 1986). Moreover, research in reading comprehension indicates that felicitous use of cues improves comprehension and recall (Goldman, 1988), but that their indiscriminate use may have detrimental effects on recall (Mills, Graesser, and Haberlandt, 1993). Our goal is to identify general strategies for cue usage that can be implemented for automatic text generation. From the generation perspective, cue usage consists of three distinct, but interrelated problems: (1) occurrence: whether or not to include a cue in the generated text, (2) placement: where</context>
</contexts>
<marker>Reichman-Adar, 1984</marker>
<rawString>Reichman-Adar, Rachel. 1984. Extended person-machine interface. Artificial Intelligence, 22(2):157-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dietmar Wisner</author>
<author>Manfred Stede</author>
</authors>
<title>Customizing RST for the automatic production of technical manuals. In</title>
<date>1992</date>
<booktitle>6th International Workshop on Natural Language Generation,</booktitle>
<pages>199--215</pages>
<editor>R. Dale, E. Hovy, D. Rosner, and 0. Stock, eds.,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<marker>Wisner, Stede, 1992</marker>
<rawString>Wisner, Dietmar and Manfred Stede. 1992. Customizing RST for the automatic production of technical manuals. In R. Dale, E. Hovy, D. Rosner, and 0. Stock, eds., 6th International Workshop on Natural Language Generation, Springer-Verlag, Berlin, pages 199-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Schiffrin</author>
</authors>
<title>Discourse Markers.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<location>New York.</location>
<contexts>
<context position="2528" citStr="Schiffrin, 1987" startWordPosition="385" endWordPosition="386">em(s) should be used. Prior work in text generation has focused on cue selection (McKeown and Elhadad, 1991; Elhadad and McKeown, 1990), or on the relation between *Learning Research St Development Center tComputer Science Department, and Learning Research 8/ Development Center *Intelligent Systems Program cue occurrence and placement and specific rhetorical structures (Rosner and Stede, 1992; Scott and de Souza, 1990; Vander Linden and Martin, 1995). Other hypotheses about cue usage derive from work on discourse coherence and structure. Previous research (Hobbs, 1985; Grosz and Sidner, 1986; Schiffrin, 1987; Mann and Thompson, 1988; Elhadad and McKeown, 1990), which has been largely descriptive, suggests factors such as structural features of the discourse (e.g., level of embedding and segment complexity), intentional and informational relations in that structure, ordering of relata, and syntactic form of discourse constituents. Moser and Moore (1995; 1997) coded a corpus of naturally occurring tutorial explanations for the range of features identified in prior work. Because they were also interested in the contrast between occurrence and non-occurrence of cues, they exhaustively coded for all o</context>
</contexts>
<marker>Schiffrin, 1987</marker>
<rawString>Schiffrin, Deborah. 1987. Discourse Markers. Cambridge University Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donia Scott</author>
<author>Clarisse Sieckenius de Souza</author>
</authors>
<title>Getting the message across in RST-based text generation. In</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation.</booktitle>
<pages>47--73</pages>
<editor>R. Dale, C. Mellish, and M. Zock, eds.,</editor>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>Scott, de Souza, 1990</marker>
<rawString>Scott, Donia and Clarisse Sieckenius de Souza. 1990. Getting the message across in RST-based text generation. In R. Dale, C. Mellish, and M. Zock, eds., Current Research in Natural Language Generation. Academic Press, New York, pages 47-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric V Siegel</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Emergent linguistic rules from inducing decision trees: Disambiguating discourse clue words.</title>
<date>1994</date>
<booktitle>In Proceedings of AA AI94,</booktitle>
<pages>820--826</pages>
<contexts>
<context position="5712" citStr="Siegel and McKeown, 1994" startWordPosition="883" endWordPosition="886">rom &amp;quot;RST trees&amp;quot;, hierarchical structures where leaf nodes contain content and internal nodes indicate the rhetorical relations, as defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), that exist between subtrees. They proposed heuristics for including and choosing cues based on the rhetorical relation between spans of text, the order of the relata, and the complexity of the related text spans. However, (Scott and de Souza, 1990) was based on a small number of constructed examples, and (11,5sner and Stede, 1992) focused on a small number of RST relations. (Litman, 1996) and (Siegel and McKeown, 1994) have applied machine learning to disambiguate between the discourse and sentential usages of cues; however, they do not consider the issues of occurrence and placement, and approach the problem from the point of view of interpretation. We closely follow the approach in (Litman, 1996) in two ways. First, we use C4.5. Second, we experiment first with each feature individually, and then with &amp;quot;interesting&amp;quot; subsets of features. 3 Relational Discourse Analysis This section briefly describes Relational Discourse Analysis (RDA) (Moser, Moore, and Glendening, 1996), the coding scheme used to tag the d</context>
</contexts>
<marker>Siegel, McKeown, 1994</marker>
<rawString>Siegel, Eric V. and Kathleen R. McKeown. 1994. Emergent linguistic rules from inducing decision trees: Disambiguating discourse clue words. In Proceedings of AA AI94, pages 820-826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vander Linden</author>
<author>Keith</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>Learning micro-planning rules for preventative expressions.</title>
<date>1996</date>
<booktitle>In 8th International Workshop on Natural Language Generation,</booktitle>
<location>Sussex, UK.</location>
<marker>Linden, Keith, Di Eugenio, 1996</marker>
<rawString>Vander Linden, Keith and Barbara Di Eugenio. 1996. Learning micro-planning rules for preventative expressions. In 8th International Workshop on Natural Language Generation, Sussex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vander Linden</author>
<author>Keith</author>
<author>James H Martin</author>
</authors>
<title>Expressing rhetorical relations in instructional text: A case study of the purpose relation.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--1</pages>
<marker>Linden, Keith, Martin, 1995</marker>
<rawString>Vander Linden, Keith and James H. Martin. 1995. Expressing rhetorical relations in instructional text: A case study of the purpose relation. Computational Linguistics, 21(1):29-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sholom M Weiss</author>
<author>Casimir Kulikowski</author>
</authors>
<title>Computer Systems that learn: classification and prediction methods from statistics, neural nets, machine learning, and expert systems.</title>
<date>1991</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11417" citStr="Weiss and Kulikowski, 1991" startWordPosition="1830" endWordPosition="1834">m the corpus 4.1 The algorithm We chose the C4.5 learning algorithm (Quinlan, 1993) because it is well suited to a domain such as ours with discrete valued attributes. Moreover, C4.5 produces decision trees and rule sets, both often used in text generation to implement mappings from function features to forms.3 Finally, C4.5 is both readily available, and is a benchmark learning algorithm that has been extensively used in NLP applications, e.g. (Litman, 1996; Mooney, 1996; Vander Linden and Di Eugenio, 1996). As our dataset is small, the results we report are based on cross-validation, which (Weiss and Kulikowski, 1991) recommends as the best method to evaluate decision trees on datasets whose cardinality is in the hundreds. Data for learning should be divided into training and test sets; however, for small datasets this has the disadvantage that a sizable portion of the data is not available for learning. Crossvalidation obviates this problem by running the algorithm N times (N=10 is a typical value): in each run, of the data, randomly chosen, is used as the training set, and the remaining Ttith used as the test 3We will discuss only decision trees here. set. The error rate of a tree obtained by using the w</context>
</contexts>
<marker>Weiss, Kulikowski, 1991</marker>
<rawString>Weiss, Sholom M. and Casimir Kulikowski. 1991. Computer Systems that learn: classification and prediction methods from statistics, neural nets, machine learning, and expert systems. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Michael Young</author>
<author>Johanna D Moore</author>
</authors>
<title>DPOCL: A Principled Approach to Discourse Planning.</title>
<date>1994</date>
<booktitle>In 7th International Workshop on Natural Language Generation,</booktitle>
<location>Kennebunkport, Maine.</location>
<contexts>
<context position="26228" citStr="Young and Moore, 1994" startWordPosition="4198" endWordPosition="4201">re, most notably the ordering of core and contributor, is crucial for determining cue occurrence. Recall that it was only by considering Corel and Core2 relations in distinct datasets that we were able to obtain perspicuous decision trees that significantly reduce the error rate. This indicates that the representations produced by discourse planners should distinguish those elements that constitute the core of each discourse segment, in addition to representing the hierarchical structure of segments. Note that the notion of core is related to the notions of nucleus in RST, intended effect in (Young and Moore, 1994), and of point of a move in (Elhadad and McKeown, 1990), and that text generators representing these notions exist. Moreover, in order to use the decision trees derived here, decisions about whether or not to make the core explicit and how to order the core and contributor(s) must be made before deciding cue occurrence, e.g., by exploiting other factors such as focus (McKeown, 1985) and a discourse history. Once decisions about core:contributor ordering and cue occurrence have been made, a generator must still determine where to place cues and select appropriate lexical items. A major focus of</context>
</contexts>
<marker>Young, Moore, 1994</marker>
<rawString>Young, R. Michael and Johanna D. Moore. 1994. DPOCL: A Principled Approach to Discourse Planning. In 7th International Workshop on Natural Language Generation, Kennebunkport, Maine.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>