<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999007">
A Provably Correct Learning Algorithm for Latent-Variable PCFGs
</title>
<author confidence="0.991961">
Shay B. Cohen
</author>
<affiliation confidence="0.9986025">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.983208">
scohen@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994525" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965384615385">
We introduce a provably correct learning
algorithm for latent-variable PCFGs. The
algorithm relies on two steps: first, the use
of a matrix-decomposition algorithm ap-
plied to a co-occurrence matrix estimated
from the parse trees in a training sample;
second, the use of EM applied to a convex
objective derived from the training sam-
ples in combination with the output from
the matrix decomposition. Experiments on
parsing and a language modeling problem
show that the algorithm is efficient and ef-
fective in practice.
</bodyText>
<sectionHeader confidence="0.998411" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995842296296296">
Latent-variable PCFGs (L-PCFGs) (Matsuzaki et
al., 2005; Petrov et al., 2006) give state-of-the-art
performance on parsing problems. The standard
approach to parameter estimation in L-PCFGs is
the EM algorithm (Dempster et al., 1977), which
has the usual problems with local optima. Re-
cent work (Cohen et al., 2012) has introduced an
alternative algorithm, based on spectral methods,
which has provable guarantees. Unfortunately this
algorithm does not return parameter estimates for
the underlying L-PCFG, instead returning the pa-
rameter values up to an (unknown) linear trans-
form. In practice, this is a limitation.
We describe an algorithm that, like EM, re-
turns estimates of the original parameters of an L-
PCFG, but, unlike EM, does not suffer from prob-
lems of local optima. The algorithm relies on two
key ideas:
1) A matrix decomposition algorithm (sec-
tion 5) which is applicable to matrices Q of the
form Qf,s = Eh p(h)p(f I h)p(g I h) where
p(h), p(f I h) and p(g I h) are multinomial dis-
tributions. This matrix form has clear relevance
to latent variable models. We apply the matrix
decomposition algorithm to a co-occurrence ma-
trix that can be estimated directly from a training
set consisting of parse trees without latent anno-
</bodyText>
<author confidence="0.577685">
Michael Collins
</author>
<affiliation confidence="0.9897425">
Department of Computer Science
Columbia University
</affiliation>
<email confidence="0.963122">
mcollins@cs.columbia.edu
</email>
<bodyText confidence="0.9978485">
tations. The resulting parameter estimates give us
significant leverage over the learning problem.
2) Optimization of a convex objective function
using EM. We show that once the matrix decom-
position step has been applied, parameter estima-
tion of the L-PCFG can be reduced to a convex
optimization problem that is easily solved by EM.
The algorithm provably learns the parameters of
an L-PCFG (theorem 1), under an assumption that
each latent state has at least one “pivot” feature.
This assumption is similar to the “pivot word” as-
sumption used by Arora et al. (2013) and Arora et
al. (2012) in the context of learning topic models.
We describe experiments on learning of L-
PCFGs, and also on learning of the latent-variable
language model of Saul and Pereira (1997). A hy-
brid method, which uses our algorithm as an ini-
tializer for EM, performs at the same accuracy as
EM, but requires significantly fewer iterations for
convergence: for example in our L-PCFG exper-
iments, it typically requires 2 EM iterations for
convergence, as opposed to 20-40 EM iterations
for initializers used in previous work.
While this paper’s focus is on L-PCFGs, the
techniques we describe are likely to be applicable
to many other latent-variable models used in NLP.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9922126875">
Recently a number of researchers have developed
provably correct algorithms for parameter esti-
mation in latent variable models such as hidden
Markov models, topic models, directed graphical
models with latent variables, and so on (Hsu et
al., 2009; Bailly et al., 2010; Siddiqi et al., 2010;
Parikh et al., 2011; Balle et al., 2011; Arora et
al., 2013; Dhillon et al., 2012; Anandkumar et
al., 2012; Arora et al., 2012; Arora et al., 2013).
Many of these algorithms have their roots in spec-
tral methods such as canonical correlation analy-
sis (CCA) (Hotelling, 1936), or higher-order ten-
sor decompositions. Previous work (Cohen et al.,
2012; Cohen et al., 2013) has developed a spec-
tral method for learning of L-PCFGs; this method
learns parameters of the model up to an unknown
</bodyText>
<page confidence="0.965361">
1052
</page>
<note confidence="0.831145">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998886571428571">
linear transformation, which cancels in the inside-
outside calculations for marginalization over la-
tent states in the L-PCFG. The lack of direct pa-
rameter estimates from this method leads to prob-
lems with negative or unnormalized probablities;
the method does not give parameters that are in-
terpretable, or that can be used in conjunction with
other algorithms, for example as an initializer for
EM steps that refine the model.
Our work is most directly related to the algo-
rithm for parameter estimation in topic models de-
scribed by Arora et al. (2013). This algorithm
forms the core of the matrix decomposition algo-
rithm described in section 5.
</bodyText>
<sectionHeader confidence="0.998908" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.9957065">
This section gives definitions and notation for L-
PCFGs, taken from (Cohen et al., 2012).
</bodyText>
<subsectionHeader confidence="0.953906">
3.1 L-PCFGs: Basic Definitions
</subsectionHeader>
<bodyText confidence="0.669479428571429">
An L-PCFG is an 8-tuple (N, Z, P, m, n, 7r, t, q)
where: N is the set of non-terminal symbols in the
grammar. Z C N is a finite set of in-terminals.
P C N is a finite set of pre-terminals. We as-
sume that N = Z U P, and Z n P = 0. Hence
we have partitioned the set of non-terminals into
two subsets. [m] is the set of possible hidden
</bodyText>
<equation confidence="0.869752666666667">
states.1 [n] is the set of possible words. For
all (a, b, c) E Z x N x N, and (h1, h2, h3) E
[m] x [m] x [m], we have a context-free rule
a(h1) —* b(h2) c(h3). The rule has an associ-
ated parameter t(a —* b c, h2, h3  |a, h1). For all
a E P, h E [m], x E [n], we have a context-free
rule a(h) —* x. The rule has an associated param-
eter q(a —* x  |a, h). For all a E Z, h E [m],
7r(a, h) is a parameter specifying the probability
</equation>
<bodyText confidence="0.993055846153846">
of a(h) being at the root of a tree.
A skeletal tree (s-tree) is a sequence of rules
r1 ... rN where each ri is either of the form a —*
b c or a —* x. The rule sequence forms a
top-down, left-most derivation under a CFG with
skeletal rules.
A full tree consists of an s-tree r1 ... rN, to-
gether with values h1 ... hN. Each hi is the value
for the hidden variable for the left-hand-side of
rule ri. Each hi can take any value in [m].
For a given skeletal tree r1 ... rN, define ai to
be the non-terminal on the left-hand-side of rule
ri. For any i E [N] such that ri is of the form
</bodyText>
<equation confidence="0.737932">
a —* b c, define h(2)
i and h(3)
</equation>
<bodyText confidence="0.698035">
i as the hidden state
</bodyText>
<footnote confidence="0.9673345">
1For any integer n, we use [n] to denote the set
{1, 2,... n}.
</footnote>
<bodyText confidence="0.995547">
value of the left and right child respectively. The
model then defines a distribution as
</bodyText>
<equation confidence="0.9897592">
p(r1 ... rN, h1 ... hN) =
11 π(a1, h1) t(ri, h (2)i, h�3) |ai, hi) 11 q(ri  |ai, hi)
i:aiEZ i:aiEP
The distribution over skeletal trees is
p(r1 ... rN) = Eh1...hN p(r1 ... rN, h1 ... hN).
</equation>
<subsectionHeader confidence="0.99991">
3.2 Definition of Random Variables
</subsectionHeader>
<bodyText confidence="0.996135962962963">
Throughout this paper we will make reference
to random variables derived from the distribution
over full trees from an L-PCFG. These random
variables are defined as follows. First, we select
a random internal node, from a random tree, as
follows: 1) Sample a full tree r1 ... rN, h1 ... hN
from the PMF p(r1 ... rN, h1 ... hN); 2) Choose
a node i uniformly at random from [N]. We then
give the following definition:
Definition 1 (Random Variables). If the rule ri for
the node i is of the form a —* b c, we define ran-
dom variables as follows: R1 is equal to the rule ri
(e.g., NP —* D N). A, B, C are the labels for node i,
the left child of node i, and the right child of node
i respectively. (E.g., A = NP, B = D, C = N.) T1
is the inside tree rooted at node i. T2 is the inside
tree rooted at the left child of node i, and T3 is the
inside tree rooted at the right child of node i. O is
the outside tree at node i. H1, H2, H3 are the hid-
den variables associated with node i, the left child
of node i, and the right child of node i respectively.
E is equal to 1 if node i is at the root of the tree
(i.e., i = 1), 0 otherwise.
If the rule ri for the selected node i is
of the form a —* x, we have random vari-
ables R1, T1, H1, A1, O, E as defined above, but
H2, H3, T2, T3, B, and C are not defined.
</bodyText>
<sectionHeader confidence="0.944887" genericHeader="method">
4 The Learning Algorithm for L-PCFGs
</sectionHeader>
<bodyText confidence="0.999885833333333">
Our goal is to design a learning algorithm for L-
PCFGs. The input to the algorithm will be a train-
ing set consisting of skeletal trees, assumed to be
sampled from some underlying L-PCFG. The out-
put of the algorithm will be estimates for the 7r,
t, and q parameters. The training set does not
include values for the latent variables; this is the
main challenge in learning.
This section focuses on an algorithm for recov-
ery of the t parameters. A description of the al-
gorithms for recovery of the 7r and q parameters
is deferred until section 6.1 of this paper; these
</bodyText>
<page confidence="0.983604">
1053
</page>
<bodyText confidence="0.999795125">
steps are straightforward once we have derived the
method for the t parameters.
We describe an algorithm that correctly recov-
ers the parameters of an L-PCFG as the size of the
training set goes to infinity (this statement is made
more precise in section 4.2). The algorithm relies
on an assumption—the “pivot” assumption—that
we now describe.
</bodyText>
<subsectionHeader confidence="0.979812">
4.1 Features, and the Pivot Assumption
</subsectionHeader>
<bodyText confidence="0.992634765957447">
We assume a function τ from inside trees to a fi-
nite set F, and a function ρ that maps outside trees
to a finite set G. The function τ(t) (ρ(o)) can be
thought of as a function that maps an inside tree
t (outside tree o) to an underlying feature. As
one example, the function τ(t) might return the
context-free rule at the root of the inside tree t;
in this case the set F would be equal to the set
of all context-free rules in the grammar. As an-
other example, the function ρ(o) might return the
context-free rule at the foot of the outside tree o.
In the more general case, we might have K sep-
arate functions τ(k)(t) for k = 1... K mapping
inside trees to K separate features, and similarly
we might have multiple features for outside trees.
Cohen et al. (2013) describe one such feature def-
inition, where features track single context-free
rules as well as larger fragments such as two or
three-level sub-trees. For simplicity of presenta-
tion we describe the case of single features τ(t)
and ρ(o) for the majority of this paper. The exten-
sion to multiple features is straightforward, and is
discussed in section 6.2; the flexibility allowed by
multiple features is important, and we use multiple
features in our experiments.
Given functions τ and ρ, we define additional
random variables: F = τ(T1), F2 = τ(T2), F3 =
τ(T3), and G = ρ(O).
We can now give the following assumption:
Assumption 1 (The Pivot Assumption). Under
the L-PCFG being learned, there exist values α &gt;
0 and β &gt; 0 such that for each non-terminal a,
for each hidden state h ∈ [m], the following state-
ments are true: 1) ∃f ∈ F such that P(F =
f  |H1 = h, A = a) &gt; α and for all h&apos; =6 h,
P(F = f  |H1 = h&apos;, A = a) = 0; 2) ∃g ∈ G
such that P(G = g  |H1 = h, A = a) &gt; β and
for all h&apos; =6h, P(G = g  |H1 = h&apos;,A = a) = 0.
This assumption is very similar to the assump-
tion made by Arora et al. (2012) in the con-
text of learning topic models. It implies that for
each (a, h) pair, there are inside and outside tree
features—which following Arora et al. (2012) we
refer to as pivot features—that occur only2 in the
presence of latent-state value h. As in (Arora et
al., 2012), the pivot features will give us consider-
able leverage in learning of the model.
</bodyText>
<subsectionHeader confidence="0.998609">
4.2 The Learning Algorithm
</subsectionHeader>
<bodyText confidence="0.898056458333333">
Figure 1 shows the learning algorithm for L-
PCFGs. The algorithm consists of the following
steps:
Step 0: Calculate estimates ˆp(a → b c  |a),
ˆp(g, f2, f3  |a → b c) and ˆp(f, g  |a). These
estimates are easily calculated using counts taken
from the training examples.
Step 1: Calculate values ˆr(f  |h, a) and ˆs(g |
h, a); these are estimates of p(f  |h1, a) and
p(g  |h1, a) respectively. This step is achieved us-
ing a matrix decomposition algorithm, described
in section 5 of this paper, on the matrix ˆQa with
entries [ˆQa]f,g = ˆp(f, g  |a).
Step 2: Use the EM algorithm to find tˆ values
that maximize the objective function in Eq. 1 (see
figure 1). Crucially, this is a convex optimization
problem, and the EM algorithm will converge to
the global maximum of this likelihood function.
Step 3: Rule estimates are calculated using an
application of the laws of probability.
Before giving a theorem concerning correctness
of the algorithm we introduce two assumptions:
Assumption 2 (Strict Convexity). If we have the
equalities ˆs(g  |h1, a) = P(G = g  |H1 =
</bodyText>
<equation confidence="0.80939275">
h1, A = a), ˆr(f2  |h2, b) = P(F2 = f2  |H2 =
h2, B = b) and ˆr(f3  |h3, c) = P(F3 = f3 |
H2 = h3, C = c), then the function in Eq. 1 (fig-
ure 1) is strictly concave.
</equation>
<bodyText confidence="0.995679142857143">
The function in Eq. 1 is always concave; this
assumption adds the restriction that the function
must be strictly concave—that is, it has a unique
global maximum—in the case that the rˆ and sˆ es-
timates are exact estimates.
Assumption 3 (Infinite Data). After running Step
0 of the algorithm we have
</bodyText>
<construct confidence="0.793089888888889">
ˆp(a → b c  |a) = p(a → b c  |a)
ˆp(g, f2, f3  |a → b c) = p(g, f2, f3  |a → b c)
ˆp(f, g  |a) = p(f, g  |a)
where p(...) is the probability under the underly-
ing L-PCFG.
2The requirements P(F = f I Hl = h&apos;, A = a) = 0
and P(G = g I Hl = h&apos;, A = a) = 0 are almost certainly
overly strict; in theory and practice these probabilities should
be able to take small but strictly positive values.
</construct>
<page confidence="0.993547">
1054
</page>
<bodyText confidence="0.927439523809524">
We use the term “infinite data” because under
standard arguments, ˆp(...) converges to p(...) as
M goes to oc.
The theorem is then as follows:
Theorem 1. Consider the algorithm in figure 1.
Assume that assumptions 1-3 (the pivot, strong
convexity, and infinite data assumptions) hold for
the underlying L-PCFG. Then there is some per-
mutation σ : [m] -* [m] such that for all
a -* b c, h1, h2, h3,
ˆt(a -* b c, h2, h3  |a -* b c, h1)
= t(a -* b c, σ(h2), σ(h3)  |a -* b c, σ(h1))
where tˆ are the parameters in the output, and t are
the parameters of the underlying L-PCFG.
This theorem states that under assumptions 1-
3, the algorithm correctly learns the t parameters
of an L-PCFG, up to a permutation over the la-
tent states defined by σ. Given the assumptions we
have made, it is not possible to do better than re-
covering the correct parameter values up to a per-
mutation, due to symmetries in the model. As-
suming that the π and q parameters are recovered
in addition to the t parameters (see section 6.1),
the resulting model will define exactly the same
distribution over full trees as the underlying L-
PCFG up to this permutation, and will define ex-
actly the same distribution over skeletal trees, so
in this sense the permutation is benign.
Proof of theorem 1: Under the assumptions of
the theorem, ˆQaf,g = p(f, g  |a) = Ph p(h |
a)p(f  |h, a)p(g  |h, a). Under the pivot assump-
tion, and theorem 2 of section 5, step 1 (the matrix
decomposition step) will therefore recover values
rˆ and sˆ such that ˆr(f  |h, a) = p(f  |σ(h), a) and
ˆs(g  |h, a) = p(g  |σ(h), a) for some permuta-
tion σ : [m] -* [m]. For simplicity, assume that
σ(j) = j for all j E [m] (the argument for other
permutations involves a straightforward extension
of the following argument). Under the assump-
tions of the theorem, ˆp(g, f2, f3  |a -* b c) =
p(g, f2, f3  |a -* b c), hence the function being
optimized in Eq. 1 is equal to
</bodyText>
<equation confidence="0.9507385">
X p(g, f2, f3  |a -* b c) log κ(g, f2, f3)
g,f2,f3
</equation>
<bodyText confidence="0.851419">
where
</bodyText>
<equation confidence="0.991885333333333">
Xκ(g, f2, f3) = (ˆt(h1, h2, h3  |a -* b c)
h1,h2,h3
Xp(g  |h1, a)p(f2  |h2, b)p(f3  |h3, c))
</equation>
<bodyText confidence="0.999925333333333">
Now consider the optimization problem in Eq. 1.
By standard results for cross entropy, the maxi-
mum of the function
</bodyText>
<equation confidence="0.9852485">
X p(g, f2, f3  |a -* b c) log q(g, f2, f3  |a -* b c)
g,f2,f3
</equation>
<bodyText confidence="0.819381333333333">
with respect to the q values is achieved at
q(g, f2, f3  |a -* b c) = p(g, f2, f3  |a -* b c). In
addition, under the assumptions of the L-PCFG,
</bodyText>
<equation confidence="0.9803115">
p(g, f2, f3  |a -* b c)
X= (p(h1, h2, h3  |a -* b c)
h1,h2,h3
Xp(g  |h1, a)p(f2  |h2, b)p(f3  |h3, c))
Hence the maximum of Eq. 1 is achieved at
ˆt(h1, h2, h3  |a -* b c) = p(h1, h2, h3  |a -* b c)
</equation>
<bodyText confidence="0.982275424242424">
(2)
because this gives κ(g, f2, f3) = p(g, f2, f3 |
a -* b c). Under the strict convexity assump-
tion the maximum of Eq. 1 is unique, hence the
tˆ values must satisfy Eq. 2. Finally, it follows
from Eq. 2, and the equality ˆp(a -* b c  |a) =
p(a -* b c  |a), that Step 3 of the algorithm gives
ˆt(a -* b c, h2, h3  |a, h1) = t(a -* b c, h2, h3 |
a, h1).
We can now see how the strict convexity as-
sumption is needed. Without this assumption,
there may be multiple settings for tˆ that achieve
κ(g, f2, f3) = p(g, f2, f3  |a -* b c); the values
ˆt(h1, h2, h3  |a -* b c) = p(h1, h2, h3  |a -* b c)
will be included in this set of solutions, but other,
inconsistent solutions will also be included.
As an extreme example of the failure of the
strict convexity assumption, consider a feature-
vector definition with |F |= |!9 |= 1. In
this case the function in Eq. 1 reduces to
log Ph1,h2,h3 ˆt(h1, h2, h3  |a -* b c). This func-
tion has a maximum value of 0, achieved at all val-
ues of ˆt. Intuitively, this definition of inside and
outside tree features loses all information about
the latent states, and does not allow successful
learning of the underlying L-PCFG. More gener-
ally, it is clear that the strict convexity assumption
will depend directly on the choice of feature func-
tions τ(t) and ρ(o).
Remark: The infinite data assumption, and
sample complexity. The infinite data assump-
tion deserves more discussion. It is clearly a
strong assumption that there is sufficient data for
</bodyText>
<page confidence="0.967502">
1055
</page>
<bodyText confidence="0.95499275">
Input: A set of M skeletal trees sampled from some underlying L-PCFG. The count[...] function counts the number of times
that event ... occurs in the training sample. For example, count[A = a] is the number of times random variable A takes value
a in the training sample.
Step 0: Calculate the following estimates from the training samples:
</bodyText>
<listItem confidence="0.9769522">
• ˆp(a --+ b c  |a) = count[R1 = a --+ b c]/count[A = a]
• ˆp(g, f2, f3  |a --+ b c) = count[G = g, F2 = f2, F3 = f3, R1 = a --+ b c]/count[R1 = a --+ b c]
• ˆp(f, g  |a) = count[F = f, G = g, A = a]/count[A = a]
• `da E Z, define a matrix ˆQa E Rd×d� where d = |F |and d&apos; = |� |as [ˆQa]f,9 = ˆp(f, g  |a).
Step 1: `da E Z, use the algorithm in figure 2 with input ˆQa to derive estimates ˆT(f  |h, a) and ˆs(g  |h, a).
</listItem>
<figure confidence="0.953756571428571">
Remark: These quantities are estimates of P(F1 = f  |H1 = h, A = a) and P(G = g  |H = h, A = a) respectively. Note
that under the independence assumptions of the L-PCFG,
P(F1 = f  |H1 = h, A = a) = P(F2 = f  |H2 = h, A2 = a) = P(F3 = f  |H3 = h, A3 = a).
Step 2: For each rule a --+ b c, find ˆt(h1, h2, h3  |a --+ b c) values that maximize
E ˆp(g, f2, f3  |a --+ b c) log � ˆt(h1, h2, h3  |a --+ b c)ˆs(g  |h1, a)ˆT(f2  |h2, b)ˆT(f3  |h3, c) (1)
9,f2,f3 h1,h2,h3
under the constraints ˆt(h1, h2, h3  |a --+ b c) &gt; 0, and Eh1,h2,h3 ˆt(h1, h2, h3  |a --+ b c) = 1.
Remark: the function in Eq. 1 is concave in the values tˆ being optimized over. We use the EM algorithm, which converges to
a global optimum.
Step 3: `da --+ b c, h1, h2, h3, calculate rule parameters as follows:
�ˆt(a --+ b c, h2, h3  |a, h1) = ˆt(a --+ b c, h1, h2, h3  |a)/ ˆt(a --+ b c, h1, h2, h3  |a)
b,c,h2,h3
where ˆt(a --+ b c, h1, h2, h3  |a) = ˆp(a --+ b c  |a) x ˆt(h1, h2, h3  |a --+ b c).
Output: Parameter estimates ˆt(a --+ b c, h2, h3  |a, h1) for all rules a --+ b c, for all (h1, h2, h3) E [m] x [m] x [m].
</figure>
<figureCaption confidence="0.999996">
Figure 1: The learning algorithm for the t(a --+ b c, h1, h2, h3  |a) parameters of an L-PCFG.
</figureCaption>
<bodyText confidence="0.99995836">
the estimates pˆ in assumption 3 to have converged
to the correct underlying values. A more detailed
analysis of the algorithm would derive sample
complexity results, giving guarantees on the sam-
ple size M required to reach a level of accuracy
c in the estimates, with probability at least 1 − 6,
as a function of c, 6, and other relevant quantities
such as n, d, d&apos;, m, α, β and so on.
In spite of the strength of the infinite data as-
sumption, we stress the importance of this result
as a guarantee for the algorithm. First, a guar-
antee of correct parameter values in the limit of
infinite data is typically the starting point for a
sample complexity result (see for example (Hsu
et al., 2009; Anandkumar et al., 2012)). Sec-
ond, our sense is that a sample complexity result
can be derived for our algorithm using standard
methods: specifically, the analysis in (Arora et
al., 2012) gives one set of guarantees; the remain-
ing optimization problems we solve are convex
maximum-likelihood problems, which are also
relatively easy to analyze. Note that several pieces
of previous work on spectral methods for latent-
variable models focus on algorithms that are cor-
rect under the infinite data assumption.
</bodyText>
<sectionHeader confidence="0.992356" genericHeader="method">
5 The Matrix Decomposition Algorithm
</sectionHeader>
<bodyText confidence="0.997946">
This section describes the matrix decomposition
algorithm used in Step 1 of the learning algorithm.
</bodyText>
<subsectionHeader confidence="0.988782">
5.1 Problem Setting
</subsectionHeader>
<bodyText confidence="0.9411472">
Our goal will be to solve the following matrix de-
composition problem:
Matrix Decomposition Problem (MDP) 1. De-
sign an algorithm with the following inputs, as-
sumptions, and outputs:
</bodyText>
<page confidence="0.977404">
1056
</page>
<bodyText confidence="0.9848725">
Inputs: Integers m, d and d0, and a matrix Q ∈
Rd×d&apos; such that Qf,g = Pmh=1 π(h)r(f  |h)s(g |
h) for some unknown parameters π(h), r(f  |h)
and s(g  |h) satisfying:
</bodyText>
<equation confidence="0.995966">
1) π(h) ≥ 0, Pmh=1 π(h) = 1;
2) r(f  |h) ≥ 0, Pdf=1 r(f  |h) = 1;
3) s(g  |h) ≥ 0, Pd&apos;
g=1 s(g  |h) = 1.
</equation>
<bodyText confidence="0.993861818181818">
Assumptions: There are values α &gt; 0 and β &gt;
0 such that the r parameters of the model are α-
separable, and the s parameters of the model are
β-separable.
Outputs: Estimates ˆπ(h), ˆr(f  |h) and ˆs(g  |h)
such that there is some permutation σ : [m] → [m]
such that ∀h, ˆπ(h) = π(σ(h)), ∀f, h, ˆr(f |
h) = r(f  |σ(h)), and ∀g, h, ˆs(g  |h) = s(g |
σ(h)).
The definition of α-separability is as follows (β-
separability for s(g  |h) is analogous):
</bodyText>
<equation confidence="0.70651475">
Definition 2 (α-separability). The parameters
r(f  |h) are α-separable iffor all h ∈ [m], there
is some j ∈ [d] such that: 1) r(j  |h) ≥ α; and 2)
r(j  |h0) = 0 for h0 =6h.
</equation>
<bodyText confidence="0.971711941176471">
s(g  |h) are returned by the algorithm. Thus an
algorithm that solves MDP 2 in some sense solves
“one half” of MDP 1.
For completeness we give a sketch of the algo-
rithm that we use; it is inspired by the algorithm
of Arora et al. (2012), but has some important dif-
ferences. The algorithm is as follows:
Step 1: Derive a function φ : [d0] → Rl that
maps each integer g ∈ [d0] to a representation
φ(g) ∈ Rl. The integer l is typically much smaller
than d0, implying that the representation is of low
dimension. Arora et al. (2012) derive φ as a ran-
dom projection with a carefully chosen dimension
l. In our experiments, we use canonical correlation
analysis (CCA) on the matrix Q to give a represen-
tation φ(g) ∈ Rl where l = m.
Step 2: For each f ∈ [d], calculate
</bodyText>
<equation confidence="0.994416">
vf = E[φ(g)  |f] = X d&apos; p(g  |f)φ(g)
g=1
</equation>
<bodyText confidence="0.993198583333333">
where p(g  |f) = Qf,g/Pg Qf,g. It follows that
This matrix decomposition problem has clear vf = X d&apos; Xm p(h  |f)p(g  |h)φ(g) = Xm p(h  |f)wh
relevance to problems in learning of latent- g=1 h=1 h=1
variable models, and in particular is a core step of
the algorithm in figure 1. When given a matrix ˆQa
with entries
h, a), where p(...) refers to a distribution derived
ˆQaf,g = Ph p(h  |a)p(f  |h, a)p(g |
from an underlying L-PCFG which satisfies the
pivot assumption, the method will recover the val-
ues for p(h  |a), p(f  |h, a) and p(g  |h, a) up to a
permutation over the latent states.
</bodyText>
<subsectionHeader confidence="0.998096">
5.2 The Algorithm of Arora et al. (2013)
</subsectionHeader>
<bodyText confidence="0.930192804878049">
This section describes a variant of the algorithm of
Arora et al. (2013), which is used as a component
of our algorithm for MDP 1. One of the proper-
ties of this algorithm is that it solves the following
problem:
Matrix Decomposition Problem (MDP) 2. De-
sign an algorithm with the following inputs, as-
sumptions, and outputs:
Inputs: Same as matrix decomposition problem 1.
Assumptions: The parameters r(f  |h) of the
model are α-separable for some value α &gt; 0.
Outputs: Estimates ˆπ(h) and ˆr(f  |h) such that
∃σ : [m] → [m] such that ∀h, ˆπ(h) = π(σ(h)),
∀f, h, ˆr(f  |h) = r(f  |σ(h)).
This is identical to Matrix Decomposition Prob-
lem 1, but without the requirement that the values
where wh ∈ Rl is equal to Pd&apos;
g=1 p(g  |h)φ(g).
Hence the vf vectors lie in the convex hull of a
set of vectors w1 ... wm ∈ Rl. Crucially, for any
pivot word f for latent state h, we have p(h  |f) =
1, hence vf = wh. Thus by the pivot assump-
tion, the set of points v1 ... vd includes the ver-
tices of the convex hull. Each point vj is a convex
combination of the vertices w1 ... wm, where the
weights in this combination are equal to p(h  |j).
Step 3: Use the FastAnchorWords algo-
rithm of (Arora et al., 2012) to identify m vectors
vs1, vs2, ... vsm. The FastAnchorWords algo-
rithm has the guarantee that there is a permutation
σ : [m] → [m] such that vsi = wσ(i) for all i. This
algorithm recovers the vertices of the convex hull
described in step 2, using a method that greedily
picks points that are as far as possible from the
subspace spanned by previously picked points.
Step 4: For each f ∈ [d] solve the problem
arg min
γ1,γ2,...,γm
subject to γh ≥ 0 and Ph γh = 1. We use the
algorithm of (Frank and Wolfe, 1956; Clarkson,
2010) for this purpose. Set q(h  |f) = γh.
</bodyText>
<equation confidence="0.948257333333333">
γhvsh − vf||2
X
||
h
1057
Return the final quantities:
Xˆπ(h) =
f p(f)q(h|f) ˆr(f|h) = p(f)q(h|f)
P f p(f)q(h|f)
</equation>
<bodyText confidence="0.993198">
where p(f) = Pg Qf,g.
</bodyText>
<subsectionHeader confidence="0.996337">
5.3 An Algorithm for MDP 1
</subsectionHeader>
<bodyText confidence="0.999792153846154">
Figure 2 shows an algorithm that solves MDP 1.
In steps 1 and 2 of the algorithm, the algorithm
of section 5.2 is used to recover estimates ˆr(f |
h) and ˆs(g  |h). These distributions are equal to
p(f  |h) and p(g  |h) up to permutations σ and
σ&apos; of the latent states respectively; unfortunately
there is no guarantee that σ and σ&apos; are the same
permutation. Step 3 estimates parameters t(h&apos; |
h) that effectively map the permutation implied by
ˆr(f  |h) to the permutation implied by ˆs(g  |h);
the latter distribution is recalculated as Ph0 ˆt(h&apos; |
h)ˆs(g  |h&apos;).
We now state the following theorem:
</bodyText>
<figureCaption confidence="0.4731915">
Theorem 2. The algorithm in figure 2 solves Ma-
trix Decomposition Problem 1.
</figureCaption>
<bodyText confidence="0.913158555555556">
Proof: See the supplementary material.
Remark: A natural alternative to the algorithm
presented would be to run Step 1 of the original
algorithm, but to replace steps 2 and 3 with a step
that finds ˆs(g  |h) values that maximize
X XQf,g log ˆr(h  |f)ˆs(g  |h)
f,g h
This is again a convex optimization problem. We
may explore this algorithm in future work.
</bodyText>
<sectionHeader confidence="0.921354" genericHeader="method">
6 Additional Details of the Algorithm
</sectionHeader>
<subsectionHeader confidence="0.999831">
6.1 Recovery of the π and q Parameters
</subsectionHeader>
<bodyText confidence="0.999643666666667">
The recovery of the π and q parameters relies on
the following additional (but benign) assumptions
on the functions τ and ρ:
</bodyText>
<listItem confidence="0.995171714285714">
1) For any inside tree t such that t is a unary
rule of the form a —* x, the function τ is defined
as τ(t) = t.3
2) The set of outside tree features !9 contains a
special symbol ❑, and g(o) = ❑ if and only if the
outside tree o is derived from a non-terminal node
at the root of a skeletal tree.
</listItem>
<footnote confidence="0.98911525">
3Note that if other features on unary rules are desired,
we can use multiple feature functions τ&apos;(t) ... τK(t), where
τ&apos;(t) = t for inside trees, and the functions τ2(t) ... τK(t)
define other features.
</footnote>
<figureCaption confidence="0.999847">
Figure 2: The algorithm for Matrix Decomposition Problem 1
</figureCaption>
<bodyText confidence="0.998825666666667">
Under these assumptions, the algorithm in fig-
ure 1 recovers estimates ˆπ(a, h) and ˆq(a —* x |
a, h). Simply set
</bodyText>
<equation confidence="0.676612">
ˆq(a —* x  |a, h) = ˆr(f  |h, a) where f = a —* x
</equation>
<bodyText confidence="0.9998324">
and ˆπ(a, h) = ˆp(❑, h, a)/Ph,a ˆp(❑, h, a) where
ˆp(❑, h, a) = ˆg(❑  |h, a)ˆp(h  |a)ˆp(a). Note that
ˆp(h  |a) can be derived from the matrix decompo-
sition step when applied to ˆQa, and ˆp(a) is easily
recovered from the training examples.
</bodyText>
<subsectionHeader confidence="0.999668">
6.2 Extension to Include Multiple Features
</subsectionHeader>
<bodyText confidence="0.999764333333333">
We now describe an extension to allow K separate
functions τ(k)(t) for k = 1... K mapping inside
trees to features, and L feature functions ρ(l)(o)
for l = 1... L over outside trees.
The algorithm in figure 1 can be extended as
follows. First, Step 1 of the algorithm (the matrix
</bodyText>
<note confidence="0.806789333333333">
Inputs: As in Matrix Decomposition Problem 1.
Assumptions: As in Matrix Decomposition Problem 1.
Algorithm:
</note>
<bodyText confidence="0.8124875">
Step 1. Run the algorithm of section 5.2 on the matrix Q
to derive estimates ˆr(f  |h) and ˆπ(h). Note that under
the guarantees of the algorithm, there is some permutation
σ such that ˆr(f  |h) = r(f  |σ(h)). Define
</bodyText>
<figure confidence="0.6186715">
ˆr(h  |f) = ˆr(f  |h)ˆπ(h)
Eh ˆr(f  |h)ˆπ(h)
</figure>
<figureCaption confidence="0.8657586">
Step 2. Run the algorithm of section 5.2 on the matrix Q&gt;
to derive estimates ˆs(g  |h). Under the guarantees of the
algorithm, there is some permutation σ0 such that ˆs(g  |h) =
s(g  |σ0(h)). Note however that it is not necessarily the case
that σ = σ0.
</figureCaption>
<figure confidence="0.694715666666667">
Step 3. Find ˆt(h0  |h) for all h, h0 E [m] that maximize
E �Qf,g log ˆr(h  |f)ˆt(h0  |h)ˆs(g  |h0) (3)
f,g h,h&apos;
</figure>
<figureCaption confidence="0.393057">
subject to ˆt(h0  |h) &gt; 0, and `dh, Eh, ˆt(h0  |h) = 1.
Remark: the function in Eq. 3 is concave in the tˆ parame-
ters. We use the EM algorithm to find a global optimum.
Step 4. Return the following values:
</figureCaption>
<listItem confidence="0.949087666666667">
• ˆπ(h) for all h, as an estimate of π(σ(h)) for some
permutation σ.
• ˆr(f  |h) for all f, h as an estimate of r(f  |σ(h)) for
the same permutation σ.
• Eh, ˆt(h0  |h)ˆs(g  |h0) as an estimate of s(f  |σ(h))
for the same permutation σ.
</listItem>
<page confidence="0.993438">
1058
</page>
<bodyText confidence="0.9754045">
decomposition step) can be extended to provide
estimates ˆr(k)(f(k)  |h, a) and ˆs(l)(g(l)  |h, a).
In brief, this involves running CCA on a matrix
E[φ(T)(ψ(O))T  |A = a] where φ and ψ are in-
side and outside binary feature vectors derived di-
rectly from the inside and outside features, using
a one-hot representation. CCA results in a low-
dimensional representation that can be used in the
steps described in section 5.2; the remainder of the
algorithm is the same. In practice, the addition of
multiple features may lead to better CCA repre-
sentations.
Next, we modify the objective function in Eq. 1
to be the following:
p(gi, fj2, fk3  |a -* b c) log κi,j,k(gi, fj2, fk3 )
where
</bodyText>
<equation confidence="0.9985898">
κi,j,k(gi, fj2, fk3 )
�= (ˆt(h1, h2, h3  |a -* b c)
h1,h2,h3
�
xˆsi(gi  |h1, a)ˆrj(fj 2  |h2, b)ˆrk(fk 3  |h3, c)
</equation>
<bodyText confidence="0.999893">
Thus the new objective function consists of a sum
of LxM2 terms, each corresponding to a different
combination of inside and outside features. The
function remains concave.
</bodyText>
<subsectionHeader confidence="0.995672">
6.3 Use as an Initializer for EM
</subsectionHeader>
<bodyText confidence="0.999966181818182">
The learning algorithm for L-PCFGs can be used
as an initializer for the EM algorithm for L-
PCFGs. Two-step estimation methods such as
these are well known in statistics; there are guar-
antees for example that if the first estimator is con-
sistent, and the second step finds the closest local
maxima of the likelihood function, then the result-
ing estimator is both consistent and efficient (in
terms of number of samples required). See for
example page 453 or Theorem 4.3 (page 454) of
(Lehmann and Casella, 1998).
</bodyText>
<sectionHeader confidence="0.960789" genericHeader="method">
7 Experiments on Parsing
</sectionHeader>
<bodyText confidence="0.9997681">
This section describes parsing experiments using
the learning algorithm for L-PCFGs. We use the
Penn WSJ treebank (Marcus et al., 1993) for our
experiments. Sections 2–21 were used as training
data, and sections 0 and 22 were used as develop-
ment data. Section 23 was used as the test set.
The experimental setup is the same as described
by Cohen et al. (2013). The trees are bina-
rized (Petrov et al., 2006) and for the EM algo-
rithm we use the initialization method described
</bodyText>
<table confidence="0.998315555555556">
M 8 sec. 22 32 sec. 23
16 24
86.69 EM 88.35 88.56 87.76
40 88.32 30 20
30
Spectral 85.60 87.77 88.53 88.82 88.05
Pivot 83.56 86.00 86.87 86.40 85.83
Pivot+EM 86.83 88.14 88.64 88.55 88.03
2 6 2 2
</table>
<tableCaption confidence="0.595390125">
Table 1: Results on the development data (section 22) and
test data (section 23) for various learning algorithms for L-
PCFGs. For EM and pivot+EM experiments, the second line
denotes the number of iterations required to reach the given
optimal performance on development data. Results for sec-
tion 23 are used with the best model for section 22 in the cor-
responding row. The results for EM and spectral are reported
from Cohen et al. (2013).
</tableCaption>
<bodyText confidence="0.993995892857143">
in Matsuzaki et al. (2005). For the pivot algo-
rithm we use multiple features τ1(t) ... τK(t) and
ρ1(o) ... ρL(o) over inside and outside trees, us-
ing the features described by Cohen et al. (2013).
Table 1 gives the F1 accuracy on the develop-
ment and test sets for the following methods:
EM: The EM algorithm as used by Matsuzaki et
al. (2005) and Petrov et al. (2006).
Spectral: The spectral algorithm of Cohen et al.
(2012) and Cohen et al. (2013).
Pivot: The algorithm described in this paper.
Pivot+EM: The algorithm described in this pa-
per, followed by 1 or more iterations of the
EM algorithm with parameters initialized by the
pivot algorithm. (See section 6.3.)
For the EM and Pivot+EM algorithms, we give
the number of iterations of EM required to reach
optimal performance on the development data.
The results show that the EM, Spectral, and
Pivot+EM algorithms all perform at a very similar
level of accuracy. The Pivot+EM results show that
very few EM iterations—just 2 iterations in most
conditions—are required to reach optimal perfor-
mance when the Pivot model is used as an ini-
tializer for EM. The Pivot results lag behind the
Pivot+EM results by around 2-3%, but they are
close enough to optimality to require very few EM
iterations when used as an initializer.
</bodyText>
<sectionHeader confidence="0.9868545" genericHeader="method">
8 Experiments on the Saul and Pereira
(1997) Model for Language Modeling
</sectionHeader>
<bodyText confidence="0.955290833333333">
We now describe a second set of experiments, on
the Saul and Pereira (1997) model for language
modeling. Define V to be the set of words in the
vocabulary. For any w1, w2 E V , the Saul and
Pereira (1997) model then defines p(w2  |w1) =
Emh=1 r(h  |w1)s(w2  |h) where r(h  |w1) and
</bodyText>
<figure confidence="0.74374825">
�
gi,fj2 ,fk3
�
i,j,k
</figure>
<page confidence="0.782383">
1059
</page>
<table confidence="0.999396222222222">
Brown NYT
m 2 4 8 16 32 128 256 test 2 4 8 16 32 128 256 test
EM 737 599 488 468 430 388 365 364 926 733 562 420 361 284 265 267
14 14 19 12 10 9 8 36 39 42 33 38 35 32
bi-KN +int. 408 415 271 279
tri-KN+int. 386 394 150 158
pivot 852 718 605 559 537 426 597 560 1227 1264 896 717 738 782 886 715
pivot+EM 758 582 502 425 374 310 327 357 898 754 553 441 394 279 292 281
2 3 2 1 1 1 1 20 14 13 15 10 19 12
</table>
<tableCaption confidence="0.8807886">
Table 2: Language model perplexity with the Brown corpus and the Gigaword corpus (New York Times portion) for the second
half of the development set, and the test set. With EM and Pivot+EM, the number of iterations for EM to reach convergence is
given below the perplexity. The best result for each column (for each m value) is in bold. The “test” column gives perplexity
results on the test set. Each perplexity calculation on the test set is done using the best model on the development set. bi-KN+int
and tri-KN+int are bigram and trigram Kneser-Ney interpolated models (Kneser and Ney, 1995), using the SRILM toolkit.
</tableCaption>
<bodyText confidence="0.99916148">
s(w2  |h) are parameters of the approach. The
conventional approach to estimation of the param-
eters r(h  |w1) and s(w2  |h) from a corpus is
to use the EM algorithm. In this section we com-
pare the EM algorithm to a pivot-based method.
It is straightforward to represent this model as an
L-PCFG, and hence to use our implementation for
estimation.
In this special case, the L-PCFG learning al-
gorithm is equivalent to a simple algorithm, with
the following steps: 1) define the matrix Q
with entries Qw1,w2 = count(w1, w2)/N where
count(w1, w2) is the number of times that bi-
gram (w1, w2) is seen in the data, and N =
Ew1 w2 count(w1, w2). Run the algorithm of sec-
tion 5.2 on Q to recover estimates ˆs(w2  |h); 2)
estimate ˆr(h  |w1) using the EM algorithm to op-
timize the function Ew1,w2 Qw1,w2 log Eh ˆr(h |
w1)ˆs(w2  |h) with respect to the rˆ parameters;
this function is concave in these parameters.
We performed the language modeling experi-
ments for a number of reasons. First, because in
this case the L-PCFG algorithm reduces to a sim-
ple algorithm, it allows us to evaluate the core
ideas in the method very directly. Second, it al-
lows us to test the pivot method on the very large
datasets that are available for language modeling.
We use two corpora for our experiments. The
first is the Brown corpus, as used by Bengio et
al. (2006) in language modeling experiments. Fol-
lowing Bengio et al. (2006), we use the first 800K
words for training (and replace all words that ap-
pear once with an UNK token), the next 200K
words for development, and the remaining data
(165,171 tokens) as a test set. The size of the
vocabulary is 24,488 words. The second corpus
we use is the New York Times portion of the Gi-
gaword corpus. Here, the training set consists of
1.31 billion tokens. We use 159 million tokens for
development set and 156 million tokens for test.
All words that appeared less than 20 times in the
training set were replaced with the UNK token.
The size of the vocabulary is 235,223 words. Un-
known words in test data are ignored when calcu-
lating perplexity (this is the standard set-up in the
SRILM toolkit).
In our experiments we use the first half of each
development set to optimize the number of itera-
tions of the EM or Pivot+EM algorithms. As be-
fore, Pivot+EM uses 1 or more EM steps with pa-
rameter initialization from the Pivot method.
Table 2 gives perplexity results for the differ-
ent algorithms. As in the parsing experiments, the
Pivot method alone performs worse than EM, but
the Pivot+EM method gives results that are com-
petitive with EM. The Pivot+EM method requires
fewer iterations of EM than the EM algorithm.
On the Brown corpus the difference is quite dra-
matic, with only 1 or 2 iterations required, as op-
posed to 10 or more for EM. For the NYT cor-
pus the Pivot+EM method requires more iterations
(around 10 or 20), but still requires significantly
fewer iterations than the EM algorithm.
On the Gigaword corpus, with m = 256, EM
takes 12h57m (32 iterations at 24m18s per itera-
tion) compared to 1h50m for the Pivot method. On
Brown, EM takes 1m47s (8 iterations) compared
to 5m44s for the Pivot method. Both the EM and
pivot algorithm implementations were highly op-
timized, and written in Matlab. Results at other
values of m are similar. From these results the
Pivot method appears to become more competitive
speed-wise as the data size increases (the Giga-
word corpus is more than 1,300 times larger than
the Brown corpus).
</bodyText>
<sectionHeader confidence="0.997915" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.9994925">
We have described a new algorithm for parameter
estimation in L-PCFGs. The algorithm is provably
correct, and performs well in practice when used
in conjunction with EM.
</bodyText>
<page confidence="0.981341">
1060
</page>
<sectionHeader confidence="0.993795" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999953391304348">
A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and
M. Telgarsky. 2012. Tensor decompositions for
learning latent-variable models. arXiv:1210.7559.
S. Arora, R. Ge, and A. Moitra. 2012. Learning
topic models–going beyond SVD. In Proceedings
of FOCS.
S. Arora, R. Ge, Y. Halpern, D. M. Mimno, A. Moitra,
D. Sontag, Y. Wu, and M. Zhu. 2013. A practical
algorithm for topic modeling with provable guaran-
tees. In Proceedings of ICML.
R. Bailly, A. Habrar, and F. Denis. 2010. A spectral
approach for probabilistic grammatical inference on
trees. In Proceedings of ALT.
B. Balle, A. Quattoni, and X. Carreras. 2011. A spec-
tral learning algorithm for finite state transducers. In
Proceedings of ECML.
Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and
J.-L. Gauvain. 2006. Neural probabilistic language
models. In Innovations in Machine Learning, pages
137–186. Springer.
K. L. Clarkson. 2010. Coresets, sparse greedy ap-
proximation, and the Frank-Wolfe algorithm. ACM
Transactions on Algorithms (TALG), 6(4):63.
S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and
L. Ungar. 2012. Spectral learning of latent-variable
PCFGs. In Proceedings of ACL.
S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and
L. Ungar. 2013. Experiments with spectral learn-
ing of latent-variable PCFGs. In Proceedings of
NAACL.
A. Dempster, N. Laird, and D. Rubin. 1977. Maxi-
mum likelihood estimation from incomplete data via
the EM algorithm. Journal of the Royal Statistical
Society B, 39:1–38.
P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H.
Ungar. 2012. Spectral dependency parsing with la-
tent variables. In Proceedings of EMNLP.
M. Frank and P. Wolfe. 1956. An algorithm for
quadratic programming. Naval research logistics
quarterly, 3(1-2):95–110.
H. Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral
algorithm for learning hidden Markov models. In
Proceedings of COLT.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
ICASSP.
E. L. Lehmann and G. Casella. 1998. Theory of Point
Estimation (Second edition). Springer.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguis-
tics, 19:313–330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic CFG with latent annotations. In Proceedings
ofACL.
A. Parikh, L. Song, and E. P. Xing. 2011. A spectral
algorithm for latent tree graphical models. In Pro-
ceedings of ICML.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of COLING-ACL.
L. Saul and F. Pereira. 1997. Aggregate and mixed-
order markov models for statistical language pro-
cessing. In Proceedings of EMNLP.
S. Siddiqi, B. Boots, and G. Gordon. 2010. Reduced-
rank hidden markov models. Journal of Machine
Learning Research, 9:741–748.
</reference>
<page confidence="0.974541">
1061
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.937286">
<title confidence="0.999379">A Provably Correct Learning Algorithm for Latent-Variable PCFGs</title>
<author confidence="0.999979">B Shay</author>
<affiliation confidence="0.9990095">School of University of</affiliation>
<email confidence="0.98652">scohen@inf.ed.ac.uk</email>
<abstract confidence="0.996579">We introduce a provably correct learning algorithm for latent-variable PCFGs. The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Anandkumar</author>
<author>R Ge</author>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>M Telgarsky</author>
</authors>
<title>Tensor decompositions for learning latent-variable models.</title>
<date>2012</date>
<pages>1210--7559</pages>
<contexts>
<context position="3675" citStr="Anandkumar et al., 2012" startWordPosition="588" endWordPosition="591">d to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics linear transforma</context>
<context position="19911" citStr="Anandkumar et al., 2012" startWordPosition="3826" endWordPosition="3829">ailed analysis of the algorithm would derive sample complexity results, giving guarantees on the sample size M required to reach a level of accuracy c in the estimates, with probability at least 1 − 6, as a function of c, 6, and other relevant quantities such as n, d, d&apos;, m, α, β and so on. In spite of the strength of the infinite data assumption, we stress the importance of this result as a guarantee for the algorithm. First, a guarantee of correct parameter values in the limit of infinite data is typically the starting point for a sample complexity result (see for example (Hsu et al., 2009; Anandkumar et al., 2012)). Second, our sense is that a sample complexity result can be derived for our algorithm using standard methods: specifically, the analysis in (Arora et al., 2012) gives one set of guarantees; the remaining optimization problems we solve are convex maximum-likelihood problems, which are also relatively easy to analyze. Note that several pieces of previous work on spectral methods for latentvariable models focus on algorithms that are correct under the infinite data assumption. 5 The Matrix Decomposition Algorithm This section describes the matrix decomposition algorithm used in Step 1 of the l</context>
</contexts>
<marker>Anandkumar, Ge, Hsu, Kakade, Telgarsky, 2012</marker>
<rawString>A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. 2012. Tensor decompositions for learning latent-variable models. arXiv:1210.7559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Arora</author>
<author>R Ge</author>
<author>A Moitra</author>
</authors>
<title>Learning topic models–going beyond SVD.</title>
<date>2012</date>
<booktitle>In Proceedings of FOCS.</booktitle>
<contexts>
<context position="2607" citStr="Arora et al. (2012)" startWordPosition="411" endWordPosition="414">ty mcollins@cs.columbia.edu tations. The resulting parameter estimates give us significant leverage over the learning problem. 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. The algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one “pivot” feature. This assumption is similar to the “pivot word” assumption used by Arora et al. (2013) and Arora et al. (2012) in the context of learning topic models. We describe experiments on learning of LPCFGs, and also on learning of the latent-variable language model of Saul and Pereira (1997). A hybrid method, which uses our algorithm as an initializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applic</context>
<context position="10956" citStr="Arora et al. (2012)" startWordPosition="2016" endWordPosition="2019">s τ and ρ, we define additional random variables: F = τ(T1), F2 = τ(T2), F3 = τ(T3), and G = ρ(O). We can now give the following assumption: Assumption 1 (The Pivot Assumption). Under the L-PCFG being learned, there exist values α &gt; 0 and β &gt; 0 such that for each non-terminal a, for each hidden state h ∈ [m], the following statements are true: 1) ∃f ∈ F such that P(F = f |H1 = h, A = a) &gt; α and for all h&apos; =6 h, P(F = f |H1 = h&apos;, A = a) = 0; 2) ∃g ∈ G such that P(G = g |H1 = h, A = a) &gt; β and for all h&apos; =6h, P(G = g |H1 = h&apos;,A = a) = 0. This assumption is very similar to the assumption made by Arora et al. (2012) in the context of learning topic models. It implies that for each (a, h) pair, there are inside and outside tree features—which following Arora et al. (2012) we refer to as pivot features—that occur only2 in the presence of latent-state value h. As in (Arora et al., 2012), the pivot features will give us considerable leverage in learning of the model. 4.2 The Learning Algorithm Figure 1 shows the learning algorithm for LPCFGs. The algorithm consists of the following steps: Step 0: Calculate estimates ˆp(a → b c |a), ˆp(g, f2, f3 |a → b c) and ˆp(f, g |a). These estimates are easily calculated</context>
<context position="20074" citStr="Arora et al., 2012" startWordPosition="3853" endWordPosition="3856"> with probability at least 1 − 6, as a function of c, 6, and other relevant quantities such as n, d, d&apos;, m, α, β and so on. In spite of the strength of the infinite data assumption, we stress the importance of this result as a guarantee for the algorithm. First, a guarantee of correct parameter values in the limit of infinite data is typically the starting point for a sample complexity result (see for example (Hsu et al., 2009; Anandkumar et al., 2012)). Second, our sense is that a sample complexity result can be derived for our algorithm using standard methods: specifically, the analysis in (Arora et al., 2012) gives one set of guarantees; the remaining optimization problems we solve are convex maximum-likelihood problems, which are also relatively easy to analyze. Note that several pieces of previous work on spectral methods for latentvariable models focus on algorithms that are correct under the infinite data assumption. 5 The Matrix Decomposition Algorithm This section describes the matrix decomposition algorithm used in Step 1 of the learning algorithm. 5.1 Problem Setting Our goal will be to solve the following matrix decomposition problem: Matrix Decomposition Problem (MDP) 1. Design an algori</context>
<context position="21845" citStr="Arora et al. (2012)" startWordPosition="4186" endWordPosition="4189">permutation σ : [m] → [m] such that ∀h, ˆπ(h) = π(σ(h)), ∀f, h, ˆr(f | h) = r(f |σ(h)), and ∀g, h, ˆs(g |h) = s(g | σ(h)). The definition of α-separability is as follows (β- separability for s(g |h) is analogous): Definition 2 (α-separability). The parameters r(f |h) are α-separable iffor all h ∈ [m], there is some j ∈ [d] such that: 1) r(j |h) ≥ α; and 2) r(j |h0) = 0 for h0 =6h. s(g |h) are returned by the algorithm. Thus an algorithm that solves MDP 2 in some sense solves “one half” of MDP 1. For completeness we give a sketch of the algorithm that we use; it is inspired by the algorithm of Arora et al. (2012), but has some important differences. The algorithm is as follows: Step 1: Derive a function φ : [d0] → Rl that maps each integer g ∈ [d0] to a representation φ(g) ∈ Rl. The integer l is typically much smaller than d0, implying that the representation is of low dimension. Arora et al. (2012) derive φ as a random projection with a carefully chosen dimension l. In our experiments, we use canonical correlation analysis (CCA) on the matrix Q to give a representation φ(g) ∈ Rl where l = m. Step 2: For each f ∈ [d], calculate vf = E[φ(g) |f] = X d&apos; p(g |f)φ(g) g=1 where p(g |f) = Qf,g/Pg Qf,g. It fo</context>
<context position="24216" citStr="Arora et al., 2012" startWordPosition="4644" endWordPosition="4647">h) = r(f |σ(h)). This is identical to Matrix Decomposition Problem 1, but without the requirement that the values where wh ∈ Rl is equal to Pd&apos; g=1 p(g |h)φ(g). Hence the vf vectors lie in the convex hull of a set of vectors w1 ... wm ∈ Rl. Crucially, for any pivot word f for latent state h, we have p(h |f) = 1, hence vf = wh. Thus by the pivot assumption, the set of points v1 ... vd includes the vertices of the convex hull. Each point vj is a convex combination of the vertices w1 ... wm, where the weights in this combination are equal to p(h |j). Step 3: Use the FastAnchorWords algorithm of (Arora et al., 2012) to identify m vectors vs1, vs2, ... vsm. The FastAnchorWords algorithm has the guarantee that there is a permutation σ : [m] → [m] such that vsi = wσ(i) for all i. This algorithm recovers the vertices of the convex hull described in step 2, using a method that greedily picks points that are as far as possible from the subspace spanned by previously picked points. Step 4: For each f ∈ [d] solve the problem arg min γ1,γ2,...,γm subject to γh ≥ 0 and Ph γh = 1. We use the algorithm of (Frank and Wolfe, 1956; Clarkson, 2010) for this purpose. Set q(h |f) = γh. γhvsh − vf||2 X || h 1057 Return the</context>
</contexts>
<marker>Arora, Ge, Moitra, 2012</marker>
<rawString>S. Arora, R. Ge, and A. Moitra. 2012. Learning topic models–going beyond SVD. In Proceedings of FOCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Arora</author>
<author>R Ge</author>
<author>Y Halpern</author>
<author>D M Mimno</author>
<author>A Moitra</author>
<author>D Sontag</author>
<author>Y Wu</author>
<author>M Zhu</author>
</authors>
<title>A practical algorithm for topic modeling with provable guarantees.</title>
<date>2013</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="2583" citStr="Arora et al. (2013)" startWordPosition="406" endWordPosition="409">cience Columbia University mcollins@cs.columbia.edu tations. The resulting parameter estimates give us significant leverage over the learning problem. 2) Optimization of a convex objective function using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. The algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one “pivot” feature. This assumption is similar to the “pivot word” assumption used by Arora et al. (2013) and Arora et al. (2012) in the context of learning topic models. We describe experiments on learning of LPCFGs, and also on learning of the latent-variable language model of Saul and Pereira (1997). A hybrid method, which uses our algorithm as an initializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe</context>
<context position="4809" citStr="Arora et al. (2013)" startWordPosition="770" endWordPosition="773">, June 23-25 2014. c�2014 Association for Computational Linguistics linear transformation, which cancels in the insideoutside calculations for marginalization over latent states in the L-PCFG. The lack of direct parameter estimates from this method leads to problems with negative or unnormalized probablities; the method does not give parameters that are interpretable, or that can be used in conjunction with other algorithms, for example as an initializer for EM steps that refine the model. Our work is most directly related to the algorithm for parameter estimation in topic models described by Arora et al. (2013). This algorithm forms the core of the matrix decomposition algorithm described in section 5. 3 Background This section gives definitions and notation for LPCFGs, taken from (Cohen et al., 2012). 3.1 L-PCFGs: Basic Definitions An L-PCFG is an 8-tuple (N, Z, P, m, n, 7r, t, q) where: N is the set of non-terminal symbols in the grammar. Z C N is a finite set of in-terminals. P C N is a finite set of pre-terminals. We assume that N = Z U P, and Z n P = 0. Hence we have partitioned the set of non-terminals into two subsets. [m] is the set of possible hidden states.1 [n] is the set of possible word</context>
<context position="23031" citStr="Arora et al. (2013)" startWordPosition="4415" endWordPosition="4418">ere p(g |f) = Qf,g/Pg Qf,g. It follows that This matrix decomposition problem has clear vf = X d&apos; Xm p(h |f)p(g |h)φ(g) = Xm p(h |f)wh relevance to problems in learning of latent- g=1 h=1 h=1 variable models, and in particular is a core step of the algorithm in figure 1. When given a matrix ˆQa with entries h, a), where p(...) refers to a distribution derived ˆQaf,g = Ph p(h |a)p(f |h, a)p(g | from an underlying L-PCFG which satisfies the pivot assumption, the method will recover the values for p(h |a), p(f |h, a) and p(g |h, a) up to a permutation over the latent states. 5.2 The Algorithm of Arora et al. (2013) This section describes a variant of the algorithm of Arora et al. (2013), which is used as a component of our algorithm for MDP 1. One of the properties of this algorithm is that it solves the following problem: Matrix Decomposition Problem (MDP) 2. Design an algorithm with the following inputs, assumptions, and outputs: Inputs: Same as matrix decomposition problem 1. Assumptions: The parameters r(f |h) of the model are α-separable for some value α &gt; 0. Outputs: Estimates ˆπ(h) and ˆr(f |h) such that ∃σ : [m] → [m] such that ∀h, ˆπ(h) = π(σ(h)), ∀f, h, ˆr(f |h) = r(f |σ(h)). This is identical</context>
</contexts>
<marker>Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, Zhu, 2013</marker>
<rawString>S. Arora, R. Ge, Y. Halpern, D. M. Mimno, A. Moitra, D. Sontag, Y. Wu, and M. Zhu. 2013. A practical algorithm for topic modeling with provable guarantees. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bailly</author>
<author>A Habrar</author>
<author>F Denis</author>
</authors>
<title>A spectral approach for probabilistic grammatical inference on trees.</title>
<date>2010</date>
<booktitle>In Proceedings of ALT.</booktitle>
<contexts>
<context position="3545" citStr="Bailly et al., 2010" startWordPosition="564" endWordPosition="567">tions for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguisti</context>
</contexts>
<marker>Bailly, Habrar, Denis, 2010</marker>
<rawString>R. Bailly, A. Habrar, and F. Denis. 2010. A spectral approach for probabilistic grammatical inference on trees. In Proceedings of ALT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Balle</author>
<author>A Quattoni</author>
<author>X Carreras</author>
</authors>
<title>A spectral learning algorithm for finite state transducers.</title>
<date>2011</date>
<booktitle>In Proceedings of ECML.</booktitle>
<contexts>
<context position="3608" citStr="Balle et al., 2011" startWordPosition="576" endWordPosition="579"> typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, USA, June 23-25 2014.</context>
</contexts>
<marker>Balle, Quattoni, Carreras, 2011</marker>
<rawString>B. Balle, A. Quattoni, and X. Carreras. 2011. A spectral learning algorithm for finite state transducers. In Proceedings of ECML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>H Schwenk</author>
<author>J-S Sen´ecal</author>
<author>F Morin</author>
<author>J-L Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<publisher>Springer.</publisher>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Y. Bengio, H. Schwenk, J.-S. Sen´ecal, F. Morin, and J.-L. Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Clarkson</author>
</authors>
<title>Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm.</title>
<date>2010</date>
<journal>ACM Transactions on Algorithms (TALG),</journal>
<volume>6</volume>
<issue>4</issue>
<contexts>
<context position="24743" citStr="Clarkson, 2010" startWordPosition="4747" endWordPosition="4748"> are equal to p(h |j). Step 3: Use the FastAnchorWords algorithm of (Arora et al., 2012) to identify m vectors vs1, vs2, ... vsm. The FastAnchorWords algorithm has the guarantee that there is a permutation σ : [m] → [m] such that vsi = wσ(i) for all i. This algorithm recovers the vertices of the convex hull described in step 2, using a method that greedily picks points that are as far as possible from the subspace spanned by previously picked points. Step 4: For each f ∈ [d] solve the problem arg min γ1,γ2,...,γm subject to γh ≥ 0 and Ph γh = 1. We use the algorithm of (Frank and Wolfe, 1956; Clarkson, 2010) for this purpose. Set q(h |f) = γh. γhvsh − vf||2 X || h 1057 Return the final quantities: Xˆπ(h) = f p(f)q(h|f) ˆr(f|h) = p(f)q(h|f) P f p(f)q(h|f) where p(f) = Pg Qf,g. 5.3 An Algorithm for MDP 1 Figure 2 shows an algorithm that solves MDP 1. In steps 1 and 2 of the algorithm, the algorithm of section 5.2 is used to recover estimates ˆr(f | h) and ˆs(g |h). These distributions are equal to p(f |h) and p(g |h) up to permutations σ and σ&apos; of the latent states respectively; unfortunately there is no guarantee that σ and σ&apos; are the same permutation. Step 3 estimates parameters t(h&apos; | h) that ef</context>
</contexts>
<marker>Clarkson, 2010</marker>
<rawString>K. L. Clarkson. 2010. Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm. ACM Transactions on Algorithms (TALG), 6(4):63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Stratos</author>
<author>M Collins</author>
<author>D F Foster</author>
<author>L Ungar</author>
</authors>
<title>Spectral learning of latent-variable PCFGs.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1001" citStr="Cohen et al., 2012" startWordPosition="147" endWordPosition="150">in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates of the original parameters of an LPCFG, but, unlike EM, does not suffer from problems of local optima. The algorithm relies on two key ideas: 1) A matrix decomposition algorithm (section 5) which is applicable to matrices Q of the form Qf,</context>
<context position="3916" citStr="Cohen et al., 2012" startWordPosition="627" endWordPosition="630">researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics linear transformation, which cancels in the insideoutside calculations for marginalization over latent states in the L-PCFG. The lack of direct parameter estimates from this method leads to problems with negative or unnormalized probablities; the method does</context>
<context position="31679" citStr="Cohen et al. (2012)" startWordPosition="6026" endWordPosition="6029">rformance on development data. Results for section 23 are used with the best model for section 22 in the corresponding row. The results for EM and spectral are reported from Cohen et al. (2013). in Matsuzaki et al. (2005). For the pivot algorithm we use multiple features τ1(t) ... τK(t) and ρ1(o) ... ρL(o) over inside and outside trees, using the features described by Cohen et al. (2013). Table 1 gives the F1 accuracy on the development and test sets for the following methods: EM: The EM algorithm as used by Matsuzaki et al. (2005) and Petrov et al. (2006). Spectral: The spectral algorithm of Cohen et al. (2012) and Cohen et al. (2013). Pivot: The algorithm described in this paper. Pivot+EM: The algorithm described in this paper, followed by 1 or more iterations of the EM algorithm with parameters initialized by the pivot algorithm. (See section 6.3.) For the EM and Pivot+EM algorithms, we give the number of iterations of EM required to reach optimal performance on the development data. The results show that the EM, Spectral, and Pivot+EM algorithms all perform at a very similar level of accuracy. The Pivot+EM results show that very few EM iterations—just 2 iterations in most conditions—are required </context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2012</marker>
<rawString>S. B. Cohen, K. Stratos, M. Collins, D. F. Foster, and L. Ungar. 2012. Spectral learning of latent-variable PCFGs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S B Cohen</author>
<author>K Stratos</author>
<author>M Collins</author>
<author>D P Foster</author>
<author>L Ungar</author>
</authors>
<title>Experiments with spectral learning of latent-variable PCFGs.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="3937" citStr="Cohen et al., 2013" startWordPosition="631" endWordPosition="634">eloped provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics linear transformation, which cancels in the insideoutside calculations for marginalization over latent states in the L-PCFG. The lack of direct parameter estimates from this method leads to problems with negative or unnormalized probablities; the method does not give parameters </context>
<context position="9858" citStr="Cohen et al. (2013)" startWordPosition="1792" endWordPosition="1795">) can be thought of as a function that maps an inside tree t (outside tree o) to an underlying feature. As one example, the function τ(t) might return the context-free rule at the root of the inside tree t; in this case the set F would be equal to the set of all context-free rules in the grammar. As another example, the function ρ(o) might return the context-free rule at the foot of the outside tree o. In the more general case, we might have K separate functions τ(k)(t) for k = 1... K mapping inside trees to K separate features, and similarly we might have multiple features for outside trees. Cohen et al. (2013) describe one such feature definition, where features track single context-free rules as well as larger fragments such as two or three-level sub-trees. For simplicity of presentation we describe the case of single features τ(t) and ρ(o) for the majority of this paper. The extension to multiple features is straightforward, and is discussed in section 6.2; the flexibility allowed by multiple features is important, and we use multiple features in our experiments. Given functions τ and ρ, we define additional random variables: F = τ(T1), F2 = τ(T2), F3 = τ(T3), and G = ρ(O). We can now give the fo</context>
<context position="30503" citStr="Cohen et al. (2013)" startWordPosition="5810" endWordPosition="5813">losest local maxima of the likelihood function, then the resulting estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of (Lehmann and Casella, 1998). 7 Experiments on Parsing This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 was used as the test set. The experimental setup is the same as described by Cohen et al. (2013). The trees are binarized (Petrov et al., 2006) and for the EM algorithm we use the initialization method described M 8 sec. 22 32 sec. 23 16 24 86.69 EM 88.35 88.56 87.76 40 88.32 30 20 30 Spectral 85.60 87.77 88.53 88.82 88.05 Pivot 83.56 86.00 86.87 86.40 85.83 Pivot+EM 86.83 88.14 88.64 88.55 88.03 2 6 2 2 Table 1: Results on the development data (section 22) and test data (section 23) for various learning algorithms for LPCFGs. For EM and pivot+EM experiments, the second line denotes the number of iterations required to reach the given optimal performance on development data. Results for </context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2013</marker>
<rawString>S. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar. 2013. Experiments with spectral learning of latent-variable PCFGs. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood estimation from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<pages>39--1</pages>
<contexts>
<context position="919" citStr="Dempster et al., 1977" startWordPosition="132" endWordPosition="135">mposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates of the original parameters of an LPCFG, but, unlike EM, does not suffer from problems of local optima. The algorithm relies on two key ideas: 1) A matrix dec</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dhillon</author>
<author>J Rodu</author>
<author>M Collins</author>
<author>D P Foster</author>
<author>L H Ungar</author>
</authors>
<title>Spectral dependency parsing with latent variables.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3650" citStr="Dhillon et al., 2012" startWordPosition="584" endWordPosition="587">convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Ling</context>
</contexts>
<marker>Dhillon, Rodu, Collins, Foster, Ungar, 2012</marker>
<rawString>P. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. H. Ungar. 2012. Spectral dependency parsing with latent variables. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Frank</author>
<author>P Wolfe</author>
</authors>
<title>An algorithm for quadratic programming. Naval research logistics quarterly,</title>
<date>1956</date>
<pages>3--1</pages>
<contexts>
<context position="24726" citStr="Frank and Wolfe, 1956" startWordPosition="4743" endWordPosition="4746">hts in this combination are equal to p(h |j). Step 3: Use the FastAnchorWords algorithm of (Arora et al., 2012) to identify m vectors vs1, vs2, ... vsm. The FastAnchorWords algorithm has the guarantee that there is a permutation σ : [m] → [m] such that vsi = wσ(i) for all i. This algorithm recovers the vertices of the convex hull described in step 2, using a method that greedily picks points that are as far as possible from the subspace spanned by previously picked points. Step 4: For each f ∈ [d] solve the problem arg min γ1,γ2,...,γm subject to γh ≥ 0 and Ph γh = 1. We use the algorithm of (Frank and Wolfe, 1956; Clarkson, 2010) for this purpose. Set q(h |f) = γh. γhvsh − vf||2 X || h 1057 Return the final quantities: Xˆπ(h) = f p(f)q(h|f) ˆr(f|h) = p(f)q(h|f) P f p(f)q(h|f) where p(f) = Pg Qf,g. 5.3 An Algorithm for MDP 1 Figure 2 shows an algorithm that solves MDP 1. In steps 1 and 2 of the algorithm, the algorithm of section 5.2 is used to recover estimates ˆr(f | h) and ˆs(g |h). These distributions are equal to p(f |h) and p(g |h) up to permutations σ and σ&apos; of the latent states respectively; unfortunately there is no guarantee that σ and σ&apos; are the same permutation. Step 3 estimates parameters </context>
</contexts>
<marker>Frank, Wolfe, 1956</marker>
<rawString>M. Frank and P. Wolfe. 1956. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="3842" citStr="Hotelling, 1936" startWordPosition="618" endWordPosition="619">latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics linear transformation, which cancels in the insideoutside calculations for marginalization over latent states in the L-PCFG. The lack of direct parameter estimates from this method lea</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>H. Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hsu</author>
<author>S M Kakade</author>
<author>T Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden Markov models.</title>
<date>2009</date>
<booktitle>In Proceedings of COLT.</booktitle>
<contexts>
<context position="3524" citStr="Hsu et al., 2009" startWordPosition="560" endWordPosition="563">cantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Co</context>
<context position="19885" citStr="Hsu et al., 2009" startWordPosition="3822" endWordPosition="3825">values. A more detailed analysis of the algorithm would derive sample complexity results, giving guarantees on the sample size M required to reach a level of accuracy c in the estimates, with probability at least 1 − 6, as a function of c, 6, and other relevant quantities such as n, d, d&apos;, m, α, β and so on. In spite of the strength of the infinite data assumption, we stress the importance of this result as a guarantee for the algorithm. First, a guarantee of correct parameter values in the limit of infinite data is typically the starting point for a sample complexity result (see for example (Hsu et al., 2009; Anandkumar et al., 2012)). Second, our sense is that a sample complexity result can be derived for our algorithm using standard methods: specifically, the analysis in (Arora et al., 2012) gives one set of guarantees; the remaining optimization problems we solve are convex maximum-likelihood problems, which are also relatively easy to analyze. Note that several pieces of previous work on spectral methods for latentvariable models focus on algorithms that are correct under the infinite data assumption. 5 The Matrix Decomposition Algorithm This section describes the matrix decomposition algorit</context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2009</marker>
<rawString>D. Hsu, S. M. Kakade, and T. Zhang. 2009. A spectral algorithm for learning hidden Markov models. In Proceedings of COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="33910" citStr="Kneser and Ney, 1995" startWordPosition="6452" endWordPosition="6455"> 1 1 20 14 13 15 10 19 12 Table 2: Language model perplexity with the Brown corpus and the Gigaword corpus (New York Times portion) for the second half of the development set, and the test set. With EM and Pivot+EM, the number of iterations for EM to reach convergence is given below the perplexity. The best result for each column (for each m value) is in bold. The “test” column gives perplexity results on the test set. Each perplexity calculation on the test set is done using the best model on the development set. bi-KN+int and tri-KN+int are bigram and trigram Kneser-Ney interpolated models (Kneser and Ney, 1995), using the SRILM toolkit. s(w2 |h) are parameters of the approach. The conventional approach to estimation of the parameters r(h |w1) and s(w2 |h) from a corpus is to use the EM algorithm. In this section we compare the EM algorithm to a pivot-based method. It is straightforward to represent this model as an L-PCFG, and hence to use our implementation for estimation. In this special case, the L-PCFG learning algorithm is equivalent to a simple algorithm, with the following steps: 1) define the matrix Q with entries Qw1,w2 = count(w1, w2)/N where count(w1, w2) is the number of times that bigra</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E L Lehmann</author>
<author>G Casella</author>
</authors>
<date>1998</date>
<publisher>Springer.</publisher>
<note>Theory of Point Estimation (Second edition).</note>
<contexts>
<context position="30117" citStr="Lehmann and Casella, 1998" startWordPosition="5743" endWordPosition="5746">ifferent combination of inside and outside features. The function remains concave. 6.3 Use as an Initializer for EM The learning algorithm for L-PCFGs can be used as an initializer for the EM algorithm for LPCFGs. Two-step estimation methods such as these are well known in statistics; there are guarantees for example that if the first estimator is consistent, and the second step finds the closest local maxima of the likelihood function, then the resulting estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of (Lehmann and Casella, 1998). 7 Experiments on Parsing This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 was used as the test set. The experimental setup is the same as described by Cohen et al. (2013). The trees are binarized (Petrov et al., 2006) and for the EM algorithm we use the initialization method described M 8 sec. 22 32 sec. 23 16 24 86.69 EM 88.35 88.56 87.76 40 88.32 30 20 30 Spectral 85.60 87.77 88.</context>
</contexts>
<marker>Lehmann, Casella, 1998</marker>
<rawString>E. L. Lehmann and G. Casella. 1998. Theory of Point Estimation (Second edition). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="30279" citStr="Marcus et al., 1993" startWordPosition="5768" endWordPosition="5771"> initializer for the EM algorithm for LPCFGs. Two-step estimation methods such as these are well known in statistics; there are guarantees for example that if the first estimator is consistent, and the second step finds the closest local maxima of the likelihood function, then the resulting estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of (Lehmann and Casella, 1998). 7 Experiments on Parsing This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 was used as the test set. The experimental setup is the same as described by Cohen et al. (2013). The trees are binarized (Petrov et al., 2006) and for the EM algorithm we use the initialization method described M 8 sec. 22 32 sec. 23 16 24 86.69 EM 88.35 88.56 87.76 40 88.32 30 20 30 Spectral 85.60 87.77 88.53 88.82 88.05 Pivot 83.56 86.00 86.87 86.40 85.83 Pivot+EM 86.83 88.14 88.64 88.55 88.03 2 6 2 2 Table 1: Results on the development data (section 22) and test d</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsuzaki</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Probabilistic CFG with latent annotations.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="741" citStr="Matsuzaki et al., 2005" startWordPosition="106" endWordPosition="109">urgh scohen@inf.ed.ac.uk Abstract We introduce a provably correct learning algorithm for latent-variable PCFGs. The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like E</context>
<context position="31281" citStr="Matsuzaki et al. (2005)" startWordPosition="5953" endWordPosition="5956">8.35 88.56 87.76 40 88.32 30 20 30 Spectral 85.60 87.77 88.53 88.82 88.05 Pivot 83.56 86.00 86.87 86.40 85.83 Pivot+EM 86.83 88.14 88.64 88.55 88.03 2 6 2 2 Table 1: Results on the development data (section 22) and test data (section 23) for various learning algorithms for LPCFGs. For EM and pivot+EM experiments, the second line denotes the number of iterations required to reach the given optimal performance on development data. Results for section 23 are used with the best model for section 22 in the corresponding row. The results for EM and spectral are reported from Cohen et al. (2013). in Matsuzaki et al. (2005). For the pivot algorithm we use multiple features τ1(t) ... τK(t) and ρ1(o) ... ρL(o) over inside and outside trees, using the features described by Cohen et al. (2013). Table 1 gives the F1 accuracy on the development and test sets for the following methods: EM: The EM algorithm as used by Matsuzaki et al. (2005) and Petrov et al. (2006). Spectral: The spectral algorithm of Cohen et al. (2012) and Cohen et al. (2013). Pivot: The algorithm described in this paper. Pivot+EM: The algorithm described in this paper, followed by 1 or more iterations of the EM algorithm with parameters initialized </context>
</contexts>
<marker>Matsuzaki, Miyao, Tsujii, 2005</marker>
<rawString>T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with latent annotations. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Parikh</author>
<author>L Song</author>
<author>E P Xing</author>
</authors>
<title>A spectral algorithm for latent tree graphical models.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="3588" citStr="Parikh et al., 2011" startWordPosition="572" endWordPosition="575">-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, Baltimore, Maryland, U</context>
</contexts>
<marker>Parikh, Song, Xing, 2011</marker>
<rawString>A. Parikh, L. Song, and E. P. Xing. 2011. A spectral algorithm for latent tree graphical models. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="763" citStr="Petrov et al., 2006" startWordPosition="110" endWordPosition="113"> Abstract We introduce a provably correct learning algorithm for latent-variable PCFGs. The algorithm relies on two steps: first, the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample; second, the use of EM applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition. Experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice. 1 Introduction Latent-variable PCFGs (L-PCFGs) (Matsuzaki et al., 2005; Petrov et al., 2006) give state-of-the-art performance on parsing problems. The standard approach to parameter estimation in L-PCFGs is the EM algorithm (Dempster et al., 1977), which has the usual problems with local optima. Recent work (Cohen et al., 2012) has introduced an alternative algorithm, based on spectral methods, which has provable guarantees. Unfortunately this algorithm does not return parameter estimates for the underlying L-PCFG, instead returning the parameter values up to an (unknown) linear transform. In practice, this is a limitation. We describe an algorithm that, like EM, returns estimates o</context>
<context position="30550" citStr="Petrov et al., 2006" startWordPosition="5819" endWordPosition="5822">, then the resulting estimator is both consistent and efficient (in terms of number of samples required). See for example page 453 or Theorem 4.3 (page 454) of (Lehmann and Casella, 1998). 7 Experiments on Parsing This section describes parsing experiments using the learning algorithm for L-PCFGs. We use the Penn WSJ treebank (Marcus et al., 1993) for our experiments. Sections 2–21 were used as training data, and sections 0 and 22 were used as development data. Section 23 was used as the test set. The experimental setup is the same as described by Cohen et al. (2013). The trees are binarized (Petrov et al., 2006) and for the EM algorithm we use the initialization method described M 8 sec. 22 32 sec. 23 16 24 86.69 EM 88.35 88.56 87.76 40 88.32 30 20 30 Spectral 85.60 87.77 88.53 88.82 88.05 Pivot 83.56 86.00 86.87 86.40 85.83 Pivot+EM 86.83 88.14 88.64 88.55 88.03 2 6 2 2 Table 1: Results on the development data (section 22) and test data (section 23) for various learning algorithms for LPCFGs. For EM and pivot+EM experiments, the second line denotes the number of iterations required to reach the given optimal performance on development data. Results for section 23 are used with the best model for sec</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Saul</author>
<author>F Pereira</author>
</authors>
<title>Aggregate and mixedorder markov models for statistical language processing.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2781" citStr="Saul and Pereira (1997)" startWordPosition="440" endWordPosition="443">ion using EM. We show that once the matrix decomposition step has been applied, parameter estimation of the L-PCFG can be reduced to a convex optimization problem that is easily solved by EM. The algorithm provably learns the parameters of an L-PCFG (theorem 1), under an assumption that each latent state has at least one “pivot” feature. This assumption is similar to the “pivot word” assumption used by Arora et al. (2013) and Arora et al. (2012) in the context of learning topic models. We describe experiments on learning of LPCFGs, and also on learning of the latent-variable language model of Saul and Pereira (1997). A hybrid method, which uses our algorithm as an initializer for EM, performs at the same accuracy as EM, but requires significantly fewer iterations for convergence: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in </context>
<context position="32576" citStr="Saul and Pereira (1997)" startWordPosition="6177" endWordPosition="6180">ithms, we give the number of iterations of EM required to reach optimal performance on the development data. The results show that the EM, Spectral, and Pivot+EM algorithms all perform at a very similar level of accuracy. The Pivot+EM results show that very few EM iterations—just 2 iterations in most conditions—are required to reach optimal performance when the Pivot model is used as an initializer for EM. The Pivot results lag behind the Pivot+EM results by around 2-3%, but they are close enough to optimality to require very few EM iterations when used as an initializer. 8 Experiments on the Saul and Pereira (1997) Model for Language Modeling We now describe a second set of experiments, on the Saul and Pereira (1997) model for language modeling. Define V to be the set of words in the vocabulary. For any w1, w2 E V , the Saul and Pereira (1997) model then defines p(w2 |w1) = Emh=1 r(h |w1)s(w2 |h) where r(h |w1) and � gi,fj2 ,fk3 � i,j,k 1059 Brown NYT m 2 4 8 16 32 128 256 test 2 4 8 16 32 128 256 test EM 737 599 488 468 430 388 365 364 926 733 562 420 361 284 265 267 14 14 19 12 10 9 8 36 39 42 33 38 35 32 bi-KN +int. 408 415 271 279 tri-KN+int. 386 394 150 158 pivot 852 718 605 559 537 426 597 560 122</context>
</contexts>
<marker>Saul, Pereira, 1997</marker>
<rawString>L. Saul and F. Pereira. 1997. Aggregate and mixedorder markov models for statistical language processing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siddiqi</author>
<author>B Boots</author>
<author>G Gordon</author>
</authors>
<title>Reducedrank hidden markov models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--741</pages>
<contexts>
<context position="3567" citStr="Siddiqi et al., 2010" startWordPosition="568" endWordPosition="571">: for example in our L-PCFG experiments, it typically requires 2 EM iterations for convergence, as opposed to 20-40 EM iterations for initializers used in previous work. While this paper’s focus is on L-PCFGs, the techniques we describe are likely to be applicable to many other latent-variable models used in NLP. 2 Related Work Recently a number of researchers have developed provably correct algorithms for parameter estimation in latent variable models such as hidden Markov models, topic models, directed graphical models with latent variables, and so on (Hsu et al., 2009; Bailly et al., 2010; Siddiqi et al., 2010; Parikh et al., 2011; Balle et al., 2011; Arora et al., 2013; Dhillon et al., 2012; Anandkumar et al., 2012; Arora et al., 2012; Arora et al., 2013). Many of these algorithms have their roots in spectral methods such as canonical correlation analysis (CCA) (Hotelling, 1936), or higher-order tensor decompositions. Previous work (Cohen et al., 2012; Cohen et al., 2013) has developed a spectral method for learning of L-PCFGs; this method learns parameters of the model up to an unknown 1052 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1052–1061, B</context>
</contexts>
<marker>Siddiqi, Boots, Gordon, 2010</marker>
<rawString>S. Siddiqi, B. Boots, and G. Gordon. 2010. Reducedrank hidden markov models. Journal of Machine Learning Research, 9:741–748.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>