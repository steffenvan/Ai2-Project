<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092804">
<title confidence="0.9961325">
Conditional Random Fields and Support Vector Machines for Disorder
Named Entity Recognition in Clinical Texts
</title>
<author confidence="0.999367">
Dingcheng Li Karin Kipper-Schuler Guergana Savova
</author>
<affiliation confidence="0.952039">
University of Minnesota Mayo Clinic College of Medicine Mayo Clinic College of Medicine
Minneapolis, Minnesota, USA Rochester, Minnesota, USA Rochester, Minnesota, USA
</affiliation>
<email confidence="0.991803">
lixxx345@umn.edu schuler.karin@mayo.edu savova.guergana@mayo.edu
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957">
We present a comparative study between
two machine learning methods, Conditional
Random Fields and Support Vector Ma-
chines for clinical named entity recognition.
We explore their applicability to clinical
domain. Evaluation against a set of gold
standard named entities shows that CRFs
outperform SVMs. The best F-score with
CRFs is 0.86 and for the SVMs is 0.64 as
compared to a baseline of 0.60.
</bodyText>
<sectionHeader confidence="0.968237" genericHeader="categories and subject descriptors">
1 Introduction and background
</sectionHeader>
<bodyText confidence="0.999848736842106">
Named entity recognition (NER) is the discovery
of named entities (NEs), or textual mentions that
belong to the same semantic class. In the biomedi-
cal domain NEs are diseases, signs/symptoms, ana-
tomical signs, and drugs. NER performance is high
as applied to scholarly text and newswire narra-
tives (Leaman et al., 2008). Clinical free-text, on
the other hand, exhibits characteristics of both in-
formal and formal linguistic styles which, in turn,
poses challenges for clinical NER. Conditional
Random Fields (CRFs) (Lafferty et al., 2001) and
and Support Vector Machines (SVMs) (Cortes and
Vapnik, 1995) are machine learning techniques
which can handle multiple features during learn-
ing. CRFs’ main strength lies in their ability to in-
clude various unrelated features, while SVMs’ in
the inclusion of overlapping features. Our goal is
to compare CRFs and SVMs performance for
clinical NER with focus on disease/disorder NEs.
</bodyText>
<sectionHeader confidence="0.663473" genericHeader="general terms">
2 Dataset and features
</sectionHeader>
<bodyText confidence="0.999903428571429">
Our dataset is a gold standard corpus of 1557 sin-
gle- and multi-word disorder annotations (Ogren et
al., 2008). For training and testing the CRF and
SVM models the IOB (inside-outside-begin) nota-
tion (Leaman, 2008) was applied. In our project,
we used 1265 gold standard annotations for train-
ing and 292 for testing. The features used for the
</bodyText>
<page confidence="0.969386">
94
</page>
<bodyText confidence="0.9998590625">
learning process are described as follows. Diction-
ary look-up is a binary value feature that represents
if the NE is in the dictionary (SNOMED-CT). Bag
of Words (BOW) is a representation of the context
by the unique words in it. Part-of-speech tags
(POS) of BOW is the pos tags of the context
words. Window size is the number of tokens repre-
senting context surrounding the target word. Ori-
entation(left or right) is the location of the feature
in regard to the target word. Distance is the prox-
imity of the feature in regard to the target word
Capitalization has one of the four token-based val-
ues: all upper case, all lower case, mixed_case and
initial upper case. Number features refer to the
presence or absence of related numbers. Feature
sets are in Table 1.
</bodyText>
<sectionHeader confidence="0.999069" genericHeader="evaluation">
3 Results and discussion
</sectionHeader>
<bodyText confidence="0.99975875">
Figure 1 shows the CRF results. The F-scores, re-
call and precision for the baseline dictionary look-
up are 0.604, 0.468 and 0.852 respectively. When
BOW is applied in feature combination 2 results
improve sharply adding 0.15, 0.17 and 0.08 points
respectively. The F-score, recall and precision im-
prove even further with the capitalization feature to
0.858, 0.774 and 0.963 respectively. Figure 2
shows SVM results. The addition of more features
to the model did not show an upward trend. The
best results are with feature combination 1 and 3.
The F-score reaches 0.643, which although an im-
provement over the baseline greatly underperforms
CRF results. BOW features seem not discrimina-
tive with SVMs. When the window size increases
to 5, performance decreases as demonstrated in
feature combinations 2, 4 and 8. Results with fea-
ture combination 4, in particular, has a pronounced
downward trend. Its F-score is 0.612, a decrease by
0.031 compared with Test 1 or Test 3. Its recall
and precision are 0.487 and 0.822 respectively, a
decrease by 0.036 and 0.01 respectively. This sup-
ports the results achieved with CRFs where a
smaller window size yields better performance.
</bodyText>
<note confidence="0.593469">
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 94–95,
</note>
<page confidence="0.638903">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics 1
</page>
<figure confidence="0.8473026">
No Features
1 dictionary look-up (baseline)
2 dictionary look-up+BOW+Orientation+distance (Win-
dow 5)
3 dictionary look-up + BOW + Orientation + distance
(Window 3)
4 dictionary look-up + BOW + POS + Orientation +
distance (Window 5)
5 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3)
6 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + bullet number
7 dictionary look-up + BOW + POS + Orientation +
distance(Window 3) + measurement
8 dictionary look-up + BOW + POS + Orientation +
distance (Window 5) + neighboring number
9 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + neighboring number
10 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3)+neighboring number+measurement
11 dictionary look-up+BOW+POS+Orientation (Window
3)+neighboring number+bullet number + measurement
12 dictionary look-up + BOW +POS + Orientation
+distance (Window 3) + neighboring number + bullet
number + measurement + capitalization
</figure>
<tableCaption confidence="0.887735">
Table 1: Feature combinations
</tableCaption>
<figureCaption confidence="0.999942">
Figure 1: CRF evaluation results
Figure 2: SVM evaluation results
</figureCaption>
<bodyText confidence="0.999990782608696">
As the results show, context represented by the
BOW feature plays an important role indicating the
importance of the words surrounding NEs. On the
other hand, POS tag features did not bring much
improvement, which perhaps hints at a hypothesis
that grammatical roles are not as important as con-
text in clinical text. Thirdly, a small window size is
more discriminative. Clinical notes are unstruc-
tured free text with short sentences. If a larger win-
dow size is used, many words will share similar
features. Fourthly, capitalization is highly dis-
criminative. Fifthly, as a finite state machine de-
rived from HMMs, CRFs can naturally consider
state-to-state dependences and feature-to-state de-
pendences. On the other hand, SVMs do not con-
sider such dependencies. SVMs separate the data
into categories via a kernel function. They imple-
ment this by mapping the data points onto an opti-
mal linear separating hyperplane. Finally, SVMs
do not behave well for large number of feature
values. For large number of feature values, it
would be more difficult to find discriminative lines
to categorize the labels.
</bodyText>
<sectionHeader confidence="0.988295" genericHeader="conclusions">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999958833333333">
We investigated the use of CRFs and SVMs for
disorder NER in clinical free-text. Our results
show that, in general, CRFs outperformed SVMs.
We demonstrated that well-chosen features along
with dictionary-based features tend to improve the
CRF model’s performance but not the SVM’s.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999987">
The work was partially supported by a Biomedical
Informatics and Computational Biology scholar-
ship from the University of Minnesota.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996652">
Corinna Cortes and Vladimir Vapnik. Support-vector
network. Machine Learning, 20:273-297, 1995.
John Lafferty, Andrew McCallum and Fernando
Pereira. Conditional Random Fields: Probabilistic
Models for Segmenting and Labeling Sequence Data.
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML-2001), 2001.
Robert Leaman and Graciela Gonzalez. BANNER: an
Executable Survey of Advances in Biomedical
Named Entity Recognition. Pacific Symposium on
Biocomputing 13:652-663. 2008.
Philip Ogren, Guergana Savova and Christopher G
Chute. Constructing evaluation corpora for auto-
mated clinical named entity recognition. Proc LREC
2008.
</reference>
<page confidence="0.9883945">
95
2
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.398999">
<title confidence="0.9991965">Conditional Random Fields and Support Vector Machines for Disorder Named Entity Recognition in Clinical Texts</title>
<author confidence="0.99861">Dingcheng Li Karin Kipper-Schuler Guergana Savova</author>
<affiliation confidence="0.999896">University of Minnesota Mayo Clinic College of Medicine Mayo Clinic College of Medicine</affiliation>
<address confidence="0.993878">Minneapolis, Minnesota, USA Rochester, Minnesota, USA Rochester, Minnesota, USA</address>
<email confidence="0.99903">lixxx345@umn.eduschuler.karin@mayo.edusavova.guergana@mayo.edu</email>
<abstract confidence="0.9756673">We present a comparative study between two machine learning methods, Conditional Random Fields and Support Vector Machines for clinical named entity recognition. We explore their applicability to clinical domain. Evaluation against a set of gold standard named entities shows that CRFs outperform SVMs. The best F-score with CRFs is 0.86 and for the SVMs is 0.64 as</abstract>
<note confidence="0.641493">compared to a baseline of 0.60.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Support-vector network.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="1433" citStr="Cortes and Vapnik, 1995" startWordPosition="205" endWordPosition="208">ction and background Named entity recognition (NER) is the discovery of named entities (NEs), or textual mentions that belong to the same semantic class. In the biomedical domain NEs are diseases, signs/symptoms, anatomical signs, and drugs. NER performance is high as applied to scholarly text and newswire narratives (Leaman et al., 2008). Clinical free-text, on the other hand, exhibits characteristics of both informal and formal linguistic styles which, in turn, poses challenges for clinical NER. Conditional Random Fields (CRFs) (Lafferty et al., 2001) and and Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are machine learning techniques which can handle multiple features during learning. CRFs’ main strength lies in their ability to include various unrelated features, while SVMs’ in the inclusion of overlapping features. Our goal is to compare CRFs and SVMs performance for clinical NER with focus on disease/disorder NEs. 2 Dataset and features Our dataset is a gold standard corpus of 1557 single- and multi-word disorder annotations (Ogren et al., 2008). For training and testing the CRF and SVM models the IOB (inside-outside-begin) notation (Leaman, 2008) was applied. In our project, we used 126</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. Support-vector network. Machine Learning, 20:273-297, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning (ICML-2001),</booktitle>
<contexts>
<context position="1368" citStr="Lafferty et al., 2001" startWordPosition="195" endWordPosition="198">r the SVMs is 0.64 as compared to a baseline of 0.60. 1 Introduction and background Named entity recognition (NER) is the discovery of named entities (NEs), or textual mentions that belong to the same semantic class. In the biomedical domain NEs are diseases, signs/symptoms, anatomical signs, and drugs. NER performance is high as applied to scholarly text and newswire narratives (Leaman et al., 2008). Clinical free-text, on the other hand, exhibits characteristics of both informal and formal linguistic styles which, in turn, poses challenges for clinical NER. Conditional Random Fields (CRFs) (Lafferty et al., 2001) and and Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are machine learning techniques which can handle multiple features during learning. CRFs’ main strength lies in their ability to include various unrelated features, while SVMs’ in the inclusion of overlapping features. Our goal is to compare CRFs and SVMs performance for clinical NER with focus on disease/disorder NEs. 2 Dataset and features Our dataset is a gold standard corpus of 1557 single- and multi-word disorder annotations (Ogren et al., 2008). For training and testing the CRF and SVM models the IOB (inside-outside-begin)</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum and Fernando Pereira. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML-2001), 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Leaman</author>
<author>Graciela Gonzalez</author>
</authors>
<date>2008</date>
<booktitle>BANNER: an Executable Survey of Advances in Biomedical Named Entity Recognition. Pacific Symposium on Biocomputing</booktitle>
<pages>13--652</pages>
<marker>Leaman, Gonzalez, 2008</marker>
<rawString>Robert Leaman and Graciela Gonzalez. BANNER: an Executable Survey of Advances in Biomedical Named Entity Recognition. Pacific Symposium on Biocomputing 13:652-663. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Ogren</author>
<author>Guergana Savova</author>
<author>Christopher G Chute</author>
</authors>
<title>Constructing evaluation corpora for automated clinical named entity recognition.</title>
<date>2008</date>
<booktitle>Proc LREC</booktitle>
<contexts>
<context position="1888" citStr="Ogren et al., 2008" startWordPosition="278" endWordPosition="281">n turn, poses challenges for clinical NER. Conditional Random Fields (CRFs) (Lafferty et al., 2001) and and Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are machine learning techniques which can handle multiple features during learning. CRFs’ main strength lies in their ability to include various unrelated features, while SVMs’ in the inclusion of overlapping features. Our goal is to compare CRFs and SVMs performance for clinical NER with focus on disease/disorder NEs. 2 Dataset and features Our dataset is a gold standard corpus of 1557 single- and multi-word disorder annotations (Ogren et al., 2008). For training and testing the CRF and SVM models the IOB (inside-outside-begin) notation (Leaman, 2008) was applied. In our project, we used 1265 gold standard annotations for training and 292 for testing. The features used for the 94 learning process are described as follows. Dictionary look-up is a binary value feature that represents if the NE is in the dictionary (SNOMED-CT). Bag of Words (BOW) is a representation of the context by the unique words in it. Part-of-speech tags (POS) of BOW is the pos tags of the context words. Window size is the number of tokens representing context surroun</context>
</contexts>
<marker>Ogren, Savova, Chute, 2008</marker>
<rawString>Philip Ogren, Guergana Savova and Christopher G Chute. Constructing evaluation corpora for automated clinical named entity recognition. Proc LREC 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>