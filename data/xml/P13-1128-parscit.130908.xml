<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996666">
Entity Linking for Tweets
</title>
<author confidence="0.999514">
Xiaohua Liu†, Yitong Li$, Haocheng Wu♯, Ming Zhou†, Furu Wei†, Yi Lu§
</author>
<affiliation confidence="0.933804428571429">
†Microsoft Research Asia, Beijing, 100190, China
$School of Electronic and Information Engineering
Beihang University, Beijing, 100191, China
♯University of Science and Technology of China
No. 96, Jinzhai Road, Hefei, Anhui, China
§School of Computer Science and Technology
Harbin Institute of Technology, Harbin, 150001, China
</affiliation>
<email confidence="0.954862">
†{xiaoliu, mingzhou, fuwei}@microsoft.com
$tong91222@126.com ♯v-haowu@microsoft.com §v-y@microsoft.com
</email>
<sectionHeader confidence="0.997187" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978578947368">
We study the task of entity linking for
tweets, which tries to associate each
mention in a tweet with a knowledge base
entry. Two main challenges of this task are
the dearth of information in a single tweet
and the rich entity mention variations.
To address these challenges, we propose
a collective inference method that
simultaneously resolves a set of mentions.
Particularly, our model integrates three
kinds of similarities, i.e., mention-entry
similarity, entry-entry similarity, and
mention-mention similarity, to enrich
the context for entity linking, and to
address irregular mentions that are not
covered by the entity-variation dictionary.
We evaluate our method on a publicly
available data set and demonstrate the
effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972214285714">
Twitter is a widely used social networking service.
With millions of active users and hundreds of
millions of new published tweets every day1,
it has become a popular platform to capture
and transmit the human experiences of the
moment. Many tweet related researches are
inspired, from named entity recognition (Liu et al.,
2012), topic detection (Mathioudakis and Koudas,
2010), clustering (Rosa et al., 2010), to event
extraction (Grinev et al., 2009).
In this work, we study the entity linking task
for tweets, which maps each entity mention in
a tweet to a unique entity, i.e., an entry ID
of a knowledge base like Wikipedia. Entity
</bodyText>
<footnote confidence="0.930643">
1http://siteanalytics.compete.com/twitter.com/
</footnote>
<bodyText confidence="0.999208966666667">
linking task is generally considered as a bridge
between unstructured text and structured machine-
readable knowledge base, and represents a critical
role in machine reading program (Singh et al.,
2011). Entity linking for tweets is particularly
meaningful, considering that tweets are often hard
to read owing to its informal written style and
length limitation of 140 characters.
Current entity linking methods are built on top
of a large scale knowledge base such as Wikipedia.
A knowledge base consists of a set of entities,
and each entity can have a variation list2. To
decide which entity should be mapped, they may
compute: 1) the similarity between the context of
a mention, e.g., a text window around the mention,
and the content of an entity, e.g., the entity page of
Wikipedia (Mihalcea and Csomai, 2007; Han and
Zhao, 2009); 2) the coherence among the mapped
entities for a set of related mentions, e.g, multiple
mentions in a document (Milne and Witten, 2008;
Kulkarni et al., 2009; Han and Zhao, 2010; Han et
al., 2011).
Tweets pose special challenges to entity linking.
First, a tweet is often too concise and too
noisy to provide enough information for similarity
computing, owing to its short and grass root
nature. Second, tweets have rich variations of
named entities3, and many of them fall out of
the scope of the existing dictionaries mined from
Wikipedia (called OOV mentions hereafter). On
</bodyText>
<footnote confidence="0.6257684">
2Entity variation lists can be extracted from the
entity resolution pages of Wikipedia. For example, the
link “http://en.wikipedia.org/wiki/Svm” will lead us to a
resolution page, where “Svm” are linked to entities like
“Space vector modulation” and “Support vector machine”.
As a result, “Svm” will be added into the variation lists of
“Space vector modulation” and “Support vector machine” ,
respectively.
3According to Liu et al. (2012), on average a named entity
has 3.3 different surface forms in tweets.
</footnote>
<page confidence="0.92331">
1304
</page>
<note confidence="0.915572">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1304–1311,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999675219512195">
the other hand, the huge redundancy in tweets
offers opportunities. That means, an entity
mention often occurs in many tweets, which
allows us to aggregate all related tweets to
compute mention-mention similarity and mention-
entity similarity.
We propose a collective inference method
that leverages tweet redundancy to address those
two challenges. Given a set of mentions, our
model tries to ensure that similar mentions are
linked to similar entities while pursuing the
high total similarity between matched mention-
entity pairs. More specifically, we define
local features, including context similarity and
edit distance, to model the similarity between
a mention and an entity. We adopt in-link
based similarity (Milne and Witten, 2008), to
measure the similarity between entities. Finally,
we introduce a set of features to compute
the similarity between mentions, including how
similar the tweets containing the mentions are,
whether they come from the tweets of the same
account, and their edit distance. Notably, our
model can resolve OOV mentions with the help
of their similar mentions. For example, for the
OOV mention “LukeBryanOnline”, our model can
find similar mentions like “TheLukeBryan” and
“LukeBryan”. Considering that most of its similar
mentions are mapped to the American country
singer “Luke Bryan”, our model tends to link
“LukeBryanOnline” to the same entity.
We evaluate our method on the public available
data set shared by Meij et al. (2012)4.
Experimental results show that our method
outperforms two baselines, i.e., Wikify! (Mihalcea
and Csomai, 2007) and system proposed by Meij
et al. (2012). We also study the effectiveness
of features related to each kind of similarity, and
demonstrate the advantage of our method for OOV
mention linkage.
We summarize our contributions as follows.
</bodyText>
<listItem confidence="0.99820025">
1. We introduce a novel collective inference
method that integrates three kinds of
similarities, i.e., mention-entity similarity,
entity-entity similarity, and mention-mention
similarity, to simultaneously map a set of
tweet mentions to their proper entities.
2. We propose modeling the mention-mention
similarity and demonstrate its effectiveness
</listItem>
<footnote confidence="0.683409">
4http://ilps.science.uva.nl/resources/wsdm2012-adding-
semantics-to-microblog-posts/
</footnote>
<bodyText confidence="0.981661285714286">
in entity linking for tweets, particularly for
OOV mentions.
3. We evaluate our method on a public data
set, and show our method compares favorably
with the baselines.
Our paper is organized as follows. In the next
section, we introduce related work. In Section
3, we give the formal definition of the task. In
Section 4, we present our solution, including
the framework, features related to different kinds
of similarities, and the training and decoding
procedures. We evaluate our method in Section 5.
Finally in Section 6, we conclude with suggestions
of future work.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999923366666667">
Existing entity linking work can roughly be
divided into two categories. Methods of the
first category resolve one mention at each time,
and mainly consider the similarity between
a mention-entity pair. In contrast, methods
of the second category take a set of related
mentions (e.g., mentions in the same document)
as input, and figure out their corresponding entities
simultaneously.
Examples of the first category include the first
Web-scale entity linking system SemTag (Dill
et al., 2003), Wikify! (Mihalcea and Csomai,
2007), and the recent work of Milne and Witten
(2008). SemTag uses the TAP knowledge
base5, and employs the cosine similarity with
TF-IDF weighting scheme to compute the
match degree between a mention and an entity,
achieving an accuracy of around 82%. Wikify!
identifies the important concepts in the text
and automatically links these concepts to the
corresponding Wikipedia pages. It introduces two
approaches to define mention-entity similarity,
i.e., the contextual overlap between the paragraph
where the mention occurs and the corresponding
Wikipedia pages, and a Naive Bayes classifier
that predicts whether a mention should be linked
to an entity. It achieves 80.69% F1 when two
approaches are combined. Milne and Witten
work on the same task of Wikify!, and also
train a classifier. However, they cleverly use the
</bodyText>
<footnote confidence="0.77360025">
5TAB (http://www.w3.org/2002/05/tap/) is a shallow
knowledge base that contains a broad range of lexical and
taxonomic information about popular objects like music,
movies, authors, sports, autos, health, etc.
</footnote>
<page confidence="0.977809">
1305
</page>
<bodyText confidence="0.99990906122449">
links found within Wikipedia articles for training,
exploiting the fact that for every link, a Wikipedian
has manually selected the correct destination to
represent the intended sense of the anchor. Their
method achieves an F1 score of 75.0%.
Representative studies of the second category
include the work of Kulkarni et al. (2009),
Han et al. (2011), and Shen et al. (2012).
One common feature of these studies is that
they leverage the global coherence between
entities. Kulkarni et al. (2009) propose
a graphical model that explicitly models the
combination of evidence from local mention-
entity compatibility and global document-level
topical coherence of the entities, and show that
considering global coherence between entities
significantly improves the performance. Han et
al. (2011) introduce a graph-based representation,
called Referent Graph, to model the global
interdependence between different entity linking
decisions, and jointly infer the referent entities of
all name mentions in a document by exploiting
the interdependence captured in Referent Graph.
Shen et al. (2012) propose LIEGE, a framework
to link the entities in web lists with the knowledge
base, with the assumption that entities mentioned
in a Web list tend to be a collection of entities of
the same conceptual type.
Most work of entity linking focuses on web
pages. Recently, Meij et al. (2012) study
this task for tweets. They propose a machine
learning based approach using n-gram features,
concept features, and tweet features, to identify
concepts semantically related to a tweet, and
for every entity mention to generate links to its
corresponding Wikipedia article. Their method
belongs to the first category, in the sense that
they only consider the similarity between mention
(tweet) and entity (Wikipedia article).
Our method belongs to the second category.
However, in contrast with existing collective
approaches, our method works on tweets which
are short and often noisy. Furthermore, our
method is based on the “similar mention with
similar entity” assumption, and explicitly models
and integrates the mention similarity into the
optimization framework. Compared with Meij et
al. (2012), our method is collective, and integrates
more features.
</bodyText>
<sectionHeader confidence="0.990123" genericHeader="method">
3 Task Definition
</sectionHeader>
<bodyText confidence="0.999892870967742">
Given a sequence of mentions, denoted by
M⃗ = (m1, m2, · · · , mn), our task is to
output a sequence of entities, denoted by
E⃗ = (e1, e2, · · · , en), where ei is the entity
corresponding to mi. Here, an entity refers
to an item of a knowledge base. Following
most existing work, we use Wikipedia as the
knowledge base, and an entity is a definition page
in Wikipedia; a mention denotes a sequence of
tokens in a tweet that can be potentially linked to
an entity.
Several notes should be made. First, we
assume that mentions are given, e.g., identified by
some named entity recognition system. Second,
mentions may come from multiple tweets. Third,
mentions with the same token sequence may
refer to different entities, depending on mention
context. Finally, we assume each entity e has
a variation list6, and a unique ID through which
all related information about that entity can be
accessed.
Here is an example to illustrate the task. Given
mentions “nbcbightlynews”, “Santiago”, “WH”
and “Libya” from the following tweet “Chuck
Todd: Prepping for @nbcnightlynews here in
Santiago, reporting on WH handling of Libya
situation.”, the expected output is “NBC Nightly
News(194735)”, “Santiago Chile(51572)”,
“White House(33057)” and “Libya(17633)”,
where the numbers in the parentheses are the IDs
of the corresponding entities.
</bodyText>
<sectionHeader confidence="0.993534" genericHeader="method">
4 Our Method
</sectionHeader>
<bodyText confidence="0.9999446">
In this section, we first present the framework of
our entity linking method. Then we introduce
features related to different kinds of similarities,
followed by a detailed discussion of the training
and decoding procedures.
</bodyText>
<subsectionHeader confidence="0.975547">
4.1 Framework
</subsectionHeader>
<bodyText confidence="0.962042">
Given the input mention sequence M⃗ =
</bodyText>
<construct confidence="0.545657666666667">
(m1, m2, · · · , mn), our method outputs the entity
sequence ⃗E* = (e*1, e*2, · · · , e*n) according to
Formula 1:
</construct>
<footnote confidence="0.998231">
6For example, the variation list of the entity “Obama” may
contain “Barack Obama”, “Barack Hussein Obama II”, etc.
</footnote>
<page confidence="0.839243">
1306
</page>
<equation confidence="0.949616">
n
⃗E* = argmaxV⃗EEC( ⃗M)A w⃗· ⃗f(ei, mi)
i=1 (1)
+(1 − A) � r(ei, ej)s(mi, mj)
i=Aj
Where:
</equation>
<listItem confidence="0.96487255">
• C( ⃗M) is the set of all possible entity
sequences for the mention sequence ⃗M;
• E⃗ denotes an entity sequence instance,
consisting of e1, e2, · · · , en;
• ⃗f(ei, mi) is the feature vector that models the
similarity between mention mi and its linked
entity ei;
• w⃗ is the feature weight vector related to ⃗f,
which is trained on the training data set; w⃗ ·
⃗f(ei, mi) is the similarity between mention
mi and entity ei;
• r(ei, ej) is the function that returns the
similarity between two entities ei and ej;
• s(mi, mj) is the function that returns the
similarity between two mentions mi and mj;
• A ∈ (0, 1) is a systematic parameter, which
is determined on the development data set; it
is used to adjust the tradeoff between local
compatibility and global consistence. It is
experimentally set to 0.8 in our work.
</listItem>
<bodyText confidence="0.99387696969697">
From Formula 1, we can see that: 1) our
method considers the mention-entity similarly,
entity-entity similarity and mention-mention
similarity. Mention-entity similarly is used to
model local compatibility, while entity-entity
similarity and mention-mention similarity
combined are to model global consistence; and 2)
our method prefers configurations where similar
mentions have similar entities and with high local
compatibility.
C( ⃗M) is worth of more discussion here.
It represents the search space, which can be
generated using the entity variation list. To
achieve this, we first build an inverted index
of all entity variation lists, with each unique
variation as an entry pointing to a list of entities.
Then for any mention m, we look up the index,
and get all possible entities, denoted by C(m).
In this way, given a mention sequence M⃗ =
(m1, m2, · · · , mn), we can enumerate all possible
entity sequence E⃗ = (e1, e2, · · · , en), where ei ∈
C(m). This means |C( ⃗M) |= HmEM |C(m) |,
which is often large. There is one special case:
if m is an OOV mention, i.e., |C(m) |= 0, then
|C( ⃗M) |= 0, and we get no solution. To address
this problem, we can generate a list of candidates
for an OOV mention using its similar mentions.
Let S(m) denote OOV mention m’s similar
mentions, we define C(m) = Um′ES(m) C(m′).
If still C(m) = 0, we remove m from ⃗M, and
report we cannot map it to any entity.
Here is an example to illustrate our framework.
Suppose we have the following tweets:
</bodyText>
<listItem confidence="0.945555">
• UserA: Yeaaahhgg #habemusfut..
I love monday night futbol =)
#EnglishPremierLeague ManU vs
Liverpool1
• UserA: Manchester United 3 - Liverpool2
2 #EnglishPremierLeague GLORY, GLORY,
MAN.UNITED!
• · · ·
</listItem>
<figureCaption confidence="0.79516">
Figure 1: An illustrative example to show our
framework. Ovals in orange and in blue represent
</figureCaption>
<bodyText confidence="0.984396214285714">
mentions and entities, respectively. Each mention
pair, entity pair, and mention entity pair have
a similarity score represented by s, r and f,
respectively.
We need find out the best entity sequence
⃗E* for mentions M⃗ = { “Liverpool1”,
“Manchester United”, “ManU”, “Liverpool2”},
from the entity sequences C( ⃗M) = { (Liverpool
(film), Manchester United F.C., Manchester
United F.C., Liverpool (film)), · · · , (Liverpool,
F.C.,Manchester United, F.C., Manchester United
F.C., Liverpool (film) }. Figure 1 illustrate
our solution, where “Liverpool1” (on the left)
and “Liverpool2” (on the right) are linked
</bodyText>
<page confidence="0.974097">
1307
</page>
<bodyText confidence="0.999873666666667">
to “Liverpool F.C.” (the football club), and
“Manchester United” and “ManU” are linked to
“Manchester United F.C.”. Notably, “ManU”
is an OOV mention, but has a similar mention
“Manchester United”, with which “ManU” is
successfully mapped.
</bodyText>
<subsectionHeader confidence="0.884393">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.9999162">
We group features into three categories: local
features related to mention-entity similarity
(⃗f(e, m)), features related to entity-entity
similarity (r(ei, ej)) , and features related to
mention-mention similarity (s(mi, mj)).
</bodyText>
<subsubsectionHeader confidence="0.930008">
4.2.1 Local Features
</subsubsectionHeader>
<listItem confidence="0.992613">
• Prior Probability:
</listItem>
<equation confidence="0.998363333333333">
count(ei)
f1(mi,ei) = (2)
E∀ek∈C(mi) count(ek)
</equation>
<bodyText confidence="0.9930165">
where count(e) denotes the frequency of
entity e in Wikipedia’s anchor texts.
</bodyText>
<listItem confidence="0.927765">
• Context Similarity:
</listItem>
<equation confidence="0.801683">
coocurence number
f2(mi, ei) = (3)
tweet length
</equation>
<bodyText confidence="0.997931">
where: coccurence number is the the
number of the words that occur in both the
tweet containing mi and the Wikipedia page
of ei; tweet length denotes the number of
tokens of the tweet containing mention mi.
</bodyText>
<listItem confidence="0.991946">
• Edit Distance Similarity:
</listItem>
<bodyText confidence="0.964351666666667">
If Length(mi)+ED(mi, ei) = Length(ei),
f3(mi,ei) = 1, otherwise 0. ED(·, ·)
computes the character level edit distance.
This feature helps to detect whether
a mention is an abbreviation of its
corresponding entity7.
</bodyText>
<listItem confidence="0.991746285714286">
• Mention Contains Title: If the mention
contains the entity title, namely the title of
the Wikipedia page introducing the entity ei,
f4(mi, ei) = 1, else 0.
• Title Contains Mention: If the entry title
contains the mention, f5(mi, ei) = 1,
otherwise 0.
</listItem>
<footnote confidence="0.87103">
7Take “ms” and “Microsoft” for example. The length of
“ms” is 2, and the edit distance between them is 7. 2 plus 7
equals to 9, which is the length of “Microsoft”.
</footnote>
<subsubsectionHeader confidence="0.839089">
4.2.2 Features Related to Entity Similarity
</subsubsectionHeader>
<bodyText confidence="0.99997">
There are two representative definitions of entity
similarity: in-link based similarity (Milne and
Witten, 2008) and category based similarity (Shen
et al., 2012). Considering that the Wikipedia
categories are often noisy (Milne and Witten,
2008), we adopt in-link based similarity, as
defined in Formula 4:
</bodyText>
<equation confidence="0.9479456">
log|g(ei) n g(ej) |− log max(|g(ei)|, |g(ej)|)
r(ei, ej) =
log(Total) − log min(|g(ei)|, |g(ej)|)
(4)
Where:
</equation>
<listItem confidence="0.994123">
• Total is the total number of knowledge base
entities;
• g(e) is the number of Wikipedia definition
pages that have a link to entity e.
</listItem>
<subsubsectionHeader confidence="0.948442">
4.2.3 Features Related to Mention Similarity
</subsubsectionHeader>
<bodyText confidence="0.99606075">
We define 5 features to model the similarity
between two mentions mi and mj, as listed
below, where t(m) denotes the tweet that contains
mention m:
</bodyText>
<listItem confidence="0.999675">
• s1(mi,mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as TF-
IDF vectors;
• s2(mi, mj): The cosine similarity of t(mi)
and t(mj); and tweets are represented as
topic distribution vectors;
• s3(mi, mj): Whether t(mi) and t(mj) are
published by the same account;
• s4(mi, mj): Whether t(mi) and t(mj)
contain any common hash tag;
• s5(mi, mj): Edit distance related similarity
between mi and mj, as defined in Formula 5.
</listItem>
<equation confidence="0.9943928">
s5(mi, mj) = 1, if min{Length(mi),Length(mj)}
+ED(mi, mj) = max{Length(mi), Length(mj)},
ED(mi, mj)
else s5(mi, mj) = 1 − max{Length(mi), Length(mj)}
(5)
</equation>
<bodyText confidence="0.9990818">
Note that: 1) before computing TF-IDF vectors,
stop words are removed; 2) we use the Stanford
Topic Modeling Toolbox8 to compute the topic
model, and experimentally set the number of
topics to 50.
</bodyText>
<footnote confidence="0.941808">
8http://nlp.stanford.edu/software/tmt/tmt-0.4/
</footnote>
<page confidence="0.95687">
1308
</page>
<bodyText confidence="0.685951">
Finally, Formula 6 is used to integrate all the
features. a⃗ = (a1, a2, a3, a4, a5) is the feature
weight vector for mention similarity, where ak E
</bodyText>
<equation confidence="0.9350955">
(0, 1), k = 1, 2, 3, 4, 5, and ∑5k=1 ak = 1.
5
s(mi, mj) = ∑ aksk(mi, mj) (6)
k=1
</equation>
<subsectionHeader confidence="0.989647">
4.3 Training and Decoding
</subsectionHeader>
<bodyText confidence="0.9999556">
Given n mentions m1, m2, &apos; &apos; &apos; , mn and their
corresponding entities e1, e2, &apos; &apos; &apos; , en, the goal of
training is to determine: ⃗w∗, the weights of local
features, and ⃗a∗, the weights of the features related
to mention similarity, according to Formula 7 9.
</bodyText>
<equation confidence="0.970313">
(⃗w∗,⃗a∗) = arg min⃗w,⃗a{ 1
n
s(mi, mj)L2(⃗a, ei, ej)1
(7)
• L1 is the loss function related to local
compatibility, which is defined as
⃗w· ⃗f(ei,mi)+1; 1
</equation>
<listItem confidence="0.811389">
• L2(⃗a, ei, ej) is the loss function related
to global coherence, which is defined as
</listItem>
<equation confidence="0.9362045">
1
r(ei,ej) ∑�=1 aksk(mi,mj)+1;
</equation>
<listItem confidence="0.82834025">
• α1 is the weight of regularization, which is
experimentally set to 1.0;
• α2 is the weight of L2 loss, which is
experimentally set to 0.2.
</listItem>
<bodyText confidence="0.8852396">
Since the decoding problem defined by
Formula 1 is NP hard (Kulkarni et al., 2009), we
develop a greedy hill-climbing approach to tackle
this challenge, as demonstrated in Algorithm 1.
In Algorithm 1, it is the number of iterations;
Score(⃗E, ⃗M) = A ∑ni=1 w⃗ &apos; ⃗f(ei, mi) + (1 −
A) ∑i̸=j r(ei, ej)s(mi, mj); ⃗Eij is the vector after
replacing ei with ej E C(mi) for current ⃗E;
scij is the score of ⃗Eij, i.e., Score(⃗Eij, ⃗M). In
each iteration, this rounding solution iteratively
substitute entry ei in E⃗ to increase the total score
cur. If the score cannot be further improved, it
stops and returns current ⃗E.
9This optimization problem is non-convex. We use
coordinate descent to get a local optimal solution.
</bodyText>
<equation confidence="0.3421342">
Algorithm 1 Decoding Algorithm.
M⃗= (m1, m2, ··· , mn)
Output: Entity Set E⃗ = (e1, e2, · · · , en)
1: for i = 1 to n do
2: Initialize e(0)
</equation>
<bodyText confidence="0.613522">
i as the entity with the largest prior
probability given mention mi.
</bodyText>
<listItem confidence="0.789674">
3: end for
4: cur = Score(
5: it = 1
6: while true do
7: for i = 1 to n do
8: for ej ∈ C(mi) do
if ej ≠ e(it−1)
9: then
i
⃗E(it) i } + {ej}.
11: end if
12: scij = Score(⃗E(it)
ij , ⃗M).
13: end for
14: end for
</listItem>
<equation confidence="0.769424333333333">
15: (l, m) = argmax(i,j)scij.
16: sc∗ = sclm
17: if sc∗ &gt; cur then
18: cur = sc∗.
⃗E(it) = ⃗E(it−1) − {e(it−1)
19: l } + {em}.
</equation>
<reference confidence="0.399973333333333">
20: it=it+1.
21: else
22: break
23: end if
24: end while
25: return ⃗E(it).
</reference>
<sectionHeader confidence="0.995755" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99973">
In this section, we introduce the data set and
experimental settings, and present results.
</bodyText>
<subsectionHeader confidence="0.994622">
5.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.999840818181818">
Following most existing studies, we choose
Wikipedia as our knowledge base10. We index
the Wikipedia definition pages, and prepare all
required prior knowledge, such as count(e), g(e),
and entity variation lists. We also build an inverted
index with about 60 million entries for the entity
variation lists.
For tweets, we use the data set shared by Meij et
al. (2012)11. This data set is annotated manually
by two volunteers. We get 502 annotated tweets
from this data set. We keep 55 of them for
</bodyText>
<footnote confidence="0.902137333333333">
10We download the December 2012 version of Wikipedia,
which contains about four million articles.
11http://ilps.science.uva.nl/resources/wsdm2012-adding-
</footnote>
<equation confidence="0.878792">
semantics-to-microblog-posts/.
∑n L1(ei, mi)
i=1
+α1I1⃗wI12 + α2
</equation>
<page confidence="0.711814">
2
</page>
<figure confidence="0.9593474">
∑n
i,j=1
Where:
Input: Mention Set
⃗E(0), ⃗M)
</figure>
<page confidence="0.991718">
1309
</page>
<bodyText confidence="0.9980235">
development, and the remaining for 5 fold cross-
validation.
</bodyText>
<subsectionHeader confidence="0.99594">
5.2 Settings
</subsectionHeader>
<bodyText confidence="0.9996985">
We consider following settings to evaluate our
method.
</bodyText>
<listItem confidence="0.697083142857143">
• Comparing our method with two baselines,
i.e., Wikify! (Mihalcea and Csomai, 2007)
and the system proposed by Meij et al. (2012)
12;
• Using only local features;
• Using various mention similarity features;
• Experiments on OOV mentions.
</listItem>
<sectionHeader confidence="0.610364" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<bodyText confidence="0.999106588235294">
Table 1 reports the comparison results. Our
method outperforms both systems in terms of
all metrics. Since the main difference between
our method and the baselines is that our method
considers not only local features, but also global
features related to entity similarity and mention
similarity, these results indicate the effectiveness
of collective inference and global features. For
example, we find two baselines incorrectly link
“Nickelodeon” in the tweet “BOH will make a
special appearance on Nickelodeon’s ‘Yo Gabba
Gabba’ tomorrow” to the theater instead of a TV
channel. In contrast, our method notices that “Yo
Gabba Gabba” in the same tweet can be linked
to “Yo Gabba Gabba (TV show)”, and thus it
correctly maps “Nickelodeon” to “Nickelodeon
(TV channel)”.
</bodyText>
<table confidence="0.9977035">
System Pre. Rec. F1
Wikify! 0.375 0.421 0.396
Meij’s Method 0.734 0.632 0.679
Our Method 0.752 0.675 0.711
</table>
<tableCaption confidence="0.999987">
Table 1: Comparison with Baselines.
</tableCaption>
<bodyText confidence="0.984936857142857">
Table 2 shows the results when local features
are incrementally added. It can be seen that:
1) using only Prior Probability feature already
yields a reasonable F1; and 2) Context Similarity
and Edit Distance Similarity feature have little
contribution to the F1, while Mention and Entity
Title Similarity feature greatly boosts the F1.
</bodyText>
<footnote confidence="0.8496865">
12We re-implement Wikify! since we use a new evaluation
data set.
</footnote>
<table confidence="0.9866048">
Local Feature Pre. Rec. F1
P.P. 0.700 0.599 0.646
+C.S. 0.694 0.597 0.642
+E.D.S. 0.696 0.598 0.643
+M.E.T.S. 0.735 0.632 0.680
</table>
<tableCaption confidence="0.928236">
Table 2: Local Feature Analysis. P.P.,C.S., E.D.S.,
</tableCaption>
<bodyText confidence="0.993982">
and M.E.T.S. denote Prior Probability, Context
Similarity, Edit Distance Similarity, and Mention
and Entity Title Similarity, respectively.
The performance of our method with various
mention similarity features is reported in Table 3.
First, we can see that with this kind of features,
the F1 can be significantly improved from 0.680
to 0.704. Second, we notice that TF-IDF (s1) and
Topic Model (s2) features perform equally well,
and combining all mention similarity features
yields the best performance.
</bodyText>
<table confidence="0.9993936">
Global Feature Pre. Rec. F1
s3+s4+s5 0.744 0.653 0.700
s3+s4+s5 +s1 0.759 0.652 0.702
s3+s4+s5+s2 0.760 0.653 0.703
s3+s4+s5+s1+s2 0.764 0.653 0.704
</table>
<tableCaption confidence="0.999715">
Table 3: Mention Similarity Feature Analysis.
</tableCaption>
<bodyText confidence="0.999823066666667">
For any OOV mention, we use the strategy
of guessing its possible entity candidates using
similar mentions, as discussed in Section 4.1.
Table 4 shows the performance of our system for
OOV mentions. It can be seen that with our
OOV strategy, the recall is improved from 0.653
to 0.675 (with p &lt; 0.05) while the Precision is
slightly dropped and the overall F1 still gets better.
A further study reveals that among all the 125
OOV mentions, there are 48 for which our method
cannot find any entity; and nearly half of these
48 OOV mentions do have corresponding entities
13. This suggests that we may need enlarge the
size of variation lists or develop some mention
normalization techniques.
</bodyText>
<table confidence="0.992715">
OOV Method Precision Recall F1
Ignore OOV Mention 0.764 0.653 0.704
+ OOV Method 0.752 0.675 0.711
</table>
<tableCaption confidence="0.995363">
Table 4: Performance for OOV Mentions.
</tableCaption>
<footnote confidence="0.998037">
13“NATO-ukraine cooperations” is such an example. It
is mapped to NULL but actually has a corresponding entity
“Ukraine-NATO relations”
</footnote>
<page confidence="0.99141">
1310
</page>
<sectionHeader confidence="0.998486" genericHeader="conclusions">
6 Conclusions and Future work
</sectionHeader>
<bodyText confidence="0.999991882352941">
We have presented a collective inference method
that jointly links a set of tweet mentions to
their corresponding entities. One distinguished
characteristic of our method is that it integrates
mention-entity similarity, entity-entity similarity,
and mention-mention similarity, to address the
information lack in a tweet and rich OOV
mentions. We evaluate our method on a
public data set. Experimental results show our
method outperforms two baselines, and suggests
the effectiveness of modeling mention-mention
similarity, particularly for OOV mention linking.
In the future, we plan to explore two directions.
First, we are going to enlarge the size of entity
variation lists. Second, we want to integrate
the entity mention normalization techniques as
introduced by Liu et al. (2012).
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999943">
We thank the anonymous reviewers for their
valuable comments. We also thank all the
QuickView team members for the helpful
discussions.
</bodyText>
<sectionHeader confidence="0.999436" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843594202898">
S. Dill, N. Eiron, D. Gibson, D. Gruhl, and R. Guha.
2003. Semtag and seeker: bootstrapping the
semantic web via automated semantic annotation. In
Proceedings of the 12th international conference on
World Wide Web, WWW ’03, pages 178–186, New
York, NY, USA. ACM.
Maxim Grinev, Maria Grineva, Alexander Boldakov,
Leonid Novak, Andrey Syssoev, and Dmitry
Lizorkin. 2009. Sifting micro-blogging stream for
events of user interest. In Proceedings of the 32nd
international ACM SIGIR conference on Research
and development in information retrieval, SIGIR
’09, pages 837–837, New York, NY, USA. ACM.
Xianpei Han and Jun Zhao. 2009. Nlpr-kbp in tac 2009
kbp track: A two-stage method to entity linking. In
Proceedings of Test Analysis Conference.
Xianpei Han and Jun Zhao. 2010. Structural
semantic relatedness: a knowledge-based method
to named entity disambiguation. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics.
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective
entity linking in web text: A graph-based method.
In SIGIR’11.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective
annotation of wikipedia entities in web text.
In Proceedings of the 15th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 457–465.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint inference
of named entity recognition and normalization for
tweets. In ACL (1), pages 526–535.
Michael Mathioudakis and Nick Koudas. 2010.
Twittermonitor: trend detection over the twitter
stream. In Proceedings of the 2010 ACM SIGMOD
International Conference on Management of data,
SIGMOD ’10, pages 1155–1158, New York, NY,
USA. ACM.
Edgar Meij, Wouter Weerkamp, and Maarten de Rijke.
2012. Adding semantics to microblog posts.
In Proceedings of the fifth ACM international
conference on Web search and data mining.
Rada Mihalcea and Andras Csomai. 2007. Wikify!:
linking documents to encyclopedic knowledge.
In Proceedings of the sixteenth ACM conference
on Conference on information and knowledge
management, CIKM ’07, pages 233–242, New York,
NY, USA. ACM.
David Milne and Ian H. Witten. 2008. Learning
to link with wikipedia. In Proceeding of the 17th
ACM conference on Information and knowledge
management.
Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole
Gershman, and Robert Frederking. 2010. Topical
clustering of tweets. In SWSM’10.
Wei Shen, Jianyong Wang, Ping Luo, and Min Wang.
2012. Liege: Link entities in web lists with
knowledge base. In KDD’12.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-
scale cross-document coreference using distributed
inference and hierarchical models. In Proceedings
of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ’11, pages 793–
803, Stroudsburg, PA, USA. Association for
Computational Linguistics.
</reference>
<page confidence="0.99288">
1311
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.155138">
<title confidence="0.999735">Entity Linking for Tweets</title>
<author confidence="0.993474">Yitong Haocheng Ming Furu Yi</author>
<address confidence="0.524533">Research Asia, Beijing, 100190,</address>
<affiliation confidence="0.630480333333333">of Electronic and Information Beihang University, Beijing, 100191, of Science and Technology of</affiliation>
<address confidence="0.80361">No. 96, Jinzhai Road, Hefei, Anhui,</address>
<affiliation confidence="0.8684955">of Computer Science and Harbin Institute of Technology, Harbin, 150001,</affiliation>
<email confidence="0.944922">mingzhou,</email>
<abstract confidence="0.99863835">We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>itit1</author>
</authors>
<title>21: else 22: break 23: end if 24: end while 25: return ⃗E(it).</title>
<marker>itit1, </marker>
<rawString>20: it=it+1. 21: else 22: break 23: end if 24: end while 25: return ⃗E(it).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dill</author>
<author>N Eiron</author>
<author>D Gibson</author>
<author>D Gruhl</author>
<author>R Guha</author>
</authors>
<title>Semtag and seeker: bootstrapping the semantic web via automated semantic annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web, WWW ’03,</booktitle>
<pages>178--186</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7435" citStr="Dill et al., 2003" startWordPosition="1120" endWordPosition="1123">ures. We evaluate our method in Section 5. Finally in Section 6, we conclude with suggestions of future work. 2 Related Work Existing entity linking work can roughly be divided into two categories. Methods of the first category resolve one mention at each time, and mainly consider the similarity between a mention-entity pair. In contrast, methods of the second category take a set of related mentions (e.g., mentions in the same document) as input, and figure out their corresponding entities simultaneously. Examples of the first category include the first Web-scale entity linking system SemTag (Dill et al., 2003), Wikify! (Mihalcea and Csomai, 2007), and the recent work of Milne and Witten (2008). SemTag uses the TAP knowledge base5, and employs the cosine similarity with TF-IDF weighting scheme to compute the match degree between a mention and an entity, achieving an accuracy of around 82%. Wikify! identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. It introduces two approaches to define mention-entity similarity, i.e., the contextual overlap between the paragraph where the mention occurs and the corresponding Wikipedia pages, and</context>
</contexts>
<marker>Dill, Eiron, Gibson, Gruhl, Guha, 2003</marker>
<rawString>S. Dill, N. Eiron, D. Gibson, D. Gruhl, and R. Guha. 2003. Semtag and seeker: bootstrapping the semantic web via automated semantic annotation. In Proceedings of the 12th international conference on World Wide Web, WWW ’03, pages 178–186, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Grinev</author>
<author>Maria Grineva</author>
<author>Alexander Boldakov</author>
<author>Leonid Novak</author>
<author>Andrey Syssoev</author>
<author>Dmitry Lizorkin</author>
</authors>
<title>Sifting micro-blogging stream for events of user interest.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09,</booktitle>
<pages>837--837</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1759" citStr="Grinev et al., 2009" startWordPosition="250" endWordPosition="253">ot covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 1 Introduction Twitter is a widely used social networking service. With millions of active users and hundreds of millions of new published tweets every day1, it has become a popular platform to capture and transmit the human experiences of the moment. Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 c</context>
</contexts>
<marker>Grinev, Grineva, Boldakov, Novak, Syssoev, Lizorkin, 2009</marker>
<rawString>Maxim Grinev, Maria Grineva, Alexander Boldakov, Leonid Novak, Andrey Syssoev, and Dmitry Lizorkin. 2009. Sifting micro-blogging stream for events of user interest. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, SIGIR ’09, pages 837–837, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Jun Zhao</author>
</authors>
<title>Nlpr-kbp in tac 2009 kbp track: A two-stage method to entity linking.</title>
<date>2009</date>
<booktitle>In Proceedings of Test Analysis Conference.</booktitle>
<contexts>
<context position="2824" citStr="Han and Zhao, 2009" startWordPosition="422" endWordPosition="425">g for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2. To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2Entity variation lis</context>
</contexts>
<marker>Han, Zhao, 2009</marker>
<rawString>Xianpei Han and Jun Zhao. 2009. Nlpr-kbp in tac 2009 kbp track: A two-stage method to entity linking. In Proceedings of Test Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Jun Zhao</author>
</authors>
<title>Structural semantic relatedness: a knowledge-based method to named entity disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3003" citStr="Han and Zhao, 2010" startWordPosition="453" endWordPosition="456">linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2. To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2Entity variation lists can be extracted from the entity resolution pages of Wikipedia. For example, the link “http://en.wikipedia.org/wiki/Svm” will lead us to a resolution page, where “Svm” are link</context>
</contexts>
<marker>Han, Zhao, 2010</marker>
<rawString>Xianpei Han and Jun Zhao. 2010. Structural semantic relatedness: a knowledge-based method to named entity disambiguation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xianpei Han</author>
<author>Le Sun</author>
<author>Jun Zhao</author>
</authors>
<title>Collective entity linking in web text: A graph-based method.</title>
<date>2011</date>
<booktitle>In SIGIR’11.</booktitle>
<marker>Han, Le Sun, Zhao, 2011</marker>
<rawString>Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: A graph-based method. In SIGIR’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayali Kulkarni</author>
<author>Amit Singh</author>
<author>Ganesh Ramakrishnan</author>
<author>Soumen Chakrabarti</author>
</authors>
<title>Collective annotation of wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>457--465</pages>
<contexts>
<context position="2983" citStr="Kulkarni et al., 2009" startWordPosition="449" endWordPosition="452">acters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2. To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2Entity variation lists can be extracted from the entity resolution pages of Wikipedia. For example, the link “http://en.wikipedia.org/wiki/Svm” will lead us to a resolution page, </context>
<context position="8837" citStr="Kulkarni et al. (2009)" startWordPosition="1334" endWordPosition="1337">ask of Wikify!, and also train a classifier. However, they cleverly use the 5TAB (http://www.w3.org/2002/05/tap/) is a shallow knowledge base that contains a broad range of lexical and taxonomic information about popular objects like music, movies, authors, sports, autos, health, etc. 1305 links found within Wikipedia articles for training, exploiting the fact that for every link, a Wikipedian has manually selected the correct destination to represent the intended sense of the anchor. Their method achieves an F1 score of 75.0%. Representative studies of the second category include the work of Kulkarni et al. (2009), Han et al. (2011), and Shen et al. (2012). One common feature of these studies is that they leverage the global coherence between entities. Kulkarni et al. (2009) propose a graphical model that explicitly models the combination of evidence from local mentionentity compatibility and global document-level topical coherence of the entities, and show that considering global coherence between entities significantly improves the performance. Han et al. (2011) introduce a graph-based representation, called Referent Graph, to model the global interdependence between different entity linking decision</context>
<context position="20173" citStr="Kulkarni et al., 2009" startWordPosition="3219" endWordPosition="3222">the weights of local features, and ⃗a∗, the weights of the features related to mention similarity, according to Formula 7 9. (⃗w∗,⃗a∗) = arg min⃗w,⃗a{ 1 n s(mi, mj)L2(⃗a, ei, ej)1 (7) • L1 is the loss function related to local compatibility, which is defined as ⃗w· ⃗f(ei,mi)+1; 1 • L2(⃗a, ei, ej) is the loss function related to global coherence, which is defined as 1 r(ei,ej) ∑�=1 aksk(mi,mj)+1; • α1 is the weight of regularization, which is experimentally set to 1.0; • α2 is the weight of L2 loss, which is experimentally set to 0.2. Since the decoding problem defined by Formula 1 is NP hard (Kulkarni et al., 2009), we develop a greedy hill-climbing approach to tackle this challenge, as demonstrated in Algorithm 1. In Algorithm 1, it is the number of iterations; Score(⃗E, ⃗M) = A ∑ni=1 w⃗ &apos; ⃗f(ei, mi) + (1 − A) ∑i̸=j r(ei, ej)s(mi, mj); ⃗Eij is the vector after replacing ei with ej E C(mi) for current ⃗E; scij is the score of ⃗Eij, i.e., Score(⃗Eij, ⃗M). In each iteration, this rounding solution iteratively substitute entry ei in E⃗ to increase the total score cur. If the score cannot be further improved, it stops and returns current ⃗E. 9This optimization problem is non-convex. We use coordinate descen</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Xiangyang Zhou</author>
<author>Zhongyang Fu</author>
<author>Furu Wei</author>
</authors>
<title>Joint inference of named entity recognition and normalization for tweets.</title>
<date>2012</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>526--535</pages>
<contexts>
<context position="1635" citStr="Liu et al., 2012" startWordPosition="232" endWordPosition="235">y, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 1 Introduction Twitter is a widely used social networking service. With millions of active users and hundreds of millions of new published tweets every day1, it has become a popular platform to capture and transmit the human experiences of the moment. Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly m</context>
<context position="3842" citStr="Liu et al. (2012)" startWordPosition="584" endWordPosition="587">ond, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2Entity variation lists can be extracted from the entity resolution pages of Wikipedia. For example, the link “http://en.wikipedia.org/wiki/Svm” will lead us to a resolution page, where “Svm” are linked to entities like “Space vector modulation” and “Support vector machine”. As a result, “Svm” will be added into the variation lists of “Space vector modulation” and “Support vector machine” , respectively. 3According to Liu et al. (2012), on average a named entity has 3.3 different surface forms in tweets. 1304 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1304–1311, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics the other hand, the huge redundancy in tweets offers opportunities. That means, an entity mention often occurs in many tweets, which allows us to aggregate all related tweets to compute mention-mention similarity and mentionentity similarity. We propose a collective inference method that leverages tweet redundancy to address those tw</context>
</contexts>
<marker>Liu, Zhou, Zhou, Fu, Wei, 2012</marker>
<rawString>Xiaohua Liu, Ming Zhou, Xiangyang Zhou, Zhongyang Fu, and Furu Wei. 2012. Joint inference of named entity recognition and normalization for tweets. In ACL (1), pages 526–535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mathioudakis</author>
<author>Nick Koudas</author>
</authors>
<title>Twittermonitor: trend detection over the twitter stream.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, SIGMOD ’10,</booktitle>
<pages>1155--1158</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1684" citStr="Mathioudakis and Koudas, 2010" startWordPosition="238" endWordPosition="241">o enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 1 Introduction Twitter is a widely used social networking service. With millions of active users and hundreds of millions of new published tweets every day1, it has become a popular platform to capture and transmit the human experiences of the moment. Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard</context>
</contexts>
<marker>Mathioudakis, Koudas, 2010</marker>
<rawString>Michael Mathioudakis and Nick Koudas. 2010. Twittermonitor: trend detection over the twitter stream. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, SIGMOD ’10, pages 1155–1158, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Meij</author>
<author>Wouter Weerkamp</author>
<author>Maarten de Rijke</author>
</authors>
<title>Adding semantics to microblog posts.</title>
<date>2012</date>
<booktitle>In Proceedings of the fifth ACM international conference on Web search and data mining.</booktitle>
<marker>Meij, Weerkamp, de Rijke, 2012</marker>
<rawString>Edgar Meij, Wouter Weerkamp, and Maarten de Rijke. 2012. Adding semantics to microblog posts. In Proceedings of the fifth ACM international conference on Web search and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07,</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2803" citStr="Mihalcea and Csomai, 2007" startWordPosition="418" endWordPosition="421">t al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2. To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On </context>
<context position="5689" citStr="Mihalcea and Csomai, 2007" startWordPosition="862" endWordPosition="865">e tweets of the same account, and their edit distance. Notably, our model can resolve OOV mentions with the help of their similar mentions. For example, for the OOV mention “LukeBryanOnline”, our model can find similar mentions like “TheLukeBryan” and “LukeBryan”. Considering that most of its similar mentions are mapped to the American country singer “Luke Bryan”, our model tends to link “LukeBryanOnline” to the same entity. We evaluate our method on the public available data set shared by Meij et al. (2012)4. Experimental results show that our method outperforms two baselines, i.e., Wikify! (Mihalcea and Csomai, 2007) and system proposed by Meij et al. (2012). We also study the effectiveness of features related to each kind of similarity, and demonstrate the advantage of our method for OOV mention linkage. We summarize our contributions as follows. 1. We introduce a novel collective inference method that integrates three kinds of similarities, i.e., mention-entity similarity, entity-entity similarity, and mention-mention similarity, to simultaneously map a set of tweet mentions to their proper entities. 2. We propose modeling the mention-mention similarity and demonstrate its effectiveness 4http://ilps.sci</context>
<context position="7472" citStr="Mihalcea and Csomai, 2007" startWordPosition="1125" endWordPosition="1128">in Section 5. Finally in Section 6, we conclude with suggestions of future work. 2 Related Work Existing entity linking work can roughly be divided into two categories. Methods of the first category resolve one mention at each time, and mainly consider the similarity between a mention-entity pair. In contrast, methods of the second category take a set of related mentions (e.g., mentions in the same document) as input, and figure out their corresponding entities simultaneously. Examples of the first category include the first Web-scale entity linking system SemTag (Dill et al., 2003), Wikify! (Mihalcea and Csomai, 2007), and the recent work of Milne and Witten (2008). SemTag uses the TAP knowledge base5, and employs the cosine similarity with TF-IDF weighting scheme to compute the match degree between a mention and an entity, achieving an accuracy of around 82%. Wikify! identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. It introduces two approaches to define mention-entity similarity, i.e., the contextual overlap between the paragraph where the mention occurs and the corresponding Wikipedia pages, and a Naive Bayes classifier that predic</context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, CIKM ’07, pages 233–242, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milne</author>
<author>Ian H Witten</author>
</authors>
<title>Learning to link with wikipedia.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM conference on Information and knowledge management.</booktitle>
<contexts>
<context position="2960" citStr="Milne and Witten, 2008" startWordPosition="445" endWordPosition="448">h limitation of 140 characters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2. To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011). Tweets pose special challenges to entity linking. First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature. Second, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter). On 2Entity variation lists can be extracted from the entity resolution pages of Wikipedia. For example, the link “http://en.wikipedia.org/wiki/Svm” will lead us</context>
<context position="4846" citStr="Milne and Witten, 2008" startWordPosition="732" endWordPosition="735">ny tweets, which allows us to aggregate all related tweets to compute mention-mention similarity and mentionentity similarity. We propose a collective inference method that leverages tweet redundancy to address those two challenges. Given a set of mentions, our model tries to ensure that similar mentions are linked to similar entities while pursuing the high total similarity between matched mentionentity pairs. More specifically, we define local features, including context similarity and edit distance, to model the similarity between a mention and an entity. We adopt in-link based similarity (Milne and Witten, 2008), to measure the similarity between entities. Finally, we introduce a set of features to compute the similarity between mentions, including how similar the tweets containing the mentions are, whether they come from the tweets of the same account, and their edit distance. Notably, our model can resolve OOV mentions with the help of their similar mentions. For example, for the OOV mention “LukeBryanOnline”, our model can find similar mentions like “TheLukeBryan” and “LukeBryan”. Considering that most of its similar mentions are mapped to the American country singer “Luke Bryan”, our model tends </context>
<context position="7520" citStr="Milne and Witten (2008)" startWordPosition="1134" endWordPosition="1137">h suggestions of future work. 2 Related Work Existing entity linking work can roughly be divided into two categories. Methods of the first category resolve one mention at each time, and mainly consider the similarity between a mention-entity pair. In contrast, methods of the second category take a set of related mentions (e.g., mentions in the same document) as input, and figure out their corresponding entities simultaneously. Examples of the first category include the first Web-scale entity linking system SemTag (Dill et al., 2003), Wikify! (Mihalcea and Csomai, 2007), and the recent work of Milne and Witten (2008). SemTag uses the TAP knowledge base5, and employs the cosine similarity with TF-IDF weighting scheme to compute the match degree between a mention and an entity, achieving an accuracy of around 82%. Wikify! identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. It introduces two approaches to define mention-entity similarity, i.e., the contextual overlap between the paragraph where the mention occurs and the corresponding Wikipedia pages, and a Naive Bayes classifier that predicts whether a mention should be linked to an enti</context>
<context position="17680" citStr="Milne and Witten, 2008" startWordPosition="2786" endWordPosition="2789">is an abbreviation of its corresponding entity7. • Mention Contains Title: If the mention contains the entity title, namely the title of the Wikipedia page introducing the entity ei, f4(mi, ei) = 1, else 0. • Title Contains Mention: If the entry title contains the mention, f5(mi, ei) = 1, otherwise 0. 7Take “ms” and “Microsoft” for example. The length of “ms” is 2, and the edit distance between them is 7. 2 plus 7 equals to 9, which is the length of “Microsoft”. 4.2.2 Features Related to Entity Similarity There are two representative definitions of entity similarity: in-link based similarity (Milne and Witten, 2008) and category based similarity (Shen et al., 2012). Considering that the Wikipedia categories are often noisy (Milne and Witten, 2008), we adopt in-link based similarity, as defined in Formula 4: log|g(ei) n g(ej) |− log max(|g(ei)|, |g(ej)|) r(ei, ej) = log(Total) − log min(|g(ei)|, |g(ej)|) (4) Where: • Total is the total number of knowledge base entities; • g(e) is the number of Wikipedia definition pages that have a link to entity e. 4.2.3 Features Related to Mention Similarity We define 5 features to model the similarity between two mentions mi and mj, as listed below, where t(m) denotes </context>
</contexts>
<marker>Milne, Witten, 2008</marker>
<rawString>David Milne and Ian H. Witten. 2008. Learning to link with wikipedia. In Proceeding of the 17th ACM conference on Information and knowledge management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Dela Rosa</author>
<author>Rushin Shah</author>
<author>Bo Lin</author>
<author>Anatole Gershman</author>
<author>Robert Frederking</author>
</authors>
<title>Topical clustering of tweets.</title>
<date>2010</date>
<booktitle>In SWSM’10.</booktitle>
<contexts>
<context position="1716" citStr="Rosa et al., 2010" startWordPosition="243" endWordPosition="246"> to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method. 1 Introduction Twitter is a widely used social networking service. With millions of active users and hundreds of millions of new published tweets every day1, it has become a popular platform to capture and transmit the human experiences of the moment. Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal w</context>
</contexts>
<marker>Rosa, Shah, Lin, Gershman, Frederking, 2010</marker>
<rawString>Kevin Dela Rosa, Rushin Shah, Bo Lin, Anatole Gershman, and Robert Frederking. 2010. Topical clustering of tweets. In SWSM’10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Shen</author>
<author>Jianyong Wang</author>
<author>Ping Luo</author>
<author>Min Wang</author>
</authors>
<title>Liege: Link entities in web lists with knowledge base.</title>
<date>2012</date>
<booktitle>In KDD’12.</booktitle>
<contexts>
<context position="8880" citStr="Shen et al. (2012)" startWordPosition="1343" endWordPosition="1346">wever, they cleverly use the 5TAB (http://www.w3.org/2002/05/tap/) is a shallow knowledge base that contains a broad range of lexical and taxonomic information about popular objects like music, movies, authors, sports, autos, health, etc. 1305 links found within Wikipedia articles for training, exploiting the fact that for every link, a Wikipedian has manually selected the correct destination to represent the intended sense of the anchor. Their method achieves an F1 score of 75.0%. Representative studies of the second category include the work of Kulkarni et al. (2009), Han et al. (2011), and Shen et al. (2012). One common feature of these studies is that they leverage the global coherence between entities. Kulkarni et al. (2009) propose a graphical model that explicitly models the combination of evidence from local mentionentity compatibility and global document-level topical coherence of the entities, and show that considering global coherence between entities significantly improves the performance. Han et al. (2011) introduce a graph-based representation, called Referent Graph, to model the global interdependence between different entity linking decisions, and jointly infer the referent entities </context>
<context position="17730" citStr="Shen et al., 2012" startWordPosition="2794" endWordPosition="2797">ion Contains Title: If the mention contains the entity title, namely the title of the Wikipedia page introducing the entity ei, f4(mi, ei) = 1, else 0. • Title Contains Mention: If the entry title contains the mention, f5(mi, ei) = 1, otherwise 0. 7Take “ms” and “Microsoft” for example. The length of “ms” is 2, and the edit distance between them is 7. 2 plus 7 equals to 9, which is the length of “Microsoft”. 4.2.2 Features Related to Entity Similarity There are two representative definitions of entity similarity: in-link based similarity (Milne and Witten, 2008) and category based similarity (Shen et al., 2012). Considering that the Wikipedia categories are often noisy (Milne and Witten, 2008), we adopt in-link based similarity, as defined in Formula 4: log|g(ei) n g(ej) |− log max(|g(ei)|, |g(ej)|) r(ei, ej) = log(Total) − log min(|g(ei)|, |g(ej)|) (4) Where: • Total is the total number of knowledge base entities; • g(e) is the number of Wikipedia definition pages that have a link to entity e. 4.2.3 Features Related to Mention Similarity We define 5 features to model the similarity between two mentions mi and mj, as listed below, where t(m) denotes the tweet that contains mention m: • s1(mi,mj): Th</context>
</contexts>
<marker>Shen, Wang, Luo, Wang, 2012</marker>
<rawString>Wei Shen, Jianyong Wang, Ping Luo, and Min Wang. 2012. Liege: Link entities in web lists with knowledge base. In KDD’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Amarnag Subramanya</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Largescale cross-document coreference using distributed inference and hierarchical models.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>793--803</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2190" citStr="Singh et al., 2011" startWordPosition="315" endWordPosition="318">s are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009). In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia. Entity 1http://siteanalytics.compete.com/twitter.com/ linking task is generally considered as a bridge between unstructured text and structured machinereadable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011). Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters. Current entity linking methods are built on top of a large scale knowledge base such as Wikipedia. A knowledge base consists of a set of entities, and each entity can have a variation list2. To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and</context>
</contexts>
<marker>Singh, Subramanya, Pereira, McCallum, 2011</marker>
<rawString>Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. 2011. Largescale cross-document coreference using distributed inference and hierarchical models. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 793– 803, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>