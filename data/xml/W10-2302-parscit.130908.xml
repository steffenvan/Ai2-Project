<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002776">
<title confidence="0.9864925">
Towards the Automatic Creation of a Wordnet from a Term-based Lexical
Network
</title>
<author confidence="0.994833">
Hugo Gonc¸alo Oliveira*
</author>
<affiliation confidence="0.997548">
CISUC, University of Coimbra
</affiliation>
<address confidence="0.422942">
Portugal
</address>
<email confidence="0.994933">
hroliv@dei.uc.pt
</email>
<sectionHeader confidence="0.993773" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999978555555556">
The work described here aims to create
a wordnet automatically from a semantic
network based on terms. So, a cluster-
ing procedure is ran over a synonymy net-
work, in order to obtain synsets. Then, the
term arguments of each relational triple
are assigned to the latter, originating a
wordnet. Experiments towards our goal
are reported and their results validated.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988407407407">
In order perform tasks where understanding the in-
formation conveyed by natural language is criti-
cal, today’s applications demand better access to
semantic knowledge. Knowledge about words
and their meanings is typically structured in lex-
ical ontologies, such as Princeton WordNet (Fell-
baum, 1998), but this kind of resources is most of
the times handcrafted, which implies much time-
consuming human effort. So, the automatic con-
struction of such resources arises as an alterna-
tive, providing less intensive labour, easier mainte-
nance and allowing for higher coverage, as a trade-
off for lower, but still acceptable, precision.
This paper is written in the scope of a project
where several textual resources are being exploited
for the construction of a lexical ontology for Por-
tuguese. We have already made a first approach
on the extraction of relational triples from text,
where, likewise Hearst (1992), we take advantage
of textual patterns indicating semantic relations.
However, the extracted triples are held between
two terms, which is not enough to build a lexical
ontology capable of dealing with ambiguity.
Therefore, we present our current approach to-
wards the automatic integration of lexico-semantic
knowledge into a single independent lexical on-
tology, which will be structured on concepts and
</bodyText>
<footnote confidence="0.81343">
*supported by FCT scholarship SFRH/BD/44955/2008.
</footnote>
<note confidence="0.495555666666667">
Paulo Gomes
CISUC, University of Coimbra
Portugal
</note>
<email confidence="0.952586">
pgomes@dei.uc.pt
</email>
<bodyText confidence="0.999961944444444">
adopt a model close to WordNet’s. The task of es-
tablishing synsets and mapping term-based triples
to them is closely related to word sense disam-
biguation, where the only available context con-
sists of the connections in the term-base network.
After contextualising this work, our approach is
described. It involves (i) a clustering procedure for
obtaining a thesaurus from a synonymy network,
(ii) the augmentation of the later with manually
created thesaurus, and (iii) mapping term-based
relational triples to the thesaurus, to obtain a word-
net. Then, our experimentation results, as well as
their validation, are presented. Briefly, we have
tested the proposed approach on a term-based lex-
ical network, extracted automatically from a dic-
tionary. Synsets were validated manually while
the attached triples were validated with the help
of a web search engine.
</bodyText>
<sectionHeader confidence="0.992696" genericHeader="introduction">
2 Context
</sectionHeader>
<bodyText confidence="0.999471">
Our ultimate goal is the automatic construction of
a broad-coverage structure of words according to
their meanings, also known as a lexical ontology,
the first subject of this section. We proceed with
a brief overview on work concerned with mov-
ing from term-based knowledge to synset-based
knowledge, often called ontologising.
</bodyText>
<subsectionHeader confidence="0.979651">
2.1 Lexical Ontologies
</subsectionHeader>
<bodyText confidence="0.999322363636364">
Despite some terminological issues, lexical on-
tologies can be seen both as a lexicon and as an on-
tology (Hirst, 2004) and are significantly different
from classic ontologies (Gruber, 1993). They are
not based on a specific domain and are intended
to provide knowledge structured on lexical items
(words) of a language by relating them according
to their meaning. Moreover, the main goal of a
lexical ontology is to assemble lexical and seman-
tic information, instead of storing common-sense
knowledge (Wandmacher et al., 2007).
</bodyText>
<page confidence="0.987131">
10
</page>
<note confidence="0.980153">
Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 10–18,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997498763636364">
Princeton WordNet (Fellbaum, 1998) is the
most representative lexico-semantic resource for
English and also the most accepted model of a
lexical ontology. It is structured around groups of
synonymous words (synsets), which describe con-
cepts, and connections, denoting semantic rela-
tions between those groups. The success of Word-
Net led to the adoption of its model by lexical re-
sources in different languages, such as the ones
in the EuroWordNet project (Vossen, 1997), or
WordNet.PT (Marrafa, 2002), for Portuguese.
However, the creation of a wordnet, as well as
the creation of most ontologies, is typically man-
ual and involves much human effort. Some au-
thors (de Melo and Weikum, 2008) propose trans-
lating Princeton WordNet to wordnets in other lan-
guages, but if this might be suitable for several ap-
plications, a problem arises because different lan-
guages represent different socio-cultural realities,
do not cover exactly the same part of the lexicon
and, even where they seem to be common, several
concepts are lexicalised differently (Hirst, 2004).
Another popular alternative is to extract lexico-
semantic knowledge and learn lexical ontologies
from text. Research on this field is not new and
varied methods have been proposed to achieve dif-
ferent steps of this task including the extraction of
semantic relations (e.g. (Hearst, 1992) (Girju et
al., 2006)) or sets of similar words (e.g. (Lin and
Pantel, 2002) (Turney, 2001)).
Whereas the aforementioned works are based
on unstructured text, dictionaries started earlier
(Calzolari et al., 1973) to be seen as an attrac-
tive target for the automatic acquisition of lexico-
semantic knowledge. MindNet (Richardson et al.,
1998) is both an extraction methodology and a lex-
ical ontology different from a wordnet, since it
was created automatically from a dictionary and
its structure is based on such resources. Neverthe-
less, it still connects sense records with semantic
relations (e.g. hyponymy, cause, manner).
For Portuguese, PAPEL (Gonc¸alo Oliveira et
al., 2009) is a lexical network consisting of triples
denoting semantic relations between words found
in a dictionary. Other Portuguese lexical ontolo-
gies, created by different means, are reviewed and
compared in (Santos et al., 2009) and (Teixeira et
al., 2010).
Besides corpora and dictionary processing, in
the later years, semi-structured collaborative re-
sources, such as Wikipedia or Wiktionary, have
proved to be important sources of lexico-semantic
knowledge and have thus been receiving more and
more attention by the community (see for instance
(Zesch et al., 2008) (Navarro et al., 2009)).
</bodyText>
<subsectionHeader confidence="0.997431">
2.2 Other Relevant Work
</subsectionHeader>
<bodyText confidence="0.9998552">
Most of the methods proposed to extract relations
from text have term-based triples as output. Such
a triple, term] RELATION term2, indicates that a
possible meaning of term] is related to a possible
meaning of term2 by means of a RELATION.
Although it is possible to create a lexical
network from the latter, this kind of networks
is often impractical for computational applica-
tions, such as the ones that deal with infer-
ence. For instance, applying a simple transitive
rule, a SYNONYM OF b n b SYNONYM OF c
—* a SYNONYM OF c over a set of term-based
triples can lead to serious inconsistencies. A curi-
ous example in Portuguese, where synonymy be-
tween two completely opposite words is inferred,
is reported in (Gonc¸alo Oliveira et al., 2009):
queda SYNONYM OF ruina n queda SYN-
ONYM OF habilidade —* ruina SYNONYM OF
habilidade. This happens because natural lan-
guage is ambiguous, especially when dealing with
broad-coverage knowledge. In the given example,
queda can either mean downfall or aptitude, while
ruina means ruin, destruction, downfall.
A possible way to deal with ambiguity is to
adopt a wordnet-like structure, where concepts
are described by synsets and ambiguous words
are included in a synset for each of their mean-
ings. Semantic relations can thereby be unambigu-
ously established between two synsets, and con-
cepts, even though described by groups of words,
bring together natural language and knowledge en-
gineering in a suitable representation, for instance,
for the Semantic Web (Berners-Lee et al., 2001).
Of course that, from a linguistic point of view,
word senses are complex and overlapping struc-
tures (Kilgarriff, 1997) (Hirst, 2004). So, despite
word sense divisions in dictionaries and ontologies
being most of the times artificial, this trade-off is
needed in order to increase the usability of broad-
coverage computational lexical resources.
In order to move from term-based triples to
an ontology, Soderland and Mandhani (2007) de-
scribe a procedure where, besides other stages,
terms in triples are assigned to WordNet synsets.
Starting with all the synsets containing a term in
</bodyText>
<page confidence="0.998561">
11
</page>
<bodyText confidence="0.999858293103449">
a triple, the term is assigned to the synset with
higher similarity to the contexts from where the
triple was extracted, computed based on the terms
in the synset, sibling synsets and direct hyponym
synsets.
Two other methods for ontologising term-based
triples are presented by Pantel and Pennacchiotti
(2008). One assumes that terms with the same
relation to a fixed term are more plausible to de-
scribe the correct sense, so, to select the correct
synset, it exploits triples of the same type sharing
one argument. The other method, which seems to
perform better, selects suitable synsets using gen-
eralisation through hypernymy links in WordNet.
There are other works where WordNet is
enriched, for instance with information in its
glosses, domain knowledge extracted from text
(e.g. (Harabagiu and Moldovan, 2000) (Navigli
et al., 2004)) or wikipedia entries (e.g. (Ruiz-
Casado et al., 2005)), thus requiring a disambigua-
tion phase where terms are assigned to synsets.
In the construction of a lexical ontology, syn-
onymy plays an important role because it defines
the conceptual base of the knowledge to be rep-
resented. One of the reasons for using WordNet
synsets as a starting point for its representation is
that, while it is quite straightforward to define a set
of textual patterns indicative of several semantic
relations between words (e.g. hyponymy, part-of,
cause) with relatively good quality, the same does
not apply for synonymy. In opposition to other
kinds of relation, synonymous words, despite typi-
cally sharing similar neighbourhoods, may not co-
occur frequently in unstructured text, especially in
the same sentence (Dorow, 2006), leading to few
indicative textual patterns. Therefore, most of the
works on synonymy extraction from corpora rely
on statistics or graph-based methods (e.g. (Lin
and Pantel, 2002) (Turney, 2001) (Dorow, 2006)).
Nevertheless, methods for synonymy identifica-
tion based on co-occurrences (e.g. (Turney, 2001))
are more prone to identify similar words or near
synonyms than real synonyms.
On the other hand, synonymy instances can be
quite easily extracted from resources structured on
words and meanings, such as dictionaries, by tak-
ing advantage not only of textual patterns, more
frequent in those resources (e.g. tamb´em con-
hecido por/como, o mesmo que, for Portuguese),
but also of definitions consisting of only one word
or a enumeration, which typically contain syn-
onyms of the defined word. So, as it is possible
to create a lexical network from a set of relational
triples (a R b), a synonymy network can be created
out of synonymy instances (a SYNONYM OF
b). Since these networks tend to have a clustered
structure, Gfeller et al. (2005) propose a clustering
procedure to improve their utility.
</bodyText>
<sectionHeader confidence="0.9821" genericHeader="method">
3 Research Goals
</sectionHeader>
<bodyText confidence="0.999974523809524">
The research presented here is in the scope of a
project whose final goal is to create a lexical ontol-
ogy for Portuguese by automatic means. Although
there are clear advantages of using resources al-
ready structured on words and meanings, dictio-
naries are static resources which contain limited
knowledge and are not always available for this
kind of research. On the other hand, there is much
text available on the most different subjects, but
free text has few boundaries, leading to more am-
biguity and parsing issues.
Therefore, it seems natural to create a lexi-
cal ontology with knowledge from several tex-
tual sources, from (i) high precision structured re-
sources, such as manually created thesaurus, to
(ii) semi-structured resources such as dictionaries
or collaborative encyclopedias, as well as (iii) un-
structured textual corpora. Likewise Wandmacher
et al. (2007) propose for creating a lexical ontol-
ogy for German, these are the general lines we will
follow in our research, but for Portuguese.
Considering each resource specificities, includ-
ing its organisation or the vocabulary used, the ex-
traction procedures might be significantly differ-
ent, but they should all have one common output:
a set of term-based relational triples that will be
integrated in a single lexical ontology.
Whereas the lexical network established by the
triples could be used, these networks are not suit-
able for several tasks, as discussed in Section 2.2.
A fragment of a synonymy network extracted from
a Portuguese dictionary can be seen in Figure 1.
Since all the connections imply synonymy, the
network suggests that all the words are synony-
mous, which is not true. For example, the word
copista may have two very distinct meanings: (a) a
person who writes copies of written documents or
(b) someone who drinks a lot of wine. On the other
hand, other words which may refer to the same
concept as, for instance, meaning (a) of copista,
such as escrevente, escriv˜ao or transcritor.
So, in order to deal with ambiguity in natural
</bodyText>
<page confidence="0.993067">
12
</page>
<bodyText confidence="0.998338666666667">
language, we will adopt a wordnet-like structure
which enables the establishment of unambiguous
semantic relations between synsets.
</bodyText>
<figureCaption confidence="0.998172">
Figure 1: Fragment of a synonymy network.
</figureCaption>
<sectionHeader confidence="0.994951" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999991055555555">
Considering our goal, a set of term-based triples
goes through the following stages: (i) clustering
over the synonymy network for the establishment
of synsets, to obtain a thesaurus; (ii) augmenta-
tion of the thesaurus by merging it with synsets
from other resources; (iii) assignment of each ar-
gument of a term-based triple (except synonymy)
to a synset in the thesaurus, to obtain a wordnet.
Note that stages (i) and (ii) are not both manda-
tory, but at least one must be performed to obtain
the synsets.
Looking at some of the works referred in Sec-
tion 2.2, ours is different because it does not re-
quire a conceptual base such as WordNet. Also,
it integrates knowledge from different sources and
tries to disambiguate each word using only knowl-
edge already extracted and not the context where
the word occurs.
</bodyText>
<subsectionHeader confidence="0.991683">
4.1 Clustering for a thesaurus
</subsectionHeader>
<bodyText confidence="0.999982129032258">
This stage was originally defined after looking
at disconnected pieces of a synonymy network
extracted from a dictionary, which had a clus-
tered structure apparently suitable for identifying
synsets. This is also noticed by Gfeller et al.
(2005) who have used the Markov Clustering al-
gorithm (MCL) (van Dongen, 2000) to find clus-
ters in a synonymy network.
Therefore, since MCL had already been applied
to problems very close to ours (e.g. (Gfeller et al.,
2005), (Dorow, 2006)), it seemed to suit our pur-
pose – it would not only organise a term-based net-
work into a thesaurus, but, if a network extracted
from several resources is used, clustering would
homogenise the synonymy representation.
MCL finds clusters by simulating random walks
within a graph by alternately computing random
walks of higher length, and increasing the prob-
abilities of intra-cluster walks. It can be briefly
described in five steps: (i) take the adjacency ma-
trix A of the graph; (ii) normalise each column of
A to 1 in order to obtain a stochastic matrix 5;
(iii) compute 52; (iv) take the -yth power of every
element of 52 and normalise each column to 11;
(v) go back to (ii) util MCL converges to a matrix
idempotent under steps (ii) and (iii).
Since MCL is a hard-clustering algorithm, it as-
signs each term to only one cluster thus remov-
ing ambiguities. To deal with this, Gfeller et al.
(2005) propose an extension to MCL for finding
unstable nodes in the graph, which frequently de-
note ambiguous words. This is done by adding
random stochastic noise, 6, to the non-zero entries
of the adjacency matrix and then running MCL
with noise several times. Looking at the clusters
obtained by each run, a new matrix can be filled
based on the probability of each pair of words be-
longing to the same cluster.
We have adopted this procedure, with slight dif-
ferences. First, we observed that, for the network
we used, the obtained clusters were closer to the
desired results if −0.5 &lt; 6 &lt; 0.5. Additionally, in
the first step of MCL, we use frequency-weighted
adjacency matrixes F, where each element Fid
corresponds to the number of existing synonymy
instances between i and j. Although using only
one dictionary each synonymy instance will be ex-
tracted at most two times (a SYNONYM OF b
and b SYNONYM OF a), if more resources are
used, it will strengthen the probability that two
words appearing frequently as synonyms belong
to the same cluster.
Therefore, the clustering stage has the follow-
ing steps: (i) split the original network into sub-
networks, such that there is no path between two
elements in different sub-networks, and calculate
the frequency-weighted adjacency matrix F of
each sub-network; (ii) add stochastic noise to each
entry of F, Fid = Fid + Fid * 6; (iii) run MCL,
with -y = 1.6, over F for 30 times; (iv) use the
(hard) clustering obtained by each one of the 30
runs to create a new matrix P with the probabil-
</bodyText>
<footnote confidence="0.933033">
1Increasing -y (typically 1.5 &lt; -y &lt; 2) increases the gran-
ularity of the clusters.
</footnote>
<page confidence="0.998754">
13
</page>
<bodyText confidence="0.998994545454545">
ities of each pair of words in F belonging to the
same cluster; (v) create the clusters based on P
and on a given threshold 0 = 0.2. If Pij &gt; 0, i and
j belong to the same cluster; (vi) in order to clean
the results, remove: (a) big clusters, B, if there
is a group of clusters C = C1, C2,...Cn such that
B = C1 UC2 U...UCn; (b) clusters completely in-
cluded in other clusters. Applying this procedure
to the network in Figure 1 results in the four repre-
sented clusters. There, ambiguous words escriv˜ao
and escriba are included in two different clusters.
</bodyText>
<subsectionHeader confidence="0.910797">
4.2 Merging synsets for thesaurus
augmentation
</subsectionHeader>
<bodyText confidence="0.9924146">
In this stage, other resources with synsets, such as
manually created thesaurus, are merged together
and then merged with the thesaurus obtained in the
previous stage, by the following procedure: (i) de-
fine one thesaurus as the basis B and the other as
T; (ii) create a new empty thesaurus M and copy
all the synsets in B to M; (iii) for each synset
Ti E T, find the synsets Bi E B with higher Jac-
card coefficient2 c, and add them to a set of synsets
J C B. (iv) considering c and J, do one of the
following: (a) if c = 1, it means that the synset is
already in M, so nothing is done; (b) if c = 0, Ti
is copied to M; (c) if |J |= 1, the synset in J is
copied to M; (d) if |J |&gt; 1, a new set, n = Ti UJ0
where J0 = U|J|
i�0Ji, Ji E J, is created, and all
elements of J are removed from M.
The synsets of the resulting thesaurus will be
used as the conceptual base in which the term-
based triples are going to be mapped.
</bodyText>
<subsectionHeader confidence="0.998677">
4.3 Assigning terms to synsets
</subsectionHeader>
<bodyText confidence="0.989330933333333">
After the previous stages, the following are avail-
able: (i) a thesaurus T and (ii) a term-based se-
mantic network, N, where each edge has a type,
R, and denotes a semantic relation held between
the meaning of the terms in the two nodes it con-
nects. Using T and N, this stage tries to map term-
based triples to synset-based triples, or, in other
words, assign each term, a and b, in each triple,
(a R b) E N, to suitable synsets. The result is a
knowledge base organised as a wordnet.
In order to assign a to a synset A, b is fixed
and all the synsets containing a, Sa C T, are col-
lected. If a is not in the thesaurus, it is assigned to
a new synset A = (a). Otherwise, for each synset
Sai E Sa, nai is the number of terms t E Sai such
</bodyText>
<equation confidence="0.9813995">
2Jaccard(A, B) = A n B/A U B
that (t R b) holds3. Then, pai = nai
</equation>
<bodyText confidence="0.968070611111111">
|Sai |is calcu-
lated. Finally, all the synsets with the highest pai
are added to C and (i) if |C |= 1 , a is assigned to
the only synset in C; (ii) if |C |&gt; 1, C0 is the set of
elements of C with the highest na and, if |C0 |= 1,
a is assigned the synset in C0, unless pai &lt; 0 4;
(iii) if it is not possible to assign a synset to a, it
remains unassigned. Term b is assigned to a synset
using this procedure, but fixing a.
If hypernymy links are already established,
semi-mapped triples, where one of the arguments
is assigned to a synset and the other is not, (A
R b) or (a R B), go to a second phase. There,
hypernymy is exploited together with the assign-
ment candidates, in C, to help assigning the unas-
signed term in each semi-mapped triple, or to re-
move triples that can be inferred. Take for instance
(A R b). If there is one synset Ci E C with:
</bodyText>
<listItem confidence="0.971138">
• a hypernym synset H, (H HYPERNYM OF
Ci) and a triple (A R H), b would be as-
signed to Ci, but, since hyponyms inherit all
the properties of their hypernym, the result-
ing triple can be inferred and is thus ignored:
</listItem>
<equation confidence="0.92287">
(A R H) ∧ (H HYPERNYM OF Ci) --+ (A R Ci)5
</equation>
<bodyText confidence="0.599774666666667">
For example, if H=(mammal) and Ci=(dog), possi-
ble values of A and R are A=(hair) R=PART OF;
A=(animal) R=HYPERNYM OF
</bodyText>
<listItem confidence="0.933236">
• a hyponym synset H, (Ci HYPERNYM OF
H) and a triple (A R H), b is assigned to Ci.
Furthermore, if all the hyponyms of Ci, (Ci
HYPERNYM OF Ii), are also related to A in
</listItem>
<bodyText confidence="0.840027">
the same way, (A R Ii), it can be inferred that
Ii inherits the relation from Ci. So, all the
later triples can be inferred and thus removed.
</bodyText>
<footnote confidence="0.94537165">
For example, if H=(dog), Ii=(cat), Ij=(mouse)
and Ci=(mammal), possible values of A and
R are A=(hair) R=PART OF; A=(animal)
R=HYPERNYM OF
3If R is a transitive relation, the procedure may benefit
from applying one level of transitivity to the network: x R y
∧ y R z --+ x R z. However, since relations are held between
terms, some obtained triples might be incorrect. So, although
the latter can be used to help selecting a suitable synset, they
should not be mapped to synsets themselves.
40 is a threshold defined to avoid that a is assigned to a
big synset where a, itself, is the only term related to b
5Before applying these rules it is necessary to make sure
that all relations are represented only in one way, otherwise
they might not work. For instance, if the decision is to rep-
resent part-of triples in the form part PART OF whole,
triples whole HAS PART part must be reversed. Further-
more, these rules assume that hypernymy relations are all rep-
resented hypernym HYPERNYM OF hyponym and not
hyponym HYPONYM OF hypernym.
</footnote>
<page confidence="0.99945">
14
</page>
<sectionHeader confidence="0.997917" genericHeader="method">
5 Experimentation
</sectionHeader>
<bodyText confidence="0.9999507">
In this section we report experimental results ob-
tained after applying our procedure to part of the
lexical network of PAPEL (Gonc¸alo Oliveira et al.,
2009). The clustering procedure was first ran over
PAPEL’s noun synonymy network in order to ob-
tain the synsets which were later merged with two
manually created thesaurus. Finally, hypernym-
of, member-of and part-of triples of PAPEL were
mapped to the thesaurus by assigning a synset to
each term argument.
</bodyText>
<subsectionHeader confidence="0.991741">
5.1 Resources used
</subsectionHeader>
<bodyText confidence="0.99999356">
For experimentation purposes, freely available
lexical resources for Portuguese were used. First,
the last version of PAPEL, 2.0, a lexical network
for Portuguese created automatically from a dic-
tionary, as referred in Section 2. PAPEL 2.0
contains approximately 100,000 words, identified
by their orthographical form, and approximately
200,000 term-based triples relating the words by
different types of semantic relations.
In order to enrich the thesaurus obtained from
PAPEL, TeP (Dias-Da-Silva and de Moraes, 2003)
and OpenThesaurus.PT6 (OT), were used. Both of
them are manually created thesaurus, for Brazil-
ian Portuguese and European Portuguese respec-
tively, modelled after Princeton WordNet (Fell-
baum, 1998) and thus containing synsets. Besides
being the only freely available thesaurus for Por-
tuguese we know about, TeP and OT were used to-
gether with PAPEL because, despite representing
the same kind of knowledge, they are mostly com-
plementary, which is also observed by (Teixeira et
al., 2010) and (Santos et al., 2009).
Note that, for experimentation purposes, we
have only used the parts of these resources con-
cerning nouns.
</bodyText>
<subsectionHeader confidence="0.997887">
5.2 Thesaurus creation
</subsectionHeader>
<bodyText confidence="0.9999767">
The first step for applying the clustering proce-
dure is to create PAPEL’s synonymy network,
which is established by its synonymy instances,
a SYNONYM OF b. After splitting the network
into independent disconnected sub-networks, we
noticed that it was composed by a huge sub-
network, with more than 16,000 nodes, and sev-
eral very small networks. If ambiguity was not
resolved, this would suggest that all the 16,000
words had the same meaning, which is not true.
</bodyText>
<footnote confidence="0.968292">
6http://openthesaurus.caixamagica.pt/
</footnote>
<table confidence="0.999152142857143">
TeP OT CLIP TOP
Quantity 17,158 5,819 23,741 30,554
Words Ambiguous 5,867 442 12,196 13,294
Most ambiguous 20 4 47 21
Quantity 8,254 1,872 7,468 9,960
Synsets Avg. size 3.51 3.37 12.57 6.6
Biggest 21 14 103 277
</table>
<tableCaption confidence="0.996828">
Table 1: (Noun) thesaurus in numbers.
</tableCaption>
<table confidence="0.999729">
Hypernym of Part of Member of
Term-based triples 62,591 2,805 5,929
Mapped 27,750 1,460 3,962
1st Same synset 233 5 12
Already present 3,970 40 167
Semi-mapped triples 7,952 262 357
Mapped 88 1 0
2nd Could be inferred 50 0 0
Already present 13 0 0
Synset-based triples 23,572 1,416 3,783
</table>
<tableCaption confidence="0.999781">
Table 2: Results of triples mapping
</tableCaption>
<bodyText confidence="0.9998616">
A small sample of this problem can be observed
in Figure 1.
We then ran the clustering procedure and the
thesaurus of PAPEL, CLIP, was obtained. Finally,
we used TeP as the base thesaurus and merged it,
first with OT, and then with CLIP, giving rise to
the noun thesaurus we used in the rest of the ex-
perimentation, TOP.
Table 1 contains information about each one
of the thesaurus, more precisely, the quantity
of words, words belonging to more than one
synset (ambiguous), the number of synsets where
the most ambiguous word occurs, the quantity
of synsets, the average synset size (number of
words), and the size of the biggest synset7.
</bodyText>
<subsectionHeader confidence="0.999507">
5.3 Mapping the triples
</subsectionHeader>
<bodyText confidence="0.999692823529412">
The mapping procedure was applied to all the
hypernym-of, part-of and member-of term-based
triples of PAPEL, distributed according to Table 2
where additional numbers on the mapping are pre-
sented. After the first phase of the mapping,
33,172 triples had both of their terms assigned to
a synset, and 10,530 had only one assigned. How-
ever, 4,427 were not really added, either because
the same synset was assigned to both of the terms
or because the triple had already been added after
analysing other term-based triple. In the second
phase, only 89 new triples were mapped and, from
those, 13 had previously been added while other
50 triples were discarded or not attached because
they could be inferred. Another interesting fact is
that 19,638 triples were attached to a synset with
only one term. From those, 5,703 had a synset
</bodyText>
<footnote confidence="0.9617875">
7Synsets with only one word were ignored in the construc-
tion of Table 1.
</footnote>
<page confidence="0.998014">
15
</page>
<bodyText confidence="0.9976905">
with only one term in both arguments.
We ended up with a wordnet with 27,637
synsets, 23,572 hypernym-of, 1,416 part-of and
3,783 member-of synset-based triples.
</bodyText>
<sectionHeader confidence="0.454696" genericHeader="method">
6 Validation of the results
</sectionHeader>
<bodyText confidence="0.99974508">
Evaluation of a new broad-coverage ontology is
most of the times performed by one of two means:
(i) manual evaluation of a representative subset of
the results; (ii) automatic comparison with a gold
standard. However, while for English most re-
searchers use Princeton WordNet as a gold stan-
dard, for other languages it is difficult to find
suitable and freely available consensual resources.
Considering Portuguese, as we have said earlier,
TeP and OT are effectively two manually created
thesaurus but, since they are more complementary
than overlapping to PAPEL, we thought it would
be better to use them to enrich our resource.
There is actually a report (Raman and Bhat-
tacharyya, 2008) with an automatic evaluation of
synsets, but we decided no to follow it because
this evaluation is heavily based on a dictionary and
we do not have unrestricted access to a full and
updated dictionary of Portuguese and also, indi-
rectly by PAPEL, a dictionary was one of our main
sources of information.
Therefore, our choice relied on manual valida-
tion of the synsets of CLIP and TOP. Furthermore,
synset-based triples were validated in an alterna-
tive automatic way using a web search engine.
</bodyText>
<subsectionHeader confidence="0.997939">
6.1 Manual validation of synsets
</subsectionHeader>
<bodyText confidence="0.999687708333333">
Ten reviewers took part in the validation of ten ran-
dom samples with approximately 50 synsets from
each thesaurus. We made sure that each synset was
not in more than one sample and synsets with more
than 50 terms were not validated. Also, in order to
measure the reviewer agreement, each sample was
analysed by two different reviewers. Given a sam-
ple, each reviewer had to classify each synset as:
correct (1), if, in some context, all the terms of the
synset could have the same meaning, or incorrect
(0), if at least one term of the synset could never
mean the same as the others. The reviewers were
advised to look for the possible meanings of each
word in different dictionaries. Still, if they could
not find them, or if they did not know how to clas-
sify the synset, they had a third option, N/A (2).
In the end, 519 synsets of CLIP and 480 of
TOP were validated. When organising the vali-
dation results we noticed that the biggest synsets
were the ones with more problems. So, besides the
complete validation results, Table 3 also contains
the results considering only synsets of ten or less
words, when a ’ is after the name of the thesaurus.
The presented numbers are the average between
the classifications given by the two reviewers and
the agreement rate corresponds to the number of
times both reviewers agreed on the classification.
Even though these results might be subjec-
tive, since they are based on the reviewers cri-
teria and on the dictionaries they used, they can
give an insight on the quality of the synsets.
The precision results are acceptable and are im-
proved if the automatically created thesaurus is
merged with the ones created manually, and
also when bigger synsets are ignored. Most
of the times, big synsets are confusing because
they bring together more than one concept that
share at least one term. For instance, take the
synset: insobriedade, desmedida, imoderac¸˜ao,
excesso, nimiedade, desmando, desbragamento,
troco, descontrolo, superabundˆancia, desbunda,
desregramento, demasia, incontinˆencia, imodici-
dade, superac¸˜ao, intemperanc¸a, descomedimento,
superfluidade, sobejid˜ao, acrasia, where there is a
mix of the concepts: (a) insobriety, not following
all the rules, heedless of the consequences and, (b)
surplus. Both of these concepts can be referred to
as an excess (excesso).
</bodyText>
<subsectionHeader confidence="0.999236">
6.2 Automatic validation of triples
</subsectionHeader>
<bodyText confidence="0.996861904761905">
The automatic validation of the triples attached to
our wordnet consisted of using Google web search
engine to look for evidence on their truth. This
procedure started by removing terms whose oc-
currences in Google were less than 5,000. Synsets
that became empty were not considered and, from
the rest, a sample was selected for each one of the
three types of relation.
Following the idea in (Gonc¸alo Oliveira et al.,
2009), a set of natural language generic patterns,
indicative of each relation, was defined having in
mind their input to Google8. Then, for each triple
(A R B), the patterns were used to search for ev-
8Hypernymy patterns included: [hypo] e´ um|uma
(tipo|forma|variedade|...)* de [hyper], [hypo] e outros|outras
[hyper] or [hyper] tais como [hypo]. Patterns for part-of and
member-of were the same because these relations can be ex-
pressed in very similar ways, and included: [part/member] e´
(parte|membro|porc¸˜ao) do|da [whole/group], [part/member]
(faz parte)* do|da [whole/group] or [whole/group] e´ um
(grupo|conjunto|...) de [part/member].
</bodyText>
<page confidence="0.994871">
16
</page>
<table confidence="0.998675">
Sample Correct Incorrect N/A Agreement
CLIP 519 sets 65.8% 31.7% 2.5% 76.1%
CLIP’ 310 sets 81.1% 16.9% 2.0% 84.2%
TOP 480 sets 83.2% 15.8% 1.0% 82.3%
TOP’ 448 sets 86.8% 12.3% 0.9% 83.0%
</table>
<tableCaption confidence="0.999913">
Table 3: Results of manual synset validation.
</tableCaption>
<bodyText confidence="0.999574757575758">
idence on each combination of terms a E A and
b E B connected by a pattern indicative of R.
The triple validation score was then calculated by
expression 1, where found(A, B, R) = 1 if evi-
dence is found for the triple or 0 otherwise.
Table 4 shows the results obtained for each val-
idated sample. Pantel and Pennacchiotti (2008)
perform a similar task and present precision results
for part-of (40.7%-57.4%) and causation (40.0%-
45%) relations. It is however not possible to make
a straight comparison. For their experimentation,
they selected only correct term-based triples ex-
tracted from text and their results were manually
validated by human judges. On the other hand, we
have used term-based triples extracted automati-
cally from a dictionary, with high but not 100%
precision, from where we did not choose only the
correct ones, and we have used synsets obtained
from our clustering procedure which, once again,
have lower precision. Moreover, we validated our
results with Google where, despite its huge dimen-
sion, there are plenty of ways to denote a seman-
tic relation, when we had just a small set textual
patterns. Also, despite occurring more than 5,000
times in Google, some terms correctly included in
a synset were conveying less common meanings.
Nevertheless, we could not agree more with
Pantel and Pennacchiotti (2008) who state that at-
taching term-based triples to an ontology is not an
easy task. Therefore, we believe our results to be
promising and, if more refined rules are added to
our set, which is still very simple, they will surely
be improved.
</bodyText>
<sectionHeader confidence="0.962954" genericHeader="conclusions">
7 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999991117647059">
We have presented our first approach on two cru-
cial steps on the automatic creation of a wordnet
lexical ontology. Clustering proved to be a good
alternative to create a thesaurus from a dictionary’s
synonymy network, while a few rules can be de-
fined to attach a substantial number of term-based
triples to a synset based resource.
Despite interesting results, in the future we will
work on refining the attachment rules and start in-
tegrating other relations such as causation or pur-
pose. Furthermore, we are devising new methods
for attaching terms to synsets. For instance, we
have recently started to do some experiences with
an attaching method which uses the lexical net-
work’s adjacency matrix to find the most similar
pair of synsets, each of them containing one of the
arguments of a term-based triple.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998186133333333">
Tim Berners-Lee, James Hendler, and Ora Lassila.
2001. The Semantic Web. Scientific American,
May.
Nicoletta Calzolari, Laura Pecchia, and Antonio Zam-
polli. 1973. Working on the italian machine dictio-
nary: a semantic approach. In Proc. 5th Conference
on Computational Linguistics, pages 49–52, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Gerard de Melo and Gerhard Weikum. 2008. On the
utility of automatically generated wordnets. In Proc.
4th Global WordNet Conf. (GWC), pages 147–161,
Szeged, Hungary. University of Szeged.
Bento Carlos Dias-Da-Silva and Helio Roberto
de Moraes. 2003. A construc¸˜ao de um the-
saurus eletrˆonico para o portuguˆes do Brasil. ALFA,
47(2):101–115.
Beate Dorow. 2006. A Graph Model for
Words and their Meanings. Ph.D. thesis, Institut
fur Maschinelle Sprachverarbeitung der Universitat
Stuttgart.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). The MIT Press.
David Gfeller, Jean-C´edric Chappelier, and Paulo
De Los Rios. 2005. Synonym Dictionary Im-
provement through Markov Clustering and Cluster-
ing Stability. In Proc. of International Symposium
on Applied Stochastic Models and Data Analysis
(ASMDA), pages 106–113.
</reference>
<figure confidence="0.980039181818182">
Relation
Sample size
Validation
Hypernymy of
Member of
Part of
419 synsets
379 synsets
290 synsets
44,1%
24,3%
24,8%
Table 4: Automatic validation of triples
score =
found(A, B, R)
(1)
|A |� |B|
JAI
i=1
 |B |
E
j=1
</figure>
<page confidence="0.983334">
17
</page>
<reference confidence="0.999812386792453">
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Hugo Gonc¸alo Oliveira, Diana Santos, and Paulo
Gomes. 2009. Relations extracted from a por-
tuguese dictionary: results and first evaluation. In
Local Proc. 14th Portuguese Conf. on Artificial In-
telligence (EPIA).
Thomas R. Gruber. 1993. A translation approach to
portable ontology specifications. Knowledge Acqui-
sition, 5(2):199–220.
Sanda M. Harabagiu and Dan I. Moldovan. 2000.
Enriching the wordnet taxonomy with contextual
knowledge acquired from text. In Natural language
processing and knowledge representation: language
for knowledge and knowledge for language, pages
301–333. MIT Press, Cambridge, MA, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. 14th Conf.
on Computational Linguistics, pages 539–545, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Graeme Hirst. 2004. Ontology and the lexicon. In
Steffen Staab and Rudi Studer, editors, Handbook
on Ontologies, International Handbooks on Informa-
tion Systems, pages 209–230. Springer.
Adam Kilgarriff. 1997. ”I don’t believe in word
senses”. Computing and the Humanities, 31(2):91–
113.
Dekang Lin and Patrick Pantel. 2002. Concept discov-
ery from text. In Proc. 19th Intl. Conf. on Computa-
tional Linguistics (COLING), pages 577–583.
Palmira Marrafa. 2002. Portuguese Wordnet: gen-
eral architecture and internal semantic relations.
DELTA, 18:131–146.
Emmanuel Navarro, Franck Sajous, Bruno Gaume,
Laurent Pr´evot, ShuKai Hsieh, Tzu Y. Kuo, Pierre
Magistry, and Chu R. Huang. 2009. Wiktionary
and nlp: Improving synonymy networks. In Proc.
Workshop on The People’s Web Meets NLP: Col-
laboratively Constructed Semantic Resources, pages
19–27, Suntec, Singapore. Association for Compu-
tational Linguistics.
Roberto Navigli, Paola Velardi, Alessandro Cuc-
chiarelli, and Francesca Neri. 2004. Extending
and enriching wordnet with ontolearn. In Proc.
2nd Global WordNet Conf. (GWC), pages 279–284,
Brno, Czech Republic. Masaryk University.
Patrick Pantel and Marco Pennacchiotti. 2008. Auto-
matically harvesting and ontologizing semantic rela-
tions. In Paul Buitelaar and Phillip Cimmiano, ed-
itors, Ontology Learning and Population: Bridging
the Gap between Text and Knowledge. IOS Press.
J. Raman and Pushpak Bhattacharyya. 2008. Towards
automatic evaluation of wordnet synsets. In Proc.
4th Global WordNet Conf. (GWC), pages 360–374,
Szeged, Hungary. University of Szeged.
Stephen D. Richardson, William B. Dolan, and Lucy
Vanderwende. 1998. Mindnet: Acquiring and struc-
turing semantic information from text. In Proc. 17th
Intl. Conf. on Computational Linguistics (COLING),
pages 1098–1102.
Maria Ruiz-Casado, Enrique Alfonseca, and Pablo
Castells. 2005. Automatic assignment of wikipedia
encyclopedic entries to wordnet synsets. In Proc.
Advances in Web Intelligence Third Intl. Atlantic
Web Intelligence Conf. (AWIC), pages 380–386.
Springer.
Diana Santos, Anabela Barreiro, Luis Costa, Cl´audia
Freitas, Paulo Gomes, Hugo Gonc¸alo Oliveira,
Jos´e Carlos Medeiros, and Ros´ario Silva. 2009. O
papel das relac¸˜oes semˆanticas em portuguˆes: Com-
parando o TeP, o MWN.PT e o PAPEL. In Actas do
XXV Encontro Nacional da Associac¸˜ao Portuguesa
de Lingu´ıstica (APL). forthcomming.
Stephen Soderland and Bhushan Mandhani. 2007.
Moving from textual relations to ontologized rela-
tions. In Proc. AAAI Spring Symposium on Machine
Reading.
Jorge Teixeira, Luis Sarmento, and Eug´enio C.
Oliveira. 2010. Comparing verb synonym resources
for portuguese. In Computational Processing of the
Portuguese Language, 9th Intl. Conference, Proc.
(PROPOR), pages 100–109.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI–IR versus LSA on TOEFL. In Proc. 12th Euro-
pean Conf. on Machine Learning (ECML), volume
2167, pages 491–502. Springer.
S. M. van Dongen. 2000. Graph Clustering by Flow
Simulation. Ph.D. thesis, University of Utrecht.
Piek Vossen. 1997. Eurowordnet: a multilingual
database for information retrieval. In Proc. DE-
LOS workshop on Cross-Language Information Re-
trieval, Zurich.
Tonio Wandmacher, Ekaterina Ovchinnikova, Ulf
Krumnack, and Henrik Dittmann. 2007. Extrac-
tion, evaluation and integration of lexical-semantic
relations for the automated construction of a lexical
ontology. In Third Australasian Ontology Workshop
(AOW), volume 85 of CRPIT, pages 61–69, Gold
Coast, Australia. ACS.
Torsten Zesch, Christof M¨uller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
Wikipedia and Wiktionary. In Proc. 6th Intl.
Language Resources and Evaluation (LREC), Mar-
rakech, Morocco.
</reference>
<page confidence="0.999288">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.321952">
<title confidence="0.997596">Towards the Automatic Creation of a Wordnet from a Term-based Lexical</title>
<author confidence="0.396033">Network</author>
<affiliation confidence="0.8324">CISUC, University of</affiliation>
<email confidence="0.810499">hroliv@dei.uc.pt</email>
<abstract confidence="0.9987567">The work described here aims to create a wordnet automatically from a semantic network based on terms. So, a clustering procedure is ran over a synonymy network, in order to obtain synsets. Then, the term arguments of each relational triple are assigned to the latter, originating a wordnet. Experiments towards our goal are reported and their results validated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tim Berners-Lee</author>
<author>James Hendler</author>
<author>Ora Lassila</author>
</authors>
<title>The Semantic Web.</title>
<date>2001</date>
<publisher>Scientific American,</publisher>
<contexts>
<context position="8070" citStr="Berners-Lee et al., 2001" startWordPosition="1257" endWordPosition="1260">aling with broad-coverage knowledge. In the given example, queda can either mean downfall or aptitude, while ruina means ruin, destruction, downfall. A possible way to deal with ambiguity is to adopt a wordnet-like structure, where concepts are described by synsets and ambiguous words are included in a synset for each of their meanings. Semantic relations can thereby be unambiguously established between two synsets, and concepts, even though described by groups of words, bring together natural language and knowledge engineering in a suitable representation, for instance, for the Semantic Web (Berners-Lee et al., 2001). Of course that, from a linguistic point of view, word senses are complex and overlapping structures (Kilgarriff, 1997) (Hirst, 2004). So, despite word sense divisions in dictionaries and ontologies being most of the times artificial, this trade-off is needed in order to increase the usability of broadcoverage computational lexical resources. In order to move from term-based triples to an ontology, Soderland and Mandhani (2007) describe a procedure where, besides other stages, terms in triples are assigned to WordNet synsets. Starting with all the synsets containing a term in 11 a triple, the</context>
</contexts>
<marker>Berners-Lee, Hendler, Lassila, 2001</marker>
<rawString>Tim Berners-Lee, James Hendler, and Ora Lassila. 2001. The Semantic Web. Scientific American, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicoletta Calzolari</author>
<author>Laura Pecchia</author>
<author>Antonio Zampolli</author>
</authors>
<title>Working on the italian machine dictionary: a semantic approach.</title>
<date>1973</date>
<booktitle>In Proc. 5th Conference on Computational Linguistics,</booktitle>
<pages>49--52</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5464" citStr="Calzolari et al., 1973" startWordPosition="840" endWordPosition="843">same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 2009) is a lexical network consisting of triples denoting semantic relations between words found in a dictionary. Other Portuguese lexical</context>
</contexts>
<marker>Calzolari, Pecchia, Zampolli, 1973</marker>
<rawString>Nicoletta Calzolari, Laura Pecchia, and Antonio Zampolli. 1973. Working on the italian machine dictionary: a semantic approach. In Proc. 5th Conference on Computational Linguistics, pages 49–52, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>On the utility of automatically generated wordnets.</title>
<date>2008</date>
<booktitle>In Proc. 4th Global WordNet Conf. (GWC),</booktitle>
<pages>147--161</pages>
<institution>Szeged, Hungary. University of Szeged.</institution>
<marker>de Melo, Weikum, 2008</marker>
<rawString>Gerard de Melo and Gerhard Weikum. 2008. On the utility of automatically generated wordnets. In Proc. 4th Global WordNet Conf. (GWC), pages 147–161, Szeged, Hungary. University of Szeged.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bento Carlos</author>
</authors>
<title>Dias-Da-Silva and Helio Roberto de Moraes.</title>
<date>2003</date>
<journal>ALFA,</journal>
<volume>47</volume>
<issue>2</issue>
<marker>Carlos, 2003</marker>
<rawString>Bento Carlos Dias-Da-Silva and Helio Roberto de Moraes. 2003. A construc¸˜ao de um thesaurus eletrˆonico para o portuguˆes do Brasil. ALFA, 47(2):101–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
</authors>
<title>A Graph Model for Words and their Meanings.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Institut fur Maschinelle Sprachverarbeitung der Universitat Stuttgart.</institution>
<contexts>
<context position="10305" citStr="Dorow, 2006" startWordPosition="1613" endWordPosition="1614">rtant role because it defines the conceptual base of the knowledge to be represented. One of the reasons for using WordNet synsets as a starting point for its representation is that, while it is quite straightforward to define a set of textual patterns indicative of several semantic relations between words (e.g. hyponymy, part-of, cause) with relatively good quality, the same does not apply for synonymy. In opposition to other kinds of relation, synonymous words, despite typically sharing similar neighbourhoods, may not cooccur frequently in unstructured text, especially in the same sentence (Dorow, 2006), leading to few indicative textual patterns. Therefore, most of the works on synonymy extraction from corpora rely on statistics or graph-based methods (e.g. (Lin and Pantel, 2002) (Turney, 2001) (Dorow, 2006)). Nevertheless, methods for synonymy identification based on co-occurrences (e.g. (Turney, 2001)) are more prone to identify similar words or near synonyms than real synonyms. On the other hand, synonymy instances can be quite easily extracted from resources structured on words and meanings, such as dictionaries, by taking advantage not only of textual patterns, more frequent in those r</context>
<context position="14950" citStr="Dorow, 2006" startWordPosition="2374" endWordPosition="2375">disambiguate each word using only knowledge already extracted and not the context where the word occurs. 4.1 Clustering for a thesaurus This stage was originally defined after looking at disconnected pieces of a synonymy network extracted from a dictionary, which had a clustered structure apparently suitable for identifying synsets. This is also noticed by Gfeller et al. (2005) who have used the Markov Clustering algorithm (MCL) (van Dongen, 2000) to find clusters in a synonymy network. Therefore, since MCL had already been applied to problems very close to ours (e.g. (Gfeller et al., 2005), (Dorow, 2006)), it seemed to suit our purpose – it would not only organise a term-based network into a thesaurus, but, if a network extracted from several resources is used, clustering would homogenise the synonymy representation. MCL finds clusters by simulating random walks within a graph by alternately computing random walks of higher length, and increasing the probabilities of intra-cluster walks. It can be briefly described in five steps: (i) take the adjacency matrix A of the graph; (ii) normalise each column of A to 1 in order to obtain a stochastic matrix 5; (iii) compute 52; (iv) take the -yth pow</context>
</contexts>
<marker>Dorow, 2006</marker>
<rawString>Beate Dorow. 2006. A Graph Model for Words and their Meanings. Ph.D. thesis, Institut fur Maschinelle Sprachverarbeitung der Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Gfeller</author>
<author>Jean-C´edric Chappelier</author>
<author>Paulo De Los Rios</author>
</authors>
<title>Synonym Dictionary Improvement through Markov Clustering and Clustering Stability.</title>
<date>2005</date>
<booktitle>In Proc. of International Symposium on Applied Stochastic Models and Data Analysis (ASMDA),</booktitle>
<pages>106--113</pages>
<contexts>
<context position="11350" citStr="Gfeller et al. (2005)" startWordPosition="1779" endWordPosition="1782"> can be quite easily extracted from resources structured on words and meanings, such as dictionaries, by taking advantage not only of textual patterns, more frequent in those resources (e.g. tamb´em conhecido por/como, o mesmo que, for Portuguese), but also of definitions consisting of only one word or a enumeration, which typically contain synonyms of the defined word. So, as it is possible to create a lexical network from a set of relational triples (a R b), a synonymy network can be created out of synonymy instances (a SYNONYM OF b). Since these networks tend to have a clustered structure, Gfeller et al. (2005) propose a clustering procedure to improve their utility. 3 Research Goals The research presented here is in the scope of a project whose final goal is to create a lexical ontology for Portuguese by automatic means. Although there are clear advantages of using resources already structured on words and meanings, dictionaries are static resources which contain limited knowledge and are not always available for this kind of research. On the other hand, there is much text available on the most different subjects, but free text has few boundaries, leading to more ambiguity and parsing issues. There</context>
<context position="14718" citStr="Gfeller et al. (2005)" startWordPosition="2332" endWordPosition="2335">ust be performed to obtain the synsets. Looking at some of the works referred in Section 2.2, ours is different because it does not require a conceptual base such as WordNet. Also, it integrates knowledge from different sources and tries to disambiguate each word using only knowledge already extracted and not the context where the word occurs. 4.1 Clustering for a thesaurus This stage was originally defined after looking at disconnected pieces of a synonymy network extracted from a dictionary, which had a clustered structure apparently suitable for identifying synsets. This is also noticed by Gfeller et al. (2005) who have used the Markov Clustering algorithm (MCL) (van Dongen, 2000) to find clusters in a synonymy network. Therefore, since MCL had already been applied to problems very close to ours (e.g. (Gfeller et al., 2005), (Dorow, 2006)), it seemed to suit our purpose – it would not only organise a term-based network into a thesaurus, but, if a network extracted from several resources is used, clustering would homogenise the synonymy representation. MCL finds clusters by simulating random walks within a graph by alternately computing random walks of higher length, and increasing the probabilities </context>
</contexts>
<marker>Gfeller, Chappelier, Rios, 2005</marker>
<rawString>David Gfeller, Jean-C´edric Chappelier, and Paulo De Los Rios. 2005. Synonym Dictionary Improvement through Markov Clustering and Clustering Stability. In Proc. of International Symposium on Applied Stochastic Models and Data Analysis (ASMDA), pages 106–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="5273" citStr="Girju et al., 2006" startWordPosition="812" endWordPosition="815">her languages, but if this might be suitable for several applications, a problem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manne</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Gonc¸alo Oliveira</author>
<author>Diana Santos</author>
<author>Paulo Gomes</author>
</authors>
<title>Relations extracted from a portuguese dictionary: results and first evaluation.</title>
<date>2009</date>
<booktitle>In Local Proc. 14th Portuguese Conf. on Artificial Intelligence (EPIA).</booktitle>
<contexts>
<context position="5931" citStr="Oliveira et al., 2009" startWordPosition="913" endWordPosition="916">n and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 2009) is a lexical network consisting of triples denoting semantic relations between words found in a dictionary. Other Portuguese lexical ontologies, created by different means, are reviewed and compared in (Santos et al., 2009) and (Teixeira et al., 2010). Besides corpora and dictionary processing, in the later years, semi-structured collaborative resources, such as Wikipedia or Wiktionary, have proved to be important sources of lexico-semantic knowledge and have thus been receiving more and more attention by the community (see for instance (Zesch et al., 2008) (Navarro et al., 2009)). 2.2 Other </context>
<context position="7288" citStr="Oliveira et al., 2009" startWordPosition="1135" endWordPosition="1138">ION term2, indicates that a possible meaning of term] is related to a possible meaning of term2 by means of a RELATION. Although it is possible to create a lexical network from the latter, this kind of networks is often impractical for computational applications, such as the ones that deal with inference. For instance, applying a simple transitive rule, a SYNONYM OF b n b SYNONYM OF c —* a SYNONYM OF c over a set of term-based triples can lead to serious inconsistencies. A curious example in Portuguese, where synonymy between two completely opposite words is inferred, is reported in (Gonc¸alo Oliveira et al., 2009): queda SYNONYM OF ruina n queda SYNONYM OF habilidade —* ruina SYNONYM OF habilidade. This happens because natural language is ambiguous, especially when dealing with broad-coverage knowledge. In the given example, queda can either mean downfall or aptitude, while ruina means ruin, destruction, downfall. A possible way to deal with ambiguity is to adopt a wordnet-like structure, where concepts are described by synsets and ambiguous words are included in a synset for each of their meanings. Semantic relations can thereby be unambiguously established between two synsets, and concepts, even thou</context>
<context position="22564" citStr="Oliveira et al., 2009" startWordPosition="3825" endWordPosition="3828">y term related to b 5Before applying these rules it is necessary to make sure that all relations are represented only in one way, otherwise they might not work. For instance, if the decision is to represent part-of triples in the form part PART OF whole, triples whole HAS PART part must be reversed. Furthermore, these rules assume that hypernymy relations are all represented hypernym HYPERNYM OF hyponym and not hyponym HYPONYM OF hypernym. 14 5 Experimentation In this section we report experimental results obtained after applying our procedure to part of the lexical network of PAPEL (Gonc¸alo Oliveira et al., 2009). The clustering procedure was first ran over PAPEL’s noun synonymy network in order to obtain the synsets which were later merged with two manually created thesaurus. Finally, hypernymof, member-of and part-of triples of PAPEL were mapped to the thesaurus by assigning a synset to each term argument. 5.1 Resources used For experimentation purposes, freely available lexical resources for Portuguese were used. First, the last version of PAPEL, 2.0, a lexical network for Portuguese created automatically from a dictionary, as referred in Section 2. PAPEL 2.0 contains approximately 100,000 words, i</context>
<context position="30875" citStr="Oliveira et al., 2009" startWordPosition="5195" endWordPosition="5198">pts: (a) insobriety, not following all the rules, heedless of the consequences and, (b) surplus. Both of these concepts can be referred to as an excess (excesso). 6.2 Automatic validation of triples The automatic validation of the triples attached to our wordnet consisted of using Google web search engine to look for evidence on their truth. This procedure started by removing terms whose occurrences in Google were less than 5,000. Synsets that became empty were not considered and, from the rest, a sample was selected for each one of the three types of relation. Following the idea in (Gonc¸alo Oliveira et al., 2009), a set of natural language generic patterns, indicative of each relation, was defined having in mind their input to Google8. Then, for each triple (A R B), the patterns were used to search for ev8Hypernymy patterns included: [hypo] e´ um|uma (tipo|forma|variedade|...)* de [hyper], [hypo] e outros|outras [hyper] or [hyper] tais como [hypo]. Patterns for part-of and member-of were the same because these relations can be expressed in very similar ways, and included: [part/member] e´ (parte|membro|porc¸˜ao) do|da [whole/group], [part/member] (faz parte)* do|da [whole/group] or [whole/group] e´ um</context>
</contexts>
<marker>Oliveira, Santos, Gomes, 2009</marker>
<rawString>Hugo Gonc¸alo Oliveira, Diana Santos, and Paulo Gomes. 2009. Relations extracted from a portuguese dictionary: results and first evaluation. In Local Proc. 14th Portuguese Conf. on Artificial Intelligence (EPIA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas R Gruber</author>
</authors>
<title>A translation approach to portable ontology specifications.</title>
<date>1993</date>
<journal>Knowledge Acquisition,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="3376" citStr="Gruber, 1993" startWordPosition="517" endWordPosition="518">ed triples were validated with the help of a web search engine. 2 Context Our ultimate goal is the automatic construction of a broad-coverage structure of words according to their meanings, also known as a lexical ontology, the first subject of this section. We proceed with a brief overview on work concerned with moving from term-based knowledge to synset-based knowledge, often called ontologising. 2.1 Lexical Ontologies Despite some terminological issues, lexical ontologies can be seen both as a lexicon and as an ontology (Hirst, 2004) and are significantly different from classic ontologies (Gruber, 1993). They are not based on a specific domain and are intended to provide knowledge structured on lexical items (words) of a language by relating them according to their meaning. Moreover, the main goal of a lexical ontology is to assemble lexical and semantic information, instead of storing common-sense knowledge (Wandmacher et al., 2007). 10 Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 10–18, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics Princeton WordNet (Fellbaum, 1998) is the most representative lexi</context>
</contexts>
<marker>Gruber, 1993</marker>
<rawString>Thomas R. Gruber. 1993. A translation approach to portable ontology specifications. Knowledge Acquisition, 5(2):199–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Dan I Moldovan</author>
</authors>
<title>Enriching the wordnet taxonomy with contextual knowledge acquired from text. In Natural language processing and knowledge representation: language for knowledge and knowledge for language,</title>
<date>2000</date>
<pages>301--333</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="9473" citStr="Harabagiu and Moldovan, 2000" startWordPosition="1479" endWordPosition="1482">ect hyponym synsets. Two other methods for ontologising term-based triples are presented by Pantel and Pennacchiotti (2008). One assumes that terms with the same relation to a fixed term are more plausible to describe the correct sense, so, to select the correct synset, it exploits triples of the same type sharing one argument. The other method, which seems to perform better, selects suitable synsets using generalisation through hypernymy links in WordNet. There are other works where WordNet is enriched, for instance with information in its glosses, domain knowledge extracted from text (e.g. (Harabagiu and Moldovan, 2000) (Navigli et al., 2004)) or wikipedia entries (e.g. (RuizCasado et al., 2005)), thus requiring a disambiguation phase where terms are assigned to synsets. In the construction of a lexical ontology, synonymy plays an important role because it defines the conceptual base of the knowledge to be represented. One of the reasons for using WordNet synsets as a starting point for its representation is that, while it is quite straightforward to define a set of textual patterns indicative of several semantic relations between words (e.g. hyponymy, part-of, cause) with relatively good quality, the same d</context>
</contexts>
<marker>Harabagiu, Moldovan, 2000</marker>
<rawString>Sanda M. Harabagiu and Dan I. Moldovan. 2000. Enriching the wordnet taxonomy with contextual knowledge acquired from text. In Natural language processing and knowledge representation: language for knowledge and knowledge for language, pages 301–333. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. 14th Conf. on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1446" citStr="Hearst (1992)" startWordPosition="225" endWordPosition="226">, 1998), but this kind of resources is most of the times handcrafted, which implies much timeconsuming human effort. So, the automatic construction of such resources arises as an alternative, providing less intensive labour, easier maintenance and allowing for higher coverage, as a tradeoff for lower, but still acceptable, precision. This paper is written in the scope of a project where several textual resources are being exploited for the construction of a lexical ontology for Portuguese. We have already made a first approach on the extraction of relational triples from text, where, likewise Hearst (1992), we take advantage of textual patterns indicating semantic relations. However, the extracted triples are held between two terms, which is not enough to build a lexical ontology capable of dealing with ambiguity. Therefore, we present our current approach towards the automatic integration of lexico-semantic knowledge into a single independent lexical ontology, which will be structured on concepts and *supported by FCT scholarship SFRH/BD/44955/2008. Paulo Gomes CISUC, University of Coimbra Portugal pgomes@dei.uc.pt adopt a model close to WordNet’s. The task of establishing synsets and mapping </context>
<context position="5252" citStr="Hearst, 1992" startWordPosition="810" endWordPosition="811"> wordnets in other languages, but if this might be suitable for several applications, a problem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. h</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. 14th Conf. on Computational Linguistics, pages 539–545, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Ontology and the lexicon.</title>
<date>2004</date>
<booktitle>Handbook on Ontologies, International Handbooks on Information Systems,</booktitle>
<pages>209--230</pages>
<editor>In Steffen Staab and Rudi Studer, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="3305" citStr="Hirst, 2004" startWordPosition="508" endWordPosition="509">ly from a dictionary. Synsets were validated manually while the attached triples were validated with the help of a web search engine. 2 Context Our ultimate goal is the automatic construction of a broad-coverage structure of words according to their meanings, also known as a lexical ontology, the first subject of this section. We proceed with a brief overview on work concerned with moving from term-based knowledge to synset-based knowledge, often called ontologising. 2.1 Lexical Ontologies Despite some terminological issues, lexical ontologies can be seen both as a lexicon and as an ontology (Hirst, 2004) and are significantly different from classic ontologies (Gruber, 1993). They are not based on a specific domain and are intended to provide knowledge structured on lexical items (words) of a language by relating them according to their meaning. Moreover, the main goal of a lexical ontology is to assemble lexical and semantic information, instead of storing common-sense knowledge (Wandmacher et al., 2007). 10 Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 10–18, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguis</context>
<context position="4964" citStr="Hirst, 2004" startWordPosition="765" endWordPosition="766">Net project (Vossen, 1997), or WordNet.PT (Marrafa, 2002), for Portuguese. However, the creation of a wordnet, as well as the creation of most ontologies, is typically manual and involves much human effort. Some authors (de Melo and Weikum, 2008) propose translating Princeton WordNet to wordnets in other languages, but if this might be suitable for several applications, a problem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindN</context>
<context position="8204" citStr="Hirst, 2004" startWordPosition="1280" endWordPosition="1281">l. A possible way to deal with ambiguity is to adopt a wordnet-like structure, where concepts are described by synsets and ambiguous words are included in a synset for each of their meanings. Semantic relations can thereby be unambiguously established between two synsets, and concepts, even though described by groups of words, bring together natural language and knowledge engineering in a suitable representation, for instance, for the Semantic Web (Berners-Lee et al., 2001). Of course that, from a linguistic point of view, word senses are complex and overlapping structures (Kilgarriff, 1997) (Hirst, 2004). So, despite word sense divisions in dictionaries and ontologies being most of the times artificial, this trade-off is needed in order to increase the usability of broadcoverage computational lexical resources. In order to move from term-based triples to an ontology, Soderland and Mandhani (2007) describe a procedure where, besides other stages, terms in triples are assigned to WordNet synsets. Starting with all the synsets containing a term in 11 a triple, the term is assigned to the synset with higher similarity to the contexts from where the triple was extracted, computed based on the term</context>
</contexts>
<marker>Hirst, 2004</marker>
<rawString>Graeme Hirst. 2004. Ontology and the lexicon. In Steffen Staab and Rudi Studer, editors, Handbook on Ontologies, International Handbooks on Information Systems, pages 209–230. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>I don’t believe in word senses”.</title>
<date>1997</date>
<journal>Computing and the Humanities,</journal>
<volume>31</volume>
<issue>2</issue>
<pages>113</pages>
<contexts>
<context position="8190" citStr="Kilgarriff, 1997" startWordPosition="1278" endWordPosition="1279">estruction, downfall. A possible way to deal with ambiguity is to adopt a wordnet-like structure, where concepts are described by synsets and ambiguous words are included in a synset for each of their meanings. Semantic relations can thereby be unambiguously established between two synsets, and concepts, even though described by groups of words, bring together natural language and knowledge engineering in a suitable representation, for instance, for the Semantic Web (Berners-Lee et al., 2001). Of course that, from a linguistic point of view, word senses are complex and overlapping structures (Kilgarriff, 1997) (Hirst, 2004). So, despite word sense divisions in dictionaries and ontologies being most of the times artificial, this trade-off is needed in order to increase the usability of broadcoverage computational lexical resources. In order to move from term-based triples to an ontology, Soderland and Mandhani (2007) describe a procedure where, besides other stages, terms in triples are assigned to WordNet synsets. Starting with all the synsets containing a term in 11 a triple, the term is assigned to the synset with higher similarity to the contexts from where the triple was extracted, computed bas</context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Adam Kilgarriff. 1997. ”I don’t believe in word senses”. Computing and the Humanities, 31(2):91– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proc. 19th Intl. Conf. on Computational Linguistics (COLING),</booktitle>
<pages>577--583</pages>
<contexts>
<context position="5328" citStr="Lin and Pantel, 2002" startWordPosition="822" endWordPosition="825">ral applications, a problem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 20</context>
<context position="10486" citStr="Lin and Pantel, 2002" startWordPosition="1638" endWordPosition="1641">n is that, while it is quite straightforward to define a set of textual patterns indicative of several semantic relations between words (e.g. hyponymy, part-of, cause) with relatively good quality, the same does not apply for synonymy. In opposition to other kinds of relation, synonymous words, despite typically sharing similar neighbourhoods, may not cooccur frequently in unstructured text, especially in the same sentence (Dorow, 2006), leading to few indicative textual patterns. Therefore, most of the works on synonymy extraction from corpora rely on statistics or graph-based methods (e.g. (Lin and Pantel, 2002) (Turney, 2001) (Dorow, 2006)). Nevertheless, methods for synonymy identification based on co-occurrences (e.g. (Turney, 2001)) are more prone to identify similar words or near synonyms than real synonyms. On the other hand, synonymy instances can be quite easily extracted from resources structured on words and meanings, such as dictionaries, by taking advantage not only of textual patterns, more frequent in those resources (e.g. tamb´em conhecido por/como, o mesmo que, for Portuguese), but also of definitions consisting of only one word or a enumeration, which typically contain synonyms of th</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>Dekang Lin and Patrick Pantel. 2002. Concept discovery from text. In Proc. 19th Intl. Conf. on Computational Linguistics (COLING), pages 577–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Palmira Marrafa</author>
</authors>
<title>Portuguese Wordnet: general architecture and internal semantic relations.</title>
<date>2002</date>
<pages>18--131</pages>
<location>DELTA,</location>
<contexts>
<context position="4409" citStr="Marrafa, 2002" startWordPosition="675" endWordPosition="676">Processing, ACL 2010, pages 10–18, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics Princeton WordNet (Fellbaum, 1998) is the most representative lexico-semantic resource for English and also the most accepted model of a lexical ontology. It is structured around groups of synonymous words (synsets), which describe concepts, and connections, denoting semantic relations between those groups. The success of WordNet led to the adoption of its model by lexical resources in different languages, such as the ones in the EuroWordNet project (Vossen, 1997), or WordNet.PT (Marrafa, 2002), for Portuguese. However, the creation of a wordnet, as well as the creation of most ontologies, is typically manual and involves much human effort. Some authors (de Melo and Weikum, 2008) propose translating Princeton WordNet to wordnets in other languages, but if this might be suitable for several applications, a problem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract l</context>
</contexts>
<marker>Marrafa, 2002</marker>
<rawString>Palmira Marrafa. 2002. Portuguese Wordnet: general architecture and internal semantic relations. DELTA, 18:131–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Navarro</author>
<author>Franck Sajous</author>
<author>Bruno Gaume</author>
<author>Laurent Pr´evot</author>
<author>ShuKai Hsieh</author>
<author>Tzu Y Kuo</author>
<author>Pierre Magistry</author>
<author>Chu R Huang</author>
</authors>
<title>Wiktionary and nlp: Improving synonymy networks.</title>
<date>2009</date>
<booktitle>In Proc. Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources,</booktitle>
<pages>pages</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<marker>Navarro, Sajous, Gaume, Pr´evot, Hsieh, Kuo, Magistry, Huang, 2009</marker>
<rawString>Emmanuel Navarro, Franck Sajous, Bruno Gaume, Laurent Pr´evot, ShuKai Hsieh, Tzu Y. Kuo, Pierre Magistry, and Chu R. Huang. 2009. Wiktionary and nlp: Improving synonymy networks. In Proc. Workshop on The People’s Web Meets NLP: Collaboratively Constructed Semantic Resources, pages 19–27, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Alessandro Cucchiarelli</author>
<author>Francesca Neri</author>
</authors>
<title>Extending and enriching wordnet with ontolearn.</title>
<date>2004</date>
<booktitle>In Proc. 2nd Global WordNet Conf. (GWC),</booktitle>
<pages>279--284</pages>
<institution>Republic. Masaryk University.</institution>
<location>Brno, Czech</location>
<contexts>
<context position="9496" citStr="Navigli et al., 2004" startWordPosition="1483" endWordPosition="1486">methods for ontologising term-based triples are presented by Pantel and Pennacchiotti (2008). One assumes that terms with the same relation to a fixed term are more plausible to describe the correct sense, so, to select the correct synset, it exploits triples of the same type sharing one argument. The other method, which seems to perform better, selects suitable synsets using generalisation through hypernymy links in WordNet. There are other works where WordNet is enriched, for instance with information in its glosses, domain knowledge extracted from text (e.g. (Harabagiu and Moldovan, 2000) (Navigli et al., 2004)) or wikipedia entries (e.g. (RuizCasado et al., 2005)), thus requiring a disambiguation phase where terms are assigned to synsets. In the construction of a lexical ontology, synonymy plays an important role because it defines the conceptual base of the knowledge to be represented. One of the reasons for using WordNet synsets as a starting point for its representation is that, while it is quite straightforward to define a set of textual patterns indicative of several semantic relations between words (e.g. hyponymy, part-of, cause) with relatively good quality, the same does not apply for synon</context>
</contexts>
<marker>Navigli, Velardi, Cucchiarelli, Neri, 2004</marker>
<rawString>Roberto Navigli, Paola Velardi, Alessandro Cucchiarelli, and Francesca Neri. 2004. Extending and enriching wordnet with ontolearn. In Proc. 2nd Global WordNet Conf. (GWC), pages 279–284, Brno, Czech Republic. Masaryk University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically harvesting and ontologizing semantic relations.</title>
<date>2008</date>
<booktitle>Ontology Learning and Population: Bridging the Gap between Text and Knowledge.</booktitle>
<editor>In Paul Buitelaar and Phillip Cimmiano, editors,</editor>
<publisher>IOS Press.</publisher>
<contexts>
<context position="8967" citStr="Pantel and Pennacchiotti (2008)" startWordPosition="1398" endWordPosition="1401">der to increase the usability of broadcoverage computational lexical resources. In order to move from term-based triples to an ontology, Soderland and Mandhani (2007) describe a procedure where, besides other stages, terms in triples are assigned to WordNet synsets. Starting with all the synsets containing a term in 11 a triple, the term is assigned to the synset with higher similarity to the contexts from where the triple was extracted, computed based on the terms in the synset, sibling synsets and direct hyponym synsets. Two other methods for ontologising term-based triples are presented by Pantel and Pennacchiotti (2008). One assumes that terms with the same relation to a fixed term are more plausible to describe the correct sense, so, to select the correct synset, it exploits triples of the same type sharing one argument. The other method, which seems to perform better, selects suitable synsets using generalisation through hypernymy links in WordNet. There are other works where WordNet is enriched, for instance with information in its glosses, domain knowledge extracted from text (e.g. (Harabagiu and Moldovan, 2000) (Navigli et al., 2004)) or wikipedia entries (e.g. (RuizCasado et al., 2005)), thus requiring</context>
<context position="32078" citStr="Pantel and Pennacchiotti (2008)" startWordPosition="5389" endWordPosition="5392">roup] or [whole/group] e´ um (grupo|conjunto|...) de [part/member]. 16 Sample Correct Incorrect N/A Agreement CLIP 519 sets 65.8% 31.7% 2.5% 76.1% CLIP’ 310 sets 81.1% 16.9% 2.0% 84.2% TOP 480 sets 83.2% 15.8% 1.0% 82.3% TOP’ 448 sets 86.8% 12.3% 0.9% 83.0% Table 3: Results of manual synset validation. idence on each combination of terms a E A and b E B connected by a pattern indicative of R. The triple validation score was then calculated by expression 1, where found(A, B, R) = 1 if evidence is found for the triple or 0 otherwise. Table 4 shows the results obtained for each validated sample. Pantel and Pennacchiotti (2008) perform a similar task and present precision results for part-of (40.7%-57.4%) and causation (40.0%- 45%) relations. It is however not possible to make a straight comparison. For their experimentation, they selected only correct term-based triples extracted from text and their results were manually validated by human judges. On the other hand, we have used term-based triples extracted automatically from a dictionary, with high but not 100% precision, from where we did not choose only the correct ones, and we have used synsets obtained from our clustering procedure which, once again, have lowe</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2008</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2008. Automatically harvesting and ontologizing semantic relations. In Paul Buitelaar and Phillip Cimmiano, editors, Ontology Learning and Population: Bridging the Gap between Text and Knowledge. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Raman</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Towards automatic evaluation of wordnet synsets.</title>
<date>2008</date>
<booktitle>In Proc. 4th Global WordNet Conf. (GWC),</booktitle>
<pages>360--374</pages>
<institution>Szeged, Hungary. University of Szeged.</institution>
<contexts>
<context position="27565" citStr="Raman and Bhattacharyya, 2008" startWordPosition="4644" endWordPosition="4648"> times performed by one of two means: (i) manual evaluation of a representative subset of the results; (ii) automatic comparison with a gold standard. However, while for English most researchers use Princeton WordNet as a gold standard, for other languages it is difficult to find suitable and freely available consensual resources. Considering Portuguese, as we have said earlier, TeP and OT are effectively two manually created thesaurus but, since they are more complementary than overlapping to PAPEL, we thought it would be better to use them to enrich our resource. There is actually a report (Raman and Bhattacharyya, 2008) with an automatic evaluation of synsets, but we decided no to follow it because this evaluation is heavily based on a dictionary and we do not have unrestricted access to a full and updated dictionary of Portuguese and also, indirectly by PAPEL, a dictionary was one of our main sources of information. Therefore, our choice relied on manual validation of the synsets of CLIP and TOP. Furthermore, synset-based triples were validated in an alternative automatic way using a web search engine. 6.1 Manual validation of synsets Ten reviewers took part in the validation of ten random samples with appr</context>
</contexts>
<marker>Raman, Bhattacharyya, 2008</marker>
<rawString>J. Raman and Pushpak Bhattacharyya. 2008. Towards automatic evaluation of wordnet synsets. In Proc. 4th Global WordNet Conf. (GWC), pages 360–374, Szeged, Hungary. University of Szeged.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Richardson</author>
<author>William B Dolan</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Mindnet: Acquiring and structuring semantic information from text.</title>
<date>1998</date>
<booktitle>In Proc. 17th Intl. Conf. on Computational Linguistics (COLING),</booktitle>
<pages>1098--1102</pages>
<contexts>
<context position="5592" citStr="Richardson et al., 1998" startWordPosition="861" endWordPosition="864">other popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 2009) is a lexical network consisting of triples denoting semantic relations between words found in a dictionary. Other Portuguese lexical ontologies, created by different means, are reviewed and compared in (Santos et al., 2009) and (Teixeira et al., 2010). Besides</context>
</contexts>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Stephen D. Richardson, William B. Dolan, and Lucy Vanderwende. 1998. Mindnet: Acquiring and structuring semantic information from text. In Proc. 17th Intl. Conf. on Computational Linguistics (COLING), pages 1098–1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Ruiz-Casado</author>
<author>Enrique Alfonseca</author>
<author>Pablo Castells</author>
</authors>
<title>Automatic assignment of wikipedia encyclopedic entries to wordnet synsets.</title>
<date>2005</date>
<booktitle>In Proc. Advances in Web Intelligence Third Intl. Atlantic Web Intelligence Conf. (AWIC),</booktitle>
<pages>380--386</pages>
<publisher>Springer.</publisher>
<marker>Ruiz-Casado, Alfonseca, Castells, 2005</marker>
<rawString>Maria Ruiz-Casado, Enrique Alfonseca, and Pablo Castells. 2005. Automatic assignment of wikipedia encyclopedic entries to wordnet synsets. In Proc. Advances in Web Intelligence Third Intl. Atlantic Web Intelligence Conf. (AWIC), pages 380–386. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Santos</author>
<author>Anabela Barreiro</author>
<author>Luis Costa</author>
<author>Cl´audia Freitas</author>
<author>Paulo Gomes</author>
<author>Hugo Gonc¸alo Oliveira</author>
<author>Jos´e Carlos Medeiros</author>
<author>Ros´ario Silva</author>
</authors>
<title>O papel das relac¸˜oes semˆanticas em portuguˆes:</title>
<date>2009</date>
<booktitle>Comparando o TeP, o MWN.PT e o PAPEL. In Actas do XXV Encontro Nacional da Associac¸˜ao Portuguesa de Lingu´ıstica (APL). forthcomming.</booktitle>
<contexts>
<context position="6155" citStr="Santos et al., 2009" startWordPosition="947" endWordPosition="950">xicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 2009) is a lexical network consisting of triples denoting semantic relations between words found in a dictionary. Other Portuguese lexical ontologies, created by different means, are reviewed and compared in (Santos et al., 2009) and (Teixeira et al., 2010). Besides corpora and dictionary processing, in the later years, semi-structured collaborative resources, such as Wikipedia or Wiktionary, have proved to be important sources of lexico-semantic knowledge and have thus been receiving more and more attention by the community (see for instance (Zesch et al., 2008) (Navarro et al., 2009)). 2.2 Other Relevant Work Most of the methods proposed to extract relations from text have term-based triples as output. Such a triple, term] RELATION term2, indicates that a possible meaning of term] is related to a possible meaning of</context>
<context position="23915" citStr="Santos et al., 2009" startWordPosition="4032" endWordPosition="4035">tic relations. In order to enrich the thesaurus obtained from PAPEL, TeP (Dias-Da-Silva and de Moraes, 2003) and OpenThesaurus.PT6 (OT), were used. Both of them are manually created thesaurus, for Brazilian Portuguese and European Portuguese respectively, modelled after Princeton WordNet (Fellbaum, 1998) and thus containing synsets. Besides being the only freely available thesaurus for Portuguese we know about, TeP and OT were used together with PAPEL because, despite representing the same kind of knowledge, they are mostly complementary, which is also observed by (Teixeira et al., 2010) and (Santos et al., 2009). Note that, for experimentation purposes, we have only used the parts of these resources concerning nouns. 5.2 Thesaurus creation The first step for applying the clustering procedure is to create PAPEL’s synonymy network, which is established by its synonymy instances, a SYNONYM OF b. After splitting the network into independent disconnected sub-networks, we noticed that it was composed by a huge subnetwork, with more than 16,000 nodes, and several very small networks. If ambiguity was not resolved, this would suggest that all the 16,000 words had the same meaning, which is not true. 6http://</context>
</contexts>
<marker>Santos, Barreiro, Costa, Freitas, Gomes, Oliveira, Medeiros, Silva, 2009</marker>
<rawString>Diana Santos, Anabela Barreiro, Luis Costa, Cl´audia Freitas, Paulo Gomes, Hugo Gonc¸alo Oliveira, Jos´e Carlos Medeiros, and Ros´ario Silva. 2009. O papel das relac¸˜oes semˆanticas em portuguˆes: Comparando o TeP, o MWN.PT e o PAPEL. In Actas do XXV Encontro Nacional da Associac¸˜ao Portuguesa de Lingu´ıstica (APL). forthcomming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Bhushan Mandhani</author>
</authors>
<title>Moving from textual relations to ontologized relations.</title>
<date>2007</date>
<booktitle>In Proc. AAAI Spring Symposium on Machine Reading.</booktitle>
<contexts>
<context position="8502" citStr="Soderland and Mandhani (2007)" startWordPosition="1323" endWordPosition="1326">epts, even though described by groups of words, bring together natural language and knowledge engineering in a suitable representation, for instance, for the Semantic Web (Berners-Lee et al., 2001). Of course that, from a linguistic point of view, word senses are complex and overlapping structures (Kilgarriff, 1997) (Hirst, 2004). So, despite word sense divisions in dictionaries and ontologies being most of the times artificial, this trade-off is needed in order to increase the usability of broadcoverage computational lexical resources. In order to move from term-based triples to an ontology, Soderland and Mandhani (2007) describe a procedure where, besides other stages, terms in triples are assigned to WordNet synsets. Starting with all the synsets containing a term in 11 a triple, the term is assigned to the synset with higher similarity to the contexts from where the triple was extracted, computed based on the terms in the synset, sibling synsets and direct hyponym synsets. Two other methods for ontologising term-based triples are presented by Pantel and Pennacchiotti (2008). One assumes that terms with the same relation to a fixed term are more plausible to describe the correct sense, so, to select the cor</context>
</contexts>
<marker>Soderland, Mandhani, 2007</marker>
<rawString>Stephen Soderland and Bhushan Mandhani. 2007. Moving from textual relations to ontologized relations. In Proc. AAAI Spring Symposium on Machine Reading.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Teixeira</author>
<author>Luis Sarmento</author>
<author>Eug´enio C Oliveira</author>
</authors>
<title>Comparing verb synonym resources for portuguese.</title>
<date>2010</date>
<booktitle>In Computational Processing of the Portuguese Language, 9th Intl. Conference, Proc. (PROPOR),</booktitle>
<pages>100--109</pages>
<contexts>
<context position="6183" citStr="Teixeira et al., 2010" startWordPosition="952" endWordPosition="955">ndNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 2009) is a lexical network consisting of triples denoting semantic relations between words found in a dictionary. Other Portuguese lexical ontologies, created by different means, are reviewed and compared in (Santos et al., 2009) and (Teixeira et al., 2010). Besides corpora and dictionary processing, in the later years, semi-structured collaborative resources, such as Wikipedia or Wiktionary, have proved to be important sources of lexico-semantic knowledge and have thus been receiving more and more attention by the community (see for instance (Zesch et al., 2008) (Navarro et al., 2009)). 2.2 Other Relevant Work Most of the methods proposed to extract relations from text have term-based triples as output. Such a triple, term] RELATION term2, indicates that a possible meaning of term] is related to a possible meaning of term2 by means of a RELATIO</context>
<context position="23889" citStr="Teixeira et al., 2010" startWordPosition="4027" endWordPosition="4030"> by different types of semantic relations. In order to enrich the thesaurus obtained from PAPEL, TeP (Dias-Da-Silva and de Moraes, 2003) and OpenThesaurus.PT6 (OT), were used. Both of them are manually created thesaurus, for Brazilian Portuguese and European Portuguese respectively, modelled after Princeton WordNet (Fellbaum, 1998) and thus containing synsets. Besides being the only freely available thesaurus for Portuguese we know about, TeP and OT were used together with PAPEL because, despite representing the same kind of knowledge, they are mostly complementary, which is also observed by (Teixeira et al., 2010) and (Santos et al., 2009). Note that, for experimentation purposes, we have only used the parts of these resources concerning nouns. 5.2 Thesaurus creation The first step for applying the clustering procedure is to create PAPEL’s synonymy network, which is established by its synonymy instances, a SYNONYM OF b. After splitting the network into independent disconnected sub-networks, we noticed that it was composed by a huge subnetwork, with more than 16,000 nodes, and several very small networks. If ambiguity was not resolved, this would suggest that all the 16,000 words had the same meaning, w</context>
</contexts>
<marker>Teixeira, Sarmento, Oliveira, 2010</marker>
<rawString>Jorge Teixeira, Luis Sarmento, and Eug´enio C. Oliveira. 2010. Comparing verb synonym resources for portuguese. In Computational Processing of the Portuguese Language, 9th Intl. Conference, Proc. (PROPOR), pages 100–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: PMI–IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proc. 12th European Conf. on Machine Learning (ECML),</booktitle>
<volume>2167</volume>
<pages>491--502</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5343" citStr="Turney, 2001" startWordPosition="826" endWordPosition="827">blem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popular alternative is to extract lexicosemantic knowledge and learn lexical ontologies from text. Research on this field is not new and varied methods have been proposed to achieve different steps of this task including the extraction of semantic relations (e.g. (Hearst, 1992) (Girju et al., 2006)) or sets of similar words (e.g. (Lin and Pantel, 2002) (Turney, 2001)). Whereas the aforementioned works are based on unstructured text, dictionaries started earlier (Calzolari et al., 1973) to be seen as an attractive target for the automatic acquisition of lexicosemantic knowledge. MindNet (Richardson et al., 1998) is both an extraction methodology and a lexical ontology different from a wordnet, since it was created automatically from a dictionary and its structure is based on such resources. Nevertheless, it still connects sense records with semantic relations (e.g. hyponymy, cause, manner). For Portuguese, PAPEL (Gonc¸alo Oliveira et al., 2009) is a lexica</context>
<context position="10501" citStr="Turney, 2001" startWordPosition="1642" endWordPosition="1643">quite straightforward to define a set of textual patterns indicative of several semantic relations between words (e.g. hyponymy, part-of, cause) with relatively good quality, the same does not apply for synonymy. In opposition to other kinds of relation, synonymous words, despite typically sharing similar neighbourhoods, may not cooccur frequently in unstructured text, especially in the same sentence (Dorow, 2006), leading to few indicative textual patterns. Therefore, most of the works on synonymy extraction from corpora rely on statistics or graph-based methods (e.g. (Lin and Pantel, 2002) (Turney, 2001) (Dorow, 2006)). Nevertheless, methods for synonymy identification based on co-occurrences (e.g. (Turney, 2001)) are more prone to identify similar words or near synonyms than real synonyms. On the other hand, synonymy instances can be quite easily extracted from resources structured on words and meanings, such as dictionaries, by taking advantage not only of textual patterns, more frequent in those resources (e.g. tamb´em conhecido por/como, o mesmo que, for Portuguese), but also of definitions consisting of only one word or a enumeration, which typically contain synonyms of the defined word.</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: PMI–IR versus LSA on TOEFL. In Proc. 12th European Conf. on Machine Learning (ECML), volume 2167, pages 491–502. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M van Dongen</author>
</authors>
<title>Graph Clustering by Flow Simulation.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Utrecht.</institution>
<marker>van Dongen, 2000</marker>
<rawString>S. M. van Dongen. 2000. Graph Clustering by Flow Simulation. Ph.D. thesis, University of Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen</author>
</authors>
<title>Eurowordnet: a multilingual database for information retrieval.</title>
<date>1997</date>
<booktitle>In Proc. DELOS workshop on Cross-Language Information Retrieval,</booktitle>
<location>Zurich.</location>
<contexts>
<context position="4378" citStr="Vossen, 1997" startWordPosition="671" endWordPosition="672"> Methods for Natural Language Processing, ACL 2010, pages 10–18, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics Princeton WordNet (Fellbaum, 1998) is the most representative lexico-semantic resource for English and also the most accepted model of a lexical ontology. It is structured around groups of synonymous words (synsets), which describe concepts, and connections, denoting semantic relations between those groups. The success of WordNet led to the adoption of its model by lexical resources in different languages, such as the ones in the EuroWordNet project (Vossen, 1997), or WordNet.PT (Marrafa, 2002), for Portuguese. However, the creation of a wordnet, as well as the creation of most ontologies, is typically manual and involves much human effort. Some authors (de Melo and Weikum, 2008) propose translating Princeton WordNet to wordnets in other languages, but if this might be suitable for several applications, a problem arises because different languages represent different socio-cultural realities, do not cover exactly the same part of the lexicon and, even where they seem to be common, several concepts are lexicalised differently (Hirst, 2004). Another popu</context>
</contexts>
<marker>Vossen, 1997</marker>
<rawString>Piek Vossen. 1997. Eurowordnet: a multilingual database for information retrieval. In Proc. DELOS workshop on Cross-Language Information Retrieval, Zurich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tonio Wandmacher</author>
<author>Ekaterina Ovchinnikova</author>
<author>Ulf Krumnack</author>
<author>Henrik Dittmann</author>
</authors>
<title>Extraction, evaluation and integration of lexical-semantic relations for the automated construction of a lexical ontology.</title>
<date>2007</date>
<booktitle>In Third Australasian Ontology Workshop (AOW),</booktitle>
<volume>85</volume>
<pages>61--69</pages>
<publisher>ACS.</publisher>
<location>Gold Coast, Australia.</location>
<contexts>
<context position="3713" citStr="Wandmacher et al., 2007" startWordPosition="569" endWordPosition="572"> term-based knowledge to synset-based knowledge, often called ontologising. 2.1 Lexical Ontologies Despite some terminological issues, lexical ontologies can be seen both as a lexicon and as an ontology (Hirst, 2004) and are significantly different from classic ontologies (Gruber, 1993). They are not based on a specific domain and are intended to provide knowledge structured on lexical items (words) of a language by relating them according to their meaning. Moreover, the main goal of a lexical ontology is to assemble lexical and semantic information, instead of storing common-sense knowledge (Wandmacher et al., 2007). 10 Proceedings of the 2010 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 10–18, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics Princeton WordNet (Fellbaum, 1998) is the most representative lexico-semantic resource for English and also the most accepted model of a lexical ontology. It is structured around groups of synonymous words (synsets), which describe concepts, and connections, denoting semantic relations between those groups. The success of WordNet led to the adoption of its model by lexical resources in different lang</context>
<context position="12296" citStr="Wandmacher et al. (2007)" startWordPosition="1930" endWordPosition="1933">ies are static resources which contain limited knowledge and are not always available for this kind of research. On the other hand, there is much text available on the most different subjects, but free text has few boundaries, leading to more ambiguity and parsing issues. Therefore, it seems natural to create a lexical ontology with knowledge from several textual sources, from (i) high precision structured resources, such as manually created thesaurus, to (ii) semi-structured resources such as dictionaries or collaborative encyclopedias, as well as (iii) unstructured textual corpora. Likewise Wandmacher et al. (2007) propose for creating a lexical ontology for German, these are the general lines we will follow in our research, but for Portuguese. Considering each resource specificities, including its organisation or the vocabulary used, the extraction procedures might be significantly different, but they should all have one common output: a set of term-based relational triples that will be integrated in a single lexical ontology. Whereas the lexical network established by the triples could be used, these networks are not suitable for several tasks, as discussed in Section 2.2. A fragment of a synonymy net</context>
</contexts>
<marker>Wandmacher, Ovchinnikova, Krumnack, Dittmann, 2007</marker>
<rawString>Tonio Wandmacher, Ekaterina Ovchinnikova, Ulf Krumnack, and Henrik Dittmann. 2007. Extraction, evaluation and integration of lexical-semantic relations for the automated construction of a lexical ontology. In Third Australasian Ontology Workshop (AOW), volume 85 of CRPIT, pages 61–69, Gold Coast, Australia. ACS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof M¨uller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting lexical semantic knowledge from Wikipedia and Wiktionary.</title>
<date>2008</date>
<booktitle>In Proc. 6th Intl. Language Resources and Evaluation (LREC),</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Zesch, M¨uller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof M¨uller, and Iryna Gurevych. 2008. Extracting lexical semantic knowledge from Wikipedia and Wiktionary. In Proc. 6th Intl. Language Resources and Evaluation (LREC), Marrakech, Morocco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>