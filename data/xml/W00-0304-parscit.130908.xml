<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004309">
<title confidence="0.858759">
NJFun: A Reinforcement Learning Spoken Dialogue System
</title>
<note confidence="0.4195394">
Diane Litman, Satinder Singh, Michael Kearns and Marilyn Walker
AT&amp;T Labs — Research
180 Park Avenue
Florham Park, NJ 07932 USA
{diane,baveja,mkearns,walker}Oresearch.att. corn
</note>
<sectionHeader confidence="0.976377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999665285714286">
This paper describes NJFun, a real-time spoken dia-
logue system -Out-provides users with information
about things to do in New Jersey. NJFun auto-
matically optimizes its dialogue strategy over time,
by using a methodology for applying reinforcement
learning to a working dialogue system with human
users.
</bodyText>
<sectionHeader confidence="0.998516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931272727272">
Using the formalism of Markov decision processes
(MDPs) and the algorithms of reinforcement learn-
ing (RL) has become a standard approach to many
Al problems that involve an agent learning to
optimize reward by interaction with its environ-
ment (Sutton and Barto, 1998). We have adapted
the methods of RL to the problem of automatically
learning a good dialogue strategy in a fielded spoken
dialogue system. Here is a summary of our proposed
methodology for developing and evaluating spoken
dialogue systems using R.L:
</bodyText>
<listItem confidence="0.998090692307693">
• Choose an appropriate reward measure for di-
alogues, and an appropriate representation for
dialogue states.
• Build an initial state-based training system that
creates an exploratory data set. Despite being
exploratory, this system should provide the de-
sired basic functionality.
• Use these training dialogues to build an empir-
ical MDP model on the state space.
• Compute the optimal dialogue policy according
to this MDP, using RL.
• Reimplement the system using the learned dia-
logue policy.
</listItem>
<bodyText confidence="0.9997409">
In this demonstration session paper, we briefly de-
scribe our system, present some sample dialogues,
and summarize our main contributions and limita-
tions. Full details of our work (e.g. our reinforce-
ment learning methodology, analysis establishing the
veracity of the MDP we learn, a description of an
experimental evaluation of NJFun, analysis of our
learned dialogue strategy) can be found in two forth-
coming technical papers (Singh et al., 2000; Litman
et al., 2000).
</bodyText>
<sectionHeader confidence="0.957427" genericHeader="method">
2 The NJFun System
</sectionHeader>
<bodyText confidence="0.999982911764706">
NJFun is a real-time spoken dialogue system that
provides users with information about things to do
in New Jersey.i An example dialogue with NJFun is
shown in Figure 1. NJFun is built using an internal
platform for spoken dialogue systems. NJFun uses
a speech recognizer with stochastic language models
trained from example user utterances, and a TTS
system based on concatenative diphone synthesis.
Its database is populated from the nj .online web-
page to contain information about activities, NJFun
indexes this database using three attributes: activity
type, location, and time of day.
Informally, the NJFun dialogue manager sequen-
tially queries the user regarding the activity, location
and time attributes, respectively. NJFun first asks
the user for the current attribute (and possibly the
other attributes, depending on the initiative). If the
current attribute&apos;s value is not obtained, NJFun asks
for the attribute (and possibly the later attributes)
again. If NJFun still does not obtain a value, NJ-
Fun moves on to the next attribute(s). Whenever
NJFun successfully obtains a value, it can confirm
the value, or move on and attempt to obtain the
next attribute(s).2 When NJFun has finished asking
about the attributes, it queries the database (using
a wildcard for each unobtained attribute value).
We use reinforcement learning (RL) to optimize
dialogue strategy. fi,L requires that all potential ac-
tions for each state be specified. Note that at some
states it is easy for a human to make the correct
action choice. We made obvious dialogue strategy
choices in advance, and used learning only to opti-
mize the difficult choices. In NJFun, we restricted
the action choices to 1) the type of initiative to use
</bodyText>
<footnote confidence="0.925862333333333">
&apos;This system description section is taken from (Litman et
al., 2000) .
2Note that it is possible for users to specify multiple at-
tributes, in any order, in a single utterance. However, NJFun
will always process multiple attributes using its predefined
sequential ordering.
</footnote>
<page confidence="0.999236">
17
</page>
<bodyText confidence="0.7735185">
Si: Welcome to NJFun. How may I help you?
Ul: I&apos;d like to find um winetasting in Lambertville in the morning.
</bodyText>
<listItem confidence="0.794057625">
tASR outptat I&apos;d like to find out winerle3 the in the Lambert-Wile in the morning.)
52: Did you say you are interested in Lambertville?
U2: Yes.
53: Did you say you want to go in the morning?
US: Yes.
S4: I found a winery near Lambertville that is open in the morning. It is [...1
Please give me feedback by saying &apos;good&apos;, &apos;so-so&apos;, or &apos;bad&apos;.
U4: Good.
</listItem>
<figureCaption confidence="0.994909">
Figure 1: An example dialogue with NJFun (after optimization via RL).
</figureCaption>
<table confidence="0.9183604">
Action Prompt
GreetS Welcome to NJFan. Please say an activity name or say &apos;list activities&apos; for a list of activities I know about.
Greet° Welcome to bliFun. How may I help you?
ReAskI5 I know about amusement parks, aquariums, cruises, historic sites, museums, parks, theaters, wineries,
ReAsklM and zoos. Please say an activity name from this list.
Please tell me the activity type.You can also tell me the location and time.
Ask2S Please say the name of the town or city that you are interested in.
Ask2U Please give me more information.
ReAsk25 Please tell me the name of the town or city that you are interested in.
ReAsk2M —Please tell me the location that you are interested in. You can also tell me the time.
</table>
<figureCaption confidence="0.996533">
Figure 2: Sample initiative strategy choices.
</figureCaption>
<bodyText confidence="0.999976484848485">
when asking or reasking for an attribute, and 2)
whether to confirm an attribute value once obtained.
The optimal actions may vary with dialogue state,
and are subject to active debate in the literature.
The examples in Figure 2 shows that NJFun can
ask the user about the first 2 attributes&apos; using three
types of initiative, based on the combination of the
wording of the system prompt (open versus direc-
tive), and the type of grammar NJFun uses during
ASR (restrictive versus non-restrictive). If NJFun
uses an open question with an unrestricted gram-
mar, it is using user initiative (e.g., GreetU). If NJ-
Fun instead uses a directive prompt with a restricted
grammar, the system is using system initiative (e.g.,
GreetS). If NJFun uses a directive question with a
non-restrictive grammar, it is using mixed initiative,
because it is giving the user an opportunity to take
the initiative by supplying extra information (e.g.,
ReAsk IM).
NJFun can also vary the strategy used to confirm
each attribute. If NJFun asks the user to explicitly
verify an attribute, it is using explicit confirmation
(e.g., ExpConf2 for the location, exemplified by S2
in Figure 1). If NJFun does not generate any con-
firmation prompt, it is using no confirmation (an
action we call NoConf).
Solely for the purposes of controlling its operation
(as opposed to the learning, which we discuss in a
moment), NJFun internally maintains an operations
vector of 14 variables. 2 variables track whether the
system has greeted the user, and which attribute
the system is currently attempting to obtain. For
each of the 3 attributes, 4 variables track whether
</bodyText>
<footnote confidence="0.899538666666667">
3 &amp;quot;Greet&amp;quot; is equivalent to asking for the first attribute. NJ-
Fun always uses system initiative for the third attribute, be-
cause at that point the user can only provide the time of day.
</footnote>
<bodyText confidence="0.999877277777778">
the system has obtained the attribute&apos;s value, the
system&apos;s confidence in the value (if obtained), the
number of times the system has asked the user about
the attribute, and the type of ASR grammar most
recently used to ask for the attribute.
The formal state space S maintained by NJFun
for the purposes of learning is much simpler than
the operations vector, due to data sparsity concerns.
The dialogue state space S contains only 7 variables,
which are summarized in Figure 3, and is easily com-
puted from the operations vector. The &amp;quot;greet&amp;quot; vari-
able tracks whether the system has greeted the user
or not (no=0, yes=1). &amp;quot;Atte specifies which at-
tribute NJFun is currently attempting to obtain or
verify (activity=1, location=2, tirne=3, done with
attributes=4). &amp;quot;Conf&amp;quot; represents the confidence
that NJFun has after obtaining a value for an at-
tribute. The values 0, I, and 2 represent low,
medium and high ASR confidence. The values 3
and 4 are set when ASH hears &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; after a
confirmation question. &amp;quot;Val&amp;quot; tracks whether NJFun
has obtained a value for the attribute (no=0, yes=1).
&amp;quot;Times&amp;quot; tracks the number of times that NJFun has
asked the user about the attribute. &amp;quot;Gram&amp;quot; tracks
the type of grammar most recently used to obtain
the attribute (0=non-restrictive, 1=restrictive). Fi-
nally, &amp;quot;history&amp;quot; represents whether NJFun had trou-
ble understanding the user in the earlier part of the
conversation (bad=0, good=1). We omit the full
definition, but as an example, when NJFun is work-
ing on the second attribute (location), the history
variable is set to 0 if NJFun does not have an ac-
tivity, has an activity but has no confidence in the
value, or needed two queries to obtain the activity,
In order to apply RL with a limited amount of
training data, we need to design a small state space
</bodyText>
<page confidence="0.985274">
18
</page>
<figure confidence="0.9283645">
greet attr conf val times gram history
0,1 1,2,3,9 0,1,2,3,4 0,1 0,1,2 0,1 0,1
</figure>
<figureCaption confidence="0.988377">
Figure 3: State features and values.
</figureCaption>
<bodyText confidence="0.996781057692308">
that makes enough critical distinctions to support
learning. The use of S yields a state space of size
62. The state space that we utilize here, although
minimal, allows us to make initiative decisions based
on the success of earlier exchanges, and confirmation
decisions based on ASP. confidence scores and gram-
mars.
In order to learn a good dialogue strategy via RL
we have to explore the state action space. The
state/action mapping representing NJ Fun&apos;s initial
exploratory dialogue strategy EIC (Exploratory for
Initiative and Confirmation) is given in Figure 4.
Only the exploratory portion of the strategy is
shown, namely all those states for which NJFun has
an action choice. For each such state, we list the
two choices of actions available, (The action choices
in boldface are the ones eventually identified as op-
timal by the learning process.) The EIC strategy
chooses randomly between these two actions when in
the indicated state, in order to maximize exploration
and minimize data sparseness when constructing our
model. Since there are 42 states with 2 choices
each, there is a search space of 242 potential dia-
logue strategies; the goal of the RL is to identify an
apparently optimal strategy from this large search
space. Note that due to the randomization of the
EIC strategy, the prompts are designed to ensure
the coherence of all possible action sequences.
Figure 5 illustrates how the dialogue strategy in
Figure 4 generates the dialogue in Figure 1. Each
row indicates the state that NJFun is in, the ac-
tion executed in this state, the corresponding turn
in Figure 1, and the reward received. The initial
state represents that NJFun will first attempt to ob-
tain attribute 1. NJFun executes GreetU (although
as shown in Figure 4, GreetS is also possible), gen-
erating the first utterance in Figure 1. After the
user&apos;s response, the next state represents that NJ-
Fun has now greeted the user and obtained the ac-
tivity value with high confidence, by using a non-
restrictive grammar. NJFun chooses not to confirm
the activity, which causes the state to change but no
prompt to be generated. The third state represents
that NJFun is now working on the second attribute
(location), that it already has this value with high
confidence (location was obtained with activity af-
ter the user&apos;s first utterance), and that the dialogue
history is good. This time NJFun chooses to confirm
the attribute with the second NJFun utterance, and
the state changes again. The processing of time is
similar to that of location, which leads NJFun to the
final state, where it performs the action &amp;quot;Tell&amp;quot; (cor-
</bodyText>
<table confidence="0.995619177777778">
gac g h Action Choices
State
v t
0 1 0 0 0 0 0 GrcetS,GreetU
1 1 0 0 1 0 0 ReAsk1S,ReAs1, 1 M
1 1 0 1 0 0 0 NoConf,ExpConfl.
1 1 0 1 0 1 0 NoConf,ExpConfl
1 1 1 1 0 0 0 NoConf,ExpConfl
1 1 1 1 0 1 0 NoConf,ExpConf1
1 1 2 1 0 0 0 NoConf,ExpConfl
1 1 2 1 0 1 0 NoConf,ExpConfl
1 1 4 0 0 0 0 1teAsk1S,ReAsk1M
1 1 4 0 1 0 0 ReAsk1S,ReAsk 1 M
1 2 0 0 0 0 0 Ask25,Ask2U
1 2 0 0 0 0 1 Ask2S,Ask 2 U
1 2 0 0 1 0 0 ReAsk2S,ReAsk2M
1 2 0 0 1 0 1 ReAsk2S,ReAsk2M
1 2 0 1 0 0 0 NoConf,ExpConf2
1 2 0 1 0 0 1 NoConf,ExpConf2
1 2 0 1 0 1 0 NoConf,ExpConf2
1 2 0 1 0 1 1 NoConf,ExpConf2
1 2 1 1 0 0 0 NoConf,ExpConf2
1 2 1 1 0 0 1 NoConf,ExpConf2
1 2 1 1 a 1 o NoConf,ExpConf2
1 2 1 1 0 1 1 NoConf,ExpConf2
1 2 2 1 0 0 0 NoConf,ExpConf2
1 2 2 1 a o 1 NoConf,ExpConf2
1 2 2 1 0- 1 0 NoConf,ExpConf2
1 2 2 1 0 1 1 NoConf,ExpConf2
1 2 4 0 0 0 0 ReAsk2S,ReAsk2M
1 2 4 0 0 0 1 ReAsk2S,ReAsk2M
1 2 4 0 1 0 0 ReAsk2S,ReAsk2M
1 2 4 0 1 0 1 ReAsk2S,ReAsk2M
1 3 0 1 0 0 0 NoConf,ExpConf3
1 3 0 1 0 0 1 NoConf,ExpConf3
1 3 0 1 0 I 0 NoConf,ExpConf3
1 3 0 1 0 1 1 NoConf,ExpConf3
1 3 1 1 0 0 0 NoConf,ExpConf3
1 3 1 1 0 0 1 NoConf,ExpConf3
1 3 1 1 0 1 0 NoConf,ExpConf3
1 3 1 1 0 1 1 NoConf,ExpConf3
1 3 2 1 0 0 0 NoConf,ExpConf3
1 3 2 1 0 0 1 NoConf,ExpConf3
1 3 2 1 0 1 0 NoConf,ExpCon(3
1 3 2 1 0 1 1 NoConf,ExpConf3
</table>
<figureCaption confidence="0.663386">
Figure 4: Exploratory portion or EIC strategy,
</figureCaption>
<table confidence="0.998941714285714">
State Action Turn Reward
gacvtgh
0 1 0 0 0 0 0 GreetU 51 0
1 1 2 1 0 0 0 NoConf 0
1 2 2 1 0 0 1 ExpConf2 S2 0
1 3 2 1 0 0 1 ExpConf3 53 0
1 4 0 0 0 0 0 Tell S4 1
</table>
<figureCaption confidence="0.998566">
Figure 5: Generating the dialogue in Figure 1.
</figureCaption>
<bodyText confidence="0.9999242">
responding to querying the database, presenting the
results to the user, and asking the user to provide a
reward). Note that in NJFun, the reward is always
0 except at the terminal state, as shown in the last
column of Figure 5.
</bodyText>
<sectionHeader confidence="0.978627" genericHeader="method">
3 Sample Dialogues
</sectionHeader>
<bodyText confidence="0.991230333333333">
Figures 1 and 6 present two dialogues that were gen-
erated using different dialogue strategies in the EIC
search space. For example, although NJFun begins
both dialogues in the first state in Figure 4, NJFun
executes GreetU in one dialogue and GreetS in the
other. Similarly, although the next state of NJFun
</bodyText>
<page confidence="0.989712">
19
</page>
<figure confidence="0.99753472972973">
Si
Ul:
US:
U3.
U9:
U5:
S6:
Us:
UT:
US:
510!
Sll:
Ull:
Welcome to NJFun. Please say an activity name or say &apos;list activities&apos; for a list of activities I know about,
would like to visit a historic site in Stanhope this morning.
(ASR output: I would like to the zoo historic sites in stanhope historic)
Did you say you are interested in going to a zoo?
No.
know about amusement parks, aquariums, cruises, historic sites, museums, parks, theaters, wineries, and zoos.
Please say art activity name from this list.
1 would like to visit a historic site. (ASR output: I would like to visit historic sites)
Did you say you are interested in going to a historic site?
Yes.
Please give me more information.
(user says nothing)
Please tell me the location that you are interested in. You can also tell me the time.
Visit a historic. (ASR output: What historic)
What time of the day do you want to go?
Stanhope. (no ASR output)
Do you want to go in the morning, in the afternoon, or in the evening?
Morning.
Did you say you want to go in the morning?
Yes.
I found 11, historic sites in New Jersey that are open in the morning. The first 3 1...] Would you like to hear more?
No.
Thank-you for using the system. Please give me feedback by saying &apos;good&apos;, &apos;so-so&apos;, or &apos;bad&apos;.
Bad.
</figure>
<figureCaption confidence="0.9998">
Figure 6: An example training dialogue with NJFun.
</figureCaption>
<bodyText confidence="0.9228095">
is the same in both dialogues (111 1 2 1 0 0 0&amp;quot;), the
activity is not confirmed in the first dialogue.
</bodyText>
<sectionHeader confidence="0.997244" genericHeader="method">
4 Contributions
</sectionHeader>
<bodyText confidence="0.999936787878788">
The main contribution of this work is that we
have developed and empirically validated a practi-
cal methodology for using RL to build a real dia-
logue system that optimizes its behavior from dia-
logue data. Unlike traditional approaches to learn-
ing dialogue strategy from data, which are limited
to searching a handful of policies, our RL approach
is able to search many tens of thousands of dialogue
strategies. In particular, the traditional approach
is to pick a handful of strategies that experts in-
tuitively feel are good, implement each policy as a
separate system, collect data from representative hu-
man users for each system, and then use standard
statistical tests on that data to pick the best sys-
tem, e.g. (Danieli and Gerbino, 1995). In contrast,
our use of RL allowed us to explore 242 strategies
that were left in our search space after we excluded
strategies that were clearly suboptimal.
An empirical validation of our approach is de-
tailed in two forthcoming technical papers (Singh
et al., 2000; Litman et al., 2000). We obtained 311
dialogues with the exploratory (i.e., training) ver-
sion of NJFun, constructed an MDP from this train-
ing data, used RL to compute the optimal dialogue
strategy in this MDP, reimplemented NJFun such
that it used this learned dialogue strategy, and ob-
tained 124 more dialogues. Our main result was
that task completion improved from 52% to 64%
from training to test data. Furthermore, analysis
of our MDP showed that the learned strategy was
not only better than EIC, but also better than other
fixed choices proposed in the literature (Singh et al.,
2000).
</bodyText>
<sectionHeader confidence="0.998199" genericHeader="conclusions">
5 Limitations
</sectionHeader>
<bodyText confidence="0.999923588235294">
The main limitation of this effort to automate the
design of a good dialogue strategy is that our current
framework has nothing to say about how to choose
the reward measure, or how to best represent dia-
logue state. In NJFun we carefully but manually de-
signed the state space of the dialogue. In the future,
we hope to develop a learning methodology to auto-
mate the choice of state space for dialogue systems.
With respect to the reward function, our empirical
evaluation investigated the impact of using a number
of reward measures (e.g,, user feedback such as U4 in
Figure 1, task completion rate, ASR accuracy), and
found that some rewards worked better than others.
We would like to better understand these differences
among the reward measures, investigate the use of
a learned reward function, and explore the use of
non-terminal rewards.
</bodyText>
<sectionHeader confidence="0.998665" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995570428571429">
M. Danieli and E. Gerbino. 1995. Metrics for eval-
uating dialogue strategies in a spoken language
system. In Proceedings of the 1995 AAAl Spring
Symposium on Empirical Methods in Discourse
Interpretation and Generation, pages 34-39.
D. Litman, M. Reams, S. Singh, and M. Walker,
2000. Automatic optimization of dialogue man-
agement. Manuscript submitted for publication.
S. Singh, M. Kearns, D. Litman, and M. Walker.
2000, Empirical evaluation of a reinforcement
learning spoken dialogue system. In Proceedings
of AA AI 2000,
R. S. Sutton and A. G. Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press.
</reference>
<page confidence="0.994854">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851927">
<title confidence="0.99967">NJFun: A Reinforcement Learning Spoken Dialogue System</title>
<author confidence="0.989549">Diane Litman</author>
<author confidence="0.989549">Satinder Singh</author>
<author confidence="0.989549">Michael Kearns</author>
<author confidence="0.989549">Marilyn</author>
<affiliation confidence="0.910121">AT&amp;T Labs —</affiliation>
<address confidence="0.9960285">180 Park Florham Park, NJ 07932</address>
<abstract confidence="0.992725875">This paper describes NJFun, a real-time spoken diasystem users with information about things to do in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Danieli</author>
<author>E Gerbino</author>
</authors>
<title>Metrics for evaluating dialogue strategies in a spoken language system.</title>
<date>1995</date>
<booktitle>In Proceedings of the 1995 AAAl Spring Symposium on Empirical Methods in Discourse Interpretation and Generation,</booktitle>
<pages>34--39</pages>
<contexts>
<context position="15962" citStr="Danieli and Gerbino, 1995" startWordPosition="2842" endWordPosition="2845">using RL to build a real dialogue system that optimizes its behavior from dialogue data. Unlike traditional approaches to learning dialogue strategy from data, which are limited to searching a handful of policies, our RL approach is able to search many tens of thousands of dialogue strategies. In particular, the traditional approach is to pick a handful of strategies that experts intuitively feel are good, implement each policy as a separate system, collect data from representative human users for each system, and then use standard statistical tests on that data to pick the best system, e.g. (Danieli and Gerbino, 1995). In contrast, our use of RL allowed us to explore 242 strategies that were left in our search space after we excluded strategies that were clearly suboptimal. An empirical validation of our approach is detailed in two forthcoming technical papers (Singh et al., 2000; Litman et al., 2000). We obtained 311 dialogues with the exploratory (i.e., training) version of NJFun, constructed an MDP from this training data, used RL to compute the optimal dialogue strategy in this MDP, reimplemented NJFun such that it used this learned dialogue strategy, and obtained 124 more dialogues. Our main result wa</context>
</contexts>
<marker>Danieli, Gerbino, 1995</marker>
<rawString>M. Danieli and E. Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language system. In Proceedings of the 1995 AAAl Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages 34-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>M Reams</author>
<author>S Singh</author>
<author>M Walker</author>
</authors>
<title>Automatic optimization of dialogue management. Manuscript submitted for publication.</title>
<date>2000</date>
<contexts>
<context position="2039" citStr="Litman et al., 2000" startWordPosition="311" endWordPosition="314"> on the state space. • Compute the optimal dialogue policy according to this MDP, using RL. • Reimplement the system using the learned dialogue policy. In this demonstration session paper, we briefly describe our system, present some sample dialogues, and summarize our main contributions and limitations. Full details of our work (e.g. our reinforcement learning methodology, analysis establishing the veracity of the MDP we learn, a description of an experimental evaluation of NJFun, analysis of our learned dialogue strategy) can be found in two forthcoming technical papers (Singh et al., 2000; Litman et al., 2000). 2 The NJFun System NJFun is a real-time spoken dialogue system that provides users with information about things to do in New Jersey.i An example dialogue with NJFun is shown in Figure 1. NJFun is built using an internal platform for spoken dialogue systems. NJFun uses a speech recognizer with stochastic language models trained from example user utterances, and a TTS system based on concatenative diphone synthesis. Its database is populated from the nj .online webpage to contain information about activities, NJFun indexes this database using three attributes: activity type, location, and tim</context>
<context position="3846" citStr="Litman et al., 2000" startWordPosition="601" endWordPosition="604">ute(s).2 When NJFun has finished asking about the attributes, it queries the database (using a wildcard for each unobtained attribute value). We use reinforcement learning (RL) to optimize dialogue strategy. fi,L requires that all potential actions for each state be specified. Note that at some states it is easy for a human to make the correct action choice. We made obvious dialogue strategy choices in advance, and used learning only to optimize the difficult choices. In NJFun, we restricted the action choices to 1) the type of initiative to use &apos;This system description section is taken from (Litman et al., 2000) . 2Note that it is possible for users to specify multiple attributes, in any order, in a single utterance. However, NJFun will always process multiple attributes using its predefined sequential ordering. 17 Si: Welcome to NJFun. How may I help you? Ul: I&apos;d like to find um winetasting in Lambertville in the morning. tASR outptat I&apos;d like to find out winerle3 the in the Lambert-Wile in the morning.) 52: Did you say you are interested in Lambertville? U2: Yes. 53: Did you say you want to go in the morning? US: Yes. S4: I found a winery near Lambertville that is open in the morning. It is [...1 P</context>
<context position="16251" citStr="Litman et al., 2000" startWordPosition="2891" endWordPosition="2894">In particular, the traditional approach is to pick a handful of strategies that experts intuitively feel are good, implement each policy as a separate system, collect data from representative human users for each system, and then use standard statistical tests on that data to pick the best system, e.g. (Danieli and Gerbino, 1995). In contrast, our use of RL allowed us to explore 242 strategies that were left in our search space after we excluded strategies that were clearly suboptimal. An empirical validation of our approach is detailed in two forthcoming technical papers (Singh et al., 2000; Litman et al., 2000). We obtained 311 dialogues with the exploratory (i.e., training) version of NJFun, constructed an MDP from this training data, used RL to compute the optimal dialogue strategy in this MDP, reimplemented NJFun such that it used this learned dialogue strategy, and obtained 124 more dialogues. Our main result was that task completion improved from 52% to 64% from training to test data. Furthermore, analysis of our MDP showed that the learned strategy was not only better than EIC, but also better than other fixed choices proposed in the literature (Singh et al., 2000). 5 Limitations The main limi</context>
</contexts>
<marker>Litman, Reams, Singh, Walker, 2000</marker>
<rawString>D. Litman, M. Reams, S. Singh, and M. Walker, 2000. Automatic optimization of dialogue management. Manuscript submitted for publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Singh</author>
<author>M Kearns</author>
<author>D Litman</author>
<author>M Walker</author>
</authors>
<title>Empirical evaluation of a reinforcement learning spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings of AA AI</booktitle>
<contexts>
<context position="2017" citStr="Singh et al., 2000" startWordPosition="307" endWordPosition="310"> empirical MDP model on the state space. • Compute the optimal dialogue policy according to this MDP, using RL. • Reimplement the system using the learned dialogue policy. In this demonstration session paper, we briefly describe our system, present some sample dialogues, and summarize our main contributions and limitations. Full details of our work (e.g. our reinforcement learning methodology, analysis establishing the veracity of the MDP we learn, a description of an experimental evaluation of NJFun, analysis of our learned dialogue strategy) can be found in two forthcoming technical papers (Singh et al., 2000; Litman et al., 2000). 2 The NJFun System NJFun is a real-time spoken dialogue system that provides users with information about things to do in New Jersey.i An example dialogue with NJFun is shown in Figure 1. NJFun is built using an internal platform for spoken dialogue systems. NJFun uses a speech recognizer with stochastic language models trained from example user utterances, and a TTS system based on concatenative diphone synthesis. Its database is populated from the nj .online webpage to contain information about activities, NJFun indexes this database using three attributes: activity t</context>
<context position="16229" citStr="Singh et al., 2000" startWordPosition="2887" endWordPosition="2890">ialogue strategies. In particular, the traditional approach is to pick a handful of strategies that experts intuitively feel are good, implement each policy as a separate system, collect data from representative human users for each system, and then use standard statistical tests on that data to pick the best system, e.g. (Danieli and Gerbino, 1995). In contrast, our use of RL allowed us to explore 242 strategies that were left in our search space after we excluded strategies that were clearly suboptimal. An empirical validation of our approach is detailed in two forthcoming technical papers (Singh et al., 2000; Litman et al., 2000). We obtained 311 dialogues with the exploratory (i.e., training) version of NJFun, constructed an MDP from this training data, used RL to compute the optimal dialogue strategy in this MDP, reimplemented NJFun such that it used this learned dialogue strategy, and obtained 124 more dialogues. Our main result was that task completion improved from 52% to 64% from training to test data. Furthermore, analysis of our MDP showed that the learned strategy was not only better than EIC, but also better than other fixed choices proposed in the literature (Singh et al., 2000). 5 Lim</context>
</contexts>
<marker>Singh, Kearns, Litman, Walker, 2000</marker>
<rawString>S. Singh, M. Kearns, D. Litman, and M. Walker. 2000, Empirical evaluation of a reinforcement learning spoken dialogue system. In Proceedings of AA AI 2000,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Sutton</author>
<author>A G Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="826" citStr="Sutton and Barto, 1998" startWordPosition="118" endWordPosition="121">s,walker}Oresearch.att. corn Abstract This paper describes NJFun, a real-time spoken dialogue system -Out-provides users with information about things to do in New Jersey. NJFun automatically optimizes its dialogue strategy over time, by using a methodology for applying reinforcement learning to a working dialogue system with human users. 1 Introduction Using the formalism of Markov decision processes (MDPs) and the algorithms of reinforcement learning (RL) has become a standard approach to many Al problems that involve an agent learning to optimize reward by interaction with its environment (Sutton and Barto, 1998). We have adapted the methods of RL to the problem of automatically learning a good dialogue strategy in a fielded spoken dialogue system. Here is a summary of our proposed methodology for developing and evaluating spoken dialogue systems using R.L: • Choose an appropriate reward measure for dialogues, and an appropriate representation for dialogue states. • Build an initial state-based training system that creates an exploratory data set. Despite being exploratory, this system should provide the desired basic functionality. • Use these training dialogues to build an empirical MDP model on the</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R. S. Sutton and A. G. Barto. 1998. Reinforcement Learning: An Introduction. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>