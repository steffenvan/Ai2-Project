<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978427">
Distant Supervision for Relation Extraction with Matrix Completion
</title>
<author confidence="0.857762">
Miao Fan†,‡,∗, Deli Zhao‡, Qiang Zhou†, Zhiyuan Liu*,‡, Thomas Fang Zheng†, Edward Y. Chang‡† CSLT, Division of Technical Innovation and Development,
</author>
<affiliation confidence="0.8420515">
Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, China.
° Department of Computer Science and Technology, Tsinghua University, China.
</affiliation>
<address confidence="0.637724">
‡ HTC Beijing Advanced Technology and Research Center, China.
</address>
<email confidence="0.992232">
∗fanmiao.cslt.thu@gmail.com
</email>
<sectionHeader confidence="0.994648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995337037037">
The essence of distantly supervised rela-
tion extraction is that it is an incomplete
multi-label classification problem with s-
parse and noisy features. To tackle the s-
parsity and noise challenges, we propose
solving the classification problem using
matrix completion on factorized matrix of
minimized rank. We formulate relation
classification as completing the unknown
labels of testing items (entity pairs) in a s-
parse matrix that concatenates training and
testing textual features with training label-
s. Our algorithmic framework is based on
the assumption that the rank of item-by-
feature and item-by-label joint matrix is
low. We apply two optimization model-
s to recover the underlying low-rank ma-
trix leveraging the sparsity of feature-label
matrix. The matrix completion problem is
then solved by the fixed point continuation
(FPC) algorithm, which can find the glob-
al optimum. Experiments on two wide-
ly used datasets with different dimension-
s of textual features demonstrate that our
low-rank matrix completion approach sig-
nificantly outperforms the baseline and the
state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997173111111111">
Relation Extraction (RE) is the process of gen-
erating structured relation knowledge from un-
structured natural language texts. Traditional su-
pervised methods (Zhou et al., 2005; Bach and
Badaskar, 2007) on small hand-labeled corpora,
such as MUC1 and ACE2, can achieve high pre-
cision and recall. However, as producing hand-
labeled corpora is laborius and expensive, the su-
pervised approach can not satisfy the increasing
</bodyText>
<footnote confidence="0.99992">
1http://www.itl.nist.gov/iaui/894.02/related projects/muc/
2http://www.itl.nist.gov/iad/mig/tests/ace/
</footnote>
<figureCaption confidence="0.8057695">
Figure 1: Training corpus generated by the basic
alignment assumption of distantly supervised re-
lation extraction. The relation instances are the
triples related to President Barack Obama in the
</figureCaption>
<bodyText confidence="0.982497714285715">
Freebase, and the relation mentions are some sen-
tences describing him in the Wikipedia.
demand of building large-scale knowledge reposi-
tories with the explosion of Web texts. To address
the lacking training data issue, we consider the dis-
tant (Mintz et al., 2009) or weak (Hoffmann et al.,
2011) supervision paradigm attractive, and we im-
prove the effectiveness of the paradigm in this pa-
per.
The intuition of the paradigm is that one
can take advantage of several knowledge bases,
such as WordNet3, Freebase4 and YAGO5, to
automatically label free texts, like Wikipedia6
and New York Times corpora7, based on some
heuristic alignment assumptions. An example
accounting for the basic but practical assumption
is illustrated in Figure 1, in which we know
that the two entities (&lt;Barack Obama,
U.S.&gt;) are not only involved in the rela-
tion instances8 coming from knowledge bases
(President-of(Barack Obama, U.S.)
</bodyText>
<footnote confidence="0.950490666666667">
and Born-in(Barack Obama, U.S.)),
3http://wordnet.princeton.edu
4http://www.freebase.com
5http://www.mpi-inf.mpg.de/yago-naga/yago
6http://www.wikipedia.org
7http://catalog.ldc.upenn.edu/LDC2008T19
8According to convention, we regard a structured triple
r(ei, ej) as a relation instance which is composed of a pair of
entities &lt;ei, ej&gt;and a relation name r with respect to them.
</footnote>
<page confidence="0.945604">
839
</page>
<note confidence="0.9196935">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 839–849,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.99180125">
Figure 2: The procedure of noise-tolerant low-rank matrix completion. In this scenario, distantly super-
vised relation extraction task is transformed into completing the labels for testing items (entity pairs) in
a sparse matrix that concatenates training and testing textual features with training labels. We seek to
recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously.
</figureCaption>
<figure confidence="0.994498888888889">
Observed Sparse Matrix
Completed Low−rank Matrix
Error Matrix
Training
Items
Testing
Items
Noisy Features
Incomplete Labels
</figure>
<bodyText confidence="0.997349769230769">
but also co-occur in several relation mentions9
appearing in free texts (Barack Obama is
the 44th and current President of
the U.S. and Barack Obama was born
in Honolulu, Hawaii, U.S., etc.). We
extract diverse textual features from all those
relation mentions and combine them into a rich
feature vector labeled by the relation names
(President-of and Born-in) to produce a
weak training corpus for relation classification.
This paradigm is promising to generate large-
scale training corpora automatically. However, it
comes up against three technical challeges:
</bodyText>
<listItem confidence="0.980344">
• Sparse features. As we cannot tell what
kinds of features are effective in advance, we
have to use NLP toolkits, such as Stanford
CoreNLP10, to extract a variety of textual fea-
tures, e.g., named entity tags, part-of-speech
tags and lexicalized dependency paths. Un-
fortunately, most of them appear only once in
the training corpus, and hence leading to very
sparse features.
• Noisy features. Not all relation mentions
express the corresponding relation instances.
For example, the second relation mention in
Figure 1 does not explicitly describe any rela-
tion instance, so features extracted from this
sentence can be noisy. Such analogous cases
commonly exist in feature extraction.
• Incomplete labels. Similar to noisy fea-
</listItem>
<footnote confidence="0.942864333333333">
9The sentences that contain the given entity pair are called
relation mentions.
10http://nlp.stanford.edu/downloads/corenlp.shtml
</footnote>
<bodyText confidence="0.999585235294118">
tures, the generated labels can be in-
complete. For example, the fourth re-
lation mention in Figure 1 should have
been labeled by the relation Senate-of.
However, the incomplete knowledge base
does not contain the corresponding relation
instance (Senate-of(Barack Obama,
U.S.)). Therefore, the distant supervision
paradigm may generate incomplete labeling
corpora.
In essence, distantly supervised relation extrac-
tion is an incomplete multi-label classification task
with sparse and noisy features.
In this paper, we formulate the relation-
extraction task from a novel perspective of using
matrix completion with low rank criterion. To the
best of our knowledge, we are the first to apply this
technique on relation extraction with distant super-
vision. More specifically, as shown in Figure 2, we
model the task with a sparse matrix whose rows
present items (entity pairs) and columns contain
noisy textual features and incomplete relation la-
bels. In such a way, relation classification is trans-
formed into a problem of completing the unknown
labels for testing items in the sparse matrix that
concatenates training and testing textual features
with training labels, based on the assumption that
the item-by-feature and item-by-label joint matrix
is of low rank. The rationale of this assumption
is that noisy features and incomplete labels are
semantically correlated. The low-rank factoriza-
tion of the sparse feature-label matrix delivers the
low-dimensional representation of de-correlation
for features and labels.
</bodyText>
<page confidence="0.990115">
840
</page>
<bodyText confidence="0.999984470588235">
We contribute two optimization models, DRM-
C11-b and DRMC-1, aiming at exploiting the s-
parsity to recover the underlying low-rank matrix
and to complete the unknown testing labels simul-
taneously. Moreover, the logistic cost function is
integrated in our models to reduce the influence of
noisy features and incomplete labels, due to that
it is suitable for binary variables. We also modify
the fixed point continuation (FPC) algorithm (Ma
et al., 2011) to find the global optimum.
Experiments on two widely used datasets
demonstrate that our noise-tolerant approaches
outperform the baseline and the state-of-the-art
methods. Furthermore, we discuss the influence of
feature sparsity, and our approaches consistently
achieve better performance than compared meth-
ods under different sparsity degrees.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999623">
The idea of distant supervision was firstly pro-
posed in the field of bioinformatics (Craven and
Kumlien, 1999). Snow et al. (2004) used Word-
Net as the knowledge base to discover more h-
pyernym/hyponym relations between entities from
news articles. However, either bioinformatic
database or WordNet is maintained by a few ex-
perts, thus hardly kept up-to-date.
As we are stepping into the big data era, the
explosion of unstructured Web texts simulates us
to build more powerful models that can automat-
ically extract relation instances from large-scale
online natural language corpora without hand-
labeled annotation. Mintz et al. (2009) adopt-
ed Freebase (Bollacker et al., 2008; Bollacker
et al., 2007), a large-scale crowdsourcing knowl-
edge base online which contains billions of rela-
tion instances and thousands of relation names, to
distantly supervise Wikipedia corpus. The basic
alignment assumption of this work is that if a pair
of entities participate in a relation, all sentences
that mention these entities are labeled by that rela-
tion name. Then we can extract a variety of textu-
al features and learn a multi-class logistic regres-
sion classifier. Inspired by multi-instance learn-
ing (Maron and Lozano-P´erez, 1998), Riedel et al.
(2010) relaxed the strong assumption and replaced
all sentences with at least one sentence. Hoff-
mann et al. (2011) pointed out that many entity
pairs have more than one relation. They extend-
</bodyText>
<footnote confidence="0.9807855">
11It is the abbreviation for Distant supervision for Relation
extraction with Matrix Completion
</footnote>
<bodyText confidence="0.99956125">
ed the multi-instance learning framework (Riedel
et al., 2010) to the multi-label circumstance. Sur-
deanu et al. (2012) proposed a novel approach to
multi-instance multi-label learning for relation ex-
traction, which jointly modeled all the sentences in
texts and all labels in knowledge bases for a giv-
en entity pair. Other literatures (Takamatsu et al.,
2012; Min et al., 2013; Zhang et al., 2013; Xu
et al., 2013) addressed more specific issues, like
how to construct the negative class in learning or
how to adopt more information, such as name en-
tity tags, to improve the performance.
Our work is more relevant to Riedel et al.’s
(2013) which considered the task as a matrix fac-
torization problem. Their approach is composed
of several models, such as PCA (Collins et al.,
2001) and collaborative filtering (Koren, 2008).
However, they did not concern about the data noise
brought by the basic assumption of distant super-
vision.
</bodyText>
<sectionHeader confidence="0.990478" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.999990944444445">
We apply a new technique in the field of ap-
plied mathematics, i.e., low-rank matrix comple-
tion with convex optimization. The breakthrough
work on this topic was made by Cand`es and Recht
(2009) who proved that most low-rank matrices
can be perfectly recovered from an incomplete
set of entries. This promising theory has been
successfully applied on many active research ar-
eas, such as computer vision (Cabral et al., 2011),
recommender system (Rennie and Srebro, 2005)
and system controlling (Fazel et al., 2001). Our
models for relation extraction are based on the
theoretic framework proposed by Goldberg et al.
(2010), which formulated the multi-label trans-
ductive learning as a matrix completion problem.
The new framework for classification enhances the
robustness to data noise by penalizing differen-
t cost functions for features and labels.
</bodyText>
<subsectionHeader confidence="0.996123">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.999923333333333">
Suppose that we have built a training corpus for
relation classification with n items (entity pairs),
d-dimensional textual features, and t labels (rela-
tions), based on the basic alignment assumption
proposed by Mintz et al. (2009). Let Xtrain ∈
Rn×d and Ytrain ∈ Rn×t denote the feature matrix
and the label matrix for training, respectively. The
linear classifier we adopt aims to explicitly learn
the weight matrix W ∈ Rd×t and the bias column
</bodyText>
<page confidence="0.989402">
841
</page>
<bodyText confidence="0.883293">
vector b E Rtx1 with the constraint of minimizing
the loss function l,
</bodyText>
<equation confidence="0.8383275">
l(Ytrain, [ 1 Xtrain L W ]),
(1)
</equation>
<bodyText confidence="0.98650775">
where 1 is the all-one column vector. Then we can
predict the label matrix Ytest E Rmxt of m testing
items with respect to the feature matrix Xtest E
Rmxd. Let
</bodyText>
<equation confidence="0.977489">
� Xtrain Ytrain �
Z = .
</equation>
<subsectionHeader confidence="0.585049">
Xtest Ytest
</subsectionHeader>
<bodyText confidence="0.999965">
This linear classification problem can be trans-
formed into completing the unobservable entries
in Ytest by means of the observable entries in
Xtrain, Ytrain and Xtest, based on the assumption
that the rank of matrix Z E R(n+m)x(d+t) is low.
The model can be written as,
</bodyText>
<equation confidence="0.980265833333333">
arg min rank(Z)
ZER(n+m)×(d+t)
s.t. b(i, j) E QX, zij = xij,
(1 &lt; i &lt; n + m, 1 &lt; j &lt; d), (2)
b(i,j) E QY , zi(j+d) = yij,
(1 &lt; i &lt; n, 1 &lt; j &lt; t),
</equation>
<bodyText confidence="0.999904142857143">
where we use QX to represent the index set of ob-
servable feature entries in Xtrain and Xtest, and
QY to denote the index set of observable label en-
tries in Ytrain.
Formula (2) is usually impractical for real prob-
lems as the entries in the matrix Z are corrupted
by noise. We thus define
</bodyText>
<equation confidence="0.993230125">
Z = Z* + E,
where Z* as the underlying low-rank matrix
Z* = [ X* Y * 1 L Xtrain* Y t*rain J
J X* Y * ,
r 1
test test
and E is the error matrix � EXtrain EYtrain �
E = EXtest 0 .
</equation>
<bodyText confidence="0.997967142857143">
The rank function in Formula (2) is a non-convex
function that is difficult to be optimized. The sur-
rogate of the function can be the convex nucle-
ar norm ||Z||* = P σk(Z) (Cand`es and Recht,
2009), where σk is the k-th largest singular val-
ue of Z. To tolerate the noise entries in the error
matrix E, we minimize the cost functions Cx and
Cy for features and labels respectively, rather than
using the hard constraints in Formula (2).
According to Formula (1), Z* E R(n+m)x(d+t)
can be represented as [X*, WX*] instead of
[X*, Y *], by explicitly modeling the bias vector
b. Therefore, this convex optimization model is
called DRMC-b,
</bodyText>
<equation confidence="0.956233875">
arg min
Z,b µ||Z ||* + |1  |XCx(zij, xij)
QX (i,j)EΩX
λ X
+ Cy(zi(j+d) + bj, yij),
|QY |
(i,j)EΩY
(3)
</equation>
<bodyText confidence="0.984507818181818">
where µ and λ are the positive trade-off weights.
More specifically, we minimize the nuclear norm
||Z||* via employing the regularization terms, i.e.,
the cost functions Cx and Cy for features and la-
bels.
If we implicitly model the bias vector b,
Z* E R(n+m)x(1+d+t) can be denoted by
[1, X*, W,X*] instead of [X*, Y *], in which W&apos;
takes the role of [bT; W] in DRMC-b. Then we
derive another optimization model called DRMC-
1,
</bodyText>
<equation confidence="0.960212857142857">
arg min
Z
+ |QY |
λ X
(i,j)EΩY Cy(zi(j+d+1),yij)
s.t. Z(:,1) = 1,
(4)
</equation>
<bodyText confidence="0.987056909090909">
where Z(:,1) denotes the first column of Z.
For our relation classification task, both features
and labels are binary. We assume that the actual
entry u belonging to the underlying matrix Z* is
randomly generated via a sigmoid function (Jor-
dan, 1995): Pr(u|v) = 1/(1 + e−uv), given the
observed binary entry v from the observed sparse
matrix Z. Then, we can apply the log-likelihood
cost function to measure the conditional probabil-
ity and derive the logistic cost function for Cx and
Cy,
</bodyText>
<equation confidence="0.692835">
C(u, v) = − log Pr(u|v) = log(1 + e−uv),
</equation>
<bodyText confidence="0.99972175">
After completing the entries in Ytest, we adop-
t the sigmoid function to calculate the conditional
probability of relation rj, given entity pair pi per-
taining to yij in Ytest,
</bodyText>
<equation confidence="0.994967">
1
Pr(rj|pi) = 1 + e−yij , yij E Ytest.
</equation>
<bodyText confidence="0.684723">
Finally, we can achieve Top-N predicted relation
instances via ranking the values of Pr(rj|pi).
arg min
W,b
</bodyText>
<equation confidence="0.42163225">
1
µ||Z||* +
XCx(zi(j+1), xij)
|QX |(i,j)EΩX
</equation>
<page confidence="0.638627">
842
</page>
<bodyText confidence="0.473753">
.
</bodyText>
<sectionHeader confidence="0.894468" genericHeader="method">
4 Algorithm
</sectionHeader>
<bodyText confidence="0.999933363636364">
The matrix rank minimization problem is NP-
hard. Therefore, Cand´es and Recht (2009) sug-
gested to use a convex relaxation, the nuclear nor-
m minimization instead. Then, Ma et al. (2011)
proposed the fixed point continuation (FPC) algo-
rithm which is fast and robust. Moreover, Gold-
frab and Ma (2011) proved the convergence of the
FPC algorithm for solving the nuclear norm mini-
mization problem. We thus adopt and modify the
algorithm aiming to find the optima for our noise-
tolerant models, i.e., Formulae (3) and (4).
</bodyText>
<subsectionHeader confidence="0.993183">
4.1 Fixed point continuation for DRMC-b
</subsectionHeader>
<bodyText confidence="0.999767666666667">
Algorithm 1 describes the modified FPC algorithm
for solving DRMC-b, which contains two steps for
each iteration,
Gradient step: In this step, we infer the ma-
trix gradient g(Z) and bias vector gradient g(b) as
follows,
</bodyText>
<equation confidence="0.9617734">
−xij
1+exijzij , (i,j) E ΩX
−yi(j−d)
1+eyi(j−d) (f.
+bj) , (i,j− d) E ΩY
0, otherwise
and
λ −yij
g(bj) = |ΩY  |Y_ 1 + eyij(zi(j+d)+bj)
i:(i,j)EQY
</equation>
<bodyText confidence="0.983178615384615">
We use the gradient descents A = Z − τzg(Z)
and b = b − τbg(b) to gradually find the global
minima of the cost function terms in Formula (3),
where τz and τb are step sizes.
Shrinkage step: The goal of this step is to min-
imize the nuclear norm ||Z||* in Formula (3). We
perform the singular value decomposition (SVD)
(Golub and Kahan, 1965) for A at first, and then
cut down each singular value. During the iteration,
any negative value in E − τZµ is assigned by zero,
so that the rank of reconstructed matrix Z will be
reduced, where Z = Umax(E − τZµ, 0)VT.
To accelerate the convergence, we use a con-
tinuation method to improve the speed. µ is ini-
tialized by a large value µ1, thus resulting in the
fast reduction of the rank at first. Then the conver-
gence slows down as µ decreases while obeying
µk+1 = max(µkηµ, µF). µF is the final value of
µ, and ηµ is the decay parameter.
For the stopping criteria in inner iterations, we
define the relative error to measure the residual of
matrix Z between two successive iterations,
Algorithm 1 FPC algorithm for solving DRMC-b
Input:
Initial matrix Z0, bias b0; Parameters µ, λ;
Step sizes τz, τb.
</bodyText>
<equation confidence="0.7749005">
Set Z = Z0, b = b0.
foreach µ = µ1 &gt; µ2 &gt; ... &gt; µF do
while relative error &gt; ε do
Gradient step:
A = Z − τzg(Z), b = b − τbg(b).
Shrinkage step:
UEVT = SVD(A),
Z = U max(E − τZµ, 0) VT.
</equation>
<listItem confidence="0.487715333333333">
end while
end foreach
Output: Completed Matrix Z, bias b.
</listItem>
<bodyText confidence="0.9727075">
max(1, ||Zk||F) &lt;_ ε,
where ε is the convergence threshold.
</bodyText>
<subsectionHeader confidence="0.984176">
4.2 Fixed point continuation for DRMC-1
</subsectionHeader>
<bodyText confidence="0.9997182">
Algorithm 2 is similar to Algorithm 1 except for
two differences. First, there is no bias vector b.
Second, a projection step is added to enforce the
first column of matrix Z to be 1. In addition, The
matrix gradient g(Z) for DRMC-1 is
</bodyText>
<equation confidence="0.888476833333333">
−xi(j−1)
1+exi(j−1)zij , (i,j − 1) E ΩX
−yi(j−d−1)
1+eyi(j−d−1)zij , (i,j − d − 1) E ΩY
0, otherwise
Algorithm 2 FPC algorithm for solving DRMC-1
</equation>
<bodyText confidence="0.609202625">
Input:
Initial matrix Z0; Parameters µ, λ;
Step sizes τz.
Set Z = Z0.
foreach µ = µ1 &gt; µ2 &gt; ... &gt; µF do
while relative error &gt; ε do
Gradient step: A = Z − τzg(Z).
Shrinkage step:
</bodyText>
<equation confidence="0.847561333333333">
UEVT = SVD(A),
Z = U max(E − τZµ, 0) VT.
Projection step: Z(:,1) = 1.
</equation>
<listItem confidence="0.638999666666667">
end while
end foreach
Output: Completed Matrix Z.
</listItem>
<equation confidence="0.944902363636363">
g(zij) = {
1
|QX|
λ
|QY |
||Zk+1 − Zk||F
g(zij) = {
1
|QX|
λ
|QY |
</equation>
<page confidence="0.957909">
843
</page>
<table confidence="0.98999675">
Dataset # of training # of testing % with more # of features # of relation
tuples tuples than one label labels
NYT’10 4,700 1,950 7.5% 244,903 51
NYT’13 8,077 3,716 0% 1,957 51
</table>
<tableCaption confidence="0.995761">
Table 1: Statistics about the two widely used datasets.
</tableCaption>
<table confidence="0.999448666666667">
Model NYT’10 (θ=2) NYT’10 (θ=3) NYT’10 (θ=4) NYT’10 (θ=5) NYT’13
DRMC-b 51.4 f 8.7 (51) 45.6 f 3.4 (46) 41.6 f 2.5 (43) 36.2 f 8.8(37) 84.6 f 19.0 (85)
DRMC-1 16.0 f 1.0 (16) 16.4 f 1.1(17) 16 f 1.4 (17) 16.8 f 1.5(17) 15.8 f 1.6 (16)
</table>
<tableCaption confidence="0.967497">
Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The
threshold θ means filtering the features that appear less than θ times. The values in brackets pertaining to
DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing
sets.
</tableCaption>
<sectionHeader confidence="0.998569" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999001333333334">
In order to conduct reliable experiments, we adjust
and estimate the parameters for our approaches,
DRMC-b and DRMC-1, and compare them with
other four kinds of landmark methods (Mintz et
al., 2009; Hoffmann et al., 2011; Surdeanu et al.,
2012; Riedel et al., 2013) on two public datasets.
</bodyText>
<subsectionHeader confidence="0.936841">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9999778125">
The two widely used datasets that we adopt are
both automatically generated by aligning Freebase
to New York Times corpora. The first dataset12,
NYT’10, was developed by Riedel et al. (2010),
and also used by Hoffmann et al. (2011) and Sur-
deanu et al. (2012). Three kinds of features, name-
ly, lexical, syntactic and named entity tag fea-
tures, were extracted from relation mentions. The
second dataset13, NYT’13, was also released by
Riedel et al. (2013), in which they only regarded
the lexicalized dependency path between two enti-
ties as features. Table 1 shows that the two datasets
differ in some main attributes. More specifically,
NYT’10 contains much higher dimensional fea-
tures than NYT’13, whereas fewer training and
testing items.
</bodyText>
<subsectionHeader confidence="0.998514">
5.2 Parameter setting
</subsectionHeader>
<bodyText confidence="0.995928285714286">
In this part, we address the issue of setting param-
eters: the trade-off weights µ and λ, the step sizes
τz and τb, and the decay parameter ηµ.
We set λ = 1 to make the contribution of the
cost function terms for feature and label matrices
equal in Formulae (3) and (4). µ is assigned by a
series of values obeying µk+1 = max(µkηµ, µF).
</bodyText>
<footnote confidence="0.98045">
12http://iesl.cs.umass.edu/riedel/ecml/
13http://iesl.cs.umass.edu/riedel/data-univSchema/
</footnote>
<bodyText confidence="0.994866769230769">
We follow the suggestion in (Goldberg et al.,
2010) that µ starts at σ1ηµ, and σ1 is the largest
singular value of the matrix Z. We set ηµ = 0.01.
The final value of µ, namely µF, is equal to 0.01.
Ma et al. (2011) revealed that as long as the non-
negative step sizes satisfy τz &lt; min(4|ΩY |
λ , |QZX|)
�b
4|ΩY  |the FPC algorithm will guaran-
and &lt; λ(n+m) ,
tee to converge to a global optimum. Therefore,
we set τz = τb = 0.5 to satisfy the above con-
straints on both two datasets.
</bodyText>
<subsectionHeader confidence="0.996409">
5.3 Rank estimation
</subsectionHeader>
<bodyText confidence="0.999762130434782">
Even though the FPC algorithm converges in iter-
ative fashion, the value of ε varying with different
datasets is difficult to be decided. In practice, we
record the rank of matrix Z at each round of iter-
ation until it converges at a rather small threshold
ε = 10−4. The reason is that we suppose the opti-
mal low-rank representation of the matrix Z con-
veys the truly effective information about underly-
ing semantic correlation between the features and
the corresponding labels.
We use the five-fold cross validation on the val-
idation set and evaluate the performance on each
fold with different ranks. At each round of itera-
tion, we gain a recovered matrix and average the
F114 scores from Top-5 to Top-all predicted rela-
tion instances to measure the performance. Figure
3 illustrates the curves of average F1 scores. After
recording the rank associated with the highest F1
score on each fold, we compute the mean and the
standard deviation to estimate the range of optimal
rank for testing. Table 2 lists the range of optimal
ranks for DRMC-b and DRMC-1 on NYT’10 and
NYT’13.
</bodyText>
<equation confidence="0.6756765">
14F1 = 2×precision×recall
precision+recall
</equation>
<page confidence="0.975585">
844
</page>
<figure confidence="0.999443014084507">
Average−F1
Average−F1
Average−F1
Rank
(a) DRMC-b on NYT’10 validation set (0 = 5).
0 100 200 300 400 500
Rank
(c) DRMC-b on NYT’13 validation set.
Rank
(b) DRMC-1 on NYT’10 validation set (0 = 5).
0.124
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.118
0.116
0.114
0.112
0.11
0.108
0.106
0 100 200 300 400 500
Rank
(d) DRMC-1 on NYT’13 validation set.
0.14
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.134
0.132
0.13
0.128
0.126
0.124
0.122
0 100 200 300 400 500
0.138
0.136
Average−F1
0.114
0.112
0.108
0.106
0.104
0.11
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.14
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.134
0.132
0.13
0.128
0.126
0.124
0 100 200 300 400 500
0.138
0.136
0.122
0.12
</figure>
<figureCaption confidence="0.999972">
Figure 3: Five-fold cross validation for rank estimation on two datasets.
</figureCaption>
<bodyText confidence="0.9999644">
On both two datasets, we observe an identical
phenomenon that the performance gradually in-
creases as the rank of the matrix declines before
reaching the optimum. However, it sharply de-
creases if we continue reducing the optimal rank.
An intuitive explanation is that the high-rank ma-
trix contains much noise and the model tends to be
overfitting, whereas the matrix of excessively low
rank is more likely to lose principal information
and the model tends to be underfitting.
</bodyText>
<subsectionHeader confidence="0.99828">
5.4 Method Comparison
</subsectionHeader>
<bodyText confidence="0.982472333333333">
Firstly, we conduct experiments to compare our
approaches with Mintz-09 (Mintz et al., 2009),
MultiR-11 (Hoffmann et al., 2011), MIML-12 and
MIML-at-least-one-12 (Surdeanu et al., 2012) on
NYT’10 dataset. Surdeanu et al. (2012) released
the open source code15 to reproduce the experi-
mental results on those previous methods. More-
over, their programs can control the feature spar-
15http://nlp.stanford.edu/software/mimlre.shtml
sity degree through a threshold 0 which filters the
features that appears less than 0 times. They set
0 = 5 in the original code by default. Therefore,
we follow their settings and adopt the same way
to filter the features. In this way, we guarantee
the fair comparison for all methods. Figure 4 (a)
shows that our approaches achieve the significant
improvement on performance.
We also perform the experiments to compare
our approaches with the state-of-the-art NFE-1316
(Riedel et al., 2013) and its sub-methods (N-13,
F-13 and NF-13) on NYT’13 dataset. Figure 4 (b)
illustrates that our approaches still outperform the
state-of-the-art methods. In practical application-
s, we also concern about the precision on Top-N
predicted relation instances. Therefore, We com-
pare the precision of Top-100s, Top-200s and Top-
500s for DRMC-1, DRMC-b and the state-of-the-
</bodyText>
<footnote confidence="0.970774">
16Readers may refer to the website,
http://www.riedelcastro.org/uschema for the details of
those methods. We bypass the description due to the
limitation of space.
</footnote>
<page confidence="0.996679">
845
</page>
<figure confidence="0.999664575">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5
Precision
Mintz−09
MultiR−11
MIML−12
MIML−at−least−one−12
DRMC−1(Rank=17)
DRMC−b(Rank=37)
Precision
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.2 0.4 0.6 0.8 1
1
N−13
F−13
NF−13
NFE−13
DRMC−1(Rank=16)
DRMC−b(Rank=85)
0.9
0.8
0.7
Recall Recall
(a) NYT’10 testing set (0 = 5). (b) NYT’13 testing set.
</figure>
<figureCaption confidence="0.995415">
Figure 4: Method comparison on two testing sets.
</figureCaption>
<figure confidence="0.999631076923077">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
00 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall Remll
(a) NYT’10 testing set (0 = 5). (b) NYT’13 testing set.
</figure>
<figureCaption confidence="0.997002">
Figure 5: Precision-Recall curve for DRMC-b and DRMC-1 with different ranks on two testing sets.
</figureCaption>
<figure confidence="0.998020333333333">
Precision
DRMC−1(Rank=1879)
DRMC−b(Rank=1993)
DRMC−1(Rank=1169)
DRMC−b(Rank=1307)
DRMC−1(Rank=384)
DRMC−b(Rank=464)
DRMC−1(Rank=17)
DRMC−b(Rank=37)
Precision
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1
DRMC−1(Rank=1378)
DRMC−b(Rank=1861)
DRMC−1(Rank=719)
DRMC−b(Rank=1703)
DRMC−1(Rank=139)
DRMC−b(Rank=655)
DRMC−1(Rank=16)
DRMC−b(Rank=85)
0.9
0.8
0.7
</figure>
<table confidence="0.9844922">
Top-N NFE-13 DRMC-b DRMC-1
Top-100 62.9% 82.0% 80.0%
Top-200 57.1% 77.0% 80.0%
Top-500 37.2% 70.2% 77.0%
Average 52.4% 76.4% 79.0%
</table>
<tableCaption confidence="0.921352333333333">
Table 3: Precision of NFE-13, DRMC-b and
DRMC-1 on Top-100, Top-200 and Top-500 pre-
dicted relation instances.
</tableCaption>
<bodyText confidence="0.998835">
art method NFE-13 (Riedel et al., 2013). Table 3
shows that DRMC-b and DRMC-1 achieve 24.0%
and 26.6% precision increments on average, re-
spectively.
</bodyText>
<sectionHeader confidence="0.998624" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9998245">
We have mentioned that the basic alignment as-
sumption of distant supervision (Mintz et al.,
2009) tends to generate noisy (noisy features and
incomplete labels) and sparse (sparse features) da-
ta. In this section, we discuss how our approaches
tackle these natural flaws.
Due to the noisy features and incomplete label-
s, the underlying low-rank data matrix with tru-
ly effective information tends to be corrupted and
the rank of observed data matrix can be extremely
high. Figure 5 demonstrates that the ranks of da-
ta matrices are approximately 2,000 for the initial
optimization of DRMC-b and DRMC-1. Howev-
er, those high ranks result in poor performance.
As the ranks decline before approaching the op-
timum, the performance gradually improves, im-
plying that our approaches filter the noise in data
and keep the principal information for classifica-
tion via recovering the underlying low-rank data
matrix.
Furthermore, we discuss the influence of the
feature sparsity for our approaches and the state-
</bodyText>
<page confidence="0.99818">
846
</page>
<figureCaption confidence="0.99713125">
Figure 6: Feature sparsity discussion on NYT’10 testing set. Each row (from top to bottom, 0 = 4, 3, 2)
illustrates a suite of experimental results. They are, from left to right, five-fold cross validation for
rank estimation on DRMC-b and DRMC-1, method comparison and precision-recall curve with different
ranks, respectively.
</figureCaption>
<figure confidence="0.999785573913044">
Rank
Rank
Recall
Recall
Rank
Rank
Recall
Recall
0.14
0.14
1
1
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.9
0.9
0.138
0.138
0.8
0.8
0.136
0.136
0.7
0.7
0.134
0.134
0.6
0.6
0.132
0.5
0.132
0.5
0.13
0.4
0.4
0.13
0.128
0.3
0.3
0.128
0.126
0.2
0.2
0.126
0.124
0.1
0.1
0
0 0.1 0.2 0.3 0.4 0.5
0.122
0 100 200 300 400
0.124
500 0 100 200 300 400 500
0
0 0.2 0.4 0.6 0.8
Precision
Precision
Average−F1
Mintz−09
MultiR−11
MIML−12
MIML−at−least−one−12
DRMC−1(Rank=17)
DRMC−b(Rank=43)
DRMC−1(Rank=2148)
DRMC−b(Rank=2291)
DRMC−1(Rank=1285)
DRMC−b(Rank=1448)
DRMC−1(Rank=404)
DRMC−b(Rank=489)
DRMC−1(Rank=17)
DRMC−b(Rank=43)
Rank
Rank
Recall
Recall
0.14
0.14
1
1
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.9
0.9
0.138
0.138
0.8
0.8
0.136
0.136
0.7
0.7
0.134
0.134
0.6
0.6
0.132
0.5
0.132
0.5
0.13
0.4
0.4
0.13
0.128
0.3
0.3
0.128
0.126
0.2
0.2
0.126
0.124
0.1
0.1
0
0 0.1 0.2 0.3 0.4 0.5
0.122
0 100 200 300 400
0.124
500 0 100 200 300 400 500
0
0 0.2 0.4 0.6 0.8
Precision
Precision
Average−F1
Mintz−09
MultiR−11
MIML−12
MIML−at−least−one−12
DRMC−1(Rank=17)
DRMC−b(Rank=46)
DRMC−1(Rank=2539)
DRMC−b(Rank=2730)
DRMC−1(Rank=1447)
DRMC−b(Rank=1644)
DRMC−1(Rank=433)
DRMC−b(Rank=531)
DRMC−1(Rank=17)
DRMC−b(Rank=46)
0.138
0.136
0.134
0.132
0.128
0.126
0.124
0 100 200 300 400
0.14
0.13
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
0.124
500 0 100 200 300 400 500
Average−F1
0.138
0.136
0.134
0.132
0.128
0.126
0.14
0.13
Fold 1
Fold 2
Fold 3
Fold 4
Fold 5
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 0.1 0.2 0.3 0.4 0.5
1
Mintz−09
MultiR−11
MIML−12
MIML−at−least−one−12
DRMC−1(Rank=16)
DRMC−b(Rank=51)
Precision
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0 0.2 0.4 0.6 0.8 1
DRMC−1(Rank=3186)
DRMC−b(Rank=3444)
DRMC−1(Rank=1728)
DRMC−b(Rank=1991)
DRMC−1(Rank=489)
DRMC−b(Rank=602)
DRMC−1(Rank=16)
DRMC−b(Rank=51)
Average−F1
Average−F1
Average−F1
</figure>
<bodyText confidence="0.998501666666667">
of-the-art methods. We relax the feature filtering
threshold (0 = 4, 3, 2) in Surdeanu et al.’s (2012)
open source program to generate more sparse fea-
tures from NYT’10 dataset. Figure 6 shows that
our approaches consistently outperform the base-
line and the state-of-the-art methods with diverse
feature sparsity degrees. Table 2 also lists the
range of optimal rank for DRMC-b and DRMC-
1 with different 0. We observe that for each ap-
proach, the optimal range is relatively stable. In
other words, for each approach, the amount of tru-
ly effective information about underlying seman-
tic correlation keeps constant for the same dataset,
which, to some extent, explains the reason why our
approaches are robust to sparse features.
</bodyText>
<sectionHeader confidence="0.993451" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9997582">
In this paper, we contributed two noise-tolerant
optimization models17, DRMC-b and DRMC-1,
for distantly supervised relation extraction task
from a novel perspective. Our models are based on
matrix completion with low-rank criterion. Exper-
</bodyText>
<footnote confidence="0.713683">
17The source code can be downloaded from https://
github.com/nlpgeek/DRMC/tree/master
</footnote>
<bodyText confidence="0.999911777777778">
iments demonstrated that the low-rank represen-
tation of the feature-label matrix can exploit the
underlying semantic correlated information for re-
lation classification and is effective to overcome
the difficulties incurred by sparse and noisy fea-
tures and incomplete labels, so that we achieved
significant improvements on performance.
Our proposed models also leave open question-
s for distantly supervised relation extraction task.
First, they can not process new coming testing
items efficiently, as we have to reconstruct the data
matrix containing not only the testing items but al-
so all the training items for relation classification,
and compute in iterative fashion again. Second,
the volume of the datasets we adopt are relatively
small. For the future work, we plan to improve our
models so that they will be capable of incremental
learning on large-scale datasets (Chang, 2011).
</bodyText>
<sectionHeader confidence="0.998073" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.94685675">
This work is supported by National Program on
Key Basic Research Project (973 Program) under
Grant 2013CB329304, National Science Founda-
tion of China (NSFC) under Grant No.61373075.
</bodyText>
<page confidence="0.996152">
847
</page>
<sectionHeader confidence="0.993854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867576576577">
Nguyen Bach and Sameer Badaskar. 2007. A review
of relation extraction. Literature review for Lan-
guage and Statistics II.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general
human knowledge. In Proceedings of the nation-
al conference on Artificial Intelligence, volume 22,
page 1962. AAAI Press.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.
Ricardo S Cabral, Fernando Torre, Jo˜ao P Costeira, and
Alexandre Bernardino. 2011. Matrix completion
for multi-label image classification. In Advances in
Neural Information Processing Systems, pages 190–
198.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717–
772.
Edward Y Chang. 2011. Foundations of Large-Scale
Multimedia Information Management and Retrieval.
Springer.
Michael Collins, Sanjoy Dasgupta, and Robert E
Schapire. 2001. A generalization of principal com-
ponents analysis to the exponential family. In Ad-
vances in neural information processing systems,
pages 617–624.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77–86.
Maryam Fazel, Haitham Hindi, and Stephen P Boyd.
2001. A rank minimization heuristic with applica-
tion to minimum order system approximation. In
American Control Conference, 2001. Proceedings of
the 2001, volume 6, pages 4734–4739. IEEE.
Andrew Goldberg, Ben Recht, Junming Xu, Robert
Nowak, and Xiaojin Zhu. 2010. Transduction with
matrix completion: Three birds with one stone. In
Advances in neural information processing systems,
pages 757–765.
Donald Goldfarb and Shiqian Ma. 2011. Conver-
gence of fixed-point continuation algorithms for ma-
trix rank minimization. Foundations of Computa-
tional Mathematics, 11(2):183–210.
Gene Golub and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial &amp; Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205–224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541–550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Michael Jordan. 1995. Why the logistic function? a
tutorial discussion on probabilities and neural net-
works. Computational Cognitive Science Technical
Report.
Yehuda Koren. 2008. Factorization meets the neigh-
borhood: a multifaceted collaborative filtering mod-
el. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 426–434. ACM.
Shiqian Ma, Donald Goldfarb, and Lifeng Chen. 2011.
Fixed point and bregman iterative methods for ma-
trix rank minimization. Mathematical Program-
ming, 128(1-2):321–353.
Oded Maron and Tom´as Lozano-P´erez. 1998. A
framework for multiple-instance learning. Advances
in neural information processing systems, pages
570–576.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777–782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003–1011. Association for
Computational Linguistics.
Jasson DM Rennie and Nathan Srebro. 2005. Fast
maximum margin matrix factorization for collabora-
tive prediction. In Proceedings of the 22nd interna-
tional conference on Machine learning, pages 713–
719. ACM.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their mention-
s without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148–163.
Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
</reference>
<page confidence="0.979205">
848
</page>
<reference confidence="0.999811525">
pages 74–84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Process-
ing Systems 17.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455–
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
Annual Meeting of the Association for Computation-
al Linguistics: Long Papers-Volume 1, pages 721–
729. Association for Computational Linguistics.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 665–670, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s (Volume 2: Short Papers), pages 810–815, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
427–434. Association for Computational Linguistic-
s.
</reference>
<page confidence="0.999024">
849
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.861286">
<title confidence="0.97759">Distant Supervision for Relation Extraction with Matrix Completion</title>
<author confidence="0.880381">Deli Qiang Zhiyuan Thomas Fang Edward Y Division of Technical Innovation</author>
<affiliation confidence="0.994644666666667">Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, of Computer Science and Technology, Tsinghua University, Beijing Advanced Technology and Research Center,</affiliation>
<abstract confidence="0.999556035714286">The essence of distantly supervised relaextraction is that it is an classification problem with s- To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-byfeature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nguyen Bach</author>
<author>Sameer Badaskar</author>
</authors>
<title>A review of relation extraction. Literature review for Language and Statistics II.</title>
<date>2007</date>
<contexts>
<context position="1804" citStr="Bach and Badaskar, 2007" startWordPosition="255" endWordPosition="258">w-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods. 1 Introduction Relation Extraction (RE) is the process of generating structured relation knowledge from unstructured natural language texts. Traditional supervised methods (Zhou et al., 2005; Bach and Badaskar, 2007) on small hand-labeled corpora, such as MUC1 and ACE2, can achieve high precision and recall. However, as producing handlabeled corpora is laborius and expensive, the supervised approach can not satisfy the increasing 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 2http://www.itl.nist.gov/iad/mig/tests/ace/ Figure 1: Training corpus generated by the basic alignment assumption of distantly supervised relation extraction. The relation instances are the triples related to President Barack Obama in the Freebase, and the relation mentions are some sentences describing him in the Wikiped</context>
</contexts>
<marker>Bach, Badaskar, 2007</marker>
<rawString>Nguyen Bach and Sameer Badaskar. 2007. A review of relation extraction. Literature review for Language and Statistics II.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Robert Cook</author>
<author>Patrick Tufts</author>
</authors>
<title>Freebase: A shared database of structured general human knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the national conference on Artificial Intelligence,</booktitle>
<volume>22</volume>
<pages>page</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="8800" citStr="Bollacker et al., 2007" startWordPosition="1282" endWordPosition="1285">rmatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P´erez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentenc</context>
</contexts>
<marker>Bollacker, Cook, Tufts, 2007</marker>
<rawString>Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007. Freebase: A shared database of structured general human knowledge. In Proceedings of the national conference on Artificial Intelligence, volume 22, page 1962. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8775" citStr="Bollacker et al., 2008" startWordPosition="1278" endWordPosition="1281"> in the field of bioinformatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P´erez, 1998), Riedel et al. (2010) relaxed the strong assumption</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ricardo S Cabral</author>
<author>Fernando Torre</author>
<author>Jo˜ao P Costeira</author>
<author>Alexandre Bernardino</author>
</authors>
<title>Matrix completion for multi-label image classification.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>190--198</pages>
<contexts>
<context position="10988" citStr="Cabral et al., 2011" startWordPosition="1638" endWordPosition="1641">ls, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cand`es and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem. The new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels. 3.1 Formulation Suppose that we have built a training corpus for relation classification with n items (entity pairs), d-dimensional textual features, and t labels (relations)</context>
</contexts>
<marker>Cabral, Torre, Costeira, Bernardino, 2011</marker>
<rawString>Ricardo S Cabral, Fernando Torre, Jo˜ao P Costeira, and Alexandre Bernardino. 2011. Matrix completion for multi-label image classification. In Advances in Neural Information Processing Systems, pages 190– 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel J Cand`es</author>
<author>Benjamin Recht</author>
</authors>
<title>Exact matrix completion via convex optimization.</title>
<date>2009</date>
<journal>Foundations of Computational mathematics,</journal>
<volume>9</volume>
<issue>6</issue>
<pages>772</pages>
<marker>Cand`es, Recht, 2009</marker>
<rawString>Emmanuel J Cand`es and Benjamin Recht. 2009. Exact matrix completion via convex optimization. Foundations of Computational mathematics, 9(6):717– 772.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Y Chang</author>
</authors>
<date>2011</date>
<booktitle>Foundations of Large-Scale Multimedia Information Management and Retrieval.</booktitle>
<publisher>Springer.</publisher>
<marker>Chang, 2011</marker>
<rawString>Edward Y Chang. 2011. Foundations of Large-Scale Multimedia Information Management and Retrieval. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Sanjoy Dasgupta</author>
<author>Robert E Schapire</author>
</authors>
<title>A generalization of principal components analysis to the exponential family. In Advances in neural information processing systems,</title>
<date>2001</date>
<pages>617--624</pages>
<contexts>
<context position="10406" citStr="Collins et al., 2001" startWordPosition="1543" endWordPosition="1546">ce multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.’s (2013) which considered the task as a matrix factorization problem. Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cand`es and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender syst</context>
</contexts>
<marker>Collins, Dasgupta, Schapire, 2001</marker>
<rawString>Michael Collins, Sanjoy Dasgupta, and Robert E Schapire. 2001. A generalization of principal components analysis to the exponential family. In Advances in neural information processing systems, pages 617–624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Johan Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In ISMB,</booktitle>
<volume>volume</volume>
<pages>77--86</pages>
<contexts>
<context position="8211" citStr="Craven and Kumlien, 1999" startWordPosition="1189" endWordPosition="1192">es and incomplete labels, due to that it is suitable for binary variables. We also modify the fixed point continuation (FPC) algorithm (Ma et al., 2011) to find the global optimum. Experiments on two widely used datasets demonstrate that our noise-tolerant approaches outperform the baseline and the state-of-the-art methods. Furthermore, we discuss the influence of feature sparsity, and our approaches consistently achieve better performance than compared methods under different sparsity degrees. 2 Related Work The idea of distant supervision was firstly proposed in the field of bioinformatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-s</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>Mark Craven and Johan Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In ISMB, volume 1999, pages 77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryam Fazel</author>
<author>Haitham Hindi</author>
<author>Stephen P Boyd</author>
</authors>
<title>A rank minimization heuristic with application to minimum order system approximation.</title>
<date>2001</date>
<booktitle>In American Control Conference,</booktitle>
<volume>6</volume>
<pages>4734--4739</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="11078" citStr="Fazel et al., 2001" startWordPosition="1651" endWordPosition="1654">they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cand`es and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem. The new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels. 3.1 Formulation Suppose that we have built a training corpus for relation classification with n items (entity pairs), d-dimensional textual features, and t labels (relations), based on the basic alignment assumption proposed by Mintz et al. (2009). Let Xtrain ∈ Rn</context>
</contexts>
<marker>Fazel, Hindi, Boyd, 2001</marker>
<rawString>Maryam Fazel, Haitham Hindi, and Stephen P Boyd. 2001. A rank minimization heuristic with application to minimum order system approximation. In American Control Conference, 2001. Proceedings of the 2001, volume 6, pages 4734–4739. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Goldberg</author>
<author>Ben Recht</author>
<author>Junming Xu</author>
<author>Robert Nowak</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Transduction with matrix completion: Three birds with one stone.</title>
<date>2010</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>757--765</pages>
<contexts>
<context position="11186" citStr="Goldberg et al. (2010)" startWordPosition="1668" endWordPosition="1671"> We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cand`es and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem. The new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels. 3.1 Formulation Suppose that we have built a training corpus for relation classification with n items (entity pairs), d-dimensional textual features, and t labels (relations), based on the basic alignment assumption proposed by Mintz et al. (2009). Let Xtrain ∈ Rn×d and Ytrain ∈ Rn×t denote the feature matrix and the label matrix for training, respectively. The linear c</context>
<context position="20702" citStr="Goldberg et al., 2010" startWordPosition="3415" endWordPosition="3418">ore specifically, NYT’10 contains much higher dimensional features than NYT’13, whereas fewer training and testing items. 5.2 Parameter setting In this part, we address the issue of setting parameters: the trade-off weights µ and λ, the step sizes τz and τb, and the decay parameter ηµ. We set λ = 1 to make the contribution of the cost function terms for feature and label matrices equal in Formulae (3) and (4). µ is assigned by a series of values obeying µk+1 = max(µkηµ, µF). 12http://iesl.cs.umass.edu/riedel/ecml/ 13http://iesl.cs.umass.edu/riedel/data-univSchema/ We follow the suggestion in (Goldberg et al., 2010) that µ starts at σ1ηµ, and σ1 is the largest singular value of the matrix Z. We set ηµ = 0.01. The final value of µ, namely µF, is equal to 0.01. Ma et al. (2011) revealed that as long as the nonnegative step sizes satisfy τz &lt; min(4|ΩY | λ , |QZX|) �b 4|ΩY |the FPC algorithm will guaranand &lt; λ(n+m) , tee to converge to a global optimum. Therefore, we set τz = τb = 0.5 to satisfy the above constraints on both two datasets. 5.3 Rank estimation Even though the FPC algorithm converges in iterative fashion, the value of ε varying with different datasets is difficult to be decided. In practice, we</context>
</contexts>
<marker>Goldberg, Recht, Xu, Nowak, Zhu, 2010</marker>
<rawString>Andrew Goldberg, Ben Recht, Junming Xu, Robert Nowak, and Xiaojin Zhu. 2010. Transduction with matrix completion: Three birds with one stone. In Advances in neural information processing systems, pages 757–765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Goldfarb</author>
<author>Shiqian Ma</author>
</authors>
<title>Convergence of fixed-point continuation algorithms for matrix rank minimization.</title>
<date>2011</date>
<journal>Foundations of Computational Mathematics,</journal>
<volume>11</volume>
<issue>2</issue>
<marker>Goldfarb, Ma, 2011</marker>
<rawString>Donald Goldfarb and Shiqian Ma. 2011. Convergence of fixed-point continuation algorithms for matrix rank minimization. Foundations of Computational Mathematics, 11(2):183–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gene Golub</author>
<author>William Kahan</author>
</authors>
<title>Calculating the singular values and pseudo-inverse of a matrix.</title>
<date>1965</date>
<journal>Journal of the Society for Industrial &amp; Applied Mathematics, Series B: Numerical Analysis,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="16467" citStr="Golub and Kahan, 1965" startWordPosition="2641" endWordPosition="2644">s two steps for each iteration, Gradient step: In this step, we infer the matrix gradient g(Z) and bias vector gradient g(b) as follows, −xij 1+exijzij , (i,j) E ΩX −yi(j−d) 1+eyi(j−d) (f. +bj) , (i,j− d) E ΩY 0, otherwise and λ −yij g(bj) = |ΩY |Y_ 1 + eyij(zi(j+d)+bj) i:(i,j)EQY We use the gradient descents A = Z − τzg(Z) and b = b − τbg(b) to gradually find the global minima of the cost function terms in Formula (3), where τz and τb are step sizes. Shrinkage step: The goal of this step is to minimize the nuclear norm ||Z||* in Formula (3). We perform the singular value decomposition (SVD) (Golub and Kahan, 1965) for A at first, and then cut down each singular value. During the iteration, any negative value in E − τZµ is assigned by zero, so that the rank of reconstructed matrix Z will be reduced, where Z = Umax(E − τZµ, 0)VT. To accelerate the convergence, we use a continuation method to improve the speed. µ is initialized by a large value µ1, thus resulting in the fast reduction of the rank at first. Then the convergence slows down as µ decreases while obeying µk+1 = max(µkηµ, µF). µF is the final value of µ, and ηµ is the decay parameter. For the stopping criteria in inner iterations, we define the</context>
</contexts>
<marker>Golub, Kahan, 1965</marker>
<rawString>Gene Golub and William Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial &amp; Applied Mathematics, Series B: Numerical Analysis, 2(2):205–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>541--550</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2615" citStr="Hoffmann et al., 2011" startWordPosition="371" endWordPosition="374">n not satisfy the increasing 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 2http://www.itl.nist.gov/iad/mig/tests/ace/ Figure 1: Training corpus generated by the basic alignment assumption of distantly supervised relation extraction. The relation instances are the triples related to President Barack Obama in the Freebase, and the relation mentions are some sentences describing him in the Wikipedia. demand of building large-scale knowledge repositories with the explosion of Web texts. To address the lacking training data issue, we consider the distant (Mintz et al., 2009) or weak (Hoffmann et al., 2011) supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper. The intuition of the paradigm is that one can take advantage of several knowledge bases, such as WordNet3, Freebase4 and YAGO5, to automatically label free texts, like Wikipedia6 and New York Times corpora7, based on some heuristic alignment assumptions. An example accounting for the basic but practical assumption is illustrated in Figure 1, in which we know that the two entities (&lt;Barack Obama, U.S.&gt;) are not only involved in the relation instances8 coming from knowledge bases (President-of(Barac</context>
<context position="9453" citStr="Hoffmann et al. (2011)" startWordPosition="1385" endWordPosition="1389">nowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P´erez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity pairs have more than one relation. They extend11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion ed the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed mor</context>
<context position="19380" citStr="Hoffmann et al., 2011" startWordPosition="3201" endWordPosition="3204">6.0 f 1.0 (16) 16.4 f 1.1(17) 16 f 1.4 (17) 16.8 f 1.5(17) 15.8 f 1.6 (16) Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The threshold θ means filtering the features that appear less than θ times. The values in brackets pertaining to DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing sets. 5 Experiments In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. 5.1 Dataset The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora. The first dataset12, NYT’10, was developed by Riedel et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT’13, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path bet</context>
<context position="23629" citStr="Hoffmann et al., 2011" startWordPosition="3947" endWordPosition="3950"> both two datasets, we observe an identical phenomenon that the performance gradually increases as the rank of the matrix declines before reaching the optimum. However, it sharply decreases if we continue reducing the optimal rank. An intuitive explanation is that the high-rank matrix contains much noise and the model tends to be overfitting, whereas the matrix of excessively low rank is more likely to lose principal information and the model tends to be underfitting. 5.4 Method Comparison Firstly, we conduct experiments to compare our approaches with Mintz-09 (Mintz et al., 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT’10 dataset. Surdeanu et al. (2012) released the open source code15 to reproduce the experimental results on those previous methods. Moreover, their programs can control the feature spar15http://nlp.stanford.edu/software/mimlre.shtml sity degree through a threshold 0 which filters the features that appears less than 0 times. They set 0 = 5 in the original code by default. Therefore, we follow their settings and adopt the same way to filter the features. In this way, we guarantee the fair comparison for all methods. Figure 4 (a) s</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 541–550, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Jordan</author>
</authors>
<title>Why the logistic function? a tutorial discussion on probabilities and neural networks.</title>
<date>1995</date>
<journal>Computational Cognitive Science</journal>
<tech>Technical Report.</tech>
<contexts>
<context position="14544" citStr="Jordan, 1995" startWordPosition="2300" endWordPosition="2302">tion terms, i.e., the cost functions Cx and Cy for features and labels. If we implicitly model the bias vector b, Z* E R(n+m)x(1+d+t) can be denoted by [1, X*, W,X*] instead of [X*, Y *], in which W&apos; takes the role of [bT; W] in DRMC-b. Then we derive another optimization model called DRMC1, arg min Z + |QY | λ X (i,j)EΩY Cy(zi(j+d+1),yij) s.t. Z(:,1) = 1, (4) where Z(:,1) denotes the first column of Z. For our relation classification task, both features and labels are binary. We assume that the actual entry u belonging to the underlying matrix Z* is randomly generated via a sigmoid function (Jordan, 1995): Pr(u|v) = 1/(1 + e−uv), given the observed binary entry v from the observed sparse matrix Z. Then, we can apply the log-likelihood cost function to measure the conditional probability and derive the logistic cost function for Cx and Cy, C(u, v) = − log Pr(u|v) = log(1 + e−uv), After completing the entries in Ytest, we adopt the sigmoid function to calculate the conditional probability of relation rj, given entity pair pi pertaining to yij in Ytest, 1 Pr(rj|pi) = 1 + e−yij , yij E Ytest. Finally, we can achieve Top-N predicted relation instances via ranking the values of Pr(rj|pi). arg min W,</context>
</contexts>
<marker>Jordan, 1995</marker>
<rawString>Michael Jordan. 1995. Why the logistic function? a tutorial discussion on probabilities and neural networks. Computational Cognitive Science Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yehuda Koren</author>
</authors>
<title>Factorization meets the neighborhood: a multifaceted collaborative filtering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>426--434</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10448" citStr="Koren, 2008" startWordPosition="1550" endWordPosition="1551">ich jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.’s (2013) which considered the task as a matrix factorization problem. Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cand`es and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system co</context>
</contexts>
<marker>Koren, 2008</marker>
<rawString>Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 426–434. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqian Ma</author>
<author>Donald Goldfarb</author>
<author>Lifeng Chen</author>
</authors>
<title>Fixed point and bregman iterative methods for matrix rank minimization.</title>
<date>2011</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>128--1</pages>
<contexts>
<context position="7738" citStr="Ma et al., 2011" startWordPosition="1122" endWordPosition="1125">correlated. The low-rank factorization of the sparse feature-label matrix delivers the low-dimensional representation of de-correlation for features and labels. 840 We contribute two optimization models, DRMC11-b and DRMC-1, aiming at exploiting the sparsity to recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously. Moreover, the logistic cost function is integrated in our models to reduce the influence of noisy features and incomplete labels, due to that it is suitable for binary variables. We also modify the fixed point continuation (FPC) algorithm (Ma et al., 2011) to find the global optimum. Experiments on two widely used datasets demonstrate that our noise-tolerant approaches outperform the baseline and the state-of-the-art methods. Furthermore, we discuss the influence of feature sparsity, and our approaches consistently achieve better performance than compared methods under different sparsity degrees. 2 Related Work The idea of distant supervision was firstly proposed in the field of bioinformatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news </context>
<context position="15391" citStr="Ma et al. (2011)" startWordPosition="2449" endWordPosition="2452">and Cy, C(u, v) = − log Pr(u|v) = log(1 + e−uv), After completing the entries in Ytest, we adopt the sigmoid function to calculate the conditional probability of relation rj, given entity pair pi pertaining to yij in Ytest, 1 Pr(rj|pi) = 1 + e−yij , yij E Ytest. Finally, we can achieve Top-N predicted relation instances via ranking the values of Pr(rj|pi). arg min W,b 1 µ||Z||* + XCx(zi(j+1), xij) |QX |(i,j)EΩX 842 . 4 Algorithm The matrix rank minimization problem is NPhard. Therefore, Cand´es and Recht (2009) suggested to use a convex relaxation, the nuclear norm minimization instead. Then, Ma et al. (2011) proposed the fixed point continuation (FPC) algorithm which is fast and robust. Moreover, Goldfrab and Ma (2011) proved the convergence of the FPC algorithm for solving the nuclear norm minimization problem. We thus adopt and modify the algorithm aiming to find the optima for our noisetolerant models, i.e., Formulae (3) and (4). 4.1 Fixed point continuation for DRMC-b Algorithm 1 describes the modified FPC algorithm for solving DRMC-b, which contains two steps for each iteration, Gradient step: In this step, we infer the matrix gradient g(Z) and bias vector gradient g(b) as follows, −xij 1+ex</context>
<context position="20865" citStr="Ma et al. (2011)" startWordPosition="3451" endWordPosition="3454">the issue of setting parameters: the trade-off weights µ and λ, the step sizes τz and τb, and the decay parameter ηµ. We set λ = 1 to make the contribution of the cost function terms for feature and label matrices equal in Formulae (3) and (4). µ is assigned by a series of values obeying µk+1 = max(µkηµ, µF). 12http://iesl.cs.umass.edu/riedel/ecml/ 13http://iesl.cs.umass.edu/riedel/data-univSchema/ We follow the suggestion in (Goldberg et al., 2010) that µ starts at σ1ηµ, and σ1 is the largest singular value of the matrix Z. We set ηµ = 0.01. The final value of µ, namely µF, is equal to 0.01. Ma et al. (2011) revealed that as long as the nonnegative step sizes satisfy τz &lt; min(4|ΩY | λ , |QZX|) �b 4|ΩY |the FPC algorithm will guaranand &lt; λ(n+m) , tee to converge to a global optimum. Therefore, we set τz = τb = 0.5 to satisfy the above constraints on both two datasets. 5.3 Rank estimation Even though the FPC algorithm converges in iterative fashion, the value of ε varying with different datasets is difficult to be decided. In practice, we record the rank of matrix Z at each round of iteration until it converges at a rather small threshold ε = 10−4. The reason is that we suppose the optimal low-rank</context>
</contexts>
<marker>Ma, Goldfarb, Chen, 2011</marker>
<rawString>Shiqian Ma, Donald Goldfarb, and Lifeng Chen. 2011. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oded Maron</author>
<author>Tom´as Lozano-P´erez</author>
</authors>
<title>A framework for multiple-instance learning. Advances in neural information processing systems,</title>
<date>1998</date>
<pages>570--576</pages>
<marker>Maron, Lozano-P´erez, 1998</marker>
<rawString>Oded Maron and Tom´as Lozano-P´erez. 1998. A framework for multiple-instance learning. Advances in neural information processing systems, pages 570–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant supervision for relation extraction with an incomplete knowledge base.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>777--782</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="10001" citStr="Min et al., 2013" startWordPosition="1472" endWordPosition="1475">ced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity pairs have more than one relation. They extend11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion ed the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.’s (2013) which considered the task as a matrix factorization problem. Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the fiel</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation extraction with an incomplete knowledge base. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 777–782, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>2</volume>
<pages>1003--1011</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2583" citStr="Mintz et al., 2009" startWordPosition="365" endWordPosition="368">e, the supervised approach can not satisfy the increasing 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 2http://www.itl.nist.gov/iad/mig/tests/ace/ Figure 1: Training corpus generated by the basic alignment assumption of distantly supervised relation extraction. The relation instances are the triples related to President Barack Obama in the Freebase, and the relation mentions are some sentences describing him in the Wikipedia. demand of building large-scale knowledge repositories with the explosion of Web texts. To address the lacking training data issue, we consider the distant (Mintz et al., 2009) or weak (Hoffmann et al., 2011) supervision paradigm attractive, and we improve the effectiveness of the paradigm in this paper. The intuition of the paradigm is that one can take advantage of several knowledge bases, such as WordNet3, Freebase4 and YAGO5, to automatically label free texts, like Wikipedia6 and New York Times corpora7, based on some heuristic alignment assumptions. An example accounting for the basic but practical assumption is illustrated in Figure 1, in which we know that the two entities (&lt;Barack Obama, U.S.&gt;) are not only involved in the relation instances8 coming from kno</context>
<context position="8734" citStr="Mintz et al. (2009)" startWordPosition="1271" endWordPosition="1274">stant supervision was firstly proposed in the field of bioinformatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P´erez, 1998), Riedel et</context>
<context position="11661" citStr="Mintz et al. (2009)" startWordPosition="1739" endWordPosition="1742">tem controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem. The new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels. 3.1 Formulation Suppose that we have built a training corpus for relation classification with n items (entity pairs), d-dimensional textual features, and t labels (relations), based on the basic alignment assumption proposed by Mintz et al. (2009). Let Xtrain ∈ Rn×d and Ytrain ∈ Rn×t denote the feature matrix and the label matrix for training, respectively. The linear classifier we adopt aims to explicitly learn the weight matrix W ∈ Rd×t and the bias column 841 vector b E Rtx1 with the constraint of minimizing the loss function l, l(Ytrain, [ 1 Xtrain L W ]), (1) where 1 is the all-one column vector. Then we can predict the label matrix Ytest E Rmxt of m testing items with respect to the feature matrix Xtest E Rmxd. Let � Xtrain Ytrain � Z = . Xtest Ytest This linear classification problem can be transformed into completing the unobse</context>
<context position="19357" citStr="Mintz et al., 2009" startWordPosition="3197" endWordPosition="3200">f 19.0 (85) DRMC-1 16.0 f 1.0 (16) 16.4 f 1.1(17) 16 f 1.4 (17) 16.8 f 1.5(17) 15.8 f 1.6 (16) Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The threshold θ means filtering the features that appear less than θ times. The values in brackets pertaining to DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing sets. 5 Experiments In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. 5.1 Dataset The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora. The first dataset12, NYT’10, was developed by Riedel et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT’13, was also released by Riedel et al. (2013), in which they only regarded the lexicali</context>
<context position="23594" citStr="Mintz et al., 2009" startWordPosition="3942" endWordPosition="3945">k estimation on two datasets. On both two datasets, we observe an identical phenomenon that the performance gradually increases as the rank of the matrix declines before reaching the optimum. However, it sharply decreases if we continue reducing the optimal rank. An intuitive explanation is that the high-rank matrix contains much noise and the model tends to be overfitting, whereas the matrix of excessively low rank is more likely to lose principal information and the model tends to be underfitting. 5.4 Method Comparison Firstly, we conduct experiments to compare our approaches with Mintz-09 (Mintz et al., 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT’10 dataset. Surdeanu et al. (2012) released the open source code15 to reproduce the experimental results on those previous methods. Moreover, their programs can control the feature spar15http://nlp.stanford.edu/software/mimlre.shtml sity degree through a threshold 0 which filters the features that appears less than 0 times. They set 0 = 5 in the original code by default. Therefore, we follow their settings and adopt the same way to filter the features. In this way, we guarantee the fair compari</context>
<context position="26470" citStr="Mintz et al., 2009" startWordPosition="4384" endWordPosition="4387">1861) DRMC−1(Rank=719) DRMC−b(Rank=1703) DRMC−1(Rank=139) DRMC−b(Rank=655) DRMC−1(Rank=16) DRMC−b(Rank=85) 0.9 0.8 0.7 Top-N NFE-13 DRMC-b DRMC-1 Top-100 62.9% 82.0% 80.0% Top-200 57.1% 77.0% 80.0% Top-500 37.2% 70.2% 77.0% Average 52.4% 76.4% 79.0% Table 3: Precision of NFE-13, DRMC-b and DRMC-1 on Top-100, Top-200 and Top-500 predicted relation instances. art method NFE-13 (Riedel et al., 2013). Table 3 shows that DRMC-b and DRMC-1 achieve 24.0% and 26.6% precision increments on average, respectively. 6 Discussion We have mentioned that the basic alignment assumption of distant supervision (Mintz et al., 2009) tends to generate noisy (noisy features and incomplete labels) and sparse (sparse features) data. In this section, we discuss how our approaches tackle these natural flaws. Due to the noisy features and incomplete labels, the underlying low-rank data matrix with truly effective information tends to be corrupted and the rank of observed data matrix can be extremely high. Figure 5 demonstrates that the ranks of data matrices are approximately 2,000 for the initial optimization of DRMC-b and DRMC-1. However, those high ranks result in poor performance. As the ranks decline before approaching the</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jasson DM Rennie</author>
<author>Nathan Srebro</author>
</authors>
<title>Fast maximum margin matrix factorization for collaborative prediction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>713--719</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11034" citStr="Rennie and Srebro, 2005" startWordPosition="1644" endWordPosition="1647"> collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the field of applied mathematics, i.e., low-rank matrix completion with convex optimization. The breakthrough work on this topic was made by Cand`es and Recht (2009) who proved that most low-rank matrices can be perfectly recovered from an incomplete set of entries. This promising theory has been successfully applied on many active research areas, such as computer vision (Cabral et al., 2011), recommender system (Rennie and Srebro, 2005) and system controlling (Fazel et al., 2001). Our models for relation extraction are based on the theoretic framework proposed by Goldberg et al. (2010), which formulated the multi-label transductive learning as a matrix completion problem. The new framework for classification enhances the robustness to data noise by penalizing different cost functions for features and labels. 3.1 Formulation Suppose that we have built a training corpus for relation classification with n items (entity pairs), d-dimensional textual features, and t labels (relations), based on the basic alignment assumption prop</context>
</contexts>
<marker>Rennie, Srebro, 2005</marker>
<rawString>Jasson DM Rennie and Nathan Srebro. 2005. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages 713– 719. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
</authors>
<title>Modeling relations and their mentions without labeled text.</title>
<date>2010</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases,</booktitle>
<pages>148--163</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9345" citStr="Riedel et al. (2010)" startWordPosition="1368" endWordPosition="1371">l. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing knowledge base online which contains billions of relation instances and thousands of relation names, to distantly supervise Wikipedia corpus. The basic alignment assumption of this work is that if a pair of entities participate in a relation, all sentences that mention these entities are labeled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P´erez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity pairs have more than one relation. They extend11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion ed the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Oth</context>
<context position="19652" citStr="Riedel et al. (2010)" startWordPosition="3246" endWordPosition="3249">ining to DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing sets. 5 Experiments In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. 5.1 Dataset The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora. The first dataset12, NYT’10, was developed by Riedel et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT’13, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path between two entities as features. Table 1 shows that the two datasets differ in some main attributes. More specifically, NYT’10 contains much higher dimensional features than NYT’13, whereas fewer training and testing items. 5.2 Parameter setting In this part, we address the</context>
</contexts>
<marker>Riedel, Yao, McCallum, 2010</marker>
<rawString>Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, pages 148–163. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>74--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="19425" citStr="Riedel et al., 2013" startWordPosition="3209" endWordPosition="3212">.8 f 1.5(17) 15.8 f 1.6 (16) Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The threshold θ means filtering the features that appear less than θ times. The values in brackets pertaining to DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing sets. 5 Experiments In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. 5.1 Dataset The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora. The first dataset12, NYT’10, was developed by Riedel et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT’13, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path between two entities as features. Table 1 shows </context>
<context position="24420" citStr="Riedel et al., 2013" startWordPosition="4068" endWordPosition="4071">on those previous methods. Moreover, their programs can control the feature spar15http://nlp.stanford.edu/software/mimlre.shtml sity degree through a threshold 0 which filters the features that appears less than 0 times. They set 0 = 5 in the original code by default. Therefore, we follow their settings and adopt the same way to filter the features. In this way, we guarantee the fair comparison for all methods. Figure 4 (a) shows that our approaches achieve the significant improvement on performance. We also perform the experiments to compare our approaches with the state-of-the-art NFE-1316 (Riedel et al., 2013) and its sub-methods (N-13, F-13 and NF-13) on NYT’13 dataset. Figure 4 (b) illustrates that our approaches still outperform the state-of-the-art methods. In practical applications, we also concern about the precision on Top-N predicted relation instances. Therefore, We compare the precision of Top-100s, Top-200s and Top500s for DRMC-1, DRMC-b and the state-of-the16Readers may refer to the website, http://www.riedelcastro.org/uschema for the details of those methods. We bypass the description due to the limitation of space. 845 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 Prec</context>
<context position="26250" citStr="Riedel et al., 2013" startWordPosition="4349" endWordPosition="4352">Rank=1993) DRMC−1(Rank=1169) DRMC−b(Rank=1307) DRMC−1(Rank=384) DRMC−b(Rank=464) DRMC−1(Rank=17) DRMC−b(Rank=37) Precision 0.6 0.5 0.4 0.3 0.2 0.1 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 DRMC−1(Rank=1378) DRMC−b(Rank=1861) DRMC−1(Rank=719) DRMC−b(Rank=1703) DRMC−1(Rank=139) DRMC−b(Rank=655) DRMC−1(Rank=16) DRMC−b(Rank=85) 0.9 0.8 0.7 Top-N NFE-13 DRMC-b DRMC-1 Top-100 62.9% 82.0% 80.0% Top-200 57.1% 77.0% 80.0% Top-500 37.2% 70.2% 77.0% Average 52.4% 76.4% 79.0% Table 3: Precision of NFE-13, DRMC-b and DRMC-1 on Top-100, Top-200 and Top-500 predicted relation instances. art method NFE-13 (Riedel et al., 2013). Table 3 shows that DRMC-b and DRMC-1 achieve 24.0% and 26.6% precision increments on average, respectively. 6 Discussion We have mentioned that the basic alignment assumption of distant supervision (Mintz et al., 2009) tends to generate noisy (noisy features and incomplete labels) and sparse (sparse features) data. In this section, we discuss how our approaches tackle these natural flaws. Due to the noisy features and incomplete labels, the underlying low-rank data matrix with truly effective information tends to be corrupted and the rank of observed data matrix can be extremely high. Figure</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74–84, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems 17.</booktitle>
<contexts>
<context position="8231" citStr="Snow et al. (2004)" startWordPosition="1193" endWordPosition="1196">ue to that it is suitable for binary variables. We also modify the fixed point continuation (FPC) algorithm (Ma et al., 2011) to find the global optimum. Experiments on two widely used datasets demonstrate that our noise-tolerant approaches outperform the baseline and the state-of-the-art methods. Furthermore, we discuss the influence of feature sparsity, and our approaches consistently achieve better performance than compared methods under different sparsity degrees. 2 Related Work The idea of distant supervision was firstly proposed in the field of bioinformatics (Craven and Kumlien, 1999). Snow et al. (2004) used WordNet as the knowledge base to discover more hpyernym/hyponym relations between entities from news articles. However, either bioinformatic database or WordNet is maintained by a few experts, thus hardly kept up-to-date. As we are stepping into the big data era, the explosion of unstructured Web texts simulates us to build more powerful models that can automatically extract relation instances from large-scale online natural language corpora without handlabeled annotation. Mintz et al. (2009) adopted Freebase (Bollacker et al., 2008; Bollacker et al., 2007), a large-scale crowdsourcing k</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. Advances in Neural Information Processing Systems 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Julie Tibshirani</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Multi-instance multi-label learning for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>455--465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9743" citStr="Surdeanu et al. (2012)" startWordPosition="1429" endWordPosition="1433">beled by that relation name. Then we can extract a variety of textual features and learn a multi-class logistic regression classifier. Inspired by multi-instance learning (Maron and Lozano-P´erez, 1998), Riedel et al. (2010) relaxed the strong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity pairs have more than one relation. They extend11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion ed the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.’s (2013) which considered the task as a matrix factorization problem. Their approach is</context>
<context position="19403" citStr="Surdeanu et al., 2012" startWordPosition="3205" endWordPosition="3208">.1(17) 16 f 1.4 (17) 16.8 f 1.5(17) 15.8 f 1.6 (16) Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The threshold θ means filtering the features that appear less than θ times. The values in brackets pertaining to DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing sets. 5 Experiments In order to conduct reliable experiments, we adjust and estimate the parameters for our approaches, DRMC-b and DRMC-1, and compare them with other four kinds of landmark methods (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2013) on two public datasets. 5.1 Dataset The two widely used datasets that we adopt are both automatically generated by aligning Freebase to New York Times corpora. The first dataset12, NYT’10, was developed by Riedel et al. (2010), and also used by Hoffmann et al. (2011) and Surdeanu et al. (2012). Three kinds of features, namely, lexical, syntactic and named entity tag features, were extracted from relation mentions. The second dataset13, NYT’13, was also released by Riedel et al. (2013), in which they only regarded the lexicalized dependency path between two entities as fe</context>
<context position="23687" citStr="Surdeanu et al., 2012" startWordPosition="3954" endWordPosition="3957">t the performance gradually increases as the rank of the matrix declines before reaching the optimum. However, it sharply decreases if we continue reducing the optimal rank. An intuitive explanation is that the high-rank matrix contains much noise and the model tends to be overfitting, whereas the matrix of excessively low rank is more likely to lose principal information and the model tends to be underfitting. 5.4 Method Comparison Firstly, we conduct experiments to compare our approaches with Mintz-09 (Mintz et al., 2009), MultiR-11 (Hoffmann et al., 2011), MIML-12 and MIML-at-least-one-12 (Surdeanu et al., 2012) on NYT’10 dataset. Surdeanu et al. (2012) released the open source code15 to reproduce the experimental results on those previous methods. Moreover, their programs can control the feature spar15http://nlp.stanford.edu/software/mimlre.shtml sity degree through a threshold 0 which filters the features that appears less than 0 times. They set 0 = 5 in the original code by default. Therefore, we follow their settings and adopt the same way to filter the features. In this way, we guarantee the fair comparison for all methods. Figure 4 (a) shows that our approaches achieve the significant improveme</context>
</contexts>
<marker>Surdeanu, Tibshirani, Nallapati, Manning, 2012</marker>
<rawString>Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi-label learning for relation extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 455– 465. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shingo Takamatsu</author>
<author>Issei Sato</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>Reducing wrong labels in distant supervision for relation extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>721--729</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9983" citStr="Takamatsu et al., 2012" startWordPosition="1468" endWordPosition="1471">ong assumption and replaced all sentences with at least one sentence. Hoffmann et al. (2011) pointed out that many entity pairs have more than one relation. They extend11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion ed the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.’s (2013) which considered the task as a matrix factorization problem. Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new tec</context>
</contexts>
<marker>Takamatsu, Sato, Nakagawa, 2012</marker>
<rawString>Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa. 2012. Reducing wrong labels in distant supervision for relation extraction. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 721– 729. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Raphael Hoffmann</author>
<author>Le Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Filling knowledge base gaps for distant supervision of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>665--670</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Xu, Hoffmann, Le Zhao, Grishman, 2013</marker>
<rawString>Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Grishman. 2013. Filling knowledge base gaps for distant supervision of relation extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 665–670, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingxing Zhang</author>
<author>Jianwen Zhang</author>
<author>Junyu Zeng</author>
<author>Jun Yan</author>
<author>Zheng Chen</author>
<author>Zhifang Sui</author>
</authors>
<title>Towards accurate distant supervision for relational facts extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>810--815</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="10021" citStr="Zhang et al., 2013" startWordPosition="1476" endWordPosition="1479">with at least one sentence. Hoffmann et al. (2011) pointed out that many entity pairs have more than one relation. They extend11It is the abbreviation for Distant supervision for Relation extraction with Matrix Completion ed the multi-instance learning framework (Riedel et al., 2010) to the multi-label circumstance. Surdeanu et al. (2012) proposed a novel approach to multi-instance multi-label learning for relation extraction, which jointly modeled all the sentences in texts and all labels in knowledge bases for a given entity pair. Other literatures (Takamatsu et al., 2012; Min et al., 2013; Zhang et al., 2013; Xu et al., 2013) addressed more specific issues, like how to construct the negative class in learning or how to adopt more information, such as name entity tags, to improve the performance. Our work is more relevant to Riedel et al.’s (2013) which considered the task as a matrix factorization problem. Their approach is composed of several models, such as PCA (Collins et al., 2001) and collaborative filtering (Koren, 2008). However, they did not concern about the data noise brought by the basic assumption of distant supervision. 3 Model We apply a new technique in the field of applied mathema</context>
</contexts>
<marker>Zhang, Zhang, Zeng, Yan, Chen, Sui, 2013</marker>
<rawString>Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zhifang Sui. 2013. Towards accurate distant supervision for relational facts extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 810–815, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Jie Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>427--434</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1778" citStr="Zhou et al., 2005" startWordPosition="251" endWordPosition="254">r the underlying low-rank matrix leveraging the sparsity of feature-label matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods. 1 Introduction Relation Extraction (RE) is the process of generating structured relation knowledge from unstructured natural language texts. Traditional supervised methods (Zhou et al., 2005; Bach and Badaskar, 2007) on small hand-labeled corpora, such as MUC1 and ACE2, can achieve high precision and recall. However, as producing handlabeled corpora is laborius and expensive, the supervised approach can not satisfy the increasing 1http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 2http://www.itl.nist.gov/iad/mig/tests/ace/ Figure 1: Training corpus generated by the basic alignment assumption of distantly supervised relation extraction. The relation instances are the triples related to President Barack Obama in the Freebase, and the relation mentions are some sentences des</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427–434. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>