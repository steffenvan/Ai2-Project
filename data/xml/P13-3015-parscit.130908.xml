<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023390">
<title confidence="0.99724">
A Comparison of Techniques to Automatically Identify Complex Words
</title>
<author confidence="0.998436">
Matthew Shardlow
</author>
<affiliation confidence="0.7642335">
School of Computer Science, University of Manchester
IT301, Kilburn Building, Manchester, M13 9PL, England
</affiliation>
<email confidence="0.995078">
m.shardlow@cs.man.ac.uk
</email>
<sectionHeader confidence="0.993811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966612903226">
Identifying complex words (CWs) is an
important, yet often overlooked, task
within lexical simplification (The process
of automatically replacing CWs with sim-
pler alternatives). If too many words are
identified then substitutions may be made
erroneously, leading to a loss of mean-
ing. If too few words are identified then
those which impede a user’s understand-
ing may be missed, resulting in a com-
plex final text. This paper addresses the
task of evaluating different methods for
CW identification. A corpus of sentences
with annotated CWs is mined from Sim-
ple Wikipedia edit histories, which is then
used as the basis for several experiments.
Firstly, the corpus design is explained and
the results of the validation experiments
using human judges are reported. Exper-
iments are carried out into the CW identi-
fication techniques of: simplifying every-
thing, frequency thresholding and training
a support vector machine. These are based
upon previous approaches to the task and
show that thresholding does not perform
significantly differently to the more naive
technique of simplifying everything. The
support vector machine achieves a slight
increase in precision over the other two
methods, but at the cost of a dramatic trade
off in recall.
</bodyText>
<sectionHeader confidence="0.999331" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998273244444444">
Complex Word (CW) identification is an impor-
tant task at the first stage of lexical simplification
and errors introduced or avoided here will affect
final results. This work looks at the process of au-
tomatically identifying difficult words for a lexi-
cal simplification system. Lexical simplification
is the task of identifying and replacing CWs in a
text to improve the overall understandability and
readability. This is a difficult task which is com-
putationally expensive and often inadequately ac-
curate.
Lexical simplification is just one method of
text simplification and is often deployed alongside
other simplification methods (Carrol et al., 1998;
Alu´ısio and Gasperin, 2010). Syntactic simplifi-
cation, statistical machine translation and seman-
tic simplification (or explanation generation) are
all current methods of text simplification. Text
simplification is typically deployed as an assistive
technology (Devlin and Tait, 1998; Alu´ısio and
Gasperin, 2010), although this is not always the
case. It may also be used alongside other tech-
nologies such as summarisation to improve their
final results.
Identifying CWs is a task which every lexical
simplification system must perform, either explic-
itly or implicitly, before simplification can take
place. CWs are difficult to define, which makes
them difficult to identify. For example, take the
following sentence:
The four largest islands are Honshu,
Hokkaido, Shikoku, and Kyushu, and
there are approximately 3,000 smaller
islands in the chain.
In the above sentence, we might identify the
proper nouns (Honshu, Hokkaido, etc.) as com-
plex (as they may be unfamiliar) or we may choose
to discount them from our scheme altogether, as
proper nouns are unlikely to have any valid re-
placements. If we discount the proper nouns then
the other valid CW would be ‘approximately’. At
13 characters it is more than twice the average of
5.7 characters per word and has more syllables
than any other word. Further, CWs are often iden-
tified by their frequency (see Section 2.1) and here,
</bodyText>
<page confidence="0.99082">
103
</page>
<note confidence="0.515209">
Proceedings of the ACL Student Research Workshop, pages 103–109,
</note>
<page confidence="0.363443">
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</page>
<bodyText confidence="0.999580193548387">
‘approximately’ exhibits a much lower frequency
than the other words.
There are many reasons to evaluate the identi-
fication of CWs. This research stems primarily
from the discovery that no previous comparison of
current techniques exists. It is hoped that by pro-
viding this, the community will be able to iden-
tify and evaluate new techniques using the meth-
ods proposed herein. If CW identification is not
performed well, then potential candidates may be
missed, and simple words may be falsely identi-
fied. This is dangerous as simplification will often
result in a minor change in a text’s semantics. For
example, the sentence:
The United Kingdom is a state in
northwest Europe.
May be simplified to give:
The United Kingdom is a country in
northwest Europe.
In this example from the corpus used in this
research, the word “state” is simplified to give
“country”. Whilst this is a valid synonym in the
given context, state and country are not necessar-
ily semantically identical. Broadly speaking, state
refers to a political entity, whereas country refers
to a physical space within a set of borders. This is
an acceptable change and even necessary for sim-
plification. However, if applied blindly, then too
many modifications may be made, resulting in ma-
jor deviations from the text’s original semantics.
The contributions of this paper are as follows:
</bodyText>
<listItem confidence="0.999868222222222">
• A report on the corpus developed and used in
the evaluation phase. Section 2.2.
• The implementation of a support vector ma-
chine for the classification of CWs. Section
2.6
• A comparison of common techniques on the
same corpus. Section 4.
• An analysis of the features used in the sup-
port vector machine. Section 4.
</listItem>
<sectionHeader confidence="0.985342" genericHeader="method">
2 Experimental Design
</sectionHeader>
<bodyText confidence="0.927415">
Several systems for detecting CWs were imple-
mented and evaluated using the CW corpus. The
two main techniques that exist in the literature
are simplifying everything (Devlin and Tait, 1998)
</bodyText>
<table confidence="0.9990802">
System Score
SUBTLEX 0.3352
Wikipedia Baseline 0.3270
Kucera-Francis 0.3097
Random Baseline 0.0157
</table>
<tableCaption confidence="0.996731">
Table 1: The results of different exper-
</tableCaption>
<bodyText confidence="0.998096866666667">
iments on the SemEval lexical simplifi-
cation data. These show that SUBTLEX
was the best word frequency measure for
rating lexical complexity. The other en-
tries correspond to alternative word fre-
quency measures. The Google Web 1T
data (Brants and Franz, 2006) has been
shown to give a higher score, however this
data was not available during the course
of this research.
and frequency based thresholding (Zeng et al.,
2005). These were implemented as well as a sup-
port vector machine classifier. This section de-
scribes the design decisions made during imple-
mentation.
</bodyText>
<subsectionHeader confidence="0.996898">
2.1 Lexical Complexity
</subsectionHeader>
<bodyText confidence="0.999968391304348">
All three of the implementations described in Sec-
tions 2.4, 2.5 and 2.6 require a word frequency
measure as an indicator of lexical complexity. If a
word occurs frequently in common language then
it is more likely to be recognised (Rayner and
Duffy, 1986).
The lexical simplification dataset from Task 1
at SemEval 2012 (De Belder and Moens, 2012)
was used to compare several measures of word
frequency as shown in Table 1. Candidate sub-
stitutions and sample sentences were provided by
the task organisers, together with a gold standard
ranking of the substitutes according to their sim-
plicity. These sentences were ranked according
to their frequency. Although the scores in Table
1 appear to be low, this is the kappa agreement
for several categories and so should be expected.
The inter-annotator agreement on the corpus was
0.488 (De Belder and Moens, 2012). The SUB-
TLEX dataset (Brysbaert and New, 2009) was the
best available for rating word familiarity. This is
a corpus of over 70,000 words collected from the
subtitles of over 8,000 American English films.
</bodyText>
<page confidence="0.99447">
104
</page>
<subsectionHeader confidence="0.980216">
2.2 CW Corpus
</subsectionHeader>
<bodyText confidence="0.999935307692308">
Simple Wikipedia edit histories were mined using
techniques similar to those in Yatskar et al. (2010).
This provided aligned pairs of sentences which
had just one word simplified. Whereas Yatskar
et al. (2010) used these pairs to learn probabili-
ties of paraphrases, the research in this paper used
them as instances of lexical simplification. The
original simplifications were performed by editors
trying to make documents as simple as possible.
The CW is identified by comparison with the sim-
plified sentence. Further information on the pro-
duction of the corpus will be published in a future
paper.
</bodyText>
<subsectionHeader confidence="0.9946">
2.3 Negative Examples
</subsectionHeader>
<bodyText confidence="0.999996451612903">
The CW corpus provides a set of CWs in appro-
priate contexts. This is useful for evaluation as
these words need to be identified. However, if
only examples of CWs were available, it would be
very easy for a technique to overfit — as it could
just classify every single word as complex and
get 100% accuracy. For example, in the case of
thresholding, if only examples of CWs are avail-
able, the threshold could be set artificially high
and still succeed for every case. When this is ap-
plied to genuine data it will classify every word it
encounters as complex, leading to high recall but
low precision.
To alleviate this effect, negative examples are
needed. These are examples of simple words
which do not require any further simplification.
There are several methods for finding these, in-
cluding: selecting words from a reference easy
word list; selecting words with high frequencies
according to some corpus or using the simplified
words from the second sentences in the CW cor-
pus. The chosen strategy picked a word at random
from the sentence in which the CW occurs. Only
one word was edited in this sentence and so the
assumption may be made that none of the other
words in the sentence require further simplifica-
tion. Only one simple word per CW is chosen to
enforce an even amount of positive and negative
data. This gave a set of negative words which were
reflective of the broad language which is expected
when processing free text.
</bodyText>
<subsectionHeader confidence="0.99623">
2.4 Simplify Everything
</subsectionHeader>
<bodyText confidence="0.99999465">
The first implementation involved simplifying ev-
erything, a brute force method, in which a simpli-
fication algorithm is applied to every word. This
assumes that words which are already simple will
not require any further simplification. A com-
mon variation is to limit the simplification to some
combination of all the nouns, verbs and adjectives.
A standard baseline lexical simplification sys-
tem was implemented following Devlin and Tait
(1998). This algorithm generated a set of syn-
onyms from WordNet and then used the SUB-
TLEX frequencies to find the most frequent syn-
onym. If the synonym was more frequent than the
original word then a substitution was made. This
technique was applied to all the words. If a CW
was changed, then it was considered a true posi-
tive; if a simple word was not changed, it was con-
sidered a true negative. Five trials were carried out
and the average accuracy and standard deviation is
reported in Figure 1 and Table 3.
</bodyText>
<subsectionHeader confidence="0.992189">
2.5 Frequency Thresholding
</subsectionHeader>
<bodyText confidence="0.9999893">
The second technique is frequency thresholding.
This relies on each word having an associated fa-
miliarity value provided by the SUBTLEX corpus.
Whilst this corpus is large, it will never cover ev-
ery possible word, and so words which are not en-
countered are considered to have a frequency of 0.
This does not affect comparison as the infrequent
words are likely to be the complex ones.
To distinguish between complex and simple
words a threshold was implemented. This was
learnt from the CW corpus by examining every
possible threshold for a training set. Firstly, the
training data was ordered by frequency, then the
accuracy1 of the algorithm was examined with the
threshold placed in between the frequency of every
adjacent pair of words in the ordered list. This was
repeated by 5-fold cross validation and the mean
threshold determined. The final accuracy of the
algorithm was then determined on a separate set
of testing data.
</bodyText>
<subsectionHeader confidence="0.996969">
2.6 Support Vector Machine
</subsectionHeader>
<bodyText confidence="0.99964825">
Support vector machines (SVM) are statistical
classifiers which use labelled training data to pre-
dict the class of unseen inputs. The training data
consist of several features which the SVM uses
to distinguish between classes. The SVM was
chosen as it has been used elsewhere for similar
tasks (Gasperin et al., 2009; Hancke et al., 2012;
Jauhar and Specia, 2012). The use of many fea-
</bodyText>
<footnote confidence="0.788135">
1The proportion of data that was correctly classified.
</footnote>
<page confidence="0.99712">
105
</page>
<bodyText confidence="0.996589333333333">
tures allows factors which may otherwise have
been missed to be taken into account. One fur-
ther advantage is that the features of an SVM can
be analysed to determine their effect on the classi-
fication. This may give some indication for future
feature classification schemes.
The SVM was trained using the LIBSVM pack-
age (Chang and Lin, 2011) in Matlab. the RBF
kernel was selected and a grid search was per-
formed to select values for the 2 parameters C and
γ. Training and testing was performed on a held-
out data-set using 5-fold cross validation.
To implement the SVM a set of features was
determined for the classification scheme. Several
external libraries were used to extract these as de-
tailed below:
Frequency The SUBTLEX frequency of each
word was used as previously described in
Section 2.1.
CD Count Also from the SUBTLEX corpus. The
number of films in which a word appeared,
ranging from 0 − 8, 388.
Length The word length in number of characters
was taken into account. It is often the case
that longer words are more difficult to pro-
cess and so may be considered ‘complex’.
Syllable Count The number of syllables con-
tained in a word is also a good estimate of
its complexity. This was computed using a
library from the morphadorner package2.
Sense Count A count of the number of ways in
which a word can be interpreted - showing
how ambiguous a word is. This measure is
taken from WordNet (Fellbaum, 1998).
Synonym Count Also taken from WordNet, this
is the number of potential synonyms with
which a word could be replaced. This again
may give some indication of a word’s degree
of ambiguity.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.9715108">
The results of the experiments in identifying CWs
are shown in Figure 1 and the values are given in
Table 3. The values presented are the mean of 5
trials and the error bars represent the standard de-
viation.
</bodyText>
<footnote confidence="0.956816">
2http://morphadorner.northwestern.edu/
</footnote>
<figureCaption confidence="0.9656704">
Figure 1: A bar chart with error bars
showing the results of the CW identifi-
cation experiments. Accuracy, F1 Score,
Precision and Recall are reported for each
measure.
</figureCaption>
<table confidence="0.997545428571429">
Feature Coefficient
Frequency 0.3973
CD Count 0.5847
Length −0.5661
Syllables −0.4414
Senses −0.0859
Synonyms −0.2882
</table>
<tableCaption confidence="0.991842">
Table 2: The correlation coefficients for
</tableCaption>
<bodyText confidence="0.990995714285714">
each feature. These show the correlation
against the language’s simplicity and so
a positive correlation indicates that if that
feature is higher then the word will be
simpler.
To analyse the features of the SVM, the corre-
lation coefficient between each feature vector and
the vector of feature labels was calculated. This is
a measure which can be used to show the relation
between two distributions. The adopted labelling
scheme assigned CWs as 0 and simple words as 1
and so the correlation of the features is notionally
against the simplicity of the words.3 The results
are reported in Table 2.
</bodyText>
<sectionHeader confidence="0.999672" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9972234">
It is clear from these results that there is a fairly
high accuracy from all the methods. This shows
that they perform well at the task in hand, reflect-
ing the methods which have been previously ap-
plied. These methods all have a higher recall than
</bodyText>
<footnote confidence="0.5732275">
3i.e. A positive correlation indicates that if the value of
that feature is higher, the word will be simpler.
</footnote>
<figure confidence="0.99618025">
Accuracy
F1
Precision
Recall
Everything Thresholding SVM
Score
0.8
0.6
0.4
0.2
0
1
</figure>
<page confidence="0.988223">
106
</page>
<table confidence="0.9996355">
System Accuracy F1 Precision Recall
Simplify Everything 0.8207 f 0.0077 0.8474 f 0.0056 0.7375 f 0.0084 0.9960 f 0
Thresholding 0.7854 f 0.0138 0.8189 f 0.0098 0.7088 f 0.0136 0.9697 f 0.0056
SVM 0.8012 f 0.0656 0.8130 f 0.0658 0.7709 f 0.0752 0.8665 f 0.0961
</table>
<tableCaption confidence="0.999881">
Table 3: The results of classification experiments for the three systems.
</tableCaption>
<bodyText confidence="0.99993275308642">
precision, which indicates that they are good at
identifying the CWs, but also that they often iden-
tify simple words as CWs. This is particularly
noticeable in the ‘simplify everything’ method,
where the recall is very high, yet the precision is
comparatively low. This indicates that many of the
simple words which are falsely identified as com-
plex are also replaced with an alternate substitu-
tion, which may result in a change in sense.
A paired t-test showed the difference between
the thresholding method and the ‘simplify ev-
erything’ method was not statistically significant
(p &gt; 0.8). Thresholding takes more data about
the words into account and would appear to be a
less naive strategy than blindly simplifying every-
thing. However, this data shows there is little dif-
ference between the results of the two methods.
The thresholding here may be limited by the re-
sources, and a corpus using a larger word count
may yield an improved result.
Whilst the thresholding and simplify everything
methods were not significantly different from each
other, the SVM method was significantly differ-
ent from the other two (p &lt; 0.001). This can be
seen in the slightly lower recall, yet higher preci-
sion attained by the SVM. This indicates that the
SVM was better at distinguishing between com-
plex and simple words, but also wrongly identified
many CWs. The results for the SVM have a wide
standard deviation (shown in the wide error bars in
Figure 1) indicating a higher variability than the
other methods. With more data for training the
model, this variability may be reduced.
One important factor in the increased precision
observed in the SVM is that it used many more
features than the other methods, and so took more
information into account. Table 2 shows that these
features had varying degrees of correlation with
the data label (i.e. whether the word was simple
or not) and hence that they had varying degrees of
effect on the classification scheme.
Frequency and CD count are moderately posi-
tively correlated as may be expected. This indi-
cates that higher frequency words are likely to be
simple. Surprisingly, CD Count has a higher cor-
relation than frequency itself, indicating that this is
a better measure of word familiarity than the fre-
quency measure. However, further investigation is
necessary to confirm this.
Word length and number of syllables are mod-
erately negatively correlated, indicating that the
longer and more polysyllabic a word is, the less
simple it becomes. This is not true in every case.
For example, ‘finger’ and ‘digit’ can be used in
the same sense (as a noun meaning an appendage
of the hand). Whilst ‘finger’ is more commonly
used than ‘digit’4, digit is one letter shorter.
The number of senses was very weakly nega-
tively correlated with word simplicity. This in-
dicates that it is not a strong indicative factor in
determining whether a word is simple or not. The
total number of synonyms was a stronger indicator
than the number of senses, but still only exhibited
weak correlation.
One area that has not been explored in this study
is the use of contextual features. Each target word
occurs in a sentence and it may be the case that
those words surrounding the target give extra in-
formation as to its complexity. It has been sug-
gested that language is produced at an even level
of complexity (Specia et al., 2012), and so simple
words will occur in the presence of other simple
words, whereas CWs will occur in the presence
of other CWs. As well as lexical contextual in-
formation, the surrounding syntax may offer some
information on word difficulty. Factors such as
a very long sentence or a complex grammatical
structure can make a word more difficult to under-
stand. These could be used to modify the familiar-
ity score in the thresholding method, or they could
be used as features in the SVM classifier.
</bodyText>
<sectionHeader confidence="0.999829" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9488655">
This research will be used for lexical simplifica-
tion. The related work in this field is also generally
</bodyText>
<footnote confidence="0.9942735">
4in the SUBTLEX corpus ‘finger’ has a frequency of
1870, whereas ‘digit’ has a frequency of 30.
</footnote>
<page confidence="0.997971">
107
</page>
<bodyText confidence="0.999978418181818">
used as a precursor to lexical simplification. This
section will explain how these previous methods
have handled the task of identifying CWs and how
these fit into the research presented in this paper.
The simplest way to identify CWs in a sentence
is to blindly assume that every word is complex, as
described earlier in Section 2.4. This was first used
in Devlin’s seminal work on lexical simplification
(Devlin and Tait, 1998). This method is some-
what naive as it does not mitigate the possibility
of words being simplified in error. Devlin and Tait
indicate that they believe less frequent words will
not be subject to meaning change. However, fur-
ther work into lexical simplification has refuted
this (Lal and R¨uger, 2002). This method is still
used, for example Thomas and Anderson (2012)
simplify all nouns and verbs. This corresponds to
the ‘Everything’ method.
Another method of identifying CWs is to use
frequency based thresholding over word familiar-
ity scores, as described in Section 2.5 and corre-
sponding to the ‘Frequency’ method in this pa-
per. This has been applied to the medical domain
(Zeng et al., 2005; Elhadad, 2006) for predicting
which words lay readers will find difficult. This
has been correlated with word difficulty via ques-
tionnaires (Zeng et al., 2005; Zeng-Treitler et al.,
2008) and via the analysis of low-level readabil-
ity corpora (Elhadad, 2006). In both these cases,
a familiarity score is used to determine how likely
a subject is to understand a term. More recently,
Bott et al. (2012) use a threshold of 1% corpus
frequency, along with other checks, to ensure that
simple words are not erroneously simplified.
Support vector machines are powerful statisti-
cal classifiers, as employed in the ‘SVM’ method
of this paper. A Support Vector Machine is used
to predict the familiarity of CWs in Zeng et al.
(2005). It takes features of term frequency and
word length and is correlated against the familiar-
ity scores which are already obtained. This proves
to have very poor performance, something which
the authors attribute to a lack of suitable train-
ing data. An SVM has also been trained for the
ranking of words according to their complexity
(Jauhar and Specia, 2012). This was done for the
SemEval lexical simplification task (Specia et al.,
2012). Although this system is designed for syn-
onym ranking, it could also be used for the CW
identification task. Machine learning has also been
applied to the task of determining whether an en-
tire sentence requires simplification (Gasperin et
al., 2009; Hancke et al., 2012). These approaches
use a wide array of morphological features which
are suited to sentence level classification.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999986483870968">
This work is intended as an initial study of meth-
ods for identifying CWs for simplification. The
methods compared, whilst typical of current CW
identification methods, are not an exhaustive set
and variations exist. One further way of expanding
this research would be to take into account word
context. This could be done using thresholding
(Zeng-Treitler et al., 2008) or an SVM (Gasperin
et al., 2009; Jauhar and Specia, 2012).
Another way to increase the accuracy of the fre-
quency count method may be to use a larger cor-
pus. Whilst the corpus used in this paper per-
formed well in the preliminary testing section,
other research has shown the Google Web1T cor-
pus (a n-gram count of over a trillion words) to be
more effective (De Belder and Moens, 2012). The
Web 1T data was not available during the course
of this research.
The large variability in accuracy shown in the
SVM method indicates that there was insufficient
training data. With more data, the SVM would
have more information about the classification
task and would provide more consistent results.
CW identification is the first step in the process
of lexical simplification. This research will be in-
tegrated in a future system which will simplify
natural language for end users. It is also hoped
that other lexical simplification systems will take
account of this work and will use the evaluation
technique proposed herein to improve their identi-
fication of CWs.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999966083333333">
This paper has provided an insight into the chal-
lenges associated with evaluating the identifica-
tion of CWs. This is a non-obvious task, which
may seem intuitively easy, but in reality is quite
difficult and rarely performed. It is hoped that
new research in this field will evaluate the tech-
niques used, rather than using inadequate tech-
niques blindly and naively. This research has also
shown that the current state of the art methods
have much room for improvement. Low precision
is a constant factor in all techniques and future re-
search should aim to address this.
</bodyText>
<page confidence="0.998766">
108
</page>
<sectionHeader confidence="0.974963" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999549333333333">
This work is supported by EPSRC grant
EP/I028099/1. Thanks go to the anonymous
reviewers for their helpful suggestions.
</bodyText>
<sectionHeader confidence="0.988403" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997085453608247">
Sandra Maria Alu´ısio and Caroline Gasperin. 2010.
Fostering digital inclusion and accessibility: the
PorSimples project for simplification of Portuguese
texts. In Proceedings of the NAACL HLT 2010
Young Investigators Workshop on Computational
Approaches to Languages of the Americas, YIW-
CALA ’10, pages 46–53, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefan Bott, Luz Rello, Biljana Drndarevix, and Hora-
cio Saggion. 2012. Can spanish be simpler? lex-
sis: Lexical simplification for spanish. In Coling
2012: The 24th International Conference on Com-
putational Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1.
Marc Brysbaert and Boris New. 2009. Moving beyond
Kucera and Francis : a critical evaluation of current
word frequency norms and the introduction of a new
and improved word frequency measure for Ameri-
can English. Behav Res Methods.
John Carrol, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplifica-
tion of english newspaper text to assist aphasic read-
ers. In AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Jan De Belder and Marie-Francine Moens. 2012. A
dataset for the evaluation of lexical simplification.
In Computational Linguistics and Intelligent Text
Processing, volume 7182 of Lecture Notes in Com-
puter Science, pages 426–437. Springer Berlin / Hei-
delberg.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text
for aphasic readers. Linguistic Databases, pages
161–173.
No´emie Elhadad. 2006. Comprehending techni-
cal texts: Predicting and defining unfamiliar terms.
In AMIA Annual Symposium proceedings, volume
2006, page 239. American Medical Informatics As-
sociation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Caroline Gasperin, Lucia Specia, Tiago Pereira, and
Sandra M. Alu´ısio. 2009. Learning when to sim-
plify sentences for natural text simplification. In En-
contro Nacional de Inteligˆencia Artificial.
Julia Hancke, Sowmya Vajjala, and Detmar Meurers.
2012. Readability classification for German using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1063–1080, Mumbai, India.
Sujay Kumar Jauhar and Lucia Specia. 2012. Uow-
shef: Simplex – lexical simplicity ranking based on
contextual and psycholinguistic features. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval2012), pages
477–481, Montr´eal, Canada, 7-8 June. Association
for Computational Linguistics.
Partha Lal and Stefan R¨uger. 2002. Extract-based
summarization with simplification. In Proceedings
of the ACL.
Keith Rayner and Susan Duffy. 1986. Lexical com-
plexity and fixation times in reading: Effects of word
frequency, verb complexity, and lexical ambiguity.
Memory &amp; Cognition, 14:191–201.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In First Joint Conference on Lexical
and Computational Semantics.
S. Rebecca Thomas and Sven Anderson. 2012.
Wordnet-based lexical simplification of a document.
In Jeremy Jancsary, editor, Proceedings of KON-
VENS 2012, pages 80–88. ¨OGAI, September. Main
track: oral presentations.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simpli-
fications from Wikipedia. In Proceedings of the
NAACL.
Qing Zeng, Eunjung Kim, Jon Crowell, and Tony Tse.
2005. A text corpora-based estimation of the famil-
iarity of health terminology. Biological and Medical
Data Analysis, pages 184–192.
Qing Zeng-Treitler, Sergey Goryachev, Tony Tse, Alla
Keselman, and Aziz Boxwala. 2008. Estimat-
ing consumer familiarity with health terminology: a
context-based approach. Journal of the American
Medical Informatics Association, 15(3):349–356.
</reference>
<page confidence="0.99896">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.476928">
<title confidence="0.999942">A Comparison of Techniques to Automatically Identify Complex Words</title>
<author confidence="0.996561">Matthew</author>
<affiliation confidence="0.999289">School of Computer Science, University of</affiliation>
<address confidence="0.486446">IT301, Kilburn Building, Manchester, M13 9PL,</address>
<email confidence="0.996074">m.shardlow@cs.man.ac.uk</email>
<abstract confidence="0.9995290625">Identifying complex words (CWs) is an important, yet often overlooked, task within lexical simplification (The process of automatically replacing CWs with simpler alternatives). If too many words are identified then substitutions may be made erroneously, leading to a loss of meaning. If too few words are identified then those which impede a user’s understanding may be missed, resulting in a complex final text. This paper addresses the task of evaluating different methods for CW identification. A corpus of sentences with annotated CWs is mined from Simple Wikipedia edit histories, which is then used as the basis for several experiments. Firstly, the corpus design is explained and the results of the validation experiments using human judges are reported. Experiments are carried out into the CW identification techniques of: simplifying everything, frequency thresholding and training a support vector machine. These are based upon previous approaches to the task and show that thresholding does not perform significantly differently to the more naive technique of simplifying everything. The support vector machine achieves a slight increase in precision over the other two methods, but at the cost of a dramatic trade off in recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sandra Maria Alu´ısio</author>
<author>Caroline Gasperin</author>
</authors>
<title>Fostering digital inclusion and accessibility: the PorSimples project for simplification of Portuguese texts.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas, YIWCALA ’10,</booktitle>
<pages>46--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Alu´ısio, Gasperin, 2010</marker>
<rawString>Sandra Maria Alu´ısio and Caroline Gasperin. 2010. Fostering digital inclusion and accessibility: the PorSimples project for simplification of Portuguese texts. In Proceedings of the NAACL HLT 2010 Young Investigators Workshop on Computational Approaches to Languages of the Americas, YIWCALA ’10, pages 46–53, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bott</author>
<author>Luz Rello</author>
<author>Biljana Drndarevix</author>
<author>Horacio Saggion</author>
</authors>
<title>Can spanish be simpler? lexsis: Lexical simplification for spanish.</title>
<date>2012</date>
<booktitle>In Coling 2012: The 24th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="20996" citStr="Bott et al. (2012)" startWordPosition="3488" endWordPosition="3491"> to use frequency based thresholding over word familiarity scores, as described in Section 2.5 and corresponding to the ‘Frequency’ method in this paper. This has been applied to the medical domain (Zeng et al., 2005; Elhadad, 2006) for predicting which words lay readers will find difficult. This has been correlated with word difficulty via questionnaires (Zeng et al., 2005; Zeng-Treitler et al., 2008) and via the analysis of low-level readability corpora (Elhadad, 2006). In both these cases, a familiarity score is used to determine how likely a subject is to understand a term. More recently, Bott et al. (2012) use a threshold of 1% corpus frequency, along with other checks, to ensure that simple words are not erroneously simplified. Support vector machines are powerful statistical classifiers, as employed in the ‘SVM’ method of this paper. A Support Vector Machine is used to predict the familiarity of CWs in Zeng et al. (2005). It takes features of term frequency and word length and is correlated against the familiarity scores which are already obtained. This proves to have very poor performance, something which the authors attribute to a lack of suitable training data. An SVM has also been trained</context>
</contexts>
<marker>Bott, Rello, Drndarevix, Saggion, 2012</marker>
<rawString>Stefan Bott, Luz Rello, Biljana Drndarevix, and Horacio Saggion. 2012. Can spanish be simpler? lexsis: Lexical simplification for spanish. In Coling 2012: The 24th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1.</booktitle>
<contexts>
<context position="5949" citStr="Brants and Franz, 2006" startWordPosition="935" endWordPosition="938">ne. Section 4. 2 Experimental Design Several systems for detecting CWs were implemented and evaluated using the CW corpus. The two main techniques that exist in the literature are simplifying everything (Devlin and Tait, 1998) System Score SUBTLEX 0.3352 Wikipedia Baseline 0.3270 Kucera-Francis 0.3097 Random Baseline 0.0157 Table 1: The results of different experiments on the SemEval lexical simplification data. These show that SUBTLEX was the best word frequency measure for rating lexical complexity. The other entries correspond to alternative word frequency measures. The Google Web 1T data (Brants and Franz, 2006) has been shown to give a higher score, however this data was not available during the course of this research. and frequency based thresholding (Zeng et al., 2005). These were implemented as well as a support vector machine classifier. This section describes the design decisions made during implementation. 2.1 Lexical Complexity All three of the implementations described in Sections 2.4, 2.5 and 2.6 require a word frequency measure as an indicator of lexical complexity. If a word occurs frequently in common language then it is more likely to be recognised (Rayner and Duffy, 1986). The lexical</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Brysbaert</author>
<author>Boris New</author>
</authors>
<title>Moving beyond Kucera and Francis : a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behav Res Methods.</title>
<date>2009</date>
<contexts>
<context position="7189" citStr="Brysbaert and New, 2009" startWordPosition="1140" endWordPosition="1143">dataset from Task 1 at SemEval 2012 (De Belder and Moens, 2012) was used to compare several measures of word frequency as shown in Table 1. Candidate substitutions and sample sentences were provided by the task organisers, together with a gold standard ranking of the substitutes according to their simplicity. These sentences were ranked according to their frequency. Although the scores in Table 1 appear to be low, this is the kappa agreement for several categories and so should be expected. The inter-annotator agreement on the corpus was 0.488 (De Belder and Moens, 2012). The SUBTLEX dataset (Brysbaert and New, 2009) was the best available for rating word familiarity. This is a corpus of over 70,000 words collected from the subtitles of over 8,000 American English films. 104 2.2 CW Corpus Simple Wikipedia edit histories were mined using techniques similar to those in Yatskar et al. (2010). This provided aligned pairs of sentences which had just one word simplified. Whereas Yatskar et al. (2010) used these pairs to learn probabilities of paraphrases, the research in this paper used them as instances of lexical simplification. The original simplifications were performed by editors trying to make documents a</context>
</contexts>
<marker>Brysbaert, New, 2009</marker>
<rawString>Marc Brysbaert and Boris New. 2009. Moving beyond Kucera and Francis : a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behav Res Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carrol</author>
<author>Guido Minnen</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Practical simplification of english newspaper text to assist aphasic readers.</title>
<date>1998</date>
<booktitle>In AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.</booktitle>
<contexts>
<context position="2134" citStr="Carrol et al., 1998" startWordPosition="322" endWordPosition="325">is an important task at the first stage of lexical simplification and errors introduced or avoided here will affect final results. This work looks at the process of automatically identifying difficult words for a lexical simplification system. Lexical simplification is the task of identifying and replacing CWs in a text to improve the overall understandability and readability. This is a difficult task which is computationally expensive and often inadequately accurate. Lexical simplification is just one method of text simplification and is often deployed alongside other simplification methods (Carrol et al., 1998; Alu´ısio and Gasperin, 2010). Syntactic simplification, statistical machine translation and semantic simplification (or explanation generation) are all current methods of text simplification. Text simplification is typically deployed as an assistive technology (Devlin and Tait, 1998; Alu´ısio and Gasperin, 2010), although this is not always the case. It may also be used alongside other technologies such as summarisation to improve their final results. Identifying CWs is a task which every lexical simplification system must perform, either explicitly or implicitly, before simplification can t</context>
</contexts>
<marker>Carrol, Minnen, Canning, Devlin, Tait, 1998</marker>
<rawString>John Carrol, Guido Minnen, Yvonne Canning, Siobhan Devlin, and John Tait. 1998. Practical simplification of english newspaper text to assist aphasic readers. In AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="12170" citStr="Chang and Lin, 2011" startWordPosition="1984" endWordPosition="1987">al features which the SVM uses to distinguish between classes. The SVM was chosen as it has been used elsewhere for similar tasks (Gasperin et al., 2009; Hancke et al., 2012; Jauhar and Specia, 2012). The use of many fea1The proportion of data that was correctly classified. 105 tures allows factors which may otherwise have been missed to be taken into account. One further advantage is that the features of an SVM can be analysed to determine their effect on the classification. This may give some indication for future feature classification schemes. The SVM was trained using the LIBSVM package (Chang and Lin, 2011) in Matlab. the RBF kernel was selected and a grid search was performed to select values for the 2 parameters C and γ. Training and testing was performed on a heldout data-set using 5-fold cross validation. To implement the SVM a set of features was determined for the classification scheme. Several external libraries were used to extract these as detailed below: Frequency The SUBTLEX frequency of each word was used as previously described in Section 2.1. CD Count Also from the SUBTLEX corpus. The number of films in which a word appeared, ranging from 0 − 8, 388. Length The word length in numbe</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan De Belder</author>
<author>Marie-Francine Moens</author>
</authors>
<title>A dataset for the evaluation of lexical simplification.</title>
<date>2012</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>7182</volume>
<pages>426--437</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<marker>De Belder, Moens, 2012</marker>
<rawString>Jan De Belder and Marie-Francine Moens. 2012. A dataset for the evaluation of lexical simplification. In Computational Linguistics and Intelligent Text Processing, volume 7182 of Lecture Notes in Computer Science, pages 426–437. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>The use of a psycholinguistic database in the simplification of text for aphasic readers. Linguistic Databases,</title>
<date>1998</date>
<pages>161--173</pages>
<contexts>
<context position="2419" citStr="Devlin and Tait, 1998" startWordPosition="359" endWordPosition="362">dentifying and replacing CWs in a text to improve the overall understandability and readability. This is a difficult task which is computationally expensive and often inadequately accurate. Lexical simplification is just one method of text simplification and is often deployed alongside other simplification methods (Carrol et al., 1998; Alu´ısio and Gasperin, 2010). Syntactic simplification, statistical machine translation and semantic simplification (or explanation generation) are all current methods of text simplification. Text simplification is typically deployed as an assistive technology (Devlin and Tait, 1998; Alu´ısio and Gasperin, 2010), although this is not always the case. It may also be used alongside other technologies such as summarisation to improve their final results. Identifying CWs is a task which every lexical simplification system must perform, either explicitly or implicitly, before simplification can take place. CWs are difficult to define, which makes them difficult to identify. For example, take the following sentence: The four largest islands are Honshu, Hokkaido, Shikoku, and Kyushu, and there are approximately 3,000 smaller islands in the chain. In the above sentence, we might</context>
<context position="5552" citStr="Devlin and Tait, 1998" startWordPosition="874" endWordPosition="877">viations from the text’s original semantics. The contributions of this paper are as follows: • A report on the corpus developed and used in the evaluation phase. Section 2.2. • The implementation of a support vector machine for the classification of CWs. Section 2.6 • A comparison of common techniques on the same corpus. Section 4. • An analysis of the features used in the support vector machine. Section 4. 2 Experimental Design Several systems for detecting CWs were implemented and evaluated using the CW corpus. The two main techniques that exist in the literature are simplifying everything (Devlin and Tait, 1998) System Score SUBTLEX 0.3352 Wikipedia Baseline 0.3270 Kucera-Francis 0.3097 Random Baseline 0.0157 Table 1: The results of different experiments on the SemEval lexical simplification data. These show that SUBTLEX was the best word frequency measure for rating lexical complexity. The other entries correspond to alternative word frequency measures. The Google Web 1T data (Brants and Franz, 2006) has been shown to give a higher score, however this data was not available during the course of this research. and frequency based thresholding (Zeng et al., 2005). These were implemented as well as a s</context>
<context position="9894" citStr="Devlin and Tait (1998)" startWordPosition="1594" endWordPosition="1597">of positive and negative data. This gave a set of negative words which were reflective of the broad language which is expected when processing free text. 2.4 Simplify Everything The first implementation involved simplifying everything, a brute force method, in which a simplification algorithm is applied to every word. This assumes that words which are already simple will not require any further simplification. A common variation is to limit the simplification to some combination of all the nouns, verbs and adjectives. A standard baseline lexical simplification system was implemented following Devlin and Tait (1998). This algorithm generated a set of synonyms from WordNet and then used the SUBTLEX frequencies to find the most frequent synonym. If the synonym was more frequent than the original word then a substitution was made. This technique was applied to all the words. If a CW was changed, then it was considered a true positive; if a simple word was not changed, it was considered a true negative. Five trials were carried out and the average accuracy and standard deviation is reported in Figure 1 and Table 3. 2.5 Frequency Thresholding The second technique is frequency thresholding. This relies on each</context>
<context position="19900" citStr="Devlin and Tait, 1998" startWordPosition="3306" endWordPosition="3309">ch will be used for lexical simplification. The related work in this field is also generally 4in the SUBTLEX corpus ‘finger’ has a frequency of 1870, whereas ‘digit’ has a frequency of 30. 107 used as a precursor to lexical simplification. This section will explain how these previous methods have handled the task of identifying CWs and how these fit into the research presented in this paper. The simplest way to identify CWs in a sentence is to blindly assume that every word is complex, as described earlier in Section 2.4. This was first used in Devlin’s seminal work on lexical simplification (Devlin and Tait, 1998). This method is somewhat naive as it does not mitigate the possibility of words being simplified in error. Devlin and Tait indicate that they believe less frequent words will not be subject to meaning change. However, further work into lexical simplification has refuted this (Lal and R¨uger, 2002). This method is still used, for example Thomas and Anderson (2012) simplify all nouns and verbs. This corresponds to the ‘Everything’ method. Another method of identifying CWs is to use frequency based thresholding over word familiarity scores, as described in Section 2.5 and corresponding to the ‘F</context>
</contexts>
<marker>Devlin, Tait, 1998</marker>
<rawString>Siobhan Devlin and John Tait. 1998. The use of a psycholinguistic database in the simplification of text for aphasic readers. Linguistic Databases, pages 161–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>No´emie Elhadad</author>
</authors>
<title>Comprehending technical texts: Predicting and defining unfamiliar terms.</title>
<date>2006</date>
<booktitle>In AMIA Annual Symposium proceedings,</booktitle>
<volume>volume</volume>
<pages>239</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="20610" citStr="Elhadad, 2006" startWordPosition="3426" endWordPosition="3427">d in error. Devlin and Tait indicate that they believe less frequent words will not be subject to meaning change. However, further work into lexical simplification has refuted this (Lal and R¨uger, 2002). This method is still used, for example Thomas and Anderson (2012) simplify all nouns and verbs. This corresponds to the ‘Everything’ method. Another method of identifying CWs is to use frequency based thresholding over word familiarity scores, as described in Section 2.5 and corresponding to the ‘Frequency’ method in this paper. This has been applied to the medical domain (Zeng et al., 2005; Elhadad, 2006) for predicting which words lay readers will find difficult. This has been correlated with word difficulty via questionnaires (Zeng et al., 2005; Zeng-Treitler et al., 2008) and via the analysis of low-level readability corpora (Elhadad, 2006). In both these cases, a familiarity score is used to determine how likely a subject is to understand a term. More recently, Bott et al. (2012) use a threshold of 1% corpus frequency, along with other checks, to ensure that simple words are not erroneously simplified. Support vector machines are powerful statistical classifiers, as employed in the ‘SVM’ m</context>
</contexts>
<marker>Elhadad, 2006</marker>
<rawString>No´emie Elhadad. 2006. Comprehending technical texts: Predicting and defining unfamiliar terms. In AMIA Annual Symposium proceedings, volume 2006, page 239. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="13246" citStr="Fellbaum, 1998" startWordPosition="2179" endWordPosition="2180"> Count Also from the SUBTLEX corpus. The number of films in which a word appeared, ranging from 0 − 8, 388. Length The word length in number of characters was taken into account. It is often the case that longer words are more difficult to process and so may be considered ‘complex’. Syllable Count The number of syllables contained in a word is also a good estimate of its complexity. This was computed using a library from the morphadorner package2. Sense Count A count of the number of ways in which a word can be interpreted - showing how ambiguous a word is. This measure is taken from WordNet (Fellbaum, 1998). Synonym Count Also taken from WordNet, this is the number of potential synonyms with which a word could be replaced. This again may give some indication of a word’s degree of ambiguity. 3 Results The results of the experiments in identifying CWs are shown in Figure 1 and the values are given in Table 3. The values presented are the mean of 5 trials and the error bars represent the standard deviation. 2http://morphadorner.northwestern.edu/ Figure 1: A bar chart with error bars showing the results of the CW identification experiments. Accuracy, F1 Score, Precision and Recall are reported for e</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Lucia Specia</author>
<author>Tiago Pereira</author>
<author>Sandra M Alu´ısio</author>
</authors>
<title>Learning when to simplify sentences for natural text simplification.</title>
<date>2009</date>
<booktitle>In Encontro Nacional de Inteligˆencia Artificial.</booktitle>
<marker>Gasperin, Specia, Pereira, Alu´ısio, 2009</marker>
<rawString>Caroline Gasperin, Lucia Specia, Tiago Pereira, and Sandra M. Alu´ısio. 2009. Learning when to simplify sentences for natural text simplification. In Encontro Nacional de Inteligˆencia Artificial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hancke</author>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>Readability classification for German using lexical, syntactic, and morphological features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1063--1080</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="11723" citStr="Hancke et al., 2012" startWordPosition="1907" endWordPosition="1910">laced in between the frequency of every adjacent pair of words in the ordered list. This was repeated by 5-fold cross validation and the mean threshold determined. The final accuracy of the algorithm was then determined on a separate set of testing data. 2.6 Support Vector Machine Support vector machines (SVM) are statistical classifiers which use labelled training data to predict the class of unseen inputs. The training data consist of several features which the SVM uses to distinguish between classes. The SVM was chosen as it has been used elsewhere for similar tasks (Gasperin et al., 2009; Hancke et al., 2012; Jauhar and Specia, 2012). The use of many fea1The proportion of data that was correctly classified. 105 tures allows factors which may otherwise have been missed to be taken into account. One further advantage is that the features of an SVM can be analysed to determine their effect on the classification. This may give some indication for future feature classification schemes. The SVM was trained using the LIBSVM package (Chang and Lin, 2011) in Matlab. the RBF kernel was selected and a grid search was performed to select values for the 2 parameters C and γ. Training and testing was performed</context>
<context position="22029" citStr="Hancke et al., 2012" startWordPosition="3660" endWordPosition="3663">arity scores which are already obtained. This proves to have very poor performance, something which the authors attribute to a lack of suitable training data. An SVM has also been trained for the ranking of words according to their complexity (Jauhar and Specia, 2012). This was done for the SemEval lexical simplification task (Specia et al., 2012). Although this system is designed for synonym ranking, it could also be used for the CW identification task. Machine learning has also been applied to the task of determining whether an entire sentence requires simplification (Gasperin et al., 2009; Hancke et al., 2012). These approaches use a wide array of morphological features which are suited to sentence level classification. 6 Future Work This work is intended as an initial study of methods for identifying CWs for simplification. The methods compared, whilst typical of current CW identification methods, are not an exhaustive set and variations exist. One further way of expanding this research would be to take into account word context. This could be done using thresholding (Zeng-Treitler et al., 2008) or an SVM (Gasperin et al., 2009; Jauhar and Specia, 2012). Another way to increase the accuracy of the</context>
</contexts>
<marker>Hancke, Vajjala, Meurers, 2012</marker>
<rawString>Julia Hancke, Sowmya Vajjala, and Detmar Meurers. 2012. Readability classification for German using lexical, syntactic, and morphological features. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1063–1080, Mumbai, India.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sujay Kumar Jauhar</author>
<author>Lucia Specia</author>
</authors>
<title>Uowshef: Simplex – lexical simplicity ranking based on contextual and psycholinguistic features.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval2012),</booktitle>
<pages>477--481</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<contexts>
<context position="11749" citStr="Jauhar and Specia, 2012" startWordPosition="1911" endWordPosition="1914">frequency of every adjacent pair of words in the ordered list. This was repeated by 5-fold cross validation and the mean threshold determined. The final accuracy of the algorithm was then determined on a separate set of testing data. 2.6 Support Vector Machine Support vector machines (SVM) are statistical classifiers which use labelled training data to predict the class of unseen inputs. The training data consist of several features which the SVM uses to distinguish between classes. The SVM was chosen as it has been used elsewhere for similar tasks (Gasperin et al., 2009; Hancke et al., 2012; Jauhar and Specia, 2012). The use of many fea1The proportion of data that was correctly classified. 105 tures allows factors which may otherwise have been missed to be taken into account. One further advantage is that the features of an SVM can be analysed to determine their effect on the classification. This may give some indication for future feature classification schemes. The SVM was trained using the LIBSVM package (Chang and Lin, 2011) in Matlab. the RBF kernel was selected and a grid search was performed to select values for the 2 parameters C and γ. Training and testing was performed on a heldout data-set usi</context>
<context position="21677" citStr="Jauhar and Specia, 2012" startWordPosition="3602" endWordPosition="3605"> checks, to ensure that simple words are not erroneously simplified. Support vector machines are powerful statistical classifiers, as employed in the ‘SVM’ method of this paper. A Support Vector Machine is used to predict the familiarity of CWs in Zeng et al. (2005). It takes features of term frequency and word length and is correlated against the familiarity scores which are already obtained. This proves to have very poor performance, something which the authors attribute to a lack of suitable training data. An SVM has also been trained for the ranking of words according to their complexity (Jauhar and Specia, 2012). This was done for the SemEval lexical simplification task (Specia et al., 2012). Although this system is designed for synonym ranking, it could also be used for the CW identification task. Machine learning has also been applied to the task of determining whether an entire sentence requires simplification (Gasperin et al., 2009; Hancke et al., 2012). These approaches use a wide array of morphological features which are suited to sentence level classification. 6 Future Work This work is intended as an initial study of methods for identifying CWs for simplification. The methods compared, whilst</context>
</contexts>
<marker>Jauhar, Specia, 2012</marker>
<rawString>Sujay Kumar Jauhar and Lucia Specia. 2012. Uowshef: Simplex – lexical simplicity ranking based on contextual and psycholinguistic features. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval2012), pages 477–481, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Lal</author>
<author>Stefan R¨uger</author>
</authors>
<title>Extract-based summarization with simplification.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<marker>Lal, R¨uger, 2002</marker>
<rawString>Partha Lal and Stefan R¨uger. 2002. Extract-based summarization with simplification. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Rayner</author>
<author>Susan Duffy</author>
</authors>
<title>Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity.</title>
<date>1986</date>
<journal>Memory &amp; Cognition,</journal>
<pages>14--191</pages>
<contexts>
<context position="6536" citStr="Rayner and Duffy, 1986" startWordPosition="1033" endWordPosition="1036">Web 1T data (Brants and Franz, 2006) has been shown to give a higher score, however this data was not available during the course of this research. and frequency based thresholding (Zeng et al., 2005). These were implemented as well as a support vector machine classifier. This section describes the design decisions made during implementation. 2.1 Lexical Complexity All three of the implementations described in Sections 2.4, 2.5 and 2.6 require a word frequency measure as an indicator of lexical complexity. If a word occurs frequently in common language then it is more likely to be recognised (Rayner and Duffy, 1986). The lexical simplification dataset from Task 1 at SemEval 2012 (De Belder and Moens, 2012) was used to compare several measures of word frequency as shown in Table 1. Candidate substitutions and sample sentences were provided by the task organisers, together with a gold standard ranking of the substitutes according to their simplicity. These sentences were ranked according to their frequency. Although the scores in Table 1 appear to be low, this is the kappa agreement for several categories and so should be expected. The inter-annotator agreement on the corpus was 0.488 (De Belder and Moens,</context>
</contexts>
<marker>Rayner, Duffy, 1986</marker>
<rawString>Keith Rayner and Susan Duffy. 1986. Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity. Memory &amp; Cognition, 14:191–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay Kumar Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<date>2012</date>
<booktitle>Semeval-2012 task 1: English lexical simplification. In First Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="18759" citStr="Specia et al., 2012" startWordPosition="3109" endWordPosition="3112">was very weakly negatively correlated with word simplicity. This indicates that it is not a strong indicative factor in determining whether a word is simple or not. The total number of synonyms was a stronger indicator than the number of senses, but still only exhibited weak correlation. One area that has not been explored in this study is the use of contextual features. Each target word occurs in a sentence and it may be the case that those words surrounding the target give extra information as to its complexity. It has been suggested that language is produced at an even level of complexity (Specia et al., 2012), and so simple words will occur in the presence of other simple words, whereas CWs will occur in the presence of other CWs. As well as lexical contextual information, the surrounding syntax may offer some information on word difficulty. Factors such as a very long sentence or a complex grammatical structure can make a word more difficult to understand. These could be used to modify the familiarity score in the thresholding method, or they could be used as features in the SVM classifier. 5 Related Work This research will be used for lexical simplification. The related work in this field is als</context>
<context position="21758" citStr="Specia et al., 2012" startWordPosition="3615" endWordPosition="3618">achines are powerful statistical classifiers, as employed in the ‘SVM’ method of this paper. A Support Vector Machine is used to predict the familiarity of CWs in Zeng et al. (2005). It takes features of term frequency and word length and is correlated against the familiarity scores which are already obtained. This proves to have very poor performance, something which the authors attribute to a lack of suitable training data. An SVM has also been trained for the ranking of words according to their complexity (Jauhar and Specia, 2012). This was done for the SemEval lexical simplification task (Specia et al., 2012). Although this system is designed for synonym ranking, it could also be used for the CW identification task. Machine learning has also been applied to the task of determining whether an entire sentence requires simplification (Gasperin et al., 2009; Hancke et al., 2012). These approaches use a wide array of morphological features which are suited to sentence level classification. 6 Future Work This work is intended as an initial study of methods for identifying CWs for simplification. The methods compared, whilst typical of current CW identification methods, are not an exhaustive set and vari</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In First Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Rebecca Thomas</author>
<author>Sven Anderson</author>
</authors>
<title>Wordnet-based lexical simplification of a document.</title>
<date>2012</date>
<booktitle>Proceedings of KONVENS 2012,</booktitle>
<pages>80--88</pages>
<editor>In Jeremy Jancsary, editor,</editor>
<contexts>
<context position="20266" citStr="Thomas and Anderson (2012)" startWordPosition="3367" endWordPosition="3370">esearch presented in this paper. The simplest way to identify CWs in a sentence is to blindly assume that every word is complex, as described earlier in Section 2.4. This was first used in Devlin’s seminal work on lexical simplification (Devlin and Tait, 1998). This method is somewhat naive as it does not mitigate the possibility of words being simplified in error. Devlin and Tait indicate that they believe less frequent words will not be subject to meaning change. However, further work into lexical simplification has refuted this (Lal and R¨uger, 2002). This method is still used, for example Thomas and Anderson (2012) simplify all nouns and verbs. This corresponds to the ‘Everything’ method. Another method of identifying CWs is to use frequency based thresholding over word familiarity scores, as described in Section 2.5 and corresponding to the ‘Frequency’ method in this paper. This has been applied to the medical domain (Zeng et al., 2005; Elhadad, 2006) for predicting which words lay readers will find difficult. This has been correlated with word difficulty via questionnaires (Zeng et al., 2005; Zeng-Treitler et al., 2008) and via the analysis of low-level readability corpora (Elhadad, 2006). In both the</context>
</contexts>
<marker>Thomas, Anderson, 2012</marker>
<rawString>S. Rebecca Thomas and Sven Anderson. 2012. Wordnet-based lexical simplification of a document. In Jeremy Jancsary, editor, Proceedings of KONVENS 2012, pages 80–88. ¨OGAI, September. Main track: oral presentations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL.</booktitle>
<contexts>
<context position="7466" citStr="Yatskar et al. (2010)" startWordPosition="1186" endWordPosition="1189">according to their simplicity. These sentences were ranked according to their frequency. Although the scores in Table 1 appear to be low, this is the kappa agreement for several categories and so should be expected. The inter-annotator agreement on the corpus was 0.488 (De Belder and Moens, 2012). The SUBTLEX dataset (Brysbaert and New, 2009) was the best available for rating word familiarity. This is a corpus of over 70,000 words collected from the subtitles of over 8,000 American English films. 104 2.2 CW Corpus Simple Wikipedia edit histories were mined using techniques similar to those in Yatskar et al. (2010). This provided aligned pairs of sentences which had just one word simplified. Whereas Yatskar et al. (2010) used these pairs to learn probabilities of paraphrases, the research in this paper used them as instances of lexical simplification. The original simplifications were performed by editors trying to make documents as simple as possible. The CW is identified by comparison with the simplified sentence. Further information on the production of the corpus will be published in a future paper. 2.3 Negative Examples The CW corpus provides a set of CWs in appropriate contexts. This is useful for</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: Unsupervised extraction of lexical simplifications from Wikipedia. In Proceedings of the NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Zeng</author>
<author>Eunjung Kim</author>
<author>Jon Crowell</author>
<author>Tony Tse</author>
</authors>
<title>A text corpora-based estimation of the familiarity of health terminology. Biological and Medical Data Analysis,</title>
<date>2005</date>
<pages>184--192</pages>
<contexts>
<context position="6113" citStr="Zeng et al., 2005" startWordPosition="963" endWordPosition="966">rature are simplifying everything (Devlin and Tait, 1998) System Score SUBTLEX 0.3352 Wikipedia Baseline 0.3270 Kucera-Francis 0.3097 Random Baseline 0.0157 Table 1: The results of different experiments on the SemEval lexical simplification data. These show that SUBTLEX was the best word frequency measure for rating lexical complexity. The other entries correspond to alternative word frequency measures. The Google Web 1T data (Brants and Franz, 2006) has been shown to give a higher score, however this data was not available during the course of this research. and frequency based thresholding (Zeng et al., 2005). These were implemented as well as a support vector machine classifier. This section describes the design decisions made during implementation. 2.1 Lexical Complexity All three of the implementations described in Sections 2.4, 2.5 and 2.6 require a word frequency measure as an indicator of lexical complexity. If a word occurs frequently in common language then it is more likely to be recognised (Rayner and Duffy, 1986). The lexical simplification dataset from Task 1 at SemEval 2012 (De Belder and Moens, 2012) was used to compare several measures of word frequency as shown in Table 1. Candidat</context>
<context position="20594" citStr="Zeng et al., 2005" startWordPosition="3422" endWordPosition="3425">rds being simplified in error. Devlin and Tait indicate that they believe less frequent words will not be subject to meaning change. However, further work into lexical simplification has refuted this (Lal and R¨uger, 2002). This method is still used, for example Thomas and Anderson (2012) simplify all nouns and verbs. This corresponds to the ‘Everything’ method. Another method of identifying CWs is to use frequency based thresholding over word familiarity scores, as described in Section 2.5 and corresponding to the ‘Frequency’ method in this paper. This has been applied to the medical domain (Zeng et al., 2005; Elhadad, 2006) for predicting which words lay readers will find difficult. This has been correlated with word difficulty via questionnaires (Zeng et al., 2005; Zeng-Treitler et al., 2008) and via the analysis of low-level readability corpora (Elhadad, 2006). In both these cases, a familiarity score is used to determine how likely a subject is to understand a term. More recently, Bott et al. (2012) use a threshold of 1% corpus frequency, along with other checks, to ensure that simple words are not erroneously simplified. Support vector machines are powerful statistical classifiers, as employe</context>
</contexts>
<marker>Zeng, Kim, Crowell, Tse, 2005</marker>
<rawString>Qing Zeng, Eunjung Kim, Jon Crowell, and Tony Tse. 2005. A text corpora-based estimation of the familiarity of health terminology. Biological and Medical Data Analysis, pages 184–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Zeng-Treitler</author>
<author>Sergey Goryachev</author>
<author>Tony Tse</author>
<author>Alla Keselman</author>
<author>Aziz Boxwala</author>
</authors>
<title>Estimating consumer familiarity with health terminology: a context-based approach.</title>
<date>2008</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="20783" citStr="Zeng-Treitler et al., 2008" startWordPosition="3451" endWordPosition="3454">ion has refuted this (Lal and R¨uger, 2002). This method is still used, for example Thomas and Anderson (2012) simplify all nouns and verbs. This corresponds to the ‘Everything’ method. Another method of identifying CWs is to use frequency based thresholding over word familiarity scores, as described in Section 2.5 and corresponding to the ‘Frequency’ method in this paper. This has been applied to the medical domain (Zeng et al., 2005; Elhadad, 2006) for predicting which words lay readers will find difficult. This has been correlated with word difficulty via questionnaires (Zeng et al., 2005; Zeng-Treitler et al., 2008) and via the analysis of low-level readability corpora (Elhadad, 2006). In both these cases, a familiarity score is used to determine how likely a subject is to understand a term. More recently, Bott et al. (2012) use a threshold of 1% corpus frequency, along with other checks, to ensure that simple words are not erroneously simplified. Support vector machines are powerful statistical classifiers, as employed in the ‘SVM’ method of this paper. A Support Vector Machine is used to predict the familiarity of CWs in Zeng et al. (2005). It takes features of term frequency and word length and is cor</context>
<context position="22525" citStr="Zeng-Treitler et al., 2008" startWordPosition="3738" endWordPosition="3741"> applied to the task of determining whether an entire sentence requires simplification (Gasperin et al., 2009; Hancke et al., 2012). These approaches use a wide array of morphological features which are suited to sentence level classification. 6 Future Work This work is intended as an initial study of methods for identifying CWs for simplification. The methods compared, whilst typical of current CW identification methods, are not an exhaustive set and variations exist. One further way of expanding this research would be to take into account word context. This could be done using thresholding (Zeng-Treitler et al., 2008) or an SVM (Gasperin et al., 2009; Jauhar and Specia, 2012). Another way to increase the accuracy of the frequency count method may be to use a larger corpus. Whilst the corpus used in this paper performed well in the preliminary testing section, other research has shown the Google Web1T corpus (a n-gram count of over a trillion words) to be more effective (De Belder and Moens, 2012). The Web 1T data was not available during the course of this research. The large variability in accuracy shown in the SVM method indicates that there was insufficient training data. With more data, the SVM would h</context>
</contexts>
<marker>Zeng-Treitler, Goryachev, Tse, Keselman, Boxwala, 2008</marker>
<rawString>Qing Zeng-Treitler, Sergey Goryachev, Tony Tse, Alla Keselman, and Aziz Boxwala. 2008. Estimating consumer familiarity with health terminology: a context-based approach. Journal of the American Medical Informatics Association, 15(3):349–356.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>