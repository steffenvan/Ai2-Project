<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.134046">
<title confidence="0.9034065">
Clueless: Explorations in unsupervised, knowledge-lean extraction of
lexical-semantic information
</title>
<author confidence="0.539311">
Invited Talk
</author>
<bodyText confidence="0.9591967">
Lillian Lee
Department of Computer Science, Cornell University
llee@cs.cornell.edu
I will discuss two current projects on automatically extracting certain types of lexical-semantic
information in settings wherein we can rely neither on annotations nor existing knowledge resources
to provide us with clues. The name of the game in such settings is to find and leverage auxiliary sources
of information.
Why is it that if you know I’ll give a silly talk, it follows that you know I’ll give a talk, whereas if you
doubt I’ll give a good talk, it doesn’t follow that you doubt I’ll give a talk? This pair of examples
shows that the word “doubt” exhibits a special but prevalent kind of behavior known as downward
entailingness — the licensing of reasoning from supersets to subsets, so to speak, but not vice versa. The
first project I’ll describe is to identify words that are downward entailing, a task that promises to enhance
the performance of systems that engage in textual inference, and one that is quite challenging since it is
difficult to characterize these items as a class and no corpus with downward-entailingness annotations
exists. We are able to surmount these challenges by utilizing some insights from the linguistics literature
regarding the relationship between downward entailing operators and what are known as negative polarity
items — words such as “ever” or the idiom “have a clue” that tend to occur only in negative contexts.
A cross-linguistic analysis indicates some potentially interesting connections to findings in linguistic
typology.
That previous paragraph was quite a mouthful, wasn’t it? Wouldn’t it be nice if it were written in plain
English that was easier to understand? The second project I’ll talk about, which has the eventual aim to
make it possible to automatically simplify text, aims to learn lexical-level simplifications, such as “work
together” for “collaborate”. (This represents a complement to prior work, which focused on syntactic
transformations, such as passive to active voice.) We exploit edit histories in Simple English Wikipedia
for this task. This isn’t as simple (ahem) as it might at first seem because Simple English Wikipedia and
the usual Wikipedia are far from a perfect parallel corpus and because many edits in Simple Wikipedia
do not constitute simplifications. We consider both explicitly modeling different kinds of operations and
various types of bootstrapping, including as clues the comments Wikipedians sometimes leave when they
edit.
Joint work with Cristian Danescu-Niculescu-Mizil, Bo Pang, and Mark Yatskar.
</bodyText>
<page confidence="0.959938">
55
</page>
<reference confidence="0.6159705">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 55,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.676248">
<title confidence="0.987902333333333">Clueless: Explorations in unsupervised, knowledge-lean extraction lexical-semantic information Invited</title>
<author confidence="0.98127">Lillian</author>
<affiliation confidence="0.988846">Department of Computer Science, Cornell</affiliation>
<email confidence="0.992756">llee@cs.cornell.edu</email>
<abstract confidence="0.998004884615385">I will discuss two current projects on automatically extracting certain types of lexical-semantic information in settings wherein we can rely neither on annotations nor existing knowledge resources to provide us with clues. The name of the game in such settings is to find and leverage auxiliary sources of information. is it that if you give a silly talk, it follows that you know I’ll give a talk, whereas if you give a good talk, it doesn’t follow that you doubt I’ll give a talk? This pair of examples shows that the word “doubt” exhibits a special but prevalent kind of behavior known as downward entailingness — the licensing of reasoning from supersets to subsets, so to speak, but not vice versa. The first project I’ll describe is to identify words that are downward entailing, a task that promises to enhance the performance of systems that engage in textual inference, and one that is quite challenging since it is difficult to characterize these items as a class and no corpus with downward-entailingness annotations exists. We are able to surmount these challenges by utilizing some insights from the linguistics literature regarding the relationship between downward entailing operators and what are known as negative polarity items — words such as “ever” or the idiom “have a clue” that tend to occur only in negative contexts. A cross-linguistic analysis indicates some potentially interesting connections to findings in linguistic typology. That previous paragraph was quite a mouthful, wasn’t it? Wouldn’t it be nice if it were written in plain English that was easier to understand? The second project I’ll talk about, which has the eventual aim to make it possible to automatically simplify text, aims to learn lexical-level simplifications, such as “work together” for “collaborate”. (This represents a complement to prior work, which focused on syntactic transformations, such as passive to active voice.) We exploit edit histories in Simple English Wikipedia for this task. This isn’t as simple (ahem) as it might at first seem because Simple English Wikipedia and the usual Wikipedia are far from a perfect parallel corpus and because many edits in Simple Wikipedia do not constitute simplifications. We consider both explicitly modeling different kinds of operations and various types of bootstrapping, including as clues the comments Wikipedians sometimes leave when they edit.</abstract>
<note confidence="0.864579">Joint work with Cristian Danescu-Niculescu-Mizil, Bo Pang, and Mark Yatskar. 55 of the Fourteenth Conference on Computational Natural Language page Sweden, 15-16 July 2010. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>55</pages>
<location>Uppsala,</location>
<marker>2010</marker>
<rawString>Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 55, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>