<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001128">
<title confidence="0.990193">
Intrinsic versus Extrinsic Evaluations of Parsing Systems
</title>
<author confidence="0.999255">
Diego Moll´a
</author>
<affiliation confidence="0.996695333333333">
Centre for Language Technology
Department of Computing
Macquarie University
</affiliation>
<address confidence="0.827347">
Sydney, NSW 2109, Australia
</address>
<email confidence="0.997493">
diego@ics.mq.edu.au
</email>
<author confidence="0.989757">
Ben Hutchinson
</author>
<affiliation confidence="0.997833">
Division of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.817598">
Edinburgh EH8 9LW, United Kingdom
</address>
<email confidence="0.997994">
B.Hutchinson@sms.ed.ac.uk
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999834">
A wide range of parser and/or grammar
evaluation methods have been reported
in the literature. However, in most cases
these evaluations take the parsers in-
dependently (intrinsic evaluations), and
only in a few cases has the effect
of different parsers in real applications
been measured (extrinsic evaluations).
This paper compares two evaluations
of the Link Grammar parser and the
Conexor Functional Dependency Gram-
mar parser. The parsing systems, de-
spite both being dependency-based, re-
turn different types of dependencies,
making a direct comparison impossi-
ble. In the intrinsic evaluation, the accu-
racy of the parsers is compared indepen-
dently by converting the dependencies
into grammatical relations and using the
methodology of Carroll et al. (1998) for
parser comparison. In the extrinsic eval-
uation, the parsers’ impact in a practi-
cal application is compared within the
context of answer extraction. The dif-
ferences in the results are significant.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999824015625">
Parsing is a principal stage in many natural lan-
guage processing (NLP) systems. A good parser is
expected to return an accurate syntactic structure
of a sentence. This structure is typically forwarded
to other modules so that they can work with un-
ambiguous and well-defined structures represent-
ing the sentences. It is to be expected that the
performance of an NLP system quickly degrades
if the parsing system returns incorrect syntactic
structures, and therefore an evaluation of parsing
coverage and accuracy is important.
According to Galliers and Sparck Jones (1993),
there are two main criteria in performance evalua-
tion: “Intrinsic criteria are those relating to a sys-
tem’s objective, extrinsic criteria those relating to
its function i.e. to its role in relation to its setup’s
purpose.” (Galliers and Sparck Jones, 1993, p22).
Thus, an intrinsic evaluation of a parser would
analyse the accuracy of the results returned by the
parser as a stand-alone system, whereas an ex-
trinsic evaluation would analyse the impact of the
parser within the context of a broader NLP appli-
cation.
There are currently several parsing
systems that attempt to achieve a wide
coverage of the English language (such
as those developed by Collins (1996),
J¨arvinen and Tapanainen (1997), and
Sleator and Temperley (1993)). There is also
substantial literature on parsing evaluation (see,
for example, work by Sutcliffe et al. (1996),
Black (1996), Carroll et al. (1998), and
Bangalore et al. (1998)). Recently there has
been a shift from constituency-based (e.g. count-
ing crossing brackets (Black et al., 1991)) to
dependency-based evaluation (Lin, 1995; Carroll
et al., 1998). Those evaluation methodologies
typically focus on comparisons of stand-alone
parsers (intrinsic evaluations). In this paper we
report on the comparison between an intrinsic
evaluation and an evaluation of the impact of
the parser in a real application (an extrinsic
evaluation).
We have chosen answer extraction as an exam-
ple of a practical application within which to test
the parsing systems. In particular, the extrinsic
evaluation uses ExtrAns, an answer extraction sys-
tem that operates over Unix manual pages (Moll´a
et al., 2000). The two grammar systems to com-
pare are Link Grammar (Sleator and Temperley,
1993) and the Conexor Functional Dependency
Grammar parser (Tapanainen and J¨arvinen, 1997)
(henceforth referred to as Conexor FDG). These
parsing systems were chosen because both include
a dependency-based parser and a comprehensive
grammar of English. However, the structures re-
turned are so different that a direct comparison be-
tween them is not straightforward. In Section 2 we
review the main differences between Link Gram-
mar and Conexor FDG. In Section 3 we present
the intrinsic comparison of parsers, and in Sec-
tion 4 we comment on the extrinsic comparison
within the context of answer extraction. The re-
sults of the evaluations are discussed in Section 5.
</bodyText>
<subsectionHeader confidence="0.581263">
2 Link Grammar and Conexor FDG
</subsectionHeader>
<bodyText confidence="0.972325538461538">
Link Grammar (Sleator and Temperley, 1993) is
a grammar theory that is strongly dependency-
based. A freely available parsing system that im-
plements the Link Grammar theory has been de-
veloped at Carnegie Mellon University. The pars-
ing system includes an extensive grammar and lex-
icon and has a wide coverage of the English lan-
guage. Conexor FDG (Tapanainen and J¨arvinen,
1997) is a commercial parser and grammar, based
on the theory of Functional Dependency Gram-
mar, and was originally developed at the Univer-
sity of Helsinki.
Despite both being dependency-based, there are
substantial differences between the structures re-
turned by the two parsers. Figure 1 shows Link
Grammar’s output for a sample sentence, and Fig-
ure 2 shows the dependency structure returned
by Conexor FDG for comparison. Table 1 ex-
plains the dependency types used in the depen-
dency structures of the figures.
The differences between the dependency struc-
tures returned by Link Grammar 2.1 and Conexor
FDG 3.6 can be summarised as follows.
Direction of dependency: Link Grammar’s
‘links’, although similar to true dependencies, do
not state which participant is the head and which
is the dependent. However, Link Grammar uses
different link types for head-right links and head-
left links, so this information can be recovered.
Conexor FDG always indicates the direction of the
dependence.
Clausal heads: Link Grammar generally
chooses the front-most element to be the head
of a clause, rather than the main verb. This is
true of both matrix and subordinate clauses, as
exemplified by the Wd and R links in Figure 1.
Conexor FDG follows the orthodox convention of
choosing the main verb as the head of the clause.
Graph structures: Link Grammar’s links com-
bine dependencies at the surface-syntactic and
deep-syntactic levels (e.g., the link Bs, which
links a noun modified by a subject-type relative
clause to the relative clause’s head verb, in Fig-
ure 1 indicates a deep-syntactic dependency). The
resulting structures are graphs rather than trees.
An example is shown in Figure 1, where the noun
man modified by a relative clause is linked to both
the complementiser and the head verb of the rela-
tive clause.
Conjunctions: Our version of Link Grammar
analyses a coordinating conjunction as the head of
a coordinated phrase (Figure 1). This is a modifi-
cation of Link Grammar’s default behaviour which
returns a list of parses, one parse per conjunct.
However in Conexor FDG’s analyses the head will
be either the first or the last conjunct, depending
on whether the coordinated phrase’s head lies to
the left or to the right (Figure 2).
Dependency types: Link Grammar uses a set of
about 90 link types and many subtypes, which ad-
dress very specific syntactic constructions (e.g. the
link type EB connects adverbs to forms of be be-
fore a noun phrase or prepositional phrase: He
is APPARENTLY a good programmer). On the
other hand, Conexor FDG uses a set of 32 de-
</bodyText>
<figure confidence="0.994021555555556">
Ss
Wd
Ds
Bs
R RS
Js
Ds
MVp
O^
</figure>
<figureCaption confidence="0.82670875">
///// the man.n that came.v ate.v bananas.n and apples.n with a fork.n1
Figure 1: Output of Link Grammar.
///// the man that came ate bananas and apples with a fork
Figure 2: Dependency structure returned by Conexor FDG.
</figureCaption>
<figure confidence="0.996768181818182">
main &lt;
&gt;det
&gt; subj
mod&lt;
&gt;subj
obj&lt;
ins &lt;
cc&lt;
cc &lt;
pcomp&lt;
&gt;det
</figure>
<bodyText confidence="0.99932875">
pendency relations, ranging from traditional gram-
matical functions (e.g. subject, object), to specific
types of modifiers (e.g. frequency, duration, loca-
tion).
Both Conexor FDG and Link Grammar also
return non-dependency information. For Link
Grammar, this consists of some word class in-
formation, shown as suffixes in Figure 1. For
Conexor FDG, the base form morphological in-
formation of each word is returned, along with a
“functional” tag or morpho-syntactic function and
a “surface syntactic” tag for each word.&apos;
</bodyText>
<sectionHeader confidence="0.999043" genericHeader="method">
3 Intrinsic Evaluations
</sectionHeader>
<bodyText confidence="0.999667358490566">
Given that both parses are dependency-based, in-
trinsic evaluations that are based on constituency
structures (e.g. (Black et al., 1991)) are hard
to perform. Dependency-based evaluations are
not easy either: directly comparing dependency
graphs (as suggested by Lin (1995), for exam-
ple) becomes difficult given the differences be-
tween the structures returned by the Link Gram-
mar parser and Conexor FDG. We there-
fore need an approach that is independent from
the format of the parser output. Following
Carroll et al. (1998) we use grammatical relations
to compare the accuracy of Link Grammar and
Conexor FDG. Carroll et al. (1998) propose a set
of twenty parser-independent grammatical rela-
tions arranged in a hierarchy representing differ-
ent degrees of specificity. Four relations from the
hierarchy are shown in Table 2. The arguments to
&apos;See (J¨arvinen and Tapanainen, 1997) for more informa-
tion on the output from Conexor FDG.
each relation specify a head, a dependent, and pos-
sibly an initial grammatical relation (in the case
of SUBJ in passive sentences, for example) or the
‘type’, which specifies the word introducing the
dependent (in the case of XCOMP).
For example, the grammatical relations of the
sentence the man that came ate bananas and ap-
ples with a fork without asking has the following
relations:
SUBJ(eat,man, ),
OBJ(eat,banana),
OBJ(eat,apple),
MOD(fork,eat,with),
SUBJ(come,man,),
MOD(that,man,come),
XCOMP(without,eat,ask)
The terms ‘head’ and ‘dependent’ used
by Carroll et al. (1998) to refer to the arguments
of grammatical relations should not be con-
fused with the similar terms in the theory of
dependency grammar. Grammatical relations
and dependency arcs represent different phe-
nomena. An example should suffice to illustrate
the difference; consider The man that came ate
bananas and apples with a fork. Independency
grammar a unique head is assigned to each word,
for example the head of man is ate. However
man is the dependent of more than one gram-
matical relation, namely SUBJ(eat,man, )
and SUBJ(come,man, ). Furthermore, in
dependency grammar a word can have at most
one dependent of each argument type, and so ate
can have at most one object, for example. But
</bodyText>
<table confidence="0.999820272727273">
Link Grammar Conexor FDG
Name Description Name Description
Bs Singular external object of relative clause cc Coordination
Ds Singular determiner det Determiner
Js Singular object of a preposition ins &lt;not documented&gt;
MVp Verb-modifying preposition main Main element
Oˆ Object mod General post-modifier
R Relative clause obj Object
RS Part of subject-type relative clause pcomp Prepositional complement
Ss Singular subject subj Subject
Wd Declarative sentence
</table>
<tableCaption confidence="0.996091">
Table 1: Some of the dependency types used by Link Grammar and Conexor FDG.
</tableCaption>
<table confidence="0.999797">
Relation Description
SUBJ(head, dependent, initial gr) Subject
OBJ(head, dependent) Object
XCOMP(type, head, dependent) Clausal complement without an overt subject
MOD(type, head, dependent) Modifier
</table>
<tableCaption confidence="0.995706">
Table 2: Grammatical relations used in the intrinsic evaluation.
</tableCaption>
<bodyText confidence="0.999553666666667">
the same is not true for grammatical relations,
and we get both OBJ(eat,banana) and
OBJ(eat,apple).
</bodyText>
<subsectionHeader confidence="0.992879">
3.1 Accuracy
</subsectionHeader>
<bodyText confidence="0.999975075">
Our intrinsic evaluation began on the assumption
that grammatical relations could be deduced from
the dependency structures returned by the parsers.
In practise, however, this deduction process is not
always straightforward; for example complexity
arises when arguments are shared across clauses.
In addition, Link Grammar’s analysis of the front-
most elements as clausal heads complicates the
grammatical relation deduction when there are
modifying clauses.
An existing corpus of 500 sentences/10,000
words annotated with grammatical relations was
used for the evaluation (Carroll et al., 1999). We
restricted the evaluation to just the four relations
shown in Table 2. This decision had two motiva-
tions. Firstly, since the dependency parsers’ out-
put did not recognise some distinctions made in
the hierarchy of relations, it did not make sense to
test these distinctions. Secondly, we wanted the
deduction of grammatical relations to be as simple
a process as possible, to minimise the chance of
introducing errors. This second consideration also
led us to purposefully ignore the sharing of argu-
ments induced by control verbs, as this could not
always be deduced reliably. Since this was done
for both parsers the comparison remains meaning-
ful.
Algorithms for producing grammatical relations
from Link Grammar and Conexor FDG output
were developed and implemented. The results of
parsing the corpus are shown in Table 3. Since
Conexor FDG returns one parse per sentence only
and Link Grammar returns all parses ranked, the
first (i.e. the best) parse returned by Link Gram-
mar was used in the intrinsic evaluation.
The table shows significantly lower values of
recall and precision for Link Grammar. This is
partly due to the fact that Link Grammar’s links
often do not connect the head of the clause, as we
have seen with the Wd link in Figure 1.
</bodyText>
<subsectionHeader confidence="0.985226">
3.2 Speed
</subsectionHeader>
<bodyText confidence="0.9937048">
Link Grammar took 1,212 seconds to parse the
10,000 word corpus, while Conexor FDG took
20.5 seconds. This difference is due partly to the
fact that Link Grammar finds and returns multiple
(and often many) alternative parses. For example,
</bodyText>
<table confidence="0.999744692307692">
With Link With
Grammar Conexor
FDG
Precision SUBJ 50.3% 73.6%
OBJ 48.5% 84.8%
XCOMP 62.2% 76.2%
MOD 57.2% 63.7%
Average 54.6% 74.6%
Recall SUBJ 39.1% 64.5%
OBJ 50% 53.4%
XCOMP 32.1% 64.7%
MOD 53.7% 56.2%
Average 43.7% 59.7%
</table>
<tableCaption confidence="0.8873025">
Table 3: Accuracy of identification of grammatical
relations.
</tableCaption>
<bodyText confidence="0.950904">
Link Grammar found a total of 410,509 parses of
the 505 corpus sentences.
</bodyText>
<sectionHeader confidence="0.997412" genericHeader="method">
4 Extrinsic Evaluations
</sectionHeader>
<bodyText confidence="0.99996125">
It is important to know not only the accuracy of
a parser but how possible parsing errors affect the
success of an NLP application. This is the goal of
an extrinsic evaluation, where the system is eval-
uated in relation to the embedding setup. Using
answer extraction as an example of an NLP appli-
cation, we compared the performance of the Link
Grammar system and Conexor FDG.
</bodyText>
<subsectionHeader confidence="0.999614">
4.1 Answer Extraction and ExtrAns
</subsectionHeader>
<bodyText confidence="0.996005868852459">
The fundamental goal of Answer Extraction (AE)
is to locate those exact phrases of unedited text
documents that answer a query worded in nat-
ural language. AE has received much attention
recently, as the increasingly active Question An-
swering track in TREC demonstrates (Voorhees,
2001b; Voorhees, 2001a).
ExtrAns is an answer extraction system that
operates over UNIX manual pages (Moll´a et al.,
2000). A core process in ExtrAns is the produc-
tion of semantic information in the shape of logi-
cal forms for each sentence of each manual page,
as well as the user query. These logical forms are
designed so that they can be derived from any sen-
tence (using robust approaches to treat very com-
plex or ungrammatical sentences), and they are op-
timised for NLP tasks that involve the semantic
comparison of sentences, such as AE.
ExtrAns’ logical forms are called minimal log-
ical forms (MLFs) because they encode the mini-
mum information required for effective answer ex-
traction. In particular, only the main dependencies
between the verb and arguments are expressed,
plus modifier and adjunct relations. Thus, com-
plex quantification, tense and aspect, temporal re-
lations, plurality, and modality are not expressed.
The MLFs use reification to achieve flat expres-
sions, very much in the line of Davidson (1967),
Hobbs (1985), and Copestake et al. (1997). In the
current implementation only reification to objects,
eventualities (events or states), and properties is
applied. For example, the MLF of the sentence cp
will quickly copy files is:
holds(e4),
object(cp,o1,[x1]),
object(s command,o2,[x1]),
evt(s copy,e4,[x1,x6]),
object(s file,o3,[x6]),
prop(quickly,p3,[e4]).
In other words, there is an entity x1 which rep-
resents an object of type command;2 there is an
entity x6 (a file); there is an entity e4, which rep-
resents a copying event where the first argument
is x1 and the second argument is x6; there is an
entity p3 which states that e4 is done quickly, and
the event e4, that is, the copying, holds.
ExtrAns finds the answers to the questions by
converting the MLFs of the questions into Prolog
queries and then running Prolog’s default resolu-
tion mechanism to find those MLFs that can prove
the question.
This default search procedure is called the syn-
onym mode since ExtrAns uses a small WordNet-
style thesaurus (Fellbaum, 1998) to convert all the
synonyms into a synonym representative. Extr-
Ans also has an approximate mode which, be-
sides normalising all synonyms, scores all docu-
ment sentences on the basis of the maximum num-
ber of predicates that unify between the MLFs of
the query and the answer candidate (Moll´a et al.,
2000). If all query predicates can be matched then
</bodyText>
<footnote confidence="0.9492155">
2ExtrAns uses additional domain knowledge to infer that
cp is a command.
</footnote>
<bodyText confidence="0.9975745">
the approximate mode returns exactly the same an-
swers as the synonym mode.
</bodyText>
<subsectionHeader confidence="0.988796">
4.2 The Comparison
</subsectionHeader>
<bodyText confidence="0.999994833333333">
Ideally, answer extraction systems should be eval-
uated according to how successful they are in help-
ing users to complete their tasks. The use of the
system will therefore depend on such factors as
how many potential answers the user is presented
with at a time, the way these potential answers are
ranked, how many potential answers the user is
prepared to read while searching for an actual an-
swer, and so on. These issues, though important,
are beyond the scope of the present evaluation. In
this evaluation we focus solely on the relevance of
the set of results returned by ExtrAns.
</bodyText>
<subsectionHeader confidence="0.888768">
4.2.1 Method
</subsectionHeader>
<bodyText confidence="0.999923">
Resources from a previous evaluation of Extr-
Ans (Moll´a et al., 2000) were re-used for this eval-
uation. These resources were: a) a collection of
500 man pages, and b) a test set of 26 queries and
relevant answers found in the 500 manual pages.
The careful and labour-intensive construction of
the test set gives us confidence that practically all
relevant answers to each query are present in the
test set. The queries themselves were selected ac-
cording to the following criteria:
</bodyText>
<listItem confidence="0.9986545">
• There must be at least one answer in the man-
ual page collection.
• The query asks how to perform a particular
action, or how a particular command works.
• The query is simple, i.e. it asks only one
question.
</listItem>
<table confidence="0.99981175">
Parser Precision Recall F-score
Conexor FDG 55.8% 8.9% 0.074
LG–best 49.7% 11.4% 0.099
LG–all 50.9% 13.1% 0.120
</table>
<tableCaption confidence="0.984792">
Table 4: Averages per query in synonym mode.
</tableCaption>
<table confidence="0.999951">
Parser Precision Recall F-score
Conexor FDG 28.3% 21.9% 0.177
LG–best 31.8% 15.8% 0.150
LG–all 40.5% 20.5% 0.183
</table>
<tableCaption confidence="0.999284">
Table 5: Averages per query in approximate mode.
</tableCaption>
<sectionHeader confidence="0.611903" genericHeader="method">
4.2.2 Results
</sectionHeader>
<bodyText confidence="0.999869818181818">
Precision, Recall and the F-score (with Preci-
sion and Recall equally weighted) for each query
were calculated.3 When no results were returned
for a query the precision could not be calculated,
but the F-score is equal to zero. The results are
shown in Tables 4 and 5. The number of times the
results for a query contained no relevant answers
are shown in Table 6.
The tables show that the approximate mode
gives better results than the synonym mode. This
is to be expected, since the synonym mode returns
exact matches only and therefore some questions
may not produce any results. For those questions,
recall and F would be zero. In fact, the number of
questions without answers in the synonym mode
is so large that the comparison between Conexor
FDG and Link Grammar becomes unreliable in
this mode. In this discussion, therefore, we will
focus on the approximate mode.
The results returned by Link Grammar when all
parses are considered are significantly better than
when only the first (i.e. the best) parse is consid-
</bodyText>
<footnote confidence="0.461823">
3F was calculated using the expression
</footnote>
<bodyText confidence="0.999815666666667">
The manual pages were parsed using Conexor
FDG and Link Grammar. The latter has a param-
eter for outputting either all parses found, or just
the best parse found, and both parameter settings
were used. The queries were then parsed by both
parsers and their logical forms were used to search
the respective databases. The experiment was re-
peated using both the synonym and approximate
search modes.
</bodyText>
<equation confidence="0.92246575">
F = 2 × |returned and relevant|
|returned |+ |relevant|
which is equivalent to the usual formulation (with  = 1):
F = (2 + 1) × Precision × Recall
</equation>
<footnote confidence="0.874438">
2Precision + Recall
4Average over queries for which precision is defined, i.e.
when the number of returns is non-zero.
</footnote>
<table confidence="0.998634222222222">
Parser Search mode No Nothing
results relevant
returned returned
Con. FDG Synonym 20 20
Con. FDG Approximate 0 8
LG–best Synonym 16 18
LG–best Approximate 1 11
LG–all Synonym 15 18
LG–all Approximate 4 12
</table>
<tableCaption confidence="0.9773825">
Table 6: Numbers of times no relevant answers
were found.
</tableCaption>
<bodyText confidence="0.998640111111111">
ered. This shows that, in the answer extraction
task, it is better to use the logical forms of all
possible sentence interpretations. Recall increases
and, remarkably, precision increases as well. This
means that the system is more likely to include
new relevant answers when all parses are consid-
ered.
In many applications it is more practical to con-
sider one parse only. Conexor FDG, for example,
returns one parse only, and the parsing speed com-
parison (Section 3.2) shows an important differ-
ence in parsing time. If we compare Conexor FDG
with Link Grammar set to return just the best parse
— since Conexor FDG returns one parse only, this
is the fairest comparison — we can see that recall
of the system using Conexor FDG is higher than
that of the system using Link Grammar, while re-
taining similar precision.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999962034482758">
The fairest extrinsic comparison between Conexor
FDG and Link Grammar is the one that uses the
best parse returned by Link Grammar, and the an-
swer extraction method follows the approximate
mode. With these settings, Conexor FDG pro-
duces better results than Link Grammar. However,
the results of the extrinsic comparison are far less
dramatic than those of the intrinsic comparison,
specially in the precision figures.
One reason for the difference in the results is
that the intrinsic evaluation compares grammatical
relation accuracy, whereas the answer extraction
system used in the extrinsic evaluation uses logi-
cal forms. A preliminary inspection of the gram-
matical relations and logical forms of questions
and correct answers shows that high overlap of
grammatical relations does not translate into high
overlap of logical forms. A reason for this differ-
ence is that the semantic interpreters used in the
extrinsic evaluation explore exhaustively the de-
pendency structures returned by both parsing sys-
tems and they try to recover as much information
as possible. In contrast with this, the generators of
grammatical relations used in the intrinsic evalua-
tion provide the most direct mapping from depen-
dency structures to grammatical relations. For ex-
ample, typically a dependency structure would not
show a long dependency like the subject of come
in the sentence John wanted Mary to come:
</bodyText>
<subsectionHeader confidence="0.404766">
John wanted.v Mary to.o come.v
</subsectionHeader>
<bodyText confidence="0.999998166666667">
As a result, the grammatical relations would not
show the subject of come. However, the subject
of come can be traced by following several de-
pendencies (I, TOo and Os above) and ExtrAns’
semantic interpreters do follow these dependen-
cies. In other words, the semantic interpreters
use more information than what is directly en-
coded in the dependency structures. Therefore,
the logical forms contain richer information than
the grammatical relations. We decided not to op-
timise the grammatical relations used in our eval-
uation because we wanted to test the expressivity
of the inherent grammars. It would be question-
able whether we should recover more information
than what is directly expressed. After all, provided
that the parse contains all the words in the origi-
nal order, we can theoretically ignore the sentence
structure and still recover all the information.
</bodyText>
<sectionHeader confidence="0.993333" genericHeader="evaluation">
6 Summary and Further Work
</sectionHeader>
<bodyText confidence="0.9999225">
We have performed intrinsic evaluations of
parsers and extrinsic evaluations within the
context of answer extraction. These evaluations
strengthen Galliers and Sparck Jones (1993)’s
claim that intrinsic evaluations are of very limited
value. In particular, our evaluations show that
intrinsic evaluations may provide results that
are distorted with respect to the most intuitive
purpose of a parsing system: to deliver syntactic
structures to subsequent modules of practical NLP
</bodyText>
<figure confidence="0.89431">
Ss
Os
TOo
I
</figure>
<bodyText confidence="0.999687153846154">
systems. There is a clear need for frameworks for
extrinsic evaluations of parsers for different NLP
applications.
Further research to confirm this conclusion will
be to try and minimise the occurrence of vari-
ables in the experiments by using the same corpus
for both the intrinsic and the extrinsic evaluations
and/or by using an answer extraction system that
operates on the level of grammatical relations in-
stead of MLFs. Additional further research will
be the use of other intrinsic evaluation methodolo-
gies and extrinsic evaluations within the context of
various other embedding setups.
</bodyText>
<sectionHeader confidence="0.992937" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.4800355">
This research is supported by the Macquarie Uni-
versity New Staff grant MUNS–9601/0069.
</bodyText>
<sectionHeader confidence="0.996675" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999774040540541">
Srinivas Bangalore, Anoop Sarkar, Christine Doran,
and Beth Ann Hockey. 1998. Grammar &amp; parser
evaluation in the XTAG project. In Proc. Workshop
on the Evaluation of Parsing Systems, LREC98.
Ezra Black, S.P. Abney, D. Flickinger, C. Gdaniec,
R. Grisham, P. Harrison, D. Hindle, R. Ingria,
F. Jelinek, J. Klavans, M. Liberman, M.P. Mar-
cus, S. Roukos, B. Santorini, and T. Strzalkowski.
1991. A procedure for quantitatively comparing the
syntactic coverage of English grammars. In Proc.
DARPA Speech and Natural Language Workshop,
pages 306–311, Pacific Grove, CA. Morgan Kauf-
mann.
Ezra Black. 1996. Evaluation of broad-coverage
natural-language parsers. In Ronald A. Cole, Joseph
Mariani, Hans Uszkoreit, Annie Zaenen, and Victor
Zue, editors, Survey of the State of the Art in Hu-
man Language Technology, pages 488–490. CSLU,
Oregon Graduate Institute.
John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new pro-
posal. In Proc. LREC98.
John Carroll, G. Minnen, and T. Briscoe. 1999. Corpus
annotation for parser evaluation.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc.
ACL. Santa Cruz.
Ann Copestake, Dan Flickinger, and Ivan A. Sag.
1997. Minimal recursion semantics: an introduc-
tion. Technical report, CSLI, Stanford University,
Stanford, CA.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81–120. Univ. of Pitts-
burgh Press.
Christiane Fellbaum. 1998. Wordnet: Introduction. In
Christiane Fellbaum, editor, WordNet: an electronic
lexical database, Language, Speech, and Communi-
cation, pages 1–19. MIT Press, Cambrige, MA.
Julia R. Galliers and Karen Sparck Jones. 1993. Evalu-
ating natural language processing systems. Techni-
cal Report TR-291, Computer Laboratory, Univer-
sity of Cambridge.
Jerry R. Hobbs. 1985. Ontological promiscuity. In
Proc. ACL’85, pages 61–69. University of Chicago,
Association for Computational Linguistics.
Timo J¨arvinen and Pasi Tapanainen. 1997. A depen-
dency parser for english. Technical Report TR-1,
Department of Linguistics, University of Helsinki,
Helsinki.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proc. IJCAI-
95, pages 1420–1425, Montreal, Canada.
Diego Moll´a, Rolf Schwitter, Michael Hess, and
Rachel Fournier. 2000. Extrans, an answer extrac-
tion system. T.A.L., 41(2):495–522.
Daniel D. Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Proc. Third Inter-
national Workshop on Parsing Technologies, pages
277–292.
Richard F. E. Sutcliffe, Heinz-Detlev Koch, and An-
nette McElligott, editors. 1996. Industrial Parsing
of Software Manuals. Rodopi, Amsterdam.
Pasi Tapanainen and Timo J¨arvinen. 1997. A non-
projective dependency parser. In Procs. ANLP-97.
ACL.
Ellen M. Voorhees. 2001a. Overview of the TREC
2001 question answering track. In Ellen M.
Voorhees and Donna K. Harman, editors, Proc.
TREC-10, number 500-250 in NIST Special Publi-
cation. NIST.
Ellen M. Voorhees. 2001b. The TREC question
answering track. Natural Language Engineering,
7(4):361–378.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526561">
<title confidence="0.999898">Intrinsic versus Extrinsic Evaluations of Parsing Systems</title>
<author confidence="0.976607">Diego</author>
<affiliation confidence="0.908479333333333">Centre for Language Department of Macquarie</affiliation>
<address confidence="0.887042">Sydney, NSW 2109,</address>
<email confidence="0.993741">diego@ics.mq.edu.au</email>
<author confidence="0.935565">Ben</author>
<affiliation confidence="0.9969805">Division of University of</affiliation>
<address confidence="0.851328">Edinburgh EH8 9LW, United</address>
<email confidence="0.980111">B.Hutchinson@sms.ed.ac.uk</email>
<abstract confidence="0.997811923076923">A wide range of parser and/or grammar evaluation methods have been reported in the literature. However, in most cases these evaluations take the parsers independently (intrinsic evaluations), and only in a few cases has the effect of different parsers in real applications been measured (extrinsic evaluations). This paper compares two evaluations of the Link Grammar parser and the Conexor Functional Dependency Grammar parser. The parsing systems, despite both being dependency-based, return different types of dependencies, making a direct comparison impossible. In the intrinsic evaluation, the accuracy of the parsers is compared independently by converting the dependencies into grammatical relations and using the methodology of Carroll et al. (1998) for parser comparison. In the extrinsic evaluation, the parsers’ impact in a practical application is compared within the context of answer extraction. The differences in the results are significant.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Anoop Sarkar</author>
<author>Christine Doran</author>
<author>Beth Ann Hockey</author>
</authors>
<title>Grammar &amp; parser evaluation in the XTAG project.</title>
<date>1998</date>
<booktitle>In Proc. Workshop on the Evaluation of Parsing Systems, LREC98.</booktitle>
<contexts>
<context position="2783" citStr="Bangalore et al. (1998)" startWordPosition="419" endWordPosition="422">ation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real application (an extrinsic evaluation). We have chosen answer extraction as an example of a practical application within which to test the parsing systems. In particular, the extrin</context>
</contexts>
<marker>Bangalore, Sarkar, Doran, Hockey, 1998</marker>
<rawString>Srinivas Bangalore, Anoop Sarkar, Christine Doran, and Beth Ann Hockey. 1998. Grammar &amp; parser evaluation in the XTAG project. In Proc. Workshop on the Evaluation of Parsing Systems, LREC98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>S P Abney</author>
<author>D Flickinger</author>
<author>C Gdaniec</author>
<author>R Grisham</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M P Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Proc. DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Pacific Grove, CA.</location>
<contexts>
<context position="2895" citStr="Black et al., 1991" startWordPosition="436" endWordPosition="439">an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real application (an extrinsic evaluation). We have chosen answer extraction as an example of a practical application within which to test the parsing systems. In particular, the extrinsic evaluation uses ExtrAns, an answer extraction system that operates over Unix manual pages (Moll´a et al., 20</context>
<context position="8191" citStr="Black et al., 1991" startWordPosition="1304" endWordPosition="1307">functions (e.g. subject, object), to specific types of modifiers (e.g. frequency, duration, location). Both Conexor FDG and Link Grammar also return non-dependency information. For Link Grammar, this consists of some word class information, shown as suffixes in Figure 1. For Conexor FDG, the base form morphological information of each word is returned, along with a “functional” tag or morpho-syntactic function and a “surface syntactic” tag for each word.&apos; 3 Intrinsic Evaluations Given that both parses are dependency-based, intrinsic evaluations that are based on constituency structures (e.g. (Black et al., 1991)) are hard to perform. Dependency-based evaluations are not easy either: directly comparing dependency graphs (as suggested by Lin (1995), for example) becomes difficult given the differences between the structures returned by the Link Grammar parser and Conexor FDG. We therefore need an approach that is independent from the format of the parser output. Following Carroll et al. (1998) we use grammatical relations to compare the accuracy of Link Grammar and Conexor FDG. Carroll et al. (1998) propose a set of twenty parser-independent grammatical relations arranged in a hierarchy representing di</context>
</contexts>
<marker>Black, Abney, Flickinger, Gdaniec, Grisham, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra Black, S.P. Abney, D. Flickinger, C. Gdaniec, R. Grisham, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M.P. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Proc. DARPA Speech and Natural Language Workshop, pages 306–311, Pacific Grove, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
</authors>
<title>Evaluation of broad-coverage natural-language parsers.</title>
<date>1996</date>
<booktitle>Survey of the State of the Art in Human Language Technology,</booktitle>
<pages>488--490</pages>
<editor>In Ronald A. Cole, Joseph Mariani, Hans Uszkoreit, Annie Zaenen, and Victor Zue, editors,</editor>
<publisher>CSLU, Oregon Graduate Institute.</publisher>
<contexts>
<context position="2731" citStr="Black (1996)" startWordPosition="412" endWordPosition="413">nes, 1993, p22). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real application (an extrinsic evaluation). We have chosen answer extraction as an example of a practical application within which to</context>
</contexts>
<marker>Black, 1996</marker>
<rawString>Ezra Black. 1996. Evaluation of broad-coverage natural-language parsers. In Ronald A. Cole, Joseph Mariani, Hans Uszkoreit, Annie Zaenen, and Victor Zue, editors, Survey of the State of the Art in Human Language Technology, pages 488–490. CSLU, Oregon Graduate Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal. In</title>
<date>1998</date>
<booktitle>Proc. LREC98.</booktitle>
<contexts>
<context position="1084" citStr="Carroll et al. (1998)" startWordPosition="149" endWordPosition="152">take the parsers independently (intrinsic evaluations), and only in a few cases has the effect of different parsers in real applications been measured (extrinsic evaluations). This paper compares two evaluations of the Link Grammar parser and the Conexor Functional Dependency Grammar parser. The parsing systems, despite both being dependency-based, return different types of dependencies, making a direct comparison impossible. In the intrinsic evaluation, the accuracy of the parsers is compared independently by converting the dependencies into grammatical relations and using the methodology of Carroll et al. (1998) for parser comparison. In the extrinsic evaluation, the parsers’ impact in a practical application is compared within the context of answer extraction. The differences in the results are significant. 1 Introduction Parsing is a principal stage in many natural language processing (NLP) systems. A good parser is expected to return an accurate syntactic structure of a sentence. This structure is typically forwarded to other modules so that they can work with unambiguous and well-defined structures representing the sentences. It is to be expected that the performance of an NLP system quickly degr</context>
<context position="2754" citStr="Carroll et al. (1998)" startWordPosition="414" endWordPosition="417">). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real application (an extrinsic evaluation). We have chosen answer extraction as an example of a practical application within which to test the parsing syste</context>
<context position="8578" citStr="Carroll et al. (1998)" startWordPosition="1366" endWordPosition="1369">or morpho-syntactic function and a “surface syntactic” tag for each word.&apos; 3 Intrinsic Evaluations Given that both parses are dependency-based, intrinsic evaluations that are based on constituency structures (e.g. (Black et al., 1991)) are hard to perform. Dependency-based evaluations are not easy either: directly comparing dependency graphs (as suggested by Lin (1995), for example) becomes difficult given the differences between the structures returned by the Link Grammar parser and Conexor FDG. We therefore need an approach that is independent from the format of the parser output. Following Carroll et al. (1998) we use grammatical relations to compare the accuracy of Link Grammar and Conexor FDG. Carroll et al. (1998) propose a set of twenty parser-independent grammatical relations arranged in a hierarchy representing different degrees of specificity. Four relations from the hierarchy are shown in Table 2. The arguments to &apos;See (J¨arvinen and Tapanainen, 1997) for more information on the output from Conexor FDG. each relation specify a head, a dependent, and possibly an initial grammatical relation (in the case of SUBJ in passive sentences, for example) or the ‘type’, which specifies the word introdu</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: a survey and a new proposal. In Proc. LREC98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>G Minnen</author>
<author>T Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<contexts>
<context position="11762" citStr="Carroll et al., 1999" startWordPosition="1842" endWordPosition="1845">J(eat,apple). 3.1 Accuracy Our intrinsic evaluation began on the assumption that grammatical relations could be deduced from the dependency structures returned by the parsers. In practise, however, this deduction process is not always straightforward; for example complexity arises when arguments are shared across clauses. In addition, Link Grammar’s analysis of the frontmost elements as clausal heads complicates the grammatical relation deduction when there are modifying clauses. An existing corpus of 500 sentences/10,000 words annotated with grammatical relations was used for the evaluation (Carroll et al., 1999). We restricted the evaluation to just the four relations shown in Table 2. This decision had two motivations. Firstly, since the dependency parsers’ output did not recognise some distinctions made in the hierarchy of relations, it did not make sense to test these distinctions. Secondly, we wanted the deduction of grammatical relations to be as simple a process as possible, to minimise the chance of introducing errors. This second consideration also led us to purposefully ignore the sharing of arguments induced by control verbs, as this could not always be deduced reliably. Since this was done</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>John Carroll, G. Minnen, and T. Briscoe. 1999. Corpus annotation for parser evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. ACL.</booktitle>
<location>Santa Cruz.</location>
<contexts>
<context position="2538" citStr="Collins (1996)" startWordPosition="384" endWordPosition="385">on: “Intrinsic criteria are those relating to a system’s objective, extrinsic criteria those relating to its function i.e. to its role in relation to its setup’s purpose.” (Galliers and Sparck Jones, 1993, p22). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic ev</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael John Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proc. ACL. Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: an introduction.</title>
<date>1997</date>
<tech>Technical report, CSLI,</tech>
<institution>Stanford University,</institution>
<location>Stanford, CA.</location>
<contexts>
<context position="15405" citStr="Copestake et al. (1997)" startWordPosition="2448" endWordPosition="2451"> and they are optimised for NLP tasks that involve the semantic comparison of sentences, such as AE. ExtrAns’ logical forms are called minimal logical forms (MLFs) because they encode the minimum information required for effective answer extraction. In particular, only the main dependencies between the verb and arguments are expressed, plus modifier and adjunct relations. Thus, complex quantification, tense and aspect, temporal relations, plurality, and modality are not expressed. The MLFs use reification to achieve flat expressions, very much in the line of Davidson (1967), Hobbs (1985), and Copestake et al. (1997). In the current implementation only reification to objects, eventualities (events or states), and properties is applied. For example, the MLF of the sentence cp will quickly copy files is: holds(e4), object(cp,o1,[x1]), object(s command,o2,[x1]), evt(s copy,e4,[x1,x6]), object(s file,o3,[x6]), prop(quickly,p3,[e4]). In other words, there is an entity x1 which represents an object of type command;2 there is an entity x6 (a file); there is an entity e4, which represents a copying event where the first argument is x1 and the second argument is x6; there is an entity p3 which states that e4 is do</context>
</contexts>
<marker>Copestake, Flickinger, Sag, 1997</marker>
<rawString>Ann Copestake, Dan Flickinger, and Ivan A. Sag. 1997. Minimal recursion semantics: an introduction. Technical report, CSLI, Stanford University, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>The Logic of Decision and Action,</booktitle>
<pages>81--120</pages>
<editor>In Nicholas Rescher, editor,</editor>
<publisher>Univ. of Pittsburgh Press.</publisher>
<contexts>
<context position="15362" citStr="Davidson (1967)" startWordPosition="2443" endWordPosition="2444">omplex or ungrammatical sentences), and they are optimised for NLP tasks that involve the semantic comparison of sentences, such as AE. ExtrAns’ logical forms are called minimal logical forms (MLFs) because they encode the minimum information required for effective answer extraction. In particular, only the main dependencies between the verb and arguments are expressed, plus modifier and adjunct relations. Thus, complex quantification, tense and aspect, temporal relations, plurality, and modality are not expressed. The MLFs use reification to achieve flat expressions, very much in the line of Davidson (1967), Hobbs (1985), and Copestake et al. (1997). In the current implementation only reification to objects, eventualities (events or states), and properties is applied. For example, the MLF of the sentence cp will quickly copy files is: holds(e4), object(cp,o1,[x1]), object(s command,o2,[x1]), evt(s copy,e4,[x1,x6]), object(s file,o3,[x6]), prop(quickly,p3,[e4]). In other words, there is an entity x1 which represents an object of type command;2 there is an entity x6 (a file); there is an entity e4, which represents a copying event where the first argument is x1 and the second argument is x6; there</context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Donald Davidson. 1967. The logical form of action sentences. In Nicholas Rescher, editor, The Logic of Decision and Action, pages 81–120. Univ. of Pittsburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: Introduction.</title>
<date>1998</date>
<booktitle>WordNet: an electronic lexical database, Language, Speech, and Communication,</booktitle>
<pages>1--19</pages>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambrige, MA.</location>
<contexts>
<context position="16393" citStr="Fellbaum, 1998" startWordPosition="2609" endWordPosition="2610"> an object of type command;2 there is an entity x6 (a file); there is an entity e4, which represents a copying event where the first argument is x1 and the second argument is x6; there is an entity p3 which states that e4 is done quickly, and the event e4, that is, the copying, holds. ExtrAns finds the answers to the questions by converting the MLFs of the questions into Prolog queries and then running Prolog’s default resolution mechanism to find those MLFs that can prove the question. This default search procedure is called the synonym mode since ExtrAns uses a small WordNetstyle thesaurus (Fellbaum, 1998) to convert all the synonyms into a synonym representative. ExtrAns also has an approximate mode which, besides normalising all synonyms, scores all document sentences on the basis of the maximum number of predicates that unify between the MLFs of the query and the answer candidate (Moll´a et al., 2000). If all query predicates can be matched then 2ExtrAns uses additional domain knowledge to infer that cp is a command. the approximate mode returns exactly the same answers as the synonym mode. 4.2 The Comparison Ideally, answer extraction systems should be evaluated according to how successful </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. Wordnet: Introduction. In Christiane Fellbaum, editor, WordNet: an electronic lexical database, Language, Speech, and Communication, pages 1–19. MIT Press, Cambrige, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia R Galliers</author>
<author>Karen Sparck Jones</author>
</authors>
<title>Evaluating natural language processing systems.</title>
<date>1993</date>
<tech>Technical Report TR-291,</tech>
<institution>Computer Laboratory, University of Cambridge.</institution>
<marker>Galliers, Jones, 1993</marker>
<rawString>Julia R. Galliers and Karen Sparck Jones. 1993. Evaluating natural language processing systems. Technical Report TR-291, Computer Laboratory, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Ontological promiscuity.</title>
<date>1985</date>
<booktitle>In Proc. ACL’85,</booktitle>
<pages>61--69</pages>
<institution>University of Chicago, Association for Computational Linguistics.</institution>
<contexts>
<context position="15376" citStr="Hobbs (1985)" startWordPosition="2445" endWordPosition="2446">atical sentences), and they are optimised for NLP tasks that involve the semantic comparison of sentences, such as AE. ExtrAns’ logical forms are called minimal logical forms (MLFs) because they encode the minimum information required for effective answer extraction. In particular, only the main dependencies between the verb and arguments are expressed, plus modifier and adjunct relations. Thus, complex quantification, tense and aspect, temporal relations, plurality, and modality are not expressed. The MLFs use reification to achieve flat expressions, very much in the line of Davidson (1967), Hobbs (1985), and Copestake et al. (1997). In the current implementation only reification to objects, eventualities (events or states), and properties is applied. For example, the MLF of the sentence cp will quickly copy files is: holds(e4), object(cp,o1,[x1]), object(s command,o2,[x1]), evt(s copy,e4,[x1,x6]), object(s file,o3,[x6]), prop(quickly,p3,[e4]). In other words, there is an entity x1 which represents an object of type command;2 there is an entity x6 (a file); there is an entity e4, which represents a copying event where the first argument is x1 and the second argument is x6; there is an entity </context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R. Hobbs. 1985. Ontological promiscuity. In Proc. ACL’85, pages 61–69. University of Chicago, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo J¨arvinen</author>
<author>Pasi Tapanainen</author>
</authors>
<title>A dependency parser for english.</title>
<date>1997</date>
<tech>Technical Report TR-1,</tech>
<institution>Department of Linguistics, University of Helsinki,</institution>
<location>Helsinki.</location>
<marker>J¨arvinen, Tapanainen, 1997</marker>
<rawString>Timo J¨arvinen and Pasi Tapanainen. 1997. A dependency parser for english. Technical Report TR-1, Department of Linguistics, University of Helsinki, Helsinki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proc. IJCAI95,</booktitle>
<pages>1420--1425</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2938" citStr="Lin, 1995" startWordPosition="443" endWordPosition="444">he parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real application (an extrinsic evaluation). We have chosen answer extraction as an example of a practical application within which to test the parsing systems. In particular, the extrinsic evaluation uses ExtrAns, an answer extraction system that operates over Unix manual pages (Moll´a et al., 2000). The two grammar systems to compare are</context>
<context position="8328" citStr="Lin (1995)" startWordPosition="1325" endWordPosition="1326">rn non-dependency information. For Link Grammar, this consists of some word class information, shown as suffixes in Figure 1. For Conexor FDG, the base form morphological information of each word is returned, along with a “functional” tag or morpho-syntactic function and a “surface syntactic” tag for each word.&apos; 3 Intrinsic Evaluations Given that both parses are dependency-based, intrinsic evaluations that are based on constituency structures (e.g. (Black et al., 1991)) are hard to perform. Dependency-based evaluations are not easy either: directly comparing dependency graphs (as suggested by Lin (1995), for example) becomes difficult given the differences between the structures returned by the Link Grammar parser and Conexor FDG. We therefore need an approach that is independent from the format of the parser output. Following Carroll et al. (1998) we use grammatical relations to compare the accuracy of Link Grammar and Conexor FDG. Carroll et al. (1998) propose a set of twenty parser-independent grammatical relations arranged in a hierarchy representing different degrees of specificity. Four relations from the hierarchy are shown in Table 2. The arguments to &apos;See (J¨arvinen and Tapanainen, </context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proc. IJCAI95, pages 1420–1425, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Moll´a</author>
<author>Rolf Schwitter</author>
<author>Michael Hess</author>
<author>Rachel Fournier</author>
</authors>
<title>Extrans, an answer extraction system.</title>
<date>2000</date>
<journal>T.A.L.,</journal>
<volume>41</volume>
<issue>2</issue>
<marker>Moll´a, Schwitter, Hess, Fournier, 2000</marker>
<rawString>Diego Moll´a, Rolf Schwitter, Michael Hess, and Rachel Fournier. 2000. Extrans, an answer extraction system. T.A.L., 41(2):495–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar. In</title>
<date>1993</date>
<booktitle>Proc. Third International Workshop on Parsing Technologies,</booktitle>
<pages>277--292</pages>
<contexts>
<context position="2605" citStr="Sleator and Temperley (1993)" startWordPosition="391" endWordPosition="394">m’s objective, extrinsic criteria those relating to its function i.e. to its role in relation to its setup’s purpose.” (Galliers and Sparck Jones, 1993, p22). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic evaluation and an evaluation of the impact of the parser in a real ap</context>
<context position="4314" citStr="Sleator and Temperley, 1993" startWordPosition="658" endWordPosition="661">ed to as Conexor FDG). These parsing systems were chosen because both include a dependency-based parser and a comprehensive grammar of English. However, the structures returned are so different that a direct comparison between them is not straightforward. In Section 2 we review the main differences between Link Grammar and Conexor FDG. In Section 3 we present the intrinsic comparison of parsers, and in Section 4 we comment on the extrinsic comparison within the context of answer extraction. The results of the evaluations are discussed in Section 5. 2 Link Grammar and Conexor FDG Link Grammar (Sleator and Temperley, 1993) is a grammar theory that is strongly dependencybased. A freely available parsing system that implements the Link Grammar theory has been developed at Carnegie Mellon University. The parsing system includes an extensive grammar and lexicon and has a wide coverage of the English language. Conexor FDG (Tapanainen and J¨arvinen, 1997) is a commercial parser and grammar, based on the theory of Functional Dependency Grammar, and was originally developed at the University of Helsinki. Despite both being dependency-based, there are substantial differences between the structures returned by the two pa</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Daniel D. Sleator and Davy Temperley. 1993. Parsing English with a link grammar. In Proc. Third International Workshop on Parsing Technologies, pages 277–292.</rawString>
</citation>
<citation valid="true">
<date>1996</date>
<booktitle>Industrial Parsing of Software Manuals. Rodopi,</booktitle>
<editor>Richard F. E. Sutcliffe, Heinz-Detlev Koch, and Annette McElligott, editors.</editor>
<location>Amsterdam.</location>
<contexts>
<context position="2538" citStr="(1996)" startWordPosition="385" endWordPosition="385">rinsic criteria are those relating to a system’s objective, extrinsic criteria those relating to its function i.e. to its role in relation to its setup’s purpose.” (Galliers and Sparck Jones, 1993, p22). Thus, an intrinsic evaluation of a parser would analyse the accuracy of the results returned by the parser as a stand-alone system, whereas an extrinsic evaluation would analyse the impact of the parser within the context of a broader NLP application. There are currently several parsing systems that attempt to achieve a wide coverage of the English language (such as those developed by Collins (1996), J¨arvinen and Tapanainen (1997), and Sleator and Temperley (1993)). There is also substantial literature on parsing evaluation (see, for example, work by Sutcliffe et al. (1996), Black (1996), Carroll et al. (1998), and Bangalore et al. (1998)). Recently there has been a shift from constituency-based (e.g. counting crossing brackets (Black et al., 1991)) to dependency-based evaluation (Lin, 1995; Carroll et al., 1998). Those evaluation methodologies typically focus on comparisons of stand-alone parsers (intrinsic evaluations). In this paper we report on the comparison between an intrinsic ev</context>
</contexts>
<marker>1996</marker>
<rawString>Richard F. E. Sutcliffe, Heinz-Detlev Koch, and Annette McElligott, editors. 1996. Industrial Parsing of Software Manuals. Rodopi, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Procs. ANLP-97.</booktitle>
<publisher>ACL.</publisher>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨arvinen. 1997. A nonprojective dependency parser. In Procs. ANLP-97. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>question answering track.</title>
<date>2001</date>
<booktitle>2001a. Overview of the TREC</booktitle>
<editor>In Ellen M. Voorhees and Donna K. Harman, editors,</editor>
<publisher>NIST.</publisher>
<contexts>
<context position="14344" citStr="Voorhees, 2001" startWordPosition="2275" endWordPosition="2276">ng errors affect the success of an NLP application. This is the goal of an extrinsic evaluation, where the system is evaluated in relation to the embedding setup. Using answer extraction as an example of an NLP application, we compared the performance of the Link Grammar system and Conexor FDG. 4.1 Answer Extraction and ExtrAns The fundamental goal of Answer Extraction (AE) is to locate those exact phrases of unedited text documents that answer a query worded in natural language. AE has received much attention recently, as the increasingly active Question Answering track in TREC demonstrates (Voorhees, 2001b; Voorhees, 2001a). ExtrAns is an answer extraction system that operates over UNIX manual pages (Moll´a et al., 2000). A core process in ExtrAns is the production of semantic information in the shape of logical forms for each sentence of each manual page, as well as the user query. These logical forms are designed so that they can be derived from any sentence (using robust approaches to treat very complex or ungrammatical sentences), and they are optimised for NLP tasks that involve the semantic comparison of sentences, such as AE. ExtrAns’ logical forms are called minimal logical forms (MLFs</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001a. Overview of the TREC 2001 question answering track. In Ellen M. Voorhees and Donna K. Harman, editors, Proc. TREC-10, number 500-250 in NIST Special Publication. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>The TREC question answering track.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="14344" citStr="Voorhees, 2001" startWordPosition="2275" endWordPosition="2276">ng errors affect the success of an NLP application. This is the goal of an extrinsic evaluation, where the system is evaluated in relation to the embedding setup. Using answer extraction as an example of an NLP application, we compared the performance of the Link Grammar system and Conexor FDG. 4.1 Answer Extraction and ExtrAns The fundamental goal of Answer Extraction (AE) is to locate those exact phrases of unedited text documents that answer a query worded in natural language. AE has received much attention recently, as the increasingly active Question Answering track in TREC demonstrates (Voorhees, 2001b; Voorhees, 2001a). ExtrAns is an answer extraction system that operates over UNIX manual pages (Moll´a et al., 2000). A core process in ExtrAns is the production of semantic information in the shape of logical forms for each sentence of each manual page, as well as the user query. These logical forms are designed so that they can be derived from any sentence (using robust approaches to treat very complex or ungrammatical sentences), and they are optimised for NLP tasks that involve the semantic comparison of sentences, such as AE. ExtrAns’ logical forms are called minimal logical forms (MLFs</context>
</contexts>
<marker>Voorhees, 2001</marker>
<rawString>Ellen M. Voorhees. 2001b. The TREC question answering track. Natural Language Engineering, 7(4):361–378.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>