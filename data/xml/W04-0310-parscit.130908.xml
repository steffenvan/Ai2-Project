<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.809995">
<title confidence="0.9985575">
Incrementality in Syntactic Processing: Computational Models
and Experimental Evidence
</title>
<author confidence="0.996797">
Patrick STURT
</author>
<affiliation confidence="0.998255666666667">
Human Communication Research Centre
Department of Psychology
University of Glasgow
</affiliation>
<address confidence="0.9827435">
58 Hillhead Street
Glasgow, SCOTLAND
</address>
<email confidence="0.998429">
patrick@psy.gla.ac.uk
</email>
<sectionHeader confidence="0.993876" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975844827587">
It is a well-known intuition that human sentence
understanding works in an incremental fashion,
with a seemingly constant update of the inter-
pretation through the left-to-right processing of
a string. Such intuitions are backed up by ex-
perimental evidence dating from at least as far
back as Marslen-Wilson (1973), showing that
under many circumstances, interpretations are
indeed updated very quickly.
From a parsing point of view it is interesting
to consider the structure-building processes that
might underlie incremental interpretation—
what kinds of partial structures are built dur-
ing sentence processing, and with what time-
course?
In this talk I will give an overview of the state-
of-the-art of experimental psycholinguistic re-
search, paying particular attention to the time-
course of structure-building. The discussion will
focus on a new line of research (some as yet un-
published) in which syntactic phenomena such
as binding relations (e.g., Sturt, 2003) and un-
bounded dependencies (e.g., Aoshima, Phillips,
&amp; Weinberg, in press) are exploited to make a
very direct test of the availability of syntactic
structure over time.
The experimental research will be viewed
from the perspective of a space of computa-
tional models, which make different predictions
about time-course of structure building. One
dimension in this space is represented by the
parsing algorithm used: For example, within
the framework of Generalized Left Corner Pars-
ing (Demers, 1977), algorithms can be char-
acterized in terms of the point at which a
context-free rule is recognized, in relation to the
recognition-point of the symbols on its right-
hand side. Another relevant dimension is repre-
sented by the type of grammar formalism that
is assumed. For example, with bottom-up pars-
ing algorithms, the degree to which structure-
building is delayed in right-branching structures
depends heavily on whether we employ a tra-
ditional phrase-structure formalism with rigid
constituency, or a cateogorial formalism with
flexible constituency (e.g., Steedman, 2000).
I will argue that the evidence is incompatible
with models which predict systematic delays in
the construction of syntactic structure. In par-
ticular, I will argue against both head-driven
strategies (e.g., Mulders, 2002), and purely
bottom-up parsing strategies, even when flex-
ible constituency is employed. Instead, I will
argue that to capture the data in the most par-
simonious way, we should turn our attention to
those models in which a fully connected syn-
tactic structure is maintained throughout the
processing of a string.
</bodyText>
<sectionHeader confidence="0.996094" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.931811476190476">
Aoshima, S., Phillips, C., &amp; Weinberg, A. (in
press). Processing iller-gap dependencies
in a head-inal language. To appear in
Journal of Memory and Language.
Demers, A. J. (1977). Generalized left cor-
ner parsing. In Proceedings of the 4th
acm sigact-sigplan symposium on princi-
ples of programming languages (pp. 170–
182). ACM Press.
Marslen-Wilson, W. (1973). Linguistic struc-
ture and speech shadowing at very short
latencies. Nature, 244, 522–533.
Mulders, I. (2002). Transparent parsing: Head-
driven processing of verb-inal structures.
Utrecht: LOT.
Steedman, M. (2000). The syntactic process.
Cambridge, MA: MIT press.
Sturt, P. (2003). The time-course of the appli-
cation of binding constraints in reference
resolution. Journal of Memory and Lan-
guage, 48(3), 542–562.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.219404">
<title confidence="0.974528">Incrementality in Syntactic Processing: Computational and Experimental Evidence</title>
<author confidence="0.985854">Patrick</author>
<affiliation confidence="0.985076333333333">Human Communication Research Department of University of</affiliation>
<address confidence="0.959985">58 Hillhead Glasgow,</address>
<email confidence="0.999633">patrick@psy.gla.ac.uk</email>
<abstract confidence="0.992409325">It is a well-known intuition that human sentence understanding works in an incremental fashion, with a seemingly constant update of the interpretation through the left-to-right processing of a string. Such intuitions are backed up by experimental evidence dating from at least as far back as Marslen-Wilson (1973), showing that under many circumstances, interpretations are indeed updated very quickly. From a parsing point of view it is interesting to consider the structure-building processes that might underlie incremental interpretation— what kinds of partial structures are built during sentence processing, and with what timecourse? In this talk I will give an overview of the stateof-the-art of experimental psycholinguistic research, paying particular attention to the timecourse of structure-building. The discussion will focus on a new line of research (some as yet unpublished) in which syntactic phenomena such as binding relations (e.g., Sturt, 2003) and unbounded dependencies (e.g., Aoshima, Phillips, &amp; Weinberg, in press) are exploited to make a very direct test of the availability of syntactic structure over time. The experimental research will be viewed from the perspective of a space of computational models, which make different predictions about time-course of structure building. One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized Left Corner Parsing (Demers, 1977), algorithms can be characterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its righthand side. Another relevant dimension is represented by the type of grammar formalism that is assumed. For example, with bottom-up parsing algorithms, the degree to which structurebuilding is delayed in right-branching structures depends heavily on whether we employ a traditional phrase-structure formalism with rigid constituency, or a cateogorial formalism with flexible constituency (e.g., Steedman, 2000). I will argue that the evidence is incompatible with models which predict systematic delays in the construction of syntactic structure. In particular, I will argue against both head-driven strategies (e.g., Mulders, 2002), and purely bottom-up parsing strategies, even when flexible constituency is employed. Instead, I will argue that to capture the data in the most parsimonious way, we should turn our attention to those models in which a fully connected syntactic structure is maintained throughout the processing of a string. References Aoshima, S., Phillips, C., &amp; Weinberg, A. (in iller-gap dependencies a head-inal language. appear in of Memory and Demers, A. J. (1977). Generalized left corparsing. In of the 4th acm sigact-sigplan symposium on princiof programming languages 170– 182). ACM Press. Marslen-Wilson, W. (1973). Linguistic structure and speech shadowing at very short 522–533. I. (2002). parsing: Headdriven processing of verb-inal structures. Utrecht: LOT. M. (2000). syntactic process. Cambridge, MA: MIT press. Sturt, P. (2003). The time-course of the application of binding constraints in reference of Memory and Lan-</abstract>
<address confidence="0.362399">542–562.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>S Aoshima</author>
<author>C Phillips</author>
<author>A Weinberg</author>
</authors>
<title>(in press). Processing iller-gap dependencies in a head-inal language.</title>
<journal>Journal of Memory and Language.</journal>
<note>To appear in</note>
<marker>Aoshima, Phillips, Weinberg, </marker>
<rawString>Aoshima, S., Phillips, C., &amp; Weinberg, A. (in press). Processing iller-gap dependencies in a head-inal language. To appear in Journal of Memory and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Demers</author>
</authors>
<title>Generalized left corner parsing.</title>
<date>1977</date>
<booktitle>In Proceedings of the 4th</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1716" citStr="Demers, 1977" startWordPosition="250" endWordPosition="251"> of research (some as yet unpublished) in which syntactic phenomena such as binding relations (e.g., Sturt, 2003) and unbounded dependencies (e.g., Aoshima, Phillips, &amp; Weinberg, in press) are exploited to make a very direct test of the availability of syntactic structure over time. The experimental research will be viewed from the perspective of a space of computational models, which make different predictions about time-course of structure building. One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized Left Corner Parsing (Demers, 1977), algorithms can be characterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its righthand side. Another relevant dimension is represented by the type of grammar formalism that is assumed. For example, with bottom-up parsing algorithms, the degree to which structurebuilding is delayed in right-branching structures depends heavily on whether we employ a traditional phrase-structure formalism with rigid constituency, or a cateogorial formalism with flexible constituency (e.g., Steedman, 2000). I will argue that the evi</context>
</contexts>
<marker>Demers, 1977</marker>
<rawString>Demers, A. J. (1977). Generalized left corner parsing. In Proceedings of the 4th acm sigact-sigplan symposium on principles of programming languages (pp. 170– 182). ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Marslen-Wilson</author>
</authors>
<title>Linguistic structure and speech shadowing at very short latencies.</title>
<date>1973</date>
<journal>Nature,</journal>
<volume>244</volume>
<pages>522--533</pages>
<marker>Marslen-Wilson, 1973</marker>
<rawString>Marslen-Wilson, W. (1973). Linguistic structure and speech shadowing at very short latencies. Nature, 244, 522–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mulders</author>
</authors>
<title>Transparent parsing: Headdriven processing of verb-inal structures.</title>
<date>2002</date>
<location>Utrecht: LOT.</location>
<marker>Mulders, 2002</marker>
<rawString>Mulders, I. (2002). Transparent parsing: Headdriven processing of verb-inal structures. Utrecht: LOT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>The syntactic process.</title>
<date>2000</date>
<publisher>MIT press.</publisher>
<location>Cambridge, MA:</location>
<marker>Steedman, 2000</marker>
<rawString>Steedman, M. (2000). The syntactic process. Cambridge, MA: MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sturt</author>
</authors>
<title>The time-course of the application of binding constraints in reference resolution.</title>
<date>2003</date>
<journal>Journal of Memory and Language,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>542--562</pages>
<contexts>
<context position="1216" citStr="Sturt, 2003" startWordPosition="173" endWordPosition="174">, interpretations are indeed updated very quickly. From a parsing point of view it is interesting to consider the structure-building processes that might underlie incremental interpretation— what kinds of partial structures are built during sentence processing, and with what timecourse? In this talk I will give an overview of the stateof-the-art of experimental psycholinguistic research, paying particular attention to the timecourse of structure-building. The discussion will focus on a new line of research (some as yet unpublished) in which syntactic phenomena such as binding relations (e.g., Sturt, 2003) and unbounded dependencies (e.g., Aoshima, Phillips, &amp; Weinberg, in press) are exploited to make a very direct test of the availability of syntactic structure over time. The experimental research will be viewed from the perspective of a space of computational models, which make different predictions about time-course of structure building. One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized Left Corner Parsing (Demers, 1977), algorithms can be characterized in terms of the point at which a context-free rule is recognized, </context>
</contexts>
<marker>Sturt, 2003</marker>
<rawString>Sturt, P. (2003). The time-course of the application of binding constraints in reference resolution. Journal of Memory and Language, 48(3), 542–562.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>