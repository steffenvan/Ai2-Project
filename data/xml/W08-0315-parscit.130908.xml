<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001317">
<title confidence="0.908768">
The TALP-UPC Ngram-based statistical machine translation system for
ACL-WMT 2008
</title>
<author confidence="0.960210666666667">
Maxim Khalilov, Adolfo Hernández H., Marta R. Costa-jussà,
Josep M. Crego, Carlos A. Henríquez Q., Patrik Lambert,
José A. R. Fonollosa, José B. Mariño and Rafael E. Banchs
</author>
<affiliation confidence="0.8505715">
Department of Signal Theory and Communications
TALP Research Center (UPC)
</affiliation>
<address confidence="0.924765">
Barcelona 08034, Spain
</address>
<email confidence="0.98716">
(khalilov, adolfohh, mruiz, jmcrego, carloshq, lambert, adrian, canton, rbanchs)@gps.tsc.upc.edu
</email>
<sectionHeader confidence="0.995543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998761352941177">
This paper reports on the participation of the TALP
Research Center of the UPC (Universitat Politècnica
de Catalunya) to the ACL WMT 2008 evaluation
campaign.
This year’s system is the evolution of the one we em-
ployed for the 2007 campaign. Main updates and
extensions involve linguistically motivated word re-
ordering based on the reordering patterns technique.
In addition, this system introduces a target language
model, based on linguistic classes (Part-of-Speech),
morphology reduction for an inflectional language
(Spanish) and an improved optimization procedure.
Results obtained over the development and test sets
on Spanish to English (and the other way round)
translations for both the traditional Europarl and
a challenging News stories tasks are analyzed and
commented.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897482758621">
Over the past few years, the Statistical Machine Transla-
tion (SMT) group of the TALP-UPC has been develop-
ing the Ngram-based SMT system (Mariño et al., 2006).
In previous evaluation campaigns the Ngram-based ap-
proach has proved to be comparable with the state-of-
the-art phrase-based systems, as shown in Koehn and
Monz(2006), Callison-Burch et al. (2007).
We present a summary of the TALP-UPC Ngram-
based SMT system used for this shared task. We dis-
cuss the system configuration and novel features, namely
linguistically motivated reordering technique, which is
applied on the decoding step. Additionally, the reorder-
ing procedure is supported by an Ngram language model
(LM) of reordered source Part-of-Speech tags (POS).
In this year’s evaluation we submitted systems for
Spanish-English and English-Spanish language pairs for
the traditional (Europarl) and challenging (News) tasks.
In each case, we used only the supplied data for each lan-
guage pair for models training and optimization.
This paper is organized as follows. Section 2 briefly
outlines the 2008 system, including tuple definition and
extraction, translation model and additional feature mod-
els, decoding tool and optimization procedure. Section 3
describes the word reordering problem and presents the
proposed technique of reordering patterns learning and
application. Later on, Section 4 reports on the experi-
mental setups of the WMT 2008 evaluation campaign. In
Section 5 we sum up the main conclusions from the pa-
per.
</bodyText>
<sectionHeader confidence="0.939846" genericHeader="method">
2 Ngram-based SMT System
</sectionHeader>
<bodyText confidence="0.99913">
Our translation system implements a log-linear model in
which a foreign language sentence fJ1 = f1, f2, ..., fJ
is translated into another language eI1 = f1, f2, ..., eI by
searching for the translation hypothesis oI1 maximizing a
log-linear combination of several feature models (Brown
et al., 1990):
</bodyText>
<equation confidence="0.9849425">
M
oI1 = argmax { m=1 E amhm(ef, fJ1 )
eI l
1
</equation>
<bodyText confidence="0.9896028">
where the feature functions
refer to the system models
and the set of
refers to the weights corresponding to
these models.
The core part of the system constructed in that way
is a translation model, which is based on bilingual n-
grams. It actually constitutes an Ngram-based LM of
bilingual units (called tuples), which approximates the
joint probability between the languages under consider-
ation. The procedure of tuples extraction from aword-
to-word alignment according to certain constraints is ex-
plained in detail in
et al. (2006).
The Ngram-based approach differs fr
</bodyText>
<equation confidence="0.911976">
hm
am
Mariño
</equation>
<bodyText confidence="0.898852">
om the phrase-
based SMT mainly by distinct representating of the bilin-
gual units defined by word alignment and using a higher
</bodyText>
<page confidence="0.988531">
127
</page>
<affiliation confidence="0.39309">
Proceedings of the Third Workshop on Statistical Machine Translation, pages
Columbus, Ohio, USA, June 2008.
Association for Computational Linguisti
</affiliation>
<page confidence="0.8024315">
127–130,
c�2008
</page>
<bodyText confidence="0.965109454545454">
cs
order HMM of the translation process. While regular
phrase-based SMT considers context only for phrase re-
ordering but not for translation, the N-gram based ap-
proach conditions translation decisions on previous trans-
lation decisions.
The TALP-UPC 2008 translation system, besides the
bilingual translation model, which consists of a 4-gram
LM of tuples with Kneser-Ney discounting (estimated
with SRI Language Modeling Toolkit1), implements a
log-linear combination of five additional feature models:
</bodyText>
<listItem confidence="0.983696454545455">
• a target language model (a 4-gram model of words,
estimated with Kneser-Ney smoothing);
• a POS target language model (a 4-gram model of
tags with Good-Turing discounting (TPOS));
• a word bonus model, which is used to compensate
the system’s preference for short output sentences;
• a source-to-target lexicon model and a target-to-
source lexicon model, these models use word-to-
word IBM Model 1 probabilities (Och and Ney,
2004) to estimate the lexical weights for each tuple
in the translation table.
</listItem>
<bodyText confidence="0.9997372">
Decisions on the particular LM configuration and
smoothing technique were taken on the minimal-
perplexity and maximal-BLEU bases.
The decoder (called MARIE), an open source tool2,
implementing a beam search strategy with distortion ca-
pabilities was used in the translation system.
Given the development set and references, the log-
linear combination of weights was adjusted using a sim-
plex optimization method (with the optimization criteria
of the highest BLEU score ) and an n-best re-ranking
just as described in http://www.statmt.org/jhuws/. This
strategy allows for a faster and more efficient adjustment
of model weights by means of a double-loop optimiza-
tion, which provides significant reduction of the number
of translations that should be carried out.
</bodyText>
<sectionHeader confidence="0.992728" genericHeader="method">
3 Reordering framework
</sectionHeader>
<bodyText confidence="0.998605555555555">
For a great number of translation tasks a certain reorder-
ing strategy is required. This is especially important
when the translation is performed between pairs of lan-
guages with non-monotonic word order. There are var-
ious types of distortion models, simplifying bilingual
translation. In our system we use an extended monotone
reordering model based on automatically learned reorder-
ing rules. A detailed description can be found in Crego
and Mariño (2006).
</bodyText>
<footnote confidence="0.9998935">
1http://www.speech.sri.com/projects/srilm/
2http://gps-tsc.upc.es/veu/soft/soft/marie/
</footnote>
<bodyText confidence="0.999296">
Apart from that, tuples were extracted by an unfold-
ing technique: this means that the tuples are broken into
smaller tuples, and these are sequenced in the order of the
target words.
</bodyText>
<subsectionHeader confidence="0.999509">
3.1 Reordering patterns
</subsectionHeader>
<bodyText confidence="0.9995095">
Word movements are realized according to the reordering
rewrite rules, which have the form of:
</bodyText>
<equation confidence="0.49371">
t1, ..., tn H i1, ..., in
</equation>
<bodyText confidence="0.999936">
where t1, ..., tn is a sequence of POS tags (relating a
sequence of source words), and i1, ..., in indicates which
order of the source words generate monotonically the tar-
get words.
Patterns are extracted in training from the crossed links
found in the word alignment, in other words, found in
translation tuples (as no word within a tuple can be linked
to a word out of it (Crego and Mariño, 2006)).
Having all the instances of rewrite patterns, a score for
each pattern on the basis of relative frequency is calcu-
lated as shown below:
</bodyText>
<equation confidence="0.981683333333333">
N(t1, ..., tn �→ i1, ..., in)
p(t1, ..., tn �→ i1, ..., in) = NN(t1, ...,
tn)
</equation>
<subsectionHeader confidence="0.998701">
3.2 Search graph extension and source POS model
</subsectionHeader>
<bodyText confidence="0.9989695">
The monotone search graph is extended with reorderings
following the patterns found in training. Once the search
graph is built, the decoder traverses the graph looking for
the best translation. Hence, the winning hypothesis is
computed using all the available information (the whole
SMT models).
</bodyText>
<figureCaption confidence="0.988663">
Figure 1: Search graph extension. NC, CC and AQ stand re-
</figureCaption>
<subsubsectionHeader confidence="0.607015">
spectively for name, conjunction and adjective.
</subsubsectionHeader>
<bodyText confidence="0.9998936">
The procedure identifies first the sequences of words
in the input sentence that match any available pattern.
Then, each ofthe matchings implies the addition of an arc
into the search graph (encoding the reordering learned in
the pattern). However, this addition of a new arc is not
</bodyText>
<page confidence="0.978041">
128
</page>
<table confidence="0.9998395">
Task BL BL+SPOS
Europarl News Europarl News
es2en 32.79 36.09 32.88 36.36
en2es 32.05 33.91 32.10 33.63
</table>
<tableCaption confidence="0.98494">
Table 1: BLEU comparison demonstrating the impact of the
source-side POS tags model.
</tableCaption>
<bodyText confidence="0.998774">
performed if a translation unit with the same source-side
words already exists in the training. Figure 1 shows how
two rewrite rules applied over an input sentence extend
the search graph given the reordering patterns that match
the source POS tag sequence.
The reordering strategy is additionally supported by
a 4-gram language model (estimated with Good-Turing
smoothing) of reordered source POS tags (SPOS). In
training, POS tags are reordered according with the ex-
tracted reordering patterns and word-to-word links. The
resulting sequence of source POS tags is used to train the
Ngram LM.
Table 1 presents the effect of the source POS LM in-
troduction to the reordering module of the Ngram-based
SMT. As it can be seen, the impactya le h of the source-
side POS LM is minimal, however we decided to consider
the model aiming at improving it in future. The reported
results are related to the Europarl and News Commen-
tary (News) development sets. BLEU calculation is case
insensitive and insensitive to tokenization. BL (baseline)
refers to the presented Ngram-based system considering
all the features, apart from the target and source POS
models.
</bodyText>
<sectionHeader confidence="0.994875" genericHeader="method">
4 WMT 2008 Evaluation Framework
</sectionHeader>
<subsectionHeader confidence="0.961906">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.9999221875">
An extraction of the official transcriptions of the 3rd re-
lease of the European Parliament Plenary Sessions3 was
provided for the ACL WMT 2008 shared translation task.
About 40 times smaller corpus from news domain (called
News Commentary) was also available. For both tasks,
our training corpus was the catenation of the Europarl and
News Commentary corpora.
TALP UPC participated in the constraint to the
provided training data track for Spanish-English and
English-Spanish translation tasks. We used the same
training material for the traditional and challenging tasks,
while the development sets used to tune the system were
distinct (2000 sentences for Europarl task and 1057
for News Commentary, one reference translation for
each of them). A brief training and development corpora
statistics is presented in Table 2.
</bodyText>
<footnote confidence="0.888792">
3http://www.statmt.org/wmt08/shared-task.html
</footnote>
<table confidence="0.990900769230769">
Spanish English
Train
Sentences 1.3 M 1.3 M
Words 38.2 M 35.8 K
Vocabulary 156 K 120 K
Development Europarl
Sentences 2000 2000
Words 61.8 K 58.7 K
Vocabulary 8 K 6.5 K
Development News Commentary
Sentences 1057 1057
Words 29.8 K 25.8 K
Vocabulary 5.4 K 4.9 K
</table>
<tableCaption confidence="0.998689">
Table 2: Basic statistics of ACL WMT 2008 corpus.
</tableCaption>
<subsectionHeader confidence="0.997895">
4.2 Processing details
</subsectionHeader>
<bodyText confidence="0.999782777777778">
The training data was preprocessed by using provided
tools for tokenizing and filtering.
POS tagging. POS information for the source and the
target languages was considered for both translation tasks
that we have participated. The software tools available
for performing POS-tagging were Freeling (Carreras et
al., 2004) for Spanish and TnT (Brants, 2000) for En-
glish. The number of classes for English is 44, while
Spanish is considered as a more inflectional language,
and the tag set contains 376 different tags.
Word Alignment. The word alignment is automati-
cally computed by using GIZA++4(Och and Ney, 2000)
in both directions, which are symmetrized by using the
union operation. Instead of aligning words themselves,
stems are used for aligning. Afterwards case sensitive
words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al were splitted into de el or
a el. As a post-processing, in the En2Es direction we
used a POS target LM as a feature (instead of the target
language model based on classes) that allowed to recover
the segmentations (de Gispert, 2006).
</bodyText>
<subsectionHeader confidence="0.996458">
4.3 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.971376">
In contrast to the last year’s system where statistical
classes were used to train the target-side tags LM, this
year we used linguistically motivated word classes
</bodyText>
<footnote confidence="0.980657">
4http://code.google.com/p/giza-pp/
</footnote>
<page confidence="0.9885">
129
</page>
<table confidence="0.999588">
Task BL+SPOS BL+SPOS+TPOS
(UPC 2008)
Europarl News Europarl News
es2en 32.88 36.36 32.89 36.31
en2es 31.52 34.13 30.72 32.72
en2es &amp;quot;clean&amp;quot;5 32.10 33.63 32.09 35.04
</table>
<tableCaption confidence="0.866975">
Table 3: BLEU scores for Spanish-English and English-Spanish
2008 development corpora (Europarl and News Commentary).
</tableCaption>
<table confidence="0.9996072">
Task UPC 2008
Europarl News
es2en 32.80 19.61
en2es 31.31 19.28
en2es &amp;quot;clean&amp;quot;5 32.34 20.05
</table>
<tableCaption confidence="0.999006">
Table 4: BLEU scores for official tests 2008.
</tableCaption>
<sectionHeader confidence="0.997046" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999510444444444">
In this paper we introduced the TALP UPC Ngram-based
SMT system participating in the WMT08 evaluation.
Apart from briefly summarizing the decoding and opti-
mization processes, we have presented the feature mod-
els that were taken into account, along with the bilingual
Ngram translation model. A reordering strategy based on
linguistically-motivated reordering patterns to harmonize
the source and target word order has been presented in
the framework of the Ngram-based system.
</bodyText>
<sectionHeader confidence="0.999218" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999904428571429">
This work has been funded by the Spanish Government
under grant TEC2006-13964-C03 (AVIVAVOZ project).
The authors want to thank Adrià de Gispert (Cambridge
University) for his contribution to this work.
(POS) which were considered to train the POS target LM
and extract the reordering patterns. Other characteristics
of this year’s system are:
</bodyText>
<listItem confidence="0.970570666666667">
• reordering patterns technique;
• source POS model, supporting word reordering;
• no LM interpolation. For this year’s evaluation, we
</listItem>
<bodyText confidence="0.99129164">
trained two separate LMs for each domain-specific
corpus (i.e., Europarl and News Commentary tasks).
It is important to mention that 2008 training material is
identical to the one provided for the 2007 shared transla-
tion task.
Table 3 presents the BLEU score obtained for the 2008
development data sets and shows the impact of the target-
side POS LM introduction, which can be characterized as
highly corpus- and language-dependent feature. BL refers
to the same system configuration as described in subsec-
tion 3.2. The computed BLEU scores are case insensitive,
insensitive to tokenization and use one translation refer-
ence.
After submitting the systems we discovered a bug re-
lated to incorrect implementation of the target LMs of
words and tags for Spanish, it caused serious reduction
of translation quality (1.4 BLEU points for development
set in case of English-to-Spanish Europarl task and 2.3
points in case of the corresponding News Commentary
task). The last raw of table 3 (en2es &amp;quot;clean&amp;quot;) repre-
sents the results corresponding to the UPC 2008 post-
evaluation system, while the previous one (en2es) refers
to the &amp;quot;bugged&amp;quot; system submitted to the evaluation.
The experiments presented in Table 4 correspond to the
2008 test evaluation sets.
</bodyText>
<footnote confidence="0.717343">
5Corrected post-evaluation results (see subsection 4.3.)
</footnote>
<sectionHeader confidence="0.997212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862">
T. Brants. 2000. TnT – a statistical part-of-speech tagger. In
Proceedings ofthe 6th Applied Natural Language Processing
(ANLP-2000).
P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek,
J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A sta-
tistical approach to machine translation. Computational Lin-
guistics, 16(2):79–85.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) evaluation of machine trans-
lation. In Proceedings of the ACL 2007 Workshop on Statis-
tical and Hybrid methods for Machine Translation (WMT),
pages 136–158.
X. Carreras, I. Chao, L. Padró, and M. Padró. 2004. Freeling:
An open-source suite of language analyzers. In Proceedings
of the 4th Int. Conf. on Language Resources and Evaluation
(LREC’04).
J. M. Crego and J. B. Mariño. 2006. Improving statistical MT
by coupling reordering and decoding. Machine Translation,
20(3):199–215.
A. de Gispert. 2006. Introducing linguistic knowledge into
statistical machine translation. Ph.D. thesis, Universitat
Politècnica de Catalunya, December.
P. Koehn and C. Monz. 2006. Manual and automatic eval-
uation of machine translation between european languages.
In Proceedings of the ACL 2006 Workshop on Statistical and
Hybrid methods forMachine Translation (WMT), pages 102–
121.
J. B. Mariño, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lam-
bert, J. A. R. Fonollosa, and M. R. Costa-jussà. 2006. N-
gram based machine translation. Computational Linguistics,
32(4):527–549, December.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In Proceedings of the the 38th Annual Meeting
on Association for Computational Linguistics (ACL), pages
440–447.
F. Och and H. Ney. 2004. The alignment template approach to
statistical machine translation. 30(4):417 – 449, December.
</reference>
<page confidence="0.99761">
130
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369236">
<title confidence="0.7286665">The TALP-UPC Ngram-based statistical machine translation system ACL-WMT 2008</title>
<author confidence="0.971469">Maxim Khalilov</author>
<author confidence="0.971469">Adolfo Hernández H</author>
<author confidence="0.971469">Marta R Josep M Crego</author>
<author confidence="0.971469">Carlos A Henríquez Q</author>
<author confidence="0.971469">Patrik A R Fonollosa</author>
<author confidence="0.971469">José B Mariño E</author>
<affiliation confidence="0.999475">Department of Signal Theory and TALP Research Center</affiliation>
<address confidence="0.995099">Barcelona 08034,</address>
<email confidence="0.998459">(khalilov,adolfohh,mruiz,jmcrego,carloshq,lambert,adrian,canton,rbanchs)@gps.tsc.upc.edu</email>
<abstract confidence="0.988684722222222">This paper reports on the participation of the TALP Research Center of the UPC (Universitat Politècnica de Catalunya) to the ACL WMT 2008 evaluation campaign. This year’s system is the evolution of the one we employed for the 2007 campaign. Main updates and extensions involve linguistically motivated word reordering based on the reordering patterns technique. In addition, this system introduces a target language model, based on linguistic classes (Part-of-Speech), morphology reduction for an inflectional language (Spanish) and an improved optimization procedure. Results obtained over the development and test sets on Spanish to English (and the other way round) translations for both the traditional Europarl and a challenging News stories tasks are analyzed and commented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – a statistical part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofthe 6th Applied Natural Language Processing (ANLP-2000).</booktitle>
<contexts>
<context position="10943" citStr="Brants, 2000" startWordPosition="1709" endWordPosition="1710">156 K 120 K Development Europarl Sentences 2000 2000 Words 61.8 K 58.7 K Vocabulary 8 K 6.5 K Development News Commentary Sentences 1057 1057 Words 29.8 K 25.8 K Vocabulary 5.4 K 4.9 K Table 2: Basic statistics of ACL WMT 2008 corpus. 4.2 Processing details The training data was preprocessed by using provided tools for tokenizing and filtering. POS tagging. POS information for the source and the target languages was considered for both translation tasks that we have participated. The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags. Word Alignment. The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation. Instead of aligning words themselves, stems are used for aligning. Afterwards case sensitive words are recovered. Spanish Morphology Reduction. We implemented a morphology reduction of the Spanish language as a preprocessing step. As a consequence, training data sparsen</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. TnT – a statistical part-of-speech tagger. In Proceedings ofthe 6th Applied Natural Language Processing (ANLP-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="3068" citStr="Brown et al., 1990" startWordPosition="457" endWordPosition="460">ocedure. Section 3 describes the word reordering problem and presents the proposed technique of reordering patterns learning and application. Later on, Section 4 reports on the experimental setups of the WMT 2008 evaluation campaign. In Section 5 we sum up the main conclusions from the paper. 2 Ngram-based SMT System Our translation system implements a log-linear model in which a foreign language sentence fJ1 = f1, f2, ..., fJ is translated into another language eI1 = f1, f2, ..., eI by searching for the translation hypothesis oI1 maximizing a log-linear combination of several feature models (Brown et al., 1990): M oI1 = argmax { m=1 E amhm(ef, fJ1 ) eI l 1 where the feature functions refer to the system models and the set of refers to the weights corresponding to these models. The core part of the system constructed in that way is a translation model, which is based on bilingual ngrams. It actually constitutes an Ngram-based LM of bilingual units (called tuples), which approximates the joint probability between the languages under consideration. The procedure of tuples extraction from awordto-word alignment according to certain constraints is explained in detail in et al. (2006). The Ngram-based app</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. Brown, J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. D. Lafferty, R. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Fordyce</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
</authors>
<title>(Meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Workshop on Statistical and Hybrid methods for Machine Translation (WMT),</booktitle>
<pages>136--158</pages>
<contexts>
<context position="1607" citStr="Callison-Burch et al. (2007)" startWordPosition="231" endWordPosition="234">anish) and an improved optimization procedure. Results obtained over the development and test sets on Spanish to English (and the other way round) translations for both the traditional Europarl and a challenging News stories tasks are analyzed and commented. 1 Introduction Over the past few years, the Statistical Machine Translation (SMT) group of the TALP-UPC has been developing the Ngram-based SMT system (Mariño et al., 2006). In previous evaluation campaigns the Ngram-based approach has proved to be comparable with the state-ofthe-art phrase-based systems, as shown in Koehn and Monz(2006), Callison-Burch et al. (2007). We present a summary of the TALP-UPC Ngrambased SMT system used for this shared task. We discuss the system configuration and novel features, namely linguistically motivated reordering technique, which is applied on the decoding step. Additionally, the reordering procedure is supported by an Ngram language model (LM) of reordered source Part-of-Speech tags (POS). In this year’s evaluation we submitted systems for Spanish-English and English-Spanish language pairs for the traditional (Europarl) and challenging (News) tasks. In each case, we used only the supplied data for each language pair f</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and J. Schroeder. 2007. (Meta-) evaluation of machine translation. In Proceedings of the ACL 2007 Workshop on Statistical and Hybrid methods for Machine Translation (WMT), pages 136–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>I Chao</author>
<author>L Padró</author>
<author>M Padró</author>
</authors>
<title>Freeling: An open-source suite of language analyzers.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th Int. Conf. on Language Resources and Evaluation (LREC’04).</booktitle>
<contexts>
<context position="10908" citStr="Carreras et al., 2004" startWordPosition="1701" endWordPosition="1704"> 1.3 M 1.3 M Words 38.2 M 35.8 K Vocabulary 156 K 120 K Development Europarl Sentences 2000 2000 Words 61.8 K 58.7 K Vocabulary 8 K 6.5 K Development News Commentary Sentences 1057 1057 Words 29.8 K 25.8 K Vocabulary 5.4 K 4.9 K Table 2: Basic statistics of ACL WMT 2008 corpus. 4.2 Processing details The training data was preprocessed by using provided tools for tokenizing and filtering. POS tagging. POS information for the source and the target languages was considered for both translation tasks that we have participated. The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags. Word Alignment. The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation. Instead of aligning words themselves, stems are used for aligning. Afterwards case sensitive words are recovered. Spanish Morphology Reduction. We implemented a morphology reduction of the Spanish language as a preprocessing step. As a</context>
</contexts>
<marker>Carreras, Chao, Padró, Padró, 2004</marker>
<rawString>X. Carreras, I. Chao, L. Padró, and M. Padró. 2004. Freeling: An open-source suite of language analyzers. In Proceedings of the 4th Int. Conf. on Language Resources and Evaluation (LREC’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mariño</author>
</authors>
<title>Improving statistical MT by coupling reordering and decoding.</title>
<date>2006</date>
<journal>Machine Translation,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="6236" citStr="Crego and Mariño (2006)" startWordPosition="949" endWordPosition="952">l weights by means of a double-loop optimization, which provides significant reduction of the number of translations that should be carried out. 3 Reordering framework For a great number of translation tasks a certain reordering strategy is required. This is especially important when the translation is performed between pairs of languages with non-monotonic word order. There are various types of distortion models, simplifying bilingual translation. In our system we use an extended monotone reordering model based on automatically learned reordering rules. A detailed description can be found in Crego and Mariño (2006). 1http://www.speech.sri.com/projects/srilm/ 2http://gps-tsc.upc.es/veu/soft/soft/marie/ Apart from that, tuples were extracted by an unfolding technique: this means that the tuples are broken into smaller tuples, and these are sequenced in the order of the target words. 3.1 Reordering patterns Word movements are realized according to the reordering rewrite rules, which have the form of: t1, ..., tn H i1, ..., in where t1, ..., tn is a sequence of POS tags (relating a sequence of source words), and i1, ..., in indicates which order of the source words generate monotonically the target words. P</context>
</contexts>
<marker>Crego, Mariño, 2006</marker>
<rawString>J. M. Crego and J. B. Mariño. 2006. Improving statistical MT by coupling reordering and decoding. Machine Translation, 20(3):199–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A de Gispert</author>
</authors>
<title>Introducing linguistic knowledge into statistical machine translation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Politècnica de Catalunya,</institution>
<marker>de Gispert, 2006</marker>
<rawString>A. de Gispert. 2006. Introducing linguistic knowledge into statistical machine translation. Ph.D. thesis, Universitat Politècnica de Catalunya, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL 2006 Workshop on Statistical and Hybrid methods forMachine Translation (WMT),</booktitle>
<pages>102--121</pages>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In Proceedings of the ACL 2006 Workshop on Statistical and Hybrid methods forMachine Translation (WMT), pages 102– 121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mariño</author>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R Costa-jussà</author>
</authors>
<title>Ngram based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mariño, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-jussà, 2006</marker>
<rawString>J. B. Mariño, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, J. A. R. Fonollosa, and M. R. Costa-jussà. 2006. Ngram based machine translation. Computational Linguistics, 32(4):527–549, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the the 38th Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>440--447</pages>
<contexts>
<context position="11200" citStr="Och and Ney, 2000" startWordPosition="1750" endWordPosition="1753">tails The training data was preprocessed by using provided tools for tokenizing and filtering. POS tagging. POS information for the source and the target languages was considered for both translation tasks that we have participated. The software tools available for performing POS-tagging were Freeling (Carreras et al., 2004) for Spanish and TnT (Brants, 2000) for English. The number of classes for English is 44, while Spanish is considered as a more inflectional language, and the tag set contains 376 different tags. Word Alignment. The word alignment is automatically computed by using GIZA++4(Och and Ney, 2000) in both directions, which are symmetrized by using the union operation. Instead of aligning words themselves, stems are used for aligning. Afterwards case sensitive words are recovered. Spanish Morphology Reduction. We implemented a morphology reduction of the Spanish language as a preprocessing step. As a consequence, training data sparseness due to Spanish morphology was reduced improving the performance of the overall translation system. In particular, the pronouns attached to the verb were separated and contractions as del or al were splitted into de el or a el. As a post-processing, in t</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings of the the 38th Annual Meeting on Association for Computational Linguistics (ACL), pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<tech>30(4):417 – 449,</tech>
<contexts>
<context position="4924" citStr="Och and Ney, 2004" startWordPosition="749" endWordPosition="752">which consists of a 4-gram LM of tuples with Kneser-Ney discounting (estimated with SRI Language Modeling Toolkit1), implements a log-linear combination of five additional feature models: • a target language model (a 4-gram model of words, estimated with Kneser-Ney smoothing); • a POS target language model (a 4-gram model of tags with Good-Turing discounting (TPOS)); • a word bonus model, which is used to compensate the system’s preference for short output sentences; • a source-to-target lexicon model and a target-tosource lexicon model, these models use word-toword IBM Model 1 probabilities (Och and Ney, 2004) to estimate the lexical weights for each tuple in the translation table. Decisions on the particular LM configuration and smoothing technique were taken on the minimalperplexity and maximal-BLEU bases. The decoder (called MARIE), an open source tool2, implementing a beam search strategy with distortion capabilities was used in the translation system. Given the development set and references, the loglinear combination of weights was adjusted using a simplex optimization method (with the optimization criteria of the highest BLEU score ) and an n-best re-ranking just as described in http://www.s</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. 30(4):417 – 449, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>