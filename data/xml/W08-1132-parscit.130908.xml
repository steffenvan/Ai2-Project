<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006672">
<title confidence="0.996291">
The Fingerprint of Human Referring Expressions and their Surface
Realization with Graph Transducers
</title>
<author confidence="0.996129">
Bernd Bohnet
</author>
<affiliation confidence="0.849857">
University of Stuttgart
Visualization and Interactive Systems Group
</affiliation>
<address confidence="0.862381">
Universit¨atstr. 58, 70569 Stuttgart, Germany
</address>
<email confidence="0.995546">
bohnet@informatik.uni-stuttgart.de
</email>
<sectionHeader confidence="0.99555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99908225">
The algorithm IS-FP takes up the idea from
the IS-FBN algorithm developed for the
shared task 2007. Both algorithms learn the
individual attribute selection style for each hu-
man that provided referring expressions to the
corpus. The IS-FP algorithm was developed
with two additional goals (1) to improve the
indentification time that was poor for the FBN
algorithm and (2) to push the dice score even
higher. In order to generate a word string for
the selected attributes, we build based on indi-
vidual preferences a surface syntactic depen-
dency tree as input. We derive the individual
preferences from the training set. Finally, a
graph transducer maps the input strucutre to a
deep morphologic structure.
</bodyText>
<sectionHeader confidence="0.906446" genericHeader="method">
1 IS-FP: Generating Referring Expression
with a Human Imprint
</sectionHeader>
<bodyText confidence="0.999658">
A review of the referring expressions shows that
humans prefer frequently distinct attributes and at-
tribute combination such as in the following exam-
ples.
grey desk (30t1), a red chair (30t2), red sofa (30t3), blue
chair (30t5), a small green desk (30t6)
the one in the top left corner (31t1), the one to the left in
the middle row (31t2), the bottom right most one (31t3),
the blue chair at the bottom center (31t5), etc.
The first individual (#30) seems to prefer colour
and size while the second one (#31) seems to prefer
the relative position (to the left) and places (the top
left corner). Because of the review, we checked, if
the Incremental Algorithm (Dale and Reiter, 1995)
using the order for the attributes due to the frequency
calculated for each individual can outperform the al-
gorithm using the order for the attributes due to the
frequency of the complete training set. This was the
case. Table 1 shows the results. Using the individual
attribute order, the IA performed as good as the FBN
algorithm, cf. Table 1.
</bodyText>
<table confidence="0.99789775">
Algorithm Furniture People Avg.
IA (complete) 0.796 0.710 0.753
IA (individual) 0.835 0.736 0.7855
FBN 0.810 0.762 0.786
</table>
<tableCaption confidence="0.999943">
Table 1: Incremental algorithm and FBN
</tableCaption>
<bodyText confidence="0.999977166666667">
However, the FBN algorithm generates all pos-
sible referring expressions and selects based on the
dice metric the most similar expressions of the same
human. Since there is usually a set of equal good re-
ferring expressions, it is possible to select a referring
expression among these results due to another met-
ric. That this would improve the results shows the
experiment to selected among these results the refer-
ring expression that is closest to the correct result.
The outcome was that the FBN algorithm has still
about 9% room for improvements. The following
sections investigates possibilities to use this chance.
</bodyText>
<subsectionHeader confidence="0.999052">
1.1 Identification Time
</subsectionHeader>
<bodyText confidence="0.999962">
An important metric is the identification time that is
the time which is used by a human to identify an en-
tity due to a given referring expression. The identi-
fication time is very loosely related with the number
of minimal referring expressions and therefore likely
with the length of a referring expression. The best
identification times had a system with 74% minimal
referring expressions and the second and third best
</bodyText>
<page confidence="0.995731">
207
</page>
<bodyText confidence="0.999959227272727">
systems had about 41%. Good identification times
had nearly all systems with only a maximum differ-
ence of 0.38 seconds except FBN and FBS which
are about 1.05 and 1.49 seconds behind the best one.
This is a huge difference compared to all other sys-
tems. What could be the reason for that? We know
of two differences of FBN to all other systems: (1)
the lowest portion of minimal referring expressions
of all systems and (2) the nearest neighbour learn-
ing technique. The number of minimal referring ex-
pressions is also different to the number of expres-
sions found in the training set. Table 2 shows in the
columns human the average length and portion of
minimal human referring expressions. Because of
the different length of the human and the generated
expressions, we conducted the experiment to chose
always the shortest. Table 2 shows the change be-
tween the random selection (FBN) and the selection
of the shortest (FP). The experiment leads to a result
that have a length and percentage of minimal refer-
ring expressions in average similar to the humans
ones, cf. columns of human and shortest.
</bodyText>
<table confidence="0.937098">
selection human random (FBM) shortest (FP)
RE Len. Min. Len. Min. Len. Min.
Furniture 3.1 26.3 3.5 9.4 3.1 15.9
People 3.0 30.9 2.8 28.8 2.8 30.8
</table>
<tableCaption confidence="0.993389">
Table 2: Length and portion of min. RE
</tableCaption>
<bodyText confidence="0.999963538461538">
The second difference is the use of the nearest
neighbour technique. Could the poor identification
time be caused by the nearest neighbour technique?
How does it influence referring expressions? – The
referring expressions are generated in all the dif-
ferent styles like the human expressions of the cor-
pus. Do humans learn the style of referring expres-
sions and expect then the next expression in the same
style? And are we confused when we don’t get what
we expact? Or does FBN look too much on the ex-
pressions of the humans and too less on the domain?
We hope to get answers for these questions from the
shared task evaluation of IS-FP.
</bodyText>
<subsectionHeader confidence="0.99119">
1.2 The IS-FP Algorithm
</subsectionHeader>
<bodyText confidence="0.9999696">
The basis for the IS-FP algorithm is an extended full
brevity implementation in terms of problem solving
by search which computes all referring expression,
cf. (Bohnet and Dale, 2005). IS-FP uses also the
nearest neighbour technique like the IS-FBN algo-
rithm that was introduced by Bohnet (2007). With
the nearest neighbour technique, IS-FP selects the
expressions which are most similar to the referring
expressions of the same human and a human that
builds referring expressions similar. The similarity
is computed as the average of all dice values be-
tween all combinations of the available trails for two
humans. From the result of the nearest neighbour
evaluation, FP selects the shortest and if still more
than one expressions remain then it computes the
similarity among them and chooses the most typi-
cal and finally, if still alternatives remain, it selects
one with the attributes having the highest frequency.
Table 3 shows the results for IS-FP trained on the
training set and applied to the development set.
</bodyText>
<table confidence="0.6601025">
Set Dice MASI Accuracy Uniq. Min.
Furniture 0.881 0.691 51.25% 100% 1.25%
People 0.790 0.558 36.8% 100% 0%
Total 0.836 0.625 44% 100% 0.62%
</table>
<tableCaption confidence="0.992458">
Table 3: Results for the IS-FP algorithm
</tableCaption>
<sectionHeader confidence="0.813799" genericHeader="method">
2 IS-GT: Realization with Graph
Transducers
</sectionHeader>
<bodyText confidence="0.9996436">
We build the input depedency tree for the text gener-
ator due to the statistical information that we collect
from the training data for each person. This pro-
cedure is consistent with our referring expression
generator IS-FP that reproduces the individual im-
print in a referring expression for the target person.
We start with the realization of the referring expres-
sions from a surface syntactic dependency tree, cf.
(Mel’ˇcuk, 1988). For the realization of the text, we
use the Text Generator and Linguistic Environment
MATE, cf. (Bohnet, 2006). We reportet the first
time about MATE on the first International Natu-
ral Language Generation Conference, cf. (Bohnet et
al., 2000). It was since then continuously enhanced
and in the last years, large grammars for several lan-
guages such as Catalan, English, Finnish, French,
German, Polish, Portougees have been developed
within the European Project MARQUIS and PatEx-
pert, cf. (Wanner et al., 2007), (Lareau and Wanner,
2007) and (Mille and Wanner, 2008).
</bodyText>
<subsectionHeader confidence="0.983889">
2.1 The Referring Expression Models
</subsectionHeader>
<bodyText confidence="0.99953325">
A learning program builds a Referring Expression
Model for each person that contributed referring ex-
pression to the corpus. The model contains the fol-
lowing information: (1) The lexicalization for the
</bodyText>
<page confidence="0.992087">
208
</page>
<bodyText confidence="0.999959181818182">
values of a attribute such as couch for the value sofa,
man for value person, etc. (2) The prefered usage of
determiners for the type that can be definite (the), in-
definite (a), no article. (3) The syntactic preferences
such as the top left chair, the chair at the bottom to
the left, etc.
The information about the determiner and the
lexicalization is collected from the annotated word
string and the word string itself. We collect the most
frequent usage for each person in the coprpus. In
order to collect the prefered syntax, we annotated
the word strings with syntactic dependency trees.
Each of the dependency tress contains additional at-
tributes, which describe the information content of a
branch outgoing from the root as well as the possi-
ble value of the attriube at the nodes which carry the
information. The learning program cuts the syntac-
tic tree at edges starting at the root node and stores
the branches in the referring expression model for
the person. For instance, the complete referring ex-
pression model of a person would contain due to the
training data the following information:
</bodyText>
<figure confidence="0.593115333333333">
article: definite
lexicalization: person → man, light → white
syntax:
t21a: wearing glasses {t:hasGlasses a1:1 v1:glasses}
t21b: with compl → beard {t:hasBeard a1:1 v1:beard} det → a
beard compl → white {t:hairColour a1:light v1:white
a2:dark v2:dark}
t22: with compl → beard {t:hasBeard a1:1 v1:beard}
beard det → a
</figure>
<figureCaption confidence="0.588359">
t23: wearing obj →glasses {t:hasGlasses a1:1 v1:glasses}
t26: with compl → glasses {t:hasGlasses a1:1 v1:glasses}
glasses coord → and compl → heair mod →
</figureCaption>
<bodyText confidence="0.728962">
dark{t:hairColour a1:dark v1:dark}
</bodyText>
<subsectionHeader confidence="0.9978">
2.2 Setting up the Input for the Generator
</subsectionHeader>
<bodyText confidence="0.9998945">
One of the input attribute sets of the people domain
looks like the following one:
</bodyText>
<figure confidence="0.659255444444445">
&lt;TRIAL CONDITION=”-LOC” ID=”s81t25”&gt;
...
&lt;ATTRIBUTE-SET&gt;
&lt;ATTRIBUTE ID=”a4” NAME=”hasBeard” VALUE=”1”/ &gt;
&lt;ATTRIBUTE ID=”a3” NAME=”hairColour” VALUE=”light”/ &gt;
&lt;ATTRIBUTE ID=”a2” NAME=”hasGlasses” VALUE=”1”/ &gt;
&lt;ATTRIBUTE ID=”a1” NAME=”type” VALUE=”person”/ &gt;
&lt; /ATTRIBUTE-SET&gt;
&lt; /TRIAL&gt;
</figure>
<bodyText confidence="0.999194">
We start to set up the input structure with the top
node which is labeled with the lexicalization of the
type or in seldom cases with elision, when the type
is not in the attribute set. Then we look up in the re-
ferring expression model which determiner the per-
</bodyText>
<figureCaption confidence="0.999999">
Figure 1: The input to the graph transducer
Figure 2: The output of the graph transducer
</figureCaption>
<bodyText confidence="0.999566619047619">
son prefers. If she prefers any then a node is build,
labeled with the determiner and connected with an
edge to the type node. After that we add the lexi-
calized values of that attributes which are nearly al-
ways directly attached to the type node such as age
in the people domain or colour and size in the fur-
niture domain. Then the program searches in the
model the syntactic annotations of attribute combi-
nations. If IS-FP has build the referring expression
then it starts to search in the trail selected by the
nearest neighbour algorithm otherwise it calculates
the closest due to the dice metric. In our example
IS-FP might have build as well the given combi-
nation since it is equal to the attribute set of trail
s81t21. Then the program would select the syntactic
part t21b first and adapt the value of the node label
white to dark. After that the the syntactic part t21a
would be selected since the attribute hasGlasses is
still not covered in the structure. This part does not
need any adaption. Figure 1 shows the result of the
process.
</bodyText>
<subsectionHeader confidence="0.994589">
2.3 Realization of the Word String
</subsectionHeader>
<bodyText confidence="0.982122">
For the realization, we use a handcrafted grammar
that generates out of the dependency trees roughly
</bodyText>
<figure confidence="0.99690075">
man
mod post_mod post_mod
the
wearing
with
dobj compl
glasses
beard
det mod
dark
a
NP
the man
b
b
wearing
VP
b
b
glasses
NP
b
with
a
b
b
PP
NP
dark
b
b
beard
</figure>
<page confidence="0.996928">
209
</page>
<bodyText confidence="0.999973074074074">
deep morphologic structure / topologic graph. The
main task of the grammar is to determine the word
order. The grammar contains four main rule groups.
The vertical rules order the parent in relation to
one of its dependent. The horizontal rules oder
two childs. The constituent creation rules build
constituents and the constituent adjoin rules adjoins
constituents with constituents. Special consideration
needed the order of prepositional constituents after
the type and the adjective before the type. The pre-
postional constituents are order because of the order
of the prepostions in the corpus. In order to be able
to derive the order of the adjective, we used the func-
tional class of the adjectives. Halliday (1994) pro-
poses for English, the classes deictic (this, those, ...),
numerative (many, second, , ...), epithet (old, blue,
...), and classifier (vegetarian, Spanish, ...). The or-
der of the adjectives in a noun phrase is in the given
order of the classes. In the lexicon entry of the ad-
jectives, we store only a number between one and
four which refers to the adjective class.
Table 5 shows the result for the TUNA-R task.
The system was developed only by looking on the
training data without any consideration of the devel-
opment data as well without any annotation of the
syntax of the development data. We used as guide
for the optimization cross validation of training data.
</bodyText>
<table confidence="0.994503">
Set Accuracy String-Edit Distance
Furniture 35 % 3,163
People 22,06 % 3,647
Total 28,53 3,405
</table>
<tableCaption confidence="0.995017">
Table 4: Results for the TUNA-R Task
</tableCaption>
<sectionHeader confidence="0.7571515" genericHeader="method">
3 IS-FP-GT: The Combination of
Attribute Selection and Realization
</sectionHeader>
<bodyText confidence="0.999983777777778">
The only change, we made in compare to IS-FP is
that we switched off the feature to add the most
similar referring expressions of another human from
the training set for the nearest neighbour evaluation
since the results have been lower. The reason for this
is that other human preferes similar attributes but the
individual preferences such as the chosen words and
syntax of the other human is different. Table 5 shows
the results.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9779315">
The IS-FP algorithm reproduces the imprint of hu-
man referring expressions. It generates combina-
</bodyText>
<table confidence="0.99692175">
Set Accuracy String-Edit Distance
Furniture 15 % 3,8625
People 8,82 % 4,764
Total 11,91 4,313
</table>
<tableCaption confidence="0.99769">
Table 5: Results for the TUNA-REG Task
</tableCaption>
<bodyText confidence="0.999982583333333">
tions such as the x-dimension and y-dimension. El-
ements of a combination have not to occur always
together, however they tent to occur together. This
is an advantage over incremental algorithms which
might have to include other attributes ordered be-
tween elements of a combination. FP has the advan-
tage over its predecessor FBN to generate expres-
sions which are additionally mostly equal in respect
to the length to human referring expressions, it en-
larges automatically the training set for an individ-
ual human and it takes into account properties of the
domain like the frequency of the attributes.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99981496969697">
B. Bohnet and R. Dale. 2005. Viewing referring expres-
sion generation as search. In IJCAI, pages 1004–1009.
B. Bohnet, A. Langjahr, and L. Wanner. 2000. A De-
velopment Environment for an MTT-Based Sentence
Generator. In Proceedings of the First International
Natural Language Generation Conference.
B. Bohnet. 2006. Textgenerierung durch Transduk-
tion linguistischer Strukturen. Ph.D. thesis, University
Stuttgart.
B. Bohnet. 2007. IS-FBN, IS-FBS, IS-IAC: The Adapta-
tion of Two Classic Algorithms for the Generation of
Referring Expressions in order to Produce Expressions
like Humans Do. In MT Summit XI, UCNLG+MT,
pages 84–86.
R. Dale and E. Reiter. 1995. Computational Interpreta-
tions of the Gricean Maxims in the Generation of Re-
ferring Expressions. Cognitive Science, 19:233–263.
M. A. K. Halliday. 1994. An Introduction to Functional
Grammar. Edward Arnold, London.
F. Lareau and L. Wanner. 2007. Towards a Generic Mul-
tilingual Dependency Grammar for Text Generation.
In GEAF-07, Palo Alto.
I.A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press, Albany.
S. Mille and L. Wanner. 2008. Making Text Resources
Available to the Reader: The Case of Patent Claims.
In LREC, Morocco, Marrakech.
L. Wanner, B. Bohnet, N. Bouayad-Agha, F. Lareau,
A. Lohmeyer, and D. Nickla. 2007. On the Challenge
of Creating and Communicating Air Quality Informa-
tion. In In: Swayne A., Hrebicek J.(Eds.): Environ-
mental Software Systems Dimensions of Environmen-
talInformatics. Vol.7.
</reference>
<page confidence="0.99896">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.007197">
<title confidence="0.9986885">The Fingerprint of Human Referring Expressions and their Realization with Graph Transducers</title>
<author confidence="0.94811">Bernd</author>
<affiliation confidence="0.981634">University of</affiliation>
<title confidence="0.809703">Visualization and Interactive Systems</title>
<address confidence="0.679092">Universit¨atstr. 58, 70569 Stuttgart,</address>
<email confidence="0.998491">bohnet@informatik.uni-stuttgart.de</email>
<abstract confidence="0.976203207317073">The algorithm IS-FP takes up the idea from the IS-FBN algorithm developed for the shared task 2007. Both algorithms learn the individual attribute selection style for each human that provided referring expressions to the corpus. The IS-FP algorithm was developed with two additional goals (1) to improve the indentification time that was poor for the FBN algorithm and (2) to push the dice score even higher. In order to generate a word string for the selected attributes, we build based on individual preferences a surface syntactic dependency tree as input. We derive the individual preferences from the training set. Finally, a graph transducer maps the input strucutre to a deep morphologic structure. 1 IS-FP: Generating Referring Expression with a Human Imprint A review of the referring expressions shows that humans prefer frequently distinct attributes and attribute combination such as in the following examples. grey desk (30t1), a red chair (30t2), red sofa (30t3), blue chair (30t5), a small green desk (30t6) the one in the top left corner (31t1), the one to the left in the middle row (31t2), the bottom right most one (31t3), the blue chair at the bottom center (31t5), etc. The first individual (#30) seems to prefer colour and size while the second one (#31) seems to prefer relative position the and places top Because of the review, we checked, if the Incremental Algorithm (Dale and Reiter, 1995) using the order for the attributes due to the frequency calculated for each individual can outperform the algorithm using the order for the attributes due to the frequency of the complete training set. This was the case. Table 1 shows the results. Using the individual attribute order, the IA performed as good as the FBN algorithm, cf. Table 1. Algorithm Furniture People Avg. IA (complete) 0.796 0.710 0.753 IA (individual) 0.835 0.736 0.7855 FBN 0.810 0.762 0.786 Table 1: Incremental algorithm and FBN However, the FBN algorithm generates all possible referring expressions and selects based on the dice metric the most similar expressions of the same human. Since there is usually a set of equal good referring expressions, it is possible to select a referring expression among these results due to another metric. That this would improve the results shows the experiment to selected among these results the referring expression that is closest to the correct result. The outcome was that the FBN algorithm has still about 9% room for improvements. The following sections investigates possibilities to use this chance. 1.1 Identification Time An important metric is the identification time that is the time which is used by a human to identify an entity due to a given referring expression. The identification time is very loosely related with the number of minimal referring expressions and therefore likely with the length of a referring expression. The best identification times had a system with 74% minimal referring expressions and the second and third best 207 systems had about 41%. Good identification times had nearly all systems with only a maximum difference of 0.38 seconds except FBN and FBS which are about 1.05 and 1.49 seconds behind the best one. This is a huge difference compared to all other systems. What could be the reason for that? We know two differences of FBN to all other systems: the lowest portion of minimal referring expressions all systems and nearest neighbour learning technique. The number of minimal referring expressions is also different to the number of expressions found in the training set. Table 2 shows in the average length and portion of minimal human referring expressions. Because of the different length of the human and the generated expressions, we conducted the experiment to chose always the shortest. Table 2 shows the change between the random selection (FBN) and the selection of the shortest (FP). The experiment leads to a result that have a length and percentage of minimal referring expressions in average similar to the humans cf. columns of selection human random (FBM) shortest (FP) RE Len. Min. Len. Min. Len. Min. Furniture 3.1 26.3 3.5 9.4 3.1 15.9 People 3.0 30.9 2.8 28.8 2.8 30.8 Table 2: Length and portion of min. RE The second difference is the use of the nearest neighbour technique. Could the poor identification time be caused by the nearest neighbour technique? How does it influence referring expressions? – The referring expressions are generated in all the different styles like the human expressions of the corpus. Do humans learn the style of referring expressions and expect then the next expression in the same style? And are we confused when we don’t get what we expact? Or does FBN look too much on the expressions of the humans and too less on the domain? We hope to get answers for these questions from the shared task evaluation of IS-FP. 1.2 The IS-FP Algorithm The basis for the IS-FP algorithm is an extended full brevity implementation in terms of problem solving by search which computes all referring expression, cf. (Bohnet and Dale, 2005). IS-FP uses also the nearest neighbour technique like the IS-FBN algorithm that was introduced by Bohnet (2007). With the nearest neighbour technique, IS-FP selects the expressions which are most similar to the referring expressions of the same human and a human that builds referring expressions similar. The similarity is computed as the average of all dice values between all combinations of the available trails for two humans. From the result of the nearest neighbour evaluation, FP selects the shortest and if still more than one expressions remain then it computes the similarity among them and chooses the most typical and finally, if still alternatives remain, it selects one with the attributes having the highest frequency. Table 3 shows the results for IS-FP trained on the training set and applied to the development set. Set Dice MASI Accuracy Uniq. Min. Furniture 0.881 0.691 51.25% 100% 1.25% People 0.790 0.558 36.8% 100% 0% Total 0.836 0.625 44% 100% 0.62% Table 3: Results for the IS-FP algorithm 2 IS-GT: Realization with Graph Transducers We build the input depedency tree for the text generator due to the statistical information that we collect from the training data for each person. This procedure is consistent with our referring expression generator IS-FP that reproduces the individual imprint in a referring expression for the target person. We start with the realization of the referring expressions from a surface syntactic dependency tree, cf. (Mel’ˇcuk, 1988). For the realization of the text, we use the Text Generator and Linguistic Environment MATE, cf. (Bohnet, 2006). We reportet the first time about MATE on the first International Natural Language Generation Conference, cf. (Bohnet et al., 2000). It was since then continuously enhanced and in the last years, large grammars for several languages such as Catalan, English, Finnish, French, German, Polish, Portougees have been developed within the European Project MARQUIS and PatExpert, cf. (Wanner et al., 2007), (Lareau and Wanner, 2007) and (Mille and Wanner, 2008). 2.1 The Referring Expression Models A learning program builds a Referring Expression Model for each person that contributed referring expression to the corpus. The model contains the following information: (1) The lexicalization for the 208 values of a attribute such as couch for the value sofa, man for value person, etc. (2) The prefered usage of for the type that can be definite inno article. (3) The syntactic preferences as top left chair at the bottom to etc. The information about the determiner and the lexicalization is collected from the annotated word string and the word string itself. We collect the most frequent usage for each person in the coprpus. In order to collect the prefered syntax, we annotated the word strings with syntactic dependency trees. Each of the dependency tress contains additional attributes, which describe the information content of a branch outgoing from the root as well as the possible value of the attriube at the nodes which carry the information. The learning program cuts the syntactic tree at edges starting at the root node and stores the branches in the referring expression model for the person. For instance, the complete referring expression model of a person would contain due to the training data the following information: article: definite person light syntax: wearing glasses a1:1 with a1:1 a1:light v1:white with a1:1 wearing a1:1 with a1:1 a1:dark 2.2 Setting up the Input for the Generator One of the input attribute sets of the people domain looks like the following one: ... ID=”a4” NAME=”hasBeard” &gt; ID=”a3” NAME=”hairColour” &gt; ID=”a2” NAME=”hasGlasses” &gt; ID=”a1” NAME=”type” &gt; We start to set up the input structure with the top node which is labeled with the lexicalization of the or in seldom cases with when the type is not in the attribute set. Then we look up in the reexpression model which determiner the per- Figure 1: The input to the graph transducer Figure 2: The output of the graph transducer son prefers. If she prefers any then a node is build, labeled with the determiner and connected with an edge to the type node. After that we add the lexicalized values of that attributes which are nearly always directly attached to the type node such as age in the people domain or colour and size in the furniture domain. Then the program searches in the model the syntactic annotations of attribute combinations. If IS-FP has build the referring expression then it starts to search in the trail selected by the nearest neighbour algorithm otherwise it calculates the closest due to the dice metric. In our example IS-FP might have build as well the given combination since it is equal to the attribute set of trail Then the program would select the syntactic part t21b first and adapt the value of the node label After that the the syntactic part t21a be selected since the attribute still not covered in the structure. This part does not need any adaption. Figure 1 shows the result of the process. 2.3 Realization of the Word String For the realization, we use a handcrafted grammar that generates out of the dependency trees roughly man mod post_mod post_mod the wearing with dobj compl glasses beard det mod dark a NP the man b b wearing VP b b glasses NP b with a b b PP NP dark b b beard 209 deep morphologic structure / topologic graph. The main task of the grammar is to determine the word order. The grammar contains four main rule groups. The vertical rules order the parent in relation to one of its dependent. The horizontal rules oder two childs. The constituent creation rules build constituents and the constituent adjoin rules adjoins constituents with constituents. Special consideration needed the order of prepositional constituents after the type and the adjective before the type. The prepostional constituents are order because of the order of the prepostions in the corpus. In order to be able to derive the order of the adjective, we used the functional class of the adjectives. Halliday (1994) proposes for English, the classes deictic (this, those, ...), numerative (many, second, , ...), epithet (old, blue, ...), and classifier (vegetarian, Spanish, ...). The order of the adjectives in a noun phrase is in the given order of the classes. In the lexicon entry of the adjectives, we store only a number between one and four which refers to the adjective class. Table 5 shows the result for the TUNA-R task. The system was developed only by looking on the training data without any consideration of the development data as well without any annotation of the syntax of the development data. We used as guide for the optimization cross validation of training data. Set Accuracy String-Edit Distance Furniture 35 % 3,163 People 22,06 % 3,647 Total 28,53 3,405 Table 4: Results for the TUNA-R Task 3 IS-FP-GT: The Combination of Attribute Selection and Realization The only change, we made in compare to IS-FP is that we switched off the feature to add the most similar referring expressions of another human from the training set for the nearest neighbour evaluation since the results have been lower. The reason for this is that other human preferes similar attributes but the individual preferences such as the chosen words and syntax of the other human is different. Table 5 shows the results. 4 Conclusion The IS-FP algorithm reproduces the imprint of hureferring expressions. It generates combina- Set Accuracy String-Edit Distance Furniture 15 % 3,8625 People 8,82 % 4,764 Total 11,91 4,313 Table 5: Results for the TUNA-REG Task tions such as the x-dimension and y-dimension. Elements of a combination have not to occur always together, however they tent to occur together. This is an advantage over incremental algorithms which might have to include other attributes ordered between elements of a combination. FP has the advantage over its predecessor FBN to generate expressions which are additionally mostly equal in respect to the length to human referring expressions, it enlarges automatically the training set for an individual human and it takes into account properties of the domain like the frequency of the attributes.</abstract>
<note confidence="0.795827166666667">References Bohnet and R. Dale. 2005. Viewing referring expresgeneration as search. In pages 1004–1009. Bohnet, A. Langjahr, and L. Wanner. 2000. A Development Environment for an MTT-Based Sentence In of the First International</note>
<title confidence="0.970609">Language Generation</title>
<author confidence="0.966602">durch Transduk-</author>
<affiliation confidence="0.988871">linguistischer Strukturen. thesis, University</affiliation>
<address confidence="0.8244555">Stuttgart. B. Bohnet. 2007. IS-FBN, IS-FBS, IS-IAC: The Adapta-</address>
<title confidence="0.860499">tion of Two Classic Algorithms for the Generation of Referring Expressions in order to Produce Expressions</title>
<author confidence="0.975182">In Summit</author>
<note confidence="0.9461605">pages 84–86. R. Dale and E. Reiter. 1995. Computational Interpretations of the Gricean Maxims in the Generation of Re- Expressions. 19:233–263.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>R Dale</author>
</authors>
<title>Viewing referring expression generation as search.</title>
<date>2005</date>
<booktitle>In IJCAI,</booktitle>
<pages>1004--1009</pages>
<contexts>
<context position="5424" citStr="Bohnet and Dale, 2005" startWordPosition="900" endWordPosition="903">re generated in all the different styles like the human expressions of the corpus. Do humans learn the style of referring expressions and expect then the next expression in the same style? And are we confused when we don’t get what we expact? Or does FBN look too much on the expressions of the humans and too less on the domain? We hope to get answers for these questions from the shared task evaluation of IS-FP. 1.2 The IS-FP Algorithm The basis for the IS-FP algorithm is an extended full brevity implementation in terms of problem solving by search which computes all referring expression, cf. (Bohnet and Dale, 2005). IS-FP uses also the nearest neighbour technique like the IS-FBN algorithm that was introduced by Bohnet (2007). With the nearest neighbour technique, IS-FP selects the expressions which are most similar to the referring expressions of the same human and a human that builds referring expressions similar. The similarity is computed as the average of all dice values between all combinations of the available trails for two humans. From the result of the nearest neighbour evaluation, FP selects the shortest and if still more than one expressions remain then it computes the similarity among them a</context>
</contexts>
<marker>Bohnet, Dale, 2005</marker>
<rawString>B. Bohnet and R. Dale. 2005. Viewing referring expression generation as search. In IJCAI, pages 1004–1009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
<author>A Langjahr</author>
<author>L Wanner</author>
</authors>
<title>A Development Environment for an MTT-Based Sentence Generator.</title>
<date>2000</date>
<booktitle>In Proceedings of the First International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="7161" citStr="Bohnet et al., 2000" startWordPosition="1182" endWordPosition="1185"> due to the statistical information that we collect from the training data for each person. This procedure is consistent with our referring expression generator IS-FP that reproduces the individual imprint in a referring expression for the target person. We start with the realization of the referring expressions from a surface syntactic dependency tree, cf. (Mel’ˇcuk, 1988). For the realization of the text, we use the Text Generator and Linguistic Environment MATE, cf. (Bohnet, 2006). We reportet the first time about MATE on the first International Natural Language Generation Conference, cf. (Bohnet et al., 2000). It was since then continuously enhanced and in the last years, large grammars for several languages such as Catalan, English, Finnish, French, German, Polish, Portougees have been developed within the European Project MARQUIS and PatExpert, cf. (Wanner et al., 2007), (Lareau and Wanner, 2007) and (Mille and Wanner, 2008). 2.1 The Referring Expression Models A learning program builds a Referring Expression Model for each person that contributed referring expression to the corpus. The model contains the following information: (1) The lexicalization for the 208 values of a attribute such as cou</context>
</contexts>
<marker>Bohnet, Langjahr, Wanner, 2000</marker>
<rawString>B. Bohnet, A. Langjahr, and L. Wanner. 2000. A Development Environment for an MTT-Based Sentence Generator. In Proceedings of the First International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Textgenerierung durch Transduktion linguistischer Strukturen.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University Stuttgart.</institution>
<contexts>
<context position="7029" citStr="Bohnet, 2006" startWordPosition="1163" endWordPosition="1164"> for the IS-FP algorithm 2 IS-GT: Realization with Graph Transducers We build the input depedency tree for the text generator due to the statistical information that we collect from the training data for each person. This procedure is consistent with our referring expression generator IS-FP that reproduces the individual imprint in a referring expression for the target person. We start with the realization of the referring expressions from a surface syntactic dependency tree, cf. (Mel’ˇcuk, 1988). For the realization of the text, we use the Text Generator and Linguistic Environment MATE, cf. (Bohnet, 2006). We reportet the first time about MATE on the first International Natural Language Generation Conference, cf. (Bohnet et al., 2000). It was since then continuously enhanced and in the last years, large grammars for several languages such as Catalan, English, Finnish, French, German, Polish, Portougees have been developed within the European Project MARQUIS and PatExpert, cf. (Wanner et al., 2007), (Lareau and Wanner, 2007) and (Mille and Wanner, 2008). 2.1 The Referring Expression Models A learning program builds a Referring Expression Model for each person that contributed referring expressi</context>
</contexts>
<marker>Bohnet, 2006</marker>
<rawString>B. Bohnet. 2006. Textgenerierung durch Transduktion linguistischer Strukturen. Ph.D. thesis, University Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>IS-FBN, IS-FBS, IS-IAC: The Adaptation of Two Classic Algorithms for the Generation of Referring Expressions in order to Produce Expressions like Humans Do.</title>
<date>2007</date>
<booktitle>In MT Summit XI, UCNLG+MT,</booktitle>
<pages>84--86</pages>
<contexts>
<context position="5536" citStr="Bohnet (2007)" startWordPosition="920" endWordPosition="921"> expressions and expect then the next expression in the same style? And are we confused when we don’t get what we expact? Or does FBN look too much on the expressions of the humans and too less on the domain? We hope to get answers for these questions from the shared task evaluation of IS-FP. 1.2 The IS-FP Algorithm The basis for the IS-FP algorithm is an extended full brevity implementation in terms of problem solving by search which computes all referring expression, cf. (Bohnet and Dale, 2005). IS-FP uses also the nearest neighbour technique like the IS-FBN algorithm that was introduced by Bohnet (2007). With the nearest neighbour technique, IS-FP selects the expressions which are most similar to the referring expressions of the same human and a human that builds referring expressions similar. The similarity is computed as the average of all dice values between all combinations of the available trails for two humans. From the result of the nearest neighbour evaluation, FP selects the shortest and if still more than one expressions remain then it computes the similarity among them and chooses the most typical and finally, if still alternatives remain, it selects one with the attributes having</context>
</contexts>
<marker>Bohnet, 2007</marker>
<rawString>B. Bohnet. 2007. IS-FBN, IS-FBS, IS-IAC: The Adaptation of Two Classic Algorithms for the Generation of Referring Expressions in order to Produce Expressions like Humans Do. In MT Summit XI, UCNLG+MT, pages 84–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<date>1995</date>
<booktitle>Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science,</booktitle>
<pages>19--233</pages>
<contexts>
<context position="1721" citStr="Dale and Reiter, 1995" startWordPosition="269" endWordPosition="272">er frequently distinct attributes and attribute combination such as in the following examples. grey desk (30t1), a red chair (30t2), red sofa (30t3), blue chair (30t5), a small green desk (30t6) the one in the top left corner (31t1), the one to the left in the middle row (31t2), the bottom right most one (31t3), the blue chair at the bottom center (31t5), etc. The first individual (#30) seems to prefer colour and size while the second one (#31) seems to prefer the relative position (to the left) and places (the top left corner). Because of the review, we checked, if the Incremental Algorithm (Dale and Reiter, 1995) using the order for the attributes due to the frequency calculated for each individual can outperform the algorithm using the order for the attributes due to the frequency of the complete training set. This was the case. Table 1 shows the results. Using the individual attribute order, the IA performed as good as the FBN algorithm, cf. Table 1. Algorithm Furniture People Avg. IA (complete) 0.796 0.710 0.753 IA (individual) 0.835 0.736 0.7855 FBN 0.810 0.762 0.786 Table 1: Incremental algorithm and FBN However, the FBN algorithm generates all possible referring expressions and selects based on </context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science, 19:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>An Introduction to Functional Grammar. Edward</title>
<date>1994</date>
<location>Arnold, London.</location>
<contexts>
<context position="12160" citStr="Halliday (1994)" startWordPosition="2016" endWordPosition="2017">. The grammar contains four main rule groups. The vertical rules order the parent in relation to one of its dependent. The horizontal rules oder two childs. The constituent creation rules build constituents and the constituent adjoin rules adjoins constituents with constituents. Special consideration needed the order of prepositional constituents after the type and the adjective before the type. The prepostional constituents are order because of the order of the prepostions in the corpus. In order to be able to derive the order of the adjective, we used the functional class of the adjectives. Halliday (1994) proposes for English, the classes deictic (this, those, ...), numerative (many, second, , ...), epithet (old, blue, ...), and classifier (vegetarian, Spanish, ...). The order of the adjectives in a noun phrase is in the given order of the classes. In the lexicon entry of the adjectives, we store only a number between one and four which refers to the adjective class. Table 5 shows the result for the TUNA-R task. The system was developed only by looking on the training data without any consideration of the development data as well without any annotation of the syntax of the development data. We</context>
</contexts>
<marker>Halliday, 1994</marker>
<rawString>M. A. K. Halliday. 1994. An Introduction to Functional Grammar. Edward Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Lareau</author>
<author>L Wanner</author>
</authors>
<title>Towards a Generic Multilingual Dependency Grammar for Text Generation. In GEAF-07,</title>
<date>2007</date>
<location>Palo Alto.</location>
<contexts>
<context position="7456" citStr="Lareau and Wanner, 2007" startWordPosition="1228" endWordPosition="1231"> referring expressions from a surface syntactic dependency tree, cf. (Mel’ˇcuk, 1988). For the realization of the text, we use the Text Generator and Linguistic Environment MATE, cf. (Bohnet, 2006). We reportet the first time about MATE on the first International Natural Language Generation Conference, cf. (Bohnet et al., 2000). It was since then continuously enhanced and in the last years, large grammars for several languages such as Catalan, English, Finnish, French, German, Polish, Portougees have been developed within the European Project MARQUIS and PatExpert, cf. (Wanner et al., 2007), (Lareau and Wanner, 2007) and (Mille and Wanner, 2008). 2.1 The Referring Expression Models A learning program builds a Referring Expression Model for each person that contributed referring expression to the corpus. The model contains the following information: (1) The lexicalization for the 208 values of a attribute such as couch for the value sofa, man for value person, etc. (2) The prefered usage of determiners for the type that can be definite (the), indefinite (a), no article. (3) The syntactic preferences such as the top left chair, the chair at the bottom to the left, etc. The information about the determiner a</context>
</contexts>
<marker>Lareau, Wanner, 2007</marker>
<rawString>F. Lareau and L. Wanner. 2007. Towards a Generic Multilingual Dependency Grammar for Text Generation. In GEAF-07, Palo Alto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>Albany.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>I.A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mille</author>
<author>L Wanner</author>
</authors>
<title>Making Text Resources Available to the Reader: The Case of Patent Claims.</title>
<date>2008</date>
<booktitle>In LREC,</booktitle>
<location>Morocco, Marrakech.</location>
<contexts>
<context position="7485" citStr="Mille and Wanner, 2008" startWordPosition="1233" endWordPosition="1236">surface syntactic dependency tree, cf. (Mel’ˇcuk, 1988). For the realization of the text, we use the Text Generator and Linguistic Environment MATE, cf. (Bohnet, 2006). We reportet the first time about MATE on the first International Natural Language Generation Conference, cf. (Bohnet et al., 2000). It was since then continuously enhanced and in the last years, large grammars for several languages such as Catalan, English, Finnish, French, German, Polish, Portougees have been developed within the European Project MARQUIS and PatExpert, cf. (Wanner et al., 2007), (Lareau and Wanner, 2007) and (Mille and Wanner, 2008). 2.1 The Referring Expression Models A learning program builds a Referring Expression Model for each person that contributed referring expression to the corpus. The model contains the following information: (1) The lexicalization for the 208 values of a attribute such as couch for the value sofa, man for value person, etc. (2) The prefered usage of determiners for the type that can be definite (the), indefinite (a), no article. (3) The syntactic preferences such as the top left chair, the chair at the bottom to the left, etc. The information about the determiner and the lexicalization is coll</context>
</contexts>
<marker>Mille, Wanner, 2008</marker>
<rawString>S. Mille and L. Wanner. 2008. Making Text Resources Available to the Reader: The Case of Patent Claims. In LREC, Morocco, Marrakech.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L Wanner</author>
<author>B Bohnet</author>
<author>N Bouayad-Agha</author>
<author>F Lareau</author>
</authors>
<marker>Wanner, Bohnet, Bouayad-Agha, Lareau, </marker>
<rawString>L. Wanner, B. Bohnet, N. Bouayad-Agha, F. Lareau,</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lohmeyer</author>
<author>D Nickla</author>
</authors>
<title>On the Challenge of Creating and Communicating Air Quality Information.</title>
<date>2007</date>
<booktitle>In In: Swayne A., Hrebicek J.(Eds.): Environmental Software Systems Dimensions of EnvironmentalInformatics. Vol.7.</booktitle>
<marker>Lohmeyer, Nickla, 2007</marker>
<rawString>A. Lohmeyer, and D. Nickla. 2007. On the Challenge of Creating and Communicating Air Quality Information. In In: Swayne A., Hrebicek J.(Eds.): Environmental Software Systems Dimensions of EnvironmentalInformatics. Vol.7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>