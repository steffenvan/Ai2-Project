<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003052">
<title confidence="0.997792">
A Test Environment for Natural Language Understanding Systems
</title>
<author confidence="0.971349">
Li Li, Deborah A. Dahl, Lewis M. Norton, Marcia C. Linebarger, Dongdong Chen
</author>
<affiliation confidence="0.947696">
Unisys Corporation
</affiliation>
<address confidence="0.949329">
2476 Swedesford Road
Malvern, PA 19355, U.S.A.
</address>
<email confidence="0.920571">
{Li.Li, Daborah.Dahl, Lewis.Norton, Marcia.Linebarger, Dong.Chen ) @unisys.com
</email>
<sectionHeader confidence="0.990097" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909809523809">
The Natural Language Understanding Engine
Test Environment (ETE) is a GUI software tool
that aids in the development and maintenance of
large, modular, natural language understanding
(NLU) systems. Natural language understanding
systems are composed of modules (such as part-
of-speech taggers, parsers and semantic
analyzers) which are difficult to test individually
because of the complexity of their output data
structures. Not only are the output data
structures of the internal modules complex, but
also many thousands of test items (messages or
sentences) are required to provide a reasonable
sample of the linguistic structures of a single
human language, even if the language is
restricted to a particular domain. The ETE
assists in the management and analysis of the
thousands of complex data structures created
during natural language processing of a large
corpus using relational database technology in a
network environment.
</bodyText>
<sectionHeader confidence="0.960875" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999750285714286">
Because of the complexity of the internal data
structures and the number of test cases involved in
testing a natural language understanding system,
evaluation of testing results by manual
comparison of the internal data structures is very
difficult. The difficulty of examining NLU
systems in turn greatly increases the difficulty of
developing and extending the coverage of these
systems, both because as the system increases in
coverage and complexity, extensions become
progressively harder to assess and because loss of
coverage of previously working test data becomes
harder to detect.
The ETE addresses these problems by:
</bodyText>
<listItem confidence="0.997137791666667">
1. managing batch input of large numbers of test
sentences or messages, whether spoken or
written.
2. storing the NLU system output for a batch run.
into a database.
3. automatically comparing multiple levels of
internal NLU data structures across batch
runs of the same data with different engine
versions. These data structures include part-
of-speech tags, syntactic analyses, and
semantic analyses.
4. flagging and displaying changed portions of
these data structures for an analyst&apos;s attention.
5. providing access to a variety of database
query options to allow an analyst to select
inputs of potential interest, for example, those
which took an abnormally long time to
process, or those which contain certain words.
6. providing a means for the analyst to annotate
and record the quality of the various
intermediate data structures.
7. providing a basis for quantifying both
regression and improvement in the NLU
system.
</listItem>
<sectionHeader confidence="0.765244" genericHeader="method">
1 Testing Natural Language
</sectionHeader>
<subsectionHeader confidence="0.958438">
Understanding Systems
</subsectionHeader>
<bodyText confidence="0.9510585">
Application level tests, in which the ability of the
system to output the correct answer on a set of
</bodyText>
<page confidence="0.99712">
763
</page>
<bodyText confidence="0.999966826923077">
inputs is measured, have been used in natural
language processing for a number of years
(ATIS-3 (1991), MUC-6 (1995), Harman and
Voorhees (1996)). Although these tests were
originally designed for comparing different
systems, they can also be used to compare the
performance of sequential versions of the same
system. These kinds of black-box tests, while
useful, do not provide insight into the correctness
of the internal NLU data structures since they are
only concerned with the end result, or answer
provided by the system. They also require the
implementation of a particular application against
which to test. This can be time-consuming and
also can give rise to the concern that the NLU
processing will become slanted toward the
particular test application as the developers
attempt to improve the system&apos;s performance on
that application.
The Parseval effort (Black (1991)) attempted to
compare parsing performance across systems
using the Treebank as a basis for comparison.
Although Parseval was very useful for comparing
parses, it did not enable developers to compare
other data structures, such as semantic
representations. In addition, in order to
accommodate many different parsing formalisms
for evaluation, it does not attempt to compare
every aspect of the parses. Finally, Treebank data
is not always available for domains which need to
be tested.
King (1996) discusses the general issues in NLU
system evaluations from a software engineering
point of view. Flickinger et al. (1987) describe in
very general terms a method for evaluation of
NLU systems in a single application domain
(database query) with a number of different
measures, such as accuracy of lexical analysis,
parsing, semantics, and correctness of query,
based on a large collection of annotated English
sentences. Neal et al. (1992) report on an effort to
develop a more general evaluation tool for NLU
systems. These approaches either focus on
application level tests or presuppose the
availability of large annotated test collections,
which in fact are very expensive to create and
maintain. For the purpose of diagnostic evaluation
of different versions of the same system, an
annotated test corpus is not absolutely necessary
because defects and regressions of the system can
be discovered from its internal data structures and
the differences between them.
</bodyText>
<sectionHeader confidence="0.98309" genericHeader="method">
2 Matrix Comparison Analysis
</sectionHeader>
<subsectionHeader confidence="0.907507">
of NLU Systems
</subsectionHeader>
<bodyText confidence="0.999990909090909">
A typical NLU system takes an input of certain
form and produces a final as well as a set of
intermediate analyses, for instance, parse trees,
represented by a variety of data structures ranging
from list to graph. These intermediate data can be
used as &amp;quot;milestones&amp;quot; to measure the behavior of
the underlying system and provide clues for
determining the types and scopes of problems.
The intermediate data can be further compared
systematically to reveal the behavior changes of a
system. In a synchronic comparison, different tests
are conducted for a version of the system by
changing its parameters, such as the presence or
absence of the lexical server, to determine the
impact of the module to the system. In a
diachronic comparison, tests are conducted for
different versions of the system with the same
parameters, to gauge the improvements of
development effort. In practice, any two tests can
be compared to determine the effect of certain
factors on the performance of a NLU system.
Conceptually, this type of matrix analysis can be
</bodyText>
<figure confidence="0.997420857142857">
ystem parameters
test data
Oi4 I •
2 system
2
•
n+1 n+2 versions
</figure>
<figureCaption confidence="0.999093">
Figure 1: Matrix Comparison Analysis
</figureCaption>
<page confidence="0.982291">
764
</page>
<bodyText confidence="0.999841384615385">
represented in a coordinate system (Figure 1) in
which a test is represented as a point and a
comparison between two tests as an arrowhead
line connecting the points. In theory, n-way and
second order comparisons are possible, but in
practice 2-way first order comparisons are most
useful.
ETE is designed for the Unisys natural language
engine (NLE), a NL system implemented in
Quintus Prolog. NLE can take as input text
(sentences or paragraphs) or nbest speech output
and produce the following intermediate data
structures as Prolog constructs:
</bodyText>
<listItem confidence="0.9997043">
• tokens (flat list)
• words (flat list)
• part-of-speech tags (flat list)
• lexical entries (nested attribute-value list)
• parse trees (tree)
• syntactic representation (graph and tree
derived from graph)
• semantic representation (graph and tree
derived from graph)
• processing time of different stages of analyses
</listItem>
<bodyText confidence="0.999835333333333">
The trees in this case are lines of text where
parent-child relationships are implied by line
indentations. A graph is expressed as a Prolog list
of terms, in which two terms are linked if they
have the same (constant) argument in a particular
position. In addition to these data structures, NLE
also generates a set of diagnostic flags, such as
backup parse (failure to achieve a full-span parse)
and incomplete semantic analysis.
A special command in NLE can be called to
produce the above data in a predefined format on
a given corpus.
</bodyText>
<sectionHeader confidence="0.994812" genericHeader="method">
3 The Engine Test Environment
</sectionHeader>
<bodyText confidence="0.999247733333333">
ETE is comprised of two components: a common
relational database that houses the test results and
a GUI program that manages and displays the test
resources and results. The central database is
stored on a file server PC and shared by the
analysts through ETE in a Windows NT network
environment . ETE communicates with NLE
through a TCP/IP socket and the Access database
with Visual Basic 5.0. Large and time-consuming
batch runs can be carried out on several machines
and imported into the database simultaneously.
Tests conducted on other platforms, such as Unix,
can be transferred into the ETE database and
analyzed as well.
The key functions of ETE are described below:
</bodyText>
<listItem confidence="0.9283582">
• Manage test resources: ETE provides an
graphical interface to manage various
resources needed for tests, including corpora,
NLE versions and parameter settings, and
connections to linguistic servers (Norton et al.
(1998)). The interface also enforces the
constraints on each test. For example, two
tests with different corpora cannot be
compared.
• Compare various types of analysis data.
</listItem>
<bodyText confidence="0.673838583333333">
ETE employs different algorithms to compute
the difference between different types of data
and display the disparate regions graphically
The comparison routines are implemented in
Prolog except for trees. Lists comparisons are
trivial in Prolog. Graph comparison is
achieved in two steps. First, all linkage
arguments in graph terms are substituted by
variables such that links are maintained by
unification. Second, set operations are applied
to compute the differences. Let U(G) denote
the variable substitution of a graph G and
diff(Gx, Gy) the set of different terms between
Gx and Gy, then diff(Gx, Gy) = Gx - U(Gy)
and diff(Gy, Gx) = Gy - U(Gx), where (-) is
the Prolog set difference operation. Under this
definition, differences in node ordering and
link labeling of two graphs are discounted in
comparison. For instance, Gx = [f(a, el),
g(el, e2)1, for which U(Gx) = [f(a, X), g(X,
Y)], is deemed identical to Gy = [g(e3, e4),
f(a, e3)], where ei are linkage arguments. It is
easy to see the time complexity of diff is
0(mn) for two graphs of size m and n
</bodyText>
<page confidence="0.995902">
765
</page>
<bodyText confidence="0.999519428571429">
respectively. Trees are treated as text files and
the DOS command fc (file comparison) is
utilized to compare the differences. Since fc
has several limits, we are considering
replacing it with a tree matching algorithm
that is more accurate and sensitive to
linguistic structures.
</bodyText>
<listItem confidence="0.956216">
• Present a hierarchical view of batch
</listItem>
<bodyText confidence="0.877048928571429">
analyses. We base our approach to visual
information management upon the notion of
&amp;quot;overview, filter, detail-on-demand.&amp;quot; For each
test, ETE displays a diagnostic report and a
table of sentence analyses. The diagnostic
report is an overview to direct an analyst&apos;s
attention to the problem areas which come
either from the system&apos;s own diagnostics, or
from comparisons. ETE is therefore still
useful even without every sentence being
annotated. The sentence analyses table
presents the intermediate data in their logical
order and shows on demand the details of each
type of data.
</bodyText>
<listItem confidence="0.8041215">
• Enable access to a variety of database
query capabilities. ETE stores all types of
intermediate data as strings in the database
and provides regular-expression based text
</listItem>
<bodyText confidence="0.879580333333333">
search for various data. A unique feature of
ETE is in-report query, which enables query
options on various reports to allow an analyst
to quickly zoom in to interesting data based on
the diagnostic information. Compared with
Tgrep (1992) which works only on Treebank
trees, ETE provides a more general and
powerful search mechanism for a complex
database.
</bodyText>
<listItem confidence="0.9694141875">
• Provide graphical and contextual
information for annotation. Annotation is a
problem because it still takes a human. ETE
offers flexible and easy access to the
intermediate data within and across batch
runs. For instance, when grading a semantic
analysis, the analyst can bring up the lexical
and syntactic analyses of the same sentence,
or look at the analyses of the sentence in other
tests at the same time, all with a few mouse
clicks. This context information helps analysts
to maintain consistency within and between
themselves during annotation.
• Facilitate access to other resources and
applications. Within ETE, an analyst can
execute other applications, such as Microsoft
</listItem>
<bodyText confidence="0.984634666666667">
Excel (spreadsheet), and interact with other
databases, such as a Problem Database which
tracks linguistic problems and an Application
Database which records test results for
specific applications, to offer an integrated
development, test and diagnosis environment
for a complex NLU system. The integration of
these databases will provide a foundation to
evaluate overall system performance. For
instance, it would be possible to determine
whether more accurate semantic analyses
increase the application accuracy.
</bodyText>
<sectionHeader confidence="0.94463" genericHeader="method">
4 Using the Engine Test
</sectionHeader>
<subsectionHeader confidence="0.85638">
Envirmunent
</subsectionHeader>
<bodyText confidence="0.9996755">
So far ETE has been used in the Unisys NLU
group for the following tasks:
</bodyText>
<listItem confidence="0.698858">
• Analyze and quantify system improvements
</listItem>
<bodyText confidence="0.991519571428572">
and regressions due to modifications to the
system, such as expanding lexicon, grammar
and knowledge base. In these diachronic
analyses, we use a baseline system and
compare subsequent versions against the
baseline performance, as well as the previous
version. ETE is used to filter out sentences
with changed syntactic and semantic analyses
so that the analyst can determine the types of
the changes in the light of other diagnostic
information. A new system can be
characterized by percentage of regression and
improvement in accuracy as well as time
speedup.
</bodyText>
<listItem confidence="0.605596333333333">
• Test the effects of new analysis strategies. For
instance, ETE has been used to study if our
system can benefit from a part-of-speech
tagger. With ETE, we were able quantify the
system&apos;s accuracy and speed improvements
with different tagging options easily and
</listItem>
<page confidence="0.993654">
766
</page>
<bodyText confidence="0.990260365853659">
quickly on test corpora and modify the system
and the tagger accordingly.
• Annotate parses and semantic analyses for
quality analysis and future reference. We have
so far used corrective and grading
annotations. In corrective annotation, the
analyst corrects a wrong analysis, for
example, a part-of-speech tag, with the correct
one. In grading annotation, the analyst assigns
proper categories to the analyses. In the tests
we found that both absolute grading (i.e. a
parse is perfect, mediocre or terrible in a test)
and relative grading (i.e. a parse is better,
same or worse in a comparison) are very
useful.
The corpora used in these tests are drawn from
various domains of English language, ranging
from single sentence questions to e-mail messages.
The performance of ETE on batch tests depends
largely on NLE, which in turn depends on the size
and complexity of a corpus. The tests therefore
range from 20 hours to 30 minutes with various
corpora in a Pentium Pro PC (200 Mhz, 256 MB
memory). A comparison of two batch test results
is independent of linguistic analysis and is linear
to the size of the corpus. So far we have
accumulated 209 MB of test data in the ETE
database. The tests show that ETE is capable of
dealing with large sets of test items (at an average
of 1,000 records per test) in a network
environment with fast database access responses.
ETE assists analysts to identify problems and
debug the system on large data sets. Without ETE,
it would be difficult, if not impossible, to perform
tasks of this complexity and scale. ETE not only
serves as a software tool for large scale tests of a
system, but also helps to enforce a sound and
systematic development strategy for the NLU
system. An issue to be further studied is whether
the presence of ETE skews the performance of
NLE as they compete for computer resources.
</bodyText>
<sectionHeader confidence="0.953047" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999978363636363">
We have described ETE, a software tool for NLU
systems and its application in our NL development
project. Even though ETE is tied to the current
NLU system architecture, its core concepts and
techniques, we believe, could be applicable to the
testing of other NLU systems. ETE is still
undergoing constant improvements, driven both by
the underlying NLU system and by users&apos; requests
for new features. The experiments with ETE so
far show that the tool is of great benefit for
advancing Unisys NLU technology
</bodyText>
<sectionHeader confidence="0.999175" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995543709677419">
ATIS-3 (1991) Proceedings of the DARPA Speech
and Natural Language Workshops, Morgan
Kaufmann
Black E. et al. (1991) A Procedure for Quantitatively
Comparing the Syntactic Coverage of English
Grammars, Proceedings of Speech and Natural
Language Workshop, DARPA, pp. 306 - 311
Flickinger D., Nerbounne J., Sag I., and Wasow T.
(1987) Toward Evaluation of NLP Systems.
Hewlett Packard Laboratories, Palo Alto, California
Harman D.K., Voorhees E.M. (1996) Proceedings of
the Fifth Text Retrieval Conference (TREC-5),
Department of Commerce and NIST
King Margaret (1996) Evaluating Natural Language
Processing Systems. Communication of ACM, Vol.
39, No. 1, January 1996, pp. 73 - 79
MUC-6 (1995) Proceedings of the Sixth Message
Understanding Conference, Columbia, Maryland,
Morgan Kaufmann
Neal J., Feit, E.L., Funke D.J., and Montgomery C.A.
(1992) An Evaluation Methodology for Natural
Language Processing Systems. Rome Laboratory
Technical Report RL-TR-92-308
Norton M.L., Dahl D.A., Li Li, Beals K.P. (1998)
Integration of Large-Scale Linguistic Resources in
a Natural Language Understanding System. to be
presented in COLING 98, August 10-14, 1998,
Universite de Montreal, Montreal, Quebec, Canada
Tgrep Documentation (1992)
http://www.Idc.upenn.eduildc/online/treebank/REA
DME.Iong
</reference>
<page confidence="0.996252">
767
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954189">
<title confidence="0.999989">A Test Environment for Natural Language Understanding Systems</title>
<author confidence="0.996056">Li Li</author>
<author confidence="0.996056">Deborah A Dahl</author>
<author confidence="0.996056">Lewis M Norton</author>
<author confidence="0.996056">Marcia C Linebarger</author>
<author confidence="0.996056">Dongdong Chen</author>
<affiliation confidence="0.999964">Unisys Corporation</affiliation>
<address confidence="0.998363">2476 Swedesford Road Malvern, PA 19355, U.S.A.</address>
<email confidence="0.987769">{Li.Li,Daborah.Dahl,Lewis.Norton,Marcia.Linebarger,Dong.Chen)@unisys.com</email>
<abstract confidence="0.998754454545454">The Natural Language Understanding Engine Test Environment (ETE) is a GUI software tool that aids in the development and maintenance of large, modular, natural language understanding (NLU) systems. Natural language understanding systems are composed of modules (such as partof-speech taggers, parsers and semantic analyzers) which are difficult to test individually because of the complexity of their output data structures. Not only are the output data structures of the internal modules complex, but also many thousands of test items (messages or sentences) are required to provide a reasonable sample of the linguistic structures of a single human language, even if the language is restricted to a particular domain. The ETE assists in the management and analysis of the thousands of complex data structures created during natural language processing of a large corpus using relational database technology in a network environment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>1991</date>
<booktitle>Proceedings of the DARPA Speech and Natural Language Workshops,</booktitle>
<publisher>Morgan Kaufmann</publisher>
<contexts>
<context position="3051" citStr="(1991)" startWordPosition="462" endWordPosition="462"> to select inputs of potential interest, for example, those which took an abnormally long time to process, or those which contain certain words. 6. providing a means for the analyst to annotate and record the quality of the various intermediate data structures. 7. providing a basis for quantifying both regression and improvement in the NLU system. 1 Testing Natural Language Understanding Systems Application level tests, in which the ability of the system to output the correct answer on a set of 763 inputs is measured, have been used in natural language processing for a number of years (ATIS-3 (1991), MUC-6 (1995), Harman and Voorhees (1996)). Although these tests were originally designed for comparing different systems, they can also be used to compare the performance of sequential versions of the same system. These kinds of black-box tests, while useful, do not provide insight into the correctness of the internal NLU data structures since they are only concerned with the end result, or answer provided by the system. They also require the implementation of a particular application against which to test. This can be time-consuming and also can give rise to the concern that the NLU process</context>
</contexts>
<marker>1991</marker>
<rawString>ATIS-3 (1991) Proceedings of the DARPA Speech and Natural Language Workshops, Morgan Kaufmann</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
</authors>
<title>A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars,</title>
<date>1991</date>
<booktitle>Proceedings of Speech and Natural Language Workshop, DARPA,</booktitle>
<pages>306--311</pages>
<contexts>
<context position="3830" citStr="Black (1991)" startWordPosition="582" endWordPosition="583">mance of sequential versions of the same system. These kinds of black-box tests, while useful, do not provide insight into the correctness of the internal NLU data structures since they are only concerned with the end result, or answer provided by the system. They also require the implementation of a particular application against which to test. This can be time-consuming and also can give rise to the concern that the NLU processing will become slanted toward the particular test application as the developers attempt to improve the system&apos;s performance on that application. The Parseval effort (Black (1991)) attempted to compare parsing performance across systems using the Treebank as a basis for comparison. Although Parseval was very useful for comparing parses, it did not enable developers to compare other data structures, such as semantic representations. In addition, in order to accommodate many different parsing formalisms for evaluation, it does not attempt to compare every aspect of the parses. Finally, Treebank data is not always available for domains which need to be tested. King (1996) discusses the general issues in NLU system evaluations from a software engineering point of view. Fli</context>
</contexts>
<marker>Black, 1991</marker>
<rawString>Black E. et al. (1991) A Procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars, Proceedings of Speech and Natural Language Workshop, DARPA, pp. 306 - 311</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>J Nerbounne</author>
<author>I Sag</author>
<author>T Wasow</author>
</authors>
<title>Toward Evaluation of NLP Systems. Hewlett Packard Laboratories,</title>
<date>1987</date>
<location>Palo Alto, California</location>
<contexts>
<context position="4451" citStr="Flickinger et al. (1987)" startWordPosition="675" endWordPosition="678">91)) attempted to compare parsing performance across systems using the Treebank as a basis for comparison. Although Parseval was very useful for comparing parses, it did not enable developers to compare other data structures, such as semantic representations. In addition, in order to accommodate many different parsing formalisms for evaluation, it does not attempt to compare every aspect of the parses. Finally, Treebank data is not always available for domains which need to be tested. King (1996) discusses the general issues in NLU system evaluations from a software engineering point of view. Flickinger et al. (1987) describe in very general terms a method for evaluation of NLU systems in a single application domain (database query) with a number of different measures, such as accuracy of lexical analysis, parsing, semantics, and correctness of query, based on a large collection of annotated English sentences. Neal et al. (1992) report on an effort to develop a more general evaluation tool for NLU systems. These approaches either focus on application level tests or presuppose the availability of large annotated test collections, which in fact are very expensive to create and maintain. For the purpose of d</context>
</contexts>
<marker>Flickinger, Nerbounne, Sag, Wasow, 1987</marker>
<rawString>Flickinger D., Nerbounne J., Sag I., and Wasow T. (1987) Toward Evaluation of NLP Systems. Hewlett Packard Laboratories, Palo Alto, California</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Harman</author>
<author>E M Voorhees</author>
</authors>
<date>1996</date>
<booktitle>Proceedings of the Fifth Text Retrieval Conference (TREC-5), Department of Commerce and NIST</booktitle>
<contexts>
<context position="3093" citStr="Harman and Voorhees (1996)" startWordPosition="465" endWordPosition="468">otential interest, for example, those which took an abnormally long time to process, or those which contain certain words. 6. providing a means for the analyst to annotate and record the quality of the various intermediate data structures. 7. providing a basis for quantifying both regression and improvement in the NLU system. 1 Testing Natural Language Understanding Systems Application level tests, in which the ability of the system to output the correct answer on a set of 763 inputs is measured, have been used in natural language processing for a number of years (ATIS-3 (1991), MUC-6 (1995), Harman and Voorhees (1996)). Although these tests were originally designed for comparing different systems, they can also be used to compare the performance of sequential versions of the same system. These kinds of black-box tests, while useful, do not provide insight into the correctness of the internal NLU data structures since they are only concerned with the end result, or answer provided by the system. They also require the implementation of a particular application against which to test. This can be time-consuming and also can give rise to the concern that the NLU processing will become slanted toward the particu</context>
</contexts>
<marker>Harman, Voorhees, 1996</marker>
<rawString>Harman D.K., Voorhees E.M. (1996) Proceedings of the Fifth Text Retrieval Conference (TREC-5), Department of Commerce and NIST</rawString>
</citation>
<citation valid="true">
<authors>
<author>King Margaret</author>
</authors>
<title>Evaluating Natural Language Processing Systems.</title>
<date>1996</date>
<journal>Communication of ACM,</journal>
<volume>39</volume>
<pages>73--79</pages>
<marker>Margaret, 1996</marker>
<rawString>King Margaret (1996) Evaluating Natural Language Processing Systems. Communication of ACM, Vol. 39, No. 1, January 1996, pp. 73 - 79</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference,</booktitle>
<publisher>Morgan Kaufmann</publisher>
<location>Columbia, Maryland,</location>
<contexts>
<context position="3065" citStr="(1995)" startWordPosition="464" endWordPosition="464">uts of potential interest, for example, those which took an abnormally long time to process, or those which contain certain words. 6. providing a means for the analyst to annotate and record the quality of the various intermediate data structures. 7. providing a basis for quantifying both regression and improvement in the NLU system. 1 Testing Natural Language Understanding Systems Application level tests, in which the ability of the system to output the correct answer on a set of 763 inputs is measured, have been used in natural language processing for a number of years (ATIS-3 (1991), MUC-6 (1995), Harman and Voorhees (1996)). Although these tests were originally designed for comparing different systems, they can also be used to compare the performance of sequential versions of the same system. These kinds of black-box tests, while useful, do not provide insight into the correctness of the internal NLU data structures since they are only concerned with the end result, or answer provided by the system. They also require the implementation of a particular application against which to test. This can be time-consuming and also can give rise to the concern that the NLU processing will becom</context>
</contexts>
<marker>1995</marker>
<rawString>MUC-6 (1995) Proceedings of the Sixth Message Understanding Conference, Columbia, Maryland, Morgan Kaufmann</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Neal</author>
<author>E L Feit</author>
<author>D J Funke</author>
<author>C A Montgomery</author>
</authors>
<title>An Evaluation Methodology for Natural Language Processing Systems. Rome Laboratory</title>
<date>1992</date>
<tech>Technical Report RL-TR-92-308</tech>
<contexts>
<context position="4769" citStr="Neal et al. (1992)" startWordPosition="725" endWordPosition="728">alisms for evaluation, it does not attempt to compare every aspect of the parses. Finally, Treebank data is not always available for domains which need to be tested. King (1996) discusses the general issues in NLU system evaluations from a software engineering point of view. Flickinger et al. (1987) describe in very general terms a method for evaluation of NLU systems in a single application domain (database query) with a number of different measures, such as accuracy of lexical analysis, parsing, semantics, and correctness of query, based on a large collection of annotated English sentences. Neal et al. (1992) report on an effort to develop a more general evaluation tool for NLU systems. These approaches either focus on application level tests or presuppose the availability of large annotated test collections, which in fact are very expensive to create and maintain. For the purpose of diagnostic evaluation of different versions of the same system, an annotated test corpus is not absolutely necessary because defects and regressions of the system can be discovered from its internal data structures and the differences between them. 2 Matrix Comparison Analysis of NLU Systems A typical NLU system takes</context>
</contexts>
<marker>Neal, Feit, Funke, Montgomery, 1992</marker>
<rawString>Neal J., Feit, E.L., Funke D.J., and Montgomery C.A. (1992) An Evaluation Methodology for Natural Language Processing Systems. Rome Laboratory Technical Report RL-TR-92-308</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Norton</author>
<author>D A Dahl</author>
<author>Li Li</author>
<author>K P Beals</author>
</authors>
<title>Integration of Large-Scale Linguistic Resources in a Natural Language Understanding System.</title>
<date>1998</date>
<institution>Universite de Montreal,</institution>
<location>Montreal, Quebec, Canada Tgrep Documentation</location>
<note>to be presented in COLING 98,</note>
<contexts>
<context position="8826" citStr="Norton et al. (1998)" startWordPosition="1387" endWordPosition="1390">twork environment . ETE communicates with NLE through a TCP/IP socket and the Access database with Visual Basic 5.0. Large and time-consuming batch runs can be carried out on several machines and imported into the database simultaneously. Tests conducted on other platforms, such as Unix, can be transferred into the ETE database and analyzed as well. The key functions of ETE are described below: • Manage test resources: ETE provides an graphical interface to manage various resources needed for tests, including corpora, NLE versions and parameter settings, and connections to linguistic servers (Norton et al. (1998)). The interface also enforces the constraints on each test. For example, two tests with different corpora cannot be compared. • Compare various types of analysis data. ETE employs different algorithms to compute the difference between different types of data and display the disparate regions graphically The comparison routines are implemented in Prolog except for trees. Lists comparisons are trivial in Prolog. Graph comparison is achieved in two steps. First, all linkage arguments in graph terms are substituted by variables such that links are maintained by unification. Second, set operations</context>
</contexts>
<marker>Norton, Dahl, Li, Beals, 1998</marker>
<rawString>Norton M.L., Dahl D.A., Li Li, Beals K.P. (1998) Integration of Large-Scale Linguistic Resources in a Natural Language Understanding System. to be presented in COLING 98, August 10-14, 1998, Universite de Montreal, Montreal, Quebec, Canada Tgrep Documentation (1992) http://www.Idc.upenn.eduildc/online/treebank/REA DME.Iong</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>