<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993142">
Incremental Parsing with Parallel Multiple Context-Free Grammars
</title>
<author confidence="0.987747">
Krasimir Angelov
</author>
<affiliation confidence="0.995402">
Chalmers University of Technology
</affiliation>
<address confidence="0.542347">
G¨oteborg, Sweden
</address>
<email confidence="0.985668">
krasimir@chalmers.se
</email>
<sectionHeader confidence="0.993496" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9977831">
Parallel Multiple Context-Free Grammar
(PMCFG) is an extension of context-free
grammar for which the recognition problem is
still solvable in polynomial time. We describe
a new parsing algorithm that has the advantage
to be incremental and to support PMCFG
directly rather than the weaker MCFG formal-
ism. The algorithm is also top-down which
allows it to be used for grammar based word
prediction.
</bodyText>
<sectionHeader confidence="0.998726" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999533206349207">
Parallel Multiple Context-Free Grammar (PMCFG)
(Seki et al., 1991) is one of the grammar formalisms
that have been proposed for the syntax of natural lan-
guages. It is an extension of context-free grammar
(CFG) where the right hand side of the production rule
is a tuple of strings instead of only one string. Using tu-
ples the grammar can model discontinuous constituents
which makes it more powerful than context-free gram-
mar. In the same time PMCFG has the advantage to be
parseable in polynomial time which makes it attractive
from computational point of view.
A parsing algorithm is incremental if it reads the in-
put one token at the time and calculates all possible
consequences of the token, before the next token is
read. There is substantial evidence showing that hu-
mans process language in an incremental fashion which
makes the incremental algorithms attractive from cog-
nitive point of view.
If the algorithm is also top-down then it is possible
to predict the next word from the sequence of preced-
ing words using the grammar. This can be used for
example in text based dialog systems or text editors for
controlled language where the user might not be aware
of the grammar coverage. In this case the system can
suggest the possible continuations.
A restricted form of PMCFG that is still stronger
than CFG is Multiple Context-Free Grammar (MCFG).
In Seki and Kato (2008) it has been shown that
MCFG is equivalent to string-based Linear Context-
Free Rewriting Systems and Finite-Copying Tree
Transducers and it is stronger than Tree Adjoining
Grammars (Joshi and Schabes, 1997). Efficient recog-
nition and parsing algorithms for MCFG have been de-
scribed in Nakanishi et al. (1997), Ljungl¨of (2004) and
Burden and Ljungl¨of (2005). They can be used with
PMCFG also but it has to be approximated with over-
generating MCFG and post processing is needed to fil-
ter out the spurious parsing trees.
We present a parsing algorithm that is incremental,
top-down and supports PMCFG directly. The algo-
rithm exploits a view of PMCFG as an infinite context-
free grammar where new context-free categories and
productions are generated during parsing. It is trivial to
turn the algorithm into statistical by attaching probabil-
ities to each rule.
In Ljungl¨of (2004) it has been shown that the Gram-
matical Framework (GF) formalism (Ranta, 2004) is
equivalent to PMCFG. The algorithm was implemented
as part of the GF interpreter and was evaluated with the
resource grammar library (Ranta, 2008) which is the
largest collection of grammars written in this formal-
ism. The incrementality was used to build a help sys-
tem which suggests the next possible words to the user.
Section 2 gives a formal definition of PMCFG. In
section 3 the procedure for “linearization” i.e. the
derivation of string from syntax tree is defined. The
definition is needed for better understanding of the for-
mal proofs in the paper. The algorithm introduction
starts with informal description of the idea in section
4 and after that the formal rules are given in section
5. The implementation details are outlined in section 6
and after that there are some comments on the evalua-
tion in section 7. Section 8 gives a conclusion.
</bodyText>
<sectionHeader confidence="0.978265" genericHeader="method">
2 PMCFG definition
</sectionHeader>
<construct confidence="0.835015">
Definition 1 A parallel multiple context-free grammar
is an 8-tuple G = (N, T, F, P, S, d, r, a) where:
</construct>
<listItem confidence="0.958620625">
• Nis a finite set of categories and a positive integer
d(A) called dimension is given for each A E N.
• T is a finite set of terminal symbols which is dis-
joint with N.
• F is a finite set offunctions where the arity a(f)
and the dimensions r(f) and di(f) (1 &lt; i &lt;
a(f)) are given for every f E F. For every posi-
tive integer d, (T*)d denote the set of all d-tuples
</listItem>
<bodyText confidence="0.312636">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 69–76,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.981725">
69
</page>
<equation confidence="0.7290325">
of strings over T. Each function f ∈ F is a to-
tal mapping from (T*)d1(f) × (T*)d2(f) × · · · ×
(T*)da(f)(f) to (T*)r(f), defined as:
f := (α1, α2, ... , αr(f))
</equation>
<bodyText confidence="0.8692065">
Here αi is a sequence of terminals and hk; li
pairs, where 1 ≤ k ≤ a(f) is called argument
index and 1 ≤ l ≤ dk(f) is called constituent
index.
</bodyText>
<equation confidence="0.6763435">
• P is a finite set ofproductions of the form:
A → f[A1,A2,...,Aa(f)]
</equation>
<bodyText confidence="0.850232">
where A ∈ N is called result category,
A1, A2, ... , Aa(f) ∈ N are called argument cat-
egories and f ∈ F is the function symbol. For
the production to be well formed the conditions
</bodyText>
<equation confidence="0.592043">
di(f) = d(Ai) (1 ≤ i ≤ a(f)) and r(f) = d(A)
must hold.
• S is the start category and d(S) = 1.
</equation>
<bodyText confidence="0.9999914">
We use the same definition of PMCFG as is used by
Seki and Kato (2008) and Seki et al. (1993) with the
minor difference that they use variable names like xkl
while we use hk; li to refer to the function arguments.
As an example we will use the anbncn language:
</bodyText>
<equation confidence="0.999371666666667">
S → c[N]
N → s[N]
N → z[]
c := (h1;1i h1; 2i h1; 3i)
s := (a h1; 1i, b h1; 2i, c h1; 3i)
z := (E, e, e)
</equation>
<bodyText confidence="0.999381666666667">
Here the dimensions are d(S) = 1 and d(N) = 3 and
the arities are a(c) = a(s) = 1 and a(z) = 0. a is the
empty string.
</bodyText>
<sectionHeader confidence="0.996799" genericHeader="method">
3 Derivation
</sectionHeader>
<bodyText confidence="0.9672585">
The derivation of a string in PMCFG is a two-step pro-
cess. First we have to build a syntax tree of a category
S and after that to linearize this tree to string. The defi-
nition of a syntax tree is recursive:
Definition 2 (f t1 ... ta(f)) is a tree of category A if
ti is a tree of category Bi and there is a production:
A → f[B1 ... Ba(f)]
The abstract notation for “t is a tree of category A”
is t : A. When a(f) = 0 then the tree does not have
children and the node is called leaf.
The linearization is bottom-up. The functions in the
leaves do not have arguments so the tuples in their defi-
nitions already contain constant strings. If the function
has arguments then they have to be linearized and the
results combined. Formally this can be defined as a
function L applied to the syntax tree:
L(f t1 t2 ... ta(f)) = (x1, x2 ... xr(f))
where xi = K(L(t1), L(t2) ... L(ta(f))) αi
and f := (α1, α2 ... αr(f)) ∈ F
The function uses a helper function K which takes the
already linearized arguments and a sequence αi of ter-
minals and hk; li pairs and returns a string. The string
is produced by simple substitution of each hk; li with
the string for constituent l from argument k:
</bodyText>
<equation confidence="0.994084">
K σ (β1hk1; l1iβ2hk2; l2i ...βn) = β1σk1l1β2σk2l2 ... βn
</equation>
<bodyText confidence="0.9992732">
where βi ∈ T*. The recursion in L terminates when a
leaf is reached.
In the example anbncn language the function z does
not have arguments and it corresponds to the base case
when n = 0. Every application of s over another tree
t : N increases n by one. For example the syntax tree
(s (s z)) will produce the tuple (aa, bb, cc). Finally the
application of c combines all elements in the tuple in
a single string i.e. c (s (s z)) will produce the string
aabbcc.
</bodyText>
<sectionHeader confidence="0.990462" genericHeader="method">
4 The Idea
</sectionHeader>
<bodyText confidence="0.998972555555556">
Although PMCFG is not context-free it can be approx-
imated with an overgenerating context-free grammar.
The problem with this approach is that the parser pro-
duces many spurious parse trees that have to be filtered
out. A direct parsing algorithm for PMCFG should
avoid this and a careful look at the difference between
PMCFG and CFG gives an idea. The context-free ap-
proximation of anbncn is the language a*b*c* with
grammar:
</bodyText>
<equation confidence="0.9995475">
S → ABC
A → �  |aA
B → �  |bB
C → �  |cC
</equation>
<bodyText confidence="0.9987395">
The string &amp;quot;aabbcc&amp;quot; is in the language and it can be
derived with the following steps:
</bodyText>
<listItem confidence="0.947443727272727">
S
⇒ ABC
⇒ aABC
⇒ aaABC
⇒ aaBC
⇒ aabBC
⇒ aabbBC
⇒ aabbC
⇒ aabbcC
⇒ aabbccC
⇒ aabbcc
</listItem>
<page confidence="0.989052">
70
</page>
<bodyText confidence="0.9997222">
The grammar is only an approximation because there
is no enforcement that we will use only equal number
of reductions for A, B and C. This can be guaranteed
if we replace B and C with new categories B&apos; and C&apos;
after the derivation of A:
</bodyText>
<figure confidence="0.823728666666667">
B&apos; → bB&apos;&apos; C&apos; → cC&apos;&apos;
B&apos;&apos; → bB&apos;&apos;&apos; C&apos;&apos; → cC&apos;&apos;&apos;
B&apos;&apos;&apos; → E C&apos;&apos;&apos; → E
</figure>
<bodyText confidence="0.998390416666667">
In this case the only possible derivation from aaB&apos;C&apos;
is aabbcc.
The PMCFG parser presented in this paper works
like context-free parser, except that during the parsing
it generates fresh categories and rules which are spe-
cializations of the originals. The newly generated rules
are always versions of already existing rules where
some category is replaced with new more specialized
category. The generation of specialized categories pre-
vents the parser from recognizing phrases that are oth-
erwise withing the scope of the context-free approxi-
mation of the original grammar.
</bodyText>
<sectionHeader confidence="0.991637" genericHeader="method">
5 Parsing
</sectionHeader>
<bodyText confidence="0.9693468">
The algorithm is described as a deductive process in
the style of (Shieber et al., 1995). The process derives
a set of items where each item is a statement about the
grammatical status of some substring in the input.
The inference rules are in natural deduction style:
</bodyText>
<equation confidence="0.704521">
X1 ... Xn
&lt; side conditions on X1, ... , Xn &gt;
Y
</equation>
<bodyText confidence="0.999919666666667">
where the premises Xi are some items and Y is the
derived item. We assume that w1 ... wn is the input
string.
</bodyText>
<subsectionHeader confidence="0.994855">
5.1 Deduction Rules
</subsectionHeader>
<bodyText confidence="0.993862090909091">
The deduction system deals with three types of items:
active, passive and production items.
Productions In Shieber’s deduction systems the
grammar is a constant and the existence of a given pro-
duction is specified as a side condition. In our case the
grammar is incrementally extended at runtime, so the
set of productions is part of the deduction set. The pro-
ductions from the original grammar are axioms and are
included in the initial deduction set.
Active Items The active items represent the partial
parsing result:
</bodyText>
<equation confidence="0.859989">
[kjA → f[~B];l : α • β] , j &lt; k
The interpretation is that there is a function f with a
corresponding production:
A → f[~B]
f := (γ1, ... γl−1, αβ, ... γr(f))
</equation>
<bodyText confidence="0.9981315">
such that the tree (f t1 ... ta(f)) will produce the sub-
string wj+1 ... wk as a prefix in constituent l for any
</bodyText>
<equation confidence="0.9596011875">
INITIAL PREDICT
S f[~B] S - start category, α = rhs(f,1)
[00S → f[~B];1 : •α]
PREDICT
Bd → g[~C] [kjA → f[~B]; l : α • (d; r) β]γ = rhs(g, r)
[kkBd → g[~C]; r : •γ]
SCAN
[kjA → f[~B];l : α • s β]
[k+1
j A → f[~B];l : α s • β]
COMPLETE
[kj A → f[~B];l : α•] N = (A,l, j, k)
N → f[~B] [kj A;l; N]
COMBINE
[uj A → f[~B]; l : α • (d; r) β] [k uBd; r; N]
[kj A → f[~B{d := N}]; l : α (d; r) • β]
</equation>
<figureCaption confidence="0.997509">
Figure 1: Deduction Rules
</figureCaption>
<bodyText confidence="0.976061">
sequence of arguments ti : Bi. The sequence α is the
part that produced the substring:
JC(G(t1), G(t2) ... G(ta(f))) α = wj+1 ... wk
and β is the part that is not processed yet.
Passive Items The passive items are of the form:
[kjA;l;N] , j &lt; k
and state that there exists at least one production:
</bodyText>
<equation confidence="0.936115">
A → f[~B]
f := (γ1,γ2,...γr(f))
</equation>
<bodyText confidence="0.974284">
and a tree (f t1 ... ta(f)) : A such that the constituent
with index l in the linearization of the tree is equal to
wj+1 ... wk. Contrary to the active items in the passive
the whole constituent is matched:
JC(G(t1), G(t2) ... G(ta(f))) γl = wj+1 ... wk
Each time when we complete an active item, a pas-
sive item is created and at the same time we cre-
ate a new category N which accumulates all produc-
tions for A that produce the wj+1 ... wk substring from
constituent l. All trees of category N must produce
wj+1 ... wk in the constituent l.
There are six inference rules (see figure 1).
The INITIAL PREDICT rule derives one item spanning
the 0 − 0 range for each production with the start cat-
egory S on the left hand side. The rhs(f, l) function
returns the constituent with index l of function f.
In the PREDICT rule, for each active item with dot be-
fore a (d; r) pair and for each production for Bd, a new
active item is derived where the dot is in the beginning
of constituent r in g.
When the dot is before some terminal s and s is equal
to the current terminal wk then the SCAN rule derives a
new item where the dot is moved to the next position.
</bodyText>
<equation confidence="0.923993">
s = wk+1
</equation>
<page confidence="0.966072">
71
</page>
<bodyText confidence="0.998668789473685">
When the dot is at the end of an active item then it
is converted to passive item in the COMPLETE rule. The
category N in the passive item is a fresh category cre-
ated for each unique (A, l, j, k) quadruple. A new pro-
duction is derived for N which has the same function
and arguments as in the active item.
The item in the premise of COMPLETE was at some
point predicted in PREDICT from some other item. The
COMBINE rule will later replace the occurence A in the
original item (the premise of PREDICT) with the special-
ization N.
The COMBINE rule has two premises: one active item
and one passive. The passive item starts from position
u and the only inference rule that can derive items with
different start positions is PREDICT. Also the passive
item must have been predicted from active item where
the dot is before (d; r), the category for argument num-
ber d must have been Bd and the item ends at u. The
active item in the premise of COMBINE is such an item
so it was one of the items used to predict the passive
one. This means that we can move the dot after (d; r)
and the d-th argument is replaced with its specialization
N.
If the string 0 contains another reference to the d-th
argument then the next time when it has to be predicted
the rule PREDICT will generate active items, only for
those productions that were successfully used to parse
the previous constituents. If a context-free approxima-
tion was used this would have been equivalent to unifi-
cation of the redundant subtrees. Instead this is done at
runtime which also reduces the search space.
The parsing is successful if we had derived the
[o S;1; S&apos;] item, where n is the length of the text, S is
the start category and S&apos; is the newly created category.
The parser is incremental because all active items
span up to position k and the only way to move to the
next position is the SCAN rule where a new symbol from
the input is consumed.
</bodyText>
<subsectionHeader confidence="0.993463">
5.2 Soundness
</subsectionHeader>
<bodyText confidence="0.872343375">
The parsing system is sound if every derivable item rep-
resents a valid grammatical statement under the inter-
pretation given to every type of item.
The derivation in INITIAL PREDICT and PREDICT is
sound because the item is derived from existing pro-
duction and the string before the dot is empty so:
1CQe=e
The rationale for SCAN is that if
</bodyText>
<equation confidence="0.62539">
1C u α = wj−1 ... wk
and s = wk+1 then
1C u (α s) = wj−1 ... wk+1
</equation>
<bodyText confidence="0.999778111111111">
If the item in the premise is valid then it is based on
existing production and function and so will be the item
in the consequent.
In the COMPLETE rule the dot is at the end of the
string. This means that wj+1 ... wk will be not just
a prefix in constituent l of the linearization but the full
string. This is exactly what is required in the semantics
of the passive item. The passive item is derived from
a valid active item so there is at least one production
for A. The category N is unique for each (A, l, j, k)
quadruple so it uniquely identifies the passive item in
which it is placed. There might be many productions
that can produce the passive item but all of them should
be able to generate wj+1 ... wk and they are exactly
the productions that are added to N. From all this ar-
guments it follows that COMPLETE is sound.
The COMBINE rule is sound because from the active
item in the premise we know that:
</bodyText>
<equation confidence="0.997470666666667">
1C u α = wj+1 ... wu
for every context u built from the trees:
t1 : B1; t2 : B2; ...ta(f) : Ba(f)
</equation>
<bodyText confidence="0.993048">
From the passive item we know that every production
for N produces the wu+1 ... wk in r. From that follows
that
</bodyText>
<equation confidence="0.982303">
1C o&apos; (α(d; r)) = wj+1 ... wk
</equation>
<bodyText confidence="0.9994076">
where u&apos; is the same as u except that Bd is replaced
with N. Note that the last conclusion will not hold if we
were using the original context because Bd is a more
general category and can contain productions that does
not derive wu+1 ... wk.
</bodyText>
<subsectionHeader confidence="0.98841">
5.3 Completeness
</subsectionHeader>
<bodyText confidence="0.999647833333333">
The parsing system is complete if it derives an item
for every valid grammatical statement. In our case we
have to prove that for every possible parse tree the cor-
responding items will be derived.
The proof for completeness requires the following
lemma:
</bodyText>
<equation confidence="0.737087666666667">
Lemma 1 For every possible syntax tree
(f t1 ... ta(f)) : A
with linearization
L(ft1 ... ta(f)) = (x1, x2 ... xd(A))
where xl = wj+1 ... wk, the system will derive an item
[kj A; l; A&apos;] if the item [kj A → f[B]; l : •αl] was pre-
dicted before that. We assume that the function defini-
tion is:
f := (α1, α2 ... α,(f))
</equation>
<bodyText confidence="0.9992786">
The proof is by induction on the depth of the tree.
If the tree has only one level then the function f does
not have arguments and from the linearization defini-
tion and from the premise in the lemma it follows that
αl = wj+1 ... wk. From the active item in the lemma
</bodyText>
<page confidence="0.996092">
72
</page>
<bodyText confidence="0.999923666666667">
by applying iteratively the SCAN rule and finally the
COMPLETE rule the system will derive the requested
item.
If the tree has subtrees then we assume that the
lemma is true for every subtree and we prove it for the
whole tree. We know that
</bodyText>
<equation confidence="0.991998">
K u al = wj+1 ... wk
</equation>
<bodyText confidence="0.999958444444444">
Since the function K does simple substitution it is pos-
sible for each (d; s) pair in al to find a new range in the
input string j0−k0 such that the lemma to be applicable
for the corresponding subtree td : Bd. The terminals in
al will be processed by the SCAN rule. Rule PREDICT
will generate the active items required for the subtrees
and the COMBINE rule will consume the produced pas-
sive items. Finally the COMPLETE rule will derive the
requested item for the whole tree.
From the lemma we can prove the completeness of
the parsing system. For every possible tree t : S such
that G(t) = (w1 ... wn) we have to prove that the
[n0 S;1; S0] item will be derived. Since the top-level
function of the tree must be from production for S the
INITIAL PREDICT rule will generate the active item in
the premise of the lemma. From this and from the as-
sumptions for t it follows that the requested passive
item will be derived.
</bodyText>
<subsectionHeader confidence="0.960965">
5.4 Complexity
</subsectionHeader>
<bodyText confidence="0.999662782608696">
The algorithm is very similar to the Earley (1970) algo-
rithm for context-free grammars. The similarity is even
more apparent when the inference rules in this paper
are compared to the inference rules for the Earley al-
gorithm presented in Shieber et al. (1995) and Ljungl¨of
(2004). This suggests that the space and time complex-
ity of the PMCFG parser should be similar to the com-
plexity of the Earley parser which is O(n2) for space
and O(n3) for time. However we generate new cate-
gories and productions at runtime and this have to be
taken into account.
Let the P(j) function be the maximal number of pro-
ductions generated from the beginning up to the state
where the parser has just consumed terminal number
j. P(j) is also the upper limit for the number of cat-
egories created because in the worst case there will be
only one production for each new category.
The active items have two variables that directly de-
pend on the input size - the start index j and the end
index k. If an item starts at position j then there are
(n − j + 1) possible values for k because j &lt; k &lt; n.
The item also contains a production and there are P(j)
possible choices for it. In total there are:
</bodyText>
<equation confidence="0.981697666666667">
n
E (n − j + 1)P(j)
j=0
</equation>
<bodyText confidence="0.9982814">
possible choices for one active item. The possibilities
for all other variables are only a constant factor. The
P(j) function is monotonic because the algorithm only
adds new productions and never removes. From that
follows the inequality:
</bodyText>
<equation confidence="0.984214666666667">
n n
E (n − j + 1)P(j) &lt; P(n) E (n − j + 1)
j=0 i=0
</equation>
<bodyText confidence="0.683395">
which gives the approximation for the upper limit:
</bodyText>
<equation confidence="0.9557295">
P(n)n(n + 1)
2
</equation>
<bodyText confidence="0.999302333333333">
The same result applies to the passive items. The only
difference is that the passive items have only a category
instead of a full production. However the upper limit
for the number of categories is the same. Finally the
upper limit for the total number of active, passive and
production items is:
</bodyText>
<equation confidence="0.983005">
P(n)(n2 + n + 1)
</equation>
<bodyText confidence="0.999927176470588">
The expression for P(n) is grammar dependent but
we can estimate that it is polynomial because the set
of productions corresponds to the compact representa-
tion of all parse trees in the context-free approximation
of the grammar. The exponent however is grammar de-
pendent. From this we can expect that asymptotic space
complexity will be O(ne) where a is some parameter
for the grammar. This is consistent with the results in
Nakanishi et al. (1997) and Ljungl¨of (2004) where the
exponent also depends on the grammar.
The time complexity is proportional to the number
of items and the time needed to derive one item. The
time is dominated by the most complex rule which in
this algorithm is COMBINE. All variables that depend
on the input size are present both in the premises and
in the consequent except u. There are n possible values
for u so the time complexity is O(ne+1).
</bodyText>
<subsectionHeader confidence="0.993462">
5.5 Tree Extraction
</subsectionHeader>
<bodyText confidence="0.99939825">
If the parsing is successful we need a way to extract the
syntax trees. Everything that we need is already in the
set of newly generated productions. If the goal item is
[n0 S; 0; S0] then every tree t of category S0 that can be
constructed is a syntax tree for the input sentence (see
definition 2 in section 3 again).
Note that the grammar can be erasing; i.e., there
might be productions like this:
</bodyText>
<equation confidence="0.982144">
S , f[B1, B2, B3]
f := ((1;1)(3;1))
</equation>
<bodyText confidence="0.977151833333333">
There are three arguments but only two of them are
used. When the string is parsed this will generate a
new specialized production:
S0 , f[B01, B2, B03]
Here S,B1 and B3 are specialized to S0, B01 and B03
but the B2 category is still the same. This is correct
</bodyText>
<page confidence="0.997383">
73
</page>
<bodyText confidence="0.999954230769231">
because actually any subtree for the second argument
will produce the same result. Despite this it is some-
times useful to know which parts of the tree were used
and which were not. In the GF interpreter such un-
used branches are replaced by meta variables. In this
case the tree extractor should check whether the cate-
gory also exists in the original set of categories N in
the grammar.
Just like with the context-free grammars the parsing
algorithm is polynomial but the chart can contain ex-
ponential or even infinite number of trees. Despite this
the chart is a compact finite representation of the set of
trees.
</bodyText>
<sectionHeader confidence="0.993415" genericHeader="method">
6 Implementation
</sectionHeader>
<bodyText confidence="0.999202534883721">
Every implementation requires a careful design of the
data structures in the parser. For efficient access the set
of items is split into four subsets: A, Sj, C and P. A
is the agenda i.e. the set of active items that have to be
analyzed. Sj contains items for which the dot is before
an argument reference and which span up to position j.
C is the set of possible continuations i.e. a set of items
for which the dot is just after a terminal. P is the set
of productions. In addition the set F is used internally
for the generatation of fresh categories. The sets C,
Sj and F are used as association maps. They contain
associations like k �--&gt; v where k is the key and v is the
value. All maps except F can contain more than one
value for one and the same key.
The pseudocode of the implementation is given in
figure 2. There are two procedures Init and Compute.
Init computes the initial values of S, P and A. The
initial agenda A is the set of all items that can be pre-
dicted from the start category S (INITIAL PREDICT rule).
Compute consumes items from the current agenda
and applies the SCAN, PREDICT, COMBINE or COMPLETE
rule. The case statement matches the current item
against the patterns of the rules and selects the proper
rule. The PREDICT and COMBINE rules have two
premises so they are used in two places. In both cases
one of the premises is related to the current item and a
loop is needed to find item matching the other premis.
The passive items are not independent entities but
are just the combination of key and value in the set F.
Only the start position of every item is kept because the
end position for the interesting passive items is always
the current position and the active items are either in
the agenda if they end at the current position or they
are in the Sj set if they end at position j. The active
items also keep only the dot position in the constituent
because the constituent definition can be retrieved from
the grammar. For this reason the runtime representation
of the items is [j; A → f[B]; l; p] where j is the start
position of the item and p is the dot position inside the
constituent.
The Compute function returns the updated S and P
sets and the set of possible continuations C. The set of
continuations is a map indexed by a terminal and the
</bodyText>
<table confidence="0.7204886">
Language Productions Constituents
Bulgarian 3516 75296
English 1165 8290
German 8078 21201
Swedish 1496 8793
</table>
<tableCaption confidence="0.66207">
Table 1: GF Resource Grammar Library size in number
of PMCFG productions and discontinuous constituents
</tableCaption>
<figureCaption confidence="0.983497">
Figure 3: Parser performance in miliseconds per token
</figureCaption>
<bodyText confidence="0.98237525">
values are active items. The parser computes the set of
continuations at each step and if the current terminal is
one of the keys the set of values for it is taken as an
agenda for the next step.
</bodyText>
<sectionHeader confidence="0.998594" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999991478260869">
The algorithm was evaluated with four languages from
the GF resource grammar library (Ranta, 2008): Bul-
garian, English, German and Swedish. These gram-
mars are not primarily intended for parsing but as a
resource from which smaller domain dependent gram-
mars are derived for every application. Despite this, the
resource grammar library is a good benchmark for the
parser because these are the biggest GF grammars.
The compiler converts a grammar written in the
high-level GF language to a low-level PMCFG gram-
mar which the parser can use directly. The sizes of
the grammars in terms of number of productions and
number of unique discontinuous constituents are given
on table 1. The number of constituents roughly cor-
responds to the number of productions in the context-
free approximation of the grammar. The parser per-
formance in terms of miliseconds per token is shown in
figure 3. In the evaluation 34272 sentences were parsed
and the average time for parsing a given number of to-
kens is drawn in the chart. As it can be seen, although
the theoretical complexity is polynomial, the real-time
performance for practically interesting grammars tends
to be linear.
</bodyText>
<sectionHeader confidence="0.997969" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9986055">
The algorithm has proven useful in the GF system. It
accomplished the initial goal to provide suggestions
</bodyText>
<figure confidence="0.94091428">
ms
1200
1000
800
600
400
200
0
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Number of Tokens
German Bulgarian Swedish English
74
procedure Init() {
k = 0
Si = ∅, for every i
P = the set ofproductions P in the grammar
A = ∅
forall S → f[CB] ∈ P do // INITIAL PREDICT
A = A + [0;S → f[CB];1;0]
return (S, P, A)
}
procedure Compute(k, (S, P, A)) {
C = ∅
F = ∅
while A =6 ∅ do {
let x ∈ A, x ≡ [j; A → f[CB]; l; p]
A = A − x
case the dot in x is {
before s ∈ T ⇒ C = C + (s 7→ [j; A → f[ CB]; l; p + 1]) // SCAN
before hd; ri ⇒ if ((Bd, r) 7→ (x, d)) ∈6 Sk then {
Sk = Sk + ((Bd, r) 7→ (x, d))
forall Bd → g[C�] ∈ P do // PREDICT
A = A + [k; Bd → g[C�]; r; 0]
}
forall (k; Bd, r) 7→ N ∈ F do // COMBINE
A = A + [j;A → f[CB{d := N}];l;p + 1]
at the end ⇒ if ∃N.((j, A, l) 7→ N ∈ F) then {
forall (N, r) 7→ (x&apos;, d&apos;) ∈ Sk do // PREDICT
A = A + [k;N → f[CB];r;0]
} else {
generate fresh N // COMPLETE
F = F + ((j,A, l) 7→ N)
forall (A, l) 7→ ([j&apos;; A&apos; → f&apos;[CB&apos;]; l&apos;; p&apos;], d) ∈ Sj do // COMBINE
A = A + [j&apos;;A&apos; → f&apos;[ CB&apos;{d := N}];l&apos;;p&apos; + 1]
}
P = P + (N → f[CB])
}
}
return (S, P, C)
}
</figure>
<figureCaption confidence="0.999339">
Figure 2: Pseudocode of the parser implementation
</figureCaption>
<page confidence="0.993612">
75
</page>
<bodyText confidence="0.999899785714286">
in text based dialog systems and in editors for con-
trolled languages. Additionally the algorithm has prop-
erties that were not envisaged in the beginning. It
works with PMCFG directly rather that by approxima-
tion with MCFG or some other weaker formalism.
Since the Linear Context-Free Rewriting Systems,
Finite-Copying Tree Transducers and Tree Adjoining
Grammars can be converted to PMCFG, the algorithm
presented in this paper can be used with the converted
grammar. The approach to represent context-dependent
grammar as infinite context-free grammar might be ap-
plicable to other formalisms as well. This will make it
very attractive in applications where some of the other
formalisms are already in use.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999818956521739">
H˚akan Burden and Peter Ljungl¨of. 2005. Parsing
linear context-free rewriting systems. In Proceed-
ings of the Ninth International Workshop on Parsing
Technologies (IWPT), pages 11–17, October.
Jay Earley. 1970. An efficient context-free parsing al-
gorithm. Commun. ACM, 13(2):94–102.
Aravind Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Lan-
guages. Vol 3: Beyond Words, chapter 2, pages 69–
123. Springer-Verlag, Berlin/Heidelberg/New York.
Peter Ljungl¨of. 2004. Expressivity and Complexity of
the Grammatical Framework. Ph.D. thesis, Depart-
ment of Computer Science, Gothenburg University
and Chalmers University of Technology, November.
Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki.
1997. An Efficient Recognition Algorithm for Mul-
tiple ContextFree Languages. In Fifth Meeting
on Mathematics of Language. The Association for
Mathematics of Language, August.
Aarne Ranta. 2004. Grammatical Framework: A
Type-Theoretical Grammar Formalism. Journal of
Functional Programming, 14(2):145–189, March.
Aarne Ranta. 2008. GF Resource Grammar Library.
digitalgrammars.com/gf/lib/.
Hiroyuki Seki and Yuki Kato. 2008. On the Genera-
tive Power of Multiple Context-Free Grammars and
Macro Grammars. IEICE-Transactions on Info and
Systems, E91-D(2):209–221.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88(2):191–229, October.
Hiroyuki Seki, Ryuichi Nakanishi, Yuichi Kaji,
Sachiko Ando, and Tadao Kasami. 1993. Par-
allel Multiple Context-Free Grammars, Finite-State
Translation Systems, and Polynomial-Time Recog-
nizable Subclasses of Lexical-Functional Grammars.
In 31st Annual Meeting of the Association for Com-
putational Linguistics, pages 130–140. Ohio State
University, Association for Computational Linguis-
tics, June.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and Implementation of
Deductive Parsing. Journal of Logic Programming,
24(1&amp;2):3–36.
</reference>
<page confidence="0.991815">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.739497">
<title confidence="0.999404">Incremental Parsing with Parallel Multiple Context-Free Grammars</title>
<author confidence="0.835604">Krasimir Angelov</author>
<affiliation confidence="0.994497">Chalmers University of Technology</affiliation>
<address confidence="0.962888">G¨oteborg, Sweden</address>
<email confidence="0.990285">krasimir@chalmers.se</email>
<abstract confidence="0.993283727272727">Parallel Multiple Context-Free Grammar (PMCFG) is an extension of context-free grammar for which the recognition problem is still solvable in polynomial time. We describe a new parsing algorithm that has the advantage to be incremental and to support PMCFG directly rather than the weaker MCFG formalism. The algorithm is also top-down which allows it to be used for grammar based word prediction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H˚akan Burden</author>
<author>Peter Ljungl¨of</author>
</authors>
<title>Parsing linear context-free rewriting systems.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>11--17</pages>
<marker>Burden, Ljungl¨of, 2005</marker>
<rawString>H˚akan Burden and Peter Ljungl¨of. 2005. Parsing linear context-free rewriting systems. In Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 11–17, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="17781" citStr="Earley (1970)" startWordPosition="3375" endWordPosition="3376">he produced passive items. Finally the COMPLETE rule will derive the requested item for the whole tree. From the lemma we can prove the completeness of the parsing system. For every possible tree t : S such that G(t) = (w1 ... wn) we have to prove that the [n0 S;1; S0] item will be derived. Since the top-level function of the tree must be from production for S the INITIAL PREDICT rule will generate the active item in the premise of the lemma. From this and from the assumptions for t it follows that the requested passive item will be derived. 5.4 Complexity The algorithm is very similar to the Earley (1970) algorithm for context-free grammars. The similarity is even more apparent when the inference rules in this paper are compared to the inference rules for the Earley algorithm presented in Shieber et al. (1995) and Ljungl¨of (2004). This suggests that the space and time complexity of the PMCFG parser should be similar to the complexity of the Earley parser which is O(n2) for space and O(n3) for time. However we generate new categories and productions at runtime and this have to be taken into account. Let the P(j) function be the maximal number of productions generated from the beginning up to t</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay Earley. 1970. An efficient context-free parsing algorithm. Commun. ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages. Vol 3: Beyond Words, chapter 2,</booktitle>
<pages>69--123</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin/Heidelberg/New York.</location>
<contexts>
<context position="2162" citStr="Joshi and Schabes, 1997" startWordPosition="343" endWordPosition="346">dict the next word from the sequence of preceding words using the grammar. This can be used for example in text based dialog systems or text editors for controlled language where the user might not be aware of the grammar coverage. In this case the system can suggest the possible continuations. A restricted form of PMCFG that is still stronger than CFG is Multiple Context-Free Grammar (MCFG). In Seki and Kato (2008) it has been shown that MCFG is equivalent to string-based Linear ContextFree Rewriting Systems and Finite-Copying Tree Transducers and it is stronger than Tree Adjoining Grammars (Joshi and Schabes, 1997). Efficient recognition and parsing algorithms for MCFG have been described in Nakanishi et al. (1997), Ljungl¨of (2004) and Burden and Ljungl¨of (2005). They can be used with PMCFG also but it has to be approximated with overgenerating MCFG and post processing is needed to filter out the spurious parsing trees. We present a parsing algorithm that is incremental, top-down and supports PMCFG directly. The algorithm exploits a view of PMCFG as an infinite contextfree grammar where new context-free categories and productions are generated during parsing. It is trivial to turn the algorithm into s</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind Joshi and Yves Schabes. 1997. Treeadjoining grammars. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages. Vol 3: Beyond Words, chapter 2, pages 69– 123. Springer-Verlag, Berlin/Heidelberg/New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ljungl¨of</author>
</authors>
<title>Expressivity and Complexity of the Grammatical Framework.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Gothenburg University and Chalmers University of Technology,</institution>
<marker>Ljungl¨of, 2004</marker>
<rawString>Peter Ljungl¨of. 2004. Expressivity and Complexity of the Grammatical Framework. Ph.D. thesis, Department of Computer Science, Gothenburg University and Chalmers University of Technology, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichi Nakanishi</author>
<author>Keita Takada</author>
<author>Hiroyuki Seki</author>
</authors>
<title>An Efficient Recognition Algorithm for Multiple ContextFree Languages.</title>
<date>1997</date>
<booktitle>In Fifth Meeting on Mathematics of Language. The Association for Mathematics of Language,</booktitle>
<contexts>
<context position="2264" citStr="Nakanishi et al. (1997)" startWordPosition="360" endWordPosition="363"> in text based dialog systems or text editors for controlled language where the user might not be aware of the grammar coverage. In this case the system can suggest the possible continuations. A restricted form of PMCFG that is still stronger than CFG is Multiple Context-Free Grammar (MCFG). In Seki and Kato (2008) it has been shown that MCFG is equivalent to string-based Linear ContextFree Rewriting Systems and Finite-Copying Tree Transducers and it is stronger than Tree Adjoining Grammars (Joshi and Schabes, 1997). Efficient recognition and parsing algorithms for MCFG have been described in Nakanishi et al. (1997), Ljungl¨of (2004) and Burden and Ljungl¨of (2005). They can be used with PMCFG also but it has to be approximated with overgenerating MCFG and post processing is needed to filter out the spurious parsing trees. We present a parsing algorithm that is incremental, top-down and supports PMCFG directly. The algorithm exploits a view of PMCFG as an infinite contextfree grammar where new context-free categories and productions are generated during parsing. It is trivial to turn the algorithm into statistical by attaching probabilities to each rule. In Ljungl¨of (2004) it has been shown that the Gra</context>
<context position="20053" citStr="Nakanishi et al. (1997)" startWordPosition="3792" endWordPosition="3795">n. However the upper limit for the number of categories is the same. Finally the upper limit for the total number of active, passive and production items is: P(n)(n2 + n + 1) The expression for P(n) is grammar dependent but we can estimate that it is polynomial because the set of productions corresponds to the compact representation of all parse trees in the context-free approximation of the grammar. The exponent however is grammar dependent. From this we can expect that asymptotic space complexity will be O(ne) where a is some parameter for the grammar. This is consistent with the results in Nakanishi et al. (1997) and Ljungl¨of (2004) where the exponent also depends on the grammar. The time complexity is proportional to the number of items and the time needed to derive one item. The time is dominated by the most complex rule which in this algorithm is COMBINE. All variables that depend on the input size are present both in the premises and in the consequent except u. There are n possible values for u so the time complexity is O(ne+1). 5.5 Tree Extraction If the parsing is successful we need a way to extract the syntax trees. Everything that we need is already in the set of newly generated productions. </context>
</contexts>
<marker>Nakanishi, Takada, Seki, 1997</marker>
<rawString>Ryuichi Nakanishi, Keita Takada, and Hiroyuki Seki. 1997. An Efficient Recognition Algorithm for Multiple ContextFree Languages. In Fifth Meeting on Mathematics of Language. The Association for Mathematics of Language, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Grammatical Framework: A Type-Theoretical Grammar Formalism.</title>
<date>2004</date>
<journal>Journal of Functional Programming,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="2911" citStr="Ranta, 2004" startWordPosition="468" endWordPosition="469">jungl¨of (2005). They can be used with PMCFG also but it has to be approximated with overgenerating MCFG and post processing is needed to filter out the spurious parsing trees. We present a parsing algorithm that is incremental, top-down and supports PMCFG directly. The algorithm exploits a view of PMCFG as an infinite contextfree grammar where new context-free categories and productions are generated during parsing. It is trivial to turn the algorithm into statistical by attaching probabilities to each rule. In Ljungl¨of (2004) it has been shown that the Grammatical Framework (GF) formalism (Ranta, 2004) is equivalent to PMCFG. The algorithm was implemented as part of the GF interpreter and was evaluated with the resource grammar library (Ranta, 2008) which is the largest collection of grammars written in this formalism. The incrementality was used to build a help system which suggests the next possible words to the user. Section 2 gives a formal definition of PMCFG. In section 3 the procedure for “linearization” i.e. the derivation of string from syntax tree is defined. The definition is needed for better understanding of the formal proofs in the paper. The algorithm introduction starts with</context>
</contexts>
<marker>Ranta, 2004</marker>
<rawString>Aarne Ranta. 2004. Grammatical Framework: A Type-Theoretical Grammar Formalism. Journal of Functional Programming, 14(2):145–189, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<date>2008</date>
<journal>GF Resource Grammar Library. digitalgrammars.com/gf/lib/.</journal>
<contexts>
<context position="3061" citStr="Ranta, 2008" startWordPosition="492" endWordPosition="493">he spurious parsing trees. We present a parsing algorithm that is incremental, top-down and supports PMCFG directly. The algorithm exploits a view of PMCFG as an infinite contextfree grammar where new context-free categories and productions are generated during parsing. It is trivial to turn the algorithm into statistical by attaching probabilities to each rule. In Ljungl¨of (2004) it has been shown that the Grammatical Framework (GF) formalism (Ranta, 2004) is equivalent to PMCFG. The algorithm was implemented as part of the GF interpreter and was evaluated with the resource grammar library (Ranta, 2008) which is the largest collection of grammars written in this formalism. The incrementality was used to build a help system which suggests the next possible words to the user. Section 2 gives a formal definition of PMCFG. In section 3 the procedure for “linearization” i.e. the derivation of string from syntax tree is defined. The definition is needed for better understanding of the formal proofs in the paper. The algorithm introduction starts with informal description of the idea in section 4 and after that the formal rules are given in section 5. The implementation details are outlined in sect</context>
<context position="24699" citStr="Ranta, 2008" startWordPosition="4644" endWordPosition="4645">s a map indexed by a terminal and the Language Productions Constituents Bulgarian 3516 75296 English 1165 8290 German 8078 21201 Swedish 1496 8793 Table 1: GF Resource Grammar Library size in number of PMCFG productions and discontinuous constituents Figure 3: Parser performance in miliseconds per token values are active items. The parser computes the set of continuations at each step and if the current terminal is one of the keys the set of values for it is taken as an agenda for the next step. 7 Evaluation The algorithm was evaluated with four languages from the GF resource grammar library (Ranta, 2008): Bulgarian, English, German and Swedish. These grammars are not primarily intended for parsing but as a resource from which smaller domain dependent grammars are derived for every application. Despite this, the resource grammar library is a good benchmark for the parser because these are the biggest GF grammars. The compiler converts a grammar written in the high-level GF language to a low-level PMCFG grammar which the parser can use directly. The sizes of the grammars in terms of number of productions and number of unique discontinuous constituents are given on table 1. The number of constit</context>
</contexts>
<marker>Ranta, 2008</marker>
<rawString>Aarne Ranta. 2008. GF Resource Grammar Library. digitalgrammars.com/gf/lib/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Yuki Kato</author>
</authors>
<date>2008</date>
<booktitle>On the Generative Power of Multiple Context-Free Grammars and Macro Grammars. IEICE-Transactions on Info and Systems, E91-D(2):209–221.</booktitle>
<contexts>
<context position="1957" citStr="Seki and Kato (2008)" startWordPosition="312" endWordPosition="315">owing that humans process language in an incremental fashion which makes the incremental algorithms attractive from cognitive point of view. If the algorithm is also top-down then it is possible to predict the next word from the sequence of preceding words using the grammar. This can be used for example in text based dialog systems or text editors for controlled language where the user might not be aware of the grammar coverage. In this case the system can suggest the possible continuations. A restricted form of PMCFG that is still stronger than CFG is Multiple Context-Free Grammar (MCFG). In Seki and Kato (2008) it has been shown that MCFG is equivalent to string-based Linear ContextFree Rewriting Systems and Finite-Copying Tree Transducers and it is stronger than Tree Adjoining Grammars (Joshi and Schabes, 1997). Efficient recognition and parsing algorithms for MCFG have been described in Nakanishi et al. (1997), Ljungl¨of (2004) and Burden and Ljungl¨of (2005). They can be used with PMCFG also but it has to be approximated with overgenerating MCFG and post processing is needed to filter out the spurious parsing trees. We present a parsing algorithm that is incremental, top-down and supports PMCFG d</context>
<context position="5153" citStr="Seki and Kato (2008)" startWordPosition="897" endWordPosition="900">a(f)(f) to (T*)r(f), defined as: f := (α1, α2, ... , αr(f)) Here αi is a sequence of terminals and hk; li pairs, where 1 ≤ k ≤ a(f) is called argument index and 1 ≤ l ≤ dk(f) is called constituent index. • P is a finite set ofproductions of the form: A → f[A1,A2,...,Aa(f)] where A ∈ N is called result category, A1, A2, ... , Aa(f) ∈ N are called argument categories and f ∈ F is the function symbol. For the production to be well formed the conditions di(f) = d(Ai) (1 ≤ i ≤ a(f)) and r(f) = d(A) must hold. • S is the start category and d(S) = 1. We use the same definition of PMCFG as is used by Seki and Kato (2008) and Seki et al. (1993) with the minor difference that they use variable names like xkl while we use hk; li to refer to the function arguments. As an example we will use the anbncn language: S → c[N] N → s[N] N → z[] c := (h1;1i h1; 2i h1; 3i) s := (a h1; 1i, b h1; 2i, c h1; 3i) z := (E, e, e) Here the dimensions are d(S) = 1 and d(N) = 3 and the arities are a(c) = a(s) = 1 and a(z) = 0. a is the empty string. 3 Derivation The derivation of a string in PMCFG is a two-step process. First we have to build a syntax tree of a category S and after that to linearize this tree to string. The definiti</context>
</contexts>
<marker>Seki, Kato, 2008</marker>
<rawString>Hiroyuki Seki and Yuki Kato. 2008. On the Generative Power of Multiple Context-Free Grammars and Macro Grammars. IEICE-Transactions on Info and Systems, E91-D(2):209–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Takashi Matsumura</author>
<author>Mamoru Fujii</author>
<author>Tadao Kasami</author>
</authors>
<title>On multiple contextfree grammars.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<volume>88</volume>
<issue>2</issue>
<contexts>
<context position="643" citStr="Seki et al., 1991" startWordPosition="87" endWordPosition="90">Parallel Multiple Context-Free Grammars Krasimir Angelov Chalmers University of Technology G¨oteborg, Sweden krasimir@chalmers.se Abstract Parallel Multiple Context-Free Grammar (PMCFG) is an extension of context-free grammar for which the recognition problem is still solvable in polynomial time. We describe a new parsing algorithm that has the advantage to be incremental and to support PMCFG directly rather than the weaker MCFG formalism. The algorithm is also top-down which allows it to be used for grammar based word prediction. 1 Introduction Parallel Multiple Context-Free Grammar (PMCFG) (Seki et al., 1991) is one of the grammar formalisms that have been proposed for the syntax of natural languages. It is an extension of context-free grammar (CFG) where the right hand side of the production rule is a tuple of strings instead of only one string. Using tuples the grammar can model discontinuous constituents which makes it more powerful than context-free grammar. In the same time PMCFG has the advantage to be parseable in polynomial time which makes it attractive from computational point of view. A parsing algorithm is incremental if it reads the input one token at the time and calculates all possi</context>
</contexts>
<marker>Seki, Matsumura, Fujii, Kasami, 1991</marker>
<rawString>Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, and Tadao Kasami. 1991. On multiple contextfree grammars. Theoretical Computer Science, 88(2):191–229, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Seki</author>
<author>Ryuichi Nakanishi</author>
<author>Yuichi Kaji</author>
<author>Sachiko Ando</author>
<author>Tadao Kasami</author>
</authors>
<title>Parallel Multiple Context-Free Grammars, Finite-State Translation Systems, and Polynomial-Time Recognizable Subclasses of Lexical-Functional Grammars.</title>
<date>1993</date>
<contexts>
<context position="5176" citStr="Seki et al. (1993)" startWordPosition="902" endWordPosition="905">ned as: f := (α1, α2, ... , αr(f)) Here αi is a sequence of terminals and hk; li pairs, where 1 ≤ k ≤ a(f) is called argument index and 1 ≤ l ≤ dk(f) is called constituent index. • P is a finite set ofproductions of the form: A → f[A1,A2,...,Aa(f)] where A ∈ N is called result category, A1, A2, ... , Aa(f) ∈ N are called argument categories and f ∈ F is the function symbol. For the production to be well formed the conditions di(f) = d(Ai) (1 ≤ i ≤ a(f)) and r(f) = d(A) must hold. • S is the start category and d(S) = 1. We use the same definition of PMCFG as is used by Seki and Kato (2008) and Seki et al. (1993) with the minor difference that they use variable names like xkl while we use hk; li to refer to the function arguments. As an example we will use the anbncn language: S → c[N] N → s[N] N → z[] c := (h1;1i h1; 2i h1; 3i) s := (a h1; 1i, b h1; 2i, c h1; 3i) z := (E, e, e) Here the dimensions are d(S) = 1 and d(N) = 3 and the arities are a(c) = a(s) = 1 and a(z) = 0. a is the empty string. 3 Derivation The derivation of a string in PMCFG is a two-step process. First we have to build a syntax tree of a category S and after that to linearize this tree to string. The definition of a syntax tree is </context>
</contexts>
<marker>Seki, Nakanishi, Kaji, Ando, Kasami, 1993</marker>
<rawString>Hiroyuki Seki, Ryuichi Nakanishi, Yuichi Kaji, Sachiko Ando, and Tadao Kasami. 1993. Parallel Multiple Context-Free Grammars, Finite-State Translation Systems, and Polynomial-Time Recognizable Subclasses of Lexical-Functional Grammars.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>130--140</pages>
<institution>Ohio State University, Association for Computational Linguistics,</institution>
<marker></marker>
<rawString>In 31st Annual Meeting of the Association for Computational Linguistics, pages 130–140. Ohio State University, Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<date>1995</date>
<booktitle>Principles and Implementation of Deductive Parsing. Journal of Logic Programming,</booktitle>
<pages>24--1</pages>
<contexts>
<context position="8893" citStr="Shieber et al., 1995" startWordPosition="1627" endWordPosition="1630">&apos; is aabbcc. The PMCFG parser presented in this paper works like context-free parser, except that during the parsing it generates fresh categories and rules which are specializations of the originals. The newly generated rules are always versions of already existing rules where some category is replaced with new more specialized category. The generation of specialized categories prevents the parser from recognizing phrases that are otherwise withing the scope of the context-free approximation of the original grammar. 5 Parsing The algorithm is described as a deductive process in the style of (Shieber et al., 1995). The process derives a set of items where each item is a statement about the grammatical status of some substring in the input. The inference rules are in natural deduction style: X1 ... Xn &lt; side conditions on X1, ... , Xn &gt; Y where the premises Xi are some items and Y is the derived item. We assume that w1 ... wn is the input string. 5.1 Deduction Rules The deduction system deals with three types of items: active, passive and production items. Productions In Shieber’s deduction systems the grammar is a constant and the existence of a given production is specified as a side condition. In our</context>
<context position="17990" citStr="Shieber et al. (1995)" startWordPosition="3408" endWordPosition="3411"> such that G(t) = (w1 ... wn) we have to prove that the [n0 S;1; S0] item will be derived. Since the top-level function of the tree must be from production for S the INITIAL PREDICT rule will generate the active item in the premise of the lemma. From this and from the assumptions for t it follows that the requested passive item will be derived. 5.4 Complexity The algorithm is very similar to the Earley (1970) algorithm for context-free grammars. The similarity is even more apparent when the inference rules in this paper are compared to the inference rules for the Earley algorithm presented in Shieber et al. (1995) and Ljungl¨of (2004). This suggests that the space and time complexity of the PMCFG parser should be similar to the complexity of the Earley parser which is O(n2) for space and O(n3) for time. However we generate new categories and productions at runtime and this have to be taken into account. Let the P(j) function be the maximal number of productions generated from the beginning up to the state where the parser has just consumed terminal number j. P(j) is also the upper limit for the number of categories created because in the worst case there will be only one production for each new categor</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and Implementation of Deductive Parsing. Journal of Logic Programming, 24(1&amp;2):3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>