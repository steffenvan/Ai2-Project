<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000169">
<note confidence="0.820219666666667">
ALIGNING SENTENCES IN PARALLEL CORPORA
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer
IBM Thomas J. Watson Research Center
</note>
<author confidence="0.60917">
P.O. Box 704
Yorktown Heights, NY 10598
</author>
<sectionHeader confidence="0.807118" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.996490857142857">
In this paper we describe a statistical tech-
nique for aligning sentences with their translations
in two parallel corpora. In addition to certain
anchor points that are available in our data, the
only information about the sentences that we use
for calculating alignments is the number of tokens
that they contain. Because we make no use of the
lexical details of the sentence, the alignment com-
putation is fast and therefore practical for appli-
cation to very large collections of text. We have
used this technique to align several million sen-
tences in the English-French Hansard corpora and
have achieved an accuracy in excess of 99% in a
random selected set of 1000 sentence pairs that we
checked by hand. We show that even without the
benefit of anchor points the correlation between
the lengths of aligned sentences is strong enough
that we should expect to achieve an accuracy of
between 96% and 97%. Thus, the technique may
be applicable to a wider variety of texts than we
have yet tried.
</bodyText>
<sectionHeader confidence="0.997739" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999947608695652">
Recent work by Brown et al., [Brown et
al., 1988, Brown et at., 1990] has quickened
anew the long dormant idea of using statistical
techniques to carry out machine translation
from one natural language to another. The
lynchpin of their approach is a large collection
of pairs of sentences that are mutual transla-
tions. Beyond providing grist to the statisti-
cal mill, such pairs of sentences are valuable
to researchers in bilingual lexicography [Kia.-
vans and Tzoukerma.nn, 1990, Warwick and
Russell, 1990] and may be useful in other ap-
proaches to machine translation [Sadler, 1989].
In this paper, we consider the problem of
extracting from parallel French and English
corpora. pairs sentences that are translations
of one another. The task is not trivial because
at times a. single sentence in one language is
translated as two or more sentences in the
other language. At other times a sentence,
or even a whole passage, may be missing from
one or the other of the corpora.
If a person is given two parallel texts and
asked to match up the sentences in them, it is
natural for him to look at the words in the sen-
tences. Elaborating this intuitively appealing
insight, researchers at Xerox and at ISSCO
[Kay, 1991, Catizone et al., 1989] have devel-
oped alignment algorithms that pair sentences
according to the words that they contain. Any
such algorithm is necessarily slow and, despite
the potential for highly accurate alignment,
may be unsuitable for very large collections
of text. Our algorithm makes no use of the
lexical details of the corpora, but deals only
with the number of words in each sentence.
Although we have used it only to align paral-
lel French and English corpora from the pro-
ceedings of the Canadian Parliament, we ex-
pect that our technique would work on other
French and English corpora and even on other
pairs of languages. The work of Gale and
Church , [Gale and Church, 1991], who use
a. very similar method but measure sentence
lengths in characters rather than in words,
supports this promise of wider applicability.
</bodyText>
<sectionHeader confidence="0.932005" genericHeader="method">
THE HANSARD CORPORA
</sectionHeader>
<bodyText confidence="0.998165">
Brown et at., [Brown et al., 1990] describe
the process by which the proceedings of the
Canadian Parliament are recorded. In Canada,
these proceedings are referred to as Hansards.
</bodyText>
<page confidence="0.998125">
169
</page>
<bodyText confidence="0.999870416666666">
Our Hansard corpora consist of the Hansards
from 1973 through 1986. There are two files
for each session of parliament: one English
and one French. After converting the obscure
text markup language of the raw data. to TEX,
we combined all of the English files into a sin-
gle, large English corpus and all of the French
files into a single, large French corpus. We
then segmented the text of each corpus into
tokens and combined the tokens into groups
that we call sentences. Generally, these con-
form to the grade-school notion of a sentence:
they begin with a capital letter, contain a
verb, and end with some type of sentence-final
punctuation. Occasionally, they fall short of
this ideal and so each corpus contains a num-
ber of sentence fragments and other groupings
of words that we nonetheless refer to as sen-
tences. With this broad interpretation, the
English corpus contains 85,016,286 tokens in
3,510,744 sentences, and the French corpus
contains 97,857,452 tokens in 3,690,425 sen-
tences. The average English sentence has 24.2
tokens, while the average French sentence is
about 9.5% longer with 26.5 tokens.
The left-hand side of Figure 1 shows the
raw data for a portion of the English corpus,
and the right-hand side shows the same por-
tion after we converted it to TEX and divided
it up into sentences. The sentence numbers do
not advance regularly because we have edited
the sample in order to display a variety of phe-
nomena.
In addition to a verbatim record of the
proceedings and its translation, the Hansards
include session numbers, names of speakers,
time stamps, question numbers, and indica-
tions of the original language in which each
speech was delivered. We retain this auxiliary
information in the form of comments sprin-
kled throughout the text. Each comment has
the form \SC M ... \EC A 11} as shown
on the right-hand side of Figure 1. In ad-
dition to these comments, which encode in-
formation explicitly present in the data, we
inserted Paragraph comments as suggested by
the space command of which we see an exam-
ple in the eighth line on the left-band side of
</bodyText>
<figureCaption confidence="0.956735">
Figure 1.
</figureCaption>
<bodyText confidence="0.99881545">
We mark the beginning of a parliamentary
session with a Document comment as shown
in Sentence 1 on the right-hand side of Fig-
ure 1. Usually, when a member addresses the
parliament, his name is recorded and we en-
code it in an Author comment. We see an ex-
ample of this in Sentence 4. If the president
speaks, he is referred to in the English cor-
pus as Mr. Speaker and in the French corpus
as At. le President. If several members speak
at once, a shockingly regular occurrence, they
are referred to as Some Hon. Members in the
English and as Des Voir in the French. Times
are recorded either as exact times on a. 24-hour
basis as in Sentence 81, or as inexact times of
which there are two forms: Time -= Later,
and Time = Recess. These are rendered in
French as Time -= Plus Tard and Time = Re-
cess. Other types of comments that appear
are shown in Table 1.
</bodyText>
<sectionHeader confidence="0.992212" genericHeader="method">
ALIGNING ANCHOR POINTS
</sectionHeader>
<bodyText confidence="0.99989564">
After examining the Hansard corpora, we
realized that the comments laced throughout
would serve as useful anchor points in any
alignment process. We divide the comments
into major and minor anchors as follows. The
comments Author = Mr. Speaker, Author =
Ai. le President, Author = Some Hon. Mem-
bers, and Author = Des Voix are called minor
anchors. All other comments are called major
anchors with the exception of the Paragraph
comment which is not, treated as an anchor at
all. The minor anchors are much more com-
mon than any particular major anchor, mak-
ing an alignment based on them less robust
against deletions than one based on the ma-
jor anchors. Therefore, we have carried out
the alignment of anchor points in two passes,
first aligning the major anchors and then the
minor anchors.
Usually, the major anchors appear in both
languages. Sometimes, however, through inat-
tention on the part of the translator or other
misadventure, the name of a speaker may be
garbled or a comment may be omitted. In the
first alignment pass, we assign to alignments
</bodyText>
<page confidence="0.791158">
170
</page>
<equation confidence="0.8094985">
/*START_COMMENT* Beginning file = 048
101 11002-108 script A *END_COMMENT*/
.TB 029 060 090 099
.PL 060
.LL 120
.NF
</equation>
<bodyText confidence="0.987812848484848">
The House met at 2 p.m.
.SP
*boMr. Donald MacInnis (Cape Breton
-East Richmond):*ro Mr. Speaker,
I rise on a question of privilege af-
fecting the rights and prerogatives
of parliamentary committees and one
which reflects on the word of two
ministers.
.SP
*boMr. Speaker: *roThe hon. member&apos;s
motion is proposed to the
House under the terms of Standing
Order 43. Is there unanimous consent?
.SP
*boSome hon. Members: *roAgreed.
s*itText*ro)
Question No. 17--*boMr. Mazankowski:
*ro
1. For the period April 1, 1973 to
January 31, 1974, what amount of
money was expended on the operation
and maintenance of the Prime
Minister&apos;s residence at Harrington
Lake, Quebec?
.SP
(1415)
s*itLater:*ro)
.SP
*boMr. Cossitt:*ro Mr. Speaker, I rise
on a point of order to ask for
clarification by the parliamentary
secretary.
</bodyText>
<listItem confidence="0.974621166666667">
1. \SCM{} Document = 048 101 11002-108
script A \ECM0
Minister&apos;s residence at Harrington
Lake, Quebec?
66. \SCM{) Paragraph \ECM{}
81. \SCM{} Time = (1415) \ECM°
82. \SCM0 Time = Later \ECHO
83. \SCM{} Paragraph \ECM°
84. \SCM{} Author = Mr. Cossitt \ECM0
85. Mr. Speaker, I rise on a point of
order to ask for clarification by
the parliamentary secretary.
</listItem>
<figure confidence="0.92654912">
2. The House met at 2 p.m.
3. \SCM{} Paragraph \ECM{}
4. \SCM{} Author = Mr. Donald MacInnis
(Cape Breton-East Richmond) \ECM{}
5. Mr. Speaker, I rise on a question of
privilege affecting the rights and
prerogatives of parliamentary
committees and one which reflects on
the word of two ministers.
21. \SCM{I Paragraph \ECM{}
22. \SCM{} Author = Mr. Speaker \ECM{}
23. The hon. member&apos;s motion is proposed
to the House under the terms of
Standing Order 43.
44. Is there unanimous consent?
45. \SCM{} Paragraph \ECM{}
46. \SCM{} Author = Some hon. Members
\ECM{)
47. Agreed.
61. \SOU() Source = Text \ECK}
62. \SCM{} Question = 17 \ECM°
63. \SCM-(1 Author = Mr. Mazankowski
\ECHO
64. 1.
65. For the period April 1, 1973 to
</figure>
<figureCaption confidence="0.95782375">
January 31, 1974, what amount of
money was expended on the operation
and maintenance of the Prime
Figure 1: A sample of text before and after cleanup
</figureCaption>
<page confidence="0.989629">
171
</page>
<bodyText confidence="0.999768309523809">
a cost that favors exact matches and penalizes
omissions or garbled matches. Thus, for ex-
ample, we assign a cost of 0 to the pair Time
= Later and Time = Plus Tard, but a cost
of 10 to the pair Time = Later and Author
= Mr. Bateman. We set the cost of a dele-
tion at 5. For two names, we set the cost by
counting the number of insertions, deletions,
and substitutions necessary to transform one
name, letter by letter, into the other. This
value is then reduced to the range 0 to 10.
Given the costs described above, it is a
standard problem in dynamic programming
to find that alignment of the major anchors
in the two corpora with the least total cost
[Bellman, 1957]. In theory, the time and space
required to find this alignment grow as the
product of the lengths of the two sequences
to be aligned. In practice, however, by using
thresholds and the partial traceback technique
described by Brown, Spohrer, Hochschild, and
Baker , [Brown et al., 1982], the time required
can be made linear in the length of the se-
quences, and the space can be made constant.
Even so, the computational demand is severe
since, in places, the two corpora are out of
alignment by as many as 90,000 sentences ow-
ing to mislabelled or missing files.
This first pass renders the data as a se-
quence of sections between aligned major an-
chors. In the second pass, we accept or reject
each section in turn according to the popula-
tion of minor anchors that it contains. Specifi-
cally, we accept a section provided that, within
the section, both corpora contain the same
number of minor anchors in the same order.
Otherwise, we reject the section. Altogether,
we reject about 10% of the data in each cor-
pus. The minor anchors serve to divide the
remaining sections into subsections that range
in size from one sentence to several thousand
sentences and average about ten sentences.
</bodyText>
<sectionHeader confidence="0.998765" genericHeader="method">
ALIGNING SENTENCES AND
PARAGRAPH BOUNDARIES
</sectionHeader>
<bodyText confidence="0.998555">
We turn now to the question of aligning
the individual sentences in a subsection be-
tween minor anchors. Since the number of
</bodyText>
<figure confidence="0.853537">
English French
Source = English Source = Traduction
Source = Translation Source = Francais
Source = Text Source = Texte
Source = List Item Source = List Item
Source = Question Source = Question
Source = Answer Source = R.eponse
</figure>
<tableCaption confidence="0.969438">
Table 1: Examples of comments
</tableCaption>
<bodyText confidence="0.999065435897436">
sentences in the French corpus differs from the
number in the English corpus, it is clear that
they cannot be in one-to-one correspondence
throughout. Visual inspection of the two cor-
pora quickly reveals that although roughly 90%
of the English sentences correspond to single
French sentences, there are many instances
where a single sentence in one corpus is rep-
resented by two consecutive sentences in the
other. Rarer, but still present, are examples
of sentences that appear in one corpus but
leave no trace in the other. If one is moder-
ately well acquainted with both English and
French, it is a simple matter to decide how the
sentences should be aligned. Unfortunately,
the sizes of our corpora make it impractical
for us to obtain a complete set of alignments
by hand. Rather, we must necessarily employ
some automatic scheme.
It is not surprising and further inspection
verifies that the number of tokens in sentences
that are translations of one another are corre-
lated. We looked, therefore, at the possibility
of obtaining alignments solely on the basis of
sentence lengths in tokens. From this point of
view, each corpus is simply a sequence of sen-
tence lengths punctuated by occasional para-
graph markers. Figure 2 shows the initial por-
tion of such a pair of corpora. We have circled
groups of sentence lengths to show the cor-
rect alignment. We call each of the groupings
a bead. In this example, we have an el-bead
followed by an eff-bead followed by an e-bead
followed by a 1,5f-bead. An alignment, then,
is simply a sequence of beads that accounts
for the observed sequences of sentence lengths
and paragraph markers. We imagine the sen-
tences in a subsection to have been generated
by a pair of random processes, the first pro-
</bodyText>
<page confidence="0.985407">
172
</page>
<figure confidence="0.971364">
• • •
</figure>
<figureCaption confidence="0.999185">
Figure 2: Division of aligned corpora into beads
</figureCaption>
<table confidence="0.335546666666667">
Bead Text
one English sentence
one French sentence
ef one English and one French sentence
eel two English and one French sentence
elf one English and two French sentences
fe one English paragraph
If one French paragraph
leiff one English and one French paragraph
</table>
<tableCaption confidence="0.982577">
Table 2: Alignment Beads
</tableCaption>
<bodyText confidence="0.986343363636364">
during a sequence of beads and the second
choosing the lengths of the sentences in each
bead.
Figure 3 shows the two-state Markov model
that we use for generating beads. We assume
that a single sentence in one language lines up
with zero, one, or two sentences in the other
and that paragraph markers may be deleted.
Thus, we allow any of the eight beads shown in
Table 2. We also assume that Pt (e) = Pr (f),
Pr (eff ) = Pr (eef ), and Pr ([e) = Pr(f f).
</bodyText>
<sectionHeader confidence="0.461832" genericHeader="method">
BEAD
</sectionHeader>
<figureCaption confidence="0.998066">
Figure 3: Finite state model for generating beads
</figureCaption>
<bodyText confidence="0.9993508125">
Given a bead, we determine the lengths of
the sentences it contains as follows. We as-
sume the probability of an English sentence
of length te given an e-bead to be the same
as the probability of an English sentence of
length Ce in the text as a. whole. We denote
this probability by Nee). Similarly, we as-
sume the probability of a French sentence of
length ef given an f-bead to be Pr (e.f). For an
ef-bead, we assume that the English sentence
has length ee with probability Pr (se) and that
log of the ratio of length of the French sen-
tence to the length of the English sentence is
normally distributed with mean p and vari-
ance a2. Thus, if r = log(tf /te), we assume
that
</bodyText>
<equation confidence="0.945958">
Pr (if lee) = a exp[—(r p)2/(2a2)], (1)
</equation>
<bodyText confidence="0.999957777777778">
with a chosen so that the sum of Pr (If lie)
over positive values of tf is equal to unity. For
an eef-bead, we assume that each of the En-
glish sentences is drawn independently from
the distribution Pr (4) and that the log of
the ratio of the length of the French sentence
to the sum of the lengths of the English sen-
tences is normally distributed with the same
mean and variance as for an el-bead. Finally,
for an elf-bead, we assume that the length of
the English sentence is drawn from the distri-
bution Pr (is) and that the log of the ratio of
the sum of the lengths of the French sentences
to the length of the English sentence is nor-
mally distributed as &apos;before. Then, given the
sum of the lengths of the French sentences,
we assume that the probability of a particular
pair of lengths, if1 and ef2, is proportional to
Pr (ef,) Pr (e12).
Together, these two random processes form
a hidden Markov model [Baum, 1972] for the
generation of aligned pairs of corpora. We de-
termined the distributions, Pr (4) and Pr (ii),
front the relative frequencies of various sen-
tence lengths in our data. Figure 4 shows for
each language a histogram of these for sen-
tences with fewer than 81 tokens. Except for
lengths 2 and 4, which include a large num-
ber of formulaic sentences in both the French
and the English, the distributions are very
smooth.
For short sentences, the relative frequency
is a reliable estimate of the corresponding prob-
ability since for both French and English we
have more than 100 sentences of each length
less than 81. We estimated the probabilities
</bodyText>
<sectionHeader confidence="0.290137" genericHeader="method">
0 STOP
</sectionHeader>
<page confidence="0.87701">
173
</page>
<figure confidence="0.996649666666667">
sentence length
80
sentence length
</figure>
<figureCaption confidence="0.999344">
Figure 4: Histograms of French (top) and English (bottom) sentence lengths
</figureCaption>
<page confidence="0.99496">
174
</page>
<bodyText confidence="0.999233333333334">
of greater lengths by fitting the observed fre-
quencies of longer sentences to the tail of a
Poisson distribution.
We determined all of the other parameters
by applying the EM algorithm to a large sam-
ple of text [Baum, 1972, Dempster et al., 1977].
The resulting values are shown in Table 3.
From these parameters, we can see that 91%
of the English sentences and 98% of the En-
glish paragraph markers line up one-to-one
with their French counterparts. A random
variable z, the log of which is normally dis-
tributed with mean it and variance 0.2, has
mean value exp(ii + 0.2/2). We can also see,
therefore, that the total length of the French
text in an ef-, eef-, or eff-bead should be about
9.8% greater on average than the total length
of the corresponding English text. Since most
sentences belong to ef-beads, this is close to
the value of 9.5% given in Section 2 for the
amount by which the length of the average
French sentences exceeds that of the average
English sentence.
We can compute from the parameters in
Table 3 that the entropy of the bead produc-
tion process is 1.26 bits per sentence. Us-
ing the parameters it and 0.2, we can combine
the observed distribution of English sentence
lengths shown in Figure 4 with the conditional
distribution of French sentence lengths given
English sentence lengths in Equation (1) to
obtain the joint distribution of French and
English sentences lengths in ef-, eef-, and eff-
beads. From this joint distribution, we can
compute that the mutual information between
French and English sentence lengths in these
beads is 1.85 bits per sentence. We see there-
fore that, even in the absence of the anchor
points produced by the first two passes, the
correlation in sentence lengths is strong enough
to allow alignment with an error rate that
is asymptotically less than 100%. Hearten-
ing though such a result may be to the theo-
retician, this is a sufficiently coarse bound on
the error rate to warrant further study. Ac-
cordingly, we wrote a program to simulate the
alignment process that we had in mind. Using
Pr (e.,), Pr (ef), and the parameters from Ta-
</bodyText>
<table confidence="0.999534875">
Parameter Estimate
Pr(e),Pr(f) .007
Pr(ef) .690
Pr (eef), Pr (eff) .020
Pr (ie) , Pr (iff) .005
Pr (IMO .245
tt .072
0.2 .043
</table>
<tableCaption confidence="0.999663">
Table 3: Parameter estimates
</tableCaption>
<bodyText confidence="0.999970407407408">
ble 3, we generated an artificial pair of aligned
corpora. We then determined the most prob-
able alignment for the data. We recorded
the fraction of ef-beads in the most probable
alignment that did not correspond to ef-beads
in the true alignment as the error rate for the
process. We repeated this process many thou-
sands of times and found that we could ex-
pect an error rate of about 0.9% given the
frequency of anchor points from the first two
Passes.
By varying the parameters of the hidden
Markov model, we explored the effect of an-
chor points and paragraph markers on the ac-
curacy of alignment. We found that with para-
graph markers but no anchor points, we could
expect an error rate of 2.0%, with anchor points
but no paragraph markers, we could expect an
error rate of 2.3%, and with neither anchor
points nor paragraph markers, we could ex-
pect an error rate of 3.2%. Thus, while anchor
points and paragraph markers are important,
alignment is still feasible without them. This
is promising since it suggests that one may
be able to apply the same technique to data
where frequent anchor points are not avail-
able.
</bodyText>
<sectionHeader confidence="0.999427" genericHeader="conclusions">
RESULTS
</sectionHeader>
<bodyText confidence="0.999747375">
We applied the alignment algorithm of Sec-
tions 3 and 4 to the Canadian Hansard data
described in Section 2. The job ran for 10
days on an IBM Model 3090 mainframe un-
der an operating system that permitted ac-
cess to 16 megabytes of virtual memory. The
most probable alignment contained 2,869,041
ef-beads. Some of our colleagues helped us
</bodyText>
<page confidence="0.995899">
175
</page>
<bodyText confidence="0.987083416666667">
And love and kisses to you, too.
mugwumps who sit on the fence with
their mugs on one side and their
wumps on the other side and do not
know which side to come down on.
At first reading, she may have.
Pareillement.
... en voula.ut 1n6ita.ger la chèvre et le choux
Hs n&apos;arrivent pas a. prendre parti.
Elle semble en Orel, avoir un grief tout a
fait valable, du moms au premier
abord.
</bodyText>
<tableCaption confidence="0.984522">
Table 4: Unusual but correct alignments
</tableCaption>
<bodyText confidence="0.999753875">
examine a random sample of 1000 of these
beads, and we found 6 in which sentences were
not translations of one another. This is con-
sistent with the expected error rate of 0.9%
mentioned above. In some cases, the algo-
rithm correctly aligns sentences with very dif-
ferent lengths. Table 4 shows some interesting
examples of this.
</bodyText>
<sectionHeader confidence="0.998927" genericHeader="references">
REFERENCES
</sectionHeader>
<bodyText confidence="0.8325002">
[Baum, 1972] Baum, L. (1972). An inequality
and associated maximization technique in
statistical estimation of probabilistic func-
tions of a Markov process. Inequalities, 3:1-
8.
</bodyText>
<reference confidence="0.997860849056604">
[Bellman, 1957] Bellman, R. (1957). Dy-
namic Programming. Princeton University
Press, Princeton N.J.
[Brown et at., 1982] Brown, P., Spolirer, J.,
Hochschild, P., and Baker, J. (1982). Par-
tial traceback and dynamic programming.
In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal
Processing, pages 1629-1632, Paris, France.
[Brown et al., 1990] Brown, P. F., Cocke, J.,
DellaPietra, S. A., DellaPietra., V. J., je-
linek, F., Lafferty, J. D., Mercer, R. L.,
and Roossin, P. S. (1990). A statistical ap-
proach to machine translation. Computa-
tional Linguistics, 16(2):79-85.
[Brown et al., 19881 Brown, P. F., Cocke, J.,
DellaPietra, S. A., DellaPietra., V. J., le-
linek, F., Mercer, R. L., and Roossin, P. S.
(1988). A statistical approach to language
translation. In Proceedings of the 12th In-
ternational. Conference on Computational
Linguistics, Budapest, Hungary.
[Catizone et al., 1989] Catizorte, R., Russell,
G., and Warwick, S. (1989). Deriving trans-
lation data from bilingual texts. In Proceed-
ings of the First International Acquisition
Workshop, Detroit, Michigan.
[Dempster et al., 1977] Dempster, A., Laird,
N., and Rubin, D. (1977). Maximum likeli-
hood from incomplete data via the EM al-
gorithm. Journal of the Royal Statistical
Society, 39(B):1-38.
[Gale and Church, 19911 Gale, W. A. and
Church, K. W. (1991). A program for align-
ing sentences in bilingual corpora. In Pro-
ceedings of the 29th Annual Meeting of the
Association for Computational Linguistics,
Berkeley, California.
[Kay, 1991] Kay, M. (1991). Text-translation
alignment. In ACH/ALLC &apos;91: &amp;quot;Mak-
ing Connections&amp;quot; Conference Handbook,
Tempe, Arizona.
[Kla.va.ns and Tzoukermann, 1990]
Klavans, J. and Tzoukermann, E. (1990).
The bicord system. In COLING-90, pages
174-179, Helsinki, Finland.
[Sadler, 1989] Sadler, V. (1989). The Bilin-
gual Knowledge Bank - A New Conceptual
Basis for MT. BSO/Research, Utrecht.
[Warwick and Russell, 1990] Warwick, S. and
Russell, G. (1990). Bilingual concordancing
and bilingual lexicography. In EURALEX
.4th International Congress, Milaga., Spain.
</reference>
<page confidence="0.998746">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.978608">
<title confidence="0.99959">ALIGNING SENTENCES IN PARALLEL CORPORA</title>
<author confidence="0.999975">Peter F Brown</author>
<author confidence="0.999975">Jennifer C Lai</author>
<author confidence="0.999975">Robert L Mercer</author>
<affiliation confidence="0.999641">IBM Thomas J. Watson Research Center</affiliation>
<address confidence="0.9924985">P.O. Box 704 Yorktown Heights, NY 10598</address>
<abstract confidence="0.999736818181818">In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Dynamic Programming.</title>
<date>1957</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton N.J.</location>
<marker>1957</marker>
<rawString>[Bellman, 1957] Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton N.J.</rawString>
</citation>
<citation valid="true">
<title>Partial traceback and dynamic programming.</title>
<date>1982</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>1629--1632</pages>
<location>Paris, France.</location>
<marker>1982</marker>
<rawString>[Brown et at., 1982] Brown, P., Spolirer, J., Hochschild, P., and Baker, J. (1982). Partial traceback and dynamic programming. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 1629-1632, Paris, France.</rawString>
</citation>
<citation valid="true">
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<pages>16--2</pages>
<marker>1990</marker>
<rawString>[Brown et al., 1990] Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra., V. J., jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. (1990). A statistical approach to machine translation. Computational Linguistics, 16(2):79-85.</rawString>
</citation>
<citation valid="true">
<title>A statistical approach to language translation.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International. Conference on Computational Linguistics,</booktitle>
<location>Budapest, Hungary.</location>
<marker>1988</marker>
<rawString>[Brown et al., 19881 Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra., V. J., lelinek, F., Mercer, R. L., and Roossin, P. S. (1988). A statistical approach to language translation. In Proceedings of the 12th International. Conference on Computational Linguistics, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<title>Deriving translation data from bilingual texts.</title>
<date>1989</date>
<booktitle>In Proceedings of the First International Acquisition Workshop,</booktitle>
<location>Detroit, Michigan.</location>
<marker>1989</marker>
<rawString>[Catizone et al., 1989] Catizorte, R., Russell, G., and Warwick, S. (1989). Deriving translation data from bilingual texts. In Proceedings of the First International Acquisition Workshop, Detroit, Michigan.</rawString>
</citation>
<citation valid="true">
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<pages>39--1</pages>
<marker>1977</marker>
<rawString>[Dempster et al., 1977] Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(B):1-38.</rawString>
</citation>
<citation valid="true">
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Berkeley, California.</location>
<marker>1991</marker>
<rawString>[Gale and Church, 19911 Gale, W. A. and Church, K. W. (1991). A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<title>Text-translation alignment.</title>
<date>1991</date>
<booktitle>In ACH/ALLC &apos;91: &amp;quot;Making Connections&amp;quot; Conference Handbook,</booktitle>
<location>Tempe, Arizona.</location>
<marker>1991</marker>
<rawString>[Kay, 1991] Kay, M. (1991). Text-translation alignment. In ACH/ALLC &apos;91: &amp;quot;Making Connections&amp;quot; Conference Handbook, Tempe, Arizona.</rawString>
</citation>
<citation valid="false">
<date>1990</date>
<marker>1990</marker>
<rawString>[Kla.va.ns and Tzoukermann, 1990]</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>The bicord system.</title>
<date>1990</date>
<booktitle>In COLING-90,</booktitle>
<pages>174--179</pages>
<location>Helsinki, Finland.</location>
<marker>Klavans, Tzoukermann, 1990</marker>
<rawString>Klavans, J. and Tzoukermann, E. (1990). The bicord system. In COLING-90, pages 174-179, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<title>The Bilingual Knowledge Bank - A New Conceptual Basis for MT. BSO/Research,</title>
<date>1989</date>
<location>Utrecht.</location>
<marker>1989</marker>
<rawString>[Sadler, 1989] Sadler, V. (1989). The Bilingual Knowledge Bank - A New Conceptual Basis for MT. BSO/Research, Utrecht.</rawString>
</citation>
<citation valid="true">
<title>Bilingual concordancing and bilingual lexicography.</title>
<date>1990</date>
<booktitle>In EURALEX .4th International Congress,</booktitle>
<location>Milaga.,</location>
<marker>1990</marker>
<rawString>[Warwick and Russell, 1990] Warwick, S. and Russell, G. (1990). Bilingual concordancing and bilingual lexicography. In EURALEX .4th International Congress, Milaga., Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>