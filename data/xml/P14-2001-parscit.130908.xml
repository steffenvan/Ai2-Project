<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000128">
<title confidence="0.998591">
Exploring the Relative Role of Bottom-up and Top-down Information in
Phoneme Learning
</title>
<author confidence="0.941383">
Abdellah Fourtassi1 , Thomas Schatz1,2 , Balakrishnan Varadarajan3 , Emmanuel Dupoux1
</author>
<affiliation confidence="0.336014">
1 Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris, France
2 SIERRA Project-Team, INRIA/ENS/CNRS, Paris, France
3 Center for Language and Speech Processing, JHU, Baltimore, USA
</affiliation>
<email confidence="0.6544785">
{abdellah.fourtassi; emmanuel.dupoux; balaji.iitm1}@gmail
thomas.schatz@laposte.net
</email>
<sectionHeader confidence="0.997138" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966461538462">
We test both bottom-up and top-down ap-
proaches in learning the phonemic status
of the sounds of English and Japanese. We
used large corpora of spontaneous speech
to provide the learner with an input that
models both the linguistic properties and
statistical regularities of each language.
We found both approaches to help dis-
criminate between allophonic and phone-
mic contrasts with a high degree of accu-
racy, although top-down cues proved to be
effective only on an interesting subset of
the data.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966033333333">
Developmental studies have shown that, during
their first year, infants tune in on the phonemic cat-
egories (consonants and vowels) of their language,
i.e., they lose the ability to distinguish some
within-category contrasts (Werker and Tees, 1984)
and enhance their ability to distinguish between-
category contrasts (Kuhl et al., 2006). Current
work in early language acquisition has proposed
two competing hypotheses that purport to account
for the acquisition of phonemes. The bottom-up
hypothesis holds that infants converge on the lin-
guistic units of their language through a similarity-
based distributional analysis of their input (Maye
et al., 2002; Vallabha et al., 2007). In contrast,
the top-down hypothesis emphasizes the role of
higher level linguistic structures in order to learn
the lower level units (Feldman et al., 2013; Mar-
tin et al., 2013). The aim of the present work is
to explore how much information can ideally be
derived from both hypotheses.
The paper is organized as follows. First we de-
scribe how we modeled phonetic variation from
audio recordings, second we introduce a bottom-
up cue based on acoustic similarity and top-
down cues based of the properties of the lexicon.
We test their performance in a task that consists
in discriminating within-category contrasts from
between-category contrasts. Finally we discuss
the role and scope of each cue for the acquisition
of phonemes.
</bodyText>
<sectionHeader confidence="0.943594" genericHeader="method">
2 Modeling phonetic variation
</sectionHeader>
<bodyText confidence="0.970917090909091">
In this section, we describe how we modeled the
representation of speech sounds putatively pro-
cessed by infants, before they learn the relevant
phonemic categories of their language. Following
Peperkamp et al. (2006), we make the assumption
that this input is quantized into context-dependent
phone-sized unit we call allophones. Consider the
example of the allophonic rule that applies to the
French /r/:
� [X] / before a voiceless obstruent
/r/ →[x] elsewhere
</bodyText>
<figureCaption confidence="0.999286">
Figure 1: Allophonic variation of French /r/
</figureCaption>
<bodyText confidence="0.999648388888889">
The phoneme /r/ surfaces as voiced ([x]) before
a voiced obstruent like in [kanax Son] (“canard
jaune”, yellow duck) and as voiceless ([X]) before
a voiceless obstruent as in [kanaX puXpx] (“ca-
nard pourpre”, purple duck). Assuming speech
sounds are coded as allophones, the challenge fac-
ing the learner is to distinguish the allophonic vari-
ation ([x], [X]) from the phonemic variation (re-
lated to a difference in the meaning) like the con-
trast ([x],[l]).
Previous work has generated allophonic varia-
tion using random contexts (Martin et al., 2013).
This procedure does not take into account the fact
that contexts belong to natural classes. In addition,
it does not enable to compute an acoustic distance.
Here, we generate linguistically and acoustically
controlled allophones using Hidden Markov Mod-
els (HMMs) trained on audio recordings.
</bodyText>
<page confidence="0.771105">
1
</page>
<bodyText confidence="0.248434">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.977064">
2.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999979">
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al., 2007), which consists of 40
hours of spontaneous conversations with 40 speak-
ers of American English, and the core of the Cor-
pus of Spontaneous Japanese (Maekawa et al.,
2000) which also consists of about 40 hours of
recorded spontaneous conversations and public
speeches in different fields. Both corpora are time-
aligned with phonetic labels. Following Boruta
(2012), we relabeled the japanese corpus using 25
phonemes. For English, we used the phonemic
version which consists of 45 phonemes.
</bodyText>
<subsectionHeader confidence="0.8830585">
2.2 Input generation
2.2.1 HMM-based allophones
</subsectionHeader>
<bodyText confidence="0.999976911764706">
In order to generate linguistically and acoustically
plausible allophones, we apply a standard Hidden
Markov Model (HMM) phoneme recognizer with
a three-state per phone architecture to the signal,
as follows.
First, we convert the raw speech waveform of
the corpora into successive vectors of Mel Fre-
quency Cepstrum Coefficients (MFCC), computed
over 25 ms windows, using a period of 10 ms
(the windows overlap). We use 12 MFCC coeffi-
cients, plus the energy, plus the first and second or-
der derivatives, yielding 39 dimensions per frame.
Second, we start HMM training using one three-
state model per phoneme. Third, each phoneme
model is cloned into context-dependent triphone
models, for each context in which the phoneme
actually occurs (for example, the phoneme /a/ oc-
curs in the context [d–a–g] as in the word /dag/
(“dog”). The triphone models are then retrained on
only the relevant subset of the data, corresponding
to the given triphone context. These detailed mod-
els are clustered back into inventories of various
sizes (from 2 to 20 times the size of the phone-
mic inventory) using a linguistic feature-based de-
cision tree, and the HMM states of linguistically
similar triphones are tied together so as to max-
imize the likelihood of the data. Finally, the tri-
phone models are trained again while the initial
gaussian emission models are replaced by mix-
ture of gaussians with a progressively increasing
number of components, until each HMM state is
modeled by a mixture of 17 diagonal-covariance
gaussians. The HMM were built using the HMM
Toolkit (HTK: Young et al., 2006).
</bodyText>
<subsubsectionHeader confidence="0.535444">
2.2.2 Random allophones
</subsubsectionHeader>
<bodyText confidence="0.999949444444445">
As a control, we also reproduce the random al-
lophones of Martin et al. (2013), in which allo-
phonic contexts are determined randomly: for a
given phoneme /p/, the set of all possible con-
texts is randomly partitioned into a fixed number
n of subsets. In the transcription, the phoneme /p/
is converted into one of its allophones (p1,p2,..,pn)
depending on the subset to which the current con-
text belongs.
</bodyText>
<sectionHeader confidence="0.914002" genericHeader="method">
3 Bottom-up and top-down hypotheses
</sectionHeader>
<subsectionHeader confidence="0.999391">
3.1 Acoustic cue
</subsectionHeader>
<bodyText confidence="0.999973571428572">
The bottom-up cue is based on the hypothesis that
instances of the same phoneme are likely to be
acoustically more similar than instances of two
different phonemes (see Cristia and Seidl, in press)
for a similar proposition). In order to provide
a proxy for the perceptual distance between al-
lophones, we measure the information theoretic
distance between the acoustic HMMs of these al-
lophones. The 3-state HMMs of the two allo-
phones were aligned with Dynamic Time Warping
(DTW), using as a distance between pairs of emit-
ting states, a symmetrized version of the Kullback-
Leibler (KL) divergence measure (each state was
approximated by a single non-diagonal Gaussian):
</bodyText>
<equation confidence="0.881251666666667">
A(x, y) = KL(Nxz||Ny;) + KL(Ny;||Nxz)
�
(i,j)∈DTW (x,y)
</equation>
<bodyText confidence="0.999723142857143">
Where {(i, j) E DTW (x, y)} is the set of in-
dex pairs over the HMM states that correspond to
the optimal DTW path in the comparison between
phone model x and y, and Nxz the full covariance
Gaussian distribution for state i of phone x. For
obvious reasons, the acoustic distance cue cannot
be computed for Random allophones.
</bodyText>
<subsectionHeader confidence="0.999891">
3.2 Lexical cues
</subsectionHeader>
<bodyText confidence="0.9999895">
The top-down information we use in this study, is
based on the insight of Martin et al. (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes.
In fact, the latter creates variants (alternants) of the
same lexical item since adjacent sounds condition
the realization of the first and final phoneme. For
example, as shown in figure 1, the phoneme /r/ sur-
faces as [x] or [x] depending on whether or not the
</bodyText>
<page confidence="0.962476">
2
</page>
<bodyText confidence="0.9999555">
next sound is a voiceless obstruent. Therefore, the
lexical item /kanar/ surfaces as [kanaX] or [kanair].
The lexical cue assumes that a pair of words dif-
fering in the first or last segment (like [kanaX] and
[kanair]) is more likely to be the result of a phono-
logical process triggered by adjacent sounds, than
a true semantic minimal pair.
However, this strategy clearly gives rise to false
alarms in the (albeit relatively rare) case of true
minimal pairs like [kanaX] (“duck”) and [kanal]
(“canal”), where ([X], [l]) will be mistakenly la-
beled as allophonic.
In order to mitigate the problem of false alarms,
we also use Boruta (2011)’s continuous version,
where each pair of phones is characterized by the
number of lexical minimal pairs it forms.
</bodyText>
<equation confidence="0.584168">
B(x, y) _ |(Ax, Ay) ∈ L2 |+ |(xA, yA) ∈ L2|
</equation>
<bodyText confidence="0.999195333333333">
where {Ax ∈ L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax, Ay) ∈
L2} is the set of phonological minimal pairs in
L × L that vary on the final segment.
In addition, we introduce another cue that could
be seen as a normalization of Boruta’s cue:
</bodyText>
<equation confidence="0.9940315">
|(Ax,Ay)∈L2|+|(xA,yA)∈L2|
N (x, y) _ |{Ax∈L}|+|{Ay∈L}|+|{xA∈L}|+|{yA∈L}|
</equation>
<sectionHeader confidence="0.998995" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.896506">
4.1 Task
</subsectionHeader>
<bodyText confidence="0.99962575">
For each corpus we list all the possible pairs of
attested allophones. Some of these pairs are allo-
phones of the same phoneme (allophonic pair) and
others are allophones of different phonemes (non-
allophonic pairs). The task is a same-different
classification, whereby each of these pairs is given
a score from the cue that is being tested. A good
cue gives higher scores to allophonic pairs.
</bodyText>
<subsectionHeader confidence="0.919301">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999991277777778">
We use the same evaluation procedure as in Mar-
tin et al. (2013). It is carried out by computing
the area under the curve of the Receiver Operat-
ing Characteristic (ROC). A value of 0.5 repre-
sents chance and a value of 1 represents perfect
performance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks (of 20 utterances each). In each run, we
draw randomly with replacement from this set of
blocks a sample of the same size as the original
corpus. This sample is then used to retrain the
acoustic models and generate a phonetic inven-
tory that we use to re-transcribe the corpus and
re-compute the cues. We report scores averaged
over 5 such runs.
</bodyText>
<subsectionHeader confidence="0.779455">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.971395125">
Table 1 shows the classification scores for the lex-
ical cues when we vary the inventory size from
2 allophones per phoneme in average, to 20 al-
lophones per phoneme, using the Random allo-
phones. The top-down scores are very high, repli-
cating Martin et al.’s results, and even improving
the performance using Boruta’s cue and our new
Normalized cue.
</bodyText>
<table confidence="0.950038833333333">
English Japanese
Allo./phon. M B N M B N
2 0.784 0.935 0.951 0.580 0.989 1.00
5 0.845 0.974 0.982 0.653 0.978 0.991
10 0.886 0.974 0.981 0.733 0.944 0.971
20 0.918 0.961 0.966 0.785 0.869 0.886
</table>
<tableCaption confidence="0.979535">
Table 1 : Same-different scores for top-down cues on
Random allophones, as a function of the average number of
allophones per phoneme. M=Martin et al., B=Boruta, N=
Normalized
</tableCaption>
<bodyText confidence="0.998330888888889">
Table 2 shows the results for HMM-based allo-
phones. The acoustic score is very accurate for
both languages and is quite robust to variation.
Top-down cues, on the other hand, perform, sur-
prisingly, almost at chance level in distinguish-
ing between allophonic and non-allophonic pairs.
A similar discrepancy for the case of Japanese
was actually noted, but not explained, in Boruta
(2012).
</bodyText>
<table confidence="0.997727333333333">
English Japanese
Allo./phon. A M B N A M B N
2 0.916 0.592 0.632 0.643 0.885 0.422 0.524 0.537
5 0.918 0.592 0.607 0.611 0.908 0.507 0.542 0.551
10 0.893 0.569 0.571 0.571 0.827 0.533 0.546 0.548
20 0.879 0.560 0.560 0.559 0.876 0.541 0.543 0.543
</table>
<tableCaption confidence="0.992497">
Table 2 : Same-different scores for bottom-up and top-down
cues on HMM-based allophones, as a function of the
average number of allophones per phoneme. A=Acoustic,
M=Martin et al., B=Boruta, N= Normalized
</tableCaption>
<sectionHeader confidence="0.994118" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.85383925">
5.1 Why does the performance drop for
realistic allophones?
When we list all possible pairs of allophones in
the inventory, some of them correspond to lexi-
</bodyText>
<page confidence="0.997391">
3
</page>
<bodyText confidence="0.9996522">
cal alternants ([X], [K]) → ([kanaX] and [kanaK]),
others to true minimal pairs ([K], [l]) → ([kanaK]
and [kanal]), and yet others will simply not gen-
erate lexical variation at all, we will call those:
invisible pairs. For instance, in English, /h/ and
/N/ occur in different syllable positions and thus
cannot appear in any minimal pair. As defined
above, top-down cues are set to 0 in such pairs
(which means that they are systematically classi-
fied as non-allophonic). This is a correct decision
for /h/ vs. /N/, but not for invisible pairs that also
happen to be allophonic, resulting in false nega-
tives. In tables 3, we show that, indeed, invisible
pairs is a major issue, and could explain to a large
extent the pattern of results found above. In fact,
the proportion of visible allophonic pairs (“allo”
column) is way lower for HMM-based allophones.
This means that the majority of allophonic pairs in
the HMM case are invisible, and therefore, will be
mistakenly classified as non-allophonic.
</bodyText>
<table confidence="0.999172428571429">
Random Japanese English BMM
English Japanese
Allo./phon. allo ¬ allo allo ¬ allo allo ¬ allo allo ¬ allo
2 92.9 36.3 100 83.9 48.9 25.3 37.1 53.2
5 97.2 28.4 99.6 69.0 31.1 14.3 25.0 25.9
10 96.8 19.9 96.7 50.1 19.8 4.23 21.0 14.4
20 94.3 10.8 83.4 26.4 14.0 1.89 12.4 4.04
</table>
<tableCaption confidence="0.998042333333333">
Table 3 : Proportion (in %) of allophonic pairs (allo), and
non-allophonic pairs (¬ allo) associated with at least one
lexical minimal pair, in Random and HMM allophones.
</tableCaption>
<bodyText confidence="0.999464818181818">
There are basically two reasons why an allo-
phonic pair would be invisible ( will not generate
lexical alternants). The first one is the absence of
evidence, e.g., if the edges of the word with the
underlying phoneme do not appear in enough con-
texts to generate the corresponding variants. This
happens when the corpus is so small that no word
ending with, say, /r/ appears in both voiced and
voiceless contexts. The second, is when the allo-
phones are triggered on maximally different con-
texts (on the right and the left) as illustrated below:
</bodyText>
<equation confidence="0.626565">
/p/ → f [p1] / A B
l [p2] / C D
</equation>
<bodyText confidence="0.999900708333333">
When A doesn’t overlap with C and B does not
overlap with D, it becomes impossible for the pair
([p1], [p2]) to generate a lexical minimal pair. This
is simply because a pair of allophones needs to
share at least one context to be able to form vari-
ants of a word (the second or penultimate segment
of this word).
When asked to split the set of contexts in two
distinct categories that trigger [p1] and [p2] (i.e.,
A B and C D), the random procedure will of-
ten make A overlap with B and C overlap with D
because it is completely oblivious to any acous-
tic or linguistic similarity, thus making it always
possible for the pair of allophones to generate lex-
ical alternants. A more realistic categorization
(like the HMM-based one), will naturally tend to
minimize within-category distance, and maximize
between-category distance. Therefore, we will
have less overlap, making the chances of the pair
to generate a lexical pair smaller. The more al-
lophones we have, the bigger is the chance to end
up with non-overlapping categories (invisible allo-
phonic pairs), and the more mistakes will be made,
as shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.999019">
5.2 Restricting the role of top-down cues
</subsectionHeader>
<bodyText confidence="0.9999693125">
The analysis above shows that top-down cues can-
not be used to classify all contrasts. The approxi-
mation that consists in considering all pairs that do
not generate lexical pairs as non-allophonic, does
not scale up to realistic input. A more intuitive,
but less ambitious, assumption is to restrict the
scope of top-down cues to contrasts that do gen-
erate lexical variation (lexical alternants or true
minimal pairs). Thus, they remain completely ag-
nostic to the status of invisible pairs. This restric-
tion makes sense since top-down information boils
down to knowing whether two word forms belong
to the same lexical category (reducing variation to
allophony), or to two different categories (varia-
tion is then considered non-allophonic). Phonetic
variation that does not cause lexical variation is, in
this particular sense, orthogonal to our knowledge
about the lexicon.
We test this hypothesis by applying the cues
only to the subset of pairs that are associated with
at least one lexical minimal pair. We vary the num-
ber of allophones per phoneme on the one hand
(Table 4) and the size of the input on the other
hand (Table 5). We refer to this subset by an aster-
isk (*), by which we also mark the cues that apply
to it. Notice that, in this new framing, the M cue is
completely uninformative since it assigns the same
value to all pairs.
As predicted, the cues perform very well on this
subset, especially the N cue. The combination of
top-down and bottom-up cues shows that the for-
mer is always useful, and that these two sources of
</bodyText>
<page confidence="0.995481">
4
</page>
<table confidence="0.999038142857143">
English Japanese
Individual cues Combination Individual cues Combination
Allo./phon. * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
2 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
4 14.3 0.918 0.964 0.858 0.951 0.975 0.991 30.88 0.908 0.917 0.850 0.936 0.934 0.976
10 4.24 0.893 0.937 0.813 0.939 0.960 0.968 16.06 0.827 0.839 0.899 0.957 0.904 0.936
20 1.67 0.879 0.907 0.802 0.907 0.942 0.940 5.02 0.876 0.856 0.882 0.959 0.913 0.950
</table>
<tableCaption confidence="0.955591">
Table 4 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of average
number of allophones per phonemes.
</tableCaption>
<table confidence="0.999679888888889">
English Japanese
Individual cues Combination Individual cues Combination
Size (hours) * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
1 9.87 0.885 0.907 0.741 0.915 0.927 0.969 34.78 0.890 0.883 0.835 0.915 0.889 0.934
4 18.3 0.918 0.958 0.798 0.917 0.967 0.989 48.00 0.917 0.939 0.860 0.937 0.938 0.973
8 21.3 0.916 0.964 0.837 0.942 0.971 0.992 51.71 0.915 0.940 0.889 0.937 0.954 0.977
20 24.4 0.911 0.960 0.827 0.936 0.969 0.994 58.12 0.921 0.954 0.865 0.912 0.945 0.971
40 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
∞ 34.82 72.16
</table>
<tableCaption confidence="0.989022">
Table 5 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of corpus size.
* (%) refers to the proportion of the subset of contrasts associated with at least one minimal pair. The cues applied to this
subset are marked with an asterisk (*)
</tableCaption>
<bodyText confidence="0.999881642857143">
information are not completely redundant. How-
ever, the scope of top-down cues (the proportion of
the subset * ) shrinks as we increase the number of
allophones. Table 5 shows that this problem can,
in principle, be mitigated by increasing the amount
of data available to the learner. As we were limited
to only 40 hours of speech, we generated an artifi-
cial corpus that uses the same lexicon but with all
possible word orders so as to maximize the num-
ber of contexts in which words appear. This artifi-
cial corpus increases the proportion of the subset,
but we are still not at 100 % coverage, which ac-
cording the analysis above, is due (at least in part)
to the irreducible set of non-overlapping pairs.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99996694117647">
In this study we explored the role of both bottom-
up and top-down hypotheses in learning the
phonemic status of the sounds of two typologically
different languages. We introduced a bottom-up
cue based on acoustic similarity, and we used al-
ready existing top-down cues to which we pro-
vided a new extension. We tested these hypothe-
ses on English and Japanese, providing the learner
with an input that mirrors closely the linguistic
and acoustic properties of each language. We
showed, on the one hand, that the bottom-up cue is
a very reliable source of information, across differ-
ent levels of variation and even with small amount
of data. Top-down cues, on the other hand, were
found to be effective only on a subset of the data,
which corresponds to the interesting contrasts that
cause lexical variation. Their role becomes more
relevant as the learner gets more linguistic experi-
ence, and their combination with bottom-up cues
shows that they can provide non-redundant infor-
mation. Note, finally, that even if this work is
based on a more realistic input compared to previ-
ous studies, it still uses simplifying assumptions,
like ideal word segmentation, and no low-level
acoustic variability. Those assumptions are, how-
ever, useful in quantifying the information that can
ideally be extracted from the input, which is a nec-
essary preliminary step before modeling how this
input is used in a cognitively plausible way. Inter-
ested readers may refer to (Fourtassi and Dupoux,
2014; Fourtassi et al., 2014) for a more learning-
oriented approach, where some of the assumptions
made here about high level representations are re-
laxed.
</bodyText>
<sectionHeader confidence="0.996976" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997454">
This project is funded in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R´egion Ile de France (DIM cerveau et pens´ee). We
thank Luc Boruta, Sanjeev Khudanpur, Isabelle
Dautriche, Sharon Peperkamp and Benoit Crabb´e
for highly useful discussions and contributions.
</bodyText>
<page confidence="0.981183">
5
</page>
<note confidence="0.657058666666667">
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
</note>
<sectionHeader confidence="0.67716" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.969953230769231">
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88–93.
Luc Boruta. 2012. Indicateurs d’allophonie et
de phon´emicit´e. Doctoral dissertation, Universit´e
Paris-Diderot - Paris VII.
A. Cristia and A. Seidl. In press. The hyperarticula-
tion hypothesis of infant-directed speech. Journal
of Child Language.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. 2013. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review, 120(4):751–778.
Abdellah Fourtassi and Emmanuel Dupoux. 2014. A
rudimentary lexicon and semantics help bootstrap
phoneme acquisition. In Proceedings of the 18th
Conference on Computational Natural Language
Learning (CoNLL).
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th Annual Meeting of the Cognitive Science
Society.
Patricia K. Kuhl, Erica Stevens, Akiko Hayashi,
Toshisada Deguchi, Shigeru Kiritani, and Paul Iver-
son. 2006. Infants show a facilitation effect for na-
tive language phonetic perception between 6 and 12
months. Developmental Science, 9(2):F13–F21.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947–952, Athens, Greece.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103–124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101–B111.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre
Nadal, and Emmanuel Dupoux. 2006. The acqui-
sition of allophonic rules: Statistical learning with
linguistic constraints. Cognition, 101(3):B31–B41.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker,
and S. Amano. 2007. Unsupervised learning
of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 – 63.
</reference>
<page confidence="0.998778">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370234">
<title confidence="0.946976">Exploring the Relative Role of Bottom-up and Top-down Information Phoneme Learning</title>
<author confidence="0.457099">Balakrishnan</author>
<affiliation confidence="0.68191">1Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris,</affiliation>
<address confidence="0.971383">2SIERRA Project-Team, INRIA/ENS/CNRS, Paris, 3Center for Language and Speech Processing, JHU, Baltimore,</address>
<email confidence="0.971429">emmanuel.dupoux;thomas.schatz@laposte.net</email>
<abstract confidence="0.999348">We test both bottom-up and top-down approaches in learning the phonemic status of the sounds of English and Japanese. We used large corpora of spontaneous speech to provide the learner with an input that models both the linguistic properties and statistical regularities of each language. We found both approaches to help discriminate between allophonic and phonemic contrasts with a high degree of accuracy, although top-down cues proved to be effective only on an interesting subset of the data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luc Boruta</author>
</authors>
<title>Combining Indicators of Allophony.</title>
<date>2011</date>
<booktitle>In Proceedings ACL-SRW,</booktitle>
<pages>88--93</pages>
<contexts>
<context position="8883" citStr="Boruta (2011)" startWordPosition="1421" endWordPosition="1422">s obstruent. Therefore, the lexical item /kanar/ surfaces as [kanaX] or [kanair]. The lexical cue assumes that a pair of words differing in the first or last segment (like [kanaX] and [kanair]) is more likely to be the result of a phonological process triggered by adjacent sounds, than a true semantic minimal pair. However, this strategy clearly gives rise to false alarms in the (albeit relatively rare) case of true minimal pairs like [kanaX] (“duck”) and [kanal] (“canal”), where ([X], [l]) will be mistakenly labeled as allophonic. In order to mitigate the problem of false alarms, we also use Boruta (2011)’s continuous version, where each pair of phones is characterized by the number of lexical minimal pairs it forms. B(x, y) _ |(Ax, Ay) ∈ L2 |+ |(xA, yA) ∈ L2| where {Ax ∈ L} is the set of words in the lexicon L that end in the phone x, and {(Ax, Ay) ∈ L2} is the set of phonological minimal pairs in L × L that vary on the final segment. In addition, we introduce another cue that could be seen as a normalization of Boruta’s cue: |(Ax,Ay)∈L2|+|(xA,yA)∈L2| N (x, y) _ |{Ax∈L}|+|{Ay∈L}|+|{xA∈L}|+|{yA∈L}| 4 Experiment 4.1 Task For each corpus we list all the possible pairs of attested allophones. Som</context>
</contexts>
<marker>Boruta, 2011</marker>
<rawString>Luc Boruta. 2011. Combining Indicators of Allophony. In Proceedings ACL-SRW, pages 88–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luc Boruta</author>
</authors>
<title>Indicateurs d’allophonie et de phon´emicit´e. Doctoral dissertation, Universit´e Paris-Diderot -</title>
<date>2012</date>
<location>Paris VII.</location>
<contexts>
<context position="4425" citStr="Boruta (2012)" startWordPosition="674" endWordPosition="675">of the Association for Computational Linguistics (Short Papers), pages 1–6, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Corpora We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which consists of 40 hours of spontaneous conversations with 40 speakers of American English, and the core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000) which also consists of about 40 hours of recorded spontaneous conversations and public speeches in different fields. Both corpora are timealigned with phonetic labels. Following Boruta (2012), we relabeled the japanese corpus using 25 phonemes. For English, we used the phonemic version which consists of 45 phonemes. 2.2 Input generation 2.2.1 HMM-based allophones In order to generate linguistically and acoustically plausible allophones, we apply a standard Hidden Markov Model (HMM) phoneme recognizer with a three-state per phone architecture to the signal, as follows. First, we convert the raw speech waveform of the corpora into successive vectors of Mel Frequency Cepstrum Coefficients (MFCC), computed over 25 ms windows, using a period of 10 ms (the windows overlap). We use 12 MF</context>
<context position="11735" citStr="Boruta (2012)" startWordPosition="1921" endWordPosition="1922">0.944 0.971 20 0.918 0.961 0.966 0.785 0.869 0.886 Table 1 : Same-different scores for top-down cues on Random allophones, as a function of the average number of allophones per phoneme. M=Martin et al., B=Boruta, N= Normalized Table 2 shows the results for HMM-based allophones. The acoustic score is very accurate for both languages and is quite robust to variation. Top-down cues, on the other hand, perform, surprisingly, almost at chance level in distinguishing between allophonic and non-allophonic pairs. A similar discrepancy for the case of Japanese was actually noted, but not explained, in Boruta (2012). English Japanese Allo./phon. A M B N A M B N 2 0.916 0.592 0.632 0.643 0.885 0.422 0.524 0.537 5 0.918 0.592 0.607 0.611 0.908 0.507 0.542 0.551 10 0.893 0.569 0.571 0.571 0.827 0.533 0.546 0.548 20 0.879 0.560 0.560 0.559 0.876 0.541 0.543 0.543 Table 2 : Same-different scores for bottom-up and top-down cues on HMM-based allophones, as a function of the average number of allophones per phoneme. A=Acoustic, M=Martin et al., B=Boruta, N= Normalized 5 Analysis 5.1 Why does the performance drop for realistic allophones? When we list all possible pairs of allophones in the inventory, some of the</context>
</contexts>
<marker>Boruta, 2012</marker>
<rawString>Luc Boruta. 2012. Indicateurs d’allophonie et de phon´emicit´e. Doctoral dissertation, Universit´e Paris-Diderot - Paris VII.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Cristia</author>
<author>A Seidl</author>
</authors>
<title>In press. The hyperarticulation hypothesis of infant-directed speech.</title>
<journal>Journal of Child Language.</journal>
<marker>Cristia, Seidl, </marker>
<rawString>A. Cristia and A. Seidl. In press. The hyperarticulation hypothesis of infant-directed speech. Journal of Child Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi H Feldman</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
<author>James L Morgan</author>
</authors>
<title>A role for the developing lexicon in phonetic category acquisition.</title>
<date>2013</date>
<journal>Psychological Review,</journal>
<volume>120</volume>
<issue>4</issue>
<contexts>
<context position="1820" citStr="Feldman et al., 2013" startWordPosition="261" endWordPosition="264">contrasts (Werker and Tees, 1984) and enhance their ability to distinguish betweencategory contrasts (Kuhl et al., 2006). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the linguistic units of their language through a similaritybased distributional analysis of their input (Maye et al., 2002; Vallabha et al., 2007). In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units (Feldman et al., 2013; Martin et al., 2013). The aim of the present work is to explore how much information can ideally be derived from both hypotheses. The paper is organized as follows. First we describe how we modeled phonetic variation from audio recordings, second we introduce a bottomup cue based on acoustic similarity and topdown cues based of the properties of the lexicon. We test their performance in a task that consists in discriminating within-category contrasts from between-category contrasts. Finally we discuss the role and scope of each cue for the acquisition of phonemes. 2 Modeling phonetic variati</context>
</contexts>
<marker>Feldman, Griffiths, Goldwater, Morgan, 2013</marker>
<rawString>Naomi H. Feldman, Thomas L. Griffiths, Sharon Goldwater, and James L. Morgan. 2013. A role for the developing lexicon in phonetic category acquisition. Psychological Review, 120(4):751–778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdellah Fourtassi</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>A rudimentary lexicon and semantics help bootstrap phoneme acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="20784" citStr="Fourtassi and Dupoux, 2014" startWordPosition="3485" endWordPosition="3488"> gets more linguistic experience, and their combination with bottom-up cues shows that they can provide non-redundant information. Note, finally, that even if this work is based on a more realistic input compared to previous studies, it still uses simplifying assumptions, like ideal word segmentation, and no low-level acoustic variability. Those assumptions are, however, useful in quantifying the information that can ideally be extracted from the input, which is a necessary preliminary step before modeling how this input is used in a cognitively plausible way. Interested readers may refer to (Fourtassi and Dupoux, 2014; Fourtassi et al., 2014) for a more learningoriented approach, where some of the assumptions made here about high level representations are relaxed. Acknowledgments This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10- IDEX-0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the R´egion Ile de France (DIM cerveau et pens´ee). We thank Luc Boruta, Sanjeev Khudanpur, Isabelle Dautriche, Sharon Peperkamp and Benoit Crabb´e for highly useful discussions and </context>
</contexts>
<marker>Fourtassi, Dupoux, 2014</marker>
<rawString>Abdellah Fourtassi and Emmanuel Dupoux. 2014. A rudimentary lexicon and semantics help bootstrap phoneme acquisition. In Proceedings of the 18th Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abdellah Fourtassi</author>
<author>Ewan Dunbar</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Self-consistency as an inductive bias in early language acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Cognitive Science Society.</booktitle>
<contexts>
<context position="20809" citStr="Fourtassi et al., 2014" startWordPosition="3489" endWordPosition="3492">ence, and their combination with bottom-up cues shows that they can provide non-redundant information. Note, finally, that even if this work is based on a more realistic input compared to previous studies, it still uses simplifying assumptions, like ideal word segmentation, and no low-level acoustic variability. Those assumptions are, however, useful in quantifying the information that can ideally be extracted from the input, which is a necessary preliminary step before modeling how this input is used in a cognitively plausible way. Interested readers may refer to (Fourtassi and Dupoux, 2014; Fourtassi et al., 2014) for a more learningoriented approach, where some of the assumptions made here about high level representations are relaxed. Acknowledgments This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOTPHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10- IDEX-0001-02 PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the R´egion Ile de France (DIM cerveau et pens´ee). We thank Luc Boruta, Sanjeev Khudanpur, Isabelle Dautriche, Sharon Peperkamp and Benoit Crabb´e for highly useful discussions and contributions. 5 Steve J.</context>
</contexts>
<marker>Fourtassi, Dunbar, Dupoux, 2014</marker>
<rawString>Abdellah Fourtassi, Ewan Dunbar, and Emmanuel Dupoux. 2014. Self-consistency as an inductive bias in early language acquisition. In Proceedings of the 36th Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patricia K Kuhl</author>
<author>Erica Stevens</author>
<author>Akiko Hayashi</author>
<author>Toshisada Deguchi</author>
<author>Shigeru Kiritani</author>
<author>Paul Iverson</author>
</authors>
<title>Infants show a facilitation effect for native language phonetic perception between 6 and 12 months.</title>
<date>2006</date>
<journal>Developmental Science,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="1320" citStr="Kuhl et al., 2006" startWordPosition="184" endWordPosition="187">c properties and statistical regularities of each language. We found both approaches to help discriminate between allophonic and phonemic contrasts with a high degree of accuracy, although top-down cues proved to be effective only on an interesting subset of the data. 1 Introduction Developmental studies have shown that, during their first year, infants tune in on the phonemic categories (consonants and vowels) of their language, i.e., they lose the ability to distinguish some within-category contrasts (Werker and Tees, 1984) and enhance their ability to distinguish betweencategory contrasts (Kuhl et al., 2006). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the linguistic units of their language through a similaritybased distributional analysis of their input (Maye et al., 2002; Vallabha et al., 2007). In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units (Feldman et al., 2013; Martin et al., 2013). The aim of the present work is to explore how much information can ideally b</context>
</contexts>
<marker>Kuhl, Stevens, Hayashi, Deguchi, Kiritani, Iverson, 2006</marker>
<rawString>Patricia K. Kuhl, Erica Stevens, Akiko Hayashi, Toshisada Deguchi, Shigeru Kiritani, and Paul Iverson. 2006. Infants show a facilitation effect for native language phonetic perception between 6 and 12 months. Developmental Science, 9(2):F13–F21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
<author>Hanae Koiso</author>
<author>Sadaoki Furui</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Spontaneous speech corpus of japanese.</title>
<date>2000</date>
<booktitle>In LREC,</booktitle>
<pages>947--952</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="4233" citStr="Maekawa et al., 2000" startWordPosition="644" endWordPosition="647">e an acoustic distance. Here, we generate linguistically and acoustically controlled allophones using Hidden Markov Models (HMMs) trained on audio recordings. 1 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Corpora We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which consists of 40 hours of spontaneous conversations with 40 speakers of American English, and the core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000) which also consists of about 40 hours of recorded spontaneous conversations and public speeches in different fields. Both corpora are timealigned with phonetic labels. Following Boruta (2012), we relabeled the japanese corpus using 25 phonemes. For English, we used the phonemic version which consists of 45 phonemes. 2.2 Input generation 2.2.1 HMM-based allophones In order to generate linguistically and acoustically plausible allophones, we apply a standard Hidden Markov Model (HMM) phoneme recognizer with a three-state per phone architecture to the signal, as follows. First, we convert the ra</context>
</contexts>
<marker>Maekawa, Koiso, Furui, Isahara, 2000</marker>
<rawString>Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000. Spontaneous speech corpus of japanese. In LREC, pages 947–952, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Martin</author>
<author>Sharon Peperkamp</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>Learning phonemes with a protolexicon.</title>
<date>2013</date>
<journal>Cognitive Science,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1842" citStr="Martin et al., 2013" startWordPosition="265" endWordPosition="269">Tees, 1984) and enhance their ability to distinguish betweencategory contrasts (Kuhl et al., 2006). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the linguistic units of their language through a similaritybased distributional analysis of their input (Maye et al., 2002; Vallabha et al., 2007). In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units (Feldman et al., 2013; Martin et al., 2013). The aim of the present work is to explore how much information can ideally be derived from both hypotheses. The paper is organized as follows. First we describe how we modeled phonetic variation from audio recordings, second we introduce a bottomup cue based on acoustic similarity and topdown cues based of the properties of the lexicon. We test their performance in a task that consists in discriminating within-category contrasts from between-category contrasts. Finally we discuss the role and scope of each cue for the acquisition of phonemes. 2 Modeling phonetic variation In this section, we</context>
<context position="3477" citStr="Martin et al., 2013" startWordPosition="528" endWordPosition="531">bstruent /r/ →[x] elsewhere Figure 1: Allophonic variation of French /r/ The phoneme /r/ surfaces as voiced ([x]) before a voiced obstruent like in [kanax Son] (“canard jaune”, yellow duck) and as voiceless ([X]) before a voiceless obstruent as in [kanaX puXpx] (“canard pourpre”, purple duck). Assuming speech sounds are coded as allophones, the challenge facing the learner is to distinguish the allophonic variation ([x], [X]) from the phonemic variation (related to a difference in the meaning) like the contrast ([x],[l]). Previous work has generated allophonic variation using random contexts (Martin et al., 2013). This procedure does not take into account the fact that contexts belong to natural classes. In addition, it does not enable to compute an acoustic distance. Here, we generate linguistically and acoustically controlled allophones using Hidden Markov Models (HMMs) trained on audio recordings. 1 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Corpora We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which consi</context>
<context position="6283" citStr="Martin et al. (2013)" startWordPosition="976" endWordPosition="979"> phonemic inventory) using a linguistic feature-based decision tree, and the HMM states of linguistically similar triphones are tied together so as to maximize the likelihood of the data. Finally, the triphone models are trained again while the initial gaussian emission models are replaced by mixture of gaussians with a progressively increasing number of components, until each HMM state is modeled by a mixture of 17 diagonal-covariance gaussians. The HMM were built using the HMM Toolkit (HTK: Young et al., 2006). 2.2.2 Random allophones As a control, we also reproduce the random allophones of Martin et al. (2013), in which allophonic contexts are determined randomly: for a given phoneme /p/, the set of all possible contexts is randomly partitioned into a fixed number n of subsets. In the transcription, the phoneme /p/ is converted into one of its allophones (p1,p2,..,pn) depending on the subset to which the current context belongs. 3 Bottom-up and top-down hypotheses 3.1 Acoustic cue The bottom-up cue is based on the hypothesis that instances of the same phoneme are likely to be acoustically more similar than instances of two different phonemes (see Cristia and Seidl, in press) for a similar propositi</context>
<context position="7821" citStr="Martin et al. (2013)" startWordPosition="1237" endWordPosition="1240">metrized version of the KullbackLeibler (KL) divergence measure (each state was approximated by a single non-diagonal Gaussian): A(x, y) = KL(Nxz||Ny;) + KL(Ny;||Nxz) � (i,j)∈DTW (x,y) Where {(i, j) E DTW (x, y)} is the set of index pairs over the HMM states that correspond to the optimal DTW path in the comparison between phone model x and y, and Nxz the full covariance Gaussian distribution for state i of phone x. For obvious reasons, the acoustic distance cue cannot be computed for Random allophones. 3.2 Lexical cues The top-down information we use in this study, is based on the insight of Martin et al. (2013). It rests on the idea that true lexical minimal pairs are not very frequent in human languages, as compared to minimal pairs due to mere phonological processes. In fact, the latter creates variants (alternants) of the same lexical item since adjacent sounds condition the realization of the first and final phoneme. For example, as shown in figure 1, the phoneme /r/ surfaces as [x] or [x] depending on whether or not the 2 next sound is a voiceless obstruent. Therefore, the lexical item /kanar/ surfaces as [kanaX] or [kanair]. The lexical cue assumes that a pair of words differing in the first o</context>
<context position="9879" citStr="Martin et al. (2013)" startWordPosition="1597" endWordPosition="1601">that could be seen as a normalization of Boruta’s cue: |(Ax,Ay)∈L2|+|(xA,yA)∈L2| N (x, y) _ |{Ax∈L}|+|{Ay∈L}|+|{xA∈L}|+|{yA∈L}| 4 Experiment 4.1 Task For each corpus we list all the possible pairs of attested allophones. Some of these pairs are allophones of the same phoneme (allophonic pair) and others are allophones of different phonemes (nonallophonic pairs). The task is a same-different classification, whereby each of these pairs is given a score from the cue that is being tested. A good cue gives higher scores to allophonic pairs. 4.2 Evaluation We use the same evaluation procedure as in Martin et al. (2013). It is carried out by computing the area under the curve of the Receiver Operating Characteristic (ROC). A value of 0.5 represents chance and a value of 1 represents perfect performance. In order to lessen the potential influence of the structure of the corpus (mainly the order of the utterances) on the results, we use a statistical resampling scheme. The corpus is divided into small blocks (of 20 utterances each). In each run, we draw randomly with replacement from this set of blocks a sample of the same size as the original corpus. This sample is then used to retrain the acoustic models and</context>
</contexts>
<marker>Martin, Peperkamp, Dupoux, 2013</marker>
<rawString>Andrew Martin, Sharon Peperkamp, and Emmanuel Dupoux. 2013. Learning phonemes with a protolexicon. Cognitive Science, 37(1):103–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Maye</author>
<author>J F Werker</author>
<author>L Gerken</author>
</authors>
<title>Infant sensitivity to distributional information can affect phonetic discrimination.</title>
<date>2002</date>
<tech>Cognition, 82:B101–B111.</tech>
<contexts>
<context position="1638" citStr="Maye et al., 2002" startWordPosition="232" endWordPosition="235">at, during their first year, infants tune in on the phonemic categories (consonants and vowels) of their language, i.e., they lose the ability to distinguish some within-category contrasts (Werker and Tees, 1984) and enhance their ability to distinguish betweencategory contrasts (Kuhl et al., 2006). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the linguistic units of their language through a similaritybased distributional analysis of their input (Maye et al., 2002; Vallabha et al., 2007). In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units (Feldman et al., 2013; Martin et al., 2013). The aim of the present work is to explore how much information can ideally be derived from both hypotheses. The paper is organized as follows. First we describe how we modeled phonetic variation from audio recordings, second we introduce a bottomup cue based on acoustic similarity and topdown cues based of the properties of the lexicon. We test their performance in a task that consists in di</context>
</contexts>
<marker>Maye, Werker, Gerken, 2002</marker>
<rawString>J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sensitivity to distributional information can affect phonetic discrimination. Cognition, 82:B101–B111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Peperkamp</author>
<author>Rozenn Le Calvez</author>
<author>Jean-Pierre Nadal</author>
<author>Emmanuel Dupoux</author>
</authors>
<title>The acquisition of allophonic rules: Statistical learning with linguistic constraints.</title>
<date>2006</date>
<journal>Cognition,</journal>
<volume>101</volume>
<issue>3</issue>
<marker>Peperkamp, Le Calvez, Nadal, Dupoux, 2006</marker>
<rawString>Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre Nadal, and Emmanuel Dupoux. 2006. The acquisition of allophonic rules: Statistical learning with linguistic constraints. Cognition, 101(3):B31–B41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Pitt</author>
<author>L Dilley</author>
<author>K Johnson</author>
<author>S Kiesling</author>
<author>W Raymond</author>
<author>E Hume</author>
<author>Fosler-Lussier</author>
</authors>
<title>Buckeye corpus of conversational speech.</title>
<date>2007</date>
<contexts>
<context position="4064" citStr="Pitt et al., 2007" startWordPosition="615" endWordPosition="618"> contexts (Martin et al., 2013). This procedure does not take into account the fact that contexts belong to natural classes. In addition, it does not enable to compute an acoustic distance. Here, we generate linguistically and acoustically controlled allophones using Hidden Markov Models (HMMs) trained on audio recordings. 1 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1–6, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2.1 Corpora We use two speech corpora: the Buckeye Speech corpus (Pitt et al., 2007), which consists of 40 hours of spontaneous conversations with 40 speakers of American English, and the core of the Corpus of Spontaneous Japanese (Maekawa et al., 2000) which also consists of about 40 hours of recorded spontaneous conversations and public speeches in different fields. Both corpora are timealigned with phonetic labels. Following Boruta (2012), we relabeled the japanese corpus using 25 phonemes. For English, we used the phonemic version which consists of 45 phonemes. 2.2 Input generation 2.2.1 HMM-based allophones In order to generate linguistically and acoustically plausible a</context>
</contexts>
<marker>Pitt, Dilley, Johnson, Kiesling, Raymond, Hume, Fosler-Lussier, 2007</marker>
<rawString>M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Raymond, E. Hume, and Fosler-Lussier. 2007. Buckeye corpus of conversational speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Vallabha</author>
<author>J L McClelland</author>
<author>F Pons</author>
<author>J F Werker</author>
<author>S Amano</author>
</authors>
<title>Unsupervised learning of vowel categories from infant-directed speech.</title>
<date>2007</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>104</volume>
<issue>33</issue>
<contexts>
<context position="1662" citStr="Vallabha et al., 2007" startWordPosition="236" endWordPosition="239">rst year, infants tune in on the phonemic categories (consonants and vowels) of their language, i.e., they lose the ability to distinguish some within-category contrasts (Werker and Tees, 1984) and enhance their ability to distinguish betweencategory contrasts (Kuhl et al., 2006). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the linguistic units of their language through a similaritybased distributional analysis of their input (Maye et al., 2002; Vallabha et al., 2007). In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units (Feldman et al., 2013; Martin et al., 2013). The aim of the present work is to explore how much information can ideally be derived from both hypotheses. The paper is organized as follows. First we describe how we modeled phonetic variation from audio recordings, second we introduce a bottomup cue based on acoustic similarity and topdown cues based of the properties of the lexicon. We test their performance in a task that consists in discriminating within-cate</context>
</contexts>
<marker>Vallabha, McClelland, Pons, Werker, Amano, 2007</marker>
<rawString>G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker, and S. Amano. 2007. Unsupervised learning of vowel categories from infant-directed speech. Proceedings of the National Academy of Sciences, 104(33):13273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet F Werker</author>
<author>Richard C Tees</author>
</authors>
<title>Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life.</title>
<date>1984</date>
<journal>Infant Behavior and Development,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="1233" citStr="Werker and Tees, 1984" startWordPosition="171" endWordPosition="174">a of spontaneous speech to provide the learner with an input that models both the linguistic properties and statistical regularities of each language. We found both approaches to help discriminate between allophonic and phonemic contrasts with a high degree of accuracy, although top-down cues proved to be effective only on an interesting subset of the data. 1 Introduction Developmental studies have shown that, during their first year, infants tune in on the phonemic categories (consonants and vowels) of their language, i.e., they lose the ability to distinguish some within-category contrasts (Werker and Tees, 1984) and enhance their ability to distinguish betweencategory contrasts (Kuhl et al., 2006). Current work in early language acquisition has proposed two competing hypotheses that purport to account for the acquisition of phonemes. The bottom-up hypothesis holds that infants converge on the linguistic units of their language through a similaritybased distributional analysis of their input (Maye et al., 2002; Vallabha et al., 2007). In contrast, the top-down hypothesis emphasizes the role of higher level linguistic structures in order to learn the lower level units (Feldman et al., 2013; Martin et a</context>
</contexts>
<marker>Werker, Tees, 1984</marker>
<rawString>Janet F. Werker and Richard C. Tees. 1984. Crosslanguage speech perception: Evidence for perceptual reorganization during the first year of life. Infant Behavior and Development, 7(1):49 – 63.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>