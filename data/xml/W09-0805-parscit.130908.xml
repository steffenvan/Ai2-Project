<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000507">
<title confidence="0.9990805">
Unsupervised Concept Discovery In Hebrew Using Simple Unsupervised
Word Prefix Segmentation for Hebrew and Arabic
</title>
<author confidence="0.998051">
Elad Dinur&apos; Dmitry Davidov2 Ari Rappoport&apos;
</author>
<affiliation confidence="0.986057666666667">
&apos;Institute of Computer Science
2ICNC
The Hebrew University of Jerusalem
</affiliation>
<sectionHeader confidence="0.978382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905590909091">
Fully unsupervised pattern-based methods
for discovery of word categories have been
proven to be useful in several languages.
The majority of these methods rely on the
existence of function words as separate
text units. However, in morphology-rich
languages, in particular Semitic languages
such as Hebrew and Arabic, the equiva-
lents of such function words are usually
written as morphemes attached as prefixes
to other words. As a result, they are missed
by word-based pattern discovery methods,
causing many useful patterns to be unde-
tected and a drastic deterioration in per-
formance. To enable high quality lexical
category acquisition, we propose a sim-
ple unsupervised word segmentation algo-
rithm that separates these morphemes. We
study the performance of the algorithm for
Hebrew and Arabic, and show that it in-
deed improves a state-of-art unsupervised
concept acquisition algorithm in Hebrew.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996978">
In many NLP tasks, we wish to extract informa-
tion or perform processing on text using minimal
knowledge on the input natural language. Towards
this goal, we sometimes find it useful to divide the
set of words in natural language to function words
and content words, a division that applies in the
vast majority of languages. Function words (or
grammatical words, e.g., a, an, the, in, of, etc) are
words that have little or highly ambiguous lexi-
cal meaning, and serve to express grammatical or
semantic relationships with the other words in a
sentence.
In some morphologically-rich languages, im-
portant function words are not written as space-
separated units but as morphemes attached as pre-
fixes to other words. This fact can cause prob-
lems when statistically analyzing text in these lan-
guages, for two main reasons: (1) the vocabulary
of the language grows, as our lexical knowledge
comes solely from a corpus (words appear with
and without the function morphemes); (2) infor-
mation derived from the presence of these mor-
phemes in the sentence is usually lost.
In this paper we address the important task of
a fully unsupervised acquisition of Hebrew lexical
categories (or concepts – words sharing a signifi-
cant aspect of their meaning). We are not aware of
any previous work on this task for Hebrew. Due
to the problem above, the performance of many
acquisition algorithms deteriorates unacceptably.
This happens, for example, in the (Davidov and
Rappoport, 2006) algorithm that utilizes automati-
cally detected function words as the main building
block for pattern construction.
In order to overcome this problem, one should
separate such prefixes from the compound words
(words consisting of function morphemes attached
to content words) in the input corpus. When
we consider some particular word, there are fre-
quently many options to split it to smaller strings.
Fortunately, the set of function words is small and
closed, and the set of grammatical sequences of
function prefixes is also small. Hence we assume
it does not cost us much to know in advance what
are the possible sequences for a specific language.
Even when considering the small number of
possible function words, the task of separating
them is not simple, as some words may be ambigu-
ous. When reading a word that starts with a prefix
known to be a function morpheme, the word may
</bodyText>
<note confidence="0.9520315">
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 36–44,
Athens, Greece, 31 March, 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.997811">
36
</page>
<bodyText confidence="0.996985971428572">
be a compound word, or it may be a meaningful
word by itself. For example, the word “hsws” in
Hebrew1 can be interpreted as “hsws” (hesitation),
or “h sws” (the horse). The segmentation of the
word is context dependent – the same string may
be segmented differently in different contexts.
One way of doing such word prefix segmenta-
tion is to perform a complete morphological dis-
ambiguation of the sentence. The disambigua-
tion algorithm finds for each word its morpho-
logical attributes (POS tag, gender, etc.), and de-
cides whether a word is a compound word or a
word without prefixes. A disambiguation algo-
rithm generally relies on a language-specific mor-
phological analyzer. It may also require a large
manually tagged corpus, construction of which for
some particular language or domain requires sub-
stantial human labor. We avoid the utilization of
such costly and language-specific disambiguation
algorithms and manually annotated data.
In this paper we present a novel method to sep-
arate function word prefixes, and evaluate it us-
ing manually labeled gold standards in Hebrew
and Arabic. We incorporate the method into a
pattern-based Hebrew concept acquisition frame-
work and show that it greatly improves state-of-art
results for unsupervised lexical category acquisi-
tion. This improvement allows the pattern-based
unsupervised framework to use one-tenth of the
Hebrew data in order to reach a similar level of
results.
Section 2 discusses related work, and Section 3
reviews the word categories discovery algorithm.
Section 4 presents the word prefix segmentation
algorithm. Results are given in Section 5.
</bodyText>
<sectionHeader confidence="0.999774" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.995920333333334">
In this paper we develop an unsupervised frame-
work for segmentation of the function words for
languages where context is important for correct
segmentation. Our main target language is He-
brew, and we experimented with Arabic as well.
As far as we know, there is no work on unsu-
pervised segmentation of words in Hebrew which
does not utilize language-specific tools such as
morphological analyzers.
Lee et al. (2003) addressed supervised word
segmentation in Arabic and have some aspects
similar to our approach. As in their study, we
</bodyText>
<footnote confidence="0.974219">
1Transcription is according to (Ornan, 2005), except for
Shin which is denoted by “$”.
</footnote>
<bodyText confidence="0.999745705882353">
also have a pre-supplied list of possible prefix
sequences and assume a trigram model in order
to find the most probable morpheme sequence.
Both studies evaluate performance on a segmented
text, and not just on words in the lexicon. How-
ever, their algorithm, while achieving good per-
formance (97% accuracy), relies on a training set
– a manually segmented corpus of about 110,000
words, while our unsupervised framework does
not require any annotation and is thus easier to im-
plement and to apply to different domains and lan-
guages.
Snyder and Barzilay (2008) study the task of un-
supervised morphological segmentation of multi-
ple languages. Their algorithm automatically in-
duces a segmentation and morpheme alignment of
short parallel phrases from a multilingual corpus.
Their corpus (The Hebrew Bible and translations)
contains parallel phrases in English, Arabic, He-
brew and Aramaic. They obtain 63.87 F-Score
for Hebrew words segmentation (prefix and suf-
fix), where recall and precision is calculated based
on all possible segmentation points.
Another type of segmentation algorithms in-
volves utilization of language-specific morpholog-
ical analyzers for complete morphological disam-
biguation. In Hebrew each word usually has more
than one possible POS (along with other attributes,
such as gender, number, etc.). Assuming we have
a morphological analyzer (producing the set of
possible analyses for a given word), we can try to
discover the correct segmentation of each word.
Levinger et al. (1995) developed a method for
disambiguation of the results provided by a mor-
phological analyzer for Hebrew. Adler and El-
hadad (2006) proposed an unsupervised algorithm
for word segmentation. They estimate an initial
language model (using (Levinger et al., 1995))
and improve this model with EM. Direct compar-
ison to their work is problematic, however, since
we avoid utilization of a language-specific mor-
phology/POS analyzer. There are also studies of
this type that utilize labeled data (Bar-Haim et al.,
2005), where the language model is learned from
the training data.
Extensive research has been done on word seg-
mentation, where, unlike in our study, the segmen-
tation is evaluated for every word, regardless of its
context. Creutz (2003) presents an algorithm for
unsupervised segmentation under these assump-
tions. He proposes a probabilistic model which
</bodyText>
<page confidence="0.998895">
37
</page>
<bodyText confidence="0.999971848101266">
utilizes the distributions of morpheme length and
frequency to estimate the quality of the induced
morphemes. Dasgupta and Ng (2007) improves
over (Creutz, 2003) by suggesting a simpler ap-
proach. They segment a prefix using the word
frequency with and without a prefix. Other re-
cent studies that follow the context-independent
setup include (Creutz and Lagus, 2005; Keshava
and Pitler, 2005; Demberg, 2007). They test
their methods on English, Finnish and Turkish.
All of these studies, however, assume context-
independency of segmentation, disregarding the
ambiguity that may come from context. This
makes it problematic to apply the proposed meth-
ods to context-dependent morphology types as in
Hebrew and Arabic.
The guiding goal in the present paper is the con-
cept acquisition problem. Concept acquisition of
different kinds has been studied extensively. The
two main classification axes for this task are the
type of human input and annotation, and the basic
algorithmic approach used. The two main algo-
rithmic approaches are clustering of context fea-
ture vectors and pattern-based discovery.
The first approach is to map each word to a fea-
ture vector and cluster these vectors. Example of
such algorithms are (Pereira et al., 1993) and (Lin,
1998) that use syntactic features in the vector def-
inition. Pantel and Lin (2002) improves on the lat-
ter by clustering by committee.
Recently, there is a growing interest in the sec-
ond main algorithmic approach, usage of lexico-
syntactic patterns. Patterns have been shown to
produce more accurate results than feature vectors,
at a lower computational cost on large corpora
(Pantel et al., 2004). Thus (Dorow et al., 2005)
discover categories using two basic pre-specified
patterns (“x and y”, “x or y”).
Some recent studies have proposed frameworks
that attempt to avoid any implicit or explicit pre-
specification of patterns. Davidov and Rappoport
(2006) proposed a method that detects function
words by their high frequency, and utilizes these
words for the discovery of symmetric patterns.
Their method is based on two assumptions: (1)
some function words in the language symmetri-
cally connect words belonging to the same cat-
egory; (2) such function words can be detected
as the most frequent words in language. While
these assumptions are reasonable for many lan-
guages, for some morphologically rich languages
the second assumption may fail. This is due to
the fact that some languages like Hebrew and Ara-
bic may express relationships not by isolated func-
tion words but by morphemes attached in writing
to other words.
As an example, consider the English word
“and’, which was shown to be very useful in con-
cept acquisition (Dorow et al., 2005). In Hebrew
this word is usually expressed as the morpheme
“w” attached to the second word in a conjunc-
tion (“... wsws” – “... and horse”). Patterns dis-
covered by such automatic pattern discovery al-
gorithms are based on isolated words, and hence
fail to capture “and”-based relationships that are
very useful for detection of words belonging to the
same concept. Davidov and Rappoport (2006) re-
ports very good results for English and Russian.
However, no previous work applies a fully unsu-
pervised concept acquisition for Hebrew.
In our study we combine their concept ac-
quisition framework with a simple unsupervised
word segmentation technique. Our evaluation con-
firms the weakness of word-based frameworks for
morphology-rich languages such as Hebrew, and
shows that utilizing the proposed word segmen-
tation can overcome this weakness while keeping
the concept acquisition approach fully unsuper-
vised.
</bodyText>
<sectionHeader confidence="0.9831255" genericHeader="method">
3 Unsupervised Discovery of Word
Categories
</sectionHeader>
<bodyText confidence="0.999967">
In this study we use word segmentation to improve
the (Davidov and Rappoport, 2006) method for
discovery of word categories, sets of words shar-
ing a significant aspect of their meaning. An ex-
ample for such a discovered category is the set of
verbs {dive, snorkel, swim, float, surf, sail, drift,
...}. Below we briefly describe this category ac-
quisition algorithm.
The algorithm consists of three stages as fol-
lows. First, it discovers a set of pattern candidates,
which are defined by a combination of high fre-
quency words (denoted by H) and slots for low
frequency (content) words (denoted by C). An ex-
ample for such a pattern candidate is ‘x belongs to
y’, where ‘x’ and ‘y’ stand for content word slots.
The patterns are found according to a predefined
set of possible meta-patterns. The meta-patterns
are language-independent2 and consist of up to 4
</bodyText>
<footnote confidence="0.6424865">
2They do not include any specific words, only a relative
order of high/low frequency words, and hence can be used on
</footnote>
<page confidence="0.997733">
38
</page>
<bodyText confidence="0.999974395348837">
words in total, from which two are (non-adjacent)
content words. Four meta-patterns are used: CHC,
CHCH, CHHC, HCHC.
Second, those patterns which give rise to sym-
metric lexical relationships are identified. The
meaning of phrases constructed from those pat-
terns is (almost) invariant to the order of the con-
tent words contained in them. An example for
such a pattern is ‘x and y’. In order to iden-
tify such useful patterns, for each pattern we build
a graph following (Widdows and Dorow, 2002).
The graph is constructed from a node for each con-
tent word, and a directed arc from the node ‘x’ to
‘y’ if the corresponding content words appear in
the pattern such that ‘x’ precedes ‘y’. Then we
calculate several symmetry measures on the graph
structure and select the patterns with best values
for these measures.
The third stage is the generation of categories.
We extract tightly connected sets of words from
the unified graph which combines all graphs of se-
lected patterns. Such sets of words define the de-
sired categories.
The patterns which include the ‘x and y’ sub-
string are among the most useful patterns for gen-
eration of categories (they were used in (Dorow et
al., 2005) and discovered in all 5 languages tested
in (Davidov and Rappoport, 2006)). However, in
Hebrew such patterns can not be found in the same
way, since the function word ‘and’ is the prefix ‘w’
and not a standalone high frequency word.
Another popular set of patterns are ones includ-
ing ‘x or y’. Such patterns can be identified in
Hebrew, as ‘or’ in Hebrew is a separate word.
However, even in this case, the content word rep-
resented by ‘x’ or ‘y’ may appear with a pre-
fix. This damages the construction of the pattern
graph, since two different nodes may be created
instead of one – one for a regular content word,
the other for the same word with a prefix. Conse-
quently, it is reasonable to assume that segmenting
the corpus in advance should improve the results
of discovery of word categories.
</bodyText>
<sectionHeader confidence="0.987746" genericHeader="method">
4 Word Segmentation Algorithm
</sectionHeader>
<bodyText confidence="0.916391">
We assume we know the small and closed set of
grammatical function word prefix sequences in the
language3. Our input is a sentence, and our ob-
any languages with explicit word segmentation.
</bodyText>
<footnote confidence="0.963233333333333">
3Unlike development of labeled training data, handcraft-
ing such a closed set is straightforward for many languages
and does not requires any significant time/human labor
</footnote>
<bodyText confidence="0.999591428571429">
jective is to return the correct segmentation of the
sentence. A sentence L is a sequence of words
{w1, w2, ..., wn}. A segmentation Si of L is a se-
quence of morphemes {m1, m2, ..., mk} and l(Si)
is the number of morphemes in the sequence. Note
that l(Si) may be different for each segmentation.
The best segmentation S will be calculated by:
</bodyText>
<equation confidence="0.9758386">
l(Si)
P(Si) = p(m1)p(m2|m1) 11 p(mi|mi−1mi−2)
i=3
S = arg max
Si
</equation>
<bodyText confidence="0.99989275">
Calculation of joint probabilities requires a tri-
gram model of the language. Below we describe
the construction of the trigram model and then we
detail the algorithm for efficient calculation of S.
</bodyText>
<subsectionHeader confidence="0.999288">
4.1 Construction of trigram model
</subsectionHeader>
<bodyText confidence="0.99993625">
Creating the trigram language model is done in
two stages: (1) we segment a corpus automati-
cally, and (2) we learn a trigram language model
from the segmented corpus.
</bodyText>
<sectionHeader confidence="0.427082" genericHeader="method">
4.1.1 Initial corpus segmentation
</sectionHeader>
<bodyText confidence="0.9998045">
For initial corpus segmentation, we define a sta-
tistical measure for the segmentation of individual
words. Let wx be a word, such that w is the pre-
fix of the word composed of a sequence of func-
tion word prefixes and x is a string of letters. Let
f(x) be the frequency of the word x in the cor-
pus. Denote by al the average length of the strings
(with prefixes) in the language. This can be eas-
ily estimated from the corpus – every string that
appears in the corpus is counted once. l(x) is the
number of characters in the word x. We utilize
two parameters G, H, where G &lt; H (we used
</bodyText>
<equation confidence="0.97416175">
G = 2.5, H = 3.5) and define the following func-
tions :
� al−G−l(x)
al−H l(x) &lt; al − G
factor(x) =
0 otherwise
Rank(wx) = f(wx)
f(wx) + f(x)+ factor(x)
</equation>
<bodyText confidence="0.969488333333333">
Note that the expression f(wx)
f(wx)+f(x) is a number
in (0, 1], inversely correlated with the frequency of
the prefixed word. Thus higher Rank(wx) values
indicate that the word is less likely to be composed
of the prefix w followed by the word x.
</bodyText>
<equation confidence="0.837114">
P(Si)
</equation>
<page confidence="0.682544">
39
</page>
<bodyText confidence="0.949958380952381">
The expression al−G−l(x) is a number in (0, 1],
al−H
therefore factor(x) E [0, 1]. H is G − 1 in order
to keep the expression smaller than 1. The term
factor(x) is greater as x is shorter. The factor
is meant to express the fact that short words are
less likely to have a prefix. We have examined
this in Hebrew – as there are no words of length
1, two letter words have no prefix. We have ana-
lyzed 102 randomly chosen three letter words, and
found that only 19 of them were prefixed words.
We have analyzed 100 randomly chosen four let-
ter words, and found that 40 of them were pre-
fixed words. The result was about the same for
five letter words. In order to decide whether a
word needs to be separated, we define a thresh-
old T E [0, 1]. We allow word separation only
when Rank(wx) is lower than T. When there
are more than two possible sequences of function
word prefixes (“mhsws”,“m hsws”, “mh sws”),
we choose the segmentation with the lower rank.
</bodyText>
<subsectionHeader confidence="0.903472">
4.1.2 Learning the trigram model
</subsectionHeader>
<bodyText confidence="0.999863222222222">
The learning of the language model is based on
counts of the corpus, assigning a special symbol,
“u/k” (unknown) for all words that do not appear
in the corpus. As estimated by (Lee et al., 2003),
we set the probability of “u/k” to be 1E − 9. The
value of the symbol “u/k” was observed to be sig-
nificant. We found that the value proposed by (Lee
et al., 2003) for Arabic gives good results also for
Hebrew.
</bodyText>
<subsectionHeader confidence="0.919573">
4.2 Dynamic programming approach for
word segmentation
</subsectionHeader>
<bodyText confidence="0.998985894736842">
The naive method to find S is to iterate over
all possible segmentations of the sentence. This
method may fail to handle long sentences, as
the number of segmentations grows exponentially
with the length of the sentence. To overcome this
problem, we use dynamic programming.
Each morpheme has an index i to its place in a
segmentation sequence. Iteratively, for index i, for
every morpheme which appears in some segmen-
tation in index i, we calculate the best segmen-
tation of the sequence m1 ... mi. Two problems
arise here: (1) we need to calculate which mor-
phemes may appear in a given index; (2) we need
to constrain the calculation, such that only valid
segmentations would be considered.
To calculate which morphemes can appear in a
given index we define the object Morpheme. It
contains the morpheme (string), the index of a
word in the sentence the morpheme belongs to,
reference to the preceding Morpheme in the same
word, and indication whether it is the last mor-
pheme in the word. For each index of the sen-
tence segmentation, we create a list of Morphemes
(index-list).
For each word wi, and for segmentation
m1i,.., mki , we create Morphemes M1i , .., Mki . We
traverse sequentially the words in the sentence,
and for each segmentation we add the sequence of
Morphemes to all possible index-lists. The index-
list for the first Morpheme M1i is the combination
of successors of all the index-lists that contain a
Morpheme Mki−1. The constraints are enforced
easily – if a Morpheme Mji is the first in a word,
the preceding Morpheme in the sequence must be
the last Morpheme of the previous word. Oth-
erwise, the preceding Morpheme must be Mj−1
i ,
which is referenced by Mji .
</bodyText>
<subsectionHeader confidence="0.994669">
4.3 Limitations
</subsectionHeader>
<bodyText confidence="0.999993875">
While our model handles the majority of cases, it
does not fully comply with a linguistic analysis of
Hebrew, as there are a few minor exceptions. We
assumed that there is no ambiguity in the function
word prefixes. This is not entirely correct, as in
Hebrew we have two different kinds of exceptions
for this rule. For example, the prefix “k$” (when),
can also be interpreted as the prefix “k” (as) fol-
lowed by the prefix “$” (that). As the second in-
terpretation is rare, we always assumed it is the
prefix “k$”. This rule was applied wherever an
ambiguity exists. However, we did not treat this
problem as it is very rare, and in the development
set and test set it did not appear even once.
A harder problem is encountered when process-
ing the word “bbyt”. Two interpretations could
be considered here: “b byt” (“in a house”), and
“b h byt” (“in the house”). Whether this actu-
ally poses a problem or not depends on the ap-
plication. We assume that the correct segmenta-
tion here is “b byt”. Without any additional lin-
guistic knowledge (for example, diacritical vowel
symbols should suffice in Hebrew), solving these
problems requires some prior discriminative data.
</bodyText>
<sectionHeader confidence="0.990448" genericHeader="method">
5 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.99994025">
We evaluate our algorithm in two stages. First we
test the quality of our unsupervised word segmen-
tation framework on Hebrew and Arabic, compar-
ing our segmentation results to a manually anno-
</bodyText>
<page confidence="0.997261">
40
</page>
<table confidence="0.999948555555556">
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.70 0.844 0.798 0.820 0.875 0.811 0.851 0.830 0.881
0.73 0.841 0.828 0.834 0.883 0.808 0.866 0.836 0.884
0.76 0.837 0.846 0.841 0.886 0.806 0.882 0.842 0.887
0.79 0.834 0.870 0.851 0.893 0.803 0.897 0.847 0.890
0.82 0.826 0.881 0.852 0.892 0.795 0.904 0.846 0.888
0.85 0.820 0.893 0.854 0.892 0.787 0.911 0.844 0.886
0.88 0.811 0.904 0.855 0.891 0.778 0.917 0.841 0.882
</table>
<tableCaption confidence="0.998066">
Table 1: Ranks vs. Threshold T for Hebrew.
</tableCaption>
<table confidence="0.99995175">
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.91 0.940 0.771 0.846 0.892 0.903 0.803 0.850 0.891
0.93 0.930 0.797 0.858 0.898 0.903 0.840 0.870 0.904
0.95 0.931 0.810 0.866 0.904 0.902 0.856 0.878 0.909
0.97 0.927 0.823 0.872 0.906 0.896 0.869 0.882 0.911
0.99 0.925 0.848 0.872 0.915 0.878 0.896 0.886 0.913
1.00 0.923 0.852 0.886 0.915 0.841 0.896 0.867 0.895
</table>
<tableCaption confidence="0.983201">
Table 2: Ranks vs. Threshold T for Arabic.
</tableCaption>
<table confidence="0.9998625">
Algorithm P R F A
Rank seg. 0.834 0.870 0.851 0.893
Baseline 0.561 0.491 0.523 0.69
Morfessor 0.630 0.689 0.658 0.814
</table>
<tableCaption confidence="0.999518">
Table 3: Segmentation results comparison.
</tableCaption>
<bodyText confidence="0.997412">
tated gold standard. Then we incorporate word
segmentation into a concept acquisition frame-
work and compare the performance of this frame-
work with and without word segmentation.
</bodyText>
<subsectionHeader confidence="0.990666">
5.1 Corpora and annotation
</subsectionHeader>
<bodyText confidence="0.999887214285714">
For our experiments in Hebrew we used a 19MB
Hebrew corpus obtained from the “Mila” Knowl-
edge Center for Processing Hebrew4. The cor-
pus consists of 143,689 different words, and a
total of 1,512,737 word tokens. A sample text
of size about 24,000 words was taken from the
corpus, manually segmented by human annotators
and used as a gold standard in our segmentation
evaluation. In order to estimate the quality of our
algorithm for Arabic, we used a 7MB Arabic news
items corpus, and a similarly manually annotated
test text of 4715 words. The Arabic corpus is too
small for meaningful category discovery, so we
used it only in the segmentation evaluation.
</bodyText>
<subsectionHeader confidence="0.999926">
5.2 Evaluation of segmentation framework
</subsectionHeader>
<bodyText confidence="0.984567666666667">
In order to estimate the performance of word seg-
mentation as a standalone algorithm we applied
our algorithm on the Hebrew and Arabic corpora,
</bodyText>
<footnote confidence="0.812132">
4http://mila.cs.technion.ac.il.
</footnote>
<bodyText confidence="0.999930909090909">
using different parameter settings. We first cal-
culated the word frequencies, then applied initial
segmentation as described in Section 4. Then we
used SRILM (Stolcke, 2002) to learn the trigram
model from the segmented corpus. We utilized
Good-Turing discounting with Katz backoff, and
we gave words that were not in the training set the
constant probability 1E − 9. Finally we utilized
the obtained trigram model to select sentence seg-
mentations. To test the influence of the factor(x)
component of the Rank value, we repeated our
experiment with and without usage of this com-
ponent. We also ran our algorithm with a set of
different threshold T values in order to study the
influence of this parameter.
Tables 1 and 2 show the obtained results for He-
brew and Arabic respectively. Precision is the ra-
tio of correct prefixes to the total number of de-
tected prefixes in the text. Recall is the ratio of pre-
fixes that were split correctly to the total number
of prefixes. Accuracy is the number of correctly
segmented words divided by the total number of
words.
As can be seen from the results, the best F-score
with and without usage of the factor(x) compo-
nent are about the same, but usage of this compo-
nent gives higher precision for the same F-score.
From comparison of Arabic and Hebrew perfor-
mance we can also see that segmentation decisions
for the task in Arabic are likely to be easier, since
the accuracy for T=1 is very high. It means that,
unlike in Hebrew (where the best results were ob-
tained for T=0.79), a word which starts with a pre-
</bodyText>
<page confidence="0.99891">
41
</page>
<table confidence="0.99759975">
Method us k-means random
avg ‘shared meaning’(%) 85 24.61 10
avg triplet score(1-4) 1.57 2.32 3.71
avg category score(1-10) 9.35 6.62 3.5
</table>
<tableCaption confidence="0.982679">
Table 4: Human evaluation results.
</tableCaption>
<table confidence="0.5220336">
abuse, robbery, murder, assault, extortion
good, cheap, beautiful, comfortable
son, daughter, brother, parent
when, how, where
essential, important, central, urgent
</table>
<tableCaption confidence="0.9867725">
Table 5: A sample from the lexical categories dis-
covered in Hebrew (translated to English).
</tableCaption>
<bodyText confidence="0.932582217391305">
fix should generally be segmented.
We also compared our best results to the base-
line and to previous work. The baseline draws a
segmentation uniformly for each word, from the
possible segmentations of the word. In an at-
tempt to partially reproduce (Creutz and Lagus,
2005) on our data, we also compared our results
to the results obtained from Morfessor Categories-
MAP, version 0.9.1 (Described in (Creutz and La-
gus, 2005)). The Morfessor Categories-MAP al-
gorithm gets a list of words and their frequen-
cies, and returns the segmentation for every word.
Since Morfessor may segment words with prefixes
which do not exist in our predefined list of valid
prefixes, we did not segment the words that had
illegal prefixes as segmented by Morfessor.
Results for this comparison are shown in Table
3. Our method significantly outperforms both the
baseline and Morfessor-based segmentation. We
have also tried to improve the language model by
a self training scheme on the same corpus but we
observed only a slight improvement, giving 0.848
Precision and 0.872 Recall.
</bodyText>
<subsectionHeader confidence="0.999468">
5.3 Discovery of word categories
</subsectionHeader>
<bodyText confidence="0.9997063">
We divide the evaluation of the word categories
discovery into two parts. The first is evaluating
the improvement in the quantity of found lexical
categories. The second is evaluating the quality
of these categories. We have applied the algo-
rithm to a Hebrew corpus of size 130MB5, which
is sufficient for a proof of concept. We compared
the output of the categories discovery on two dif-
ferent settings, with function word separation and
without such separation. In both settings we omit-
</bodyText>
<footnote confidence="0.965548">
5Again obtained from the “Mila” Knowledge Center for
Processing Hebrew.
</footnote>
<table confidence="0.997133333333333">
N A J
With Separation 148 4.1 1
No Separation 36 2.9 0
</table>
<tableCaption confidence="0.742189">
Table 6: Lexical categories discovery results com-
parison. N: number of categories. A: average cat-
egory size. J: ‘junk’ words.
</tableCaption>
<bodyText confidence="0.98951">
ted all punctuation symbols. In both runs of the
algorithm we used the same parameters. Eight
symmetric patterns were automatically chosen for
each run. Two of the patterns that were chosen
by the algorithm in the unseparated case were also
chosen in the separated case.
</bodyText>
<subsubsectionHeader confidence="0.643928">
5.3.1 Manual estimation of category quality
</subsubsectionHeader>
<bodyText confidence="0.999954305555555">
Evaluating category quality is challenging since
no exhaustive lists or gold standards are widely
accepted even in English, certainly so in resource-
poor languages such as Hebrew. Hence we follow
the human judgment evaluation scheme presented
in (Davidov and Rappoport, 2006), for the cate-
gories obtained from the segmented corpus.
We compared three methods of word categories
discovery. The first is random sampling of words
into categories. The second is k-means, where
each word is mapped to a vector, and similarity is
calculated as described in (Pantel and Lin, 2002).
We applied k-means to the set of vectors, with sim-
ilarity as a distance function. If a vector had low
similarity with all means, we leave it unattached.
Therefore some clusters contained only one vec-
tor. Running the algorithm 10 times, with different
initial means each time, produced 60 clusters with
three or more words. An interesting phenomenon
we observed is that this method produces very nice
clusters of named entities. The last method is the
one in (Davidov and Rappoport, 2006).
The experiment contained two parts. In Part
I, subjects were given 40 triplets of words and
were asked to rank them using the following scale:
(1) the words definitely share a significant part
of their meaning; (2) the words have a shared
meaning but only in some context; (3) the words
have a shared meaning only under a very un-
usual context/situation; (4) the words do not share
any meaning; (5) I am not familiar enough with
some/all of the words.
The 40 triplets were obtained as follows. 20 of
our categories were selected at random from the
non-overlapping categories we have discovered,
and three words were selected from each of these
</bodyText>
<page confidence="0.996994">
42
</page>
<bodyText confidence="0.998510333333333">
at random. 10 triplets were selected in the same
manner from the categories produced by k-means,
and 10 triplets were selected at random from con-
tent words in the same document.
In Part II, subjects were given the full categories
represented by the triplets that were graded as 1 or
2 in Part I (the full “good” categories in terms of
sharing of meaning). Subjects were asked to grade
the categories from 1 (worst) to 10 (best) accord-
ing to how much the full category had met the ex-
pectations they had when seeing only the triplet.
Nine people participated in the evaluation. A
summary of the results is given in Table 4.
The categories obtained from the unsegmented
corpus are too few and too small for a significant
evaluation. Therefore we applied the evaluation
scheme only for the segmented corpus.
The results from the segmented corpus contain
some interesting categories, with a 100% preci-
sion, like colors, Arab leaders, family members
and cities. An interesting category is {Arabic, En-
glish, Russian, French, German, Yiddish, Polish,
Math}. A sample of some other interesting cate-
gories can be seen in Table 5.
</bodyText>
<sectionHeader confidence="0.5455015" genericHeader="method">
5.3.2 Segmentation effect on category
discovery
</sectionHeader>
<bodyText confidence="0.999931739130435">
In Table 6, we find that there is a major improve-
ment in the number of acquired categories, and an
interesting improvement in the average category
size. One might expect that as a consequence of
an incorrect segmentation of a word, “junk” words
may appear in the discovered categories. As can
be seen, only one “junk” word was categorized.
Throughout this paper we have assumed that
function word properties of languages such as He-
brew and Arabic decrease performance of whole-
word pattern-based concept acquisition methods.
To check this assumption, we have applied the
concept acquisition algorithm on several web-
based corpora of several languages, while choos-
ing corpora size to be exactly equal to the size of
the Hebrew corpus (130Mb) and utilizing exactly
the same parameters. We did not perform quality
evaluation6, but measured the number of concepts
and concept size. Indeed the number of categories
was (190, 170, 159, 162, 150, 29) for Russian, En-
glish, Spanish, French, Turkish and Arabic respec-
tively, clearly inferior for Arabic in comparison to
these European and Slavic languages. A similar
</bodyText>
<footnote confidence="0.962902">
6Brief manual examination suggests no significant drops
in concept quality.
</footnote>
<bodyText confidence="0.999930384615385">
tendency was observed for average concept size.
At the same time prefix separation does help to ex-
tract 148 concepts for Hebrew, making it nearly in-
line with other languages. In contrast, our prelim-
inary experiments on English and Russian suggest
that the effect of applying similar morphological
segmentation on these languages in insignificant.
In order to test whether more data can substi-
tute segmentation even for Hebrew, we have ob-
tained by means of crawling and web queries a
larger (while potentially much more noisy) web-
based 2GB Hebrew corpus which is based on fo-
rum and news contents. Our goal was to estimate
which unsegmented corpus size (if any) can bring
similar performance (in terms of concept number,
size and quality). We gradually increased corpus
size and applied the concept acquisition algorithm
on this corpus. Finally, we have obtained similar,
nearly matching, results to our 130MB corpus for
a 1.2GB Hebrew subcorpus of the 2GB Hebrew
corpus. The results remain stable for 4 different
1.2GB subsets taken from the same 2GB corpus.
This suggests that while segmentation can be sub-
stituted with more data, it may take roughly x10
more data for Hebrew to obtain the same results
without segmentation as with it.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.999956666666666">
We presented a simple method for separating func-
tion word prefixes from words. The method re-
quires very little language-specific knowledge (the
prefixes), and it can be applied to any morpholog-
ically rich language. We showed that this segmen-
tation dramatically improves lexical acquisition in
Hebrew, where nearly ×10 data is required to ob-
tain the same number of concepts without segmen-
tation.
While in this paper we evaluated our framework
on the discovery of concepts, we have recently
proposed fully unsupervised frameworks for the
discovery of different relationship types (Davidov
et al., 2007; Davidov and Rappoport, 2008a; Davi-
dov and Rappoport, 2008b). Many of these meth-
ods are mostly based on function words, and may
greatly benefit from the proposed segmentation
framework.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.941525">
Meni Adler, Michael Elhadad, 2006. An Unsupervised
Morpheme-Based HMM for Hebrew Morphological
Disambiguation. ACL ’06
</reference>
<page confidence="0.995767">
43
</page>
<reference confidence="0.999544242857143">
Roy Bar-Haim, Khalil Simaan, Yoad Winter, 2005.
Choosing an Optimal Architecture for Segmentation
and POS-Tagging of Modern Hebrew. ACL Work-
shop on Computational Approaches to Semitic Lan-
guages ’05.
Mathias Creutz, 2003. Unsupervised Segmentation of
Words Using Prior Distributions of Morph Length
and Frequency. ACL ’03.
Mathias Creutz and Krista Lagus, 2005. Unsuper-
vised Morpheme Segmentation and Morphology In-
duction from Text Corpora Using Morfessor 1.0.
In Computer and Information Science, Report A81,
Helsinki University of Technology.
Sajib Dasgupta and Vincent Ng, 2007. High Perfor-
mance, Language-Independent Morphological Seg-
mentation. NAACL/HLT ’07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
ACL ’06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ’07.
Dmitry Davidov, Ari Rappoport, 2008a. Classification
of Semantic Relationships between Nominals Using
Pattern Clusters. ACL ’08.
Dmitry Davidov, Ari Rappoport, 2008b. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ’08.
Vera Demberg, 2007. A Language-Independent Un-
supervised Model for Morphological Segmentation.
ACL ’07.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using Curvature and Markov Clustering in Graphs
for Lexical Acquisition and Word Sense Discrimi-
nation. MEANING ’05.
Samarth Keshava, Emily Pitler, 2006. A Simpler, Intu-
itive Approach to Morpheme Induction. In Proceed-
ings of 2nd Pascal Challenges Workshop, Venice,
Italy.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, Hany Hassan, 2003. Language Model
Based Arabic Word Segmentation. ACL ’03.
Moshe Levinger, Uzzi Ornan, Alon Itai, 1995. Learn-
ing Morpho-Lexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Comput.
Linguistics, 21:383:404.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ’98.
Uzzi Ornan, 2005. Hebrew in Latin Script. Lesonenu,
LMV:137:151 (in Hebrew).
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ’02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ’04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional Clustering of English Words. ACL
’93.
Benjamin Snyder, Regina Bazilay, 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. ACL/HLT ’08.
Andreas Stolcke, 2002. SRILM – an Extensible Lan-
guage Modeling Toolkit. ICSLP, pages 901-904,
Denver, Colorado.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ’02.
</reference>
<page confidence="0.999275">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.654460">
<title confidence="0.992412">Unsupervised Concept Discovery In Hebrew Using Simple</title>
<author confidence="0.720379">Word Prefix Segmentation for Hebrew</author>
<author confidence="0.720379">Arabic</author>
<affiliation confidence="0.788324">of Computer The Hebrew University of Jerusalem</affiliation>
<abstract confidence="0.998119391304348">Fully unsupervised pattern-based methods for discovery of word categories have been proven to be useful in several languages. The majority of these methods rely on the existence of function words as separate text units. However, in morphology-rich languages, in particular Semitic languages such as Hebrew and Arabic, the equivalents of such function words are usually written as morphemes attached as prefixes to other words. As a result, they are missed by word-based pattern discovery methods, causing many useful patterns to be undetected and a drastic deterioration in performance. To enable high quality lexical category acquisition, we propose a simple unsupervised word segmentation algorithm that separates these morphemes. We study the performance of the algorithm for Hebrew and Arabic, and show that it indeed improves a state-of-art unsupervised concept acquisition algorithm in Hebrew.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Michael Elhadad</author>
</authors>
<title>An Unsupervised Morpheme-Based HMM for Hebrew Morphological Disambiguation.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<contexts>
<context position="7565" citStr="Adler and Elhadad (2006)" startWordPosition="1198" endWordPosition="1202">based on all possible segmentation points. Another type of segmentation algorithms involves utilization of language-specific morphological analyzers for complete morphological disambiguation. In Hebrew each word usually has more than one possible POS (along with other attributes, such as gender, number, etc.). Assuming we have a morphological analyzer (producing the set of possible analyses for a given word), we can try to discover the correct segmentation of each word. Levinger et al. (1995) developed a method for disambiguation of the results provided by a morphological analyzer for Hebrew. Adler and Elhadad (2006) proposed an unsupervised algorithm for word segmentation. They estimate an initial language model (using (Levinger et al., 1995)) and improve this model with EM. Direct comparison to their work is problematic, however, since we avoid utilization of a language-specific morphology/POS analyzer. There are also studies of this type that utilize labeled data (Bar-Haim et al., 2005), where the language model is learned from the training data. Extensive research has been done on word segmentation, where, unlike in our study, the segmentation is evaluated for every word, regardless of its context. Cr</context>
</contexts>
<marker>Adler, Elhadad, 2006</marker>
<rawString>Meni Adler, Michael Elhadad, 2006. An Unsupervised Morpheme-Based HMM for Hebrew Morphological Disambiguation. ACL ’06</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Khalil Simaan</author>
<author>Yoad Winter</author>
</authors>
<title>Choosing an Optimal Architecture for Segmentation and POS-Tagging of Modern Hebrew.</title>
<date>2005</date>
<booktitle>ACL Workshop on Computational Approaches to Semitic Languages ’05.</booktitle>
<contexts>
<context position="7945" citStr="Bar-Haim et al., 2005" startWordPosition="1257" endWordPosition="1260">le analyses for a given word), we can try to discover the correct segmentation of each word. Levinger et al. (1995) developed a method for disambiguation of the results provided by a morphological analyzer for Hebrew. Adler and Elhadad (2006) proposed an unsupervised algorithm for word segmentation. They estimate an initial language model (using (Levinger et al., 1995)) and improve this model with EM. Direct comparison to their work is problematic, however, since we avoid utilization of a language-specific morphology/POS analyzer. There are also studies of this type that utilize labeled data (Bar-Haim et al., 2005), where the language model is learned from the training data. Extensive research has been done on word segmentation, where, unlike in our study, the segmentation is evaluated for every word, regardless of its context. Creutz (2003) presents an algorithm for unsupervised segmentation under these assumptions. He proposes a probabilistic model which 37 utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with </context>
</contexts>
<marker>Bar-Haim, Simaan, Winter, 2005</marker>
<rawString>Roy Bar-Haim, Khalil Simaan, Yoad Winter, 2005. Choosing an Optimal Architecture for Segmentation and POS-Tagging of Modern Hebrew. ACL Workshop on Computational Approaches to Semitic Languages ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
</authors>
<title>Unsupervised Segmentation of Words Using Prior Distributions of Morph Length and Frequency.</title>
<date>2003</date>
<journal>ACL</journal>
<volume>03</volume>
<contexts>
<context position="8176" citStr="Creutz (2003)" startWordPosition="1297" endWordPosition="1298">6) proposed an unsupervised algorithm for word segmentation. They estimate an initial language model (using (Levinger et al., 1995)) and improve this model with EM. Direct comparison to their work is problematic, however, since we avoid utilization of a language-specific morphology/POS analyzer. There are also studies of this type that utilize labeled data (Bar-Haim et al., 2005), where the language model is learned from the training data. Extensive research has been done on word segmentation, where, unlike in our study, the segmentation is evaluated for every word, regardless of its context. Creutz (2003) presents an algorithm for unsupervised segmentation under these assumptions. He proposes a probabilistic model which 37 utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with and without a prefix. Other recent studies that follow the context-independent setup include (Creutz and Lagus, 2005; Keshava and Pitler, 2005; Demberg, 2007). They test their methods on English, Finnish and Turkish. All of these s</context>
</contexts>
<marker>Creutz, 2003</marker>
<rawString>Mathias Creutz, 2003. Unsupervised Segmentation of Words Using Prior Distributions of Morph Length and Frequency. ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised Morpheme Segmentation and Morphology Induction from Text Corpora Using Morfessor 1.0.</title>
<date>2005</date>
<booktitle>In Computer and Information Science, Report A81,</booktitle>
<institution>Helsinki University of Technology.</institution>
<contexts>
<context position="8661" citStr="Creutz and Lagus, 2005" startWordPosition="1369" endWordPosition="1372"> on word segmentation, where, unlike in our study, the segmentation is evaluated for every word, regardless of its context. Creutz (2003) presents an algorithm for unsupervised segmentation under these assumptions. He proposes a probabilistic model which 37 utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with and without a prefix. Other recent studies that follow the context-independent setup include (Creutz and Lagus, 2005; Keshava and Pitler, 2005; Demberg, 2007). They test their methods on English, Finnish and Turkish. All of these studies, however, assume contextindependency of segmentation, disregarding the ambiguity that may come from context. This makes it problematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic al</context>
<context position="26104" citStr="Creutz and Lagus, 2005" startWordPosition="4363" endWordPosition="4366">1-4) 1.57 2.32 3.71 avg category score(1-10) 9.35 6.62 3.5 Table 4: Human evaluation results. abuse, robbery, murder, assault, extortion good, cheap, beautiful, comfortable son, daughter, brother, parent when, how, where essential, important, central, urgent Table 5: A sample from the lexical categories discovered in Hebrew (translated to English). fix should generally be segmented. We also compared our best results to the baseline and to previous work. The baseline draws a segmentation uniformly for each word, from the possible segmentations of the word. In an attempt to partially reproduce (Creutz and Lagus, 2005) on our data, we also compared our results to the results obtained from Morfessor CategoriesMAP, version 0.9.1 (Described in (Creutz and Lagus, 2005)). The Morfessor Categories-MAP algorithm gets a list of words and their frequencies, and returns the segmentation for every word. Since Morfessor may segment words with prefixes which do not exist in our predefined list of valid prefixes, we did not segment the words that had illegal prefixes as segmented by Morfessor. Results for this comparison are shown in Table 3. Our method significantly outperforms both the baseline and Morfessor-based segm</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus, 2005. Unsupervised Morpheme Segmentation and Morphology Induction from Text Corpora Using Morfessor 1.0. In Computer and Information Science, Report A81, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sajib Dasgupta</author>
<author>Vincent Ng</author>
</authors>
<title>High Performance, Language-Independent Morphological Segmentation.</title>
<date>2007</date>
<journal>NAACL/HLT</journal>
<volume>07</volume>
<contexts>
<context position="8429" citStr="Dasgupta and Ng (2007)" startWordPosition="1332" endWordPosition="1335">tion of a language-specific morphology/POS analyzer. There are also studies of this type that utilize labeled data (Bar-Haim et al., 2005), where the language model is learned from the training data. Extensive research has been done on word segmentation, where, unlike in our study, the segmentation is evaluated for every word, regardless of its context. Creutz (2003) presents an algorithm for unsupervised segmentation under these assumptions. He proposes a probabilistic model which 37 utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with and without a prefix. Other recent studies that follow the context-independent setup include (Creutz and Lagus, 2005; Keshava and Pitler, 2005; Demberg, 2007). They test their methods on English, Finnish and Turkish. All of these studies, however, assume contextindependency of segmentation, disregarding the ambiguity that may come from context. This makes it problematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal i</context>
</contexts>
<marker>Dasgupta, Ng, 2007</marker>
<rawString>Sajib Dasgupta and Vincent Ng, 2007. High Performance, Language-Independent Morphological Segmentation. NAACL/HLT ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Efficient Unsupervised Discovery of Word Categories Using Symmetric Patterns and High Frequency Words.</title>
<date>2006</date>
<journal>ACL</journal>
<volume>06</volume>
<contexts>
<context position="2618" citStr="Davidov and Rappoport, 2006" startWordPosition="411" endWordPosition="414">f the language grows, as our lexical knowledge comes solely from a corpus (words appear with and without the function morphemes); (2) information derived from the presence of these morphemes in the sentence is usually lost. In this paper we address the important task of a fully unsupervised acquisition of Hebrew lexical categories (or concepts – words sharing a significant aspect of their meaning). We are not aware of any previous work on this task for Hebrew. Due to the problem above, the performance of many acquisition algorithms deteriorates unacceptably. This happens, for example, in the (Davidov and Rappoport, 2006) algorithm that utilizes automatically detected function words as the main building block for pattern construction. In order to overcome this problem, one should separate such prefixes from the compound words (words consisting of function morphemes attached to content words) in the input corpus. When we consider some particular word, there are frequently many options to split it to smaller strings. Fortunately, the set of function words is small and closed, and the set of grammatical sequences of function prefixes is also small. Hence we assume it does not cost us much to know in advance what </context>
<context position="10198" citStr="Davidov and Rappoport (2006)" startWordPosition="1614" endWordPosition="1617">atures in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). Some recent studies have proposed frameworks that attempt to avoid any implicit or explicit prespecification of patterns. Davidov and Rappoport (2006) proposed a method that detects function words by their high frequency, and utilizes these words for the discovery of symmetric patterns. Their method is based on two assumptions: (1) some function words in the language symmetrically connect words belonging to the same category; (2) such function words can be detected as the most frequent words in language. While these assumptions are reasonable for many languages, for some morphologically rich languages the second assumption may fail. This is due to the fact that some languages like Hebrew and Arabic may express relationships not by isolated </context>
<context position="12022" citStr="Davidov and Rappoport, 2006" startWordPosition="1908" endWordPosition="1911">y good results for English and Russian. However, no previous work applies a fully unsupervised concept acquisition for Hebrew. In our study we combine their concept acquisition framework with a simple unsupervised word segmentation technique. Our evaluation confirms the weakness of word-based frameworks for morphology-rich languages such as Hebrew, and shows that utilizing the proposed word segmentation can overcome this weakness while keeping the concept acquisition approach fully unsupervised. 3 Unsupervised Discovery of Word Categories In this study we use word segmentation to improve the (Davidov and Rappoport, 2006) method for discovery of word categories, sets of words sharing a significant aspect of their meaning. An example for such a discovered category is the set of verbs {dive, snorkel, swim, float, surf, sail, drift, ...}. Below we briefly describe this category acquisition algorithm. The algorithm consists of three stages as follows. First, it discovers a set of pattern candidates, which are defined by a combination of high frequency words (denoted by H) and slots for low frequency (content) words (denoted by C). An example for such a pattern candidate is ‘x belongs to y’, where ‘x’ and ‘y’ stand</context>
<context position="14167" citStr="Davidov and Rappoport, 2006" startWordPosition="2279" endWordPosition="2282">nt words appear in the pattern such that ‘x’ precedes ‘y’. Then we calculate several symmetry measures on the graph structure and select the patterns with best values for these measures. The third stage is the generation of categories. We extract tightly connected sets of words from the unified graph which combines all graphs of selected patterns. Such sets of words define the desired categories. The patterns which include the ‘x and y’ substring are among the most useful patterns for generation of categories (they were used in (Dorow et al., 2005) and discovered in all 5 languages tested in (Davidov and Rappoport, 2006)). However, in Hebrew such patterns can not be found in the same way, since the function word ‘and’ is the prefix ‘w’ and not a standalone high frequency word. Another popular set of patterns are ones including ‘x or y’. Such patterns can be identified in Hebrew, as ‘or’ in Hebrew is a separate word. However, even in this case, the content word represented by ‘x’ or ‘y’ may appear with a prefix. This damages the construction of the pattern graph, since two different nodes may be created instead of one – one for a regular content word, the other for the same word with a prefix. Consequently, it</context>
<context position="28254" citStr="Davidov and Rappoport, 2006" startWordPosition="4712" endWordPosition="4715">verage category size. J: ‘junk’ words. ted all punctuation symbols. In both runs of the algorithm we used the same parameters. Eight symmetric patterns were automatically chosen for each run. Two of the patterns that were chosen by the algorithm in the unseparated case were also chosen in the separated case. 5.3.1 Manual estimation of category quality Evaluating category quality is challenging since no exhaustive lists or gold standards are widely accepted even in English, certainly so in resourcepoor languages such as Hebrew. Hence we follow the human judgment evaluation scheme presented in (Davidov and Rappoport, 2006), for the categories obtained from the segmented corpus. We compared three methods of word categories discovery. The first is random sampling of words into categories. The second is k-means, where each word is mapped to a vector, and similarity is calculated as described in (Pantel and Lin, 2002). We applied k-means to the set of vectors, with similarity as a distance function. If a vector had low similarity with all means, we leave it unattached. Therefore some clusters contained only one vector. Running the algorithm 10 times, with different initial means each time, produced 60 clusters with</context>
</contexts>
<marker>Davidov, Rappoport, 2006</marker>
<rawString>Dmitry Davidov, Ari Rappoport, 2006. Efficient Unsupervised Discovery of Word Categories Using Symmetric Patterns and High Frequency Words. ACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
<author>Moshe Koppel</author>
</authors>
<title>Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007. Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Classification of Semantic Relationships between Nominals Using Pattern Clusters.</title>
<date>2008</date>
<journal>ACL</journal>
<volume>08</volume>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov, Ari Rappoport, 2008a. Classification of Semantic Relationships between Nominals Using Pattern Clusters. ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Ari Rappoport</author>
</authors>
<title>Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions.</title>
<date>2008</date>
<journal>ACL</journal>
<volume>08</volume>
<marker>Davidov, Rappoport, 2008</marker>
<rawString>Dmitry Davidov, Ari Rappoport, 2008b. Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions. ACL ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vera Demberg</author>
</authors>
<title>A Language-Independent Unsupervised Model for Morphological Segmentation.</title>
<date>2007</date>
<journal>ACL</journal>
<volume>07</volume>
<contexts>
<context position="8703" citStr="Demberg, 2007" startWordPosition="1377" endWordPosition="1378"> the segmentation is evaluated for every word, regardless of its context. Creutz (2003) presents an algorithm for unsupervised segmentation under these assumptions. He proposes a probabilistic model which 37 utilizes the distributions of morpheme length and frequency to estimate the quality of the induced morphemes. Dasgupta and Ng (2007) improves over (Creutz, 2003) by suggesting a simpler approach. They segment a prefix using the word frequency with and without a prefix. Other recent studies that follow the context-independent setup include (Creutz and Lagus, 2005; Keshava and Pitler, 2005; Demberg, 2007). They test their methods on English, Finnish and Turkish. All of these studies, however, assume contextindependency of segmentation, disregarding the ambiguity that may come from context. This makes it problematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algo</context>
</contexts>
<marker>Demberg, 2007</marker>
<rawString>Vera Demberg, 2007. A Language-Independent Unsupervised Model for Morphological Segmentation. ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Dominic Widdows</author>
<author>Katarina Ling</author>
<author>JeanPierre Eckmann</author>
<author>Danilo Sergi</author>
<author>Elisha Moses</author>
</authors>
<title>Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination.</title>
<date>2005</date>
<journal>MEANING</journal>
<volume>05</volume>
<contexts>
<context position="9965" citStr="Dorow et al., 2005" startWordPosition="1579" endWordPosition="1582">t feature vectors and pattern-based discovery. The first approach is to map each word to a feature vector and cluster these vectors. Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). Some recent studies have proposed frameworks that attempt to avoid any implicit or explicit prespecification of patterns. Davidov and Rappoport (2006) proposed a method that detects function words by their high frequency, and utilizes these words for the discovery of symmetric patterns. Their method is based on two assumptions: (1) some function words in the language symmetrically connect words belonging to the same category; (2) such function words can be detected as the most frequent words in language. While t</context>
<context position="14093" citStr="Dorow et al., 2005" startWordPosition="2267" endWordPosition="2270"> directed arc from the node ‘x’ to ‘y’ if the corresponding content words appear in the pattern such that ‘x’ precedes ‘y’. Then we calculate several symmetry measures on the graph structure and select the patterns with best values for these measures. The third stage is the generation of categories. We extract tightly connected sets of words from the unified graph which combines all graphs of selected patterns. Such sets of words define the desired categories. The patterns which include the ‘x and y’ substring are among the most useful patterns for generation of categories (they were used in (Dorow et al., 2005) and discovered in all 5 languages tested in (Davidov and Rappoport, 2006)). However, in Hebrew such patterns can not be found in the same way, since the function word ‘and’ is the prefix ‘w’ and not a standalone high frequency word. Another popular set of patterns are ones including ‘x or y’. Such patterns can be identified in Hebrew, as ‘or’ in Hebrew is a separate word. However, even in this case, the content word represented by ‘x’ or ‘y’ may appear with a prefix. This damages the construction of the pattern graph, since two different nodes may be created instead of one – one for a regular</context>
</contexts>
<marker>Dorow, Widdows, Ling, Eckmann, Sergi, Moses, 2005</marker>
<rawString>Beate Dorow, Dominic Widdows, Katarina Ling, JeanPierre Eckmann, Danilo Sergi, Elisha Moses, 2005. Using Curvature and Markov Clustering in Graphs for Lexical Acquisition and Word Sense Discrimination. MEANING ’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samarth Keshava</author>
<author>Emily Pitler</author>
</authors>
<title>A Simpler, Intuitive Approach to Morpheme Induction.</title>
<date>2006</date>
<booktitle>In Proceedings of 2nd Pascal Challenges Workshop,</booktitle>
<location>Venice, Italy.</location>
<marker>Keshava, Pitler, 2006</marker>
<rawString>Samarth Keshava, Emily Pitler, 2006. A Simpler, Intuitive Approach to Morpheme Induction. In Proceedings of 2nd Pascal Challenges Workshop, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Kishore Papineni, Salim Roukos, Ossama Emam, Hany Hassan,</title>
<date>2003</date>
<journal>ACL</journal>
<volume>03</volume>
<marker>Lee, 2003</marker>
<rawString>Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, Hany Hassan, 2003. Language Model Based Arabic Word Segmentation. ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Levinger</author>
</authors>
<title>Uzzi Ornan, Alon Itai,</title>
<date>1995</date>
<journal>Comput. Linguistics,</journal>
<pages>21--383</pages>
<marker>Levinger, 1995</marker>
<rawString>Moshe Levinger, Uzzi Ornan, Alon Itai, 1995. Learning Morpho-Lexical Probabilities from an Untagged Corpus with an Application to Hebrew. Comput. Linguistics, 21:383:404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<journal>COLING</journal>
<volume>98</volume>
<contexts>
<context position="9548" citStr="Lin, 1998" startWordPosition="1512" endWordPosition="1513">d methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algorithmic approaches are clustering of context feature vectors and pattern-based discovery. The first approach is to map each word to a feature vector and cluster these vectors. Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). Some recent studies have proposed frameworks that attempt to avoid any implicit or explicit prespeci</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin, 1998. Automatic Retrieval and Clustering of Similar Words. COLING ’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uzzi Ornan</author>
</authors>
<date>2005</date>
<booktitle>Hebrew in Latin Script. Lesonenu, LMV:137:151 (in Hebrew).</booktitle>
<contexts>
<context position="5894" citStr="Ornan, 2005" startWordPosition="939" endWordPosition="940">iven in Section 5. 2 Related Work In this paper we develop an unsupervised framework for segmentation of the function words for languages where context is important for correct segmentation. Our main target language is Hebrew, and we experimented with Arabic as well. As far as we know, there is no work on unsupervised segmentation of words in Hebrew which does not utilize language-specific tools such as morphological analyzers. Lee et al. (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach. As in their study, we 1Transcription is according to (Ornan, 2005), except for Shin which is denoted by “$”. also have a pre-supplied list of possible prefix sequences and assume a trigram model in order to find the most probable morpheme sequence. Both studies evaluate performance on a segmented text, and not just on words in the lexicon. However, their algorithm, while achieving good performance (97% accuracy), relies on a training set – a manually segmented corpus of about 110,000 words, while our unsupervised framework does not require any annotation and is thus easier to implement and to apply to different domains and languages. Snyder and Barzilay (200</context>
</contexts>
<marker>Ornan, 2005</marker>
<rawString>Uzzi Ornan, 2005. Hebrew in Latin Script. Lesonenu, LMV:137:151 (in Hebrew).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering Word Senses from Text.</title>
<date>2002</date>
<journal>SIGKDD</journal>
<volume>02</volume>
<contexts>
<context position="9624" citStr="Pantel and Lin (2002)" startWordPosition="1523" endWordPosition="1526">Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algorithmic approaches are clustering of context feature vectors and pattern-based discovery. The first approach is to map each word to a feature vector and cluster these vectors. Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). Some recent studies have proposed frameworks that attempt to avoid any implicit or explicit prespecification of patterns. Davidov and Rappoport (2006) proposed a method that de</context>
<context position="28551" citStr="Pantel and Lin, 2002" startWordPosition="4761" endWordPosition="4764">case. 5.3.1 Manual estimation of category quality Evaluating category quality is challenging since no exhaustive lists or gold standards are widely accepted even in English, certainly so in resourcepoor languages such as Hebrew. Hence we follow the human judgment evaluation scheme presented in (Davidov and Rappoport, 2006), for the categories obtained from the segmented corpus. We compared three methods of word categories discovery. The first is random sampling of words into categories. The second is k-means, where each word is mapped to a vector, and similarity is calculated as described in (Pantel and Lin, 2002). We applied k-means to the set of vectors, with similarity as a distance function. If a vector had low similarity with all means, we leave it unattached. Therefore some clusters contained only one vector. Running the algorithm 10 times, with different initial means each time, produced 60 clusters with three or more words. An interesting phenomenon we observed is that this method produces very nice clusters of named entities. The last method is the one in (Davidov and Rappoport, 2006). The experiment contained two parts. In Part I, subjects were given 40 triplets of words and were asked to ran</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel, Dekang Lin, 2002. Discovering Word Senses from Text. SIGKDD ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Deepak Ravichandran, Eduard Hovy,</title>
<date>2004</date>
<journal>COLING</journal>
<volume>04</volume>
<marker>Pantel, 2004</marker>
<rawString>Patrick Pantel, Deepak Ravichandran, Eduard Hovy, 2004. Towards Terascale Knowledge Acquisition. COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional Clustering of English Words.</title>
<date>1993</date>
<journal>ACL</journal>
<volume>93</volume>
<contexts>
<context position="9532" citStr="Pereira et al., 1993" startWordPosition="1507" endWordPosition="1510">ematic to apply the proposed methods to context-dependent morphology types as in Hebrew and Arabic. The guiding goal in the present paper is the concept acquisition problem. Concept acquisition of different kinds has been studied extensively. The two main classification axes for this task are the type of human input and annotation, and the basic algorithmic approach used. The two main algorithmic approaches are clustering of context feature vectors and pattern-based discovery. The first approach is to map each word to a feature vector and cluster these vectors. Example of such algorithms are (Pereira et al., 1993) and (Lin, 1998) that use syntactic features in the vector definition. Pantel and Lin (2002) improves on the latter by clustering by committee. Recently, there is a growing interest in the second main algorithmic approach, usage of lexicosyntactic patterns. Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004). Thus (Dorow et al., 2005) discover categories using two basic pre-specified patterns (“x and y”, “x or y”). Some recent studies have proposed frameworks that attempt to avoid any implicit or e</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, Lillian Lee, 1993. Distributional Clustering of English Words. ACL ’93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Regina Bazilay</author>
</authors>
<title>Unsupervised Multilingual Learning for Morphological Segmentation.</title>
<date>2008</date>
<journal>ACL/HLT</journal>
<volume>08</volume>
<marker>Snyder, Bazilay, 2008</marker>
<rawString>Benjamin Snyder, Regina Bazilay, 2008. Unsupervised Multilingual Learning for Morphological Segmentation. ACL/HLT ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>ICSLP,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado.</location>
<contexts>
<context position="24023" citStr="Stolcke, 2002" startWordPosition="4009" endWordPosition="4010">Arabic, we used a 7MB Arabic news items corpus, and a similarly manually annotated test text of 4715 words. The Arabic corpus is too small for meaningful category discovery, so we used it only in the segmentation evaluation. 5.2 Evaluation of segmentation framework In order to estimate the performance of word segmentation as a standalone algorithm we applied our algorithm on the Hebrew and Arabic corpora, 4http://mila.cs.technion.ac.il. using different parameter settings. We first calculated the word frequencies, then applied initial segmentation as described in Section 4. Then we used SRILM (Stolcke, 2002) to learn the trigram model from the segmented corpus. We utilized Good-Turing discounting with Katz backoff, and we gave words that were not in the training set the constant probability 1E − 9. Finally we utilized the obtained trigram model to select sentence segmentations. To test the influence of the factor(x) component of the Rank value, we repeated our experiment with and without usage of this component. We also ran our algorithm with a set of different threshold T values in order to study the influence of this parameter. Tables 1 and 2 show the obtained results for Hebrew and Arabic resp</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke, 2002. SRILM – an Extensible Language Modeling Toolkit. ICSLP, pages 901-904, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
<author>Beate Dorow</author>
</authors>
<title>A Graph Model for Unsupervised Lexical Acquisition.</title>
<date>2002</date>
<journal>COLING</journal>
<volume>02</volume>
<contexts>
<context position="13407" citStr="Widdows and Dorow, 2002" startWordPosition="2146" endWordPosition="2149">of up to 4 2They do not include any specific words, only a relative order of high/low frequency words, and hence can be used on 38 words in total, from which two are (non-adjacent) content words. Four meta-patterns are used: CHC, CHCH, CHHC, HCHC. Second, those patterns which give rise to symmetric lexical relationships are identified. The meaning of phrases constructed from those patterns is (almost) invariant to the order of the content words contained in them. An example for such a pattern is ‘x and y’. In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node ‘x’ to ‘y’ if the corresponding content words appear in the pattern such that ‘x’ precedes ‘y’. Then we calculate several symmetry measures on the graph structure and select the patterns with best values for these measures. The third stage is the generation of categories. We extract tightly connected sets of words from the unified graph which combines all graphs of selected patterns. Such sets of words define the desired categories. The patterns which include the ‘x and y’ substring are among the mos</context>
</contexts>
<marker>Widdows, Dorow, 2002</marker>
<rawString>Dominic Widdows, Beate Dorow, 2002. A Graph Model for Unsupervised Lexical Acquisition. COLING ’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>