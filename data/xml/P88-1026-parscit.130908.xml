<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.983654">
Lexicon and grammar in probabilistic tagging
of written English.
</title>
<author confidence="0.988084">
Andrew David Beale
</author>
<affiliation confidence="0.981329">
Unit for Computer Research on the English Language
University of Lancaster
</affiliation>
<address confidence="0.688389">
Bailrigg, Lancaster
England LAI 4YT
</address>
<email confidence="0.734428">
enb02.1(guk.aciancs.vaxl
</email>
<sectionHeader confidence="0.989625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999932214285714">
The paper describes the development of software for
automatic grammatical analysis of unrestricted, unedited
English text at the Unit for Computer Research on the English
Language (UCREL) at the University of Lancaster. The work
is currently funded by IBM and carried out in collaboration
with colleagues at IBM UK (Winchester) and IBM Yorktown
Heights. The paper will focus on the lexicon component of the
word tagging system. the UCREL grammar, the databanks of
parsed sentences, and the tools that have been written to
support development of these components. This work has
applications to speech technology, spelling correction. and
other areas of natural language processing. Currently, our goal
is to provide a language model using transition statistics to
disambiguate alternative parses for a speech recognition device.
</bodyText>
<sectionHeader confidence="0.953208" genericHeader="keywords">
1. Text Corpora
</sectionHeader>
<bodyText confidence="0.9999809375">
Historically, the use of text corpora to provide empirical
data for testing grammatical theories has been regarded as
important to varying degrees by philologists and linguists of
differing persuasions. The use of corpus citations in grammars
and dictionaries pm-dates electronic data processing (Brown,
1984: 34). While most of the generative grammarians of the
60s and 70s ignored campus data, the increased power of the
new technology nevertheless points the way to new
applications of computerized text corpora in dictionary making,
style checking and speech recognition. Computer corpora
present the computational linguist with the diversity and
complexity of real language which is more challenging for
testing language models than intuitively derived examples.
Ultimately grammars must be judged by their ability to
contend with the real facts of language and not just basic
constructs extrapolated by grammarians.
</bodyText>
<sectionHeader confidence="0.973312" genericHeader="introduction">
2. Word Tagging
</sectionHeader>
<bodyText confidence="0.999741272727273">
The system devised for automatic word tagging or part of
speech selection for processing running English text, known as
the Constituent-Likelihood Automatic Word-tagging System
(CLAWS) (Garside et aL, 1987) serves as the basis for the
current woric. The word tagging system is an automated
component of the probabilistic parsing system we are currently
working on. In word tagging, each of the nmning words in the
corpus text to be processed is associated with a pie-terminal
symbol, denoting word class. In essence. the CLAWS suite can
be conceptually divided into two phases: tag assignment and
tag selection.
</bodyText>
<figure confidence="0.907489090909091">
constable NNS1 NNS1: NP1:
constant JJ NN1
constituent NN1
constitutional JJ NN1@
construction NN1
consultant NN1
consummate JJ VVO
contact NN1 VVO
contained VVD wig JJâ‚¬
containing VVG NN1%
contemporary JJ NNIA
content NN1 JJ VVO@
contessa NNS1 NNS1:
contest NN1 VV0e
contestant NN1
continue VVO
continued VVD VVN J1:1@
contraband NN1 JJ
contract NN1 VVO@
contradictory JJ
contrary JJ NN1
contrast NN1 VVO@
</figure>
<figureCaption confidence="0.999859">
Figure 1: Section of the CLAWS Lexicon
</figureCaption>
<bodyText confidence="0.999742833333333">
JB = attributive adjective; JJ = general adjective; NN1 =
singular.common noun; NNS1 = noun of style or title; NPI =
singular proper noun; VVO = base form of lexical verb; VVD
= past tense of lexical verb; VVG = -ing form of lexical verb;
VVN = past participle of lexical verb; %, @ = probability
markers; = word initial capital marker.
</bodyText>
<page confidence="0.99722">
211
</page>
<bodyText confidence="0.9996492">
cent of running words are correctly disambiguated in this way.)
Exceptions are dealt with by invoking a look up procedure that
searches through a limited list of groups of two or more
words, or by automatically adjusting the probabilities of
sequences of three tags in cases when the intermediate tag is
misleading.
The current version of the CLAWS system requires no pre-
editing and attributes the correct word tag to over 96 per cern
of the input miming words, leaving 3 to 4 per cern to be
corrected by human post-editors.
</bodyText>
<sectionHeader confidence="0.997337" genericHeader="method">
3. Error Analysis
</sectionHeader>
<bodyText confidence="0.999680285714286">
Error analysis of CLAWS output has resulted, and
condnues to result, in diverse improvements to the system,
from the simple adjustment of probability weightings against
tags in the lexicon to the inclusion of additional procedures,
for instance to deal with the distinction between proper names
and common nouns.
Parts of the system can also be used to develop new parts.
to extend existing pans, or to interface with other systems. For
instance, in order to produce a lexicon sufficiently large and
detailed enough for parsing, we needed to extend the original
list of about 8,000 entries to over 20.000 (the new CLAWS
lexicon contains about 26,500 entries).&apos; In order to do this, a
list of 15,000 words not already in the CLAWS lexicon was
tagged using the CLAWS tag assignment program. (Since they
were not already in the lexicon, the candidate tags for each
new entry were assigned by suffudist lookup or default tag
assignment.) The new list was then post-edited by interactive
screen editing and merged with the old lexicon.
Another example of &apos;self improvement&apos; is in the production
of a better set of one-step transition probabilities. The first
CLAWS system used a matrix of tag transition probabilities
derived from the tagged Brown corpus (Francis and Kaera,
1982). Some cells of this matrix were inaccurate because of
incompatibility of the Brown tagset and the CLAWS tagset. To
remedy this, a new matrix was created by a statistics-gathering
program that processed the post-edited version of a corpus of
one million words tagged by the original CLAWS suite of
programs.
</bodyText>
<sectionHeader confidence="0.985561" genericHeader="method">
4. Subcategorization
</sectionHeader>
<bodyText confidence="0.943647826086956">
Apart from extending the vocabulary coverage of the
CLAWS lexicon, we are also subcategorizing words belonging
to the major word classes in order to reduce the over-
generation of alternative panes of sentences of greater than
trivial length. The task of subcategolization involves:
(1) a linguist&apos;s specification of a schema or typology of
lexical subcategories based on distributional and
Tag assignment involves, for each input running word or
punctuation mark, lexicon look-up, which provides one or
more potential word tags for each input word or punctuation
mark. The lexicon is a list of about 8.000 records containing
fields for
(1) the word form
(2) the set of one or more candidate tags denoting the word&apos;s
word class(es) with probability markers attached
indicating three different levels of probability.
Words not in the CLAWS lexicon are assigned potential
tags either by suffixlist look-up, which attempts to match end
characters of the input word with a suffix in the suffudist, or.
if the input word does not have a word-ending to match one of
these entries, default tags are assigned. The procedures ensure
that rare words and neologisms not in the lexicon are still
given an analysis.
</bodyText>
<figure confidence="0.998902285714286">
de NN1
ade NN1
made JJ
ede VVO
ide NN1
side NN1
wide JJ
oxide NN1
ode NN1
ude VVO
tude NN1
ee NN1
free JJ
fe NN1
ge NN1
dge NN1
ridge NN1
NP1:
VVO NP1:
VVO
NP1:
</figure>
<figureCaption confidence="0.999981">
Figure 2: Section of the Suffudist
</figureCaption>
<bodyText confidence="0.999983555555556">
Tag selection disambiguates the alternative tags that are
assigned to some of the running words. Disambiguation is
achieved by invoking one-step probabilities of tag pair
likelihoods extracted from a previously tagged training corpus
and upgrading or downgrading likelihoods according to the
probability markers against word tags in the lexicon or
suffixlist. In the majority of cases, this first order Markov
model is sufficient to correctly select the most likely sequence
of tags associated with the input running text. (Over 90 per
</bodyText>
<page confidence="0.99513">
212
</page>
<sectionHeader confidence="0.755227" genericHeader="method">
3
</sectionHeader>
<bodyText confidence="0.833572473684211">
functional criteria.
(2) a lexicographer&apos;s judgement in assigning one or more of
the subcategory codes in the linguist&apos;s schema to the
major lexical word forms (verbs, nouns, adjectives).
The amount of detail demarcated by the subcategorization
typology is dependent. in part, on the practical requirements of
the system. Existing subcategorization systems, such as the one
provided in the Longman Dictionary of Contemporary English
(1978) or Sager&apos;s (1981) subcategories, need to be taken into
account. But these are assessed critically rather than adopted
wholesale (see for instance Akkaman et al.. 1985 and
Boguraev et at.. 1987, for a discussion of the strengths and
weaknesses of the LDOCE grammar codes).
[1] intransirive verb : ache, age, allow, care, conflict, escape.
fish, occur, reply, snow, stay, sun-bathe, swoon, talk, vanish.
[2] transitive verb : abandon, abhor, allow, build, complete,
contain, demand, exchange, get, give, house, keep, mail,
master, oppose, pardon. spend, strengthen, warn.
(3]copular verb: appear, become, feel, get, grow, remain,
seem.
[4] prepositional verb : abstract, aim, ask, belong, cater.
consist, prey, pry, search, vote.
[5] phrasal verb : blow, build, cry, dons, ease, farm, fill,
hand, jazz, look, open, pop, share, wort.
[6] verb followed by that-clause : accept, believe. demand,
doubt, feel, guess, know, maintain, reckon, require, think.
[7] verb followed by to-infinitive : ask, come, dare, demand,
fail, hope, intend, need, prefer, propose, refuse, seem, try,
wish.
[8] verb followed by -big construction : abhor, begin.
continue, deny, dislike, enjoy, keep, recall, remember, risk,
suggest.
[9] anibitransitive verb : accept, answer, close, compile, cook,
develop, feed, fly, move, obey, practice, quit, sing, stop, teach.
[A] verb habitually followed by an adverbial : appear, come,
go, keep, lie, live, move, put, sit, stand, swim, veer.
[W] verb followed by a wit-clause : ask, choose, doubt.
imagine, know, matter, mind, wonder.
</bodyText>
<figureCaption confidence="0.997017">
Figure 3: The initial schema of eleven verb subcategories
</figureCaption>
<bodyText confidence="0.999887076923077">
We began subcategorization of the CLAWS lexicon by
word-tagging the 3,000 most frequent words in the Brown
corpus (Kutera and Francis, 1967). An initial system of eleven
verb subcategories was proposed, and judgements about which
subcategory(ies) each verb belonged to were empirically tested
by looking up entries in the microfiche concordance of the
tagged Lancaster/Oslo-Bergen corpus (Holland and Johansson.
1982; Johansson et al. 1986) which shows every occurrence of
a tagged word in the corpus together with its context.
About 2,500 verbs have been coded in this way, and we are
now working on a more detailed system of about 80 different
verb subcategories using the Lexicon Development
Environment of Boguraev et al. (1987).
</bodyText>
<sectionHeader confidence="0.962674" genericHeader="method">
5. Constituent Analysis
</sectionHeader>
<bodyText confidence="0.9997464">
The task of implementing a probabilistic parsing algorithm
to provide a disambiguated constituent analysis of unrestricted
English is more demanding than implementing the word
tagging suite, not least because, in order to operate in a
manner similar to the word-tagging model, the system requires
</bodyText>
<listItem confidence="0.91345">
(1) specification of an appropriate grammar of rules and
symbols and
(2) the construction of a sufficiently large databank of parsed
</listItem>
<bodyText confidence="0.832097714285714">
sentences conforming to the (optimal) grammar specified
in (1) to provide statistics of&apos; the relative likelihoods of
constituent tag transitions for constituent tag
disambiguation.
In order to meet these prior requirements, researchers have
been employed on a full-time basis to assemble a corpus of
parsed sentences.
</bodyText>
<sectionHeader confidence="0.974651" genericHeader="method">
6. Grammar Development and Parsed
Subcorpora
</sectionHeader>
<bodyText confidence="0.999947454545455">
The databank of approximately 45,000 words of manually
parsed sentences of the Lancaster/Oslo-Bergen corpus
(Sampson, 1987: 83ff). was processed to .show the distinct
types of production rules and their frequency of occurrence in
the grammar associated with the Sampson neebank. Experience
of the UCREL probabilistic system (Garside and Leech. 1987:
66ff) and suggestions from other researchers prompting new
rules resulted in a new context-free grammar of about 6,000
productions creating more steeply nested structures than those
of the Sampson grammar. (It was anticipated that steeper
nesting would reduce the size of the treebank required to
obtain adequate frequency statistics.) The new grammar is
defined descriptively in a Parser&apos;s Manual (Leech, 1987) and
formalised as a set of context-free phrase-structure productions.
Development of the grammar then proceeded in tandem
with the construction of a second databank of parsed sentences,
fitting, as closely as possible, the rules expressed by the
grammar. The new databank comprises extracts from
newspaper reports daring from 1979-80 in the Associated Press
(AP) corpus. Any difficulties the grammarians had in parsing
were resolved, where appropriate, by amending or adding rules
to the grammar. This methodology resulted in the grammar
</bodyText>
<page confidence="0.998133">
213
</page>
<bodyText confidence="0.699651666666667">
-
being modified and extended to nearly 10,000 context-free
productions by December 1987.
</bodyText>
<figure confidence="0.987002">
V. V
Od (I) (V)
Oh (I) (Vn)
Ob (I) (Vg) / (vn) }
</figure>
<figureCaption confidence="0.999966">
Figure 4: Fragment of the Grammar front the Parser&apos;s Manual
</figureCaption>
<bodyText confidence="0.999775333333333">
Ob = operator consisting of. or ending with, a form of be; Od
se operator consisting of, or ending with, a form of do; Oh =
operator constisting of, or ending with, a form of the verb
have; V = main verb with complementation; V. = predicate;
Vg = an -ing verb phrase; Vn es a past participle phrase; 0=
optional constituents; (/) 13 alternative constituents.
</bodyText>
<sectionHeader confidence="0.566991" genericHeader="method">
7. Constructing the Parsed Databank
</sectionHeader>
<bodyText confidence="0.9975475">
For convenience of screen editing and computer processing,
the constituent structures are represented in a linear form, as
strings of grammatical words with labelled bracketing. The
grammarians are given print-outs of post-edited output from
the CLAWS suite. They then construct a constituent analysis
for each sentence on the print-out, either in detail or in outline,
according to the rules described in the Parser&apos;s Manual, and
key in their structures using an input program that checks for
well-formedress. The well-formedness constraints imposed by
the program are:
</bodyText>
<listItem confidence="0.9965555">
(1) that labels are legal non-terminal symbols
(2) that labelled brackets balance
</listItem>
<bodyText confidence="0.892237444444444">
(3) that the productions obtained by the input analysis are
contained in the existing grammar.
One sentence is presented at a time. Any errors found by
the program are reported back to the screen, once the
grammarian has sent what s/he considers to be the completed
parse. Sentences which are not well formed can be re-edited or
abandoned. A validity marker is appended to the reference for
each sentence indicating whether the sentence has been
abandoned with errors contained in it.
</bodyText>
<table confidence="0.685488142857143">
Shortages_NN2 of_IO gasoline_NN1 and_CC
rapidly_RR rising_VVG prices_NN2 for_IF
the_AT fuel_NN1 are_VBR given_VVN as_II
the_AT reasons_NN2 for_IF a_AT1 6.7_MC
percent_NNU reduction_NN1 in_II traffic_NN1
deaths_NN2 on_II New_NP1 York_NP1 state_NN1
&apos;s_$ roads_NNL2 last_MD year_NNT1
</table>
<figureCaption confidence="0.996095">
Figure 5: A word-tagged sentence f70111 the AP corpus
</figureCaption>
<bodyText confidence="0.978342625">
AT = article; ATI = singular article; CC = coordinating
conjunction:- IF = for as preposition; 11 = preposition; JO = of
as preposition: MC = cardinal number, MD = ordinal number.
NN2 = plural common noun; NNL2 = plural locative noun;
NNT1 = temporal noun; NNU = unit of measurement RR =
general adverb; VBR = are; = germanic genitive marker.
8. Assessing the Parsed Databank and the
Grammar
We have written ancillary programs, to help in the
development of the grammar and to check the validity of the
parses in the databank. One program searches through the
parsed databank for every occurrence of a constituent matching
a specified constituent tag. Output is a list of all occurrences of
the specified constituent together with frequencies. This facility
allows selective searching through the databank, which is a
useful tool for revising parts of the grammar.
</bodyText>
<sectionHeader confidence="0.905433" genericHeader="method">
9. Skeleton Parsing
</sectionHeader>
<bodyText confidence="0.999048375">
We are aiming to produce a million word corpus of parsed
sentences by December 1988 so that we can implement a
variant of the CYK algorithm (Hopcmft and Ullman. 1979:
140) to obtain a set of parses for each sentence. Viterbi
labelling (Bahl et al. 1983; Fomey, 1973) could be used to
select the most probable pane from the output parse set. But
problems associated with assembling a fully parsed databank
are:
</bodyText>
<listItem confidence="0.8967075">
(1) speed of production and
(2) matching the parsed databank to an evolving grammar.
</listItem>
<bodyText confidence="0.999966142857143">
In order to circumvent these problems, a strategy of
skeleton parsing has been introduced. In skeleton parsing,
grammarians enter minimal labelled bracketing by insetting
only those labelled brackets that are uncontroversial and, in
sonic cases, by inserting brackets with no labels. The grammar
validation routine is de-coupled from the input program so that
changes to the grammar can be made without disrupting the
input parsing. The strategy also prevents extensive
retrospective editing whenever the grammar is modified.
Grammar development and parsed databank consauction are
not entirely independent however. A subset (10 per cent) of the
skeleton pluses are extracted for comparison with the current
grammar, while another subset (1 per cent) is checked by
independan grammarians.
Skeleton parsing will give us a partially parsed databank
which should limit the alternative parses compatible with the
final grammar. We can either assume each parse is equally
likely and use the frequency weighted productions generated
by the partially parsed databank to upgrade or downgrade
alternative parses or we can use a &apos;restrained&apos; outside/inside
algorithm (Baker. 1979) to find the optimal parse.
</bodyText>
<page confidence="0.997337">
214
</page>
<table confidence="0.907987444444444">
A010 1 v
[S&apos; (Sd[N&apos; (N&apos;&amp;[N Shortages_NN2 [Po of_IO [N gasoline_NN1 NIN&apos;]Po]N]
N&apos;&amp;] and CC (N&apos;+[Jm rapicily_RR rising_VVG Jet] (29 prices_NN2 [P for IF
[N&apos; [Da the_AT Da] [N fuel_NN1 N]N&apos;]P]N]N&apos;+]N&apos;] [V&apos; (Ob are_VBR Ob] [Vn
given_VVN (P as_II [N&apos; (Da the AT Da] [N reasons_NN2 N]N&apos; ]P] [P for_IF
[N&apos; [D a_AT1 [M 6.7_MC MID] [N percent_NNII reduction_NN1 (13 in_II [N&apos; (N
traffic_NN1 deaths_NN2 (P on_II (19&apos; (13[G(N New_NP1 York_NP1 state_NN1
N] &apos;s_$ GID][N roads_NNL2 N][Q(Nr&apos; (DM last MD MID] year_NNT1 Nr&apos; ]Q]
N&apos;]12]N3N&apos;]P]N]le]P]Vn]V&apos;]Sd] S&apos;]
</table>
<figureCaption confidence="0.998559">
Figure 6: A Fully Parsed Version of the Sentence in figure 5.
</figureCaption>
<bodyText confidence="0.999321666666667">
D = general determinative element Da = determinative element containing an article as
the last or only word; G = genitive construction; len = adjective phrase; M = numeral
phrase; N nominal; N&apos; = noun phrase; N&apos;&amp; = first conjunct of co-ordinated noun
phrase; N&apos;+ = non-initial conjunct following a conjunction; Nr&apos; = temporal noun phrase; p
= prepositional phrase; Po = prepositional phrase; Q = qualifier S&apos; = sentence; Sd =
declarative sentence.
</bodyText>
<figure confidence="0.942021666666667">
A062 96 v
&amp;quot;_&amp;quot; (S Now_RT &amp;quot;_&amp;quot; [Si[N he_PPBS1 191(V said_VVD V]Si] &amp;quot;_&amp;quot; [S&amp;
[N we_PPIS2 N] (V are_VBR negotiating_VVG [P under_II (N duress_NN1 N]
P]V]S&amp;] and_CC (S+[14 they_PPBS2 N](V can_VM play_VVO (P with_IW
[N us_PPI02 N]P](P like_ICS [N a_ATI cat_NN1 (P with_IW [N a_AT1
mouse_NN1 N]P]N]P]V]S+]S]
</figure>
<figureCaption confidence="0.999966">
Figure 7: A Skeleton Parsed Sentence.
</figureCaption>
<bodyText confidence="0.9977484">
word tags: ICS = preposition-conjunctiort IW = with, without as prepositions;
PPHS1 = he, she; PPHS2 = they: PP102 us; PPIS2 we; RT = nominal adverb of
time; VM = modal auxiliary verb; hypertags: S = included sentence; S&amp; = first
coordinated main clause; S+ non-inital coordinated main clause following a
conjunction; Si = interpolated or appended sentence.
</bodyText>
<sectionHeader confidence="0.931942" genericHeader="method">
10. Featurisation
</sectionHeader>
<bodyText confidence="0.9999305">
The development of the CLAWS tagset and UCREL
grammar owes much to the work of Quirk a al. (1985) while
the tags themselves have evolved from the Brown tagset
(Francis and Kutera. 1982). However, the rules and symbols
chosen have been translated into a notation compatible with
other theories of grammar. For instance, tags from the
extended version of the CLAWS lexicon have been translated
into a formalism compatible with the Winchester parser
(Sharman. 1988). A program has also been written to map all
of the ten thousand productions of the current UCREL
grammar into the notation used by the Grammar Development
Environment (ODE) (Briscoe et al., 1987; Grover et aL, 1988;
Carroll et al.. 1988). This is a preliminary step in the task of
recasting the grammar into a feature-based unification
formalism which will allow us to radically reduce the size of
the rule set while preventing the grammar from overgenerating.
</bodyText>
<table confidence="0.592384142857143">
1
[ VVO* ] 50 85
[ VVO* N&apos; ] 800 86
VVO* J ] 80 87
[ VVO* P ] 900 88
[ VVO* R ] 80 89
[ VVO* Fn ] 100 90
</table>
<figureCaption confidence="0.936318">
Figure 8: A Fragment of the UCREL grammar
</figureCaption>
<page confidence="0.995409">
215
</page>
<construct confidence="0.974936166666667">
PSRULE V85 : VI V.
PSRULE V86 : VI --â–  V NP.
PSRULE V87 : VI. -+ V AP.
PSRULE V88 : VI -+ V PP.
PSRULE V89 : VI -4 V ADVP .
PSRULE V90 : VI. -+ V V2 (FIN1 .
</construct>
<figureCaption confidence="0.9870045">
Figure 9: Translation of the Rules in Figure 8
into ODE representation
</figureCaption>
<sectionHeader confidence="0.893752" genericHeader="method">
11. Summary
</sectionHeader>
<bodyText confidence="0.982218714285714">
In summary, we have a word tagging system that requires
minimal post-editing, a steadily accumulating corpus of parsed
sentences and a context-free grammar of about ten thousand
productions which is currently being recast into a feature-based
unification formalism. Additionally, we have programs for
extracting statistical and collocational data from both word
tagged and parsed text corpora.
</bodyText>
<sectionHeader confidence="0.909179" genericHeader="method">
12. Acknowledgements
</sectionHeader>
<reference confidence="0.925154465753425">
The author is a member of a group of researchers working
at the Unit for Computer Research on the English Language at
Lancaster University. The present members of UCREL are
Geoffrey Leech, Roger Garside (UCREL directors), Andrew
Beale, Louise Denmark, Steve Elliott. Jean Forrest, Fanny
Leech and Lila Taylor. The work is currently funded by IBM
UK (research grant 823105) and carried out in collaboration
with Claire Grover, Richard Sharman, Peter Alderson, Ezra
Black and Frederick Jelinek of IBM.
13. References
Erik Akkerman, Pieter Masereeuw and Willem Meijs (1985).
&apos;Designing a Computerised Lexicon for Linguistic Purposes&apos;,
ASCOT Report No. 1, CIP-Gegevens Koninklijke Bibliotheek,
Den Haag, Netherlands.
Lalit R. Bahl, Frederick Jelinek and Robert L. Mercer (1983).
&apos;A Maximum Likelihood Approach to Continuous Speech
Recognition&apos;, IEEE Transactions on Pattern Analysis and
Machine Intelligence, Vol. PAMI-5, No. 2, March 1983.
J. K. Baker (1979). &apos;Trainable Grammars for Speech
Recognition,&apos; Proceedings of the Spring Conference of the
Acoustical Society of America.
Bran Boguraev, Ted Briscoe, John Carroll, David Carter and
Claire Grover (1987). &apos;The Derivation of a Grammatically
Indexed Lexicon from the Longman Dictionary of
Contemporary English&apos;, Proceedings of ACL-87, Stanford.
California.
Ted Briscoe, Claire Grover, Bran Boguraev, John Carroll
(1987). &apos;A Formalism and Environment for the Development
of a Large Grammar of English&apos;, Proceedings of IJCAI, Milan.
Keith Brown (1984). Linguistics Today, Fontana, U.K.
John Carroll, Bran Boguraev, Claire Grover, Ted Briscoe
(1988). &apos;The Grammar Development Environment User
Manual&apos;, Cambridge Computer Laboratory Technical Report
127, Cambridge, England.
Roger Garside, Geoffrey Leech and Geoffrey Sampson (1987).
The Computational Analysis of English: A Corpus-Based
Approach, Longman. London and New York.
Claire Grover, Ted Briscoe. John Carroll, Bran Boguraev
(1988). &apos;The Alvey Natural Language Tools Project Grammar:
A Wide-Coverage Computational Grammar of English&apos;,
Lancaster Papers in Linguistics 47. Department of Linguistics.
University of Lancaster: March 1988.
G. Forney, Jr. (1973). &amp;quot;The Viterbi Algorithm&apos;, Proc. IEEE,
Vol 61: March 1973. pp. 268-278.
W. Nelson Francis and Henry Kubera (1982). Frequency
â€¢ Analysis of English Usage: Lexicon and Grammar, Houghton
Mifflin, Boston.
IC= Holland and Stig Johansson (1982). Word Frequencies in
British and American English, Norwegian Computing Centre
for the Humanities. Bergen: Longman, London.
John E. Hoperoft and Jeffrey D. Ullman (1979). Introduction
to Automata 77teory, Languages, and Computation, Addison-
Wesley, Reading, Mass.
Stig Johansson, Eric Atwell, Roger Garside and Geoffrey
Leech (1986). &apos;The Tagged LOB Corpus Users&apos; Manual.&apos;
Norwegian Computing Centre for the Humanities, Bergen.
Henry Kutera and W. Nelson Francis (1967). Computational
Analysis of Present-day American English, Brown University
Press, Providence, Rhode Island.
Geoffrey Leech (1987). Parsers&apos; Manual&apos;, Department of
Linguistics. University of Lancaster.
Longman Dictionary of Contentporary English (1978), second
edition (1987), Longman Group Limited. Harlow and London.
England. .
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech and Jan
Svartvilt (1985). A Comprehensive Grammar of the English
Language, Longman Inc., New York.
Naomi Sager (1981). Natural Language Information
Processing, Addison-Wesley, Reading, Mass.
Geoffrey Sampson (1987). &apos;The grammatical database and
parsing scheme&apos; in Garside, Leech and Sampson. pp 82-96.
Richard A. Sharman (1988). &apos;The Winchester Unification
Parsing System&apos;, IBM UKSC Report 999: April 1988.
</reference>
<page confidence="0.999143">
216
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9329535">Lexicon and grammar in probabilistic tagging of written English.</title>
<author confidence="0.998771">Andrew David Beale</author>
<affiliation confidence="0.9952805">Unit for Computer Research on the English Language University of Lancaster</affiliation>
<address confidence="0.969343">Bailrigg, Lancaster England LAI 4YT</address>
<email confidence="0.976775">enb02.1(guk.aciancs.vaxl</email>
<abstract confidence="0.979361191358025">The paper describes the development of software for automatic grammatical analysis of unrestricted, unedited English text at the Unit for Computer Research on the English Language (UCREL) at the University of Lancaster. The work is currently funded by IBM and carried out in collaboration with colleagues at IBM UK (Winchester) and IBM Yorktown Heights. The paper will focus on the lexicon component of the word tagging system. the UCREL grammar, the databanks of parsed sentences, and the tools that have been written to support development of these components. This work has applications to speech technology, spelling correction. and other areas of natural language processing. Currently, our goal is to provide a language model using transition statistics to disambiguate alternative parses for a speech recognition device. 1. Text Corpora Historically, the use of text corpora to provide empirical data for testing grammatical theories has been regarded as important to varying degrees by philologists and linguists of differing persuasions. The use of corpus citations in grammars and dictionaries pm-dates electronic data processing (Brown, 1984: 34). While most of the generative grammarians of the 60s and 70s ignored campus data, the increased power of the new technology nevertheless points the way to new applications of computerized text corpora in dictionary making, style checking and speech recognition. Computer corpora present the computational linguist with the diversity and complexity of real language which is more challenging for testing language models than intuitively derived examples. Ultimately grammars must be judged by their ability to contend with the real facts of language and not just basic constructs extrapolated by grammarians. 2. Word Tagging The system devised for automatic word tagging or part of speech selection for processing running English text, known as the Constituent-Likelihood Automatic Word-tagging System (CLAWS) (Garside et aL, 1987) serves as the basis for the current woric. The word tagging system is an automated component of the probabilistic parsing system we are currently working on. In word tagging, each of the nmning words in the corpus text to be processed is associated with a pie-terminal denoting word class. In essence. the CLAWS suite be conceptually divided into two phases: tag assignment and tag selection. NNS1: NP1: NN1@ VVO wig NN1% NNS1: VV0e VVN J1:1@ NN1 VVO@ 1: of the CLAWS Lexicon JB = attributive adjective; JJ = general adjective; NN1 = singular.common noun; NNS1 = noun of style or title; NPI = singular proper noun; VVO = base form of lexical verb; VVD past tense of lexical verb; VVG = of lexical verb; VVN = past participle of lexical verb; %, @ = probability markers; = word initial capital marker. 211 cent of running words are correctly disambiguated in this way.) Exceptions are dealt with by invoking a look up procedure that searches through a limited list of groups of two or more words, or by automatically adjusting the probabilities of sequences of three tags in cases when the intermediate tag is misleading. The current version of the CLAWS system requires no preediting and attributes the correct word tag to over 96 per cern of the input miming words, leaving 3 to 4 per cern to be corrected by human post-editors. 3. Error Analysis Error analysis of CLAWS output has resulted, and condnues to result, in diverse improvements to the system, from the simple adjustment of probability weightings against tags in the lexicon to the inclusion of additional procedures, for instance to deal with the distinction between proper names and common nouns. Parts of the system can also be used to develop new parts. to extend existing pans, or to interface with other systems. For instance, in order to produce a lexicon sufficiently large and detailed enough for parsing, we needed to extend the original list of about 8,000 entries to over 20.000 (the new CLAWS lexicon contains about 26,500 entries).&apos; In order to do this, a list of 15,000 words not already in the CLAWS lexicon was tagged using the CLAWS tag assignment program. (Since they were not already in the lexicon, the candidate tags for each new entry were assigned by suffudist lookup or default tag assignment.) The new list was then post-edited by interactive screen editing and merged with the old lexicon. Another example of &apos;self improvement&apos; is in the production of a better set of one-step transition probabilities. The first CLAWS system used a matrix of tag transition probabilities derived from the tagged Brown corpus (Francis and Kaera, 1982). Some cells of this matrix were inaccurate because of incompatibility of the Brown tagset and the CLAWS tagset. To remedy this, a new matrix was created by a statistics-gathering program that processed the post-edited version of a corpus of one million words tagged by the original CLAWS suite of programs. 4. Subcategorization Apart from extending the vocabulary coverage of the CLAWS lexicon, we are also subcategorizing words belonging to the major word classes in order to reduce the overgeneration of alternative panes of sentences of greater than trivial length. The task of subcategolization involves: (1) a linguist&apos;s specification of a schema or typology of lexical subcategories based on distributional and Tag assignment involves, for each input running word or punctuation mark, lexicon look-up, which provides one or more potential word tags for each input word or punctuation mark. The lexicon is a list of about 8.000 records containing fields for (1) the word form (2) the set of one or more candidate tags denoting the word&apos;s word class(es) with probability markers attached indicating three different levels of probability. Words not in the CLAWS lexicon are assigned potential tags either by suffixlist look-up, which attempts to match end characters of the input word with a suffix in the suffudist, or. if the input word does not have a word-ending to match one of these entries, default tags are assigned. The procedures ensure rare words and neologisms not in the lexicon are given an analysis. NP1: VVO NP1: VVO NP1: 2: of the Suffudist Tag selection disambiguates the alternative tags that are assigned to some of the running words. Disambiguation is achieved by invoking one-step probabilities of tag pair likelihoods extracted from a previously tagged training corpus and upgrading or downgrading likelihoods according to the probability markers against word tags in the lexicon or In the majority of cases, order Markov model is sufficient to correctly select the most likely sequence of tags associated with the input running text. (Over 90 per 212 3 functional criteria. (2) a lexicographer&apos;s judgement in assigning one or more of the subcategory codes in the linguist&apos;s schema to the major lexical word forms (verbs, nouns, adjectives). The amount of detail demarcated by the subcategorization typology is dependent. in part, on the practical requirements of the system. Existing subcategorization systems, such as the one provided in the Longman Dictionary of Contemporary English (1978) or Sager&apos;s (1981) subcategories, need to be taken into account. But these are assessed critically rather than adopted wholesale (see for instance Akkaman et al.. 1985 and Boguraev et at.. 1987, for a discussion of the strengths and weaknesses of the LDOCE grammar codes). [1] intransirive verb : ache, age, allow, care, conflict, escape. fish, occur, reply, snow, stay, sun-bathe, swoon, talk, vanish. [2] transitive verb : abandon, abhor, allow, build, complete, contain, demand, exchange, get, give, house, keep, mail, master, oppose, pardon. spend, strengthen, warn. (3]copular verb: appear, become, feel, get, grow, remain, seem. [4] prepositional verb : abstract, aim, ask, belong, cater. consist, prey, pry, search, vote. phrasal verb : blow, build, ease, hand, jazz, look, open, pop, share, wort. [6] verb followed by that-clause : accept, believe. demand, guess, maintain, reckon, require, think. [7] verb followed by to-infinitive : ask, come, dare, demand, fail, hope, intend, need, prefer, propose, refuse, seem, try, wish. [8] verb followed by -big construction : abhor, begin. continue, deny, dislike, enjoy, keep, recall, remember, risk, suggest. [9] anibitransitive verb : accept, answer, close, compile, cook, develop, feed, fly, move, obey, practice, quit, sing, stop, teach. [A] verb habitually followed by an adverbial : appear, come, go, keep, lie, live, move, put, sit, stand, swim, veer. [W] verb followed by a wit-clause : ask, choose, doubt. imagine, know, matter, mind, wonder. 3: initial schema of eleven verb subcategories We began subcategorization of the CLAWS lexicon by word-tagging the 3,000 most frequent words in the Brown corpus (Kutera and Francis, 1967). An initial system of eleven verb subcategories was proposed, and judgements about which subcategory(ies) each verb belonged to were empirically tested by looking up entries in the microfiche concordance of the tagged Lancaster/Oslo-Bergen corpus (Holland and Johansson. 1982; Johansson et al. 1986) which shows every occurrence of a tagged word in the corpus together with its context. About 2,500 verbs have been coded in this way, and we are now working on a more detailed system of about 80 different verb subcategories using the Lexicon Development Environment of Boguraev et al. (1987). 5. Constituent Analysis The task of implementing a probabilistic parsing algorithm to provide a disambiguated constituent analysis of unrestricted English is more demanding than implementing the word tagging suite, not least because, in order to operate in a manner similar to the word-tagging model, the system requires (1) specification of an appropriate grammar of rules and symbols and (2) the construction of a sufficiently large databank of parsed sentences conforming to the (optimal) grammar specified in (1) to provide statistics of&apos; the relative likelihoods of constituent tag transitions for constituent tag disambiguation. In order to meet these prior requirements, researchers have been employed on a full-time basis to assemble a corpus of parsed sentences. 6. Grammar Development and Parsed Subcorpora The databank of approximately 45,000 words of manually parsed sentences of the Lancaster/Oslo-Bergen corpus 1987: was processed to .show the distinct types of production rules and their frequency of occurrence in the grammar associated with the Sampson neebank. Experience of the UCREL probabilistic system (Garside and Leech. 1987: 66ff) and suggestions from other researchers prompting new rules resulted in a new context-free grammar of about 6,000 productions creating more steeply nested structures than those of the Sampson grammar. (It was anticipated that steeper nesting would reduce the size of the treebank required to obtain adequate frequency statistics.) The new grammar is defined descriptively in a Parser&apos;s Manual (Leech, 1987) and formalised as a set of context-free phrase-structure productions. Development of the grammar then proceeded in tandem with the construction of a second databank of parsed sentences, fitting, as closely as possible, the rules expressed by the grammar. The new databank comprises extracts from newspaper reports daring from 1979-80 in the Associated Press (AP) corpus. Any difficulties the grammarians had in parsing were resolved, where appropriate, by amending or adding rules to the grammar. This methodology resulted in the grammar 213 being modified and extended to nearly 10,000 context-free productions by December 1987. Od (I) (V) Oh (I) (Vn) Ob (I) (Vg) / (vn) } Fragment of the Grammar front the Parser&apos;s Manual = operator consisting of. or ending with, a form of operator consisting of, or ending with, a form of = operator constisting of, or ending with, a form of the verb = main verb with complementation; = predicate; = an phrase; Vn es a past participle phrase; 0= constituents; (/) 13alternative constituents. 7. Constructing the Parsed Databank For convenience of screen editing and computer processing, the constituent structures are represented in a linear form, as strings of grammatical words with labelled bracketing. The grammarians are given print-outs of post-edited output from the CLAWS suite. They then construct a constituent analysis for each sentence on the print-out, either in detail or in outline, according to the rules described in the Parser&apos;s Manual, and key in their structures using an input program that checks for well-formedress. The well-formedness constraints imposed by the program are: (1) that labels are legal non-terminal symbols (2) that labelled brackets balance (3) that the productions obtained by the input analysis are contained in the existing grammar. One sentence is presented at a time. Any errors found by the program are reported back to the screen, once the grammarian has sent what s/he considers to be the completed parse. Sentences which are not well formed can be re-edited or abandoned. A validity marker is appended to the reference for each sentence indicating whether the sentence has been abandoned with errors contained in it. Shortages_NN2 of_IO gasoline_NN1 and_CC rapidly_RR rising_VVG prices_NN2 for_IF the_AT fuel_NN1 are_VBR given_VVN as_II the_AT reasons_NN2 for_IF a_AT1 6.7_MC percent_NNU reduction_NN1 in_II traffic_NN1 deaths_NN2 on_II New_NP1 York_NP1 state_NN1 &apos;s_$ roads_NNL2 last_MD year_NNT1 5: word-tagged sentence f70111 the AP corpus AT = article; ATI = singular article; CC = coordinating IF = preposition; 11 = preposition; JO = as preposition: MC = cardinal number, MD = ordinal number. NN2 = plural common noun; NNL2 = plural locative noun; NNT1 = temporal noun; NNU = unit of measurement RR = adverb; VBR = = genitive marker. 8. Assessing the Parsed Databank and the Grammar We have written ancillary programs, to help in the development of the grammar and to check the validity of the parses in the databank. One program searches through the parsed databank for every occurrence of a constituent matching a specified constituent tag. Output is a list of all occurrences of the specified constituent together with frequencies. This facility allows selective searching through the databank, which is a useful tool for revising parts of the grammar. 9. Skeleton Parsing aiming to produce a million word corpus of parsed sentences by December 1988 so that we can implement a variant of the CYK algorithm (Hopcmft and Ullman. 1979: 140) to obtain a set of parses for each sentence. Viterbi labelling (Bahl et al. 1983; Fomey, 1973) could be used to select the most probable pane from the output parse set. But problems associated with assembling a fully parsed databank are: (1) speed of production and (2) matching the parsed databank to an evolving grammar. In order to circumvent these problems, a strategy of skeleton parsing has been introduced. In skeleton parsing, grammarians enter minimal labelled bracketing by insetting only those labelled brackets that are uncontroversial and, in sonic cases, by inserting brackets with no labels. The grammar validation routine is de-coupled from the input program so that changes to the grammar can be made without disrupting the input parsing. The strategy also prevents extensive retrospective editing whenever the grammar is modified. Grammar development and parsed databank consauction are not entirely independent however. A subset (10 per cent) of the skeleton pluses are extracted for comparison with the current grammar, while another subset (1 per cent) is checked by independan grammarians. Skeleton parsing will give us a partially parsed databank which should limit the alternative parses compatible with the final grammar. We can either assume each parse is equally likely and use the frequency weighted productions generated by the partially parsed databank to upgrade or downgrade alternative parses or we can use a &apos;restrained&apos; outside/inside algorithm (Baker. 1979) to find the optimal parse.</abstract>
<note confidence="0.902948222222222">214 A010 1 v [S&apos; (Sd[N&apos; (N&apos;&amp;[N Shortages_NN2 [Po of_IO [N gasoline_NN1 NIN&apos;]Po]N] CC (N&apos;+[Jm rapicily_RR rising_VVG Jet] (29 prices_NN2 [P for IF [N&apos; [Da the_AT Da] [N fuel_NN1 N]N&apos;]P]N]N&apos;+]N&apos;] [V&apos; (Ob are_VBR Ob] [Vn given_VVN (P as_II [N&apos; (Da the AT Da] [N reasons_NN2 N]N&apos; ]P] [P for_IF [D a_AT1 [M 6.7_MC MID] [N percent_NNII reduction_NN1 in_II [N&apos; (N traffic_NN1 deaths_NN2 (P on_II (19&apos; (13[G(N New_NP1 York_NP1 state_NN1 N] &apos;s_$ GID][N roads_NNL2 N][Q(Nr&apos; (DM last MD MID] year_NNT1 Nr&apos; ]Q</note>
<abstract confidence="0.936532675675676">S&apos;] 6: Fully Parsed Version of the Sentence in figure 5. D = general determinative element Da = determinative element containing an article as the last or only word; G = genitive construction; len = adjective phrase; M = numeral phrase; N nominal; N&apos; = noun phrase; N&apos;&amp; = first conjunct of co-ordinated noun N&apos;+ = non-initial conjunct following a conjunction; Nr&apos; = noun p = prepositional phrase; Po = prepositional phrase; Q = qualifier S&apos; = sentence; Sd = declarative sentence. A062 96 v &amp;quot;_&amp;quot; (S Now_RT &amp;quot;_&amp;quot; [Si[N he_PPBS1 191(V said_VVD V]Si] &amp;quot;_&amp;quot; [S&amp; [N we_PPIS2 N] (V are_VBR negotiating_VVG [P under_II (N duress_NN1 N] P]V]S&amp;] and_CC (S+[14 they_PPBS2 N](V can_VM play_VVO (P with_IW [N us_PPI02 N]P](P like_ICS [N a_ATI cat_NN1 (P with_IW [N a_AT1 mouse_NN1 N]P]N]P]V]S+]S] A Skeleton Parsed Sentence. tags: = preposition-conjunctiort IW = without prepositions; = she; = = nominal adverb of VM = modal auxiliary verb; = included sentence; S&amp; = first coordinated main clause; S+ non-inital coordinated main clause following a conjunction; Si = interpolated or appended sentence. 10. Featurisation The development of the CLAWS tagset and UCREL grammar owes much to the work of Quirk a al. (1985) while the tags themselves have evolved from the Brown tagset (Francis and Kutera. 1982). However, the rules and symbols chosen have been translated into a notation compatible with other theories of grammar. For instance, tags from the extended version of the CLAWS lexicon have been translated into a formalism compatible with the Winchester parser (Sharman. 1988). A program has also been written to map all of the ten thousand productions of the current UCREL grammar into the notation used by the Grammar Development Environment (ODE) (Briscoe et al., 1987; Grover et aL, 1988; Carroll et al.. 1988). This is a preliminary step in the task of recasting the grammar into a feature-based unification formalism which will allow us to radically reduce the size of the rule set while preventing the grammar from overgenerating.</abstract>
<note confidence="0.6787354375">1 [ VVO* ] 50 85 [ VVO* N&apos; ] 800 86 VVO* 80 87 P ] 900 88 [ VVO* R ] 80 89 [ VVO* Fn ] 100 90 8: of the UCREL grammar 215 PSRULE V85 : VI V. PSRULE V86 : VI --â–  V NP. PSRULE V87 : VI. -+ V AP. PSRULE V88 : VI -+ V PP. V89 : VI ADVP . PSRULE V90 : VI. -+ V V2 (FIN1 . of the Rules in Figure 8</note>
<abstract confidence="0.897327583333333">11. Summary In summary, we have a word tagging system that requires minimal post-editing, a steadily accumulating corpus of parsed sentences and a context-free grammar of about ten thousand productions which is currently being recast into a feature-based unification formalism. Additionally, we have programs for extracting statistical and collocational data from both word tagged and parsed text corpora. 12. Acknowledgements The author is a member of a group of researchers working at the Unit for Computer Research on the English Language at Lancaster University. The present members of UCREL are</abstract>
<author confidence="0.8112315">Jean Forrest</author>
<author confidence="0.8112315">Fanny</author>
<note confidence="0.878488608695652">Leech and Lila Taylor. The work is currently funded by IBM UK (research grant 823105) and carried out in collaboration with Claire Grover, Richard Sharman, Peter Alderson, Ezra Black and Frederick Jelinek of IBM. 13. References Erik Akkerman, Pieter Masereeuw and Willem Meijs (1985). &apos;Designing a Computerised Lexicon for Linguistic Purposes&apos;, ASCOT Report No. 1, CIP-Gegevens Koninklijke Bibliotheek, Den Haag, Netherlands. Lalit R. Bahl, Frederick Jelinek and Robert L. Mercer (1983). &apos;A Maximum Likelihood Approach to Continuous Speech Recognition&apos;, IEEE Transactions on Pattern Analysis and Intelligence, Vol. No. 2, March 1983. J. K. Baker (1979). &apos;Trainable Grammars for Speech of the Spring Conference of the Acoustical Society of America. Bran Boguraev, Ted Briscoe, John Carroll, David Carter and Claire Grover (1987). &apos;The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of English&apos;, of ACL-87, California. Ted Briscoe, Claire Grover, Bran Boguraev, John Carroll (1987). &apos;A Formalism and Environment for the Development</note>
<affiliation confidence="0.926233">a Large Grammar of English&apos;, of IJCAI,</affiliation>
<address confidence="0.90938">Brown (1984). Today, U.K.</address>
<author confidence="0.811866">John Carroll</author>
<author confidence="0.811866">Bran Boguraev</author>
<author confidence="0.811866">Claire Grover</author>
<author confidence="0.811866">Ted Briscoe</author>
<affiliation confidence="0.669959">(1988). &apos;The Grammar Development Environment User</affiliation>
<address confidence="0.649829">Manual&apos;, Cambridge Computer Laboratory Technical Report 127, Cambridge, England.</address>
<note confidence="0.940284347826087">Roger Garside, Geoffrey Leech and Geoffrey Sampson (1987). The Computational Analysis of English: A Corpus-Based London and New York. Claire Grover, Ted Briscoe. John Carroll, Bran Boguraev (1988). &apos;The Alvey Natural Language Tools Project Grammar: A Wide-Coverage Computational Grammar of English&apos;, Papers in Linguistics 47. of Linguistics. University of Lancaster: March 1988. Forney, Jr. (1973). &amp;quot;The Viterbi Algorithm&apos;, IEEE, Vol 61: March 1973. pp. 268-278. Nelson Francis and Henry Kubera (1982). Analysis of English Usage: Lexicon and Grammar, Mifflin, Boston. Holland and Stig Johansson (1982). Frequencies in and American English, Computing Centre for the Humanities. Bergen: Longman, London. E. Hoperoft and Jeffrey D. Ullman (1979). Automata 77teory, Languages, and Computation, Addison- Wesley, Reading, Mass. Stig Johansson, Eric Atwell, Roger Garside and Geoffrey Leech (1986). &apos;The Tagged LOB Corpus Users&apos; Manual.&apos; Norwegian Computing Centre for the Humanities, Bergen. Kutera and W. Nelson Francis (1967).</note>
<affiliation confidence="0.439286">of Present-day American English, University</affiliation>
<address confidence="0.537933">Press, Providence, Rhode Island. Geoffrey Leech (1987). Parsers&apos; Manual&apos;, Department of</address>
<affiliation confidence="0.981838">Linguistics. University of Lancaster. Dictionary of Contentporary English second</affiliation>
<address confidence="0.820217">(1987), Longman Group Limited. Harlow England. .</address>
<note confidence="0.8453718">Randolph Quirk, Sidney Greenbaum, Geoffrey Leech and Jan (1985). Comprehensive Grammar of the English Inc., New York. Sager (1981). Language Information Reading, Mass. Geoffrey Sampson (1987). &apos;The grammatical database and scheme&apos; Garside, Leech and Sampson. pp Richard A. Sharman (1988). &apos;The Winchester Unification System&apos;, UKSC Report 1988. 216</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Jean Forrest</author>
</authors>
<title>The author is a member of a group of researchers working at the Unit for Computer Research on the English Language at Lancaster University. The present members of UCREL are Geoffrey Leech, Roger Garside (UCREL directors),</title>
<location>Andrew Beale, Louise</location>
<marker>Forrest, </marker>
<rawString>The author is a member of a group of researchers working at the Unit for Computer Research on the English Language at Lancaster University. The present members of UCREL are Geoffrey Leech, Roger Garside (UCREL directors), Andrew Beale, Louise Denmark, Steve Elliott. Jean Forrest, Fanny Leech and Lila Taylor. The work is currently funded by IBM UK (research grant 823105) and carried out in collaboration with Claire Grover, Richard Sharman, Peter Alderson, Ezra Black and Frederick Jelinek of IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Akkerman</author>
<author>Pieter Masereeuw</author>
<author>Willem Meijs</author>
</authors>
<title>Designing a Computerised Lexicon for Linguistic Purposes&apos;,</title>
<date>1985</date>
<journal>ASCOT Report</journal>
<volume>1</volume>
<location>Den Haag, Netherlands.</location>
<marker>Akkerman, Masereeuw, Meijs, 1985</marker>
<rawString>Erik Akkerman, Pieter Masereeuw and Willem Meijs (1985). &apos;Designing a Computerised Lexicon for Linguistic Purposes&apos;, ASCOT Report No. 1, CIP-Gegevens Koninklijke Bibliotheek, Den Haag, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>A Maximum Likelihood Approach to Continuous Speech Recognition&apos;,</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<contexts>
<context position="15601" citStr="Bahl et al. 1983" startWordPosition="2454" endWordPosition="2457">bank. One program searches through the parsed databank for every occurrence of a constituent matching a specified constituent tag. Output is a list of all occurrences of the specified constituent together with frequencies. This facility allows selective searching through the databank, which is a useful tool for revising parts of the grammar. 9. Skeleton Parsing We are aiming to produce a million word corpus of parsed sentences by December 1988 so that we can implement a variant of the CYK algorithm (Hopcmft and Ullman. 1979: 140) to obtain a set of parses for each sentence. Viterbi labelling (Bahl et al. 1983; Fomey, 1973) could be used to select the most probable pane from the output parse set. But problems associated with assembling a fully parsed databank are: (1) speed of production and (2) matching the parsed databank to an evolving grammar. In order to circumvent these problems, a strategy of skeleton parsing has been introduced. In skeleton parsing, grammarians enter minimal labelled bracketing by insetting only those labelled brackets that are uncontroversial and, in sonic cases, by inserting brackets with no labels. The grammar validation routine is de-coupled from the input program so th</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Lalit R. Bahl, Frederick Jelinek and Robert L. Mercer (1983). &apos;A Maximum Likelihood Approach to Continuous Speech Recognition&apos;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-5, No. 2, March 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable Grammars for Speech Recognition,&apos;</title>
<date>1979</date>
<booktitle>Proceedings of the Spring Conference of the Acoustical</booktitle>
<publisher>Society of America.</publisher>
<marker>Baker, 1979</marker>
<rawString>J. K. Baker (1979). &apos;Trainable Grammars for Speech Recognition,&apos; Proceedings of the Spring Conference of the Acoustical Society of America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bran Boguraev</author>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>David Carter</author>
<author>Claire Grover</author>
</authors>
<title>The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of Contemporary English&apos;,</title>
<date>1987</date>
<booktitle>Proceedings of ACL-87,</booktitle>
<location>Stanford. California.</location>
<contexts>
<context position="10274" citStr="Boguraev et al. (1987)" startWordPosition="1622" endWordPosition="1625">pus (Kutera and Francis, 1967). An initial system of eleven verb subcategories was proposed, and judgements about which subcategory(ies) each verb belonged to were empirically tested by looking up entries in the microfiche concordance of the tagged Lancaster/Oslo-Bergen corpus (Holland and Johansson. 1982; Johansson et al. 1986) which shows every occurrence of a tagged word in the corpus together with its context. About 2,500 verbs have been coded in this way, and we are now working on a more detailed system of about 80 different verb subcategories using the Lexicon Development Environment of Boguraev et al. (1987). 5. Constituent Analysis The task of implementing a probabilistic parsing algorithm to provide a disambiguated constituent analysis of unrestricted English is more demanding than implementing the word tagging suite, not least because, in order to operate in a manner similar to the word-tagging model, the system requires (1) specification of an appropriate grammar of rules and symbols and (2) the construction of a sufficiently large databank of parsed sentences conforming to the (optimal) grammar specified in (1) to provide statistics of&apos; the relative likelihoods of constituent tag transitions</context>
</contexts>
<marker>Boguraev, Briscoe, Carroll, Carter, Grover, 1987</marker>
<rawString>Bran Boguraev, Ted Briscoe, John Carroll, David Carter and Claire Grover (1987). &apos;The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of Contemporary English&apos;, Proceedings of ACL-87, Stanford. California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>Claire Grover</author>
<author>Bran Boguraev</author>
<author>John Carroll</author>
</authors>
<title>A Formalism and Environment for the Development of a Large Grammar of English&apos;,</title>
<date>1987</date>
<booktitle>Proceedings of IJCAI,</booktitle>
<location>Milan.</location>
<contexts>
<context position="19472" citStr="Briscoe et al., 1987" startWordPosition="3066" endWordPosition="3069">nd UCREL grammar owes much to the work of Quirk a al. (1985) while the tags themselves have evolved from the Brown tagset (Francis and Kutera. 1982). However, the rules and symbols chosen have been translated into a notation compatible with other theories of grammar. For instance, tags from the extended version of the CLAWS lexicon have been translated into a formalism compatible with the Winchester parser (Sharman. 1988). A program has also been written to map all of the ten thousand productions of the current UCREL grammar into the notation used by the Grammar Development Environment (ODE) (Briscoe et al., 1987; Grover et aL, 1988; Carroll et al.. 1988). This is a preliminary step in the task of recasting the grammar into a feature-based unification formalism which will allow us to radically reduce the size of the rule set while preventing the grammar from overgenerating. 1 [ VVO* ] 50 85 [ VVO* N&apos; ] 800 86 VVO* J ] 80 87 [ VVO* P ] 900 88 [ VVO* R ] 80 89 [ VVO* Fn ] 100 90 Figure 8: A Fragment of the UCREL grammar 215 PSRULE V85 : VI V. PSRULE V86 : VI --â–  V NP. PSRULE V87 : VI. -+ V AP. PSRULE V88 : VI -+ V PP. PSRULE V89 : VI -4 V ADVP . PSRULE V90 : VI. -+ V V2 (FIN1 . Figure 9: Translation of </context>
</contexts>
<marker>Briscoe, Grover, Boguraev, Carroll, 1987</marker>
<rawString>Ted Briscoe, Claire Grover, Bran Boguraev, John Carroll (1987). &apos;A Formalism and Environment for the Development of a Large Grammar of English&apos;, Proceedings of IJCAI, Milan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith Brown</author>
</authors>
<title>Linguistics Today,</title>
<date>1984</date>
<tech>Technical Report 127,</tech>
<institution>Cambridge Computer Laboratory</institution>
<location>Fontana, U.K. John Carroll, Bran Boguraev, Claire Grover, Ted Briscoe</location>
<contexts>
<context position="1381" citStr="Brown, 1984" startWordPosition="200" endWordPosition="201"> of these components. This work has applications to speech technology, spelling correction. and other areas of natural language processing. Currently, our goal is to provide a language model using transition statistics to disambiguate alternative parses for a speech recognition device. 1. Text Corpora Historically, the use of text corpora to provide empirical data for testing grammatical theories has been regarded as important to varying degrees by philologists and linguists of differing persuasions. The use of corpus citations in grammars and dictionaries pm-dates electronic data processing (Brown, 1984: 34). While most of the generative grammarians of the 60s and 70s ignored campus data, the increased power of the new technology nevertheless points the way to new applications of computerized text corpora in dictionary making, style checking and speech recognition. Computer corpora present the computational linguist with the diversity and complexity of real language which is more challenging for testing language models than intuitively derived examples. Ultimately grammars must be judged by their ability to contend with the real facts of language and not just basic constructs extrapolated by</context>
</contexts>
<marker>Brown, 1984</marker>
<rawString>Keith Brown (1984). Linguistics Today, Fontana, U.K. John Carroll, Bran Boguraev, Claire Grover, Ted Briscoe (1988). &apos;The Grammar Development Environment User Manual&apos;, Cambridge Computer Laboratory Technical Report 127, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Garside</author>
<author>Geoffrey Leech</author>
<author>Geoffrey Sampson</author>
</authors>
<title>The Computational Analysis of English: A Corpus-Based Approach, Longman. London and</title>
<date>1987</date>
<location>New York.</location>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Roger Garside, Geoffrey Leech and Geoffrey Sampson (1987). The Computational Analysis of English: A Corpus-Based Approach, Longman. London and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Bran Boguraev</author>
</authors>
<title>The Alvey Natural Language Tools Project Grammar: A Wide-Coverage Computational Grammar of English&apos;,</title>
<date>1988</date>
<booktitle>Lancaster Papers in Linguistics 47.</booktitle>
<institution>Department of Linguistics. University of Lancaster:</institution>
<marker>Carroll, Boguraev, 1988</marker>
<rawString>Claire Grover, Ted Briscoe. John Carroll, Bran Boguraev (1988). &apos;The Alvey Natural Language Tools Project Grammar: A Wide-Coverage Computational Grammar of English&apos;, Lancaster Papers in Linguistics 47. Department of Linguistics. University of Lancaster: March 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Forney</author>
</authors>
<title>The Viterbi Algorithm&apos;,</title>
<date>1973</date>
<booktitle>Proc. IEEE, Vol 61:</booktitle>
<pages>268--278</pages>
<marker>Forney, 1973</marker>
<rawString>G. Forney, Jr. (1973). &amp;quot;The Viterbi Algorithm&apos;, Proc. IEEE, Vol 61: March 1973. pp. 268-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kubera</author>
</authors>
<title>Frequency â€¢ Analysis of English Usage: Lexicon and Grammar,</title>
<date>1982</date>
<location>Houghton Mifflin, Boston.</location>
<marker>Francis, Kubera, 1982</marker>
<rawString>W. Nelson Francis and Henry Kubera (1982). Frequency â€¢ Analysis of English Usage: Lexicon and Grammar, Houghton Mifflin, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IC Holland</author>
<author>Stig Johansson</author>
</authors>
<title>Word Frequencies in British and American English, Norwegian Computing Centre for the Humanities.</title>
<date>1982</date>
<location>Bergen: Longman, London.</location>
<marker>Holland, Johansson, 1982</marker>
<rawString>IC= Holland and Stig Johansson (1982). Word Frequencies in British and American English, Norwegian Computing Centre for the Humanities. Bergen: Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1979</date>
<booktitle>Introduction to Automata 77teory, Languages, and Computation,</booktitle>
<publisher>AddisonWesley,</publisher>
<location>Reading, Mass.</location>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>John E. Hoperoft and Jeffrey D. Ullman (1979). Introduction to Automata 77teory, Languages, and Computation, AddisonWesley, Reading, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stig Johansson</author>
</authors>
<title>Eric Atwell, Roger Garside and Geoffrey Leech</title>
<date>1986</date>
<location>Bergen.</location>
<marker>Johansson, 1986</marker>
<rawString>Stig Johansson, Eric Atwell, Roger Garside and Geoffrey Leech (1986). &apos;The Tagged LOB Corpus Users&apos; Manual.&apos; Norwegian Computing Centre for the Humanities, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kutera</author>
<author>W Nelson Francis</author>
</authors>
<date>1967</date>
<journal>Computational Analysis of Present-day American English, Brown</journal>
<publisher>University Press,</publisher>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="9682" citStr="Kutera and Francis, 1967" startWordPosition="1530" endWordPosition="1533">abhor, begin. continue, deny, dislike, enjoy, keep, recall, remember, risk, suggest. [9] anibitransitive verb : accept, answer, close, compile, cook, develop, feed, fly, move, obey, practice, quit, sing, stop, teach. [A] verb habitually followed by an adverbial : appear, come, go, keep, lie, live, move, put, sit, stand, swim, veer. [W] verb followed by a wit-clause : ask, choose, doubt. imagine, know, matter, mind, wonder. Figure 3: The initial schema of eleven verb subcategories We began subcategorization of the CLAWS lexicon by word-tagging the 3,000 most frequent words in the Brown corpus (Kutera and Francis, 1967). An initial system of eleven verb subcategories was proposed, and judgements about which subcategory(ies) each verb belonged to were empirically tested by looking up entries in the microfiche concordance of the tagged Lancaster/Oslo-Bergen corpus (Holland and Johansson. 1982; Johansson et al. 1986) which shows every occurrence of a tagged word in the corpus together with its context. About 2,500 verbs have been coded in this way, and we are now working on a more detailed system of about 80 different verb subcategories using the Lexicon Development Environment of Boguraev et al. (1987). 5. Con</context>
</contexts>
<marker>Kutera, Francis, 1967</marker>
<rawString>Henry Kutera and W. Nelson Francis (1967). Computational Analysis of Present-day American English, Brown University Press, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
</authors>
<date>1987</date>
<institution>Parsers&apos; Manual&apos;, Department of Linguistics. University of Lancaster.</institution>
<contexts>
<context position="11854" citStr="Leech, 1987" startWordPosition="1855" endWordPosition="1856">show the distinct types of production rules and their frequency of occurrence in the grammar associated with the Sampson neebank. Experience of the UCREL probabilistic system (Garside and Leech. 1987: 66ff) and suggestions from other researchers prompting new rules resulted in a new context-free grammar of about 6,000 productions creating more steeply nested structures than those of the Sampson grammar. (It was anticipated that steeper nesting would reduce the size of the treebank required to obtain adequate frequency statistics.) The new grammar is defined descriptively in a Parser&apos;s Manual (Leech, 1987) and formalised as a set of context-free phrase-structure productions. Development of the grammar then proceeded in tandem with the construction of a second databank of parsed sentences, fitting, as closely as possible, the rules expressed by the grammar. The new databank comprises extracts from newspaper reports daring from 1979-80 in the Associated Press (AP) corpus. Any difficulties the grammarians had in parsing were resolved, where appropriate, by amending or adding rules to the grammar. This methodology resulted in the grammar 213 - being modified and extended to nearly 10,000 context-fr</context>
</contexts>
<marker>Leech, 1987</marker>
<rawString>Geoffrey Leech (1987). Parsers&apos; Manual&apos;, Department of Linguistics. University of Lancaster.</rawString>
</citation>
<citation valid="true">
<title>Longman Dictionary of Contentporary English</title>
<date>1978</date>
<location>England. .</location>
<note>second edition</note>
<contexts>
<context position="7934" citStr="(1978)" startWordPosition="1266" endWordPosition="1266"> of cases, this first order Markov model is sufficient to correctly select the most likely sequence of tags associated with the input running text. (Over 90 per 212 3 functional criteria. (2) a lexicographer&apos;s judgement in assigning one or more of the subcategory codes in the linguist&apos;s schema to the major lexical word forms (verbs, nouns, adjectives). The amount of detail demarcated by the subcategorization typology is dependent. in part, on the practical requirements of the system. Existing subcategorization systems, such as the one provided in the Longman Dictionary of Contemporary English (1978) or Sager&apos;s (1981) subcategories, need to be taken into account. But these are assessed critically rather than adopted wholesale (see for instance Akkaman et al.. 1985 and Boguraev et at.. 1987, for a discussion of the strengths and weaknesses of the LDOCE grammar codes). [1] intransirive verb : ache, age, allow, care, conflict, escape. fish, occur, reply, snow, stay, sun-bathe, swoon, talk, vanish. [2] transitive verb : abandon, abhor, allow, build, complete, contain, demand, exchange, get, give, house, keep, mail, master, oppose, pardon. spend, strengthen, warn. (3]copular verb: appear, beco</context>
</contexts>
<marker>1978</marker>
<rawString>Longman Dictionary of Contentporary English (1978), second edition (1987), Longman Group Limited. Harlow and London. England. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvilt</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language,</journal>
<publisher>Longman Inc.,</publisher>
<location>New York.</location>
<marker>Quirk, Greenbaum, Leech, Svartvilt, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech and Jan Svartvilt (1985). A Comprehensive Grammar of the English Language, Longman Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Sager</author>
</authors>
<date>1981</date>
<booktitle>Natural Language Information Processing,</booktitle>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Mass.</location>
<marker>Sager, 1981</marker>
<rawString>Naomi Sager (1981). Natural Language Information Processing, Addison-Wesley, Reading, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>The grammatical database and parsing scheme&apos; in Garside, Leech and Sampson.</title>
<date>1987</date>
<pages>82--96</pages>
<contexts>
<context position="11215" citStr="Sampson, 1987" startWordPosition="1760" endWordPosition="1761">pecification of an appropriate grammar of rules and symbols and (2) the construction of a sufficiently large databank of parsed sentences conforming to the (optimal) grammar specified in (1) to provide statistics of&apos; the relative likelihoods of constituent tag transitions for constituent tag disambiguation. In order to meet these prior requirements, researchers have been employed on a full-time basis to assemble a corpus of parsed sentences. 6. Grammar Development and Parsed Subcorpora The databank of approximately 45,000 words of manually parsed sentences of the Lancaster/Oslo-Bergen corpus (Sampson, 1987: 83ff). was processed to .show the distinct types of production rules and their frequency of occurrence in the grammar associated with the Sampson neebank. Experience of the UCREL probabilistic system (Garside and Leech. 1987: 66ff) and suggestions from other researchers prompting new rules resulted in a new context-free grammar of about 6,000 productions creating more steeply nested structures than those of the Sampson grammar. (It was anticipated that steeper nesting would reduce the size of the treebank required to obtain adequate frequency statistics.) The new grammar is defined descripti</context>
</contexts>
<marker>Sampson, 1987</marker>
<rawString>Geoffrey Sampson (1987). &apos;The grammatical database and parsing scheme&apos; in Garside, Leech and Sampson. pp 82-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard A Sharman</author>
</authors>
<title>The Winchester Unification Parsing System&apos;,</title>
<date>1988</date>
<tech>IBM UKSC Report 999:</tech>
<marker>Sharman, 1988</marker>
<rawString>Richard A. Sharman (1988). &apos;The Winchester Unification Parsing System&apos;, IBM UKSC Report 999: April 1988.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>