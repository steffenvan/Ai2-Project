<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054294">
<title confidence="0.9983465">
Inferring semantic roles using sub-categorization frames and
maximum entropy model
</title>
<author confidence="0.970653">
Akshar Bharati, Sriram Venkatapathy and Prashanth Reddy
</author>
<affiliation confidence="0.97658">
Language Technologies Research Centre, IIIT - Hyderabad, India.
</affiliation>
<email confidence="0.99818">
{sriram,prashanth}@research.iiit.ac.in
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950730769231">
In this paper, we propose an approach
for inferring semantic role using sub-
categorization frames and maximum
entropy model. Our approach aims to
use the sub-categorization information
of the verb to label the mandatory ar-
guments of the verb in various possi-
ble ways. The ambiguity between the
assignment of roles to mandatory argu-
ments is resolved using the maximum
entropy model. The unlabelled manda-
tory arguments and the optional argu-
ments are labelled directly using the
maximum entropy model such that their
labels are not one among the frame el-
ements of the sub-categorization frame
used. Maximum entropy model is pre-
ferred because of its novel approach
of smoothing. Using this approach,
we obtained an F-measure of 68.14%
on the development set of the data
provided for the CONLL-2005 shared
task. We show that this approach per-
forms well in comparison to an ap-
proach which uses only the maximum
entropy model.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999367692307692">
Semantic role labelling is the task of assigning
appropriate semantic roles to the arguments of
a verb. The semantic role information is impor-
tant for various applications in NLP such as Ma-
chine Translation, Question Answering, Informa-
tion Extraction etc. In general, semantic role in-
formation is useful for sentence understanding.
We submitted our system for closed challenge
at CONLL-2005 shared task. This task encour-
ages participants to use novel machine learning
techniques suited to the task of semantic role la-
belling. Previous approaches on semantic role
labelling can be classified into three categories
</bodyText>
<listItem confidence="0.9986834">
(1) Explicit Probabilistic methods (Gildea and
Jurafsky, 2002). (2) General machine learning
algorithms (Pradhan et al., 2003) (Lim et al.,
2004) and (3) Generative model (Thompson et
al., 2003).
</listItem>
<bodyText confidence="0.995223681818182">
Our approach has two stages; first, identifica-
tion whether the argument is mandatory or op-
tional and second, the classification or labelling
of the arguments. In the first stage, the arguments
of a verb are put into three classes, (1) mandatory,
(2) optional or (3) null. Null stands for the fact
that the constituent of the verb in the sentence is
not an semantic argument of the verb. It is used to
rule out the false argument of the verb which were
obtained using the parser. The maximum entropy
based classifier is used to classify the arguments
into one of the above three labels.
After obtaining information about the nature of
the non-null arguments, we proceed in the second
stage to classify the mandatory and optional ar-
guments into their semantic roles. The propbank
sub-categorization frames are used to assign roles
to the mandatory arguments. For example, in the
sentence ”John saw a tree”, the sub-categorization
frame ”A0 v A1” would assign the roles A0 to
John and A1 to tree respectively. After using
all the sub-categorization frames of the verb irre-
</bodyText>
<page confidence="0.98244">
165
</page>
<note confidence="0.433125">
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 165–168, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99990275">
spective of the verb sense, there could be ambigu-
ity in the assignment of semantic roles to manda-
tory arguments. The unlabelled mandatory argu-
ments and the optional arguments are assigned
the most probable semantic role which is not one
of the frame elements of the sub-categorization
frame using the maximum entropy model. Now,
among all the sequences of roles assigned to the
non-null arguments, the sequence which has the
maximum joint probability is chosen. We ob-
tained an accuracy of 68.14% using our approach.
We also show that our approach performs better
in comparision to an approach with uses a simple
maximum entropy model. In section 4, we will
talk about our approach in greater detail.
This paper is organised as follows, (2) Features,
(3) Maximum entropy model, (4) Description of
our system, (5) Results, (6) Comparison with our
other experiments, (7) Conclusion and (8) Future
work.
</bodyText>
<sectionHeader confidence="0.998088" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.999829333333333">
The following are the features used to train the
maximum entropy classifier for both the argument
identification and argument classification. We
used only simple features for these experiments,
we are planning to use richer features in the near
future.
</bodyText>
<listItem confidence="0.9997002">
1. Verb/Predicate.
2. Voice of the verb.
3. Constituent head and Part of Speech tag.
4. Label of the constituent.
5. Relative position of the constituent with re-
spect to the verb.
6. The path of the constituent to the verb
phrase.
7. Preposition of the constituent, NULL if it
doesn’t exist.
</listItem>
<sectionHeader confidence="0.986658" genericHeader="method">
3 Maximum entropy model
</sectionHeader>
<bodyText confidence="0.999981571428571">
The maximum entropy approach became the pre-
ferred approach of probabilistic model builders
for its flexibility and its novel approach to
smoothing (Ratnaparakhi, 1999).
Many classification tasks are most naturally
handled by representing the instance to be classi-
fied as a vector of features. We represent features
as binary functions of two arguments, f(a,H),
where ’a’ is the observation or the class and ’H’ is
the history. For example, a feature fi(a, H) is true
if ’a’ is Ram and ’H’ is ’AGENT of a verb’. In a
log linear model, the probability function P(a|H)
with a set of features f1, f2, ....fj that connects ’a’
to the history ’H’, takes the following form.
</bodyText>
<equation confidence="0.996528">
P(a|H) =
Z(H)
</equation>
<bodyText confidence="0.999927142857143">
Here Ai’s are weights between negative and
positive infinity that indicate the relative impor-
tance of a feature: the more relevant the feature to
the value of the probability, the higher the abso-
lute value of the associated lambda. Z(H), called
the partition function, is the normalizing constant
(for a fixed H).
</bodyText>
<sectionHeader confidence="0.963493" genericHeader="method">
4 Description of our system
</sectionHeader>
<bodyText confidence="0.999949111111111">
Our approach labels the semantic roles in two
stages, (1) argument identification and (2) ar-
gument classification. As input to our sys-
tem, we use full syntactic information (Collins,
1999), Named-entities, Verb senses and Propbank
frames. For our experiments, we use Zhang Le’s
Maxent Toolkit 1, and the L-BFGS parameter esti-
mation algorithm with Gaussian prior smoothing
(Chen and Rosenfield, 1999).
</bodyText>
<subsectionHeader confidence="0.994017">
4.1 Argument identification
</subsectionHeader>
<bodyText confidence="0.9988044">
The first task in this stage is to find the candidate
arguments and their boundaries using a parser.
We use Collins parser to infer a list of candidate
arguments for every predicate. The following are
some of the sub-stages in this task.
</bodyText>
<listItem confidence="0.9979372">
• Convert the CFG tree given by Collins parser
to a dependency tree.
• Eliminate auxilliary verbs etc.
• Mark the head of relative clause as an argu-
ment of the verb.
</listItem>
<footnote confidence="0.701602">
1http://www.nlplab.cn/zhangle/maxent toolkit.html
,Ei Ai(a,H)*fi(a,H)
</footnote>
<page confidence="0.964608">
166
</page>
<listItem confidence="0.9358566">
• If a verb is modified by another verb, the
syntactic arguments of the superior verb
are considered as shared arguments between
both the verbs.
• If a prepositional phrase attached to a verb
</listItem>
<bodyText confidence="0.972469413793103">
contains more than one noun phrase, attach
the second noun phrase to the verb.
The second task is to filter out the constituents
which are not really the arguments of the pred-
icate. Given our approach towards argument
classification, we also need information about
whether an argument is mandatory or optional.
Hence, in this stage the constituents are marked
using three labels, (1) MANDATORY argument,
(2) OPTIONAL argument and (3) NULL, using a
maximum entropy classifier. For example, a sen-
tence ”John was playing football in the evening”,
”John” is marked MANDATORY, ”football” is
marked MANDATORY and ”in the evening” is
marked OPTIONAL.
For training, the Collins parser is run on the
training data and the syntactic arguments are
identified. Among these arguments, the ones
which do not exist in the propbank annotation of
the training data are marked as null. Among the
remaining arguments, the arguments are marked
as mandatory or optional according to the prop-
bank frame information. Mandatory roles are
those appearing in the propbank frames of the
verb and its sense, the rest are marked as optional.
A propbank frame contains information as illus-
trated by the following example:
If Verb = play, sense = 01,
then the roles A0, A1 are MANDATORY.
</bodyText>
<subsectionHeader confidence="0.995959">
4.2 Argument classification
</subsectionHeader>
<bodyText confidence="0.997325153846154">
Argument classification is done in two steps. In
the first step, the propbank sub-categorization
frames are used to assign the semantic roles to the
mandatory arguments in the order specified by the
sub-categorization frames. Sometimes, the num-
ber of mandatory arguments of a verb in the sen-
tence may be less than the number of roles which
can be assigned by the sub-categorization frame.
For example, in the sentence
”MAN1 MAN2 V MAN3 OPT1”, roles could
be assigned in the following two possible ways by
the sub-categorization frame ”A0 v A1” of verb
V1.
</bodyText>
<listItem confidence="0.992407">
• A0[MAN1] MAN2 V1 A1[MAN3] OPT1
• MAN1 A0[MAN2] V A1[MAN3] OPT1
</listItem>
<bodyText confidence="0.999809090909091">
In the second step, the task is to label the un-
labelled mandatory arguments and the arguments
which are marked as optional. This is done by
marking these arguments with the most probable
semantic role which is not one of the frame ele-
ments of the sub-categorization frame ”A0 v A1”.
In the above example, the unlabelled mandatory
arguments and the optional arguments cannot be
labelled as either A0 or A1. Hence, after this step,
the following might be the role-labelling for the
sentence ”MAN1 MAN2 V1 MAN3 OPT1”.
</bodyText>
<listItem confidence="0.987905">
• A0[MAN1] AM-TMP[MAN2] V1
A1[MAN3] AM-LOC[OPT1]
• AM-MNC[MAN1] A0[MAN2] V1
A1[MAN3] AM-LOC[OPT1]
</listItem>
<bodyText confidence="0.99467225">
The best possible sequence of semantic roles
(R) is decided by the taking the product of prob-
abilities of individual assignments. This also dis-
ambiguates the ambiguity in the assignment of
mandatory roles. The individual probabilities are
computed using the maximum entropy model.
For a sequence R, the product of the probabilities
is defined as
</bodyText>
<equation confidence="0.992828">
P(R) = IIRzERP(Rz|Argz)
</equation>
<bodyText confidence="0.989029">
The best sequence of semantic roles R is de-
fined as
</bodyText>
<equation confidence="0.991937">
R = argmax P(R)
</equation>
<bodyText confidence="0.999908375">
For training the maximum entropy model, the
outcomes are all the possible semantic roles. The
list of sub-categorization frames for a verb is ob-
tained from the training data using information
about mandatory roles from the propbank. The
propbank sub-categorization frames are also ap-
pended to this list.
We present our results in the next section.
</bodyText>
<page confidence="0.991585">
167
</page>
<table confidence="0.999996631578948">
Precision Recall Fp=1
Development 71.88% 64.76% 68.14
Test WSJ 73.76% 65.52% 69.40
Test Brown 65.25% 55.72% 60.11
Test WSJ+Brown 72.66% 64.21% 68.17
Test WSJ Precision Recall Fp=1
Overall 73.76% 65.52% 69.40
A0 85.17% 73.34% 78.81
A1 74.08% 66.08% 69.86
A2 54.51% 48.47% 51.31
A3 52.54% 35.84% 42.61
A4 71.13% 67.65% 69.35
A5 25.00% 20.00% 22.22
AM-ADV 52.18% 47.23% 49.59
AM-CAU 60.42% 39.73% 47.93
AM-DIR 45.65% 24.71% 32.06
AM-DIS 75.24% 73.12% 74.17
AM-EXT 73.68% 43.75% 54.90
AM-LOC 50.80% 43.53% 46.88
AM-MNR 47.24% 49.71% 48.44
AM-MOD 93.67% 91.29% 92.46
AM-NEG 94.67% 92.61% 93.63
AM-PNC 42.02% 43.48% 42.74
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 74.13% 66.97% 70.37
R-A0 82.27% 80.80% 81.53
R-A1 73.28% 61.54% 66.90
R-A2 75.00% 37.50% 50.00
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 100.00% 57.14% 72.73
R-AM-MNR 25.00% 16.67% 20.00
R-AM-TMP 70.00% 53.85% 60.87
V 97.28% 97.28% 97.28
</table>
<tableCaption confidence="0.970579">
Table 1: Overall results (top) and detailed results
on the WSJ test (bottom).
</tableCaption>
<sectionHeader confidence="0.99995" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999460625">
The results of our approach are presented in table
1.
When we used an approach which uses a sim-
ple maximum entropy model, we obtained an F-
measure of 67.03%. Hence, we show that the
sub-categorization frames help in predicting the
semantic roles of the mandatory arguments, thus
improving the overall performance.
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999925666666667">
In this paper, we propose an approach for in-
ferring semantic role using sub-categorization
frames and maximum entropy model. Using this
approach, we obtained an F-measure of 68.14%
on the development set of the data provided for
the CONLL-2005 shared task.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="method">
7 Future work
</sectionHeader>
<bodyText confidence="0.999998090909091">
We have observed that the main limitation of our
system was in argument identification. Currently,
the recall of the arguments inferred from the out-
put of the parser is 75.52% which makes it the up-
per bound of recall of our system. In near future,
we would focus on increasing the upper bound
of recall. In this direction, we would also use
the partial syntactic information. The accuracy
of the first stage of our approach would increase
if we include the mandatory/optional information
for training the parser (Yi and Palmer, 1999).
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99984275">
We would like to thank Prof. Rajeev Sangal, Dr.
Sushama Bendre and Dr. Dipti Misra Sharma for
guiding us in this project. We would like to thank
Szu-ting for giving some valuable advice.
</bodyText>
<sectionHeader confidence="0.999454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995708">
S. Chen and R. Rosenfield. 1999. A gaussian prior for
smoothing maximum entropy models.
M. Collins. 1999. Head driven statistical models for
natural language processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles.
Hwang Young Sook Lim, Joon-H and, So-Young Park,
and Hae-Chang Rim. 2004. Semantic role labelling
using maximum entropy model.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James. H. Martin, and Daniel Juraf-
sky. 2003. Support Vector Learning for Semantic
Argument Classification.
Adwait Ratnaparakhi. 1999. Learning to parse natural
language with maximum entropy models.
Cynthia A. Thompson, Roger Levy, and Christo-
pher D. Manning. 2003. A generative model for
semantic role labelling.
Szu-ting Yi and M. Palmer. 1999. The integration of
syntactic parsing and semantic role labeling.
</reference>
<page confidence="0.997291">
168
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.199438">
<title confidence="0.91234">Inferring semantic roles using sub-categorization frames</title>
<abstract confidence="0.930956966666667">maximum entropy model Akshar Bharati, Sriram Venkatapathy and Prashanth Language Technologies Research Centre, IIIT - Hyderabad, Abstract In this paper, we propose an approach for inferring semantic role using subcategorization frames and maximum entropy model. Our approach aims to use the sub-categorization information of the verb to label the mandatory arguments of the verb in various possible ways. The ambiguity between the assignment of roles to mandatory arguments is resolved using the maximum entropy model. The unlabelled mandatory arguments and the optional arguments are labelled directly using the maximum entropy model such that their labels are not one among the frame elements of the sub-categorization frame used. Maximum entropy model is preferred because of its novel approach of smoothing. Using this approach, we obtained an F-measure of 68.14% on the development set of the data provided for the CONLL-2005 shared task. We show that this approach performs well in comparison to an approach which uses only the maximum entropy model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>R Rosenfield</author>
</authors>
<title>A gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<contexts>
<context position="6142" citStr="Chen and Rosenfield, 1999" startWordPosition="986" endWordPosition="989">e more relevant the feature to the value of the probability, the higher the absolute value of the associated lambda. Z(H), called the partition function, is the normalizing constant (for a fixed H). 4 Description of our system Our approach labels the semantic roles in two stages, (1) argument identification and (2) argument classification. As input to our system, we use full syntactic information (Collins, 1999), Named-entities, Verb senses and Propbank frames. For our experiments, we use Zhang Le’s Maxent Toolkit 1, and the L-BFGS parameter estimation algorithm with Gaussian prior smoothing (Chen and Rosenfield, 1999). 4.1 Argument identification The first task in this stage is to find the candidate arguments and their boundaries using a parser. We use Collins parser to infer a list of candidate arguments for every predicate. The following are some of the sub-stages in this task. • Convert the CFG tree given by Collins parser to a dependency tree. • Eliminate auxilliary verbs etc. • Mark the head of relative clause as an argument of the verb. 1http://www.nlplab.cn/zhangle/maxent toolkit.html ,Ei Ai(a,H)*fi(a,H) 166 • If a verb is modified by another verb, the syntactic arguments of the superior verb are co</context>
</contexts>
<marker>Chen, Rosenfield, 1999</marker>
<rawString>S. Chen and R. Rosenfield. 1999. A gaussian prior for smoothing maximum entropy models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head driven statistical models for natural language processing.</title>
<date>1999</date>
<contexts>
<context position="5931" citStr="Collins, 1999" startWordPosition="957" endWordPosition="958">...fj that connects ’a’ to the history ’H’, takes the following form. P(a|H) = Z(H) Here Ai’s are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated lambda. Z(H), called the partition function, is the normalizing constant (for a fixed H). 4 Description of our system Our approach labels the semantic roles in two stages, (1) argument identification and (2) argument classification. As input to our system, we use full syntactic information (Collins, 1999), Named-entities, Verb senses and Propbank frames. For our experiments, we use Zhang Le’s Maxent Toolkit 1, and the L-BFGS parameter estimation algorithm with Gaussian prior smoothing (Chen and Rosenfield, 1999). 4.1 Argument identification The first task in this stage is to find the candidate arguments and their boundaries using a parser. We use Collins parser to infer a list of candidate arguments for every predicate. The following are some of the sub-stages in this task. • Convert the CFG tree given by Collins parser to a dependency tree. • Eliminate auxilliary verbs etc. • Mark the head of</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head driven statistical models for natural language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<contexts>
<context position="1860" citStr="Gildea and Jurafsky, 2002" startWordPosition="280" endWordPosition="283">appropriate semantic roles to the arguments of a verb. The semantic role information is important for various applications in NLP such as Machine Translation, Question Answering, Information Extraction etc. In general, semantic role information is useful for sentence understanding. We submitted our system for closed challenge at CONLL-2005 shared task. This task encourages participants to use novel machine learning techniques suited to the task of semantic role labelling. Previous approaches on semantic role labelling can be classified into three categories (1) Explicit Probabilistic methods (Gildea and Jurafsky, 2002). (2) General machine learning algorithms (Pradhan et al., 2003) (Lim et al., 2004) and (3) Generative model (Thompson et al., 2003). Our approach has two stages; first, identification whether the argument is mandatory or optional and second, the classification or labelling of the arguments. In the first stage, the arguments of a verb are put into three classes, (1) mandatory, (2) optional or (3) null. Null stands for the fact that the constituent of the verb in the sentence is not an semantic argument of the verb. It is used to rule out the false argument of the verb which were obtained using</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwang Young Sook Lim</author>
<author>Joon-H and</author>
<author>So-Young Park</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Semantic role labelling using maximum entropy model.</title>
<date>2004</date>
<contexts>
<context position="1943" citStr="Lim et al., 2004" startWordPosition="293" endWordPosition="296">rtant for various applications in NLP such as Machine Translation, Question Answering, Information Extraction etc. In general, semantic role information is useful for sentence understanding. We submitted our system for closed challenge at CONLL-2005 shared task. This task encourages participants to use novel machine learning techniques suited to the task of semantic role labelling. Previous approaches on semantic role labelling can be classified into three categories (1) Explicit Probabilistic methods (Gildea and Jurafsky, 2002). (2) General machine learning algorithms (Pradhan et al., 2003) (Lim et al., 2004) and (3) Generative model (Thompson et al., 2003). Our approach has two stages; first, identification whether the argument is mandatory or optional and second, the classification or labelling of the arguments. In the first stage, the arguments of a verb are put into three classes, (1) mandatory, (2) optional or (3) null. Null stands for the fact that the constituent of the verb in the sentence is not an semantic argument of the verb. It is used to rule out the false argument of the verb which were obtained using the parser. The maximum entropy based classifier is used to classify the arguments</context>
</contexts>
<marker>Lim, and, Park, Rim, 2004</marker>
<rawString>Hwang Young Sook Lim, Joon-H and, So-Young Park, and Hae-Chang Rim. 2004. Semantic role labelling using maximum entropy model.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support Vector Learning for Semantic Argument Classification.</title>
<date>2003</date>
<marker>Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James. H. Martin, and Daniel Jurafsky. 2003. Support Vector Learning for Semantic Argument Classification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparakhi</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<contexts>
<context position="4886" citStr="Ratnaparakhi, 1999" startWordPosition="777" endWordPosition="778">argument classification. We used only simple features for these experiments, we are planning to use richer features in the near future. 1. Verb/Predicate. 2. Voice of the verb. 3. Constituent head and Part of Speech tag. 4. Label of the constituent. 5. Relative position of the constituent with respect to the verb. 6. The path of the constituent to the verb phrase. 7. Preposition of the constituent, NULL if it doesn’t exist. 3 Maximum entropy model The maximum entropy approach became the preferred approach of probabilistic model builders for its flexibility and its novel approach to smoothing (Ratnaparakhi, 1999). Many classification tasks are most naturally handled by representing the instance to be classified as a vector of features. We represent features as binary functions of two arguments, f(a,H), where ’a’ is the observation or the class and ’H’ is the history. For example, a feature fi(a, H) is true if ’a’ is Ram and ’H’ is ’AGENT of a verb’. In a log linear model, the probability function P(a|H) with a set of features f1, f2, ....fj that connects ’a’ to the history ’H’, takes the following form. P(a|H) = Z(H) Here Ai’s are weights between negative and positive infinity that indicate the relati</context>
</contexts>
<marker>Ratnaparakhi, 1999</marker>
<rawString>Adwait Ratnaparakhi. 1999. Learning to parse natural language with maximum entropy models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative model for semantic role labelling.</title>
<date>2003</date>
<contexts>
<context position="1992" citStr="Thompson et al., 2003" startWordPosition="301" endWordPosition="304">s Machine Translation, Question Answering, Information Extraction etc. In general, semantic role information is useful for sentence understanding. We submitted our system for closed challenge at CONLL-2005 shared task. This task encourages participants to use novel machine learning techniques suited to the task of semantic role labelling. Previous approaches on semantic role labelling can be classified into three categories (1) Explicit Probabilistic methods (Gildea and Jurafsky, 2002). (2) General machine learning algorithms (Pradhan et al., 2003) (Lim et al., 2004) and (3) Generative model (Thompson et al., 2003). Our approach has two stages; first, identification whether the argument is mandatory or optional and second, the classification or labelling of the arguments. In the first stage, the arguments of a verb are put into three classes, (1) mandatory, (2) optional or (3) null. Null stands for the fact that the constituent of the verb in the sentence is not an semantic argument of the verb. It is used to rule out the false argument of the verb which were obtained using the parser. The maximum entropy based classifier is used to classify the arguments into one of the above three labels. After obtain</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>Cynthia A. Thompson, Roger Levy, and Christopher D. Manning. 2003. A generative model for semantic role labelling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Szu-ting Yi</author>
<author>M Palmer</author>
</authors>
<title>The integration of syntactic parsing and semantic role labeling.</title>
<date>1999</date>
<marker>Yi, Palmer, 1999</marker>
<rawString>Szu-ting Yi and M. Palmer. 1999. The integration of syntactic parsing and semantic role labeling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>