<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002112">
<note confidence="0.539005666666667">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
Association for Computational Linguistics
</note>
<title confidence="0.9975285">
Using Automatically Acquired Predominant Senses for Word Sense
Disambiguation
</title>
<author confidence="0.999823">
Diana McCarthy &amp; Rob Koeling &amp; Julie Weeds &amp; John Carroll
</author>
<affiliation confidence="0.995125">
Department of Informatics,
University of Sussex
</affiliation>
<address confidence="0.995062">
Brighton BN1 9QH, UK
</address>
<email confidence="0.999076">
dianam,robk,juliewe,johnca @sussex.ac.uk
</email>
<sectionHeader confidence="0.995642" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99960585">
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The first (or predominant)
sense heuristic assumes the availability of hand-
tagged data. Whilst there are hand-tagged corpora
available for some languages, these are relatively
small in size and many word forms either do not
occur, or occur infrequently. In this paper we in-
vestigate the performance of an unsupervised first
sense heuristic where predominant senses are ac-
quired automatically from raw text. We evaluate on
both the SENSEVAL-2 and SENSEVAL-3 English all-
words data. For accurate WSD the first sense heuris-
tic should be used only as a back-off, where the evi-
dence from the context is not strong enough. In this
paper however, we examine the performance of the
automatically acquired first sense in isolation since
it turned out that the first sense taken from SemCor
outperformed many systems in SENSEVAL-2.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998941235294118">
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account (McCarthy et al., 2004). The high
performance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where evi-
dence from the context is not sufficient (Hoste et al.,
2001).
The first sense heuristic is a powerful one. Us-
ing the first sense listed in SemCor on the SENSE-
VAL-2 English all-words data we obtained the re-
sults given in table 1, (where the PoS was given by
the gold-standard data in the SENSEVAL-2 data it-
self). 1 Recall is lower than precision because there
are many words which do not occur in SemCor. Use
</bodyText>
<footnote confidence="0.996347">
1We did not include items which were tagged ’U’
(unassignable) by the human annotators.
</footnote>
<table confidence="0.9998635">
PoS precision recall baseline
Noun 70 60 45
Verb 48 44 22
Adjective 71 59 44
Adverb 83 79 59
All PoS 67 59 41
</table>
<tableCaption confidence="0.844403">
Table 1: The SemCor first sense on the SENSEVAL-
2 English all-words data
</tableCaption>
<bodyText confidence="0.998753071428572">
of the first sense listed in WordNet gives 65% pre-
cision and recall for all PoS on these items. The
fourth column on table 1 gives the random base-
line which reflects the polysemy of the data. Ta-
ble 2 shows results obtained when we use the most
common sense for an item and PoS using the fre-
quency in the SENSEVAL-2 English all-words data
itself. Recall is lower than precision since we only
use the heuristic on lemmas which have occurred
more than once and where there is one sense which
has a greater frequency than the others, apart from
trivial monosemous cases. 2 Precision is higher in
table 2 than in table 1 reflecting the difference be-
tween an a priori first sense determined by Sem-
Cor, and an upper bound on the performance of this
heuristic for this data. This upper bound is quite
high because of the very skewed sense distributions
in the test data itself. The upper bound for a docu-
ment, or document collection, will depend on how
homogenous the content of that document collec-
tion is, and the skew of the word sense distributions
therein. Indeed, the bias towards one sense for a
given word in a given document or discourse was
observed by Gale et al. (1992).
Whilst a first sense heuristic based on a sense-
tagged corpus such as SemCor is clearly useful,
there is a case for obtaining a first, or predomi-
nant, sense from untagged corpus data so that a WSD
</bodyText>
<footnote confidence="0.997350666666667">
2If we include polysemous items that have only occurred
once in the data we obtain a precision of 92% and a recall of
85% over all PoS.
</footnote>
<table confidence="0.999848833333333">
PoS precision recall baseline
Noun 95 73 45
Verb 79 43 22
Adjective 88 59 44
Adverb 91 72 59
All PoS 90 63 41
</table>
<tableCaption confidence="0.9532535">
Table 2: The SENSEVAL-2 first sense on the SEN-
SEVAL-2 English all-words data
</tableCaption>
<bodyText confidence="0.9989061">
system can be tuned to a given genre or domain
(McCarthy et al., 2004) and also because there will
be words that occur with insufficient frequency in
the hand-tagged resources available. SemCor com-
prises a relatively small sample of 250,000 words.
There are words where the first sense in WordNet is
counter-intuitive, because this is a small sample, and
because where the frequency data does not indicate
a first sense, the ordering is arbitrary. For exam-
ple the first sense of tiger in WordNet is audacious
person whereas one might expect that carnivorous
animal is a more common usage.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
are investigating a method of automatically ranking
WordNet senses from raw text, with no reliance on
manually sense-tagged data such as that in SemCor.
The paper is structured as follows. We discuss
our method in the following section. Section 3 de-
scribes an experiment using predominant senses ac-
quired from the BNC evaluated on the SENSEVAL-2
English all-words task. In section 4 we present our
results on the SENSEVAL-3 English all-words task.
We discuss related work in section 5 and conclude
in section 6.
</bodyText>
<sectionHeader confidence="0.948711" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999976909090909">
The method is described in (McCarthy et al., 2004),
which we summarise here. We acquire thesauruses
for nouns, verbs, adjectives and adverbs based on
the method proposed by Lin (1998) using grammat-
ical relations output from the RASP parser (Briscoe
and Carroll, 2002). The grammatical contexts used
are listed in table 3, but there is scope for extending
or restricting the contexts for a given PoS.
We use the thesauruses for ranking the senses of
the target words. Each target word ( ) e.g. plant
in the thesaurus is associated with a list of nearest
</bodyText>
<table confidence="0.999095333333333">
PoS grammatical contexts
Noun verb in direct object or subject relation
Verb adjective or noun modifier
Adjective noun as direct object or subject
Adverb modified noun, modifing adverb
modified adjective or verb
</table>
<tableCaption confidence="0.962098">
Table 3: Grammatical contexts used for acquiring
the thesauruses
</tableCaption>
<bodyText confidence="0.9669748125">
neighbours ( ) with distributional similarity
scores ( ) e.g. factory 0.28, refinery 0.17,
tree 0.14 etc... 3 Distributional similarity is a mea-
sure indicating the degree that two words, a word
and its neighbour, occur in similar contexts. The
neighbours reflect the various senses of the word
( ). We assume that the quantity
and similarity of the neighbours pertaining to differ-
ent senses will reflect the relative dominance of the
senses. This is because there will be more relational
data for the more prevalent senses compared to the
less frequent senses. We relate the neighbours to
these senses by a semantic similarity measure using
the WordNet similarity package (Patwardhan and
Pedersen, 2003) ( ), where the sense
of the neighbour ( ) that maximises the similar-
ity to is selected. The measure used for ranking
the senses of a word is calculated using the distribu-
tional similarity scores of the neighbours weighted
by the semantic similarity between the neighbour
and the sense of the target word as shown in equa-
tion 1. The frequency data required by the semantic
similarity measure (jcn (Jiang and Conrath, 1997))
is obtained using the BNC so that no hand-tagged
data is used and our method is fully unsupervised.
where:
For SENSEVAL-3 we obtained thesaurus entries
for all nouns, verbs, adjectives and adverbs using
parsed text from the 90 million words of written En-
glish from the BNC. We created entries for words
which occurred at least 10 times in frames involving
the grammatical relations listed in table 3. We used
</bodyText>
<footnote confidence="0.847778333333333">
3This example it taken from the data at
http://www.cs.ualberta.ca/˜lindek/demos/depsim.htm.
We have removed some intervening neighbours for brevity.
</footnote>
<table confidence="0.973967666666667">
(1)
We rank each sense using:
PoS precision recall
Noun 60 26
Verb 30 07
Adjective 63 09
Adverb 65 07
All PoS 53 49
Noun Adj and Adverbs 61 43
</table>
<tableCaption confidence="0.7948">
Table 4: Using the automatically acquired first sense
on the SENSEVAL-2 English all-words data
</tableCaption>
<bodyText confidence="0.988444">
50 nearest neighbours for ranking, since this thresh-
old has given good results in other experiments.
</bodyText>
<sectionHeader confidence="0.389812" genericHeader="method">
3 Performance of the automatically
acquired First sense on SENSEVAL-2
</sectionHeader>
<bodyText confidence="0.999873375">
We acquired sense rankings for polysemous nouns
in WordNet 1.7.1 that occurred with 10 frames.
This version was used in preparation for SENSE-
VAL-3. We then applied the predominant sense
heuristic from the automatically acquired rankings
to the SENSEVAL-2 data. 4 Recall and precision fig-
ures are calculated using the SENSEVAL-2 scorer;
recall is therefore particularly low for any given PoS
in isolation since this is calculated over the entire
corpus.
The method produces lower results for verbs than
for other PoS, this is in line with the lower per-
formance of a manually acquired first sense heuris-
tic and also reflects the greater polysemy of verbs
shown by the lower random baseline as in tables 1
and 2.
</bodyText>
<sectionHeader confidence="0.999302" genericHeader="method">
4 Results from SENSEVAL-3
</sectionHeader>
<bodyText confidence="0.999884833333333">
For SENSEVAL-3 we used the predominant senses
from the automatic rankings for i) all PoS (autoPS)
and ii) all PoS except verbs (autoPSNVs). The
results are given in table 5. The “without U” re-
sults are used since the lack of a response by our
system occurred when there were no nearest neigh-
bours and so no ranking was available for selecting a
predominant sense, rather than as an indication that
the sense is missing from WordNet. Our system per-
forms well in comparison with the results in SEN-
SEVAL-2 for unsupervised systems which do not use
manually labelled data such as SemCor.
</bodyText>
<sectionHeader confidence="0.999724" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.978742666666667">
There is some related work on ranking the senses of
words. Buitelaar and Sacaleanu (2001) have pre-
viously explored ranking and selection of synsets
</bodyText>
<footnote confidence="0.9988315">
4For this we used the mapping between 1.7 and 1.7.1 avail-
able from http://www.cs.unt.edu/˜rada/downloads.html.
</footnote>
<note confidence="0.683098">
System precision recall
autoPS 49 43
autoPSNVs 56 35
</note>
<tableCaption confidence="0.7875145">
Table 5: Using the automatically acquired first sense
on the SENSEVAL-3 English all-words data
</tableCaption>
<bodyText confidence="0.99913205">
in GermaNet for specific domains using the words
in a given synset, and those related by hyponymy,
and a term relevance measure taken from informa-
tion retrieval. Buitelaar and Bogdan have evaluated
their method on identifying domain specific con-
cepts, rather than for WSD. In recent work, La-
pata and Brew (2004) obtain predominant senses
of verbs occurring in subcategorization frames,
where the senses of verbs are defined using Levin
classes (Levin, 1993). They demonstrate that these
priors are useful for WSD of verbs.
Our ranking method is related to work by Pantel
and Lin (2002) who use automatic thesauruses for
discovering word senses from corpora, rather than
for detecting predominance. In their work, the lists
of neighbours are themselves clustered to bring out
the various senses of the word. They evaluate us-
ing a WordNet similarity measure to determine the
precision and recall of these discovered classes with
respect to WordNet synsets.
</bodyText>
<sectionHeader confidence="0.997467" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99996445">
We have demonstrated that it is possible to acquire
predominant senses from raw textual corpora, and
that these can be used as an unsupervised first sense
heuristic that does not not rely on manually pro-
duced corpora such as SemCor. This approach is
useful for words where there is no manually-tagged
data available. Our predominant senses have been
used within a WSD system as a back-off method
when data is not available from other resources (Vil-
larejo et al., 2004). The method could be particu-
larly useful when tailoring a WSD system to a par-
ticular domain.
We intend to experiment further using a wider va-
riety of grammatical relations, which we hope will
improve performance for verbs, and with data from
larger corpora, such as the Gigaword corpus and the
web, which should allow us to cover a great many
more words which do not occur in manually created
resources such as SemCor. We also intend to apply
our method to domain specific text.
</bodyText>
<sectionHeader confidence="0.997037" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999347428571429">
We would like to thank Siddharth Patwardhan and
Ted Pedersen for making the WN Similarity pack-
age publically available. This work was funded
by EU-2001-34460 project MEANING: Develop-
ing Multilingual Web-scale Language Technolo-
gies, and UK EPSRC project Robust Accurate Sta-
tistical Parsing (RASP).
</bodyText>
<sectionHeader confidence="0.993613" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.980739684210527">
Edward Briscoe and John Carroll. 2002. Robust
accurate statistical annotation of general text.
In Proceedings of the Third International Con-
ference on Language Resources and Evaluation
(LREC), pages 1499–1504, Las Palmas, Canary
Islands, Spain.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance.
In Proceedings of WordNet and Other Lexical
Resources: Applications, Extensions and Cus-
tomizations, NAACL 2001 Workshop, Pittsburgh,
PA.
William Gale, Kenneth Church, and David
Yarowsky. 1992. A method for disambiguating
word senses in a large corpus. Computers and the
Humanities, 26:415–439.
V´eronique Hoste, Anne Kool, and Walter Daele-
mans. 2001. Classifier optimization and combi-
nation in the English all words task. In Proceed-
ings of the SENSEVAL-2 workshop, pages 84–86.
Jay Jiang and David Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical tax-
onomy. In International Conference on Research
in Computational Linguistics, Taiwan.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Com-
putational Linguistics, 30(1):45–75.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a Preliminary Investigation. University
of Chicago Press, Chicago and London.
Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of
COLING-ACL 98, Montreal, Canada.
Diana McCarthy, Rob Koeling, Julie Weeds, and
John Carroll. 2004. Finding predominant senses
in untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computa-
tional Linguistics, Barcelona, Spain.
Patrick Pantel and Dekang Lin. 2002. Discover-
ing word senses from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, pages 613–619, Ed-
monton, Canada.
Siddharth Patwardhan and Ted Pedersen. 2003.
The cpan wordnet::similarity package.
http://search.cpan.org/author/SID/WordNet-
Similarity-0.03/.
Luis Villarejo, Lluis M`arquez, Eneko Agirre, David
Martinez, Bernardo Magnini, Carlo Strapparava,
Diana McCarthy, Andr´es Monotoyo, and Ar-
mando Su´arez. 2004. The “MEANING” system
on the English all words task. In Proceedings of
the SENSEVAL-3 workshop.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation performance across di-
verse parameter spaces. Natural Language Engi-
neering, 8(4):293–310.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.281377">
<note confidence="0.775298666666667">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics</note>
<title confidence="0.9978125">Using Automatically Acquired Predominant Senses for Word Sense Disambiguation</title>
<author confidence="0.99831">Diana McCarthy</author>
<author confidence="0.99831">Rob Koeling</author>
<author confidence="0.99831">Julie Weeds</author>
<author confidence="0.99831">John</author>
<affiliation confidence="0.999272">Department of University of</affiliation>
<address confidence="0.906014">Brighton BN1 9QH,</address>
<email confidence="0.961058">dianam,robk,juliewe,johnca@sussex.ac.uk</email>
<abstract confidence="0.979209380952381">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The first (or predominant) sense heuristic assumes the availability of handtagged data. Whilst there are hand-tagged corpora available for some languages, these are relatively small in size and many word forms either do not occur, or occur infrequently. In this paper we investigate the performance of an unsupervised first sense heuristic where predominant senses are acquired automatically from raw text. We evaluate on the and English alldata. For accurate first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough. In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor many systems in</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Edward Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>1499--1504</pages>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="5936" citStr="Briscoe and Carroll, 2002" startWordPosition="1012" endWordPosition="1015"> is structured as follows. We discuss our method in the following section. Section 3 describes an experiment using predominant senses acquired from the BNC evaluated on the SENSEVAL-2 English all-words task. In section 4 we present our results on the SENSEVAL-3 English all-words task. We discuss related work in section 5 and conclude in section 6. 2 Method The method is described in (McCarthy et al., 2004), which we summarise here. We acquire thesauruses for nouns, verbs, adjectives and adverbs based on the method proposed by Lin (1998) using grammatical relations output from the RASP parser (Briscoe and Carroll, 2002). The grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts for a given PoS. We use the thesauruses for ranking the senses of the target words. Each target word ( ) e.g. plant in the thesaurus is associated with a list of nearest PoS grammatical contexts Noun verb in direct object or subject relation Verb adjective or noun modifier Adjective noun as direct object or subject Adverb modified noun, modifing adverb modified adjective or verb Table 3: Grammatical contexts used for acquiring the thesauruses neighbours ( ) with distributional si</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Edward Briscoe and John Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC), pages 1499–1504, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Buitelaar</author>
<author>Bogdan Sacaleanu</author>
</authors>
<title>Ranking and selecting synsets by domain relevance.</title>
<date>2001</date>
<booktitle>In Proceedings of WordNet and Other Lexical Resources: Applications, Extensions and Customizations, NAACL 2001 Workshop,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="10017" citStr="Buitelaar and Sacaleanu (2001)" startWordPosition="1695" endWordPosition="1698">omatic rankings for i) all PoS (autoPS) and ii) all PoS except verbs (autoPSNVs). The results are given in table 5. The “without U” results are used since the lack of a response by our system occurred when there were no nearest neighbours and so no ranking was available for selecting a predominant sense, rather than as an indication that the sense is missing from WordNet. Our system performs well in comparison with the results in SENSEVAL-2 for unsupervised systems which do not use manually labelled data such as SemCor. 5 Related Work There is some related work on ranking the senses of words. Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets 4For this we used the mapping between 1.7 and 1.7.1 available from http://www.cs.unt.edu/˜rada/downloads.html. System precision recall autoPS 49 43 autoPSNVs 56 35 Table 5: Using the automatically acquired first sense on the SENSEVAL-3 English all-words data in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval. Buitelaar and Bogdan have evaluated their method on identifying domain specific concepts, rather than for WSD. In recent</context>
</contexts>
<marker>Buitelaar, Sacaleanu, 2001</marker>
<rawString>Paul Buitelaar and Bogdan Sacaleanu. 2001. Ranking and selecting synsets by domain relevance. In Proceedings of WordNet and Other Lexical Resources: Applications, Extensions and Customizations, NAACL 2001 Workshop, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="3693" citStr="Gale et al. (1992)" startWordPosition="624" endWordPosition="627">mous cases. 2 Precision is higher in table 2 than in table 1 reflecting the difference between an a priori first sense determined by SemCor, and an upper bound on the performance of this heuristic for this data. This upper bound is quite high because of the very skewed sense distributions in the test data itself. The upper bound for a document, or document collection, will depend on how homogenous the content of that document collection is, and the skew of the word sense distributions therein. Indeed, the bias towards one sense for a given word in a given document or discourse was observed by Gale et al. (1992). Whilst a first sense heuristic based on a sensetagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD 2If we include polysemous items that have only occurred once in the data we obtain a precision of 92% and a recall of 85% over all PoS. PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´eronique Hoste</author>
<author>Anne Kool</author>
<author>Walter Daelemans</author>
</authors>
<title>Classifier optimization and combination in the English all words task.</title>
<date>2001</date>
<booktitle>In Proceedings of the SENSEVAL-2 workshop,</booktitle>
<pages>84--86</pages>
<contexts>
<context position="1902" citStr="Hoste et al., 2001" startWordPosition="292" endWordPosition="295">cally acquired first sense in isolation since it turned out that the first sense taken from SemCor outperformed many systems in SENSEVAL-2. 1 Introduction The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al., 2004). The high performance of the first sense baseline is due to the skewed frequency distribution of word senses. Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001). The first sense heuristic is a powerful one. Using the first sense listed in SemCor on the SENSEVAL-2 English all-words data we obtained the results given in table 1, (where the PoS was given by the gold-standard data in the SENSEVAL-2 data itself). 1 Recall is lower than precision because there are many words which do not occur in SemCor. Use 1We did not include items which were tagged ’U’ (unassignable) by the human annotators. PoS precision recall baseline Noun 70 60 45 Verb 48 44 22 Adjective 71 59 44 Adverb 83 79 59 All PoS 67 59 41 Table 1: The SemCor first sense on the SENSEVAL2 Engli</context>
</contexts>
<marker>Hoste, Kool, Daelemans, 2001</marker>
<rawString>V´eronique Hoste, Anne Kool, and Walter Daelemans. 2001. Classifier optimization and combination in the English all words task. In Proceedings of the SENSEVAL-2 workshop, pages 84–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="7624" citStr="Jiang and Conrath, 1997" startWordPosition="1293" endWordPosition="1296">more prevalent senses compared to the less frequent senses. We relate the neighbours to these senses by a semantic similarity measure using the WordNet similarity package (Patwardhan and Pedersen, 2003) ( ), where the sense of the neighbour ( ) that maximises the similarity to is selected. The measure used for ranking the senses of a word is calculated using the distributional similarity scores of the neighbours weighted by the semantic similarity between the neighbour and the sense of the target word as shown in equation 1. The frequency data required by the semantic similarity measure (jcn (Jiang and Conrath, 1997)) is obtained using the BNC so that no hand-tagged data is used and our method is fully unsupervised. where: For SENSEVAL-3 we obtained thesaurus entries for all nouns, verbs, adjectives and adverbs using parsed text from the 90 million words of written English from the BNC. We created entries for words which occurred at least 10 times in frames involving the grammatical relations listed in table 3. We used 3This example it taken from the data at http://www.cs.ualberta.ca/˜lindek/demos/depsim.htm. We have removed some intervening neighbours for brevity. (1) We rank each sense using: PoS precis</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="10646" citStr="Lapata and Brew (2004)" startWordPosition="1792" endWordPosition="1796">reviously explored ranking and selection of synsets 4For this we used the mapping between 1.7 and 1.7.1 available from http://www.cs.unt.edu/˜rada/downloads.html. System precision recall autoPS 49 43 autoPSNVs 56 35 Table 5: Using the automatically acquired first sense on the SENSEVAL-3 English all-words data in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval. Buitelaar and Bogdan have evaluated their method on identifying domain specific concepts, rather than for WSD. In recent work, Lapata and Brew (2004) obtain predominant senses of verbs occurring in subcategorization frames, where the senses of verbs are defined using Levin classes (Levin, 1993). They demonstrate that these priors are useful for WSD of verbs. Our ranking method is related to work by Pantel and Lin (2002) who use automatic thesauruses for discovering word senses from corpora, rather than for detecting predominance. In their work, the lists of neighbours are themselves clustered to bring out the various senses of the word. They evaluate using a WordNet similarity measure to determine the precision and recall of these discover</context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>Mirella Lapata and Chris Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: a Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="10792" citStr="Levin, 1993" startWordPosition="1816" endWordPosition="1817">.html. System precision recall autoPS 49 43 autoPSNVs 56 35 Table 5: Using the automatically acquired first sense on the SENSEVAL-3 English all-words data in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval. Buitelaar and Bogdan have evaluated their method on identifying domain specific concepts, rather than for WSD. In recent work, Lapata and Brew (2004) obtain predominant senses of verbs occurring in subcategorization frames, where the senses of verbs are defined using Levin classes (Levin, 1993). They demonstrate that these priors are useful for WSD of verbs. Our ranking method is related to work by Pantel and Lin (2002) who use automatic thesauruses for discovering word senses from corpora, rather than for detecting predominance. In their work, the lists of neighbours are themselves clustered to bring out the various senses of the word. They evaluate using a WordNet similarity measure to determine the precision and recall of these discovered classes with respect to WordNet synsets. 6 Conclusions We have demonstrated that it is possible to acquire predominant senses from raw textual </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: a Preliminary Investigation. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="5852" citStr="Lin (1998)" startWordPosition="1001" endWordPosition="1002">ance on manually sense-tagged data such as that in SemCor. The paper is structured as follows. We discuss our method in the following section. Section 3 describes an experiment using predominant senses acquired from the BNC evaluated on the SENSEVAL-2 English all-words task. In section 4 we present our results on the SENSEVAL-3 English all-words task. We discuss related work in section 5 and conclude in section 6. 2 Method The method is described in (McCarthy et al., 2004), which we summarise here. We acquire thesauruses for nouns, verbs, adjectives and adverbs based on the method proposed by Lin (1998) using grammatical relations output from the RASP parser (Briscoe and Carroll, 2002). The grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts for a given PoS. We use the thesauruses for ranking the senses of the target words. Each target word ( ) e.g. plant in the thesaurus is associated with a list of nearest PoS grammatical contexts Noun verb in direct object or subject relation Verb adjective or noun modifier Adjective noun as direct object or subject Adverb modified noun, modifing adverb modified adjective or verb Table 3: Grammatic</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1626" citStr="McCarthy et al., 2004" startWordPosition="246" endWordPosition="249">ext. We evaluate on both the SENSEVAL-2 and SENSEVAL-3 English allwords data. For accurate WSD the first sense heuristic should be used only as a back-off, where the evidence from the context is not strong enough. In this paper however, we examine the performance of the automatically acquired first sense in isolation since it turned out that the first sense taken from SemCor outperformed many systems in SENSEVAL-2. 1 Introduction The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al., 2004). The high performance of the first sense baseline is due to the skewed frequency distribution of word senses. Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001). The first sense heuristic is a powerful one. Using the first sense listed in SemCor on the SENSEVAL-2 English all-words data we obtained the results given in table 1, (where the PoS was given by the gold-standard data in the SENSEVAL-2 data itself). 1 Recall is lower than precision because there are many words which do n</context>
<context position="4284" citStr="McCarthy et al., 2004" startWordPosition="740" endWordPosition="743"> observed by Gale et al. (1992). Whilst a first sense heuristic based on a sensetagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD 2If we include polysemous items that have only occurred once in the data we obtain a precision of 92% and a recall of 85% over all PoS. PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41 Table 2: The SENSEVAL-2 first sense on the SENSEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available. SemCor comprises a relatively small sample of 250,000 words. There are words where the first sense in WordNet is counter-intuitive, because this is a small sample, and because where the frequency data does not indicate a first sense, the ordering is arbitrary. For example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage. Assuming that one had an accurate WSD system then one could obtain frequency coun</context>
<context position="5719" citStr="McCarthy et al., 2004" startWordPosition="978" endWordPosition="981">xamples (Yarowsky and Florian, 2002) available. We are investigating a method of automatically ranking WordNet senses from raw text, with no reliance on manually sense-tagged data such as that in SemCor. The paper is structured as follows. We discuss our method in the following section. Section 3 describes an experiment using predominant senses acquired from the BNC evaluated on the SENSEVAL-2 English all-words task. In section 4 we present our results on the SENSEVAL-3 English all-words task. We discuss related work in section 5 and conclude in section 6. 2 Method The method is described in (McCarthy et al., 2004), which we summarise here. We acquire thesauruses for nouns, verbs, adjectives and adverbs based on the method proposed by Lin (1998) using grammatical relations output from the RASP parser (Briscoe and Carroll, 2002). The grammatical contexts used are listed in table 3, but there is scope for extending or restricting the contexts for a given PoS. We use the thesauruses for ranking the senses of the target words. Each target word ( ) e.g. plant in the thesaurus is associated with a list of nearest PoS grammatical contexts Noun verb in direct object or subject relation Verb adjective or noun mo</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="10920" citStr="Pantel and Lin (2002)" startWordPosition="1837" endWordPosition="1840">e SENSEVAL-3 English all-words data in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval. Buitelaar and Bogdan have evaluated their method on identifying domain specific concepts, rather than for WSD. In recent work, Lapata and Brew (2004) obtain predominant senses of verbs occurring in subcategorization frames, where the senses of verbs are defined using Levin classes (Levin, 1993). They demonstrate that these priors are useful for WSD of verbs. Our ranking method is related to work by Pantel and Lin (2002) who use automatic thesauruses for discovering word senses from corpora, rather than for detecting predominance. In their work, the lists of neighbours are themselves clustered to bring out the various senses of the word. They evaluate using a WordNet similarity measure to determine the precision and recall of these discovered classes with respect to WordNet synsets. 6 Conclusions We have demonstrated that it is possible to acquire predominant senses from raw textual corpora, and that these can be used as an unsupervised first sense heuristic that does not not rely on manually produced corpora</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 613–619, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>The cpan wordnet::similarity package.</title>
<date>2003</date>
<note>http://search.cpan.org/author/SID/WordNetSimilarity-0.03/.</note>
<contexts>
<context position="7202" citStr="Patwardhan and Pedersen, 2003" startWordPosition="1219" endWordPosition="1222">8, refinery 0.17, tree 0.14 etc... 3 Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts. The neighbours reflect the various senses of the word ( ). We assume that the quantity and similarity of the neighbours pertaining to different senses will reflect the relative dominance of the senses. This is because there will be more relational data for the more prevalent senses compared to the less frequent senses. We relate the neighbours to these senses by a semantic similarity measure using the WordNet similarity package (Patwardhan and Pedersen, 2003) ( ), where the sense of the neighbour ( ) that maximises the similarity to is selected. The measure used for ranking the senses of a word is calculated using the distributional similarity scores of the neighbours weighted by the semantic similarity between the neighbour and the sense of the target word as shown in equation 1. The frequency data required by the semantic similarity measure (jcn (Jiang and Conrath, 1997)) is obtained using the BNC so that no hand-tagged data is used and our method is fully unsupervised. where: For SENSEVAL-3 we obtained thesaurus entries for all nouns, verbs, ad</context>
</contexts>
<marker>Patwardhan, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2003. The cpan wordnet::similarity package. http://search.cpan.org/author/SID/WordNetSimilarity-0.03/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Villarejo</author>
<author>Lluis M`arquez</author>
<author>Eneko Agirre</author>
<author>David Martinez</author>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Diana McCarthy</author>
<author>Andr´es Monotoyo</author>
<author>Armando Su´arez</author>
</authors>
<title>The “MEANING” system on the English all words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the SENSEVAL-3</booktitle>
<pages>workshop.</pages>
<marker>Villarejo, M`arquez, Agirre, Martinez, Magnini, Strapparava, McCarthy, Monotoyo, Su´arez, 2004</marker>
<rawString>Luis Villarejo, Lluis M`arquez, Eneko Agirre, David Martinez, Bernardo Magnini, Carlo Strapparava, Diana McCarthy, Andr´es Monotoyo, and Armando Su´arez. 2004. The “MEANING” system on the English all words task. In Proceedings of the SENSEVAL-3 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation performance across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="5133" citStr="Yarowsky and Florian, 2002" startWordPosition="881" endWordPosition="884">dNet is counter-intuitive, because this is a small sample, and because where the frequency data does not indicate a first sense, the ordering is arbitrary. For example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage. Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts. However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available. We are investigating a method of automatically ranking WordNet senses from raw text, with no reliance on manually sense-tagged data such as that in SemCor. The paper is structured as follows. We discuss our method in the following section. Section 3 describes an experiment using predominant senses acquired from the BNC evaluated on the SENSEVAL-2 English all-words task. In section 4 we present our results on the SENSEVAL-3 English all-words task. We discuss related work in section 5 and conclude in section 6. 2 Method The method is described in (McCarthy et al., 2004), which we sum</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation performance across diverse parameter spaces. Natural Language Engineering, 8(4):293–310.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>