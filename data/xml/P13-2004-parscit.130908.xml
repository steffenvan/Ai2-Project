<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000416">
<title confidence="0.998004">
Supervised Model Learning with Feature Grouping
based on a Discrete Constraint
</title>
<author confidence="0.841583">
Jun Suzuki and Masaaki Nagata
</author>
<affiliation confidence="0.731436">
NTT Communication Science Laboratories, NTT Corporation
</affiliation>
<address confidence="0.81234">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
</address>
<email confidence="0.997874">
{suzuki.jun, nagata.masaaki}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962933333333">
This paper proposes a framework of super-
vised model learning that realizes feature
grouping to obtain lower complexity mod-
els. The main idea of our method is to
integrate a discrete constraint into model
learning with the help of the dual decom-
position technique. Experiments on two
well-studied NLP tasks, dependency pars-
ing and NER, demonstrate that our method
can provide state-of-the-art performance
even if the degrees of freedom in trained
models are surprisingly small, i.e., 8 or
even 2. This significant benefit enables us
to provide compact model representation,
which is especially useful in actual use.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999949">
This paper focuses on the topic of supervised
model learning, which is typically represented as
the following form of the optimization problem:
</bodyText>
<equation confidence="0.9938345">
wˆ = arg min 1O(w; D)},
O(w; D) = L(w; D) + Q(w),
</equation>
<bodyText confidence="0.999961756097561">
where D is supervised training data that consists
of the corresponding input x and output y pairs,
that is, (x, y) E D. w is an N-dimensional vector
representation of a set of optimization variables,
which are also interpreted as feature weights.
L(w; D) and Q(w) represent a loss function and
a regularization term, respectively. Nowadays, we,
in most cases, utilize a supervised learning method
expressed as the above optimization problem to
estimate the feature weights of many natural lan-
guage processing (NLP) tasks, such as text clas-
sification, POS-tagging, named entity recognition,
dependency parsing, and semantic role labeling.
In the last decade, the L1-regularization tech-
nique, which incorporates L1-norm into Q(w),
has become popular and widely-used in many
NLP tasks (Gao et al., 2007; Tsuruoka et al.,
2009). The reason is that L1-regularizers encour-
age feature weights to be zero as much as pos-
sible in model learning, which makes the resul-
tant model a sparse solution (many zero-weights
exist). We can discard all features whose weight
is zero from the trained model1 without any loss.
Therefore, L1-regularizers have the ability to eas-
ily and automatically yield compact models with-
out strong concern over feature selection.
Compact models generally have significant and
clear advantages in practice: instances are faster
loading speed to memory, less memory occupa-
tion, and even faster decoding is possible if the
model is small enough to be stored in cache mem-
ory. Given this background, our aim is to establish
a model learning framework that can reduce the
model complexity beyond that possible by sim-
ply applying L1-regularizers. To achieve our goal,
we focus on the recently developed concept of au-
tomatic feature grouping (Tibshirani et al., 2005;
Bondell and Reich, 2008). We introduce a model
learning framework that achieves feature group-
ing by incorporating a discrete constraint during
model learning.
</bodyText>
<sectionHeader confidence="0.993403" genericHeader="method">
2 Feature Grouping Concept
</sectionHeader>
<bodyText confidence="0.998727928571428">
Going beyond L1-regularized sparse modeling,
the idea of ‘automatic feature grouping’ has re-
cently been developed. Examples are fused
lasso (Tibshirani et al., 2005), grouping pur-
suit (Shen and Huang, 2010), and OSCAR (Bon-
dell and Reich, 2008). The concept of automatic
feature grouping is to find accurate models that
have fewer degrees of freedom. This is equiva-
lent to enforce every optimization variables to be
equal as much as possible. A simple example is
that ˆw1 = (0.1, 0.5, 0.1, 0.5, 0.1) is preferred over
ˆw2 = (0.1, 0.3, 0.2, 0.5, 0.3) since ˆw1 and ˆw2
have two and four unique values, respectively.
There are several merits to reducing the degree
</bodyText>
<footnote confidence="0.963989">
1This paper refers to model after completion of (super-
vised) model learning as “trained model”
</footnote>
<equation confidence="0.697227">
(1)
</equation>
<page confidence="0.965332">
18
</page>
<bodyText confidence="0.923268666666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 18–23,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
of freedom. For example, previous studies clari-
fied that it can reduce the chance of over-fitting to
the training data (Shen and Huang, 2010). This is
an important property for many NLP tasks since
they are often modeled with a high-dimensional
feature space, and thus, the over-fitting problem is
readily triggered. It has also been reported that it
can improve the stability of selecting non-zero fea-
tures beyond that possible with the standard L1-
regularizer given the existence of many highly cor-
related features (J¨ornsten and Yu, 2003; Zou and
Hastie, 2005). Moreover, it can dramatically re-
duce model complexity. This is because we can
merge all features whose feature weight values are
equivalent in the trained model into a single fea-
ture cluster without any loss.
</bodyText>
<sectionHeader confidence="0.890473" genericHeader="method">
3 Modeling with Feature Grouping
</sectionHeader>
<bodyText confidence="0.997014">
This section describes our proposal for obtaining
a feature grouping solution.
</bodyText>
<subsectionHeader confidence="0.999927">
3.1 Integration of a Discrete Constraint
</subsectionHeader>
<bodyText confidence="0.999959571428572">
Let S be a finite set of discrete values, i.e., a set
integer from −4 to 4, that is, S = {−4,..., −1, 0,
1, ... , 4}. The detailed discussion how we define
S can be found in our experiments section since
it deeply depends on training data. Then, we de-
fine the objective that can simultaneously achieve
a feature grouping and model learning as follows:
</bodyText>
<equation confidence="0.982647333333333">
O(w; D) = L(w; D) + Q(w)
(2)
s.t. w E SN.
</equation>
<bodyText confidence="0.999543">
where SN is the cartesian power of a set S. The
only difference with Eq. 1 is the additional dis-
crete constraint, namely, w E SN. This con-
straint means that each variable (feature weight)
in trained models must take a value in S, that is,
ˆwn E S, where ˆwn is the n-th factor of ˆw, and
n E {1, ... , N}. As a result, feature weights in
trained models are automatically grouped in terms
of the basis of model learning. This is the basic
idea of feature grouping proposed in this paper.
However, a concern is how we can efficiently
optimize Eq. 2 since it involves a NP-hard combi-
natorial optimization problem. The time complex-
ity of the direct optimization is exponential against
N. Next section introduces a feasible algorithm.
</bodyText>
<subsectionHeader confidence="0.998704">
3.2 Dual Decomposition Formulation
</subsectionHeader>
<bodyText confidence="0.99266975">
Hereafter, we strictly assume that L(w; D) and
Q(w) are both convex in w. Then, the proper-
ties of our method are unaffected by the selection
of L(w; D) and Q(w). Thus, we ignore their spe-
cific definition in this section. Typical cases can
be found in the experiments section. Then, we re-
formulate Eq. 2 by using the dual decomposition
technique (Everett, 1963):
</bodyText>
<equation confidence="0.959035333333333">
O(w, u; D) = L(w; D) + Q(w) + T(u)
(3)
s.t. w = u, and u E SN.
</equation>
<bodyText confidence="0.98631965">
Difference from Eq. 2, Eq. 3 has an additional term
T(u), which is similar to the regularizer Q(w),
whose optimization variables w and u are tight-
ened with equality constraint w = u. Here, this
paper only considers the case T(u) = λ22 ||u||22 +
A1||u||1, and A2 &gt; 0 and A1 &gt; 02. This objec-
tive can also be viewed as the decomposition of
the standard loss minimization problem shown in
Eq. 1 and the additional discrete constraint regu-
larizer by the dual decomposition technique.
To solve the optimization in Eq. 3, we lever-
age the alternating direction method of multiplier
(ADMM) (Gabay and Mercier, 1976; Boyd et al.,
2011). ADMM provides a very efficient optimiza-
tion framework for the problem in the dual decom-
position form. Here, α represents dual variables
for the equivalence constraint w = u. ADMM in-
troduces the augmented Lagrangian term ρ2||w −
u||2 2 with p&gt;0 which ensures strict convexity and
increases robustness3.
Finally, the optimization problem in Eq. 3 can
be converted into a series of iterative optimiza-
tion problems. Detailed derivation in the general
case can be found in (Boyd et al., 2011). Fig. 1
shows the entire model learning framework of our
proposed method. The remarkable point is that
ADMM works by iteratively computing one of the
three optimization variable sets w, u, and α while
holding the other variables fixed in the iterations
t = 1, 2, ... until convergence.
Step1 (w-update): This part of the optimiza-
tion problem shown in Eq. 4 is essentially Eq. 1
with a ‘biased’ L2-regularizer. ‘bias’ means here
that the direction of regularization is toward point
a instead of the origin. Note that it becomes a
standard L2-regularizer if a = 0. We can select
any learning algorithm that can handle the L2-
regularizer for this part of the optimization.
Step2 (u-update): This part of the optimization
problem shown in Eq. 5 can be rewritten in the
</bodyText>
<footnote confidence="0.99005825">
2Note that this setting includes the use of only L1-, L2-,
or without regularizers (L1 only: A1 &gt;0 and A2 =0, L2 only:
A1 =0 and A2 &gt;0, and without regularizer: A1=0, A2 =0).
3Standard dual decomposition can be viewed as p=0
</footnote>
<page confidence="0.996775">
19
</page>
<bodyText confidence="0.673238">
Input: Training data:D, parameters:ρ, ξ, eprimal, and edual
Initialize: w(1) = 0, u(1) = 0, α(1) = 0, and t = 1.
</bodyText>
<equation confidence="0.960773833333333">
Step1 w-update:
Solve w(t+1) = arg minty,{O(w; D, u(t), α(t))}.
For our case,
O(w; D, u, α) = O(w; D) + ρ2||w − a||22, (4)
where a = u − α.
Step2 u-update:
Solve u(t+1) = arg mina{O(u; D, w(t+1), α(t))}.
For our case,
λ2
O(u; D, w, α) = 2 ||u||22 + λ1||u||1 + ρ2||b − u||22
s.t. u E SN,
Input: b&apos; = (b&apos; n)Nn=1, λ&apos;1, and S.
</equation>
<bodyText confidence="0.962299625">
1, Find the optimal solution of Eq. 8 without the constraint.
The optimization of mixed L2 and L1-norms is known
to have a closed form solution, i.e., (Beck and Teboulle,
2009), that is;
ˆu&apos;n = sgn(b&apos;n) max(0, |b&apos;n |− λ&apos;1),
where (ˆu&apos;n)Nn=1 =ˆu&apos;.
2, Find the nearest valid point in SN fromˆu&apos; in terms of the
L2-distance;
</bodyText>
<equation confidence="0.9145015">
ˆun = arg min (ˆu&apos;n − u)2
uES
</equation>
<bodyText confidence="0.99456">
where (ˆun)Nn=1 = ˆu. This can be performed by a binary
search, whose time complexity is generally O(log |S|).
</bodyText>
<figure confidence="0.52714">
Output: uˆ
</figure>
<figureCaption confidence="0.857291">
Figure 2: Procedure for solving Step2
</figureCaption>
<equation confidence="0.837868333333333">
||w(t+1) − u(t+1)||22/N &lt; eprimal
(7)
||u(t+1) − u(t)||2 2/N &lt; ~dual
Break the loop if the above two conditions are reached,
or go back to Step1 with t = t + 1.
Output: u(t+1)
</equation>
<figureCaption confidence="0.97985">
Figure 1: Entire learning framework of our
method derived from ADMM (Boyd et al., 2011).
</figureCaption>
<bodyText confidence="0.99587">
following equivalent simple form:
</bodyText>
<equation confidence="0.743124">
uˆ= arg minu{12||u − b0||22 + λ01||u||1J (8)
where b0 = ρ
λ2+ρb, and λ0 1 = λ1
</equation>
<bodyText confidence="0.999685743589744">
λ2+ρ. This
optimization is still a combinatorial optimization
problem. However unlike Eq. 2, this optimization
can be efficiently solved.
Fig. 2 shows the procedure to obtain the exact
solution of Eq. 5, namely u(t+1). The remarkable
point is that the costly combinatorial optimization
problem is disappeared, and instead, we are only
required to perform two feature-wise calculations
whose total time complexities is O(N log |S|) and
fully parallelizable. The similar technique has
been introduced in Zhong and Kwok (2011) for
discarding a costly combinatorial problem from
the optimization with OSCAR-regularizers with
the help of proximal gradient methods, i.e., (Beck
and Teboulle, 2009).
We omit to show the detailed derivation of
Fig. 2 because of the space reason. However, this
is easily understandable. The key properties are
the following two folds; (i) The objective shown
in Eq. 8 is a convex and also symmetric function
with respect to ˆu0, where ˆu0 is the optimal solution
of Eq. 8 without the discrete constraint. Therefore,
the optimal solution uˆ is at the point where the
nearest valid point given SN from ˆu0 in terms of
the L2-distance. (ii) The valid points given SN are
always located at the vertexes of axis-aligned or-
thotopes (hyperrectangles) in the parameter space
of feature weights. Thus, the solution ˆu, which is
the nearest valid point from ˆu0, can be obtained by
individually taking the nearest value in S from ˆu0n
for all n.
Step3 (α-update): We perform gradient ascent
on dual variables to tighten the constraint w = u.
Note that ξ is the learning rate; we can simply set
it to 1.0 for every iteration (Boyd et al., 2011).
Step4 (convergence check): It can be evaluated
both primal and dual residuals as defined in Eq. 7
with suitably small cprimal and cdual.
</bodyText>
<subsectionHeader confidence="0.986165">
3.3 Online Learning
</subsectionHeader>
<bodyText confidence="0.999848777777778">
We can select an online learning algorithm for
Step1 since the ADMM framework does not re-
quire exact minimization of Eq. 4. In this case, we
perform one-pass update through the data in each
ADMM iteration (Duh et al., 2011). Note that the
total calculation cost of our method does not in-
crease much from original online learning algo-
rithm since the calculation cost of Steps 2 through
4 is relatively much smaller than that of Step1.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999812444444445">
We conducted experiments on two well-studied
NLP tasks, namely named entity recognition
(NER) and dependency parsing (DEPAR).
Basic settings: We simply reused the settings
of most previous studies. We used CoNLL’03
data (Tjong Kim Sang and De Meulder, 2003)
for NER, and the Penn Treebank (PTB) III cor-
pus (Marcus et al., 1994) converted to depen-
dency trees for DEPAR (McDonald et al., 2005).
</bodyText>
<figure confidence="0.55698">
s.t. u E SN,
where b = w + α (5)
Step3 α-update:
α(t+1) = α(t) + ξ(w(t+1) − u(t+1)) (6)
Step4 convergence check:
</figure>
<page confidence="0.880751">
20
</page>
<bodyText confidence="0.999958222222222">
Our decoding models are the Viterbi algorithm
on CRF (Lafferty et al., 2001), and the second-
order parsing model proposed by (Carreras, 2007)
for NER and DEPAR, respectively. Features
are automatically generated according to the pre-
defined feature templates widely-used in the pre-
vious studies. We also integrated the cluster fea-
tures obtained by the method explained in (Koo et
al., 2008) as additional features for evaluating our
method in the range of the current best systems.
Evaluation measures: The purpose of our ex-
periments is to investigate the effectiveness of our
proposed method in terms of both its performance
and the complexity of the trained model. There-
fore, our evaluation measures consist of two axes.
Task performance was mainly evaluated in terms
of the complete sentence accuracy (COMP) since
the objective of all model learning methods eval-
uated in our experiments is to maximize COMP.
We also report the Fβ=1 score (F-sc) for NER,
and the unlabeled attachment score (UAS) for DE-
PAR for comparison with previous studies. Model
complexity is evaluated by the number of non-zero
active features (#nzF) and the degree of freedom
(#DoF) (Zhong and Kwok, 2011). #nzF is the
number of features whose corresponding feature
weight is non-zero in the trained model, and #DoF
is the number of unique non-zero feature weights.
Baseline methods: Our main baseline is L1-
regularized sparse modeling. To cover both batch
and online leaning, we selected L1-regularized
CRF (L1CRF) (Lafferty et al., 2001) optimized by
OWL-QN (Andrew and Gao, 2007) for the NER
experiment, and the L1-regularized regularized
dual averaging (L1RDA) method (Xiao, 2010)4
for DEPAR. Additionally, we also evaluated L2-
regularized CRF (L2CRF) with L-BFGS (Liu and
Nocedal, 1989) for NER, and passive-aggressive
algorithm (L2PA) (Crammer et al., 2006)5 for DE-
PAR since L2-regularizer often provides better re-
sults than L1-regularizer (Gao et al., 2007).
For a fair comparison, we applied the proce-
dure of Step2 as a simple quantization method
to trained models obtained from L1-regularized
model learning, which we refer to as (QT).
</bodyText>
<footnote confidence="0.997877666666667">
4RDA provided better results at least in our experiments
than Ll-regularized FOBOS (Duchi and Singer, 2009), and
its variant (Tsuruoka et al., 2009), which are more familiar to
the NLP community.
5L2PA is also known as a loss augmented variant of one-
best MIRA, well-known in DEPAR (McDonald et al., 2005).
</footnote>
<subsectionHeader confidence="0.985014">
4.1 Configurations of Our Method
</subsectionHeader>
<bodyText confidence="0.998981785714286">
Base learning algorithm: The settings of our
method in our experiments imitate L1-regularized
learning algorithm since the purpose of our
experiments is to investigate the effectiveness
against standard L1-regularized learning algo-
rithms. Then, we have the following two possible
settings; DC-ADMM: we leveraged the baseline
L1-regularized learning algorithm to solve Step1,
and set A1 = 0 and A2 = 0 for Step2. DCwL1-
ADMM: we leveraged the baseline L2-regularized
learning algorithm, but without L2-regularizer, to
solve Step1, and set A1 &gt; 0 and A2 = 0 for Step2.
The difference can be found in the objective func-
tion O(w, u; D) shown in Eq. 3;
</bodyText>
<equation confidence="0.995551">
(DC-ADMM) : O(w, u; D)=L(w; D)+A1||w||1
(DCwL1-ADMM) : O(w, u; D)=L(w; D)+A1||u||1
</equation>
<bodyText confidence="0.989057970588235">
In other words, DC-ADMM utilizes L1-
regularizer as a part of base leaning algorithm
Q(w)=A1||w||1, while DCwL1-ADMM discards
regularizer of base learning algorithm Q(w), but
instead introducing T(u) = A1||u||1. Note that
these two configurations are essentially identical
since objectives are identical, even though the
formulation and algorithm is different. We only
report results of DC-ADMM because of the space
reason since the results of DCwL1-ADMM were
nearly equivalent to those of DC-ADMM.
Definition of S: DC-ADMM can utilize any fi-
nite set for S. However, we have to carefully se-
lect it since it deeply affects the performance. Ac-
tually, this is the most considerable point of our
method. We preliminarily investigated the several
settings. Here, we introduce an example of tem-
plate which is suitable for large feature set. Let
q, 6, and n represent non-negative real-value con-
stants, be a positive integer, Q = {−1,1}, and
a function fη,δ,κ(x, y) = y(qnx + 6). Then, we
define a finite set of values S as follows:
Sη,δ,κ,ζ ={fη,δ,κ(x, y)|(x, y) ∈ Sζ ×Q} ∪ {0},
where Sζ is a set of non-negative integers from
zero to − 1, that is, Sζ ={m}ζ−1
m=0. For example,
if we set q = 0.1, 6 = 0.4, n = 4, and = 3, then
Sη,δ,κ,ζ = {−2.0, −0.8, −0.5, 0, 0.5, 0.8, 2.0}.
The intuition of this template is that the distribu-
tion of the feature weights in trained model often
takes a form a similar to that of the ‘power law’
in the case of the large feature sets. Therefore, us-
ing an exponential function with a scale and bias
seems to be appropriate for fitting them.
</bodyText>
<page confidence="0.997718">
21
</page>
<figure confidence="0.998886">
(a) NER (b) DEPAR
</figure>
<figureCaption confidence="0.9927725">
Figure 3: Performance vs. degree of freedom in
the trained model for the development data
</figureCaption>
<bodyText confidence="0.999952714285714">
Note that we can control the upper bound of
#DoF in trained model by (, namely if ( = 4 then
the upper bound of #DoF is 8 (doubled by posi-
tive and negative sides). We fixed p = 1, � = 1,
A2 = 0, K = 4 (or 2 if ( ≥ 5), S = q/2 in all ex-
periments. Thus the only tunable parameter in our
experiments is q for each (.
</bodyText>
<subsectionHeader confidence="0.842893">
4.2 Results and Discussions
</subsectionHeader>
<bodyText confidence="0.999304482758621">
Fig. 3 shows the task performance on the develop-
ment data against the model complexities in terms
of the degrees of freedom in the trained models.
Plots are given by changing the ( value for DC-
ADMM and L1-regularized methods with QT. The
plots of the standard L1-regularized methods are
given by changing the regularization constants A1.
Moreover, Table 1 shows the final results of our
experiments on the test data. The tunable param-
eters were fixed at values that provided the best
performance on the development data.
According to the figure and table, the most re-
markable point is that DC-ADMM successfully
maintained the task performance even if #DoF (the
degree of freedom) was 8, and the performance
drop-offs were surprisingly limited even if #DoF
was 2, which is the upper bound of feature group-
ing. Moreover, it is worth noting that the DC-
ADMM performance is sometimes improved. The
reason may be that such low degrees of freedom
prevent over-fitting to the training data. Surpris-
ingly, the simple quantization method (QT) pro-
vided fairly good results. However, we empha-
size that the models produced by the QT approach
offer no guarantee as to the optimal solution. In
contrast, DC-ADMM can truly provide the opti-
mal solution of Eq. 3 since the discrete constraint
is also considered during the model learning.
In general, a trained model consists of two parts:
</bodyText>
<table confidence="0.9999585">
NER Test Model complex.
COMP F-sc #nzF #DoF
L2CRF 84.88 89.97 61.6M 38.6M
L1CRF 84.85 89.99 614K 321K
(w/ QT ζ =4) 78.39 85.33 568K 8
(w/ QT ζ =2) 73.40 81.45 454K 4
(w/ QT ζ =1) 65.53 75.87 454K 2
DC-ADMM (ζ =4) 84.96 89.92 643K 8
(ζ =2) 84.04 89.35 455K 4
(ζ =1) 83.06 88.62 364K 2
Test Model complex.
DEPER COMP UAS #nzF #DoF
L2PA 49.67 93.51 15.5M 5.59M
L1RDA 49.54 93.48 7.76M 3.56M
(w/ QT ζ =4) 38.58 90.85 6.32M 8
(w/ QT ζ =2) 34.19 89.42 3.08M 4
(w/ QT ζ =1) 30.42 88.67 3.08M 2
DC-ADMM (ζ =4) 49.83 93.55 5.81M 8
(ζ =2) 48.97 93.18 4.11M 4
(ζ =1) 46.56 92.86 6.37M 2
</table>
<tableCaption confidence="0.9373535">
Table 1: Comparison results of the methods on test
data (K: thousand, M: million)
</tableCaption>
<bodyText confidence="0.999931857142857">
feature weights and an indexed structure of fea-
ture strings, which are used as the key for obtain-
ing the corresponding feature weight. This paper
mainly discussed how to reduce the size of the for-
mer part, and described its successful reduction.
We note that it is also possible to reduce the lat-
ter part especially if the feature string structure is
TRIE. We omit the details here since it is not the
main topic of this paper, but by merging feature
strings that have the same feature weights, the size
of entire trained models in our DEPAR case can be
reduced to about 10 times smaller than those ob-
tained by standard L1-regularization, i.e., to 12.2
MB from 124.5 MB.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99983375">
This paper proposed a model learning framework
that can simultaneously realize feature grouping
by the incorporation of a simple discrete con-
straint into model learning optimization. This
paper also introduced a feasible algorithm, DC-
ADMM, which can vanish the infeasible combi-
natorial optimization part from the entire learning
algorithm with the help of the ADMM technique.
Experiments showed that DC-ADMM drastically
reduced model complexity in terms of the degrees
of freedom in trained models while maintaining
the performance. There may exist theoretically
cleverer approaches to feature grouping, but the
performance of DC-ADMM is close to the upper
bound. We believe our method, DC-ADMM, to be
very useful for actual use.
</bodyText>
<figure confidence="0.999393428571429">
91.0
55.0
quantized
quantized
81.0
1.0E+00 1.0E+03 1.0E+06
# of degrees of freedom (#DoF) [log-scale]
50.0
45.0
40.0
DC-ADMM
L1BAD (w/ QT)
L1BDA
L2PA
30.0
1.0E+00 1.0E+03 1.0E+06
# of degrees of freedom (#DoF) [log-scale]
Complete Sentence Accuracy
35.0
DC-ADMM
L1CBF (w/ QT)
L1CBF
L2CBF
Complete Sentence Accuracy
89.0
87.0
85.0
83.0
</figure>
<page confidence="0.994588">
22
</page>
<sectionHeader confidence="0.995848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999554150943397">
Galen Andrew and Jianfeng Gao. 2007. Scal-
able Training of L1-regularized Log-linear Models.
In Zoubin Ghahramani, editor, Proceedings of the
24th Annual International Conference on Machine
Learning (ICML 2007), pages 33–40. Omnipress.
Amir Beck and Marc Teboulle. 2009. A Fast Iter-
ative Shrinkage-thresholding Algorithm for Linear
Inverse Problems. SIAM Journal on Imaging Sci-
ences, 2(1):183–202.
Howard D. Bondell and Brian J. Reich. 2008. Simulta-
neous Regression Shrinkage, Variable Selection and
Clustering of Predictors with OSCAR. Biometrics,
64(1):115.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato,
and Jonathan Eckstein. 2011. Distributed Opti-
mization and Statistical Learning via the Alternat-
ing Direction Method of Multipliers. Foundations
and Trends in Machine Learning.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proceed-
ings of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, pages 957–961.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line Passive-Aggressive Algorithms. Journal of Ma-
chine Learning Research, 7:551–585.
John Duchi and Yoram Singer. 2009. Efficient On-
line and Batch Learning Using Forward Backward
Splitting. Journal of Machine Learning Research,
10:2899–2934.
Kevin Duh, Jun Suzuki, and Masaaki Nagata. 2011.
Distributed Learning-to-Rank on Streaming Data
using Alternating Direction Method of Multipliers.
In NIPS’11 Big Learning Workshop.
Hugh Everett. 1963. Generalized Lagrange Multiplier
Method for Solving Problems of Optimum Alloca-
tion of Resources. Operations Research, 11(3):399–
417.
Daniel Gabay and Bertrand Mercier. 1976. A Dual
Algorithm for the Solution of Nonlinear Variational
Problems via Finite Element Approximation. Com-
puters and Mathematics with Applications, 2(1):17
– 40.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824–831, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Rebecka J¨ornsten and Bin Yu. 2003. Simulta-
neous Gene Clustering and Subset Selection for
Sample Classification Via MDL. Bioinformatics,
19(9):1100–1109.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple Semi-supervised Dependency Pars-
ing. In Proceedings of ACL-08: HLT, pages 595–
603.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the International
Conference on Machine Learning (ICML 2001),
pages 282–289.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited
Memory BFGS Method for Large Scale Optimiza-
tion. Math. Programming, Ser. B, 45(3):503–528.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online Large-margin Training of
Dependency Parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91–98.
Xiaotong Shen and Hsin-Cheng Huang. 2010. Group-
ing Pursuit Through a Regularization Solution Sur-
face. Journal of the American Statistical Associa-
tion, 105(490):727–739.
Robert Tibshirani, Michael Saunders, Saharon Ros-
set, Ji Zhu, and Keith Knight. 2005. Sparsity and
Smoothness via the Fused Lasso. Journal of the
Royal Statistical Society Series B, pages 91–108.
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In Proceedings of CoNLL-2003, pages 142–147.
Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic Gradient Descent Training
for L1-regularized Log-linear Models with Cumu-
lative Penalty. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 477–485.
Lin Xiao. 2010. Dual Averaging Methods for Regular-
ized Stochastic Learning and Online Optimization.
Journal of Machine Learning Research, 11:2543–
2596.
Leon Wenliang Zhong and James T. Kwok. 2011.
Efficient Sparse Modeling with Automatic Feature
Grouping. In ICML.
Hui Zou and Trevor Hastie. 2005. Regularization and
Variable Selection via the Elastic Net. Journal of the
Royal Statistical Society, Series B, 67:301–320.
</reference>
<page confidence="0.998919">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911412">
<title confidence="0.984977">Supervised Model Learning with Feature based on a Discrete Constraint</title>
<author confidence="0.999278">Jun Suzuki</author>
<author confidence="0.999278">Masaaki Nagata</author>
<affiliation confidence="0.984805">NTT Communication Science Laboratories, NTT</affiliation>
<address confidence="0.951616">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237</address>
<abstract confidence="0.9994535">This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models. The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique. Experiments on two well-studied NLP tasks, dependency parsing and NER, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained are surprisingly small, or even 2. This significant benefit enables us to provide compact model representation, which is especially useful in actual use.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Galen Andrew</author>
<author>Jianfeng Gao</author>
</authors>
<title>Scalable Training of L1-regularized Log-linear Models.</title>
<date>2007</date>
<booktitle>In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007),</booktitle>
<pages>33--40</pages>
<publisher>Omnipress.</publisher>
<contexts>
<context position="14287" citStr="Andrew and Gao, 2007" startWordPosition="2413" endWordPosition="2416">ore (F-sc) for NER, and the unlabeled attachment score (UAS) for DEPAR for comparison with previous studies. Model complexity is evaluated by the number of non-zero active features (#nzF) and the degree of freedom (#DoF) (Zhong and Kwok, 2011). #nzF is the number of features whose corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at leas</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Galen Andrew and Jianfeng Gao. 2007. Scalable Training of L1-regularized Log-linear Models. In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning (ICML 2007), pages 33–40. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Beck</author>
<author>Marc Teboulle</author>
</authors>
<title>A Fast Iterative Shrinkage-thresholding Algorithm for Linear Inverse Problems.</title>
<date>2009</date>
<journal>SIAM Journal on Imaging Sciences,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="9243" citStr="Beck and Teboulle, 2009" startWordPosition="1563" endWordPosition="1566">d as p=0 19 Input: Training data:D, parameters:ρ, ξ, eprimal, and edual Initialize: w(1) = 0, u(1) = 0, α(1) = 0, and t = 1. Step1 w-update: Solve w(t+1) = arg minty,{O(w; D, u(t), α(t))}. For our case, O(w; D, u, α) = O(w; D) + ρ2||w − a||22, (4) where a = u − α. Step2 u-update: Solve u(t+1) = arg mina{O(u; D, w(t+1), α(t))}. For our case, λ2 O(u; D, w, α) = 2 ||u||22 + λ1||u||1 + ρ2||b − u||22 s.t. u E SN, Input: b&apos; = (b&apos; n)Nn=1, λ&apos;1, and S. 1, Find the optimal solution of Eq. 8 without the constraint. The optimization of mixed L2 and L1-norms is known to have a closed form solution, i.e., (Beck and Teboulle, 2009), that is; ˆu&apos;n = sgn(b&apos;n) max(0, |b&apos;n |− λ&apos;1), where (ˆu&apos;n)Nn=1 =ˆu&apos;. 2, Find the nearest valid point in SN fromˆu&apos; in terms of the L2-distance; ˆun = arg min (ˆu&apos;n − u)2 uES where (ˆun)Nn=1 = ˆu. This can be performed by a binary search, whose time complexity is generally O(log |S|). Output: uˆ Figure 2: Procedure for solving Step2 ||w(t+1) − u(t+1)||22/N &lt; eprimal (7) ||u(t+1) − u(t)||2 2/N &lt; ~dual Break the loop if the above two conditions are reached, or go back to Step1 with t = t + 1. Output: u(t+1) Figure 1: Entire learning framework of our method derived from ADMM (Boyd et al., 2011).</context>
<context position="10648" citStr="Beck and Teboulle, 2009" startWordPosition="1798" endWordPosition="1801">em. However unlike Eq. 2, this optimization can be efficiently solved. Fig. 2 shows the procedure to obtain the exact solution of Eq. 5, namely u(t+1). The remarkable point is that the costly combinatorial optimization problem is disappeared, and instead, we are only required to perform two feature-wise calculations whose total time complexities is O(N log |S|) and fully parallelizable. The similar technique has been introduced in Zhong and Kwok (2011) for discarding a costly combinatorial problem from the optimization with OSCAR-regularizers with the help of proximal gradient methods, i.e., (Beck and Teboulle, 2009). We omit to show the detailed derivation of Fig. 2 because of the space reason. However, this is easily understandable. The key properties are the following two folds; (i) The objective shown in Eq. 8 is a convex and also symmetric function with respect to ˆu0, where ˆu0 is the optimal solution of Eq. 8 without the discrete constraint. Therefore, the optimal solution uˆ is at the point where the nearest valid point given SN from ˆu0 in terms of the L2-distance. (ii) The valid points given SN are always located at the vertexes of axis-aligned orthotopes (hyperrectangles) in the parameter space</context>
</contexts>
<marker>Beck, Teboulle, 2009</marker>
<rawString>Amir Beck and Marc Teboulle. 2009. A Fast Iterative Shrinkage-thresholding Algorithm for Linear Inverse Problems. SIAM Journal on Imaging Sciences, 2(1):183–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard D Bondell</author>
<author>Brian J Reich</author>
</authors>
<title>Simultaneous Regression Shrinkage, Variable Selection and Clustering of Predictors with OSCAR.</title>
<date>2008</date>
<journal>Biometrics,</journal>
<volume>64</volume>
<issue>1</issue>
<contexts>
<context position="2899" citStr="Bondell and Reich, 2008" startWordPosition="449" endWordPosition="452">y yield compact models without strong concern over feature selection. Compact models generally have significant and clear advantages in practice: instances are faster loading speed to memory, less memory occupation, and even faster decoding is possible if the model is small enough to be stored in cache memory. Given this background, our aim is to establish a model learning framework that can reduce the model complexity beyond that possible by simply applying L1-regularizers. To achieve our goal, we focus on the recently developed concept of automatic feature grouping (Tibshirani et al., 2005; Bondell and Reich, 2008). We introduce a model learning framework that achieves feature grouping by incorporating a discrete constraint during model learning. 2 Feature Grouping Concept Going beyond L1-regularized sparse modeling, the idea of ‘automatic feature grouping’ has recently been developed. Examples are fused lasso (Tibshirani et al., 2005), grouping pursuit (Shen and Huang, 2010), and OSCAR (Bondell and Reich, 2008). The concept of automatic feature grouping is to find accurate models that have fewer degrees of freedom. This is equivalent to enforce every optimization variables to be equal as much as possib</context>
</contexts>
<marker>Bondell, Reich, 2008</marker>
<rawString>Howard D. Bondell and Brian J. Reich. 2008. Simultaneous Regression Shrinkage, Variable Selection and Clustering of Predictors with OSCAR. Biometrics, 64(1):115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boyd</author>
<author>Neal Parikh</author>
<author>Eric Chu</author>
<author>Borja Peleato</author>
<author>Jonathan Eckstein</author>
</authors>
<title>Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends</title>
<date>2011</date>
<booktitle>in Machine Learning.</booktitle>
<contexts>
<context position="7147" citStr="Boyd et al., 2011" startWordPosition="1184" endWordPosition="1187">nce from Eq. 2, Eq. 3 has an additional term T(u), which is similar to the regularizer Q(w), whose optimization variables w and u are tightened with equality constraint w = u. Here, this paper only considers the case T(u) = λ22 ||u||22 + A1||u||1, and A2 &gt; 0 and A1 &gt; 02. This objective can also be viewed as the decomposition of the standard loss minimization problem shown in Eq. 1 and the additional discrete constraint regularizer by the dual decomposition technique. To solve the optimization in Eq. 3, we leverage the alternating direction method of multiplier (ADMM) (Gabay and Mercier, 1976; Boyd et al., 2011). ADMM provides a very efficient optimization framework for the problem in the dual decomposition form. Here, α represents dual variables for the equivalence constraint w = u. ADMM introduces the augmented Lagrangian term ρ2||w − u||2 2 with p&gt;0 which ensures strict convexity and increases robustness3. Finally, the optimization problem in Eq. 3 can be converted into a series of iterative optimization problems. Detailed derivation in the general case can be found in (Boyd et al., 2011). Fig. 1 shows the entire model learning framework of our proposed method. The remarkable point is that ADMM wo</context>
<context position="9842" citStr="Boyd et al., 2011" startWordPosition="1674" endWordPosition="1677">and Teboulle, 2009), that is; ˆu&apos;n = sgn(b&apos;n) max(0, |b&apos;n |− λ&apos;1), where (ˆu&apos;n)Nn=1 =ˆu&apos;. 2, Find the nearest valid point in SN fromˆu&apos; in terms of the L2-distance; ˆun = arg min (ˆu&apos;n − u)2 uES where (ˆun)Nn=1 = ˆu. This can be performed by a binary search, whose time complexity is generally O(log |S|). Output: uˆ Figure 2: Procedure for solving Step2 ||w(t+1) − u(t+1)||22/N &lt; eprimal (7) ||u(t+1) − u(t)||2 2/N &lt; ~dual Break the loop if the above two conditions are reached, or go back to Step1 with t = t + 1. Output: u(t+1) Figure 1: Entire learning framework of our method derived from ADMM (Boyd et al., 2011). following equivalent simple form: uˆ= arg minu{12||u − b0||22 + λ01||u||1J (8) where b0 = ρ λ2+ρb, and λ0 1 = λ1 λ2+ρ. This optimization is still a combinatorial optimization problem. However unlike Eq. 2, this optimization can be efficiently solved. Fig. 2 shows the procedure to obtain the exact solution of Eq. 5, namely u(t+1). The remarkable point is that the costly combinatorial optimization problem is disappeared, and instead, we are only required to perform two feature-wise calculations whose total time complexities is O(N log |S|) and fully parallelizable. The similar technique has be</context>
<context position="11615" citStr="Boyd et al., 2011" startWordPosition="1969" endWordPosition="1972"> the optimal solution uˆ is at the point where the nearest valid point given SN from ˆu0 in terms of the L2-distance. (ii) The valid points given SN are always located at the vertexes of axis-aligned orthotopes (hyperrectangles) in the parameter space of feature weights. Thus, the solution ˆu, which is the nearest valid point from ˆu0, can be obtained by individually taking the nearest value in S from ˆu0n for all n. Step3 (α-update): We perform gradient ascent on dual variables to tighten the constraint w = u. Note that ξ is the learning rate; we can simply set it to 1.0 for every iteration (Boyd et al., 2011). Step4 (convergence check): It can be evaluated both primal and dual residuals as defined in Eq. 7 with suitably small cprimal and cdual. 3.3 Online Learning We can select an online learning algorithm for Step1 since the ADMM framework does not require exact minimization of Eq. 4. In this case, we perform one-pass update through the data in each ADMM iteration (Duh et al., 2011). Note that the total calculation cost of our method does not increase much from original online learning algorithm since the calculation cost of Steps 2 through 4 is relatively much smaller than that of Step1. 4 Exper</context>
</contexts>
<marker>Boyd, Parikh, Chu, Peleato, Eckstein, 2011</marker>
<rawString>Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers. Foundations and Trends in Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a HigherOrder Projective Dependency Parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL</booktitle>
<pages>957--961</pages>
<contexts>
<context position="12871" citStr="Carreras, 2007" startWordPosition="2188" endWordPosition="2189">ll-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). s.t. u E SN, where b = w + α (5) Step3 α-update: α(t+1) = α(t) + ξ(w(t+1) − u(t+1)) (6) Step4 convergence check: 20 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Evaluation measures: The purpose of our experiments is to investigate the effectiveness of our proposed method in terms of both its performance and the complexity of the trained model. Therefore, our evaluation measures consist of two axes. Task performance wa</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a HigherOrder Projective Dependency Parser. In Proceedings of the CoNLL Shared Task Session of EMNLPCoNLL 2007, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="14569" citStr="Crammer et al., 2006" startWordPosition="2453" endWordPosition="2456">corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 4.1 Configur</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Yoram Singer</author>
</authors>
<title>Efficient Online and Batch Learning Using Forward Backward Splitting.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>10--2899</pages>
<contexts>
<context position="14958" citStr="Duchi and Singer, 2009" startWordPosition="2515" endWordPosition="2518">regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 4.1 Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1-regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1-regularized learning algorithms. Then, we have the following two possible settings; DC-ADMM: we leveraged the baseline L1-regularized learning algorithm to solve </context>
</contexts>
<marker>Duchi, Singer, 2009</marker>
<rawString>John Duchi and Yoram Singer. 2009. Efficient Online and Batch Learning Using Forward Backward Splitting. Journal of Machine Learning Research, 10:2899–2934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Jun Suzuki</author>
<author>Masaaki Nagata</author>
</authors>
<title>Distributed Learning-to-Rank on Streaming Data using Alternating Direction Method of Multipliers.</title>
<date>2011</date>
<booktitle>In NIPS’11 Big Learning Workshop.</booktitle>
<contexts>
<context position="11997" citStr="Duh et al., 2011" startWordPosition="2035" endWordPosition="2038">arest value in S from ˆu0n for all n. Step3 (α-update): We perform gradient ascent on dual variables to tighten the constraint w = u. Note that ξ is the learning rate; we can simply set it to 1.0 for every iteration (Boyd et al., 2011). Step4 (convergence check): It can be evaluated both primal and dual residuals as defined in Eq. 7 with suitably small cprimal and cdual. 3.3 Online Learning We can select an online learning algorithm for Step1 since the ADMM framework does not require exact minimization of Eq. 4. In this case, we perform one-pass update through the data in each ADMM iteration (Duh et al., 2011). Note that the total calculation cost of our method does not increase much from original online learning algorithm since the calculation cost of Steps 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDona</context>
</contexts>
<marker>Duh, Suzuki, Nagata, 2011</marker>
<rawString>Kevin Duh, Jun Suzuki, and Masaaki Nagata. 2011. Distributed Learning-to-Rank on Streaming Data using Alternating Direction Method of Multipliers. In NIPS’11 Big Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugh Everett</author>
</authors>
<title>Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of Resources.</title>
<date>1963</date>
<journal>Operations Research,</journal>
<volume>11</volume>
<issue>3</issue>
<pages>417</pages>
<contexts>
<context position="6457" citStr="Everett, 1963" startWordPosition="1056" endWordPosition="1057">w we can efficiently optimize Eq. 2 since it involves a NP-hard combinatorial optimization problem. The time complexity of the direct optimization is exponential against N. Next section introduces a feasible algorithm. 3.2 Dual Decomposition Formulation Hereafter, we strictly assume that L(w; D) and Q(w) are both convex in w. Then, the properties of our method are unaffected by the selection of L(w; D) and Q(w). Thus, we ignore their specific definition in this section. Typical cases can be found in the experiments section. Then, we reformulate Eq. 2 by using the dual decomposition technique (Everett, 1963): O(w, u; D) = L(w; D) + Q(w) + T(u) (3) s.t. w = u, and u E SN. Difference from Eq. 2, Eq. 3 has an additional term T(u), which is similar to the regularizer Q(w), whose optimization variables w and u are tightened with equality constraint w = u. Here, this paper only considers the case T(u) = λ22 ||u||22 + A1||u||1, and A2 &gt; 0 and A1 &gt; 02. This objective can also be viewed as the decomposition of the standard loss minimization problem shown in Eq. 1 and the additional discrete constraint regularizer by the dual decomposition technique. To solve the optimization in Eq. 3, we leverage the alte</context>
</contexts>
<marker>Everett, 1963</marker>
<rawString>Hugh Everett. 1963. Generalized Lagrange Multiplier Method for Solving Problems of Optimum Allocation of Resources. Operations Research, 11(3):399– 417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gabay</author>
<author>Bertrand Mercier</author>
</authors>
<title>A Dual Algorithm for the Solution of Nonlinear Variational Problems via Finite Element Approximation.</title>
<date>1976</date>
<journal>Computers and Mathematics with Applications,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7127" citStr="Gabay and Mercier, 1976" startWordPosition="1180" endWordPosition="1183"> = u, and u E SN. Difference from Eq. 2, Eq. 3 has an additional term T(u), which is similar to the regularizer Q(w), whose optimization variables w and u are tightened with equality constraint w = u. Here, this paper only considers the case T(u) = λ22 ||u||22 + A1||u||1, and A2 &gt; 0 and A1 &gt; 02. This objective can also be viewed as the decomposition of the standard loss minimization problem shown in Eq. 1 and the additional discrete constraint regularizer by the dual decomposition technique. To solve the optimization in Eq. 3, we leverage the alternating direction method of multiplier (ADMM) (Gabay and Mercier, 1976; Boyd et al., 2011). ADMM provides a very efficient optimization framework for the problem in the dual decomposition form. Here, α represents dual variables for the equivalence constraint w = u. ADMM introduces the augmented Lagrangian term ρ2||w − u||2 2 with p&gt;0 which ensures strict convexity and increases robustness3. Finally, the optimization problem in Eq. 3 can be converted into a series of iterative optimization problems. Detailed derivation in the general case can be found in (Boyd et al., 2011). Fig. 1 shows the entire model learning framework of our proposed method. The remarkable p</context>
</contexts>
<marker>Gabay, Mercier, 1976</marker>
<rawString>Daniel Gabay and Bertrand Mercier. 1976. A Dual Algorithm for the Solution of Nonlinear Variational Problems via Finite Element Approximation. Computers and Mathematics with Applications, 2(1):17 – 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Galen Andrew</author>
<author>Mark Johnson</author>
<author>Kristina Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>824--831</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1900" citStr="Gao et al., 2007" startWordPosition="287" endWordPosition="290">ation variables, which are also interpreted as feature weights. L(w; D) and Q(w) represent a loss function and a regularization term, respectively. Nowadays, we, in most cases, utilize a supervised learning method expressed as the above optimization problem to estimate the feature weights of many natural language processing (NLP) tasks, such as text classification, POS-tagging, named entity recognition, dependency parsing, and semantic role labeling. In the last decade, the L1-regularization technique, which incorporates L1-norm into Q(w), has become popular and widely-used in many NLP tasks (Gao et al., 2007; Tsuruoka et al., 2009). The reason is that L1-regularizers encourage feature weights to be zero as much as possible in model learning, which makes the resultant model a sparse solution (many zero-weights exist). We can discard all features whose weight is zero from the trained model1 without any loss. Therefore, L1-regularizers have the ability to easily and automatically yield compact models without strong concern over feature selection. Compact models generally have significant and clear advantages in practice: instances are faster loading speed to memory, less memory occupation, and even </context>
<context position="14670" citStr="Gao et al., 2007" startWordPosition="2469" endWordPosition="2472">feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 4.1 Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Jianfeng Gao, Galen Andrew, Mark Johnson, and Kristina Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 824–831, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecka J¨ornsten</author>
<author>Bin Yu</author>
</authors>
<title>Simultaneous Gene Clustering and Subset Selection for Sample Classification Via MDL.</title>
<date>2003</date>
<journal>Bioinformatics,</journal>
<volume>19</volume>
<issue>9</issue>
<marker>J¨ornsten, Yu, 2003</marker>
<rawString>Rebecka J¨ornsten and Bin Yu. 2003. Simultaneous Gene Clustering and Subset Selection for Sample Classification Via MDL. Bioinformatics, 19(9):1100–1109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple Semi-supervised Dependency Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="13119" citStr="Koo et al., 2008" startWordPosition="2225" endWordPosition="2228">Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). s.t. u E SN, where b = w + α (5) Step3 α-update: α(t+1) = α(t) + ξ(w(t+1) − u(t+1)) (6) Step4 convergence check: 20 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Evaluation measures: The purpose of our experiments is to investigate the effectiveness of our proposed method in terms of both its performance and the complexity of the trained model. Therefore, our evaluation measures consist of two axes. Task performance was mainly evaluated in terms of the complete sentence accuracy (COMP) since the objective of all model learning methods evaluated in our experiments is to maximize COMP. We also report the Fβ=1 score (F-sc) for NER, and the unlabeled attachment scor</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple Semi-supervised Dependency Parsing. In Proceedings of ACL-08: HLT, pages 595– 603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="12807" citStr="Lafferty et al., 2001" startWordPosition="2176" endWordPosition="2179">er than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). s.t. u E SN, where b = w + α (5) Step3 α-update: α(t+1) = α(t) + ξ(w(t+1) − u(t+1)) (6) Step4 convergence check: 20 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Evaluation measures: The purpose of our experiments is to investigate the effectiveness of our proposed method in terms of both its performance and the complexity of the trained model. Therefore, </context>
<context position="14244" citStr="Lafferty et al., 2001" startWordPosition="2406" endWordPosition="2409">to maximize COMP. We also report the Fβ=1 score (F-sc) for NER, and the unlabeled attachment score (UAS) for DEPAR for comparison with previous studies. Model complexity is evaluated by the number of non-zero active features (#nzF) and the degree of freedom (#DoF) (Zhong and Kwok, 2011). #nzF is the number of features whose corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the International Conference on Machine Learning (ICML 2001), pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the Limited Memory BFGS Method for Large Scale Optimization.</title>
<date>1989</date>
<journal>Math. Programming, Ser. B,</journal>
<volume>45</volume>
<issue>3</issue>
<contexts>
<context position="14497" citStr="Liu and Nocedal, 1989" startWordPosition="2443" endWordPosition="2446">edom (#DoF) (Zhong and Kwok, 2011). #nzF is the number of features whose corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5L2PA is also known as a loss augmented variant of</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C. Liu and Jorge Nocedal. 1989. On the Limited Memory BFGS Method for Large Scale Optimization. Math. Programming, Ser. B, 45(3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="12549" citStr="Marcus et al., 1994" startWordPosition="2127" endWordPosition="2130">ss update through the data in each ADMM iteration (Duh et al., 2011). Note that the total calculation cost of our method does not increase much from original online learning algorithm since the calculation cost of Steps 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). s.t. u E SN, where b = w + α (5) Step3 α-update: α(t+1) = α(t) + ξ(w(t+1) − u(t+1)) (6) Step4 convergence check: 20 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for ev</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online Large-margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="12613" citStr="McDonald et al., 2005" startWordPosition="2138" endWordPosition="2141"> 2011). Note that the total calculation cost of our method does not increase much from original online learning algorithm since the calculation cost of Steps 2 through 4 is relatively much smaller than that of Step1. 4 Experiments We conducted experiments on two well-studied NLP tasks, namely named entity recognition (NER) and dependency parsing (DEPAR). Basic settings: We simply reused the settings of most previous studies. We used CoNLL’03 data (Tjong Kim Sang and De Meulder, 2003) for NER, and the Penn Treebank (PTB) III corpus (Marcus et al., 1994) converted to dependency trees for DEPAR (McDonald et al., 2005). s.t. u E SN, where b = w + α (5) Step3 α-update: α(t+1) = α(t) + ξ(w(t+1) − u(t+1)) (6) Step4 convergence check: 20 Our decoding models are the Viterbi algorithm on CRF (Lafferty et al., 2001), and the secondorder parsing model proposed by (Carreras, 2007) for NER and DEPAR, respectively. Features are automatically generated according to the predefined feature templates widely-used in the previous studies. We also integrated the cluster features obtained by the method explained in (Koo et al., 2008) as additional features for evaluating our method in the range of the current best systems. Ev</context>
<context position="15155" citStr="McDonald et al., 2005" startWordPosition="2550" endWordPosition="2553">orithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 4.1 Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1-regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1-regularized learning algorithms. Then, we have the following two possible settings; DC-ADMM: we leveraged the baseline L1-regularized learning algorithm to solve Step1, and set A1 = 0 and A2 = 0 for Step2. DCwL1- ADMM: we leveraged the baseline L2-regularized learning algorithm, but without L2-regularizer, to solve Step1, and set A1 &gt; 0 and A2 = 0 for Step2</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online Large-margin Training of Dependency Parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaotong Shen</author>
<author>Hsin-Cheng Huang</author>
</authors>
<title>Grouping Pursuit Through a Regularization Solution Surface.</title>
<date>2010</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>105</volume>
<issue>490</issue>
<contexts>
<context position="3267" citStr="Shen and Huang, 2010" startWordPosition="503" endWordPosition="506">ning framework that can reduce the model complexity beyond that possible by simply applying L1-regularizers. To achieve our goal, we focus on the recently developed concept of automatic feature grouping (Tibshirani et al., 2005; Bondell and Reich, 2008). We introduce a model learning framework that achieves feature grouping by incorporating a discrete constraint during model learning. 2 Feature Grouping Concept Going beyond L1-regularized sparse modeling, the idea of ‘automatic feature grouping’ has recently been developed. Examples are fused lasso (Tibshirani et al., 2005), grouping pursuit (Shen and Huang, 2010), and OSCAR (Bondell and Reich, 2008). The concept of automatic feature grouping is to find accurate models that have fewer degrees of freedom. This is equivalent to enforce every optimization variables to be equal as much as possible. A simple example is that ˆw1 = (0.1, 0.5, 0.1, 0.5, 0.1) is preferred over ˆw2 = (0.1, 0.3, 0.2, 0.5, 0.3) since ˆw1 and ˆw2 have two and four unique values, respectively. There are several merits to reducing the degree 1This paper refers to model after completion of (supervised) model learning as “trained model” (1) 18 Proceedings of the 51st Annual Meeting of </context>
</contexts>
<marker>Shen, Huang, 2010</marker>
<rawString>Xiaotong Shen and Hsin-Cheng Huang. 2010. Grouping Pursuit Through a Regularization Solution Surface. Journal of the American Statistical Association, 105(490):727–739.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
<author>Michael Saunders</author>
<author>Saharon Rosset</author>
<author>Ji Zhu</author>
<author>Keith Knight</author>
</authors>
<title>Sparsity and Smoothness via the Fused Lasso.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society Series B,</journal>
<pages>91--108</pages>
<contexts>
<context position="2873" citStr="Tibshirani et al., 2005" startWordPosition="445" endWordPosition="448">o easily and automatically yield compact models without strong concern over feature selection. Compact models generally have significant and clear advantages in practice: instances are faster loading speed to memory, less memory occupation, and even faster decoding is possible if the model is small enough to be stored in cache memory. Given this background, our aim is to establish a model learning framework that can reduce the model complexity beyond that possible by simply applying L1-regularizers. To achieve our goal, we focus on the recently developed concept of automatic feature grouping (Tibshirani et al., 2005; Bondell and Reich, 2008). We introduce a model learning framework that achieves feature grouping by incorporating a discrete constraint during model learning. 2 Feature Grouping Concept Going beyond L1-regularized sparse modeling, the idea of ‘automatic feature grouping’ has recently been developed. Examples are fused lasso (Tibshirani et al., 2005), grouping pursuit (Shen and Huang, 2010), and OSCAR (Bondell and Reich, 2008). The concept of automatic feature grouping is to find accurate models that have fewer degrees of freedom. This is equivalent to enforce every optimization variables to </context>
</contexts>
<marker>Tibshirani, Saunders, Rosset, Zhu, Knight, 2005</marker>
<rawString>Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. 2005. Sparsity and Smoothness via the Fused Lasso. Journal of the Royal Statistical Society Series B, pages 91–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>142--147</pages>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of CoNLL-2003, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>477--485</pages>
<contexts>
<context position="1924" citStr="Tsuruoka et al., 2009" startWordPosition="291" endWordPosition="294">hich are also interpreted as feature weights. L(w; D) and Q(w) represent a loss function and a regularization term, respectively. Nowadays, we, in most cases, utilize a supervised learning method expressed as the above optimization problem to estimate the feature weights of many natural language processing (NLP) tasks, such as text classification, POS-tagging, named entity recognition, dependency parsing, and semantic role labeling. In the last decade, the L1-regularization technique, which incorporates L1-norm into Q(w), has become popular and widely-used in many NLP tasks (Gao et al., 2007; Tsuruoka et al., 2009). The reason is that L1-regularizers encourage feature weights to be zero as much as possible in model learning, which makes the resultant model a sparse solution (many zero-weights exist). We can discard all features whose weight is zero from the trained model1 without any loss. Therefore, L1-regularizers have the ability to easily and automatically yield compact models without strong concern over feature selection. Compact models generally have significant and clear advantages in practice: instances are faster loading speed to memory, less memory occupation, and even faster decoding is possi</context>
<context position="14999" citStr="Tsuruoka et al., 2009" startWordPosition="2522" endWordPosition="2525">(Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et al., 2009), which are more familiar to the NLP community. 5L2PA is also known as a loss augmented variant of onebest MIRA, well-known in DEPAR (McDonald et al., 2005). 4.1 Configurations of Our Method Base learning algorithm: The settings of our method in our experiments imitate L1-regularized learning algorithm since the purpose of our experiments is to investigate the effectiveness against standard L1-regularized learning algorithms. Then, we have the following two possible settings; DC-ADMM: we leveraged the baseline L1-regularized learning algorithm to solve Step1, and set A1 = 0 and A2 = 0 for Step</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Yoshimasa Tsuruoka, Jun’ichi Tsujii, and Sophia Ananiadou. 2009. Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 477–485.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Xiao</author>
</authors>
<title>Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>2596</pages>
<contexts>
<context position="14389" citStr="Xiao, 2010" startWordPosition="2429" endWordPosition="2430">el complexity is evaluated by the number of non-zero active features (#nzF) and the degree of freedom (#DoF) (Zhong and Kwok, 2011). #nzF is the number of features whose corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, and passive-aggressive algorithm (L2PA) (Crammer et al., 2006)5 for DEPAR since L2-regularizer often provides better results than L1-regularizer (Gao et al., 2007). For a fair comparison, we applied the procedure of Step2 as a simple quantization method to trained models obtained from L1-regularized model learning, which we refer to as (QT). 4RDA provided better results at least in our experiments than Ll-regularized FOBOS (Duchi and Singer, 2009), and its variant (Tsuruoka et </context>
</contexts>
<marker>Xiao, 2010</marker>
<rawString>Lin Xiao. 2010. Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization. Journal of Machine Learning Research, 11:2543– 2596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Wenliang Zhong</author>
<author>James T Kwok</author>
</authors>
<title>Efficient Sparse Modeling with Automatic Feature Grouping.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="10480" citStr="Zhong and Kwok (2011)" startWordPosition="1775" endWordPosition="1778">alent simple form: uˆ= arg minu{12||u − b0||22 + λ01||u||1J (8) where b0 = ρ λ2+ρb, and λ0 1 = λ1 λ2+ρ. This optimization is still a combinatorial optimization problem. However unlike Eq. 2, this optimization can be efficiently solved. Fig. 2 shows the procedure to obtain the exact solution of Eq. 5, namely u(t+1). The remarkable point is that the costly combinatorial optimization problem is disappeared, and instead, we are only required to perform two feature-wise calculations whose total time complexities is O(N log |S|) and fully parallelizable. The similar technique has been introduced in Zhong and Kwok (2011) for discarding a costly combinatorial problem from the optimization with OSCAR-regularizers with the help of proximal gradient methods, i.e., (Beck and Teboulle, 2009). We omit to show the detailed derivation of Fig. 2 because of the space reason. However, this is easily understandable. The key properties are the following two folds; (i) The objective shown in Eq. 8 is a convex and also symmetric function with respect to ˆu0, where ˆu0 is the optimal solution of Eq. 8 without the discrete constraint. Therefore, the optimal solution uˆ is at the point where the nearest valid point given SN fro</context>
<context position="13909" citStr="Zhong and Kwok, 2011" startWordPosition="2354" endWordPosition="2357">ectiveness of our proposed method in terms of both its performance and the complexity of the trained model. Therefore, our evaluation measures consist of two axes. Task performance was mainly evaluated in terms of the complete sentence accuracy (COMP) since the objective of all model learning methods evaluated in our experiments is to maximize COMP. We also report the Fβ=1 score (F-sc) for NER, and the unlabeled attachment score (UAS) for DEPAR for comparison with previous studies. Model complexity is evaluated by the number of non-zero active features (#nzF) and the degree of freedom (#DoF) (Zhong and Kwok, 2011). #nzF is the number of features whose corresponding feature weight is non-zero in the trained model, and #DoF is the number of unique non-zero feature weights. Baseline methods: Our main baseline is L1- regularized sparse modeling. To cover both batch and online leaning, we selected L1-regularized CRF (L1CRF) (Lafferty et al., 2001) optimized by OWL-QN (Andrew and Gao, 2007) for the NER experiment, and the L1-regularized regularized dual averaging (L1RDA) method (Xiao, 2010)4 for DEPAR. Additionally, we also evaluated L2- regularized CRF (L2CRF) with L-BFGS (Liu and Nocedal, 1989) for NER, an</context>
</contexts>
<marker>Zhong, Kwok, 2011</marker>
<rawString>Leon Wenliang Zhong and James T. Kwok. 2011. Efficient Sparse Modeling with Automatic Feature Grouping. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zou</author>
<author>Trevor Hastie</author>
</authors>
<title>Regularization and Variable Selection via the Elastic Net.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<pages>67--301</pages>
<contexts>
<context position="4574" citStr="Zou and Hastie, 2005" startWordPosition="717" endWordPosition="720">9 2013. c�2013 Association for Computational Linguistics of freedom. For example, previous studies clarified that it can reduce the chance of over-fitting to the training data (Shen and Huang, 2010). This is an important property for many NLP tasks since they are often modeled with a high-dimensional feature space, and thus, the over-fitting problem is readily triggered. It has also been reported that it can improve the stability of selecting non-zero features beyond that possible with the standard L1- regularizer given the existence of many highly correlated features (J¨ornsten and Yu, 2003; Zou and Hastie, 2005). Moreover, it can dramatically reduce model complexity. This is because we can merge all features whose feature weight values are equivalent in the trained model into a single feature cluster without any loss. 3 Modeling with Feature Grouping This section describes our proposal for obtaining a feature grouping solution. 3.1 Integration of a Discrete Constraint Let S be a finite set of discrete values, i.e., a set integer from −4 to 4, that is, S = {−4,..., −1, 0, 1, ... , 4}. The detailed discussion how we define S can be found in our experiments section since it deeply depends on training da</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>Hui Zou and Trevor Hastie. 2005. Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, 67:301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>