<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<note confidence="0.6781825">
Monte Carlo inference and maximization for phrase-based translation
Abhishek Arun* Chris Dyer† Barry Haddow*
</note>
<email confidence="0.846079">
a.arun@sms.ed.ac.uk redpony@umd.edu bhaddow@inf.ed.ac.uk
</email>
<author confidence="0.640241">
Phil Blunsom* Adam Lopez* Philipp Koehn*
</author>
<affiliation confidence="0.788833666666667">
pblunsom@inf.ed.ac.uk alopez@inf.ed.ac.uk pkoehn@inf.ed.ac.uk
*Department of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.822998">
Edinburgh, EH8 9AB, UK
</address>
<sectionHeader confidence="0.972699" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983828125">
Recent advances in statistical machine
translation have used beam search for
approximate NP-complete inference within
probabilistic translation models. We present
an alternative approach of sampling from the
posterior distribution defined by a translation
model. We define a novel Gibbs sampler
for sampling translations given a source
sentence and show that it effectively explores
this posterior distribution. In doing so
we overcome the limitations of heuristic
beam search and obtain theoretically sound
solutions to inference problems such as
finding the maximum probability translation
and minimum expected risk training and
decoding.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999448647058824">
Statistical machine translation (SMT) poses the
problem: given a foreign sentence f, find the
translation e* that maximises the conditional
posterior probability p(elf). This probabilistic
formulation of translation has driven development
of state-of-the-art systems which are able to learn
from parallel corpora which were generated for
other purposes — a direct result of employing a
mathematical framework that we can reason about
independently of any particular model.
For example, we can train SMT models using
maximum likelihood estimation (Brown et al., 1993;
Och and Ney, 2000; Marcu and Wong, 2002). Alter-
natively, we can train to minimise probabilistic con-
ceptions of risk (expected loss) with respect to trans-
lation metrics, thereby obtaining better results for
those metrics (Kumar and Byrne, 2004; Smith and
</bodyText>
<affiliation confidence="0.9782345">
†Department of Linguistics
University of Maryland
</affiliation>
<address confidence="0.584995">
College Park, MD 20742, USA
</address>
<bodyText confidence="0.994889105263158">
Eisner, 2006; Zens and Ney, 2007). We can also use
Bayesian inference techniques to avoid resorting to
heuristics that damage the probabilistic interpreta-
tion of the models (Zhang et al., 2008; DeNero et
al., 2008; Blunsom et al., 2009).
Most models define multiple derivations for each
translation; the probability of a translation is thus
the sum over all of its derivations. Unfortunately,
finding the maximum probability translation is NP-
hard for all but the most trivial of models in this
setting (Sima’an, 1996). It is thus necessary to resort
to approximations for this sum and the search for its
maximum e*.
The most common of these approximations is
the max-derivation approximation, which for many
models can be computed in polynomial time via
dynamic programming (DP). Though effective for
some problems, it has many serious drawbacks for
probabilistic inference:
</bodyText>
<listItem confidence="0.990676888888889">
1. It typically differs from the true model maxi-
mum.
2. It often requires additional approximations in
search, leading to further error.
3. It introduces restrictions on models, such as
use of only local features.
4. It provides no good solution to compute the
normalization factor Z(f) required by many prob-
abilistic algorithms.
</listItem>
<bodyText confidence="0.999743857142857">
In this work, we solve these problems using a
Monte Carlo technique with none of the above draw-
backs. Our technique is based on a novel Gibbs
sampler that draws samples from the posterior dis-
tribution of a phrase-based translation model (Koehn
et al., 2003) but operates in linear time with respect
to the number of input words (Section 2). We show
</bodyText>
<page confidence="0.980487">
102
</page>
<note confidence="0.99042">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.771201">
that it is effective for both decoding (Section 3) and
minimum risk training (Section 4).
</bodyText>
<sectionHeader confidence="0.9764965" genericHeader="introduction">
2 A Gibbs sampler for phrase-based
translation models
</sectionHeader>
<bodyText confidence="0.999953363636364">
We begin by assuming a phrase-based translation
model in which the input sentence, f, is segmented
into phrases, which are sequences of adjacent
words.1 Each foreign phrase is translated into the
target language, to produce an output sentence e
and an alignment a representing the mapping from
source to target phrases. Phrases are allowed to be
reordered during translation.
The model is defined with a log-linear form,
with feature function vector h and parametrised by
weight vector 0, as described in Koehn et al. (2003).
</bodyText>
<equation confidence="0.996416333333333">
exp[0 - h(e,a,f)]
P(e,a�f;0) = (1)
E(e, a,) exp [0 - h(el, al, f)]
</equation>
<bodyText confidence="0.999985818181818">
The features h of the model are usually few and
are themselves typically probabilistic models
indicating e.g, the relative frequency of a target
phrase translation given a source phrase (translation
model), the fluency of the target phrase (language
model) and how phrases reorder with respect
to adjacent phrases (reordering model). There
is a further parameter Λ that limits how many
source language words may intervene between
two adjacent target language phrases. For the
experiments in this paper, we use Λ = 6.
</bodyText>
<subsectionHeader confidence="0.997699">
2.1 Gibbs sampling
</subsectionHeader>
<bodyText confidence="0.998974428571429">
We use Markov chain Monte Carlo (MCMC) as an
alternative to DP search (Geman and Geman, 1984;
Metropolis and Ulam, 1949). MCMC probabilis-
tically generates sample derivations from the com-
plete search space. The probability of generating
each sample is conditioned on the previous sam-
ple, forming a Markov chain. After a long enough
interval (referred to as the burn-in) this chain returns
samples from the desired distribution.
Our MCMC sampler uses Gibbs sampling, which
obtains samples from the joint distribution of a set
of random variables X = {X1, ... , Xn1. It starts
with some initial state (X1 = x10, . . . , Xn = xn0),
and generates a Markov chain of samples, where
</bodyText>
<footnote confidence="0.727464">
1These phrases are not necessarily linguistically motivated.
</footnote>
<bodyText confidence="0.999496230769231">
each sample is the result of applying a set of Gibbs
operators to the previous sample. Each operator is
defined by specifying a subset of the random vari-
ables Y C X, which the operator updates by sam-
pling from the conditional distribution P(YIX \Y ).
The set X \ Y is referred to as the Markov blanket
and is unchanged by the operator.
In the case of translation, we require a Gibbs sam-
pler that produces a sequence of samples, SN1 =
(e1, a1) ... (eN, aN), that are drawn from the dis-
tribution P(e, alf). These samples can thus be used
to estimate the expectation of a function h(e, a, f)
under the distribution as follows:
</bodyText>
<equation confidence="0.9996434">
N h(ai, ei, f) (2)
1
EP(a,elf)[h] = lim
N� N
i=1
</equation>
<bodyText confidence="0.999685">
Taking h to be an indicator function
h = δ(a, ˆa)δ(e, ˆe) provides an estimate of
P(ˆa, ˆe1f), and using h = δ(e, ˆe) marginalises over
all derivations a&apos;, yielding an estimate of P(ˆe1f).
</bodyText>
<subsectionHeader confidence="0.999548">
2.2 Gibbs operators
</subsectionHeader>
<bodyText confidence="0.99751308">
Our sampler consists of three operators. Examples
of these are depicted in Figure 1.
The RETRANS operator varies the translation of a
single source phrase. Segmentation, alignment, and
all other translations are held constant.
The MERGE-SPLIT operator varies the source
segmentation at a single word boundary. If the
boundary is a split point in the current hypothesis,
the adjoining phrases can be merged, provided
that the corresponding target phrases are adjacent
and the phrase table contains a translation of the
merged phrase. If the boundary is not a split point,
the covering phrase may be split, provided that
the phrase table contains a translation of both new
phrases. Remaining segmentation points, phrase
alignment and phrase translations are held constant.
The REORDER operator varies the target phrase
order for a pair of source phrases, provided that
the new alignment does not violate reordering limit
Λ. Segmentation, phrase translations, and all other
alignments are held constant.
To illustrate the RETRANS operator, we will
assume a simplified model with two features: a
bigram language model Plm and a translation model
Ptm. Both features are assigned a weight of 1.
</bodyText>
<page confidence="0.998929">
103
</page>
<bodyText confidence="0.984109">
c&apos;est un resultat remarquable
it is some result remarkable
</bodyText>
<figureCaption confidence="0.995713333333333">
Figure 1: Example evolution of an initial hypothesis via
application of several operators, with Markov blanket
indicated by shading.
</figureCaption>
<bodyText confidence="0.999089833333333">
We denote the start of the sentence with S and the
language model context with C. Assuming the
French phrase c’est can be translated either as it is or
but, the RETRANS operator at step (b) stochastically
chooses an English phrase, eˆ in proportion to the
phrases’ conditional probabilities.
</bodyText>
<equation confidence="0.964253714285714">
Ptm(but|c’est) · Plm(S but some)
P (but|c’est, C) =
Z
and
Ptm(it is|c’est) · Plm(S it is some)
P (it is|c’est, C) =
Z
</equation>
<bodyText confidence="0.564729">
where
</bodyText>
<equation confidence="0.998121">
Z = Ptm(but|c’est) · Plm(S but some) +
Ptm(it is|c’est) · Plm(S it is some)
</equation>
<bodyText confidence="0.9946695">
Conditional distributions for the MERGE-SPLIT and
REORDER operators can be derived in an analogous
fashion.
A complete iteration of the sampler consists of
applying each operator at each possible point in the
1
sentence, and a sample is collected after each opera-
tor has performed a complete pass.
</bodyText>
<subsectionHeader confidence="0.992924">
2.3 Algorithmic complexity
</subsectionHeader>
<bodyText confidence="0.999938909090909">
Since both the RETRANS and MERGE-SPLIT oper-
ators are applied by iterating over source side word
positions, their complexity is linear in the size of the
input.
The REORDER operator iterates over the positions
in the input and for the source phrase found at that
position considers swapping its target phrase with
that of every other source phrase, provided that the
reordering limit is not violated. This means that it
can only consider swaps within a fixed-length win-
dow, so complexity is linear in sentence length.
</bodyText>
<subsectionHeader confidence="0.99076">
2.4 Experimental verification
</subsectionHeader>
<bodyText confidence="0.999797583333333">
To verify that our sampler was behaving as expected,
we computed the KL divergence between its
inferred distribution ˆq(e|f) and the true distribution
over a single sentence (Figure 2). We computed
the true posterior distribution p(e|f) under an
Arabic-English phrase-based translation model
with parameters trained to maximise expected
BLEU (Section 4), summing out the derivations for
identical translations and computing the partition
term Z(f). As the number of iterations increases,
the KL divergence between the distributions
approaches zero.
</bodyText>
<sectionHeader confidence="0.994032" genericHeader="method">
3 Decoding
</sectionHeader>
<bodyText confidence="0.999201428571429">
The task of decoding amounts to finding the single
translation e* that maximises or minimises some cri-
terion given a source sentence f. In this section
we consider three common approaches to decod-
ing, maximum translation (MaxTrans), maximum
derivation (MaxDeriv), and minimum risk decoding
(MinRisk):
</bodyText>
<equation confidence="0.886116">
arg max(e,a) p(e, a|f) (MaxDeriv)
arg maxe p(e|f) (MaxTrans)
E
arg mine e� Eel(e)p(e&apos;|f) (MinRisk)
</equation>
<bodyText confidence="0.987840181818182">
In the minimum risk decoder, Eel(e) is any real-
valued loss (error) function that computes the error
of one hypothesis e with respect to some reference
e&apos;. Our loss is a sentence-level approximation of
(1 − BLEU).
As noted in section 2, the Gibbs sampler can
be used to provide an estimate of the probability
distribution P(a, e|f) and therefore to determine
the maximum of this distribution, in other words
the most likely derivation. Furthermore, we can
marginalise over the alignments to estimate P(e|f)
</bodyText>
<figure confidence="0.990648304347826">
(d)
REORDER
RETRANS
MERGE
c&apos;est un resultat remarquable
c&apos;est un resultat remarquable
c&apos;est un resultat remarquable
but some result remarkable
it is a result remarkable
it is a
remarkable
result
(a)
Initial
⎧
⎨
⎩
e* =
104
● KL Divergence
● ●
10 100 1000 10000 100000 1000000
Iterations
</figure>
<figureCaption confidence="0.780985">
Figure 2: The KL divergence of the true posterior distri-
bution and the distribution estimated by the Gibbs sam-
pler at different numbers of iterations for the Arabic
</figureCaption>
<bodyText confidence="0.972454">
source sentence r}ys wzrA’ mAlyzyA yzwr Al�byn (in
English, The prime minister of Malaysia visits the Philip-
pines).
and so obtain the most likely translation. The Gibbs
sampler can therefore be used as a decoder, either
running in max-derivation and max-translation
mode. Using the Gibbs sampler in this way makes
max-translation decoding tractable, and so will
help determine whether max-translation offers any
benefit over the usual max-derivation. Using the
Gibbs sampler as a decoder also allows us to verify
that it is producing valid samples from the desired
distribution.
</bodyText>
<subsectionHeader confidence="0.999982">
3.1 Training data and preparation.
</subsectionHeader>
<bodyText confidence="0.9999653">
The experiments in this section were performed
using the French-English and German-English
parallel corpora from the WMT09 shared translation
task (Callison-Burch et al., 2009), as well as 300k
parallel Arabic-English sentences from the NIST
MT evaluation training data.2 For all language
pairs, we constructed a phrase-based translation
model as described in Koehn et al. (2003), limiting
the phrase length to 5. The target side of the parallel
corpus was used to train a 3-gram language model.
</bodyText>
<footnote confidence="0.9969522">
2The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
sentences with confidence c &gt; 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
</footnote>
<bodyText confidence="0.998427363636363">
For the German and French systems, the DEV2006
set was used for model tuning and the TEST2007
(in-domain) and NEWS-DEV2009B (out-of-domain)
sets for testing. For the Arabic system, the MT02
set (10 reference translations) was used for tuning
and MT03 (4 reference translations) was used for
evaluation. To reduce the size of the phrase table,
we used the association-score technique suggested
by Johnson et al. (2007a). Translation quality is
reported using case-insensitive BLEU (Papineni et
al., 2002).
</bodyText>
<subsectionHeader confidence="0.999717">
3.2 Translation performance
</subsectionHeader>
<bodyText confidence="0.9999833125">
For the experiments reported in this section, we
used feature weights trained with minimum error
rate training (MERT; Och, 2003) . Because MERT
ignores the denominator in Equation 1, it is invari-
ant with respect to the scale of the weight vector
θ — the Moses implementation simply normalises
the weight vector it finds by its `1-norm. However,
when we use these weights in a true probabilistic
model, the scaling factor affects the behaviour of
the model since it determines how peaked or flat the
distribution is. If the scaling factor is too small, then
the distribution is too flat and the sampler spends
too much time exploring unimportant probability
regions. If it is too large, then the distribution is too
peaked and the sampler may concentrate on a very
narrow probability region. We optimised the scaling
factor on a 200-sentence portion of the tuning set,
finding that a multiplicative factor of 10 worked best
for fr-en and a multiplicative factor of 6 for de-en. 3
The first experiment shows the effect of different
initialisations and numbers of sampler iterations on
max-derivation decoding performance of the sam-
pler. The Moses decoder (Koehn et al., 2007) was
used to generate the starting hypothesis, either in
full DP max-derivation mode, or alternatively with
restrictions on the features and reordering, or with
zero weights to simulate a random initialisation, and
the number of iterations varied from 100 to 200,000,
with a 100 iteration burn-in in each case. Figure 3
shows the variation of model score with sampler iter-
ation, for the different starting points, and for both
language pairs.
</bodyText>
<footnote confidence="0.420273666666667">
3We experimented with annealing, where the scale factor is
gradually increased to sharpen the distribution while sampling.
However, we found no improvements with annealing.
</footnote>
<table confidence="0.622027">
KL divergence
0.001 0.01 0.1
</table>
<page confidence="0.702886">
105
</page>
<figure confidence="0.9989212">
French−English
Iterations
German−English
Iterations
100 1000 10000
Model score
−20.1 −20.0 −19.9 −19.8 −19.7 −19.6
Initialisation
full
mono
nolm
zero
100 1000 10000 100000
Initialisation
full
mono
nolm
zero
−40.6 −40.4 −40.2 −40.0 −39.8
Model score
</figure>
<figureCaption confidence="0.967789">
Figure 3: Mean maximum model score, as a function of iteration number and starting point. The starting point can
either be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with no
language model (nolm) or the monotone translation with all weights set to zero (zero).
</figureCaption>
<bodyText confidence="0.999985375">
Comparing the best model scores found by the
sampler, with those found by the Moses decoder
with its default settings, we found that around
50,000 sampling iterations were required for
fr-en and 100,000 for de-en, for the sampler to
give equivalent model scores to Moses. From
Figure 3 we can see that the starting point did not
have an appreciable effect on the model score of
the best derivation, except with low numbers of
iterations. This indicates that the sampler is able
to move fairly quickly towards the maximum of
the distribution from any starting point, in other
words it has good mobility. Running the sampler
for 100,000 iterations took on average 1670 seconds
per sentence on the French-English data set and
1552 seconds per sentence on German-English.
A further indication of the dependence of sampler
accuracy on the iteration count is provided by Fig-
ure 4. In this graph, we show the mean Spearman’s
rank correlation between the nbest lists of deriva-
tions when ranked by (i) model score and (ii) the
posterior probability estimated by the sampler. This
measure of sampler accuracy also shows a logarith-
mic dependence on the sample size.
</bodyText>
<subsectionHeader confidence="0.993014">
3.3 Minimum risk decoding
</subsectionHeader>
<bodyText confidence="0.999098666666667">
The sampler also allows us to perform minimum
Bayes risk (MBR) decoding, a technique introduced
by Kumar and Byrne (2004). In their work, as an
</bodyText>
<figure confidence="0.336913">
Iterations
</figure>
<figureCaption confidence="0.982415">
Figure 4: Mean Spearman’s rank correlation of 1000-best
list of derivations ranked according to (i) model score and
(ii) posterior probability estimated by sampler. This was
measured on a 200 sentence subset of DEV2006.
</figureCaption>
<bodyText confidence="0.99998525">
approximation of the model probability distribution,
the expected loss of the decoder is calculated by
summing over an n-best list. With the Gibbs sam-
pler, however, we should be able to obtain a much
more accurate view of the model probability distri-
bution. In order to compare max-translation, max-
derivation and MBR decoding with the Gibbs sam-
pler, and the Moses baseline, we ran experiments
</bodyText>
<figure confidence="0.989932333333333">
100 1000 10000 100000
Correlation
0.2 0.3 0.4 0.5 0.6 0.7 0.8
Language Pairs
fr−en
de−en
</figure>
<page confidence="0.983977">
106
</page>
<table confidence="0.998556166666667">
fr-en de-en
in out in out
Moses 32.7 19.1 27.4 15.9
MaxD 32.6 19.1 27.0 15.5
MaxT 32.6 19.1 27.4 16.0
MBR 32.6 19.2 27.3 16.0
</table>
<tableCaption confidence="0.9299548">
Table 1: Comparison of the BLEU score of the Moses
decoder with the sampler running in max-derivation
(MaxD), max-translation (MaxT) and minumum Bayes
risk (MBR) modes. The test sets are TEST2007 (in) and
NEWS-DEV2009B (out)
</tableCaption>
<bodyText confidence="0.999756111111111">
on both European language pairs, using both the in-
domain and out-of-domain test sets. The sampler
was initialised with the output of Moses with the
feature weights set to zero and restricted to mono-
tone, and run for 100,000 iterations with a 100 iter-
ation burn-in. The scale factors were set to the same
values as in the previous experiment. The relative
translation quality (measured according to BLEU) is
shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.846923">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999994">
These results show very little difference between the
decoding methods, indicating that the Gibbs sam-
pling decoder can perform as well as a standard DP
based max-derivation decoder with these models,
and that there is no gain from doing max-translation
or MBR decoding. However it should be noted that
the model used for these experiments was optimised
by MERT, for max-derivation decoding, and so the
experiments do not rule out the possibility that max-
translation and MBR decoding will offer an advan-
tage on an appropriately optimised model.
</bodyText>
<sectionHeader confidence="0.995489" genericHeader="method">
4 Minimum risk training
</sectionHeader>
<bodyText confidence="0.999935956521739">
In the previous section, we described how our sam-
pler can be used to search for the best translation
under a variety of decoding criteria (max deriva-
tion, translation, and minimum risk). However, there
appeared to be little benefit to marginalizing over
the latent derivations. This is almost certainly a side
effect of the MERT training approach that was used
to construct the models so as to maximise the per-
formance of the model on its single best derivation,
without regard to the shape of the rest of the dis-
tribution (Blunsom et al., 2008). In this section we
describe a further application of the Gibbs sampler:
to do unbiased minimum risk training.
While there have been at least two previous
attempts to do minimum risk training for MT, both
approaches relied on biased k-best approximations
(Smith and Eisner, 2006; Zens and Ney, 2007).
Since we sample from the whole distribution, we
will have a more accurate risk assessment.
The risk, or expected loss, of a probabilistic trans-
lation model on a corpus D, defined with respect to
a particular loss function `e(e), where eˆ is the refer-
ence translation and e is a hypothesis translation
</bodyText>
<equation confidence="0.9992125">
L = � � p(e|f)`�e(e) (3)
(e,f)ED e
</equation>
<bodyText confidence="0.999748">
This value can be trivially computed using equa-
tion (2). In this section, we are concerned with find-
ing the parameters 0 that minimise (3). Fortunately,
with the log-linear parameterization of p(e|f), L is
differentiable with respect to 0:
</bodyText>
<equation confidence="0.853301333333333">
p(e|f)`�e(e) (hk − Ep(e|f)[hk])
(4)
Equation (4) is slightly more complicated to com-
</equation>
<bodyText confidence="0.999957045454545">
pute using the sampler since it requires the feature
expectation in order to evaluate the final term. How-
ever, this can be done simply by making two passes
over the samples, computing the feature expecta-
tions on the first pass and the gradient on the second.
We have now shown how to compute our
objective (3), the expected loss, and a gradient
with respect to the model parameters we want
to optimise, (4), so we can use any standard
first-order optimization technique. Since the
sampler introduces stochasticity into the gradient
and objective, we use stochastic gradient descent
methods which are more robust to noise than
more sophisticated quasi-Newtonian methods
like L-BFGS (Schraudolph et al., 2007). For the
experiments below, we updated the learning rate
after each step proportionally to difference in
successive gradients (Schraudolph, 1999).
For the experiments reported in this section, we
used sample sizes of 8000 and estimated the gradi-
ent on sets of 100 sentences drawn randomly (with
replacement) from the development corpus. For a
</bodyText>
<equation confidence="0.962245">
�= �
(e,f)ED e
∂L
∂θk
</equation>
<page confidence="0.991756">
107
</page>
<table confidence="0.999886">
Training Decoder MT03
Moses Max Derivation 44.6
MERT Moses MBR 44.8
Gibbs MBR 44.9
Moses Max Derivation 40.6
MinRisk MaxTrans 41.8
Gibbs MBR 42.9
</table>
<tableCaption confidence="0.992336333333333">
Table 2: Decoding with minimum risk trained systems,
compared with decoding with MERT-trained systems on
Arabic to English MT03 data
</tableCaption>
<bodyText confidence="0.9962635">
loss function we use 4-gram (1 − BLEU) computed
individually for each sentence4. By examining per-
formance on held-out data, we find the model con-
verges typically in fewer than 20 iterations.
</bodyText>
<subsectionHeader confidence="0.998107">
4.1 Training experiments
</subsectionHeader>
<bodyText confidence="0.999989642857143">
During preliminary experiments with training, we
observed on a held-out data set (portions of MT04)
that the magnitude of the weights vector increased
steadily (effectively sharpening the distribution), but
without any obvious change in the objective. Since
this resulted in poor generalization we added a reg-
ularization term of ||�B − #||2/2σ2 to L. We initially
set the means to zero, but after further observing that
the translations under all decoding criteria tended to
be shorter than the reference (causing a significant
drop in performance when evaluated using BLEU),
we found that performance could be improved by
setting pwp = −0.5, indicating a preference for a
lower weight on this parameter.
Table 2 compares the performance on Arabic to
English translation of systems tuned with MERT
(maximizing corpus BLEU) with systems tuned to
maximise expected sentence-level BLEU. Although
the performance of the minimum risk model under
all decoding criteria is lower than that of the orig-
inal MERT model, we note that the positive effect
of marginalizing over derivations as well as using
minimum risk decoding for obtaining good results
on this model. A full exploration of minimum risk
training is beyond the scope of this paper, but these
initial experiments should help emphasise the versa-
tility of the sampler and its utility in solving a variety
of problems. In the conclusion, we will, however,
</bodyText>
<footnote confidence="0.8108785">
4The ngram precision counts are smoothed by adding 0.01
for n &gt; 1
</footnote>
<bodyText confidence="0.99607">
discuss some possible future directions that can be
taken to make this style of training more competitive
with standard baseline systems.
</bodyText>
<sectionHeader confidence="0.97268" genericHeader="method">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999653170731707">
We have described an algorithmic technique that
solves certain problems, but also verifies the utility
of standard approximation techniques. For exam-
ple, we found that on standard test sets the sampler
performs similarly to the DP max-derivation solu-
tion and equally well regardless of how it is ini-
tialised. From this we conclude that at least for
MERT-trained models, the max-derivation approx-
imation is adequate for finding the best translation.
Although the training approach presented in
Section 4 has a number of theoretical advantages,
its performance in a one-best evaluation falls short
when compared with a system tuned for optimal
one-best performance using MERT. This contradicts
the results of Zens and Ney (2007), who optimise
the same objective and report improvements over a
MERT baseline. We conjecture that the difference
is due to the biased k-best approximation they used.
By considering only the most probable derivations,
they optimise a smoothed error surface (as one
does in minimum risk training), but not one that
is indicative of the true risk. If our hypothesis
is accurate, then the advantage is accidental and
ultimately a liability. Our results are in line with
those reported by Smith and Eisner (2006) who
find degradation in performance when minimizing
risk, but compensate by “sharpening” the model
distribution for the final training iterations,
effectively maximising one-best performance
rather minimising risk over the full distribution
defined by their model. In future work, we will
explore possibilities for artificially sharpening the
distribution during training so as to better anticipate
the one-best evaluation conditions typical of MT.
However, for applications which truly do require a
distribution over translations, such as re-ranking,
our method for minimising expected risk would be
the objective of choice.
Using sampling for model induction has two fur-
ther advantages that we intend to explore. First,
although MERT performs quite well on models with
</bodyText>
<page confidence="0.99746">
108
</page>
<bodyText confidence="0.999990545454546">
small numbers of features (such as those we consid-
ered in this paper), in general the algorithm severely
limits the number of features that can be used since
it does not use gradient-based updates during opti-
mization, instead updating one feature at a time. Our
training method (Section 4) does not have this limi-
tation, so it can use many more features.
Finally, for the DP-based max-derivation approx-
imation to be computationally efficient, the features
characterizing the steps in the derivation must be
either computable independently of each other or
with only limited local context (as in the case of the
language model or distortion costs). This has led to
a situation where entire classes of potentially use-
ful features are not considered because they would
be impractical to integrate into a DP based trans-
lation system. With the sampler this restriction is
mitigated: any function of h(e, f, a) may partici-
pate in the translation model subject only to its own
computability. Freed from the rusty manacles of
dynamic programming, we anticipate development
of many useful features.
</bodyText>
<sectionHeader confidence="0.999985" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.99964">
Our sampler is similar to the decoder of Germann
et al. (2001), which starts with an approximate solu-
tion and then incrementally improves it via operators
such as RETRANS and MERGE-SPLIT. It is also
similar to the estimator of Marcu and Wong (2002),
who employ the same operators to search the align-
ment space from a heuristic initialisation. Although
the operators are similar, the use is different. These
previous efforts employed their operators in a greedy
hill-climbing search. In contrast, our operators are
applied probabilistically, making them theoretically
well-founded for a variety of inference problems.
Our use of Gibbs sampling follows from its
increasing use in Bayesian inference problems in
NLP (Finkel et al., 2006; Johnson et al., 2007b).
Most closely related is the work of DeNero
et al. (2008), who derive a Gibbs sampler for
phrase-based alignment, using it to infer phrase
translation probabilities. The use of Monte Carlo
techniques to calculate posteriors is similar to that
of Chappelier and Rajman (2000) who use those
techniques to find the best parse under models where
the derivation and the parse are not isomorphic.
To our knowledge, we are the first to apply Monte
Carlo methods to maximum translation and mini-
mum risk translation. Approaches to the former
(Blunsom et al., 2008; May and Knight, 2006) rely
on dynamic programming techniques which do not
scale well without heuristic approximations, while
approaches to the latter (Smith and Eisner, 2006;
Zens et al., 2007) use biased k-best approximations.
</bodyText>
<sectionHeader confidence="0.998864" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999975">
We have described a Gibbs sampler for approxi-
mating two intractable problems in SMT: maximum
translation decoding (and its variant, minimum risk
decoding) and minimum risk training. By using
Monte Carlo techniques we avoid the biases associ-
ated with the more commonly used DP based max-
derivation (or k-best derivation) approximation. In
doing so we provide a further tool to the translation
community that we envision will allow the devel-
opment and analysis of increasing theoretically well
motivated techniques.
</bodyText>
<sectionHeader confidence="0.998389" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999461666666666">
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001; and by the
EuroMatrix project funded by the European Commission
(6th Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9990375">
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. In Advances in Neu-
ral Information Processing Systems 21, pages 161–
168.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,
editors. 2009. Proc. of Workshop on Machine Trans-
lations, Athens.
J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo
sampling for NP-hard maximization problems in the
</reference>
<page confidence="0.983899">
109
</page>
<reference confidence="0.999806">
framework of weighted parsing. In Natural Language
Processing – NLP 2000, number 1835 in Lecture Notes
in Artificial Intelligence, pages 106–117. Springer.
J. DeNero, A. Bouchard, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. of EMNLP.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721–741.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proceedings of ACL.
Association for Computational Linguistics, July.
J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a.
Improving translation quality by discarding most of
the phrasetable. In Proc. of EMNLP-CoNLL, Prague.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL-HLT, pages 139–
146, Rochester, New York, April.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48–
54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177–180, June.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133–139.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proc. of NAACL-HLT.
N. Metropolis and S. Ulam. 1949. The Monte Carlo
method. Journal of the American Statistical Associa-
tion, 44(247):335–341.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proc. of
COLING, Saarbrucken, Germany, July.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160–167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311–318.
N. N. Schraudolph, J. Yu, and S. G¨unter. 2007. A
stochastic quasi-Newton method for online convex
optimization. In Proc. of Artificial Intelligence and
Statistics.
N. N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-
09-99, IDSIA.
K. Sima’an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree grammars. In
Proc. of COLING, Copenhagen.
D. A. Smith and J. Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING-ACL, pages 787–794.
R. Zens and H. Ney. 2007. Efficient phrase-table repre-
sentation for machine translation with applications to
online MT and speech translation. In Proc. of NAACL-
HLT, Rochester, New York.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP, pages 524–532, Prague,
Czech Republic.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proc. of ACL: HLT, pages
97–105, Columbus, Ohio.
</reference>
<page confidence="0.9984">
110
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.339463">
<title confidence="0.948115">Monte Carlo inference and maximization for phrase-based translation</title>
<author confidence="0.600854">a arunsms ed ac uk redponyumd edu bhaddowinf ed ac uk pblunsominf ed ac uk alopezinf ed ac uk pkoehninf ed ac uk</author>
<affiliation confidence="0.987422">of Informatics University of Edinburgh</affiliation>
<address confidence="0.998368">Edinburgh, EH8 9AB, UK</address>
<abstract confidence="0.992825117647059">Recent advances in statistical machine translation have used beam search for approximate NP-complete inference within probabilistic translation models. We present an alternative approach of sampling from the posterior distribution defined by a translation model. We define a novel Gibbs sampler for sampling translations given a source sentence and show that it effectively explores this posterior distribution. In doing we overcome the limitations of heuristic beam search and obtain theoretically sound solutions to inference problems such as finding the maximum probability translation and minimum expected risk training and decoding.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>A discriminative latent variable model for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="19447" citStr="Blunsom et al., 2008" startWordPosition="3108" endWordPosition="3111">r an advantage on an appropriately optimised model. 4 Minimum risk training In the previous section, we described how our sampler can be used to search for the best translation under a variety of decoding criteria (max derivation, translation, and minimum risk). However, there appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `e(e), where eˆ is the reference translation and e is a hypothesis translati</context>
<context position="28048" citStr="Blunsom et al., 2008" startWordPosition="4487" endWordPosition="4490">ence problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2008</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blunsom</author>
<author>T Cohn</author>
<author>M Osborne</author>
</authors>
<title>Bayesian synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems 21,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="2143" citStr="Blunsom et al., 2009" startWordPosition="297" endWordPosition="300">g maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e*. The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some problems, it has many serious </context>
</contexts>
<marker>Blunsom, Cohn, Osborne, 2009</marker>
<rawString>P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian synchronous grammar induction. In Advances in Neural Information Processing Systems 21, pages 161– 168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della Pietra</author>
<author>S A Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1573" citStr="Brown et al., 1993" startWordPosition="206" endWordPosition="209"> expected risk training and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f, find the translation e* that maximises the conditional posterior probability p(elf). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>J Schroeder</author>
<author>editors</author>
</authors>
<date>2009</date>
<booktitle>Proc. of Workshop on Machine Translations,</booktitle>
<location>Athens.</location>
<contexts>
<context position="11999" citStr="Callison-Burch et al., 2009" startWordPosition="1896" endWordPosition="1899"> Gibbs sampler can therefore be used as a decoder, either running in max-derivation and max-translation mode. Using the Gibbs sampler in this way makes max-translation decoding tractable, and so will help determine whether max-translation offers any benefit over the usual max-derivation. Using the Gibbs sampler as a decoder also allows us to verify that it is producing valid samples from the desired distribution. 3.1 Training data and preparation. The experiments in this section were performed using the French-English and German-English parallel corpora from the WMT09 shared translation task (Callison-Burch et al., 2009), as well as 300k parallel Arabic-English sentences from the NIST MT evaluation training data.2 For all language pairs, we constructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For the German and Fre</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, editors, 2009</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder, editors. 2009. Proc. of Workshop on Machine Translations, Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-C Chappelier</author>
<author>M Rajman</author>
</authors>
<title>Monte-Carlo sampling for NP-hard maximization problems in the framework of weighted parsing.</title>
<date>2000</date>
<booktitle>In Natural Language Processing – NLP 2000, number 1835 in Lecture Notes in Artificial Intelligence,</booktitle>
<pages>106--117</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="27768" citStr="Chappelier and Rajman (2000)" startWordPosition="4439" endWordPosition="4442">se previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems </context>
</contexts>
<marker>Chappelier, Rajman, 2000</marker>
<rawString>J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo sampling for NP-hard maximization problems in the framework of weighted parsing. In Natural Language Processing – NLP 2000, number 1835 in Lecture Notes in Artificial Intelligence, pages 106–117. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>A Bouchard</author>
<author>D Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2120" citStr="DeNero et al., 2008" startWordPosition="293" endWordPosition="296">train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e*. The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some problem</context>
<context position="27551" citStr="DeNero et al. (2008)" startWordPosition="4406" endWordPosition="4409">lso similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristi</context>
</contexts>
<marker>DeNero, Bouchard, Klein, 2008</marker>
<rawString>J. DeNero, A. Bouchard, and D. Klein. 2008. Sampling alignment structure under a Bayesian translation model. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C D Manning</author>
<author>A Y Ng</author>
</authors>
<title>Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="27469" citStr="Finkel et al., 2006" startWordPosition="4391" endWordPosition="4394"> incrementally improves it via operators such as RETRANS and MERGE-SPLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 200</context>
</contexts>
<marker>Finkel, Manning, Ng, 2006</marker>
<rawString>J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>D Geman</author>
</authors>
<title>Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.</title>
<date>1984</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>6--721</pages>
<contexts>
<context position="5017" citStr="Geman and Geman, 1984" startWordPosition="758" endWordPosition="761">tures h of the model are usually few and are themselves typically probabilistic models indicating e.g, the relative frequency of a target phrase translation given a source phrase (translation model), the fluency of the target phrase (language model) and how phrases reorder with respect to adjacent phrases (reordering model). There is a further parameter Λ that limits how many source language words may intervene between two adjacent target language phrases. For the experiments in this paper, we use Λ = 6. 2.1 Gibbs sampling We use Markov chain Monte Carlo (MCMC) as an alternative to DP search (Geman and Geman, 1984; Metropolis and Ulam, 1949). MCMC probabilistically generates sample derivations from the complete search space. The probability of generating each sample is conditioned on the previous sample, forming a Markov chain. After a long enough interval (referred to as the burn-in) this chain returns samples from the desired distribution. Our MCMC sampler uses Gibbs sampling, which obtains samples from the joint distribution of a set of random variables X = {X1, ... , Xn1. It starts with some initial state (X1 = x10, . . . , Xn = xn0), and generates a Markov chain of samples, where 1These phrases ar</context>
</contexts>
<marker>Geman, Geman, 1984</marker>
<rawString>S. Geman and D. Geman. 1984. Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
<author>M Jahr</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>K Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="26798" citStr="Germann et al. (2001)" startWordPosition="4288" endWordPosition="4291"> other or with only limited local context (as in the case of the language model or distortion costs). This has led to a situation where entire classes of potentially useful features are not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as RETRANS and MERGE-SPLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its in</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Proceedings of ACL. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Johnson</author>
<author>J Martin</author>
<author>G Foster</author>
<author>R Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL,</booktitle>
<location>Prague.</location>
<contexts>
<context position="12993" citStr="Johnson et al. (2007" startWordPosition="2050" endWordPosition="2053">R corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For the German and French systems, the DEV2006 set was used for model tuning and the TEST2007 (in-domain) and NEWS-DEV2009B (out-of-domain) sets for testing. For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). 3.2 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1-norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how pea</context>
<context position="27491" citStr="Johnson et al., 2007" startWordPosition="4395" endWordPosition="4398">es it via operators such as RETRANS and MERGE-SPLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic pro</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a. Improving translation quality by discarding most of the phrasetable. In Proc. of EMNLP-CoNLL, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
<author>T Griffiths</author>
<author>S Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>139--146</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="12993" citStr="Johnson et al. (2007" startWordPosition="2050" endWordPosition="2053">R corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For the German and French systems, the DEV2006 set was used for model tuning and the TEST2007 (in-domain) and NEWS-DEV2009B (out-of-domain) sets for testing. For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). 3.2 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1-norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how pea</context>
<context position="27491" citStr="Johnson et al., 2007" startWordPosition="4395" endWordPosition="4398">es it via operators such as RETRANS and MERGE-SPLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic pro</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>M. Johnson, T. Griffiths, and S. Goldwater. 2007b. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Proc. of NAACL-HLT, pages 139– 146, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrasebased translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3369" citStr="Koehn et al., 2003" startWordPosition="494" endWordPosition="497">ks for probabilistic inference: 1. It typically differs from the true model maximum. 2. It often requires additional approximations in search, leading to further error. 3. It introduces restrictions on models, such as use of only local features. 4. It provides no good solution to compute the normalization factor Z(f) required by many probabilistic algorithms. In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks. Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model (Koehn et al., 2003) but operates in linear time with respect to the number of input words (Section 2). We show 102 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics that it is effective for both decoding (Section 3) and minimum risk training (Section 4). 2 A Gibbs sampler for phrase-based translation models We begin by assuming a phrase-based translation model in which the input sentence, f, is segmented into phrases, which are sequences of adjacent words.1 Each foreign phrase </context>
<context position="12202" citStr="Koehn et al. (2003)" startWordPosition="1927" endWordPosition="1930">ne whether max-translation offers any benefit over the usual max-derivation. Using the Gibbs sampler as a decoder also allows us to verify that it is producing valid samples from the desired distribution. 3.1 Training data and preparation. The experiments in this section were performed using the French-English and German-English parallel corpora from the WMT09 shared translation task (Callison-Burch et al., 2009), as well as 300k parallel Arabic-English sentences from the NIST MT evaluation training data.2 For all language pairs, we constructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For the German and French systems, the DEV2006 set was used for model tuning and the TEST2007 (in-domain) and NEWS-DEV2009B (out-of-domain) sets for testing. For the Arabic system, the MT02 set (10 reference translations) was</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrasebased translation. In Proc. of HLT-NAACL, pages 48– 54, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A B Mayne</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL Demonstration Session,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="14283" citStr="Koehn et al., 2007" startWordPosition="2259" endWordPosition="2262">en the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and the sampler may concentrate on a very narrow probability region. We optimised the scaling factor on a 200-sentence portion of the tuning set, finding that a multiplicative factor of 10 worked best for fr-en and a multiplicative factor of 6 for de-en. 3 The first experiment shows the effect of different initialisations and numbers of sampler iterations on max-derivation decoding performance of the sampler. The Moses decoder (Koehn et al., 2007) was used to generate the starting hypothesis, either in full DP max-derivation mode, or alternatively with restrictions on the features and reordering, or with zero weights to simulate a random initialisation, and the number of iterations varied from 100 to 200,000, with a 100 iteration burn-in in each case. Figure 3 shows the variation of model score with sampler iteration, for the different starting points, and for both language pairs. 3We experimented with annealing, where the scale factor is gradually increased to sharpen the distribution while sampling. However, we found no improvements </context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL Demonstration Session, pages 177–180, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Processings of HLT-NAACL.</booktitle>
<contexts>
<context position="1817" citStr="Kumar and Byrne, 2004" startWordPosition="245" endWordPosition="248"> formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an</context>
<context position="16803" citStr="Kumar and Byrne (2004)" startWordPosition="2663" endWordPosition="2666">entence on the French-English data set and 1552 seconds per sentence on German-English. A further indication of the dependence of sampler accuracy on the iteration count is provided by Figure 4. In this graph, we show the mean Spearman’s rank correlation between the nbest lists of derivations when ranked by (i) model score and (ii) the posterior probability estimated by the sampler. This measure of sampler accuracy also shows a logarithmic dependence on the sample size. 3.3 Minimum risk decoding The sampler also allows us to perform minimum Bayes risk (MBR) decoding, a technique introduced by Kumar and Byrne (2004). In their work, as an Iterations Figure 4: Mean Spearman’s rank correlation of 1000-best list of derivations ranked according to (i) model score and (ii) posterior probability estimated by sampler. This was measured on a 200 sentence subset of DEV2006. approximation of the model probability distribution, the expected loss of the decoder is calculated by summing over an n-best list. With the Gibbs sampler, however, we should be able to obtain a much more accurate view of the model probability distribution. In order to compare max-translation, maxderivation and MBR decoding with the Gibbs sampl</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. In Processings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>133--139</pages>
<contexts>
<context position="1615" citStr="Marcu and Wong, 2002" startWordPosition="214" endWordPosition="217"> Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f, find the translation e* that maximises the conditional posterior probability p(elf). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the prob</context>
<context position="26984" citStr="Marcu and Wong (2002)" startWordPosition="4319" endWordPosition="4322">not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as RETRANS and MERGE-SPLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>D. Marcu and W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proc. of EMNLP, pages 133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J May</author>
<author>K Knight</author>
</authors>
<title>A better n-best list: Practical determinization of weighted finite tree automata.</title>
<date>2006</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="28071" citStr="May and Knight, 2006" startWordPosition="4491" endWordPosition="4494">Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the t</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>J. May and K. Knight. 2006. A better n-best list: Practical determinization of weighted finite tree automata. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Metropolis</author>
<author>S Ulam</author>
</authors>
<title>The Monte Carlo method.</title>
<date>1949</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>44</volume>
<issue>247</issue>
<contexts>
<context position="5045" citStr="Metropolis and Ulam, 1949" startWordPosition="762" endWordPosition="765">e usually few and are themselves typically probabilistic models indicating e.g, the relative frequency of a target phrase translation given a source phrase (translation model), the fluency of the target phrase (language model) and how phrases reorder with respect to adjacent phrases (reordering model). There is a further parameter Λ that limits how many source language words may intervene between two adjacent target language phrases. For the experiments in this paper, we use Λ = 6. 2.1 Gibbs sampling We use Markov chain Monte Carlo (MCMC) as an alternative to DP search (Geman and Geman, 1984; Metropolis and Ulam, 1949). MCMC probabilistically generates sample derivations from the complete search space. The probability of generating each sample is conditioned on the previous sample, forming a Markov chain. After a long enough interval (referred to as the burn-in) this chain returns samples from the desired distribution. Our MCMC sampler uses Gibbs sampling, which obtains samples from the joint distribution of a set of random variables X = {X1, ... , Xn1. It starts with some initial state (X1 = x10, . . . , Xn = xn0), and generates a Markov chain of samples, where 1These phrases are not necessarily linguistic</context>
</contexts>
<marker>Metropolis, Ulam, 1949</marker>
<rawString>N. Metropolis and S. Ulam. 1949. The Monte Carlo method. Journal of the American Statistical Association, 44(247):335–341.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In Proc. of COLING,</booktitle>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="1592" citStr="Och and Ney, 2000" startWordPosition="210" endWordPosition="213">ing and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f, find the translation e* that maximises the conditional posterior probability p(elf). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for eac</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. Och and H. Ney. 2000. A comparison of alignment models for statistical machine translation. In Proc. of COLING, Saarbrucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="13238" citStr="Och, 2003" startWordPosition="2086" endWordPosition="2087">was used for model tuning and the TEST2007 (in-domain) and NEWS-DEV2009B (out-of-domain) sets for testing. For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). 3.2 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1-norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and th</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="13080" citStr="Papineni et al., 2002" startWordPosition="2061" endWordPosition="2064">T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For the German and French systems, the DEV2006 set was used for model tuning and the TEST2007 (in-domain) and NEWS-DEV2009B (out-of-domain) sets for testing. For the Arabic system, the MT02 set (10 reference translations) was used for tuning and MT03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). 3.2 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1-norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribut</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N N Schraudolph</author>
<author>J Yu</author>
<author>S G¨unter</author>
</authors>
<title>A stochastic quasi-Newton method for online convex optimization.</title>
<date>2007</date>
<booktitle>In Proc. of Artificial Intelligence and Statistics.</booktitle>
<marker>Schraudolph, Yu, G¨unter, 2007</marker>
<rawString>N. N. Schraudolph, J. Yu, and S. G¨unter. 2007. A stochastic quasi-Newton method for online convex optimization. In Proc. of Artificial Intelligence and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N N Schraudolph</author>
</authors>
<title>Local gain adaptation in stochastic gradient descent.</title>
<date>1999</date>
<tech>Technical Report IDSIA09-99, IDSIA.</tech>
<contexts>
<context position="21262" citStr="Schraudolph, 1999" startWordPosition="3406" endWordPosition="3407">gradient on the second. We have now shown how to compute our objective (3), the expected loss, and a gradient with respect to the model parameters we want to optimise, (4), so we can use any standard first-order optimization technique. Since the sampler introduces stochasticity into the gradient and objective, we use stochastic gradient descent methods which are more robust to noise than more sophisticated quasi-Newtonian methods like L-BFGS (Schraudolph et al., 2007). For the experiments below, we updated the learning rate after each step proportionally to difference in successive gradients (Schraudolph, 1999). For the experiments reported in this section, we used sample sizes of 8000 and estimated the gradient on sets of 100 sentences drawn randomly (with replacement) from the development corpus. For a �= � (e,f)ED e ∂L ∂θk 107 Training Decoder MT03 Moses Max Derivation 44.6 MERT Moses MBR 44.8 Gibbs MBR 44.9 Moses Max Derivation 40.6 MinRisk MaxTrans 41.8 Gibbs MBR 42.9 Table 2: Decoding with minimum risk trained systems, compared with decoding with MERT-trained systems on Arabic to English MT03 data loss function we use 4-gram (1 − BLEU) computed individually for each sentence4. By examining per</context>
</contexts>
<marker>Schraudolph, 1999</marker>
<rawString>N. N. Schraudolph. 1999. Local gain adaptation in stochastic gradient descent. Technical Report IDSIA09-99, IDSIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation by means of tree grammars.</title>
<date>1996</date>
<booktitle>In Proc. of COLING,</booktitle>
<location>Copenhagen.</location>
<marker>Sima’an, 1996</marker>
<rawString>K. Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of tree grammars. In Proc. of COLING, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>J Eisner</author>
</authors>
<title>Minimum risk annealing for training log-linear models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL,</booktitle>
<pages>787--794</pages>
<contexts>
<context position="19726" citStr="Smith and Eisner, 2006" startWordPosition="3153" endWordPosition="3156">appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `e(e), where eˆ is the reference translation and e is a hypothesis translation L = � � p(e|f)`�e(e) (3) (e,f)ED e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters 0 that minimise (3). Fortunately, with the log-linear parameterization of p(e|f), L is differentiable with respect to 0: p</context>
<context position="24855" citStr="Smith and Eisner (2006)" startWordPosition="3983" endWordPosition="3986">ith a system tuned for optimal one-best performance using MERT. This contradicts the results of Zens and Ney (2007), who optimise the same objective and report improvements over a MERT baseline. We conjecture that the difference is due to the biased k-best approximation they used. By considering only the most probable derivations, they optimise a smoothed error surface (as one does in minimum risk training), but not one that is indicative of the true risk. If our hypothesis is accurate, then the advantage is accidental and ultimately a liability. Our results are in line with those reported by Smith and Eisner (2006) who find degradation in performance when minimizing risk, but compensate by “sharpening” the model distribution for the final training iterations, effectively maximising one-best performance rather minimising risk over the full distribution defined by their model. In future work, we will explore possibilities for artificially sharpening the distribution during training so as to better anticipate the one-best evaluation conditions typical of MT. However, for applications which truly do require a distribution over translations, such as re-ranking, our method for minimising expected risk would b</context>
<context position="28223" citStr="Smith and Eisner, 2006" startWordPosition="4513" endWordPosition="4516">nment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Thi</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. A. Smith and J. Eisner. 2006. Minimum risk annealing for training log-linear models. In Proc. of COLING-ACL, pages 787–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Efficient phrase-table representation for machine translation with applications to online MT and speech translation.</title>
<date>2007</date>
<booktitle>In Proc. of NAACLHLT,</booktitle>
<location>Rochester, New York.</location>
<contexts>
<context position="1940" citStr="Zens and Ney, 2007" startWordPosition="264" endWordPosition="267">which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e*. The most common o</context>
<context position="19747" citStr="Zens and Ney, 2007" startWordPosition="3157" endWordPosition="3160">nefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `e(e), where eˆ is the reference translation and e is a hypothesis translation L = � � p(e|f)`�e(e) (3) (e,f)ED e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters 0 that minimise (3). Fortunately, with the log-linear parameterization of p(e|f), L is differentiable with respect to 0: p(e|f)`�e(e) (hk − Ep(</context>
<context position="24347" citStr="Zens and Ney (2007)" startWordPosition="3900" endWordPosition="3903">tandard approximation techniques. For example, we found that on standard test sets the sampler performs similarly to the DP max-derivation solution and equally well regardless of how it is initialised. From this we conclude that at least for MERT-trained models, the max-derivation approximation is adequate for finding the best translation. Although the training approach presented in Section 4 has a number of theoretical advantages, its performance in a one-best evaluation falls short when compared with a system tuned for optimal one-best performance using MERT. This contradicts the results of Zens and Ney (2007), who optimise the same objective and report improvements over a MERT baseline. We conjecture that the difference is due to the biased k-best approximation they used. By considering only the most probable derivations, they optimise a smoothed error surface (as one does in minimum risk training), but not one that is indicative of the true risk. If our hypothesis is accurate, then the advantage is accidental and ultimately a liability. Our results are in line with those reported by Smith and Eisner (2006) who find degradation in performance when minimizing risk, but compensate by “sharpening” th</context>
</contexts>
<marker>Zens, Ney, 2007</marker>
<rawString>R. Zens and H. Ney. 2007. Efficient phrase-table representation for machine translation with applications to online MT and speech translation. In Proc. of NAACLHLT, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of training criteria for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>524--532</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="28243" citStr="Zens et al., 2007" startWordPosition="4517" endWordPosition="4520"> phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments This research was suppo</context>
</contexts>
<marker>Zens, Hasan, Ney, 2007</marker>
<rawString>R. Zens, S. Hasan, and H. Ney. 2007. A systematic comparison of training criteria for statistical machine translation. In Proc. of EMNLP, pages 524–532, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>C Quirk</author>
<author>R C Moore</author>
<author>D Gildea</author>
</authors>
<title>Bayesian learning of non-compositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proc. of ACL: HLT,</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2099" citStr="Zhang et al., 2008" startWordPosition="289" endWordPosition="292">For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and †Department of Linguistics University of Maryland College Park, MD 20742, USA Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e*. The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effec</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008. Bayesian learning of non-compositional phrases with synchronous parsing. In Proc. of ACL: HLT, pages 97–105, Columbus, Ohio.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>