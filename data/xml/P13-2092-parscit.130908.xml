<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001956">
<title confidence="0.997928">
An Annotated Corpus of Quoted Opinions in News Articles
</title>
<author confidence="0.976633">
Tim O’Keefe James R. Curran Peter Ashwell Irena Koprinska
</author>
<affiliation confidence="0.9688185">
-lab, School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.426327">
NSW 2006, Australia
</address>
<email confidence="0.996693">
{tokeefe,james,pash4408,irena}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997362" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99939725">
Quotes are used in news articles as evi-
dence of a person’s opinion, and thus are
a useful target for opinion mining. How-
ever, labelling each quote with a polarity
score directed at a textually-anchored tar-
get can ignore the broader issue that the
speaker is commenting on. We address
this by instead labelling quotes as support-
ing or opposing a clear expression of a
point of view on a topic, called a position
statement. Using this we construct a cor-
pus covering 7 topics with 2,228 quotes.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997386">
News articles are a useful target for opinion min-
ing as they discuss salient opinions by newswor-
thy people. Rather than asserting what a person’s
opinion is, journalists typically provide evidence
by using reported speech, and in particular, direct
quotes. We focus on direct quotes as expressions
of opinion, as they can be accurately extracted and
attributed to a speaker (O’Keefe et al., 2012).
Characterising the opinions in quotes remains
challenging. In sentiment analysis over product
reviews, polarity labels are commonly used be-
cause the target, the product, is clearly identified.
However, for quotes on topics of debate, the target
and meaning of polarity labels is less clear. For ex-
ample, labelling a quote about abortion as simply
positive or negative is uninformative, as a speaker
can use either positive or negative language to sup-
port or oppose either side of the debate.
Previous work (Wiebe et al., 2005; Balahur
et al., 2010) has addressed this by giving each
expression of opinion a textually-anchored target.
While this makes sense for named entities, it does
not apply as obviously for topics, such as abortion,
that may not be directly mentioned. Our solution
is to instead define position statements, which are
</bodyText>
<note confidence="0.354039">
Abortion: Women should have the right to choose an abortion.
</note>
<tableCaption confidence="0.828704928571429">
Carbon tax: Australia should introduce a tax on carbon or an
emissions trading scheme to combat global warming.
Immigration: Immigration into Australia should be maintained
or increased because its benefits outweigh any negatives.
Reconciliation: The Australian government should formally
apologise to the Aboriginal people for past injustices.
Republic: Australia should cease to be a monarchy with the
Queen as head of state and become a republic with an Australian
head of state.
Same-sex marriage: Same-sex couples should have the right to
attain the legal state of marriage as it is for heterosexual couples.
Work choices: Australia should introduce WorkChoices to give
employers more control over wages and conditions.
Table 1: Topics and their position statements.
</tableCaption>
<bodyText confidence="0.995273851851852">
clear statements of a viewpoint or position on a
particular topic. Quotes related to this topic can
then be labelled as supporting, neutral, or oppos-
ing the position statement. This disambiguates the
meaning of the polarity labels, and allows us to
determine the side of the debate that the speaker
is on. Table 1 shows the topics and position state-
ments used in this work, and some example quotes
from the republic topic are given below. Note that
the first example includes no explicit mention of
the monarchy or the republic.
Positive: “I now believe that the time has come... for us to
have a truly Australian constitutional head of state.”
Neutral: “The establishment of an Australian republic is es-
sentially a symbolic change, with the main arguments, for
and against, turning on national identity... ”
Negative: “I personally think that the monarchy is a tradition
which we want to keep.”
With this formulation we define an annotation
scheme and build a corpus covering 7 topics, with
100 documents per topic. This corpus includes
3,428 quotes, of which 1,183 were marked in-
valid, leaving 2,228 that were marked as support-
ing, neutral, or opposing the relevant topic state-
ment. All quotes in our corpus were annotated by
three annotators, with Fleiss’ κ values of between
0.43 and 0.45, which is moderate.
</bodyText>
<page confidence="0.974806">
516
</page>
<note confidence="0.530542">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.98563" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999943418604651">
Early work in sentiment analysis (Turney, 2002;
Pang et al., 2002; Dave et al., 2003; Blitzer et al.,
2007) focused on product and movie reviews,
where the text under analysis discusses a single
product or movie. In these cases, labels like posi-
tive and negative are appropriate as they align well
with the overall communicative goal of the text.
Later work established aspect-oriented opinion
mining (Hu and Liu, 2004), where the aim is to
find features or aspects of products that are dis-
cussed in a review. The reviewer’s position on
each aspect can then be classified as positive or
negative, which results in a more fine-grained clas-
sification that can be combined to form an opin-
ion summary. These approaches assume that each
document has a single source (the document’s au-
thor), whose communicative goal is to evaluate a
well-defined target, such as a product or a movie.
However this does not hold in news articles, where
the goal of the journalist is to present the view-
points of potentially many people.
Several studies (Wiebe et al., 2005; Wilson
et al., 2005; Kim and Hovy, 2006; Godbole et al.,
2007) have looked at sentiment in news text, with
some (Balahur and Steinberger, 2009; Balahur
et al., 2009, 2010) focusing on quotes. In all of
these studies the authors have textually-anchored
the target of the sentiment. While this makes sense
for targets that can be resolved back to named enti-
ties, it does not apply as obviously when the quote
is arguing for a particular viewpoint in a debate,
as the topic may not be mentioned explicitly and
polarity labels may not align to sides of the debate.
Work on debate summarisation and subgroup
detection (Somasundaran and Wiebe, 2010; Abu-
Jbara et al., 2012; Hassan et al., 2012) has of-
ten used data from online debate forums, partic-
ularly those forums where users are asked to se-
lect whether they support or oppose a given propo-
sition before they can participate. This is simi-
lar to our aim with news text, where instead of a
textually-anchored target, we have a proposition,
against which we can evaluate quotes.
</bodyText>
<sectionHeader confidence="0.949838" genericHeader="method">
3 Position Statements
</sectionHeader>
<bodyText confidence="0.999951">
Our goal in this study is to determine which side of
a debate a given quote supports. Assigning polar-
ity labels to a textually-anchored target does not
work here for several reasons. Quotes may not
mention the debate topic, there may be many rel-
</bodyText>
<table confidence="0.9993852">
No cont. Context
Topic Quotes AA κ AA κ
Abortion 343 .77 .57 .73 .53
Carbon tax 278 .71 .42 .57 .34
Immigration 249 .58 .18 .58 .25
Reconcil. 513 .66 .37 .68 .44
Republic 347 .68 .51 .71 .58
Same-sex m. 246 .72 .51 .71 .55
Work choices 269 .72 .45 .65 .44
Total 2,245 .69 .43 .66 .45
</table>
<tableCaption confidence="0.9152235">
Table 2: Average Agreement (AA) and Fleiss’ κ
over the valid quotes
</tableCaption>
<bodyText confidence="0.999853090909091">
evant textually-anchored targets for a single topic,
and polarity labels do not necessarily align with
sides of a debate.
We instead define position statements, which
clearly state the position that one side of the debate
is arguing for. We can then characterise opinions
as supporting, neutral towards, or opposing this
particular position. Position statements should not
argue for a particular position, rather they should
simply state what the position is. Table 1 shows
the position statements that we use in this work.
</bodyText>
<sectionHeader confidence="0.994274" genericHeader="method">
4 Annotation
</sectionHeader>
<bodyText confidence="0.99997180952381">
For our task we expect a set of news articles on
a given topic as input, where the direct quotes in
the articles have been extracted and attributed to
speakers. A position statement will have been de-
fined, that states a point of view on the topic, and
a small subset of quotes will have been labelled as
supporting, neutral, or opposing the given state-
ment. A system performing this task would then
label the remaining quotes as supporting, neutral,
or opposing, and return them to the user.
A major contribution of this work is that we
construct a fully labelled corpus, which can be
used to evaluate systems that perform the task de-
scribed above. To build this corpus we employed
three annotators, one of whom is an author, while
the other two were hired using the outsourcing
website Freelancer1. Our data is drawn from the
Sydney Morning Herald2 archive, which ranges
from 1986 until 2009, and it covers seven topics
that were subject to debate within Australian news
media during that time. For each topic we used
</bodyText>
<footnote confidence="0.9999735">
1http://www.freelancer.com
2http://www.smh.com.au
</footnote>
<page confidence="0.950085">
517
</page>
<table confidence="0.9997682">
No cont. Context
Topic Quotes AA n AA n
Abortion 343 .78 .52 .74 .46
Carbon tax 278 .72 .39 .59 .19
Immigration 249 .58 .08 .58 .14
Reconcil. 513 .66 .31 .69 .36
Republic 347 .69 .39 .72 .41
Same-sex m. 246 .73 .43 .73 .40
Work choices 269 .73 .40 .67 .32
Total 2,245 .70 .36 .68 .32
</table>
<tableCaption confidence="0.965317">
Table 3: Average Agreement (AA) and Fleiss’ n
when the labels are neutral versus non-neutral
</tableCaption>
<bodyText confidence="0.9948525">
Apache Solr3 to find the top 100 documents that
matched a manually-constructed search query. All
documents were tokenised and POS-tagged and the
named entities were found using the system from
Hachey et al. (2013). Finally, the quotes were ex-
tracted and attributed to speakers using the system
from O’Keefe et al. (2012).
For the first part of the task, annotators were
asked to label each quote without considering any
context. In other words they were asked to only
use the text of the quote itself as evidence for an
opinion, not the speaker’s prior opinions or the
text of the document. They were then asked to la-
bel the quote a second time, while considering the
text surrounding the quote, although they were still
asked to ignore the prior opinions of the speaker.
For each of these choices annotators were given a
five-point scale ranging from strong or clear op-
position to strong or clear support, where support
or opposition is relative to the position statement.
Annotators were also asked to mark instances
where either the speaker or quote span was incor-
rectly identified, although they were asked to con-
tinue annotating the quote as though it were cor-
rect. They were also asked to mark quotes that
were invalid due to either the quote being off-
topic, or the item not being a quote (e.g. book ti-
tles, scare quotes, etc.).
</bodyText>
<sectionHeader confidence="0.990047" genericHeader="method">
5 Corpus results
</sectionHeader>
<bodyText confidence="0.999979">
In order to achieve the least amount of noise in
our corpus, we opted to discard quotes that any an-
notator had marked as invalid. From the original
set of 3,428 quotes, 1,183 (35%) were removed,
which leaves 2,245 (65%). From the original cor-
pus, 23% were marked off-topic, which shows that
</bodyText>
<footnote confidence="0.736395">
3http://lucene.apache.org/solr/
</footnote>
<bodyText confidence="0.999980595744681">
in order to label opinions in news, a system would
first have to identify the topic-relevant parts of the
text. The annotators further indicated that 16%
were not quotes, and there were a small number of
cases (&lt;1%) where the quote span was incorrect.
Annotators were able to select multiple reasons for
a quote being invalid.
Table 2 shows both Fleiss’ n and the raw agree-
ment averaged between annotators for each topic.
We collapsed the two supporting labels together,
as well as the two opposing labels, such that we
end up with a classification of opposes vs. neu-
tral vs. supports. The no context and context cases
scored 0.69 and 0.66 in raw agreement, while the
n values were 0.43 and 0.45, which is moderate.
Intuitively we expect that the confusion is
largely between neutral and the two polar labels.
To examine this we merged all the non-neutral la-
bels into one group and calculated the agreement
between the non-neutral group and the neutral la-
bel, as shown in Table 3. For the non-neutral vs.
neutral agreement we find that despite stability in
raw agreement, Fleiss’ n drops substantially, to
0.36 (no context) and 0.32 (context).
For comparison we remove all neutral annota-
tions and focus on disagreement between the po-
lar labels. For this we cannot use Fleiss’ n, as it
requires a fixed number of annotations per quote,
however we can average the pairwise n values be-
tween annotators, which results in values of 0.93
(no context) and 0.92 (context). Though they are
not directly comparable, the magnitude of the dif-
ference between the numbers (0.36 and 0.32 vs.
0.93 and 0.92) indicates that deciding when an
opinion provides sufficient evidence of support or
opposition is the main challenge facing annotators.
To adjudicate the decisions annotators made, we
opted to take a majority vote for cases of two
or three-way agreement, while discarding cases
where annotators did not agree (1% of quotes).
The final distribution of labels in the corpus is
shown in Table 4. For both the no context and
context cases the largest class was neutral with
61% and 46% of the corpus respectively. The drop
in neutrality between the no context and context
cases shows that the interpretation of a quote can
change based on the context it is placed in.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999747">
In refining our annotation scheme we noted several
factors that make annotation difficult.
</bodyText>
<page confidence="0.994201">
518
</page>
<table confidence="0.9997706">
No context Context
Topic Quotes Opp. Neut. Supp. Quotes Opp. Neut. Supp.
Abortion 343 .13 .63 .25 340 .16 .52 .32
Carbon tax 273 .09 .70 .21 273 .14 .44 .42
Immigration 247 .09 .72 .19 245 .12 .64 .23
Reconciliation 509 .05 .57 .38 503 .07 .42 .50
Republic 345 .24 .48 .28 342 .32 .37 .32
Same-sex marriage 246 .16 .55 .28 243 .25 .38 .37
Work choices 265 .14 .72 .14 266 .26 .50 .24
Total 2,228 .12 .61 .26 2,212 .18 .46 .36
</table>
<tableCaption confidence="0.999392">
Table 4: Label distribution for the final corpus.
</tableCaption>
<bodyText confidence="0.998026714285714">
Opinion relevance When discussing a topic,
journalists will often delve into the related aspects
and opinions that people hold. This introduces a
challenge as annotators need to decide whether a
particular quote is on-topic enough to be labelled.
For instance, these quotes by the same speaker
were in an article on the carbon tax:
</bodyText>
<listItem confidence="0.7976988">
1) “Whether it’s a stealth tax, the emissions trading scheme,
whether it’s an upfront... tax like a carbon tax, there will not
be any new taxes as part of the Coalition’s policy”
2) “I don’t think it’s something that we should rush into. But
certainly I’m happy to see a debate about the nuclear option.”
</listItem>
<bodyText confidence="0.998140714285714">
In the first quote the speaker is voicing opposi-
tion to a tax on carbon, which is easy to annotate
with our scheme. However in the second quote,
the speaker is discussing nuclear power in relation
to a carbon tax, which is much more difficult, as it
is unclear whether is is off-topic or neutral.
Obfuscation and self-contradiction While
journalists usually quote someone to provide
evidence of the person’s opinion, there are some
cases where they include quotes to show that
the person is inconsistent. The following quotes
by the same speaker were included in an arti-
cle to illustrate that the speaker’s position was
inconsistent:
</bodyText>
<listItem confidence="0.927543714285714">
1) “My point is that... the most potent argument in favour of
the republic, is that why should we have a Briton as the Queen
– who, of course, in reality is also the Queen of Australia –
but a Briton as the head of State of Australia”
2) “The Coalition supports the Constitution not because we
support the... notion of the monarchy, but because we support
the way our present Constitution works”
</listItem>
<bodyText confidence="0.9999535">
The above example also indicates a level of ob-
fuscation that is reasonably common for politi-
cians. Neither of the quotes actually expresses a
clear statement of how the speaker feels about a
potential republic. The first quote is an opinion
about the strongest argument in favour of a re-
public, without necessarily making that argument,
while the second quote states a party line, with a
caveat that might indicate personal disagreement.
Annotator bias This task is prone to be influ-
enced by an annotator’s biases, including their po-
litical or cultural background, their opinion about
the topic or speaker, or their level of knowledge
about the topic.
</bodyText>
<sectionHeader confidence="0.993394" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999954444444445">
In this work we examined the problem of anno-
tating opinions in news articles. We proposed to
exploit quotes, as they are used by journalists to
provide evidence of an opinion, and are easy to
extract and attribute to speakers. Our key con-
tribution is that rather than requiring a textually-
anchored target for each quote, we instead label
quotes as supporting, neutral, or opposing a posi-
tion statement, which states a particular viewpoint
on a topic. This allowed us to resolve ambigu-
ities that arise when considering a polarity label
towards a topic. We next defined an annotation
scheme and built a corpus, which covers 7 top-
ics, with 100 documents per topic, and a total of
2,228 annotated quotes. Future work will include
building a system able to perform the task we have
defined, as well as extending this work to include
indirect quotes.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996676">
O’Keefe was supported by a University of Sydney
Merit scholarship and a Capital Markets CRC top-
up scholarship. This work was supported by ARC
Discovery grant DP1097291 and the Capital Mar-
kets CRC Computable News project.
</bodyText>
<page confidence="0.997397">
519
</page>
<sectionHeader confidence="0.990269" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.743003125">
Amjad Abu-Jbara, Mona Diab, Pradeep Dasigi,
and Dragomir Radev. 2012. Subgroup detec-
tion in ideological discussions. In Proceedings
of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 399–409.
Alexandra Balahur and Ralf Steinberger. 2009.
Rethinking sentiment analysis in the news:
From theory to practice and back. Proceedings
</bodyText>
<reference confidence="0.991658561797753">
of the First Workshop on Opinion Mining and
Sentiment Analysis.
Alexandra Balahur, Ralf Steinberger, Mijail
Kabadjov, Vanni Zavarella, Erik Van Der Goot,
Matina Halkia, Bruno Pouliquen, and Jenya
Belyaeva. 2010. Sentiment analysis in the news.
In Proceedings of the 7th International Confer-
ence on Language Resources and Evaluation,
pages 2216–2220.
Alexandra Balahur, Ralf Steinberger, Erik Van
Der Goot, Bruno Pouliquen, and Mijail Kabad-
jov. 2009. Opinion mining on newspa-
per quotations. In Proceedings of the 2009
IEEE/WIC/ACM International Joint Confer-
ence on Web Intelligence and Intelligent Agent
Technology, pages 523–526.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes
and blenders: Domain adaptation for sentiment
classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computa-
tional Linguistics, pages 440–447.
Kushal Dave, Steve Lawrence, and David Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of prod-
uct reviews. In Proceedings of the 12th inter-
national conference on World Wide Web, pages
519–528.
Namrata Godbole, Manjunath Srinivasaiah, and
Steven Skiena. 2007. Large-scale sentiment
analysis for news and blogs. In Proceedings
of the International Conference on Weblogs and
Social Media.
Ben Hachey, Will Radford, Joel Nothman,
Matthew Honnibal, and James R. Curran. 2013.
Evaluating entity linking with Wikipedia. Arti-
ficial Intelligence.
Ahmed Hassan, Amjad Abu-Jbara, and Dragomir
Radev. 2012. Detecting subgroups in online dis-
cussions by modeling positive and negative re-
lations among participants. In Proceedings of
the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning, pages
59–70.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of
the tenth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Min-
ing, pages 168–177.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of the
Workshop on Sentiment and Subjectivity in Text,
pages 1–8.
Tim O’Keefe, Silvia Pareti, James R. Curran,
Irena Koprinska, and Matthew Honnibal. 2012.
A sequence labelling approach to quote attri-
bution. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural
Language Learning, pages 790–799.
Bo Pang, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up?: Senti-
ment classification using machine learning
techniques. In Proceedings of the 2002
Conference on Empirical Methods in Natural
Language Processing, pages 79–86.
Swapna Somasundaran and Janyce Wiebe. 2010.
Recognizing stances in ideological on-line de-
bates. In Proceedings of the NAACL HLT
2010 Workshop on Computational Approaches
to Analysis and Generation of Emotion in Text,
pages 116–124.
Peter Turney. 2002. Thumbs up or thumbs down?:
Semantic orientation applied to unsupervised
classification of reviews. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, pages 417–424.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2/3):165–210.
Theresa Wilson, Paul Hoffmann, Swapna Soma-
sundaran, Jason Kessler, Janyce Wiebe, Yejin
Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. OpinionFinder: a system
for subjectivity analysis. In Proceedings of
HLT/EMNLP Interactive Demonstrations.
</reference>
<page confidence="0.996929">
520
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455462">
<title confidence="0.999868">An Annotated Corpus of Quoted Opinions in News Articles</title>
<author confidence="0.999877">Tim O’Keefe James R Curran Peter Ashwell Irena Koprinska</author>
<affiliation confidence="0.9838255">lab, School of Information University of</affiliation>
<address confidence="0.513488">NSW 2006,</address>
<abstract confidence="0.991367538461539">Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address by instead labelling quotes as supportclear expression of a of view on a topic, called a Using this we construct a corpus covering 7 topics with 2,228 quotes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>of the First Workshop on Opinion Mining and Sentiment Analysis.</title>
<marker></marker>
<rawString>of the First Workshop on Opinion Mining and Sentiment Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Ralf Steinberger</author>
<author>Mijail Kabadjov</author>
<author>Vanni Zavarella</author>
<author>Erik Van Der Goot</author>
<author>Matina Halkia</author>
<author>Bruno Pouliquen</author>
<author>Jenya Belyaeva</author>
</authors>
<title>Sentiment analysis in the news.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2216--2220</pages>
<marker>Balahur, Steinberger, Kabadjov, Zavarella, Van Der Goot, Halkia, Pouliquen, Belyaeva, 2010</marker>
<rawString>Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella, Erik Van Der Goot, Matina Halkia, Bruno Pouliquen, and Jenya Belyaeva. 2010. Sentiment analysis in the news. In Proceedings of the 7th International Conference on Language Resources and Evaluation, pages 2216–2220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Ralf Steinberger</author>
<author>Erik Van Der Goot</author>
<author>Bruno Pouliquen</author>
<author>Mijail Kabadjov</author>
</authors>
<title>Opinion mining on newspaper quotations.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology,</booktitle>
<pages>523--526</pages>
<marker>Balahur, Steinberger, Van Der Goot, Pouliquen, Kabadjov, 2009</marker>
<rawString>Alexandra Balahur, Ralf Steinberger, Erik Van Der Goot, Bruno Pouliquen, and Mijail Kabadjov. 2009. Opinion mining on newspaper quotations. In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology, pages 523–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="4452" citStr="Blitzer et al., 2007" startWordPosition="708" endWordPosition="711">ts per topic. This corpus includes 3,428 quotes, of which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Background Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These app</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<contexts>
<context position="4429" citStr="Dave et al., 2003" startWordPosition="704" endWordPosition="707">s, with 100 documents per topic. This corpus includes 3,428 quotes, of which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Background Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opi</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings of the 12th international conference on World Wide Web, pages 519–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Namrata Godbole</author>
<author>Manjunath Srinivasaiah</author>
<author>Steven Skiena</author>
</authors>
<title>Large-scale sentiment analysis for news and blogs.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="5458" citStr="Godbole et al., 2007" startWordPosition="880" endWordPosition="883">ssed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Several studies (Wiebe et al., 2005; Wilson et al., 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2010; AbuJbara et al., 2</context>
</contexts>
<marker>Godbole, Srinivasaiah, Skiena, 2007</marker>
<rawString>Namrata Godbole, Manjunath Srinivasaiah, and Steven Skiena. 2007. Large-scale sentiment analysis for news and blogs. In Proceedings of the International Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hachey</author>
<author>Will Radford</author>
<author>Joel Nothman</author>
<author>Matthew Honnibal</author>
<author>James R Curran</author>
</authors>
<title>Evaluating entity linking with Wikipedia.</title>
<date>2013</date>
<journal>Artificial Intelligence.</journal>
<contexts>
<context position="9238" citStr="Hachey et al. (2013)" startWordPosition="1540" endWordPosition="1543">om 2http://www.smh.com.au 517 No cont. Context Topic Quotes AA n AA n Abortion 343 .78 .52 .74 .46 Carbon tax 278 .72 .39 .59 .19 Immigration 249 .58 .08 .58 .14 Reconcil. 513 .66 .31 .69 .36 Republic 347 .69 .39 .72 .41 Same-sex m. 246 .73 .43 .73 .40 Work choices 269 .73 .40 .67 .32 Total 2,245 .70 .36 .68 .32 Table 3: Average Agreement (AA) and Fleiss’ n when the labels are neutral versus non-neutral Apache Solr3 to find the top 100 documents that matched a manually-constructed search query. All documents were tokenised and POS-tagged and the named entities were found using the system from Hachey et al. (2013). Finally, the quotes were extracted and attributed to speakers using the system from O’Keefe et al. (2012). For the first part of the task, annotators were asked to label each quote without considering any context. In other words they were asked to only use the text of the quote itself as evidence for an opinion, not the speaker’s prior opinions or the text of the document. They were then asked to label the quote a second time, while considering the text surrounding the quote, although they were still asked to ignore the prior opinions of the speaker. For each of these choices annotators were</context>
</contexts>
<marker>Hachey, Radford, Nothman, Honnibal, Curran, 2013</marker>
<rawString>Ben Hachey, Will Radford, Joel Nothman, Matthew Honnibal, and James R. Curran. 2013. Evaluating entity linking with Wikipedia. Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Amjad Abu-Jbara</author>
<author>Dragomir Radev</author>
</authors>
<title>Detecting subgroups in online discussions by modeling positive and negative relations among participants.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>59--70</pages>
<contexts>
<context position="6083" citStr="Hassan et al., 2012" startWordPosition="987" endWordPosition="990"> looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2010; AbuJbara et al., 2012; Hassan et al., 2012) has often used data from online debate forums, particularly those forums where users are asked to select whether they support or oppose a given proposition before they can participate. This is similar to our aim with news text, where instead of a textually-anchored target, we have a proposition, against which we can evaluate quotes. 3 Position Statements Our goal in this study is to determine which side of a debate a given quote supports. Assigning polarity labels to a textually-anchored target does not work here for several reasons. Quotes may not mention the debate topic, there may be many </context>
</contexts>
<marker>Hassan, Abu-Jbara, Radev, 2012</marker>
<rawString>Ahmed Hassan, Amjad Abu-Jbara, and Dragomir Radev. 2012. Detecting subgroups in online discussions by modeling positive and negative relations among participants. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 59–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="4764" citStr="Hu and Liu, 2004" startWordPosition="758" endWordPosition="761">oceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Background Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Sever</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5435" citStr="Kim and Hovy, 2006" startWordPosition="876" endWordPosition="879">ducts that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Several studies (Wiebe et al., 2005; Wilson et al., 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim O’Keefe</author>
<author>Silvia Pareti</author>
<author>James R Curran</author>
<author>Irena Koprinska</author>
<author>Matthew Honnibal</author>
</authors>
<title>A sequence labelling approach to quote attribution.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>790--799</pages>
<marker>O’Keefe, Pareti, Curran, Koprinska, Honnibal, 2012</marker>
<rawString>Tim O’Keefe, Silvia Pareti, James R. Curran, Irena Koprinska, and Matthew Honnibal. 2012. A sequence labelling approach to quote attribution. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 790–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="4410" citStr="Pang et al., 2002" startWordPosition="700" endWordPosition="703">us covering 7 topics, with 100 documents per topic. This corpus includes 3,428 quotes, of which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Background Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be comb</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Janyce Wiebe</author>
</authors>
<title>Recognizing stances in ideological on-line debates.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>116--124</pages>
<contexts>
<context position="6038" citStr="Somasundaran and Wiebe, 2010" startWordPosition="978" endWordPosition="981"> 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2010; AbuJbara et al., 2012; Hassan et al., 2012) has often used data from online debate forums, particularly those forums where users are asked to select whether they support or oppose a given proposition before they can participate. This is similar to our aim with news text, where instead of a textually-anchored target, we have a proposition, against which we can evaluate quotes. 3 Position Statements Our goal in this study is to determine which side of a debate a given quote supports. Assigning polarity labels to a textually-anchored target does not work here for several reasons. Quotes may not</context>
</contexts>
<marker>Somasundaran, Wiebe, 2010</marker>
<rawString>Swapna Somasundaran and Janyce Wiebe. 2010. Recognizing stances in ideological on-line debates. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 116–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<contexts>
<context position="4391" citStr="Turney, 2002" startWordPosition="698" endWordPosition="699">d build a corpus covering 7 topics, with 100 documents per topic. This corpus includes 3,428 quotes, of which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0.43 and 0.45, which is moderate. 516 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 516–520, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Background Early work in sentiment analysis (Turney, 2002; Pang et al., 2002; Dave et al., 2003; Blitzer et al., 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classificati</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter Turney. 2002. Thumbs up or thumbs down?: Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="1681" citStr="Wiebe et al., 2005" startWordPosition="266" endWordPosition="269">ion, as they can be accurately extracted and attributed to a speaker (O’Keefe et al., 2012). Characterising the opinions in quotes remains challenging. In sentiment analysis over product reviews, polarity labels are commonly used because the target, the product, is clearly identified. However, for quotes on topics of debate, the target and meaning of polarity labels is less clear. For example, labelling a quote about abortion as simply positive or negative is uninformative, as a speaker can use either positive or negative language to support or oppose either side of the debate. Previous work (Wiebe et al., 2005; Balahur et al., 2010) has addressed this by giving each expression of opinion a textually-anchored target. While this makes sense for named entities, it does not apply as obviously for topics, such as abortion, that may not be directly mentioned. Our solution is to instead define position statements, which are Abortion: Women should have the right to choose an abortion. Carbon tax: Australia should introduce a tax on carbon or an emissions trading scheme to combat global warming. Immigration: Immigration into Australia should be maintained or increased because its benefits outweigh any negat</context>
<context position="5394" citStr="Wiebe et al., 2005" startWordPosition="868" endWordPosition="871">aim is to find features or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Several studies (Wiebe et al., 2005; Wilson et al., 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subg</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2/3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>OpinionFinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP Interactive Demonstrations.</booktitle>
<contexts>
<context position="5415" citStr="Wilson et al., 2005" startWordPosition="872" endWordPosition="875">res or aspects of products that are discussed in a review. The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. Several studies (Wiebe et al., 2005; Wilson et al., 2005; Kim and Hovy, 2006; Godbole et al., 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al., 2009, 2010) focusing on quotes. In all of these studies the authors have textually-anchored the target of the sentiment. While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. Work on debate summarisation and subgroup detection (Somas</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. OpinionFinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP Interactive Demonstrations.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>