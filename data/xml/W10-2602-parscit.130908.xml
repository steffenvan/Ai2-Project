<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.9994785">
Context Adaptation in Statistical Machine Translation Using Models with
Exponentially Decaying Cache
</title>
<author confidence="0.997433">
J¨org Tiedemann
</author>
<affiliation confidence="0.9900045">
Department of Linguistics and Philology
Uppsala University, Uppsala/Sweden
</affiliation>
<email confidence="0.995062">
jorg.tiedemann@lingfil.uu.se
</email>
<sectionHeader confidence="0.993804" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967454545455">
We report results from a domain adapta-
tion task for statistical machine translation
(SMT) using cache-based adaptive lan-
guage and translation models. We apply
an exponential decay factor and integrate
the cache models in a standard phrase-
based SMT decoder. Without the need for
any domain-specific resources we obtain a
2.6% relative improvement on average in
BLEU scores using our dynamic adapta-
tion procedure.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999858125">
Most data-driven approaches to natural lan-
guage processing (NLP) are subject to the well-
known problem of lack of portability to new do-
mains/genres. Usually there is a substantial drop
in performance when testing on data from a do-
main different to the training data. Statistical ma-
chine translation is no exception. Despite its pop-
ularity, standard SMT approaches fail to provide a
framework for general application across domains
unless appropriate training data is available and
used in parameter estimation and tuning.
The main problem is the general assumption
of independent and identically distributed (i.i.d.)
variables in machine learning approaches applied
in the estimation of static global models. Recently,
there has been quite some attention to the prob-
lem of domain switching in SMT (Zhao et al.,
2004; Ueffing et al., 2007; Civera and Juan, 2007;
Bertoldi and Federico, 2009) but ground breaking
success is still missing. In this paper we report
our findings in dynamic model adaptation using
cache-based techniques when applying a standard
model to the task of translating documents from a
very different domain.
The remaining part of the paper is organized as
follows: First, we will motivate the chosen ap-
proach by reviewing the general phenomenon of
repetition and consistency in natural language text.
Thereafter, we will briefly discuss the dynamic ex-
tensions to language and translation models ap-
plied in the experiments presented in the second
last section followed by some final conclusions.
</bodyText>
<sectionHeader confidence="0.993264" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999970029411765">
Domain adaptation can be tackled in various ways.
An obvious choice for empirical systems is to ap-
ply supervised techniques in case domain-specific
training data is available. It has been shown that
small(er) amounts of in-domain data are suffi-
cient for such an approach (Koehn and Schroeder,
2007). However, this is not really a useful alter-
native for truly open-domain systems, which will
be confronted with changing domains all the time
including many new, previously unknown ones
among them.
There are also some interesting approaches to
dynamic domain adaptation mainly using flexible
mixture models or techniques for the automatic se-
lection of appropriate resources (Hildebrand et al.,
2005; Foster and Kuhn, 2007; Finch and Sumita,
2008). Ideally, a system would adjust itself to the
current context (and thus to the current domain)
without the need of explicit topic mixtures. There-
fore, we like to investigate techniques for general
context adaptation and their use in out-of-domain
translation.
There are two types of properties in natural lan-
guage and translation that we like to explore. First
of all, repetition is very common – much more
than standard stochastic language models would
predict. This is especially true for content words.
See, for instance, the sample of a medical docu-
ment shown in figure 1. Many content words are
repeated in close context. Hence, appropriate lan-
guage models should incorporate changing occur-
rence likelihoods to account for these very com-
mon repetitions. This is exactly what adaptive lan-
guage models try to do (Bellegarda, 2004).
</bodyText>
<page confidence="0.98119">
8
</page>
<note confidence="0.988517">
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8–15,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.822727285714286">
“They may also have episodes of depression . Abilify is
used to treat moderate to severe manic episodes and to
prevent manic episodes in patients who have responded to
the medicine in the past. The solution for injection is used
for the rapid control of agitation or disturbed behaviour
when taking the medicine by mouth is not appropriate .
The medicine can only be obtained with a prescription .”
</bodyText>
<figureCaption confidence="0.9873075">
Figure 1: A short example from a document from
the European Medicines Agency (EMEA)
</figureCaption>
<bodyText confidence="0.999933916666667">
Another known fact about natural language is con-
sistency which is also often ignored in statistical
models. A main problem in most NLP applica-
tions is ambiguity. However, ambiguity is largely
removed within specific domains and contexts in
which ambiguous items have a well-defined and
consistent meaning. This effect of “meaning con-
sistency” also known as the principle of “one sense
per discourse” has been applied in word sense
disambiguation with quite some success (Gale et
al., 1992). For machine translation this means
that adapting to the local domain and sticking to
consistent translation choices within a discourse
seems to be better than using a global static model
and context independent translations of sentences
in isolation. For an illustration, look at the exam-
ples in figure 2 taken from translated movie subti-
tles. Interesting is not only the consistent meaning
of “honey” within each discourse but also the con-
sistent choice among equivalent translations (syn-
onyms “¨alskling” och “gumman”). Here, the dis-
tinction between “honey” and “sweetheart” has
been transferred to Swedish using consistent trans-
lations.
</bodyText>
<subsectionHeader confidence="0.642243">
The 10 commandments Kerd ma lui
</subsectionHeader>
<bodyText confidence="0.999542133333333">
To some land flowing with Mari honey ...
milk and honey! Mari, gumman ...
Till ett land fullt av mj¨olk Sweetheart, where are
och honung. you going?
¨Alskling, var ska du?
...
Who was that, honey?
Vem var det, gumman?
I’ve never tasted honey.
Jag har aldrig smakat ho-
nung.
...
discourse phenomena. In the next section we dis-
cuss the cache-based models that we implemented
to address this challenge.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="method">
3 Cache-based Models
</sectionHeader>
<bodyText confidence="0.94914805">
The main idea behind cache-based language mod-
els (Kuhn and Mori, 1990) is to mix a large global
(static) language model with a small local (dy-
namic) model estimated from recent items in the
history of the input stream. It is common to use
simple linear interpolations and fixed cache sizes k
(100-5000 words) to achieve this: P(wn history) =
(1 − A)Pn−gram(wn history) + APcache(wn history)
Due to data sparseness one is usually restricted
to simple cache models. However, unigram mod-
els are often sufficient and smoothing is not nec-
essary due to the interpolation with the smoothed
background model. From the language model-
ing literature we know that caching is an effi-
cient way to reduce perplexity (usually leading to
modest improvements on in-domain data and large
improvements on out-of-domain data). Table 1
shows this effect yielding 53% reduction of per-
plexity on our out-of-domain data.
different settings for A
</bodyText>
<table confidence="0.9989721">
cache 0.05 0.1 0.2 0.3
0 376.1 376.1 376.1 376.1
50 270.7 259.2 256.4 264.9
100 261.1 246.6 239.2 243.3
500 252.2 233.1 219.1 217.0
1000 240.6 218.0 199.2 192.9
2000 234.6 209.6 187.9 179.1
5000 235.3 209.1 185.8 175.8
10000 237.6 210.7 186.6 176.1
20000 239.9 212.5 187.7 176.7
</table>
<tableCaption confidence="0.672636333333333">
Table 1: Perplexity of medical texts (EMEA) us-
ing a language model estimated on Europarl and a
unigram cache component
</tableCaption>
<bodyText confidence="0.9993208">
Even though a simple unigram cache is quite ef-
fective it now requires a careful optimization of its
size. In order to avoid the dependence on cache
size and to account for recency a decaying factor
can be introduced (Clarkson and Robinson, 1997):
</bodyText>
<figureCaption confidence="0.9736915">
Figure 2: Consistency in subtitle translations Pcache(wn wn−k..wn−1) N 1 n−1E I(wn = wi)e−«(n−i)
Z i=n−k
</figureCaption>
<bodyText confidence="0.997797111111111">
In summary: Repetition and consistency are very
important when modeling natural language and
translation. A proper translation engine should
move away from translating sentences in isolation
but should consider wider context to include these
Here, I(A) = 1 if A is true and 0 otherwise. Z
is a normalizing constant. Figure 3 illustrates the
effect of cache decay on our data yielding another
significant reduction in perplexity (even though
</bodyText>
<page confidence="0.952095">
9
</page>
<figure confidence="0.996107333333333">
perplexity
0 0.0005 0.001 0.0015 0.002 0.0025 0.003
decay ratio
</figure>
<figureCaption confidence="0.964576">
Figure 3: Out-of-domain perplexity using lan-
guage models with decaying cache.
</figureCaption>
<bodyText confidence="0.999781214285714">
the improvement is much less impressive than the
one obtained by introducing the cache).
The motivation of using these successful tech-
niques in SMT is obvious. Language models play
a crucial role in fluency ranking and a better fit
to real data (supporting the tendency of repetition)
should be preferred. This, of course, assumes cor-
rect translation decisions in the history in our SMT
setting which will almost never be the case. Fur-
thermore, simple cache models like the unigram
model may wrongly push forward certain expres-
sions without considering local context when us-
ing language models to discriminate between var-
ious translation candidates. Therefore, success-
fully applying these adaptive language models in
SMT is surprisingly difficult (Raab, 2007) espe-
cially due to the risk of adding noise (leading to
error propagation) and corrupting local dependen-
cies.
In SMT another type of adaptation can be ap-
plied: cache-based adaptation of the translation
model. Here, not only the repetition of content
words is supported but also the consistency of
translations as discussed earlier. This technique
has already been tried in the context of interactive
machine translation (Nepveu et al., 2004) in which
cache features are introduced to adapt both the lan-
guage model and the translation model. However,
in their model they require an automatic align-
ment of words in the user edited translation and the
source language input. In our experiments we in-
vestigate a close integration of the caching proce-
dure into the decoding process of fully automatic
translation. For this, we fill our cache with trans-
lation options used in the best (final) translation
hypothesis of previous sentences. In our imple-
mentation of the translation model cache we use
again a decaying factor in order to account for re-
cency. For known source language items (f,,, for
which translation options exist in the cache) the
following formula is used to compute the cache
translation score:
</bodyText>
<equation confidence="0.999208333333333">
�� i=1 I((en, fn) = (ei, fi)) * e−�i
φcache(en|fn) =
i=1 I(fn = fi)
</equation>
<bodyText confidence="0.996515333333333">
Unknown items receive a score of zero. This score
is then used as an additional feature in the standard
log-linear model of phrase-based SMT1.
</bodyText>
<sectionHeader confidence="0.999543" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999981142857143">
Our experiments are focused on the unsupervised
dynamic adaptation of language and translation
models to a new domain using the cache-based
mixture models as described above. We apply
these techniques to a standard task of translat-
ing French to English using a model trained on
the publicly available Europarl corpus (Koehn,
2005) using standard settings and tools such as the
Moses toolkit (Koehn et al., 2007), GIZA++ (Och
and Ney, 2003) and SRILM (Stolcke, 2002). The
log-linear model is then tuned as usual with mini-
mum error rate training (Och, 2003) on a separate
development set coming from the same domain
(Europarl). We modified SRILM to include a de-
caying cache model and implemented the phrase
translation cache within the Moses decoder. Fur-
thermore, we added the caching procedures and
other features for testing the adaptive approach.
Now we can simply switch the cache models on
or off using additional command-line arguments
when running Moses as usual.
</bodyText>
<subsectionHeader confidence="0.992962">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9999235">
For testing we chose to use documents from the
medical domain coming from the EMEA corpus
that is part of the freely available collection of
parallel corpora OPUS2 (Tiedemann, 2009). The
reason for selecting this domain is that these doc-
uments include very consistent instructions and
repetitive texts which ought to favor our caching
techniques. Furthermore, they are very different
</bodyText>
<footnote confidence="0.98635625">
1Logarithmic values are used in the actual implementation
which are floored to a low constant in case of zero φ scores.
2The OPUS corpus is available at this URL:
http://www.let.rug.nl/tiedeman/OPUS/.
</footnote>
<figure confidence="0.963516333333333">
184
182
180
178
176
174
172
170
168
cache size = 2000
cache size = 5000
cache size = 10000
</figure>
<page confidence="0.939415">
10
</page>
<bodyText confidence="0.982828266666667">
from the training data and, thus, domain adapta-
tion is very important for proper translations. We
randomly selected 102 pairs of documents with al-
together 5,478 sentences. Sentences have an aver-
age length of about 19 tokens with a lot of varia-
tion among them. Documents are compiled from
the European Public Assessment Reports (EPAR)
which reflect scientific conclusions at the end of a
centralized evaluation procedure for medical prod-
ucts. They include a lot of domain-specific ter-
minology, short facts, lists and tables but also de-
tailed textual descriptions of medicines and their
use. The overall lowercased type/token ratio in the
English part of our test collection is about 0.045
which indicates quite substantial repetitions in the
text. This ratio is, however, much higher for indi-
vidual documents.
In the experiment each document is processed
individually in order to apply appropriate dis-
course breaks. The baseline score for applying a
standard phrase-based SMT model yields an aver-
age score of 28.67 BLEU per document (28.60 per
sentence) which is quite reasonable for an out-of-
domain test. Intuitively, the baseline performance
should be crucial for the adaptation. As discussed
earlier the cache-based approach assumes correct
history and better baseline performance should in-
crease the chance of adding appropriate items to
the cache.
BLEU score difference
</bodyText>
<subsectionHeader confidence="0.993874">
4.2 Applying the LM Cache
</subsectionHeader>
<bodyText confidence="0.99999055">
In our first experiment we applied a decaying uni-
gram cache in the language model. We performed
a simple linear search on a separate development
set for optimizing the interpolation weight which
gave as a value of A = 0.001. The size of the cache
was set to 10,000 and the decay factor was set to
α = 0.0005 (according to our findings in figure
3). The results on our test data compared to the
standard model are illustrated (with white boxes)
in figure 4.
There is quite some variation in the effect of the
cache LM on our test documents. The translations
of most EMEA documents could be improved ac-
cording to BLEU scores, some of them substan-
tially, whereas others degraded slightly. Note that
the documents differ in size and some of them are
very short which makes it a bit difficult to interpret
and directly compare these scores. On average the
BLEU score is improved by 0.43 points per doc-
ument and 0.39 points per sentence. This might
</bodyText>
<figure confidence="0.99735475">
5
4
3
2
1
0
-1
-2
</figure>
<figureCaption confidence="0.60273275">
Figure 4: The differences in BLEU between a
standard model and models with cache for 102
EMEA documents (sorted by overall BLEU score
gain – see figure 5)
</figureCaption>
<bodyText confidence="0.998813333333333">
be not as impressive as we were hoping for af-
ter the tremendous perplexity reduction presented
earlier. However, considering the simplicity of the
approach that does not require any additional re-
sources nor training it is still a valuable achieve-
ment.
</bodyText>
<subsectionHeader confidence="0.998731">
4.3 Applying the TM Cache
</subsectionHeader>
<bodyText confidence="0.978325037037037">
In the next experiment we tested the effect of the
TM cache on translation quality. Using our hy-
pothesis of translation consistency we expected
another gain on our test set. In order to reduce
problems of noise we added two additional con-
straints: We only cache phrases that contain at
least one word longer than 4 characters (a simplis-
tic attempt to focus on content words rather than
function words) and we only cache translation op-
tions for which the transition costs (of adding this
option to the current hypothesis) in the global de-
coding model is larger than a given threshold (an
attempt to use some notion of confidence for the
current phrase pair; in our experiments we used a
log score of -4). Using this setup and applying the
phrase cache in decoding we obtained the results
illustrated with filled boxes in the figure 4 above.
Again, we can observe a varied outcome but
mostly improvements. The impact of the phrase
translation cache (with a size of 5,000 items) is not
as strong as for the language model cache which
might be due to the rather conservative settings
(A = 0.001, α = 0.001) and the fact that matching
phrase pairs are less likely to appear than matching
target words. On average the gain is about 0.275
cache LM vs. standard LM
cache TM vs. standard TM
</bodyText>
<page confidence="0.978491">
11
</page>
<bodyText confidence="0.555225">
BLEU points per document (0.26 per sentence).
</bodyText>
<subsectionHeader confidence="0.991099">
4.4 Combining the Cache Models
</subsectionHeader>
<bodyText confidence="0.999871">
Finally, we applied both types of cache in one
common system using the same settings from the
individual runs. The differences to the baseline
model are shown in figure 5.
</bodyText>
<figure confidence="0.993965111111111">
5
cache models vs. standard models
4
BLEU score difference 3
2
1
0
-1
-2
</figure>
<figureCaption confidence="0.999298">
Figure 5: The BLEU score differences between a
</figureCaption>
<bodyText confidence="0.9143912">
standard model and a model with cache for both
TM and LM (sorted by BLEU score gain).
In most cases, applying the two types of cache
together has a positive effect on the final BLEU
score. Now, we see only a few documents with
a drop in translation performance. On average
the gain has increased to about 0.78 BLEU points
per document (0.74 per sentence) which is about
2.7% relative improvement compared to the base-
line (2.6% per sentence).
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9985276">
Our experiments seem to suggest that caching
could be a way to improve translation quality on
a new domain. However, the differences are small
and the assumption that previous translation hy-
potheses are good enough to be cached is risky.
One obvious question is if the approach is ro-
bust enough to be helpful in general. If that is
the the case we should also see positive effects
on in-domain data where a cache model could ad-
just to topical shifts within that domain. In order
to test this ability we ran an experiment with the
2006 test data from the workshop on statistical ma-
chine translation (Koehn and Monz, 2006) using
the same models and settings as above. This re-
sulted in the following scores (lowercased BLEU):
BLEUbaseli.e = 32.46 (65.0/38.3/25.4/17.6, BP=0.999)
BLEU..he = 31.91 (65.1/38.1/25.1/17.3, BP=0.991)
Clearly, the cache models failed on this test even
though the difference between the two runs is not
large. There is a slight improvement in unigram
matches (first value in brackets) but a drop on
larger n-gram scores and also a stronger brevity
penalty (BP). This could be an effect of the sim-
plicity of the LM cache (a simple unigram model)
which may improve the choice of individual lexi-
cal items but without respecting contextual depen-
dencies.
One difference is that the in-domain data was
translated in one step without clearing the cache at
topical shifts. EMEA documents were translated
one by one with empty caches at the beginning. It
is now the question if proper initialization is es-
sential and if there is a correlation between docu-
ment length and the effect of caching. How much
data is actually needed to take advantage of cached
items and is there a point where a positive effect
degrades because of topical shifts within the docu-
ment? Let us, therefore, have a look at the relation
between document length and BLEU score gain in
our test collection (figure 6).
</bodyText>
<figure confidence="0.9732695">
0 200 400 600 800 1000 1200 1400 1600 1800 2000
document length
</figure>
<figureCaption confidence="0.960141666666667">
Figure 6: Correlation between document lengths
(in number of tokens) and BLEU score gains with
caching.
</figureCaption>
<bodyText confidence="0.997716272727272">
Concluding from this figure there does not seem
to be any correlation. The length of the document
does not seem to influence the outcome. What
else could be the reason for the different behaviour
among our test documents? One possibility is the
quality of baseline translations assuming that bet-
ter performance increases the chance of caching
correct translation hypotheses. Figure 7 plots the
BLEU score gains in comparison with the baseline
scores.
Again, no immediate correlation can be seen.
</bodyText>
<figure confidence="0.9829241">
BLEU score difference 5
4
3
2
1
0
-1
-2
12
BLEU score difference
5
4
3
2
1
0
-1
-2
10 15 20 25 30 35 40 45
baseline BLEU
</figure>
<figureCaption confidence="0.993014">
Figure 7: Correlation between baseline BLEU
scores and BLEU score gains with caching
</figureCaption>
<bodyText confidence="0.9992725">
The baseline performance does not seem to give
any clues for a possible success of caching. This
comes as a surprise as our intuitions suggested that
good baseline performance should be essential for
the adaptive approach.
Another reason for their success should be the
amount of repetition (especially among content
words) in the documents to be translated. An in-
dication for this can be given by type/token ratios
assuming that documents with lower ratios contain
a larger amount of repetitive text. Figure 8 plots
the type/token ratios of all test documents in com-
parison with the BLEU score gains obtained with
caching.
</bodyText>
<figure confidence="0.9828663">
5
4
BLEU score difference 3
2
1
0
-1
-2
0.25 0.3 0.35 0.4 0.45 0.5 0.55
type/token ratio
</figure>
<figureCaption confidence="0.880673">
Figure 8: Correlation between type/token ratios
and BLEU score gains with caching
</figureCaption>
<bodyText confidence="0.931813928571429">
Once again there does not seem to be any obvi-
ous correlation. So far we could not identify any
particular property of documents that might help
to reliably predict the success of caching. The an-
swer is probably a combination of various factors.
Further experiments are needed to see the effect on
different data sets and document types.
Note that some results may also be an artifact
of the automatic evaluation metrics applied. Qual-
itative evaluations using manual inspection could
probably reveal important aspects of the caching
approach. However, tracing changes caused by
caching is rather difficult due to the interaction
with other factors in the global decoding process.
Some typical cases may still be identified. Fig-
ure 9 shows an example of a translation that has
been improved in the cached model by making the
translation more consistent (this is from a docu-
ment that actually got a lower BLEU score in the
end with caching).
baseline: report ( evaluation of european public epar )
vivanza
in the short epar public
this document is a summary of the european public to
evaluation report ( epar ) .
cache: report european public assessment ( epar )
vivanza
epar to sum up the public
</bodyText>
<figure confidence="0.663109">
this document is a summary of the european public as-
sessment report ( epar ) .
reference: european public assessment report ( epar )
vivanza
epar summary for the public
this document is a summary of the european public as-
sessment report ( epar ) .
</figure>
<figureCaption confidence="0.999806">
Figure 9: A translation improved by caching.
</figureCaption>
<bodyText confidence="0.938675">
Other improvements may not be recognized by au-
tomatic evaluation metrics and certain acceptable
differences may be penalized. Look, for instance,
at the examples in figure 10.
This is, of course, not a general claim that
cache-based translations are more effected by this
problem than, for example, the baseline system.
However, this could be a direction for further in-
vestigations to quantify these issues.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.998750333333333">
In this paper we presented adaptive language and
translation models that use an exponentially de-
caying cache. We applied these models to a do-
main adaptation task translating medical docu-
ments with a standard model trained on Europarl.
On average the dynamic adaptation approach led
to a gain of about 2.6% relative BLEU points per
sentence. The main advantage of this approach is
that it does not require any domain-specific train-
</bodyText>
<page confidence="0.996983">
13
</page>
<bodyText confidence="0.882372733333333">
baseline: the medication is issued on orders.
cache: the medication is issued on prescription-only .
reference: the medicine can only be obtained with a prescription.
baseline: benefix is a powder keg, and a solvent to dissolve the injection for.
cache: benefix consists of a powder and a solvent to dissolve the injection for.
reference: benefix is a powder and solvent that are mixed together for injection.
baseline: the principle of active benefix is the nonacog alfa ( ix coagulation factor of recombinant) which favours
the coagulation blood.
cache: the principle of benefix is the nonacog alfa ( ix coagulation factor of recombinant ) which favours the
coagulation blood.
reference: benefix contains the active ingredient nonacog alfa ( recombinant coagulation factor ix , which helps
blood to clot ) .
baseline: in any case , it is benefix used ?
cache: in which case it is benefix used ?
reference: what is benefix used for ?
</bodyText>
<tableCaption confidence="0.594985777777778">
baseline: benefix is used for the treatment and prevention of saignements among patients with haemophilia b ( a
disorder h´emorragique hereditary due to a deficiency in factor ix ) .
cache: benefix is used for the treatment and prevention of saignements among patients suffering haemophilia
b ( a disorder h´emorragique hereditary due to a lack factor in ix ) .
reference: benefix is used for the treatment and prevention of bleeding in patients with haemophilia b ( an inher-
ited bleeding disorder caused by lack of factor ix ) .
baseline: benefix can be used for adults and children over 6 years.
cache: benefix can be used for adults and children of more than 6 years
reference: benfix can be used in adults and children over the age of 6.
</tableCaption>
<figureCaption confidence="0.861229">
Figure 10: Examples translations with and without caching.
</figureCaption>
<bodyText confidence="0.99993475">
ing, tuning (assuming that interpolation weights
and other cache parameters can be fixed after some
initial experiments) nor the incorporation of any
other in-domain resources. Cache based adapta-
tion can directly be applied to any new domain
and similar gains should be possible. However, a
general conclusion cannot be drawn from our ini-
tial results presented in this paper. Further exper-
iments are required to verify these findings and to
explore the potentials of cache-based techniques.
The main obstacle is the invalid assumption that
initial translations are correct. The success of the
entire method crucially depends on this assump-
tion. Error propagation and the reinforcement of
wrong decisions is the largest risk. Therefore,
strategies to reduce noise in the cache are impor-
tant and can still be improved using better selec-
tion criteria. A possible strategy could be to iden-
tify simple cases in a first run that can be used
to reliably fill the cache and to use the full cache
model on the entire text in a second run. Another
idea for improvement is to attach weights to cache
entries according to the translation costs assigned
by the model. These weights could easily be incor-
porated into the cache scores returned for match-
ing items. In future, we would like to explore these
ideas and also possibilities to combine cache mod-
els with other types of adaptation techniques.
</bodyText>
<sectionHeader confidence="0.997501" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991848580645161">
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93–108.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In StatMT ’09: Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 182–189, Morristown, NJ,
USA. Association for Computational Linguistics.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 177–180,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
P.R. Clarkson and A. J. Robinson. 1997. Language
model adaptation using mixtures and an exponen-
tially decaying cache. In International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 799–802, Munich, Germany.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In StatMT ’08: Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 208–
215, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128–135, Prague, Czech Republic. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.984207">
14
</page>
<reference confidence="0.999919727272728">
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
’91: Proceedings of the workshop on Speech and
Natural Language, pages 233–237, Morristown, NJ,
USA. Association for Computational Linguistics.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In Proceedings of
the 10th Conference of the European Association for
Machine Translation (EAMT), pages 133–142, Bu-
dapest.
Philipp Koehn and Christof Monz, editors. 2006. Pro-
ceedings on the Workshop on Statistical Machine
Translation. Association for Computational Lin-
guistics, New York City, June.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224–227,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
ACL ’07: Proceedings of the 45th Annual Meet-
ing of the ACL on Interactive Poster and Demon-
stration Sessions, pages 177–180, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit (MT Summit X).
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12(6):570–583.
Laurent Nepveu, Lapalme, Guy, Langlais, Philippe,
and George Foster. 2004. Adaptive Language and
Translation Models for Interactive Machine Trans-
lation. In Proceedings of the 9th Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 190–197, Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ’03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160–
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Martin Raab. 2007. Language Modeling for Machine
Translation. VDM Verlag, Saarbr¨ucken, Germany.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on spoken language pro-
cessing (ICSLP 2002), pages 901–904, Denver, CO,
USA.
J¨org Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Recent Advances in Natural Language
Processing, volume V, pages 237–248. John Ben-
jamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Semi-supervised model adaptation
for statistical machine translation. Machine Trans-
lation, 21(2):77–94.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In COL-
ING ’04: Proceedings of the 20th international con-
ference on Computational Linguistics, page 411,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.997927">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.810579">
<title confidence="0.971261">Context Adaptation in Statistical Machine Translation Using Models with Exponentially Decaying Cache</title>
<author confidence="0.876114">J¨org</author>
<affiliation confidence="0.999914">Department of Linguistics and Uppsala University,</affiliation>
<email confidence="0.985771">jorg.tiedemann@lingfil.uu.se</email>
<abstract confidence="0.9975565">We report results from a domain adaptation task for statistical machine translation (SMT) using cache-based adaptive language and translation models. We apply an exponential decay factor and integrate the cache models in a standard phrasebased SMT decoder. Without the need for any domain-specific resources we obtain a 2.6% relative improvement on average in BLEU scores using our dynamic adaptation procedure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jerome R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>42--93</pages>
<contexts>
<context position="3763" citStr="Bellegarda, 2004" startWordPosition="579" endWordPosition="580">and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical document shown in figure 1. Many content words are repeated in close context. Hence, appropriate language models should incorporate changing occurrence likelihoods to account for these very common repetitions. This is exactly what adaptive language models try to do (Bellegarda, 2004). 8 Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8–15, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics “They may also have episodes of depression . Abilify is used to treat moderate to severe manic episodes and to prevent manic episodes in patients who have responded to the medicine in the past. The solution for injection is used for the rapid control of agitation or disturbed behaviour when taking the medicine by mouth is not appropriate . The medicine can only be obtained with a prescription .” Figure 1</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>Jerome R. Bellegarda. 2004. Statistical language model adaptation: review and perspectives. Speech Communication, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Bertoldi</author>
<author>Marcello Federico</author>
</authors>
<title>Domain adaptation for statistical machine translation with monolingual resources.</title>
<date>2009</date>
<booktitle>In StatMT ’09: Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>182--189</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1546" citStr="Bertoldi and Federico, 2009" startWordPosition="226" endWordPosition="229">ata. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of repetition and consistency in natural language text. Thereafter, we will briefly discuss the dynamic extensions to language and translation models applied in the experiments presented in the second last section followed b</context>
</contexts>
<marker>Bertoldi, Federico, 2009</marker>
<rawString>Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolingual resources. In StatMT ’09: Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 182–189, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Civera</author>
<author>Alfons Juan</author>
</authors>
<title>Domain adaptation in statistical machine translation with mixture modelling.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1516" citStr="Civera and Juan, 2007" startWordPosition="222" endWordPosition="225">erent to the training data. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of repetition and consistency in natural language text. Thereafter, we will briefly discuss the dynamic extensions to language and translation models applied in the experiments presented in the </context>
</contexts>
<marker>Civera, Juan, 2007</marker>
<rawString>Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P R Clarkson</author>
<author>A J Robinson</author>
</authors>
<title>Language model adaptation using mixtures and an exponentially decaying cache.</title>
<date>1997</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<pages>799--802</pages>
<location>Munich, Germany.</location>
<contexts>
<context position="7586" citStr="Clarkson and Robinson, 1997" startWordPosition="1207" endWordPosition="1210">0.2 0.3 0 376.1 376.1 376.1 376.1 50 270.7 259.2 256.4 264.9 100 261.1 246.6 239.2 243.3 500 252.2 233.1 219.1 217.0 1000 240.6 218.0 199.2 192.9 2000 234.6 209.6 187.9 179.1 5000 235.3 209.1 185.8 175.8 10000 237.6 210.7 186.6 176.1 20000 239.9 212.5 187.7 176.7 Table 1: Perplexity of medical texts (EMEA) using a language model estimated on Europarl and a unigram cache component Even though a simple unigram cache is quite effective it now requires a careful optimization of its size. In order to avoid the dependence on cache size and to account for recency a decaying factor can be introduced (Clarkson and Robinson, 1997): Figure 2: Consistency in subtitle translations Pcache(wn wn−k..wn−1) N 1 n−1E I(wn = wi)e−«(n−i) Z i=n−k In summary: Repetition and consistency are very important when modeling natural language and translation. A proper translation engine should move away from translating sentences in isolation but should consider wider context to include these Here, I(A) = 1 if A is true and 0 otherwise. Z is a normalizing constant. Figure 3 illustrates the effect of cache decay on our data yielding another significant reduction in perplexity (even though 9 perplexity 0 0.0005 0.001 0.0015 0.002 0.0025 0.00</context>
</contexts>
<marker>Clarkson, Robinson, 1997</marker>
<rawString>P.R. Clarkson and A. J. Robinson. 1997. Language model adaptation using mixtures and an exponentially decaying cache. In International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 799–802, Munich, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Dynamic model interpolation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>208--215</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2929" citStr="Finch and Sumita, 2008" startWordPosition="442" endWordPosition="445">es in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we like to investigate techniques for general context adaptation and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical document shown in figure 1. Many content words are</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Dynamic model interpolation for statistical machine translation. In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation, pages 208– 215, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2904" citStr="Foster and Kuhn, 2007" startWordPosition="438" endWordPosition="441">ply supervised techniques in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we like to investigate techniques for general context adaptation and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical document shown in figure </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In HLT ’91: Proceedings of the workshop on Speech and Natural Language,</booktitle>
<pages>233--237</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4928" citStr="Gale et al., 1992" startWordPosition="765" endWordPosition="768"> can only be obtained with a prescription .” Figure 1: A short example from a document from the European Medicines Agency (EMEA) Another known fact about natural language is consistency which is also often ignored in statistical models. A main problem in most NLP applications is ambiguity. However, ambiguity is largely removed within specific domains and contexts in which ambiguous items have a well-defined and consistent meaning. This effect of “meaning consistency” also known as the principle of “one sense per discourse” has been applied in word sense disambiguation with quite some success (Gale et al., 1992). For machine translation this means that adapting to the local domain and sticking to consistent translation choices within a discourse seems to be better than using a global static model and context independent translations of sentences in isolation. For an illustration, look at the examples in figure 2 taken from translated movie subtitles. Interesting is not only the consistent meaning of “honey” within each discourse but also the consistent choice among equivalent translations (synonyms “¨alskling” och “gumman”). Here, the distinction between “honey” and “sweetheart” has been transferred </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. One sense per discourse. In HLT ’91: Proceedings of the workshop on Speech and Natural Language, pages 233–237, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>133--142</pages>
<location>Budapest.</location>
<contexts>
<context position="2881" citStr="Hildebrand et al., 2005" startWordPosition="434" endWordPosition="437">mpirical systems is to apply supervised techniques in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we like to investigate techniques for general context adaptation and their use in out-of-domain translation. There are two types of properties in natural language and translation that we like to explore. First of all, repetition is very common – much more than standard stochastic language models would predict. This is especially true for content words. See, for instance, the sample of a medical do</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT), pages 133–142, Budapest.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>Proceedings on the Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<editor>Philipp Koehn and Christof Monz, editors.</editor>
<location>New York City,</location>
<marker>2006</marker>
<rawString>Philipp Koehn and Christof Monz, editors. 2006. Proceedings on the Workshop on Statistical Machine Translation. Association for Computational Linguistics, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in domain adaptation for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>224--227</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2483" citStr="Koehn and Schroeder, 2007" startWordPosition="374" endWordPosition="377">motivate the chosen approach by reviewing the general phenomenon of repetition and consistency in natural language text. Thereafter, we will briefly discuss the dynamic extensions to language and translation models applied in the experiments presented in the second last section followed by some final conclusions. 2 Motivation Domain adaptation can be tackled in various ways. An obvious choice for empirical systems is to apply supervised techniques in case domain-specific training data is available. It has been shown that small(er) amounts of in-domain data are sufficient for such an approach (Koehn and Schroeder, 2007). However, this is not really a useful alternative for truly open-domain systems, which will be confronted with changing domains all the time including many new, previously unknown ones among them. There are also some interesting approaches to dynamic domain adaptation mainly using flexible mixture models or techniques for the automatic selection of appropriate resources (Hildebrand et al., 2005; Foster and Kuhn, 2007; Finch and Sumita, 2008). Ideally, a system would adjust itself to the current context (and thus to the current domain) without the need of explicit topic mixtures. Therefore, we</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10881" citStr="Koehn et al., 2007" startWordPosition="1738" endWordPosition="1741"> = (ei, fi)) * e−�i φcache(en|fn) = i=1 I(fn = fi) Unknown items receive a score of zero. This score is then used as an additional feature in the standard log-linear model of phrase-based SMT1. 4 Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chos</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In ACL ’07: Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit (MT Summit X).</booktitle>
<contexts>
<context position="10800" citStr="Koehn, 2005" startWordPosition="1726" endWordPosition="1727"> formula is used to compute the cache translation score: �� i=1 I((en, fn) = (ei, fi)) * e−�i φcache(en|fn) = i=1 I(fn = fi) Unknown items receive a score of zero. This score is then used as an additional feature in the standard log-linear model of phrase-based SMT1. 4 Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the 10th Machine Translation Summit (MT Summit X).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Kuhn</author>
<author>Renato De Mori</author>
</authors>
<title>A cachebased natural language model for speech recognition.</title>
<date>1990</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>12</volume>
<issue>6</issue>
<marker>Kuhn, De Mori, 1990</marker>
<rawString>Roland Kuhn and Renato De Mori. 1990. A cachebased natural language model for speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(6):570–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Nepveu</author>
<author>Guy Lapalme</author>
<author>Philippe Langlais</author>
<author>George Foster</author>
</authors>
<title>Adaptive Language and Translation Models for Interactive Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>190--197</pages>
<location>Barcelona,</location>
<contexts>
<context position="9478" citStr="Nepveu et al., 2004" startWordPosition="1503" endWordPosition="1506">nguage models to discriminate between various translation candidates. Therefore, successfully applying these adaptive language models in SMT is surprisingly difficult (Raab, 2007) especially due to the risk of adding noise (leading to error propagation) and corrupting local dependencies. In SMT another type of adaptation can be applied: cache-based adaptation of the translation model. Here, not only the repetition of content words is supported but also the consistency of translations as discussed earlier. This technique has already been tried in the context of interactive machine translation (Nepveu et al., 2004) in which cache features are introduced to adapt both the language model and the translation model. However, in their model they require an automatic alignment of words in the user edited translation and the source language input. In our experiments we investigate a close integration of the caching procedure into the decoding process of fully automatic translation. For this, we fill our cache with translation options used in the best (final) translation hypothesis of previous sentences. In our implementation of the translation model cache we use again a decaying factor in order to account for </context>
</contexts>
<marker>Nepveu, Lapalme, Langlais, Foster, 2004</marker>
<rawString>Laurent Nepveu, Lapalme, Guy, Langlais, Philippe, and George Foster. 2004. Adaptive Language and Translation Models for Interactive Machine Translation. In Proceedings of the 9th Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 190–197, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10909" citStr="Och and Ney, 2003" startWordPosition="1743" endWordPosition="1746">|fn) = i=1 I(fn = fi) Unknown items receive a score of zero. This score is then used as an additional feature in the standard log-linear model of phrase-based SMT1. 4 Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chose to use documents from the </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11025" citStr="Och, 2003" startWordPosition="1765" endWordPosition="1766">d log-linear model of phrase-based SMT1. 4 Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chose to use documents from the medical domain coming from the EMEA corpus that is part of the freely available collection of parallel corpora OPUS2</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL ’03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160– 167, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Raab</author>
</authors>
<title>Language Modeling for Machine Translation.</title>
<date>2007</date>
<publisher>VDM Verlag,</publisher>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="9037" citStr="Raab, 2007" startWordPosition="1435" endWordPosition="1436">is obvious. Language models play a crucial role in fluency ranking and a better fit to real data (supporting the tendency of repetition) should be preferred. This, of course, assumes correct translation decisions in the history in our SMT setting which will almost never be the case. Furthermore, simple cache models like the unigram model may wrongly push forward certain expressions without considering local context when using language models to discriminate between various translation candidates. Therefore, successfully applying these adaptive language models in SMT is surprisingly difficult (Raab, 2007) especially due to the risk of adding noise (leading to error propagation) and corrupting local dependencies. In SMT another type of adaptation can be applied: cache-based adaptation of the translation model. Here, not only the repetition of content words is supported but also the consistency of translations as discussed earlier. This technique has already been tried in the context of interactive machine translation (Nepveu et al., 2004) in which cache features are introduced to adapt both the language model and the translation model. However, in their model they require an automatic alignment</context>
</contexts>
<marker>Raab, 2007</marker>
<rawString>Martin Raab. 2007. Language Modeling for Machine Translation. VDM Verlag, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the 7th international conference on spoken language processing (ICSLP</booktitle>
<pages>901--904</pages>
<location>Denver, CO, USA.</location>
<contexts>
<context position="10935" citStr="Stolcke, 2002" startWordPosition="1749" endWordPosition="1750">items receive a score of zero. This score is then used as an additional feature in the standard log-linear model of phrase-based SMT1. 4 Experiments Our experiments are focused on the unsupervised dynamic adaptation of language and translation models to a new domain using the cache-based mixture models as described above. We apply these techniques to a standard task of translating French to English using a model trained on the publicly available Europarl corpus (Koehn, 2005) using standard settings and tools such as the Moses toolkit (Koehn et al., 2007), GIZA++ (Och and Ney, 2003) and SRILM (Stolcke, 2002). The log-linear model is then tuned as usual with minimum error rate training (Och, 2003) on a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chose to use documents from the medical domain coming from</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the 7th international conference on spoken language processing (ICSLP 2002), pages 901–904, Denver, CO, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing,</booktitle>
<volume>volume V,</volume>
<pages>237--248</pages>
<publisher>John Benjamins, Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="11643" citStr="Tiedemann, 2009" startWordPosition="1864" endWordPosition="1865">n a separate development set coming from the same domain (Europarl). We modified SRILM to include a decaying cache model and implemented the phrase translation cache within the Moses decoder. Furthermore, we added the caching procedures and other features for testing the adaptive approach. Now we can simply switch the cache models on or off using additional command-line arguments when running Moses as usual. 4.1 Experimental Setup For testing we chose to use documents from the medical domain coming from the EMEA corpus that is part of the freely available collection of parallel corpora OPUS2 (Tiedemann, 2009). The reason for selecting this domain is that these documents include very consistent instructions and repetitive texts which ought to favor our caching techniques. Furthermore, they are very different 1Logarithmic values are used in the actual implementation which are floored to a low constant in case of zero φ scores. 2The OPUS corpus is available at this URL: http://www.let.rug.nl/tiedeman/OPUS/. 184 182 180 178 176 174 172 170 168 cache size = 2000 cache size = 5000 cache size = 10000 10 from the training data and, thus, domain adaptation is very important for proper translations. We rand</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>J¨org Tiedemann. 2009. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Gholamreza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Semi-supervised model adaptation for statistical machine translation.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1493" citStr="Ueffing et al., 2007" startWordPosition="218" endWordPosition="221">ata from a domain different to the training data. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of repetition and consistency in natural language text. Thereafter, we will briefly discuss the dynamic extensions to language and translation models applied in the experi</context>
</contexts>
<marker>Ueffing, Haffari, Sarkar, 2007</marker>
<rawString>Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar. 2007. Semi-supervised model adaptation for statistical machine translation. Machine Translation, 21(2):77–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
</authors>
<title>Language model adaptation for statistical machine translation with structured query models.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1471" citStr="Zhao et al., 2004" startWordPosition="214" endWordPosition="217">e when testing on data from a domain different to the training data. Statistical machine translation is no exception. Despite its popularity, standard SMT approaches fail to provide a framework for general application across domains unless appropriate training data is available and used in parameter estimation and tuning. The main problem is the general assumption of independent and identically distributed (i.i.d.) variables in machine learning approaches applied in the estimation of static global models. Recently, there has been quite some attention to the problem of domain switching in SMT (Zhao et al., 2004; Ueffing et al., 2007; Civera and Juan, 2007; Bertoldi and Federico, 2009) but ground breaking success is still missing. In this paper we report our findings in dynamic model adaptation using cache-based techniques when applying a standard model to the task of translating documents from a very different domain. The remaining part of the paper is organized as follows: First, we will motivate the chosen approach by reviewing the general phenomenon of repetition and consistency in natural language text. Thereafter, we will briefly discuss the dynamic extensions to language and translation models</context>
</contexts>
<marker>Zhao, Eck, Vogel, 2004</marker>
<rawString>Bing Zhao, Matthias Eck, and Stephan Vogel. 2004. Language model adaptation for statistical machine translation with structured query models. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 411, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>