<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.950159">
SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)
</title>
<author confidence="0.99646">
Wei Xu and Chris Callison-Burch William B. Dolan
</author>
<affiliation confidence="0.998197">
University of Pennsylvania Microsoft Research
</affiliation>
<address confidence="0.744317">
Philadelphia, PA, USA Redmond, WA, USA
</address>
<email confidence="0.998351">
xwe, ccb@cis.upenn.edu billdol@microsoft.com
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999549692307692">
In this shared task, we present evaluations
on two related tasks Paraphrase Identification
(PI) and Semantic Textual Similarity (SS) sys-
tems for the Twitter data. Given a pair of
sentences, participants are asked to produce
a binary yes/no judgement or a graded score
to measure their semantic equivalence. The
task features a newly constructed Twitter Para-
phrase Corpus that contains 18,762 sentence
pairs. A total of 19 teams participated, sub-
mitting 36 runs to the PI task and 26 runs to
the SS task. The evaluation shows encourag-
ing results and open challenges for future re-
search. The best systems scored a F1-measure
of 0.674 for the PI task and a Pearson corre-
lation of 0.619 for the SS task respectively,
comparing to a strong baseline using logis-
tic regression model of 0.589 F1 and 0.511
Pearson; while the best SS systems can of-
ten reach &gt;0.80 Pearson on well-formed text.
This shared task also provides insights into the
relation between the PI and SS tasks and sug-
gests the importance to bringing these two re-
search areas together. We make all the data,
baseline systems and evaluation scripts pub-
licly available.1
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997004">
The ability to identify paraphrases, i.e. alternative
expressions of the same (or similar) meaning, and
the degree of their semantic similarity has proven
useful for a wide variety of natural language pro-
cessing applications (Madnani and Dorr, 2010). It
</bodyText>
<footnote confidence="0.9887625">
1http://www.cis.upenn.edu/˜xwe/
semeval2015pit/
</footnote>
<bodyText confidence="0.999655866666667">
is particularly useful to overcome the challenge of
high redundancy in Twitter and the sparsity inherent
in their short texts (e.g. oscar nom’d doc ↔ Oscar-
nominated documentary; some1 shot a cop ↔ some-
one shot a police). Emerging research shows para-
phrasing techniques applied to Twitter data can im-
prove tasks like first story detection (Petrovi´c et al.,
2012), information retrieval (Zanzotto et al., 2011)
and text normalization (Xu et al., 2013; Wang et al.,
2013).
Previously, many researchers have investigated
ways of automatically detecting paraphrases on
more formal texts, like newswire text. The ACL
Wiki2 gives an excellent summary of the state-of-
the-art paraphrase identification techniques. These
can be categorized into supervised methods (Qiu
et al., 2006; Wan et al., 2006; Das and Smith, 2009;
Socher et al., 2011; Blacoe and Lapata, 2012; Mad-
nani et al., 2012; Ji and Eisenstein, 2013) and unsu-
pervised methods (Mihalcea et al., 2006; Rus et al.,
2008; Fernando and Stevenson, 2008; Islam and
Inkpen, 2007; Hassan and Mihalcea, 2011). A few
recent studies have highlighted the potential and
importance of developing paraphrase identification
(Zanzotto et al., 2011; Xu et al., 2013) and semantic
similarity techniques (Guo and Diab, 2012) specif-
ically for tweets. They also indicated that the very
informal language, especially the high degree of lex-
ical variation, used in social media has posed serious
challenges to both tasks.
</bodyText>
<footnote confidence="0.950043333333333">
2http://aclweb.org/aclwiki/index.php?
title=Paraphrase_Identification_(State_of_
the_art)
</footnote>
<page confidence="0.518608">
1
</page>
<note confidence="0.976883333333333">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 1–11,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
Paraphrase? Sentence 1 Sentence 2
yes Ezekiel Ansah wearing 3D glasses wout Wait Ezekiel ansah is wearing 3d movie
the lens glasses with the lenses knocked out
yes Marriage equality law passed in Rhode Congrats to Rhode Island becoming the
Island 10th state to enact marriage equality
yes Aaaaaaaaand stephen curry is on fire What a incredible performance from
Stephen Curry
no Finally saw the Ciara body party video ciara s Body Party video is on point
no Now lazy to watch Manchester united vs Early lead for Arsenal against Manch-
arsenal ester United
debatable That s the new Ciroc flavor Need a little taste of that new Ciroc
debatable sarah Palin at the IndyMia game Sarah Palin is at the game are you
pumped
</note>
<tableCaption confidence="0.999675">
Table 1: Representative examples from PIT-2015 Twitter Paraphrase Corpus
</tableCaption>
<table confidence="0.99942275">
# Unique Sent # Sent Pair # Paraphrase # Non-Paraphrase # Debatable
Train 13231 13063 3996 (30.6%) 7534 (57.7%) 1533 (11.7%)
Dev 4772 4727 1470 (31.1%) 2672 (56.5%) 585 (12.4%)
Test 1295 972 175 (18.0%) 663 (68.2%) 134 (13.8%)
</table>
<tableCaption confidence="0.9819795">
Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from
annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection.
</tableCaption>
<bodyText confidence="0.999601782608696">
The SemEval-2015 shared task on Paraphrase and
Semantic Similarity In Twitter (PIT) uses a training
and development set of 17,790 sentence pairs and a
test set of 972 sentence pairs with paraphrase anno-
tations (see examples in Table 1) that is the same as
the Twitter Paraphrase Corpus we developed earlier
in (Xu, 2014) and (Xu et al., 2014). This PIT-2015
paraphrase dataset is distinct from the data used in
previous studies in many aspects: (i) it contains sen-
tences that are opinionated and colloquial, represent-
ing realistic informal language usage; (ii) it con-
tains paraphrases that are lexically diverse; and (iii)
it contains sentences that are lexically similar but se-
mantically dissimilar. It raises many interesting re-
search questions and could lead to a better under-
standing of our daily used language and how seman-
tics can be captured in such language. We believe
that such a common testbed will facilitate docking
of the different approaches for purposes of compari-
son, lead to a better understanding of how semantics
are conveyed in natural language, and help advance
other NLP techniques for noisy user-generated text
in the long run.
</bodyText>
<sectionHeader confidence="0.761951" genericHeader="introduction">
2 Task Description and Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.9999775">
The task has two sentence-level sub-tasks: a para-
phrase identification task and an optional semantic
textual similarity task. The two sub-tasks share the
same data but differ in annotation and evaluation.
</bodyText>
<subsectionHeader confidence="0.787092">
Task A – Paraphrase Identification (PI)
</subsectionHeader>
<bodyText confidence="0.998162666666667">
Given two sentences, determine whether they
express the same or very similar meaning. Fol-
lowing the literature on paraphrase identifica-
tion, we evaluate system performance by the F-
1 score (harmonic mean of precision and recall)
against human judgements.
</bodyText>
<subsectionHeader confidence="0.760408">
Task B – Semantic Textual Similarity (SS)
</subsectionHeader>
<bodyText confidence="0.999852375">
Given two sentences, determine a numerical
score between 0 (no relation) and 1 (semantic
equivalence) to indicate their semantic similar-
ity. Following the literature, the system outputs
are compared by Pearson correlation with hu-
man scores. We also compute the maximum
F-1 score over the precision-recall curve as an
additional data point.
</bodyText>
<page confidence="0.996327">
2
</page>
<sectionHeader confidence="0.997267" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999836954545455">
In this shared task, we use the Twitter Paraphrase
Corpus that we first presented in (Xu, 2014) and (Xu
et al., 2014). Table 2 shows the basic statistics of the
corpus. The sentences are preprocessed with tok-
enization,3 POS and named entity tags.4 The train-
ing and development set consists of 17,790 sentence
pairs posted between April 24th and May 3rd, 2013
from 500+ trending topics featured on Twitter (ex-
cluding hashtags). The training and development set
is a random split. Each sentence pair is annotated by
5 different crowdsourcing workers. For the test set,
we obtain both crowdsourced and expert labels on
972 sentence pairs from 20 randomly sampled Twit-
ter trending topics between May 13th and June 10th,
2013. We use expert labels in this SemEval eval-
uation. Our dataset is more realistic and balanced,
containing about 70% non-paraphrases vs. the 34%
non-paraphrases in the benchmark Microsoft Para-
phrase Corpus derived from news articles by Dolan
et al. (2004). As noted in (Das and Smith, 2009), the
lack of natural non-paraphrases in the MSR corpus
creates bias towards certain models.
</bodyText>
<sectionHeader confidence="0.994398" genericHeader="method">
4 Annotation
</sectionHeader>
<bodyText confidence="0.9997375">
In this section, we describe our data collection and
annotation methodology. Since Twitter users are
free to talk about anything regarding any topic, a
random pair of sentences about the same topic has
a low chance of expressing the same meaning (em-
pirically, this is less than 8%). This causes two prob-
lems: a) it is expensive to obtain paraphrases via
manual annotation; b) non-expert annotators tend to
loosen the criteria and are more likely to make false
positive errors. To address these challenges, we de-
sign a simple annotation task and introduce two se-
lection mechanisms to select sentences which are
more likely to be paraphrases, while preserving di-
versity and representativeness.
</bodyText>
<footnote confidence="0.9952752">
3The tokenizer was developed by O’Connor et al. (2010):
https://github.com/brendano/tweetmotif
4The POS tagger was developed by Derczynski et al. (2013)
and the NER tagger was developed by Ritter et al. (2011):
https://github.com/aritter/twitter_nlp
</footnote>
<figure confidence="0.640823333333333">
expert=5
expert=4
expert=3
expert=2
expert=1
expert=0
</figure>
<figureCaption confidence="0.783638666666667">
Figure 1: A heat-map showing overlap between ex-
pert and crowdsourcing annotation. The intensity along
the diagonal indicates good reliability of crowdsourcing
</figureCaption>
<tableCaption confidence="0.745226333333333">
workers for this particular task; and the shift above the di-
agonal reflects the difference between the two annotation
schemas. For crowdsourcing (turk), the numbers indicate
how many annotators out of 5 picked the sentence pair as
paraphrases; 0,1 are considered non-paraphrases; 3,4,5
are paraphrases. For expert annotation, all 0,1,2 are non-
paraphrases; 4,5 are paraphrases. Medium-scored cases
(2 for crowdsourcing; 3 for expert annotation) are dis-
carded in the system evaluation of the PI sub-task.
</tableCaption>
<subsectionHeader confidence="0.999073">
4.1 Raw Data from Twitter
</subsectionHeader>
<bodyText confidence="0.999984142857143">
We crawl Twitter’s trending topics and their associ-
ated tweets using public APIs.5 According to Twit-
ter, trends are determined by an algorithm which
identifies topics that are immediately popular, rather
than those that have been popular for longer periods
of time or which trend on a daily basis. We tokenize,
remove emoticons6 and split tweet into sentences.
</bodyText>
<subsectionHeader confidence="0.990549">
4.2 Task Design on Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.999826777777778">
We show the annotator an original sentence, then
ask them to pick sentences with the same mean-
ing from 10 candidate sentences. The original and
candidate sentences are randomly sampled from the
same topic. For each such 1 vs. 10 question, we ob-
tain binary judgements from 5 different annotators,
paying each annotator $0.02 per question. On aver-
age, each question takes one annotator about 30 ∼
45 seconds to answer.
</bodyText>
<footnote confidence="0.998811">
5More information about Twitter’s APIs: https://dev.
twitter.com/docs/api/1.1/overview
6We use the toolkit developed by O’Connor et al. (2010):
https://github.com/brendano/tweetmotif
</footnote>
<page confidence="0.98972">
3
</page>
<figure confidence="0.998912291666667">
U.S.
Facebook
Dwight Howard
GWB
filtered
random
Netflix
Ronaldo
Dortmund
Momma Dee
Morning
Huck
Klay
Milwaukee
Harvick
Jeff Green
Ryu
The Clippers
Candice
Robert Woods
Amber
Reggie Miller
0.0 0.2 0.4 0.6 0.8
Percentage of Positive Judgements
</figure>
<figureCaption confidence="0.9989395">
Figure 2: The proportion of paraphrases (percentage of positive votes from annotators) vary greatly across different
topics. Automatic filtering in Section 4.4 roughly doubles the paraphrase yield.
</figureCaption>
<bodyText confidence="0.565502">
Trending Topics
</bodyText>
<subsectionHeader confidence="0.999094">
4.3 Annotation Quality
</subsectionHeader>
<bodyText confidence="0.998638444444444">
We remove problematic annotators by checking
their Cohen’s Kappa agreement (Artstein and Poe-
sio, 2008) with other annotators. We also compute
inter-annotator agreement with an expert annotator
on the test dataset of 972 sentence pairs. In the ex-
pert annotation, we adopt a 5-point Likert scale to
measure the degree of semantic similarity between
sentences, which is defined by Agirre et al. (2012)
as follows:
</bodyText>
<listItem confidence="0.8136818">
5: Completely equivalent, as they mean the same
thing;
4: Mostly equivalent, but some unimportant details
differ;
3: Roughly equivalent, but some important informa-
</listItem>
<bodyText confidence="0.89953025">
tion differs/missing.
2: Not equivalent, but share some details;
1: Not equivalent, but are on the same topic;
0: On different topics.
Although the two scales of expert and crowd-
sourcing annotation are defined differently, their
Pearson correlation coefficient reaches 0.735 (two-
tailed significance 0.001). Figure 1 shows a heat-
map representing the detailed overlap between the
two annotations. It suggests that the graded simi-
larity annotation task could be reduced to a binary
choice in a crowdsourcing setup. As for the binary
paraphrase judgements, the integrated judgement of
five crowdsourcing workers achieve a F1-score of
0.823, precision of 0.752 and recall of 0.908 against
expert annotations.
</bodyText>
<subsectionHeader confidence="0.9992125">
4.4 Automatic Summarization Inspired
Sentence Filtering
</subsectionHeader>
<bodyText confidence="0.999997642857143">
We filter the sentences within each topic to se-
lect more probable paraphrases for annotation. Our
method is inspired by a typical problem in extractive
summarization, that the salient sentences are likely
redundant (paraphrases) and need to be removed
in the output summaries. We employ the scoring
method used in SumBasic (Nenkova and Vander-
wende, 2005; Vanderwende et al., 2007), a simple
but powerful summarization system, to find salient
sentences. For each topic, we compute the probabil-
ity of each word P(wi) by simply dividing its fre-
quency by the total number of all words in all sen-
tences. Each sentence s is scored as the average of
the probabilities of the words in it, i.e.
</bodyText>
<equation confidence="0.9869815">
P(wi) (1)
|{wi|wi E s}|
</equation>
<bodyText confidence="0.999945833333333">
We then rank the sentences and pick the original
sentence randomly from top 10% salient sentences
and candidate sentences from top 50% to present to
the annotators.
In a trial experiment of 20 topics, the filtering
technique double the yield of paraphrases from 152
</bodyText>
<equation confidence="0.9379135">
�Salience(s) =
wi∈s
</equation>
<page confidence="0.925538">
4
</page>
<bodyText confidence="0.99995875">
to 329 out of 2000 sentence pairs over naive ran-
dom sampling (Figure 2 and Figure 3). We also use
PINC (Chen and Dolan, 2011) to measure the qual-
ity of paraphrases collected (Figure 4). PINC was
designed to measure n-gram dissimilarity between
two sentences, and in essence it is the inverse of
BLEU. In general, the cases with high PINC scores
include more complex and interesting rephrasings.
</bodyText>
<sectionHeader confidence="0.4795465" genericHeader="method">
4.5 Topic Selection using Multi-Armed Bandits
(MAB) Algorithm
</sectionHeader>
<bodyText confidence="0.9882097">
Another approach to increasing paraphrase yield is
to choose more appropriate topics. This is partic-
ularly important because the number of paraphrases
varies greatly from topic to topic and thus the chance
to encounter paraphrases during annotation (Fig-
ure 2). We treat this topic selection problem as a
variation of the Multi-Armed Bandit (MAB) prob-
lem (Robbins, 1985) and adapt a greedy algorithm,
the bounded c-first algorithm, of Tran-Thanh et al.
(2012) to accelerate our corpus construction.
Our strategy consists of two phases. In the first
exploration phase, we dedicate a fraction of the to-
tal budget, c, to explore randomly chosen arms of
each slot machine (trending topic on Twitter), each
m times. In the second exploitation phase, we sort
all topics according to their estimated proportion
of paraphrases, and sequentially annotate �(1−�)s
l−m �
arms that have the highest estimated reward until
reaching the maximum l = 10 annotations for any
topic to insure data diversity.
We tune the parameters m to be 1 and c to be be-
tween 0.35 ∼ 0.55 through simulation experiments,
by artificially duplicating a small amount of real an-
notation data. We then apply this MAB algorithm
in the real-world. We explore 500 random topics
and then exploited 100 of them. The yield of para-
phrases rises to 688 out of 2000 sentence pairs by
using MAB and sentence filtering, a 4-fold increase
compared to only using random selection (Figure 3).
</bodyText>
<sectionHeader confidence="0.999665" genericHeader="method">
5 Baselines
</sectionHeader>
<subsectionHeader confidence="0.994291">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.9519075">
ber between [0, 1] for each test sentence pair as
semantic similarity score, and uses 0.5 as cutoff
for binary paraphrase identification output.
Logistic Regression:
This is a supervised logistic regression (LR)
baseline used by Das and Smith (2009). It uses
simple n-gram (also in stemmed form) overlap-
ping features but shows very competitive per-
formance on the MSR news paraphrase corpus.
It uses 0.5 as cutoff to create binary outputs for
the paraphrase identification task.
Weighted Matrix Factorization (WTMF):7
The third baseline is a state-of-the-art unsu-
pervised method developed by Guo and Diab
</bodyText>
<figureCaption confidence="0.754478333333333">
(2012). It is specially developed for short sen-
tences by modeling the semantic space of both
words that are present in and absent from the
sentences (Guo and Diab, 2012). The model
was learned from WordNet (Fellbaum, 2010),
OntoNotes (Hovy et al., 2006), Wiktionary, the
Brown corpus (Francis and Kucera, 1979). It
uses 0.5 as cutoff in the binary paraphrase iden-
tification task.
</figureCaption>
<sectionHeader confidence="0.575209" genericHeader="method">
6 Systems and Results
</sectionHeader>
<bodyText confidence="0.811546">
A total of 18 teams participated in the PI task (re-
quired), 13 of which also submitted to the SS task
(optional). Every team submitted 2 runs except one
(up to 2 were are allowed).
</bodyText>
<subsectionHeader confidence="0.98661">
6.1 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999848181818182">
Table 3 shows the evaluation results. We use the F1-
score and Pearson correlation as the primary eval-
uation metric for the PI and SS task respectively.
The results are very exciting that most systems out-
performed the two strong baselines we chose, while
still showing room for improvement towards the hu-
man upper-bound estimated by the crowdsourcing
worker’s performance.
We provide three baselines, including a random
baseline, a strong supervised baseline and a state-
of-the-art unsupervised system:
</bodyText>
<sectionHeader confidence="0.866504" genericHeader="method">
Random:
</sectionHeader>
<bodyText confidence="0.957371333333333">
This baseline provides a randomized real num-
Most participants choose supervised methods, ex-
cept for MathLingBp who uses semi-supervised,
</bodyText>
<footnote confidence="0.989034333333333">
7The source code and data for WTMF is available at:
http://www.cs.columbia.edu/˜weiwei/code.
html
</footnote>
<page confidence="0.910285">
5
</page>
<figure confidence="0.999568636363636">
5 4 3 2 1 0
PINC (lexical dissimilarity)
0.95
0.90
0.85
0.80
0.75
1.00
random
filtered
MAB
5 4 3 2 1
Number of Sentence Pairs (out of 2000)
400
300
200
100
0
random
filtered
MAB
Number of Annotators Judging as Paraphrases (out of 5)
</figure>
<figureCaption confidence="0.9971548">
Figure 3: Numbers of paraphrases collected by different
methods. The annotation efficiency (3,4,5 are regarded
as paraphrases) is significantly improved by the sentence
filtering and Multi-Armed Bandits (MAB) based topic
selection.
</figureCaption>
<bodyText confidence="0.998328192307692">
Columbia and Yamraj who use unsupervised meth-
ods. While the best performed systems are super-
vised, the best unsupervised system still outperforms
some supervised systems and the state-of-the-art un-
supervised baseline. About half of systems use word
embeddings and many use neural networks.
To out best knowledge, this is the first time to
have a large number of systems in an evaluation that
has the two related tasks — paraphrase identification
and semantic similarity, side by side for compari-
son. One interesting observation that comes out is
the performance of the same system on the two tasks
(“F1 vs. Pearson”) are not necessarily related. For
example, ASOBEK ranked 1st (out of 35 runs) and
18th (out of 25 runs) in the PI and SS tasks respec-
tively, RTM-DCU ranked 27th and 3rd, while the
MITRE system ranked 3nd and 1st place. Neither
“F1 vs. max-F1” nor “Pearson vs. maxF1” nor “F1
vs. Pearson” show a strong correlation. It implies
that (i) high-performance PI systems can be devel-
oped focusing on the binary classification problem
without focusing on the degree of similarity; (ii) it
is crucial to select the threshold to balance precision
and recall for the PI binary classification problem;
(iii) it is important for SS system to handle the de-
batable cases proporiately.
</bodyText>
<subsectionHeader confidence="0.997974">
6.3 Participants’ Systems
</subsectionHeader>
<bodyText confidence="0.99511">
There are in total 19 teams participated:
</bodyText>
<figure confidence="0.431496">
Number of Annotators Judging as Paraphrases (out of 5)
</figure>
<figureCaption confidence="0.9661624">
Figure 4: PINC scores of paraphrases collected. The
higher the PINC, the more significant the rewording.
Our proposed annotation strategy quadruples paraphrase
yield, while not greatly reducing diversity as measured
by PINC.
</figureCaption>
<bodyText confidence="0.981126185185185">
AJ: This team utilizes TERp and BLEU – auto-
matic evaluation metrics for Machine Trans-
lation. The system uses a logistic regression
model and performs threshold selection.
AMRITACEN: This team uses Recursive Auto
Encoders (RAEs). The matrix generated for
the given input sentences is of variable size,
then converted to equal sized matrix using re-
peat matrix concept.
ASOBEK (Eyecioglu and Keller, 2015): This
team uses SVM classifier with simple lexical
word overlap and character n-grams features.
CDTDS (Karampatsis, 2015): This team uses
support vector regression trained only on the
training set using the numbers of positive votes
out of the 5 crowdsourcing annotations.
Columbia: This system maps each original sen-
tence to a low dimensional vector as Orthog-
onal Matrix Factorization (Guo et al., 2014),
and then computes similarity score based on the
low dimensional vectors.
Depth: This team uses neural network that learns
representation of sentences, then compute sim-
ilarity scores based on hidden vector represen-
tations between two sentences.
EBIQUITY (Satyapanich et al., 2015): This
team trains supervised SVM and logistic re-
</bodyText>
<page confidence="0.999341">
6
</page>
<table confidence="0.999921595238095">
Rank Team Run Paraphrase Identification (PI) Semantic Similarity (SS)
PI SS F1 Precision Recall Pearson maxF1 mPrec mRecall
Human Upperbound 0.823 0.752 0.908 0.735 —— —— ——
1 ASOBEK 01 svckernel 0.6741 0.680 0.669 0.47518 0.616 0.732 0.531
8 ASOBEK 02linearsvm 0.6722 0.682 0.663 0.50414 0.663 0.723 0.611
2 1 MITRE 01 ikr 0.6673 0.569 0.806 0.6191 0.716 0.750 0.686
3 ECNU 02 nnfeats 0.6624 0.767 0.583 —— —— —— ——
4 FBK-HLT 01 voted 0.6595 0.685 0.634 0.46219 0.607 0.551 0.674
5 TKLBLIIR 02 gs0105 0.6595 0.645 0.674 —— —— —— ——
MITRE 02 bieber 0.6527 0.559 0.783 0.6122 0.724 0.753 0.697
6 HLTC-HKUST 02 run2 0.6527 0.574 0.754 0.5456 0.669 0.738 0.611
3 HLTC-HKUST 01 run1 0.6519 0.594 0.720 0.5635 0.676 0.697 0.657
ECNU 01 mlfeats 0.64310 0.754 0.560 —— —— —— ——
7 4 AJ 01 first 0.62211 0.523 0.766 0.5277 0.642 0.571 0.731
8 5 DEPTH 02 modelx23 0.61912 0.652 0.589 0.5188 0.636 0.602 0.674
9 9 CDTDS 01 simple 0.61313 0.547 0.697 0.49415 0.626 0.675 0.583
CDTDS 02 simplews 0.61214 0.542 0.703 0.49116 0.624 0.589 0.663
DEPTH 01 modelh22 0.61015 0.647 0.577 0.50513 0.638 0.642 0.634
10 FBK-HLT 02 multilayer 0.60616 0.676 0.549 0.48017 0.604 0.504 0.754
10 ROB 01 all 0.60117 0.519 0.714 0.51310 0.612 0.721 0.531
11 EBIQUITY 01 run 0.59918 0.651 0.554 —— —— —— ——
TKLBLIIR 01 gsc054 0.59019 0.461 0.817 —— —— —— ——
EBIQUITY 02 run 0.59019 0.646 0.543 —— —— —— ——
BASELINE logistic reg. 0.58921 0.679 0.520 0.51111 0.601 0.674 0.543
12 11 COLUMBIA 02 ormf o 0.58822 0.593 0.583 0.42520 0.599 0.623 0.577
13 12 HASSY 01 train 0.57123 0.449 0.783 0.40522 0.645 0.657 0.634
14 RTM-DCU 01 PLSSVR 0.56224 0.859 0.417 0.5644 0.678 0.649 0.709
COLUMBIA 01 ormf o 0.56125 0.831 0.423 0.42520 0.599 0.623 0.577
HASSY 02 traindev 0.55125 0.423 0.789 0.40522 0.629 0.648 0.611
2 RTM-DCU 02 SVR 0.54027 0.883 0.389 0.5703 0.693 0.695 0.691
BASELINE WTMF o 0.53628 0.450 0.663 0.35026 0.587 0.570 0.606
6 ROB 02 all 0.53229 0.388 0.846 0.5159 0.616 0.685 0.560
7 MATHLING 02 twimash o 0.51530 0.364 0.880 0.51111 0.650 0.648 0.651
15 MATHLING 01 twiemb o 0.51530 0.454 0.594 0.22927 0.562 0.638 0.503
16 YAMRAJ 01 google o 0.49632 0.725 0.377 0.36025 0.542 0.502 0.589
17 STANFORD 01 vs 0.48033 0.800 0.343 —— —— —— ——
AJ 02 second 0.47734 0.618 0.389 —— —— —— ——
13 YAMRAJ 02 lexical o 0.47035 0.677 0.360 0.36324 0.511 0.508 0.514
late late AMRITACEN 01 RAE 0.457 0.543 0.394 0.303 0.457 0.543 0.394
18 WHUHJP 02 whuhjp 0.42536 0.299 0.731 —— —— —— ——
WHUHJP 01 whuhjp 0.38737 0.275 0.651 —— —— —— ——
BASELINE random o 0.26638 0.192 0.434 0.01728 0.350 0.215 0.949
</table>
<tableCaption confidence="0.932351">
Table 3: Evaluation results. The first column presents the rank of each team in the two tasks based on each team’s best
system. The superscripts are the ranks of systems, ordered by F1 for Paraphrase Identification (PI) task and Pearson
</tableCaption>
<bodyText confidence="0.854502428571429">
for Semantic Similarity (SS) task. o indicates unsupervised or semi-supervised system. In total, 19 teams participated
in the PI task, of which 14 teams also participated in the SS task. Note that although the two sub-tasks share the same
test set of 972 sentence pairs, the PI task ignores 134 debatable cases (received a medium-score from expert annotator)
and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs.
This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that
the F1-scores of the baselines in the PI task are higher than reported in the Table 2 of (Xu et al., 2014), because the
later reported maximum F1-scores on the PI task, ignoring the debatable cases.
</bodyText>
<page confidence="0.998894">
7
</page>
<bodyText confidence="0.997151578313254">
gression models using features of semantic
similarities between sentence pairs.
ECNU (Zhao and Lan, 2015): This team adopts
typical machine learning classifiers and uses a
variety of features, such as surface text, seman-
tic level, textual entailment, word distributional
representations by deep learning methods.
FBK-HLT (Ngoc Phuoc An Vo and Popescu,
2015): This team uses supervised learning
model with different features for the 2 runs,
such as n-gram overlap, word alignment and
edit distance.
Hassy: This team uses a bag-of-embeddings ap-
proach via supervised learning. Two sentences
are first embedded into a vector space, and then
the system computes the dot-product of the two
sentence embeddings.
HLTC-HKUST (Bertero and Fung, 2015): This
team uses supervised classification with a stan-
dard two-layer neural network classifier. The
features used include translation metrics, lex-
ical, syntactic and semantic similarity scores,
the latter with an emphasis on aligned semantic
roles comparison.
MathLingBp: This team implements the align-
and-penalize architecture described by Han
et al. (2013) with slight modifications and
makes use of several word similarity metrics.
One metric relies on a mapping of words to
vectors built from the Rovereto Twitter N-
Gram corpus, another on a synonym list built
from Wiktionary’s translations, while a third
approach derives word similarity from concept
graphs built using the 4lang lexicon and the
Longman Dictionary of Contemporary English
(Kornai et al., 2015).
MITRE (Zarrella et al., 2015): A recurrent neu-
ral network models semantic similarity be-
tween sentences using the sequence of sym-
metric word alignments that maximize cosine
similarity between word embeddings. We in-
clude features from local similarity of char-
acters, random projection, matching word se-
quences, pooling of word embeddings, and
alignment quality metrics. The resulting en-
semble uses both semantic and string matching
at many levels of granularity.
RTM-DCU (Bicici, 2015): This team uses ref-
erential translation machines (RTM) and ma-
chine translation performance prediction sys-
tem (MTPP) for predicting semantic similar-
ity where indicators of translatability are used
as features (Bic¸ici and Way, 2014) and in-
stance selection for RTM is performed with
FDA5 (Bic¸ici and Yuret, 2014). RTM works
as follows: FDA5 → MTPP → ML training →
predict.
Rob (van der Goot and van Noord, 2015): This
system is inspired by a state-of-the-art semantic
relatedness prediction system by Bjerva et al.
(2014). It combines features from different
parses with lexical and compositional distribu-
tional feature using a logistic regression model.
STANFORD: This team uses a supervised sys-
tem with sentiment, phrase similarity matrix,
and alignment features. Similarity metrics are
based on vector space representation of phrases
which was trained on a large corpus.
TkLbLiiR (Glavaˇs et al., 2015): This team uses
a supervised model with about 15 comparison-
based numeric features. The most important
features are the distributional features weighted
by the topic-specific information.
WHUHJP: This team uses the word2vec tool to
train a vector model on the training data, then
computes distributed representations of sen-
tences in the test set and their cosine similarity.
Yamraj: This team uses pre-trained word and
phrase vectors on Google News data set (about
100 billion words) and Wikipeida articles. The
system relies on the cosine distance between
vectors representing the sentences computed
using open-source toolkit Gensim.
</bodyText>
<sectionHeader confidence="0.990486" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.995635">
We have presented the task definition, data annota-
tion and evaluation results to the first Paraphrase and
Semantic Similarity In Twitter (PIT) shared task.
</bodyText>
<page confidence="0.991811">
8
</page>
<bodyText confidence="0.9999319">
Our analysis provides some initial insights into the
relation and the difference between paraphrase iden-
tification and semantic similarity problems. We
make all the data, baseline systems and evaluation
scripts publicly available.8
In the future, we plan to extend the task to allow
leverage of more information from social networks,
for example, by providing the full tweets (and their
ids) that are associated with each sentence and with
each topic.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999608642857143">
We would like to thank all participants, review-
ers and SemEval organizers Preslav Nakov, Torsten
Zesch, Daniel Cer, David Jurgens. This material
is based in part on research sponsored by the NSF
under grant IIS-1430651, DARPA under agreement
number FA8750-13-2-0017 (the DEFT program)
and through a Google Faculty Research Award to
Chris Callison-Burch. The U.S. Government is au-
thorized to reproduce and distribute reprints for gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing offi-
cial policies or endorsements of DARPA or the U.S.
Government.
</bodyText>
<sectionHeader confidence="0.980093" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.986581">
Agirre, E., Diab, M., Cer, D., and Gonzalez-Agirre,
A. (2012). Semeval-2012 task 6: A pilot on se-
mantic textual similarity. In Proceedings of Se-
mEval.
Artstein, R. and Poesio, M. (2008). Inter-coder
agreement for computational linguistics. Compu-
tational Linguistics, 34(4).
Bertero, D. and Fung, P. (2015). HLTC-HKUST: A
neural network paraphrase classifier using transla-
tion metrics, semantic roles and lexical similarity
features. In Proceedings of SemEval.
Bic¸ici, E. and Way, A. (2014). RTM-DCU: Referen-
tial translation machines for semantic similarity.
In Proceedings of SemEval.
Bic¸ici, E. and Yuret, D. (2014). Optimizing instance
selection for statistical machine translation with
</bodyText>
<footnote confidence="0.9483105">
8https://github.com/cocoxu/
SemEval-PIT2015
</footnote>
<reference confidence="0.725483681818182">
feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Process-
ing (TASLP).
Bicici, E. (2015). RTM-DCU: Predicting semantic
similarity with referential translation machines.
In Proceedings of SemEval.
Bjerva, J., Bos, J., van der Goot, R., and Nissim,
M. (2014). The meaning factory: Formal seman-
tics for recognizing textual entailment and deter-
mining semantic similarity. In Proceedings of Se-
mEval.
Blacoe, W. and Lapata, M. (2012). A comparison of
vector-based representations for semantic compo-
sition. In Proceedings of EMNLP-CoNLL.
Chen, D. L. and Dolan, W. B. (2011). Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of ACL.
Das, D. and Smith, N. A. (2009). Paraphrase identi-
fication as probabilistic quasi-synchronous recog-
nition. In Proceedings ofACL-IJCNLP.
Derczynski, L., Ritter, A., Clark, S., and Bontcheva,
K. (2013). Twitter part-of-speech tagging for all:
Overcoming sparse and noisy data. In Proceed-
ings of RANLP.
Dolan, B., Quirk, C., and Brockett, C. (2004). Un-
supervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of COLING.
Eyecioglu, A. and Keller, B. (2015). ASOBEK:
Twitter paraphrase identification with simple
overlap features and SVMs. In Proceedings of Se-
mEval.
Fellbaum, C. (2010). WordNet. In Theory and Ap-
plications of Ontology: Computer Applications.
Springer.
Fernando, S. and Stevenson, M. (2008). A semantic
similarity approach to paraphrase detection. Com-
putational Linguistics UK (CLUK) 11th Annual
Research Colloquium.
Francis, W. N. and Kucera, H. (1979). Brown cor-
pus manual. Technical report, Brown University.
Department of Linguistics.
Glavaˇs, G., Karan, M., ˇSnajder, J., Baˇsi´c, B. D.,
Vuli´c, I., and Moens, M.-F. (2015). TKLBLIIR:
</reference>
<page confidence="0.987916">
9
</page>
<reference confidence="0.98006104494382">
Detecting Twitter paraphrases with TweetingJay.
In Proceedings of SemEval.
Guo, W. and Diab, M. (2012). Modeling sentences
in the latent space. In Proceedings of ACL.
Guo, W., Liu, W., and Diab, M. (2014). Fast tweet
retrieval with compact binary codes. In Proceed-
ings of COLING.
Han, L., Kashyap, A., Finin, T., Mayfield, J., and
Weese, J. (2013). UMBC EBIQUITY-CORE: Se-
mantic textual similarity systems. In Proceedings
of *SEM.
Hassan, S. and Mihalcea, R. (2011). Semantic re-
latedness using salient semantic analysis. In Pro-
ceedings of AAAI.
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L.,
and Weischedel, R. (2006). OntoNotes: the 90%
solution. In Proceedings of HLT-NAACL.
Islam, A. and Inkpen, D. (2007). Semantic similarity
of short texts. In Proceedings of RANLP.
Ji, Y. and Eisenstein, J. (2013). Discriminative im-
provements to distributional sentence similarity.
In Proceedings of EMNLP.
Karampatsis, R.-M. (2015). CDTDS: Predicting
paraphrases in Twitter via support vector regres-
sion. In Proceedings of SemEval.
Kornai, A., Makrai, M., Nemeskey, D., and Recski,
G. (2015). Extending 4lang using monolingual
dictionaries. Unpublished manuscript.
Madnani, N. and Dorr, B. J. (2010). Generating
phrasal and sentential paraphrases: A survey of
data-driven methods. Computational Linguistics.
Madnani, N., Tetreault, J., and Chodorow, M.
(2012). Re-examining machine translation met-
rics for paraphrase identification. In Proceedings
of NAACL-HLT.
Mihalcea, R., Corley, C., and Strapparava, C.
(2006). Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings
of AAAI.
Nenkova, A. and Vanderwende, L. (2005). The im-
pact of frequency on summarization. Technical
report, Microsoft Research. MSR-TR-2005-101.
Ngoc Phuoc An Vo, S. M. and Popescu, O. (2015).
FBK-HLT: An application of semantic textual
similarity for paraphrase and semantic similarity
in Twitter. In Proceedings of SemEval.
O’Connor, B., Krieger, M., and Ahn, D. (2010).
Tweetmotif: Exploratory search and topic sum-
marization for Twitter. In Proceedings of ICWSM.
Petrovi´c, S., Osborne, M., and Lavrenko, V. (2012).
Using paraphrases for improving first story de-
tection in news and twitter. In Proceedings of
NAACL-HLT.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2006). Para-
phrase recognition via dissimilarity significance
classification. In Proceedings of EMNLP.
Ritter, A., Clark, S., and Etzioni, O. (2011). Named
entity recognition in tweets: an experimental
study. In Proceedings of EMNLP.
Robbins, H. (1985). Some aspects of the sequen-
tial design of experiments. In Herbert Robbins
Selected Papers. Springer.
Rus, V., McCarthy, P. M., Lintean, M. C., McNa-
mara, D. S., and Graesser, A. C. (2008). Para-
phrase identification with lexico-syntactic graph
subsumption. In Proceedings of FLAIRS.
Satyapanich, T., Gao, H., and Finin, T. (2015). Ebiq-
uity: Paraphrase and semantic similarity in Twit-
ter using skipgrams. In Proceedings of SemEval.
Socher, R., Huang, E. H., Pennin, J., Manning,
C. D., and Ng, A. (2011). Dynamic pooling and
unfolding recursive autoencoders for paraphrase
detection. In Proceedings of NIPS.
Tran-Thanh, L., Stein, S., Rogers, A., and Jennings,
N. R. (2012). Efficient crowdsourcing of un-
known experts using multi-armed bandits. In Pro-
ceedings of ECAI.
van der Goot, R. and van Noord, G. (2015).
ROB: Using semantic meaning to recognize para-
phrases. In Proceedings of SemEval.
Vanderwende, L., Suzuki, H., Brockett, C., and
Nenkova, A. (2007). Beyond SumBasic: Task-
focused summarization with sentence simplifica-
tion and lexical expansion. Information Process-
ing &amp; Management, 43.
Wan, S., Dras, M., Dale, R., and Paris, C. (2006).
Using dependency-based features to take the para-
farce out of paraphrase. In Proceedings of the
Australasian Language Technology Workshop.
</reference>
<page confidence="0.950338">
10
</page>
<reference confidence="0.995792178571429">
Wang, L., Dyer, C., Black, A. W., and Trancoso, I.
(2013). Paraphrasing 4 microblog normalization.
In Proceedings of EMNLP.
Xu, W. (2014). Data-Drive Approaches for Para-
phrasing Across Language Variations. PhD the-
sis, Department of Computer Science, New York
University.
Xu, W., Ritter, A., Callison-Burch, C., Dolan, W. B.,
and Ji, Y. (2014). Extracting lexically divergent
paraphrases from Twitter. Transactions of the As-
sociation for Computational Linguistics (TACL),
2(1).
Xu, W., Ritter, A., and Grishman, R. (2013). Gather-
ing and generating paraphrases from twitter with
application to normalization. In Proceedings of
the Sixth Workshop on Building and Using Com-
parable Corpora.
Zanzotto, F. M., Pennacchiotti, M., and Tsiout-
siouliklis, K. (2011). Linguistic redundancy in
twitter. In Proceedings of EMNLP.
Zarrella, G., Henderson, J., Merkhofer, E. M., and
Strickhart, L. (2015). MITRE: Seven systems for
semantic similarity in tweets. In Proceedings of
SemEval.
Zhao, J. and Lan, M. (2015). ECNU: Boosting per-
formance for paraphrase and semantic similarity
in Twitter by leveraging word embeddings. In
Proceedings of SemEval.
</reference>
<page confidence="0.999469">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.728964">
<title confidence="0.90078">SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT)</title>
<author confidence="0.999966">Wei Xu</author>
<author confidence="0.999966">Chris Callison-Burch William B Dolan</author>
<affiliation confidence="0.999979">University of Pennsylvania Microsoft Research</affiliation>
<address confidence="0.998016">Philadelphia, PA, USA Redmond, WA, USA</address>
<email confidence="0.99963">xwe,ccb@cis.upenn.edubilldol@microsoft.com</email>
<abstract confidence="0.992620846153846">In this shared task, we present evaluations on two related tasks Paraphrase Identification (PI) and Semantic Textual Similarity (SS) systems for the Twitter data. Given a pair of sentences, participants are asked to produce a binary yes/no judgement or a graded score to measure their semantic equivalence. The task features a newly constructed Twitter Paraphrase Corpus that contains 18,762 sentence pairs. A total of 19 teams participated, submitting 36 runs to the PI task and 26 runs to the SS task. The evaluation shows encouraging results and open challenges for future research. The best systems scored a F1-measure of 0.674 for the PI task and a Pearson correlation of 0.619 for the SS task respectively, comparing to a strong baseline using logistic regression model of 0.589 F1 and 0.511 Pearson; while the best SS systems can ofreach Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts pub-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>feature decay algorithms.</title>
<journal>IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</journal>
<marker></marker>
<rawString>feature decay algorithms. IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bicici</author>
</authors>
<title>RTM-DCU: Predicting semantic similarity with referential translation machines.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="26068" citStr="Bicici, 2015" startWordPosition="4172" endWordPosition="4173">arity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and alignment quality metrics. The resulting ensemble uses both semantic and string matching at many levels of granularity. RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP) for predicting semantic similarity where indicators of translatability are used as features (Bic¸ici and Way, 2014) and instance selection for RTM is performed with FDA5 (Bic¸ici and Yuret, 2014). RTM works as follows: FDA5 → MTPP → ML training → predict. Rob (van der Goot and van Noord, 2015): This system is inspired by a state-of-the-art semantic relatedness prediction system by Bjerva et al. (2014). It combines features from different parses with lexical and compositional dis</context>
</contexts>
<marker>Bicici, 2015</marker>
<rawString>Bicici, E. (2015). RTM-DCU: Predicting semantic similarity with referential translation machines. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bjerva</author>
<author>J Bos</author>
<author>R van der Goot</author>
<author>M Nissim</author>
</authors>
<title>The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<marker>Bjerva, Bos, van der Goot, Nissim, 2014</marker>
<rawString>Bjerva, J., Bos, J., van der Goot, R., and Nissim, M. (2014). The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Blacoe</author>
<author>M Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="2563" citStr="Blacoe and Lapata, 2012" startWordPosition="399" endWordPosition="402">arch shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http:</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>Blacoe, W. and Lapata, M. (2012). A comparison of vector-based representations for semantic composition. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Chen</author>
<author>W B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13494" citStr="Chen and Dolan, 2011" startWordPosition="2117" endWordPosition="2120">h word P(wi) by simply dividing its frequency by the total number of all words in all sentences. Each sentence s is scored as the average of the probabilities of the words in it, i.e. P(wi) (1) |{wi|wi E s}| We then rank the sentences and pick the original sentence randomly from top 10% salient sentences and candidate sentences from top 50% to present to the annotators. In a trial experiment of 20 topics, the filtering technique double the yield of paraphrases from 152 �Salience(s) = wi∈s 4 to 329 out of 2000 sentence pairs over naive random sampling (Figure 2 and Figure 3). We also use PINC (Chen and Dolan, 2011) to measure the quality of paraphrases collected (Figure 4). PINC was designed to measure n-gram dissimilarity between two sentences, and in essence it is the inverse of BLEU. In general, the cases with high PINC scores include more complex and interesting rephrasings. 4.5 Topic Selection using Multi-Armed Bandits (MAB) Algorithm Another approach to increasing paraphrase yield is to choose more appropriate topics. This is particularly important because the number of paraphrases varies greatly from topic to topic and thus the chance to encounter paraphrases during annotation (Figure 2). We trea</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>Chen, D. L. and Dolan, W. B. (2011). Collecting highly parallel data for paraphrase evaluation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="2517" citStr="Das and Smith, 2009" startWordPosition="391" endWordPosition="394">op ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has </context>
<context position="7770" citStr="Das and Smith, 2009" startWordPosition="1228" endWordPosition="1231">ured on Twitter (excluding hashtags). The training and development set is a random split. Each sentence pair is annotated by 5 different crowdsourcing workers. For the test set, we obtain both crowdsourced and expert labels on 972 sentence pairs from 20 randomly sampled Twitter trending topics between May 13th and June 10th, 2013. We use expert labels in this SemEval evaluation. Our dataset is more realistic and balanced, containing about 70% non-paraphrases vs. the 34% non-paraphrases in the benchmark Microsoft Paraphrase Corpus derived from news articles by Dolan et al. (2004). As noted in (Das and Smith, 2009), the lack of natural non-paraphrases in the MSR corpus creates bias towards certain models. 4 Annotation In this section, we describe our data collection and annotation methodology. Since Twitter users are free to talk about anything regarding any topic, a random pair of sentences about the same topic has a low chance of expressing the same meaning (empirically, this is less than 8%). This causes two problems: a) it is expensive to obtain paraphrases via manual annotation; b) non-expert annotators tend to loosen the criteria and are more likely to make false positive errors. To address these </context>
<context position="15541" citStr="Das and Smith (2009)" startWordPosition="2450" endWordPosition="2453">cially duplicating a small amount of real annotation data. We then apply this MAB algorithm in the real-world. We explore 500 random topics and then exploited 100 of them. The yield of paraphrases rises to 688 out of 2000 sentence pairs by using MAB and sentence filtering, a 4-fold increase compared to only using random selection (Figure 3). 5 Baselines 6.2 Discussion ber between [0, 1] for each test sentence pair as semantic similarity score, and uses 0.5 as cutoff for binary paraphrase identification output. Logistic Regression: This is a supervised logistic regression (LR) baseline used by Das and Smith (2009). It uses simple n-gram (also in stemmed form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Das, D. and Smith, N. A. (2009). Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Derczynski</author>
<author>A Ritter</author>
<author>S Clark</author>
<author>K Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: Overcoming sparse and noisy data.</title>
<date>2013</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="8720" citStr="Derczynski et al. (2013)" startWordPosition="1377" endWordPosition="1380">pressing the same meaning (empirically, this is less than 8%). This causes two problems: a) it is expensive to obtain paraphrases via manual annotation; b) non-expert annotators tend to loosen the criteria and are more likely to make false positive errors. To address these challenges, we design a simple annotation task and introduce two selection mechanisms to select sentences which are more likely to be paraphrases, while preserving diversity and representativeness. 3The tokenizer was developed by O’Connor et al. (2010): https://github.com/brendano/tweetmotif 4The POS tagger was developed by Derczynski et al. (2013) and the NER tagger was developed by Ritter et al. (2011): https://github.com/aritter/twitter_nlp expert=5 expert=4 expert=3 expert=2 expert=1 expert=0 Figure 1: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Derczynski, L., Ritter, A., Clark, S., and Bontcheva, K. (2013). Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="7735" citStr="Dolan et al. (2004)" startWordPosition="1221" endWordPosition="1224">013 from 500+ trending topics featured on Twitter (excluding hashtags). The training and development set is a random split. Each sentence pair is annotated by 5 different crowdsourcing workers. For the test set, we obtain both crowdsourced and expert labels on 972 sentence pairs from 20 randomly sampled Twitter trending topics between May 13th and June 10th, 2013. We use expert labels in this SemEval evaluation. Our dataset is more realistic and balanced, containing about 70% non-paraphrases vs. the 34% non-paraphrases in the benchmark Microsoft Paraphrase Corpus derived from news articles by Dolan et al. (2004). As noted in (Das and Smith, 2009), the lack of natural non-paraphrases in the MSR corpus creates bias towards certain models. 4 Annotation In this section, we describe our data collection and annotation methodology. Since Twitter users are free to talk about anything regarding any topic, a random pair of sentences about the same topic has a low chance of expressing the same meaning (empirically, this is less than 8%). This causes two problems: a) it is expensive to obtain paraphrases via manual annotation; b) non-expert annotators tend to loosen the criteria and are more likely to make false</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Dolan, B., Quirk, C., and Brockett, C. (2004). Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Eyecioglu</author>
<author>B Keller</author>
</authors>
<title>ASOBEK: Twitter paraphrase identification with simple overlap features and SVMs.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="19749" citStr="Eyecioglu and Keller, 2015" startWordPosition="3127" endWordPosition="3130">Figure 4: PINC scores of paraphrases collected. The higher the PINC, the more significant the rewording. Our proposed annotation strategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC. AJ: This team utilizes TERp and BLEU – automatic evaluation metrics for Machine Translation. The system uses a logistic regression model and performs threshold selection. AMRITACEN: This team uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hi</context>
</contexts>
<marker>Eyecioglu, Keller, 2015</marker>
<rawString>Eyecioglu, A. and Keller, B. (2015). ASOBEK: Twitter paraphrase identification with simple overlap features and SVMs. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<date>2010</date>
<booktitle>WordNet. In Theory and Applications of Ontology: Computer Applications.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="16118" citStr="Fellbaum, 2010" startWordPosition="2545" endWordPosition="2546"> baseline used by Das and Smith (2009). It uses simple n-gram (also in stemmed form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). It uses 0.5 as cutoff in the binary paraphrase identification task. 6 Systems and Results A total of 18 teams participated in the PI task (required), 13 of which also submitted to the SS task (optional). Every team submitted 2 runs except one (up to 2 were are allowed). 6.1 Evaluation Results Table 3 shows the evaluation results. We use the F1- score and Pearson correlation as the primary evaluation metric for the PI and SS task respectively. The results are very exciting that most systems outperformed th</context>
</contexts>
<marker>Fellbaum, 2010</marker>
<rawString>Fellbaum, C. (2010). WordNet. In Theory and Applications of Ontology: Computer Applications. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fernando</author>
<author>M Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<booktitle>Computational Linguistics UK (CLUK) 11th Annual Research Colloquium.</booktitle>
<contexts>
<context position="2707" citStr="Fernando and Stevenson, 2008" startWordPosition="424" endWordPosition="427">on retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th International Workshop on Semantic </context>
</contexts>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Fernando, S. and Stevenson, M. (2008). A semantic similarity approach to paraphrase detection. Computational Linguistics UK (CLUK) 11th Annual Research Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Brown corpus manual.</title>
<date>1979</date>
<tech>Technical report,</tech>
<institution>Brown University. Department of Linguistics.</institution>
<contexts>
<context position="16206" citStr="Francis and Kucera, 1979" startWordPosition="2556" endWordPosition="2559"> form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). It uses 0.5 as cutoff in the binary paraphrase identification task. 6 Systems and Results A total of 18 teams participated in the PI task (required), 13 of which also submitted to the SS task (optional). Every team submitted 2 runs except one (up to 2 were are allowed). 6.1 Evaluation Results Table 3 shows the evaluation results. We use the F1- score and Pearson correlation as the primary evaluation metric for the PI and SS task respectively. The results are very exciting that most systems outperformed the two strong baselines we chose, while still showing room for improvement towards the hu</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>Francis, W. N. and Kucera, H. (1979). Brown corpus manual. Technical report, Brown University. Department of Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Glavaˇs</author>
<author>M Karan</author>
<author>J ˇSnajder</author>
<author>B D Baˇsi´c</author>
<author>I Vuli´c</author>
<author>M-F Moens</author>
</authors>
<title>TKLBLIIR: Detecting Twitter paraphrases with TweetingJay.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<marker>Glavaˇs, Karan, ˇSnajder, Baˇsi´c, Vuli´c, Moens, 2015</marker>
<rawString>Glavaˇs, G., Karan, M., ˇSnajder, J., Baˇsi´c, B. D., Vuli´c, I., and Moens, M.-F. (2015). TKLBLIIR: Detecting Twitter paraphrases with TweetingJay. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Guo</author>
<author>M Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2964" citStr="Guo and Diab, 2012" startWordPosition="462" endWordPosition="465">mmary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 1–11, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Paraphrase? Sentence 1 Sentence 2 yes Ezekiel Ansah wearing 3D glasses wout Wait Ezekiel ansah is wearing 3d movie the lens glasses wit</context>
<context position="15902" citStr="Guo and Diab (2012)" startWordPosition="2506" endWordPosition="2509">scussion ber between [0, 1] for each test sentence pair as semantic similarity score, and uses 0.5 as cutoff for binary paraphrase identification output. Logistic Regression: This is a supervised logistic regression (LR) baseline used by Das and Smith (2009). It uses simple n-gram (also in stemmed form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). It uses 0.5 as cutoff in the binary paraphrase identification task. 6 Systems and Results A total of 18 teams participated in the PI task (required), 13 of which also submitted to the SS task (optional). Every team submitted 2 runs except one (up to 2 were are allowed). 6.1 Evaluation Results </context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Guo, W. and Diab, M. (2012). Modeling sentences in the latent space. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Guo</author>
<author>W Liu</author>
<author>M Diab</author>
</authors>
<title>Fast tweet retrieval with compact binary codes.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="20154" citStr="Guo et al., 2014" startWordPosition="3190" endWordPosition="3193">m uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM and logistic re6 Rank Team Run Paraphrase Identification (PI) Semantic Similarity (SS) PI SS F1 Precision Recall Pearson maxF1 mPrec mRecall Human Upperbound 0.823 0.752 0.908 0.735 —— —— —— 1 ASOBEK 01 svckernel 0.6741 0.680 0.669 0.47518 0.616 0.732 0.531 8 ASOBEK 02linearsvm 0.6722</context>
</contexts>
<marker>Guo, Liu, Diab, 2014</marker>
<rawString>Guo, W., Liu, W., and Diab, M. (2014). Fast tweet retrieval with compact binary codes. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Han</author>
<author>A Kashyap</author>
<author>T Finin</author>
<author>J Mayfield</author>
<author>J Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proceedings of *SEM.</booktitle>
<contexts>
<context position="25176" citStr="Han et al. (2013)" startWordPosition="4033" endWordPosition="4036">d edit distance. Hassy: This team uses a bag-of-embeddings approach via supervised learning. Two sentences are first embedded into a vector space, and then the system computes the dot-product of the two sentence embeddings. HLTC-HKUST (Bertero and Fung, 2015): This team uses supervised classification with a standard two-layer neural network classifier. The features used include translation metrics, lexical, syntactic and semantic similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between </context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Han, L., Kashyap, A., Finin, T., Mayfield, J., and Weese, J. (2013). UMBC EBIQUITY-CORE: Semantic textual similarity systems. In Proceedings of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hassan</author>
<author>R Mihalcea</author>
</authors>
<title>Semantic relatedness using salient semantic analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2759" citStr="Hassan and Mihalcea, 2011" startWordPosition="432" endWordPosition="435">ation (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 1–11, Denver, Color</context>
</contexts>
<marker>Hassan, Mihalcea, 2011</marker>
<rawString>Hassan, S. and Mihalcea, R. (2011). Semantic relatedness using salient semantic analysis. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>M Marcus</author>
<author>M Palmer</author>
<author>L Ramshaw</author>
<author>R Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="16149" citStr="Hovy et al., 2006" startWordPosition="2548" endWordPosition="2551">ith (2009). It uses simple n-gram (also in stemmed form) overlapping features but shows very competitive performance on the MSR news paraphrase corpus. It uses 0.5 as cutoff to create binary outputs for the paraphrase identification task. Weighted Matrix Factorization (WTMF):7 The third baseline is a state-of-the-art unsupervised method developed by Guo and Diab (2012). It is specially developed for short sentences by modeling the semantic space of both words that are present in and absent from the sentences (Guo and Diab, 2012). The model was learned from WordNet (Fellbaum, 2010), OntoNotes (Hovy et al., 2006), Wiktionary, the Brown corpus (Francis and Kucera, 1979). It uses 0.5 as cutoff in the binary paraphrase identification task. 6 Systems and Results A total of 18 teams participated in the PI task (required), 13 of which also submitted to the SS task (optional). Every team submitted 2 runs except one (up to 2 were are allowed). 6.1 Evaluation Results Table 3 shows the evaluation results. We use the F1- score and Pearson correlation as the primary evaluation metric for the PI and SS task respectively. The results are very exciting that most systems outperformed the two strong baselines we chose</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and Weischedel, R. (2006). OntoNotes: the 90% solution. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Islam</author>
<author>D Inkpen</author>
</authors>
<title>Semantic similarity of short texts.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP.</booktitle>
<contexts>
<context position="2731" citStr="Islam and Inkpen, 2007" startWordPosition="428" endWordPosition="431"> 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015</context>
</contexts>
<marker>Islam, Inkpen, 2007</marker>
<rawString>Islam, A. and Inkpen, D. (2007). Semantic similarity of short texts. In Proceedings of RANLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ji</author>
<author>J Eisenstein</author>
</authors>
<title>Discriminative improvements to distributional sentence similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2611" citStr="Ji and Eisenstein, 2013" startWordPosition="408" endWordPosition="411">witter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase</context>
</contexts>
<marker>Ji, Eisenstein, 2013</marker>
<rawString>Ji, Y. and Eisenstein, J. (2013). Discriminative improvements to distributional sentence similarity. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R-M Karampatsis</author>
</authors>
<title>CDTDS: Predicting paraphrases in Twitter via support vector regression.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="19871" citStr="Karampatsis, 2015" startWordPosition="3146" endWordPosition="3147">rategy quadruples paraphrase yield, while not greatly reducing diversity as measured by PINC. AJ: This team utilizes TERp and BLEU – automatic evaluation metrics for Machine Translation. The system uses a logistic regression model and performs threshold selection. AMRITACEN: This team uses Recursive Auto Encoders (RAEs). The matrix generated for the given input sentences is of variable size, then converted to equal sized matrix using repeat matrix concept. ASOBEK (Eyecioglu and Keller, 2015): This team uses SVM classifier with simple lexical word overlap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM an</context>
</contexts>
<marker>Karampatsis, 2015</marker>
<rawString>Karampatsis, R.-M. (2015). CDTDS: Predicting paraphrases in Twitter via support vector regression. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kornai</author>
<author>M Makrai</author>
<author>D Nemeskey</author>
<author>G Recski</author>
</authors>
<title>Extending 4lang using monolingual dictionaries.</title>
<date>2015</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="25583" citStr="Kornai et al., 2015" startWordPosition="4097" endWordPosition="4100">xical, syntactic and semantic similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and alignment quality metrics. The resulting ensemble uses both semantic and string matching at many levels of granularity. RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP</context>
</contexts>
<marker>Kornai, Makrai, Nemeskey, Recski, 2015</marker>
<rawString>Kornai, A., Makrai, M., Nemeskey, D., and Recski, G. (2015). Extending 4lang using monolingual dictionaries. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>B J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of data-driven methods.</title>
<date>2010</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="1653" citStr="Madnani and Dorr, 2010" startWordPosition="261" endWordPosition="264"> model of 0.589 F1 and 0.511 Pearson; while the best SS systems can often reach &gt;0.80 Pearson on well-formed text. This shared task also provides insights into the relation between the PI and SS tasks and suggests the importance to bringing these two research areas together. We make all the data, baseline systems and evaluation scripts publicly available.1 1 Introduction The ability to identify paraphrases, i.e. alternative expressions of the same (or similar) meaning, and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecti</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Madnani, N. and Dorr, B. J. (2010). Generating phrasal and sentential paraphrases: A survey of data-driven methods. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2585" citStr="Madnani et al., 2012" startWordPosition="403" endWordPosition="407">echniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/i</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Madnani, N., Tetreault, J., and Chodorow, M. (2012). Re-examining machine translation metrics for paraphrase identification. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="2659" citStr="Mihalcea et al., 2006" startWordPosition="416" endWordPosition="419">ction (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceeding</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Mihalcea, R., Corley, C., and Strapparava, C. (2006). Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research. MSR-TR-2005-101.</tech>
<contexts>
<context position="12724" citStr="Nenkova and Vanderwende, 2005" startWordPosition="1978" endWordPosition="1982">e in a crowdsourcing setup. As for the binary paraphrase judgements, the integrated judgement of five crowdsourcing workers achieve a F1-score of 0.823, precision of 0.752 and recall of 0.908 against expert annotations. 4.4 Automatic Summarization Inspired Sentence Filtering We filter the sentences within each topic to select more probable paraphrases for annotation. Our method is inspired by a typical problem in extractive summarization, that the salient sentences are likely redundant (paraphrases) and need to be removed in the output summaries. We employ the scoring method used in SumBasic (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007), a simple but powerful summarization system, to find salient sentences. For each topic, we compute the probability of each word P(wi) by simply dividing its frequency by the total number of all words in all sentences. Each sentence s is scored as the average of the probabilities of the words in it, i.e. P(wi) (1) |{wi|wi E s}| We then rank the sentences and pick the original sentence randomly from top 10% salient sentences and candidate sentences from top 50% to present to the annotators. In a trial experiment of 20 topics, the filtering technique double the yield o</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Nenkova, A. and Vanderwende, L. (2005). The impact of frequency on summarization. Technical report, Microsoft Research. MSR-TR-2005-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Phuoc An Vo</author>
<author>S M</author>
<author>O Popescu</author>
</authors>
<title>FBK-HLT: An application of semantic textual similarity for paraphrase and semantic similarity in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<marker>Vo, M, Popescu, 2015</marker>
<rawString>Ngoc Phuoc An Vo, S. M. and Popescu, O. (2015). FBK-HLT: An application of semantic textual similarity for paraphrase and semantic similarity in Twitter. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B O’Connor</author>
<author>M Krieger</author>
<author>D Ahn</author>
</authors>
<title>Tweetmotif: Exploratory search and topic summarization for Twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>O’Connor, B., Krieger, M., and Ahn, D. (2010). Tweetmotif: Exploratory search and topic summarization for Twitter. In Proceedings of ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrovi´c</author>
<author>M Osborne</author>
<author>V Lavrenko</author>
</authors>
<title>Using paraphrases for improving first story detection in news and twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<marker>Petrovi´c, Osborne, Lavrenko, 2012</marker>
<rawString>Petrovi´c, S., Osborne, M., and Lavrenko, V. (2012). Using paraphrases for improving first story detection in news and twitter. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qiu</author>
<author>M-Y Kan</author>
<author>T-S Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2478" citStr="Qiu et al., 2006" startWordPosition="383" endWordPosition="386">ominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexic</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Qiu, L., Kan, M.-Y., and Chua, T.-S. (2006). Paraphrase recognition via dissimilarity significance classification. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ritter</author>
<author>S Clark</author>
<author>O Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8777" citStr="Ritter et al. (2011)" startWordPosition="1388" endWordPosition="1391">. This causes two problems: a) it is expensive to obtain paraphrases via manual annotation; b) non-expert annotators tend to loosen the criteria and are more likely to make false positive errors. To address these challenges, we design a simple annotation task and introduce two selection mechanisms to select sentences which are more likely to be paraphrases, while preserving diversity and representativeness. 3The tokenizer was developed by O’Connor et al. (2010): https://github.com/brendano/tweetmotif 4The POS tagger was developed by Derczynski et al. (2013) and the NER tagger was developed by Ritter et al. (2011): https://github.com/aritter/twitter_nlp expert=5 expert=4 expert=3 expert=2 expert=1 expert=0 Figure 1: A heat-map showing overlap between expert and crowdsourcing annotation. The intensity along the diagonal indicates good reliability of crowdsourcing workers for this particular task; and the shift above the diagonal reflects the difference between the two annotation schemas. For crowdsourcing (turk), the numbers indicate how many annotators out of 5 picked the sentence pair as paraphrases; 0,1 are considered non-paraphrases; 3,4,5 are paraphrases. For expert annotation, all 0,1,2 are nonpar</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Ritter, A., Clark, S., and Etzioni, O. (2011). Named entity recognition in tweets: an experimental study. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Robbins</author>
</authors>
<title>Some aspects of the sequential design of experiments. In Herbert Robbins Selected Papers.</title>
<date>1985</date>
<publisher>Springer.</publisher>
<contexts>
<context position="14195" citStr="Robbins, 1985" startWordPosition="2228" endWordPosition="2229">re n-gram dissimilarity between two sentences, and in essence it is the inverse of BLEU. In general, the cases with high PINC scores include more complex and interesting rephrasings. 4.5 Topic Selection using Multi-Armed Bandits (MAB) Algorithm Another approach to increasing paraphrase yield is to choose more appropriate topics. This is particularly important because the number of paraphrases varies greatly from topic to topic and thus the chance to encounter paraphrases during annotation (Figure 2). We treat this topic selection problem as a variation of the Multi-Armed Bandit (MAB) problem (Robbins, 1985) and adapt a greedy algorithm, the bounded c-first algorithm, of Tran-Thanh et al. (2012) to accelerate our corpus construction. Our strategy consists of two phases. In the first exploration phase, we dedicate a fraction of the total budget, c, to explore randomly chosen arms of each slot machine (trending topic on Twitter), each m times. In the second exploitation phase, we sort all topics according to their estimated proportion of paraphrases, and sequentially annotate �(1−�)s l−m � arms that have the highest estimated reward until reaching the maximum l = 10 annotations for any topic to ins</context>
</contexts>
<marker>Robbins, 1985</marker>
<rawString>Robbins, H. (1985). Some aspects of the sequential design of experiments. In Herbert Robbins Selected Papers. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Rus</author>
<author>P M McCarthy</author>
<author>M C Lintean</author>
<author>D S McNamara</author>
<author>A C Graesser</author>
</authors>
<title>Paraphrase identification with lexico-syntactic graph subsumption.</title>
<date>2008</date>
<booktitle>In Proceedings of FLAIRS.</booktitle>
<contexts>
<context position="2677" citStr="Rus et al., 2008" startWordPosition="420" endWordPosition="423">, 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challenges to both tasks. 2http://aclweb.org/aclwiki/index.php? title=Paraphrase_Identification_(State_of_ the_art) 1 Proceedings of the 9th Inter</context>
</contexts>
<marker>Rus, McCarthy, Lintean, McNamara, Graesser, 2008</marker>
<rawString>Rus, V., McCarthy, P. M., Lintean, M. C., McNamara, D. S., and Graesser, A. C. (2008). Paraphrase identification with lexico-syntactic graph subsumption. In Proceedings of FLAIRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Satyapanich</author>
<author>H Gao</author>
<author>T Finin</author>
</authors>
<title>Ebiquity: Paraphrase and semantic similarity in Twitter using skipgrams.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="20435" citStr="Satyapanich et al., 2015" startWordPosition="3231" endWordPosition="3234">lap and character n-grams features. CDTDS (Karampatsis, 2015): This team uses support vector regression trained only on the training set using the numbers of positive votes out of the 5 crowdsourcing annotations. Columbia: This system maps each original sentence to a low dimensional vector as Orthogonal Matrix Factorization (Guo et al., 2014), and then computes similarity score based on the low dimensional vectors. Depth: This team uses neural network that learns representation of sentences, then compute similarity scores based on hidden vector representations between two sentences. EBIQUITY (Satyapanich et al., 2015): This team trains supervised SVM and logistic re6 Rank Team Run Paraphrase Identification (PI) Semantic Similarity (SS) PI SS F1 Precision Recall Pearson maxF1 mPrec mRecall Human Upperbound 0.823 0.752 0.908 0.735 —— —— —— 1 ASOBEK 01 svckernel 0.6741 0.680 0.669 0.47518 0.616 0.732 0.531 8 ASOBEK 02linearsvm 0.6722 0.682 0.663 0.50414 0.663 0.723 0.611 2 1 MITRE 01 ikr 0.6673 0.569 0.806 0.6191 0.716 0.750 0.686 3 ECNU 02 nnfeats 0.6624 0.767 0.583 —— —— —— —— 4 FBK-HLT 01 voted 0.6595 0.685 0.634 0.46219 0.607 0.551 0.674 5 TKLBLIIR 02 gs0105 0.6595 0.645 0.674 —— —— —— —— MITRE 02 bieber </context>
</contexts>
<marker>Satyapanich, Gao, Finin, 2015</marker>
<rawString>Satyapanich, T., Gao, H., and Finin, T. (2015). Ebiquity: Paraphrase and semantic similarity in Twitter using skipgrams. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>E H Huang</author>
<author>J Pennin</author>
<author>C D Manning</author>
<author>A Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="2538" citStr="Socher et al., 2011" startWordPosition="395" endWordPosition="398">olice). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used in social media has posed serious challen</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Socher, R., Huang, E. H., Pennin, J., Manning, C. D., and Ng, A. (2011). Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tran-Thanh</author>
<author>S Stein</author>
<author>A Rogers</author>
<author>N R Jennings</author>
</authors>
<title>Efficient crowdsourcing of unknown experts using multi-armed bandits.</title>
<date>2012</date>
<booktitle>In Proceedings of ECAI.</booktitle>
<contexts>
<context position="14284" citStr="Tran-Thanh et al. (2012)" startWordPosition="2240" endWordPosition="2243"> of BLEU. In general, the cases with high PINC scores include more complex and interesting rephrasings. 4.5 Topic Selection using Multi-Armed Bandits (MAB) Algorithm Another approach to increasing paraphrase yield is to choose more appropriate topics. This is particularly important because the number of paraphrases varies greatly from topic to topic and thus the chance to encounter paraphrases during annotation (Figure 2). We treat this topic selection problem as a variation of the Multi-Armed Bandit (MAB) problem (Robbins, 1985) and adapt a greedy algorithm, the bounded c-first algorithm, of Tran-Thanh et al. (2012) to accelerate our corpus construction. Our strategy consists of two phases. In the first exploration phase, we dedicate a fraction of the total budget, c, to explore randomly chosen arms of each slot machine (trending topic on Twitter), each m times. In the second exploitation phase, we sort all topics according to their estimated proportion of paraphrases, and sequentially annotate �(1−�)s l−m � arms that have the highest estimated reward until reaching the maximum l = 10 annotations for any topic to insure data diversity. We tune the parameters m to be 1 and c to be between 0.35 ∼ 0.55 thro</context>
</contexts>
<marker>Tran-Thanh, Stein, Rogers, Jennings, 2012</marker>
<rawString>Tran-Thanh, L., Stein, S., Rogers, A., and Jennings, N. R. (2012). Efficient crowdsourcing of unknown experts using multi-armed bandits. In Proceedings of ECAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R van der Goot</author>
<author>G van Noord</author>
</authors>
<title>ROB: Using semantic meaning to recognize paraphrases.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<marker>van der Goot, van Noord, 2015</marker>
<rawString>van der Goot, R. and van Noord, G. (2015). ROB: Using semantic meaning to recognize paraphrases. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vanderwende</author>
<author>H Suzuki</author>
<author>C Brockett</author>
<author>A Nenkova</author>
</authors>
<title>Beyond SumBasic: Taskfocused summarization with sentence simplification and lexical expansion.</title>
<date>2007</date>
<journal>Information Processing &amp; Management,</journal>
<volume>43</volume>
<contexts>
<context position="12751" citStr="Vanderwende et al., 2007" startWordPosition="1983" endWordPosition="1986">for the binary paraphrase judgements, the integrated judgement of five crowdsourcing workers achieve a F1-score of 0.823, precision of 0.752 and recall of 0.908 against expert annotations. 4.4 Automatic Summarization Inspired Sentence Filtering We filter the sentences within each topic to select more probable paraphrases for annotation. Our method is inspired by a typical problem in extractive summarization, that the salient sentences are likely redundant (paraphrases) and need to be removed in the output summaries. We employ the scoring method used in SumBasic (Nenkova and Vanderwende, 2005; Vanderwende et al., 2007), a simple but powerful summarization system, to find salient sentences. For each topic, we compute the probability of each word P(wi) by simply dividing its frequency by the total number of all words in all sentences. Each sentence s is scored as the average of the probabilities of the words in it, i.e. P(wi) (1) |{wi|wi E s}| We then rank the sentences and pick the original sentence randomly from top 10% salient sentences and candidate sentences from top 50% to present to the annotators. In a trial experiment of 20 topics, the filtering technique double the yield of paraphrases from 152 �Sal</context>
</contexts>
<marker>Vanderwende, Suzuki, Brockett, Nenkova, 2007</marker>
<rawString>Vanderwende, L., Suzuki, H., Brockett, C., and Nenkova, A. (2007). Beyond SumBasic: Taskfocused summarization with sentence simplification and lexical expansion. Information Processing &amp; Management, 43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wan</author>
<author>M Dras</author>
<author>R Dale</author>
<author>C Paris</author>
</authors>
<title>Using dependency-based features to take the parafarce out of paraphrase.</title>
<date>2006</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop.</booktitle>
<contexts>
<context position="2496" citStr="Wan et al., 2006" startWordPosition="387" endWordPosition="390">ry; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent studies have highlighted the potential and importance of developing paraphrase identification (Zanzotto et al., 2011; Xu et al., 2013) and semantic similarity techniques (Guo and Diab, 2012) specifically for tweets. They also indicated that the very informal language, especially the high degree of lexical variation, used</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Wan, S., Dras, M., Dale, R., and Paris, C. (2006). Using dependency-based features to take the parafarce out of paraphrase. In Proceedings of the Australasian Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wang</author>
<author>C Dyer</author>
<author>A W Black</author>
<author>I Trancoso</author>
</authors>
<title>Paraphrasing 4 microblog normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2175" citStr="Wang et al., 2013" startWordPosition="341" endWordPosition="344">n useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2011). A few recent s</context>
</contexts>
<marker>Wang, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang, L., Dyer, C., Black, A. W., and Trancoso, I. (2013). Paraphrasing 4 microblog normalization. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
</authors>
<title>Data-Drive Approaches for Paraphrasing Across Language Variations.</title>
<date>2014</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, New York University.</institution>
<contexts>
<context position="4995" citStr="Xu, 2014" startWordPosition="783" endWordPosition="784">2.4%) Test 1295 972 175 (18.0%) 663 (68.2%) 134 (13.8%) Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection. The SemEval-2015 shared task on Paraphrase and Semantic Similarity In Twitter (PIT) uses a training and development set of 17,790 sentence pairs and a test set of 972 sentence pairs with paraphrase annotations (see examples in Table 1) that is the same as the Twitter Paraphrase Corpus we developed earlier in (Xu, 2014) and (Xu et al., 2014). This PIT-2015 paraphrase dataset is distinct from the data used in previous studies in many aspects: (i) it contains sentences that are opinionated and colloquial, representing realistic informal language usage; (ii) it contains paraphrases that are lexically diverse; and (iii) it contains sentences that are lexically similar but semantically dissimilar. It raises many interesting research questions and could lead to a better understanding of our daily used language and how semantics can be captured in such language. We believe that such a common testbed will facilitate</context>
<context position="6856" citStr="Xu, 2014" startWordPosition="1079" endWordPosition="1080">cation, we evaluate system performance by the F1 score (harmonic mean of precision and recall) against human judgements. Task B – Semantic Textual Similarity (SS) Given two sentences, determine a numerical score between 0 (no relation) and 1 (semantic equivalence) to indicate their semantic similarity. Following the literature, the system outputs are compared by Pearson correlation with human scores. We also compute the maximum F-1 score over the precision-recall curve as an additional data point. 2 3 Corpus In this shared task, we use the Twitter Paraphrase Corpus that we first presented in (Xu, 2014) and (Xu et al., 2014). Table 2 shows the basic statistics of the corpus. The sentences are preprocessed with tokenization,3 POS and named entity tags.4 The training and development set consists of 17,790 sentence pairs posted between April 24th and May 3rd, 2013 from 500+ trending topics featured on Twitter (excluding hashtags). The training and development set is a random split. Each sentence pair is annotated by 5 different crowdsourcing workers. For the test set, we obtain both crowdsourced and expert labels on 972 sentence pairs from 20 randomly sampled Twitter trending topics between May</context>
</contexts>
<marker>Xu, 2014</marker>
<rawString>Xu, W. (2014). Data-Drive Approaches for Paraphrasing Across Language Variations. PhD thesis, Department of Computer Science, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>A Ritter</author>
<author>C Callison-Burch</author>
<author>W B Dolan</author>
<author>Y Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from Twitter.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics (TACL),</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="5017" citStr="Xu et al., 2014" startWordPosition="786" endWordPosition="789"> 972 175 (18.0%) 663 (68.2%) 134 (13.8%) Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Debatable cases are those received a medium-score from annotators. The percentage of paraphrases is lower in the test set because it was constructed without topic selection. The SemEval-2015 shared task on Paraphrase and Semantic Similarity In Twitter (PIT) uses a training and development set of 17,790 sentence pairs and a test set of 972 sentence pairs with paraphrase annotations (see examples in Table 1) that is the same as the Twitter Paraphrase Corpus we developed earlier in (Xu, 2014) and (Xu et al., 2014). This PIT-2015 paraphrase dataset is distinct from the data used in previous studies in many aspects: (i) it contains sentences that are opinionated and colloquial, representing realistic informal language usage; (ii) it contains paraphrases that are lexically diverse; and (iii) it contains sentences that are lexically similar but semantically dissimilar. It raises many interesting research questions and could lead to a better understanding of our daily used language and how semantics can be captured in such language. We believe that such a common testbed will facilitate docking of the differ</context>
<context position="6878" citStr="Xu et al., 2014" startWordPosition="1082" endWordPosition="1085">uate system performance by the F1 score (harmonic mean of precision and recall) against human judgements. Task B – Semantic Textual Similarity (SS) Given two sentences, determine a numerical score between 0 (no relation) and 1 (semantic equivalence) to indicate their semantic similarity. Following the literature, the system outputs are compared by Pearson correlation with human scores. We also compute the maximum F-1 score over the precision-recall curve as an additional data point. 2 3 Corpus In this shared task, we use the Twitter Paraphrase Corpus that we first presented in (Xu, 2014) and (Xu et al., 2014). Table 2 shows the basic statistics of the corpus. The sentences are preprocessed with tokenization,3 POS and named entity tags.4 The training and development set consists of 17,790 sentence pairs posted between April 24th and May 3rd, 2013 from 500+ trending topics featured on Twitter (excluding hashtags). The training and development set is a random split. Each sentence pair is annotated by 5 different crowdsourcing workers. For the test set, we obtain both crowdsourced and expert labels on 972 sentence pairs from 20 randomly sampled Twitter trending topics between May 13th and June 10th, 2</context>
<context position="23983" citStr="Xu et al., 2014" startWordPosition="3857" endWordPosition="3860">ystem. In total, 19 teams participated in the PI task, of which 14 teams also participated in the SS task. Note that although the two sub-tasks share the same test set of 972 sentence pairs, the PI task ignores 134 debatable cases (received a medium-score from expert annotator) and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs. This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that the F1-scores of the baselines in the PI task are higher than reported in the Table 2 of (Xu et al., 2014), because the later reported maximum F1-scores on the PI task, ignoring the debatable cases. 7 gression models using features of semantic similarities between sentence pairs. ECNU (Zhao and Lan, 2015): This team adopts typical machine learning classifiers and uses a variety of features, such as surface text, semantic level, textual entailment, word distributional representations by deep learning methods. FBK-HLT (Ngoc Phuoc An Vo and Popescu, 2015): This team uses supervised learning model with different features for the 2 runs, such as n-gram overlap, word alignment and edit distance. Hassy: </context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Xu, W., Ritter, A., Callison-Burch, C., Dolan, W. B., and Ji, Y. (2014). Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics (TACL), 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>A Ritter</author>
<author>R Grishman</author>
</authors>
<title>Gathering and generating paraphrases from twitter with application to normalization.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora.</booktitle>
<contexts>
<context position="2155" citStr="Xu et al., 2013" startWordPosition="337" endWordPosition="340">ilarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam and Inkpen, 2007; Hassan and Mihalcea, 2</context>
</contexts>
<marker>Xu, Ritter, Grishman, 2013</marker>
<rawString>Xu, W., Ritter, A., and Grishman, R. (2013). Gathering and generating paraphrases from twitter with application to normalization. In Proceedings of the Sixth Workshop on Building and Using Comparable Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>M Pennacchiotti</author>
<author>K Tsioutsiouliklis</author>
</authors>
<title>Linguistic redundancy in twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2115" citStr="Zanzotto et al., 2011" startWordPosition="330" endWordPosition="333">) meaning, and the degree of their semantic similarity has proven useful for a wide variety of natural language processing applications (Madnani and Dorr, 2010). It 1http://www.cis.upenn.edu/˜xwe/ semeval2015pit/ is particularly useful to overcome the challenge of high redundancy in Twitter and the sparsity inherent in their short texts (e.g. oscar nom’d doc ↔ Oscarnominated documentary; some1 shot a cop ↔ someone shot a police). Emerging research shows paraphrasing techniques applied to Twitter data can improve tasks like first story detection (Petrovi´c et al., 2012), information retrieval (Zanzotto et al., 2011) and text normalization (Xu et al., 2013; Wang et al., 2013). Previously, many researchers have investigated ways of automatically detecting paraphrases on more formal texts, like newswire text. The ACL Wiki2 gives an excellent summary of the state-ofthe-art paraphrase identification techniques. These can be categorized into supervised methods (Qiu et al., 2006; Wan et al., 2006; Das and Smith, 2009; Socher et al., 2011; Blacoe and Lapata, 2012; Madnani et al., 2012; Ji and Eisenstein, 2013) and unsupervised methods (Mihalcea et al., 2006; Rus et al., 2008; Fernando and Stevenson, 2008; Islam </context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Tsioutsiouliklis, 2011</marker>
<rawString>Zanzotto, F. M., Pennacchiotti, M., and Tsioutsiouliklis, K. (2011). Linguistic redundancy in twitter. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zarrella</author>
<author>J Henderson</author>
<author>E M Merkhofer</author>
<author>L Strickhart</author>
</authors>
<title>MITRE: Seven systems for semantic similarity in tweets.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="25614" citStr="Zarrella et al., 2015" startWordPosition="4102" endWordPosition="4105"> similarity scores, the latter with an emphasis on aligned semantic roles comparison. MathLingBp: This team implements the alignand-penalize architecture described by Han et al. (2013) with slight modifications and makes use of several word similarity metrics. One metric relies on a mapping of words to vectors built from the Rovereto Twitter NGram corpus, another on a synonym list built from Wiktionary’s translations, while a third approach derives word similarity from concept graphs built using the 4lang lexicon and the Longman Dictionary of Contemporary English (Kornai et al., 2015). MITRE (Zarrella et al., 2015): A recurrent neural network models semantic similarity between sentences using the sequence of symmetric word alignments that maximize cosine similarity between word embeddings. We include features from local similarity of characters, random projection, matching word sequences, pooling of word embeddings, and alignment quality metrics. The resulting ensemble uses both semantic and string matching at many levels of granularity. RTM-DCU (Bicici, 2015): This team uses referential translation machines (RTM) and machine translation performance prediction system (MTPP) for predicting semantic simil</context>
</contexts>
<marker>Zarrella, Henderson, Merkhofer, Strickhart, 2015</marker>
<rawString>Zarrella, G., Henderson, J., Merkhofer, E. M., and Strickhart, L. (2015). MITRE: Seven systems for semantic similarity in tweets. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhao</author>
<author>M Lan</author>
</authors>
<title>ECNU: Boosting performance for paraphrase and semantic similarity in Twitter by leveraging word embeddings.</title>
<date>2015</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="24183" citStr="Zhao and Lan, 2015" startWordPosition="3887" endWordPosition="3890"> task ignores 134 debatable cases (received a medium-score from expert annotator) and uses only 838 pairs (663 paraphrases and 175 non-paraphrases) in evaluation, while SS task uses all 972 pairs. This causes that the F1-score in the PI task can be higher than the maximum F1-score in the SS task. Also note that the F1-scores of the baselines in the PI task are higher than reported in the Table 2 of (Xu et al., 2014), because the later reported maximum F1-scores on the PI task, ignoring the debatable cases. 7 gression models using features of semantic similarities between sentence pairs. ECNU (Zhao and Lan, 2015): This team adopts typical machine learning classifiers and uses a variety of features, such as surface text, semantic level, textual entailment, word distributional representations by deep learning methods. FBK-HLT (Ngoc Phuoc An Vo and Popescu, 2015): This team uses supervised learning model with different features for the 2 runs, such as n-gram overlap, word alignment and edit distance. Hassy: This team uses a bag-of-embeddings approach via supervised learning. Two sentences are first embedded into a vector space, and then the system computes the dot-product of the two sentence embeddings. </context>
</contexts>
<marker>Zhao, Lan, 2015</marker>
<rawString>Zhao, J. and Lan, M. (2015). ECNU: Boosting performance for paraphrase and semantic similarity in Twitter by leveraging word embeddings. In Proceedings of SemEval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>