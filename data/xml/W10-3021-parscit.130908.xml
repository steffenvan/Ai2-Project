<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013017">
<title confidence="0.994537">
A Simple Ensemble Method for Hedge Identification
</title>
<author confidence="0.987722">
Ferenc P. Szidarovszky&apos;, Ill´es Solt&apos;, Domonkos Tikk&apos;,2
</author>
<affiliation confidence="0.7415545">
&apos; Budapest University of Technology and Economics, Budapest, Hungary
2 Humboldt-Universit¨at zu Berlin, Berlin, Germany
</affiliation>
<email confidence="0.996849">
ferenc.szidarovszky@hotmail.com,{solt,tikk}@tmit.bme.hu
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961583333334">
We present in this paper a simple hedge
identification method and its application
on biomedical text. The problem at hand
is a subtask of CoNLL-2010 shared task.
Our solution consists of two classifiers, a
statistical one and a CRF model, and a
simple combination schema that combines
their predictions. We report in detail on
each component of our system and discuss
the results. We also show that a more so-
phisticated combination schema could im-
prove the F-score significantly.
</bodyText>
<sectionHeader confidence="0.910377" genericHeader="keywords">
1 Problem definition
</sectionHeader>
<bodyText confidence="0.999287322580645">
The CoNLL-2010 Shared Task focused on the
identification and localization of uncertain infor-
mation and its scope in text. In the first task, a
binary classification of sentences had to be per-
formed, based on whether they are uncertain or
not. The second task concentrated on the identi-
fication of the source of uncertainty – specifying
the keyword/phrase that makes its context uncer-
tain –, and the localization of its scope. The orga-
nizers provided training data from two application
domains: biomedical texts and Wikipedia articles.
For more details see the overview paper by the or-
ganizers (Farkas et al., 2010). We focused on task
1 and worked with biomedical texts exclusively.
The biomedical training corpus contains se-
lected abstracts and full text articles from the Bio-
Scope corpus (Vincze et al., 2008). The corpus
was manually annotated for hedge cues on the
phrase level. Sentences containing at least one cue
are considered as uncertain, while sentences with
no cues are considered as factual. Though cue tag-
ging was given in the training data, their marking
in the submission was not mandatory.
The evaluation of systems at task 1 was per-
formed on the sentence level with the F-measure
of the uncertain class being the official evaluation
metric. For evaluation, corpora also from both do-
mains were provided that allowed for in-domain
and cross-domain experiments as well. Neverthe-
less, we restricted the scope of our system to the
in-domain biomedical subtask.
</bodyText>
<sectionHeader confidence="0.982147" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.998225757575758">
Automatic information extraction methods may
incorrectly extract facts that are mentioned in a
negated or speculative context. If aiming at high
accuracy, it is therefore crucial to be able to clas-
sify assertions to avoid such false positives. The
importance of assertion classification has been re-
cently recognized by the text mining community,
which yielded several text-mining challenges cov-
ering this task. For example, the main task of
Obesity Challenge (Uzuner, 2008) was to iden-
tify based on a free text medical record whether
a patient is known to, speculated to or known
not to have a disease; in the BioNLP’09 Shared
Task (Kim et al., 2009), mentions of bio-molecular
events had to be classified as either positive or neg-
ative statements or speculations.
Approaches to tackle assertion classification
can be roughly organized into following classes:
rule based models (Chapman et al., 2001), sta-
tistical models (Szarvas, 2008), machine learning
(Medlock and Briscoe, 2007), though most con-
tributions can be seen as a combination of these
(Uzuner et al., 2009). Even when classifying sen-
tences, the most common approach is to look for
cues below the sentence-level (¨Ozg¨ur and Radev,
2009). The common in these approaches is that
they use a text representation richer than bag-of-
words, usually tokens from a fixed-width window
with additional surface features.
Evaluation of assertion classification is mostly
performed at the sentence level, where state-of-
the-art systems have been reported to achieve
an F-measure of 83–85% for hedge detection in
</bodyText>
<page confidence="0.980083">
144
</page>
<bodyText confidence="0.689067">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 144–147,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
biomedical literature (Medlock and Briscoe, 2007;
Szarvas, 2008).
</bodyText>
<sectionHeader confidence="0.996853" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.998186555555556">
Although the problem itself is a binary categoriza-
tion problem, we approach the problem at the to-
ken/phrase level. We search for hedge cues and
used the decision model also applied by the an-
notators of the training corpus: when a sentence
contains at least one uncertainty cue then it is un-
certain, otherwise factual.
We applied two different models to identify
hedge cues:
</bodyText>
<listItem confidence="0.848844">
• a statistical model that creates a candidate list
of cue words/phrases from the training sam-
ples, and cuts off the list based on the preci-
sion measured on the trial set;
• a sequence tagger CRF model, trained again
with hedge cues using various feature sets.
</listItem>
<bodyText confidence="0.9999521">
Finally, we combined the outputs of the meth-
ods at the sentence level. Here we applied two
very simple ways of combination: the aggres-
sive one assigns a sentence to the uncertain class
if any of the models finds a cue phrase therein
(OR merger), while the conservative only if both
models predict the sentence as uncertain (AND
merger). We submitted the version which pro-
duced better result on the trial set. The overview
of our system is depicted on Figure 1.
</bodyText>
<subsectionHeader confidence="0.998708">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999790722222222">
The biomedical corpus was provided in two
train/trial pairs (abstracts and full texts), see also
Table 1. Because the ratio of uncertain sentences
is similar in both train and trial sets, we merged
the two train sets and the two trial sets, respec-
tively, to obtain a single train/trial pair. Since the
trial set was originally included also in the train
set, we removed the elements of the merged trial
set from the merged train set. In the following, we
refer to them as train and trial sets. All data (train,
trial, evaluation) were given as separate sentences;
therefore no sentence segmentation had to be per-
formed.
Merging train and trial sets was also motivated
by the sparsity of data and the massively differ-
ent train/trial ratio observed for the two types of
biomedical texts (Table 1). Therefore building
separate models for abstracts and full texts may
</bodyText>
<figureCaption confidence="0.999818">
Figure 1: System overview
</figureCaption>
<bodyText confidence="0.991699">
only yield overfitting, particularly because such a
distinction is not available for the evaluation set.
</bodyText>
<subsectionHeader confidence="0.996157">
3.2 Statistical model
</subsectionHeader>
<bodyText confidence="0.999535421052632">
The statistical model considers a sentence uncer-
tain, if it contains at least one cue from a validated
set of cue phrases. To determine the set of cue
phrases to be used, we first collected all annotated
cues from the training data. From this candidate
cue set we retained those ones that had a precision
over a predefined threshold. To this end we mea-
sured on the training set the precision of each cue
phrase. We depicted on Figure 2 the precision, re-
call and F-measure values obtained on the trial set
with different cue phrase precision thresholds.
The candidate cue set contains 186 cue phrases,
among which 83 has precision 1.0 and 141 has
precision greater or equal 0.5. Best cue phrases
include words/phrases like cannot + verb phrase,
hypothesis, indicate, may, no(t) + verb/noun, raise
the + noun, seem, suggest, whether etc., while low
precision cues are, e.g., assume, not fully under-
stood, not, or, prediction, likelihood.
</bodyText>
<subsectionHeader confidence="0.959739">
3.3 CRF model
</subsectionHeader>
<bodyText confidence="0.999969">
Identifying entities such as speculation cues can be
efficiently solved by training conditional random
field (CRF) models. As a general sequence tagger,
a CRF can be naturally extended to incorporate to-
ken features and features of neighboring tokens.
The trained CRF model is then applied to unseen
</bodyText>
<figure confidence="0.995606363636364">
Input
Statistical
model
CRF model
OR merger
CRF
classification
Statistical
classification
Final
classification
</figure>
<page confidence="0.987079">
145
</page>
<table confidence="0.9745272">
Train set Trial set Evaluation set
sentences uncertain ratio sentences uncertain ratio sentences uncertain ratio
Abstract 11832 2091 17.7% 39 10 25.6% – – –
Full text 2 442 468 19.2% 228 51 22.4% – – –
Total 14 274 2 559 17.9% 267 61 22.9% 5 003 790 15.8%
</table>
<tableCaption confidence="0.999818">
Table 1: Basic statistics of the provided train, trial, and evaluation sets
</tableCaption>
<figure confidence="0.571777">
Threshold
</figure>
<figureCaption confidence="0.996722">
Figure 2: Cue phrase threshold selection
</figureCaption>
<bodyText confidence="0.999938375">
text, whenever a speculation cue is found the con-
taining sentence is annotated as being speculative.
In our experiments, we used MALLET (McCal-
lum, 2002) to train CRF models using custom to-
kenization (Section 3.3.1) and feature sets (Sec-
tion 3.3.2). We included features of 2–2 neigh-
boring tokens in each direction, not surpassing the
sentence limits.
</bodyText>
<subsectionHeader confidence="0.809798">
3.3.1 Tokenization
</subsectionHeader>
<bodyText confidence="0.999990666666667">
We split text into tokens using punctuation and
white-space tokenization, keeping punctuation
symbols as separate tokens.
</bodyText>
<subsectionHeader confidence="0.936532">
3.3.2 Feature sets
</subsectionHeader>
<bodyText confidence="0.993392">
We experimented with the following binary sur-
face features:
</bodyText>
<listItem confidence="0.9974637">
1. token text
2. token text in lowercase
3. stem of token in lowercase
4. indicator of the token being all lowercase
5. indicator whether the token is in sentence
case (first character upper-, others lowercase)
6. indicator whether the token contains at least
one digit
7. indicator of token being a punctuation sym-
bol
</listItem>
<bodyText confidence="0.9999345">
These features were evaluated both in isolation
and in combination on the trial set. The best per-
forming combination was then used to train the fi-
nal model.
</bodyText>
<subsectionHeader confidence="0.932898">
3.3.3 Feature selection
</subsectionHeader>
<bodyText confidence="0.999926428571429">
Evaluating all combinations of the above features,
we found that the combination of features 2 and 4
produced the best results on the trial set. For com-
putational efficiency, when selecting the best per-
forming feature subset, we considered lower fea-
ture count to overrule a slight increase in perfor-
mance.
</bodyText>
<sectionHeader confidence="0.999916" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999680222222222">
Table 2 and Table 3 summarize the results for the
statistical and CRF models and their AND and OR
combinations on the trial and on the evaluation
sets, respectively. For the latter, we used naturally
all available labeled data (train and trial sets) for
training. Numbers shown correspond to the out-
put of the official evaluation tool. Results on the
combination OR represent our official shared task
evaluation.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99993385">
In the development scenario (Table 2), the main
difference between the statistical and CRF model
was that the former was superior in recall while the
latter in precision. It was thus unclear which of the
combinations OR and AND would perform better,
we chose OR, the combination method which per-
formed better on the trial set. Unfortunately, the
rank of combination methods was different when
measured on the evaluation set (Table 3). A possi-
ble explanation for this non-extrapolability is the
different prior probability of speculative sentences
in each set, e.g., 17.9% on the train set while
22.9% on the trial set and 15.8% on the evaluation
set.
While using only a minimal amount of features,
both of our models were on par with other partic-
ipants’ solutions. Overfitting was observed by the
statistical model only (14% drop in precision on
the evaluation set), the CRF model showed more
consistent behavior across the datasets.
</bodyText>
<figure confidence="0.9984288125">
0.00 0.20 0.40 0.60 0.80 1.00
Percentage
100
90
70
40
30
80
60
50
85.71 86.40
73.62
78.57
F-Measure
Precision
Recall
</figure>
<page confidence="0.972698">
146
</page>
<table confidence="0.9400082">
Model
Statistical CRF Combination AND Combination OR
Precision (%) 84.4 92.3 93.9 83.6
Recall (%) 88.6 78.7 75.4 91.8
F-measure (%) 86.4 85.0 83.6 87.5
</table>
<tableCaption confidence="0.991492">
Table 2: Results on trial set (development)
</tableCaption>
<table confidence="0.9627788">
Model
Statistical CRF Combination AND Combination OR
Precision (%) 70.5 87.0 88.0 70.1
Recall (%) 89.4 82.7 81.0 91.0
F-measure (%) 78.8 84.8 84.4 79.2
</table>
<tableCaption confidence="0.999878">
Table 3: Results on evaluation set
</tableCaption>
<sectionHeader confidence="0.997933" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995166666667">
We presented our method to identify hedging in
biomedical literature, and its evaluation at the
CoNLL-2010 shared task. We solved the sen-
tence level assertion classification problem by us-
ing an ensemble of statistical and CRF mod-
els that identify speculation cue phrases. The
non-extrapolability of the combination methods’
performance observed emphasizes the sensitivity
of ensemble methods to the distributions of the
datasets they are applied to. While using only a
minimal set of standard surface features, our CRF
model was on par with participants’ systems.
</bodyText>
<sectionHeader confidence="0.969629" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.8624285">
D. Tikk was supported by the Alexander-von-
Humboldt Foundation.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996556166666666">
Wendy W. Chapman, Will Bridewell, Paul Hanbury,
Gregory F. Cooper, and Bruce G. Buchanan. 2001.
A simple algorithm for identifying negated findings
and diseases in discharge summaries. Journal of
Biomedical Informatics, 2001:34–301.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1–12, Uppsala, Sweden. ACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 shared task on event extraction. In
BioNLP ’09: Proc. of the Workshop on BioNLP,
pages 1–9, Morristown, NJ, USA. ACL.
Andrew K. McCallum. 2002. MALLET: A Ma-
chine Learning for Language Toolkit. http://
mallet.cs.umass.edu.
Ben Medlock and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific
literature. In Proc. of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992–999, Prague, Czech Republic, June. ACL.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In EMNLP ’09: Proc. of Conf. on Empirical Meth-
ods in Natural Language Processing, pages 1398–
1407, Morristown, NJ, USA. ACL.
Gy¨orgy Szarvas. 2008. Hedge Classification in
Biomedical Texts with a Weakly Supervised Selec-
tion of Keywords. In Proceedings of ACL-08: HLT,
pages 281–289, Columbus, Ohio, June. ACL.
¨Ozlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda.
2009. Machine Learning and Rule-based Ap-
proaches to Assertion Classification. Journal
of the American Medical Informatics Association,
16(1):109–115.
¨Ozlem Uzuner. 2008. Second i2b2 workshop on
natural language processing challenges for clinical
records. In AMIA Annual Symposium Proceedings,
pages 1252–3.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
</reference>
<page confidence="0.998098">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000187">
<title confidence="0.999973">A Simple Ensemble Method for Hedge Identification</title>
<author confidence="0.976956">P Ill´es Domonkos</author>
<affiliation confidence="0.999016">University of Technology and Economics, Budapest,</affiliation>
<address confidence="0.876665">2Humboldt-Universit¨at zu Berlin, Berlin,</address>
<abstract confidence="0.99081706557377">We present in this paper a simple hedge identification method and its application on biomedical text. The problem at hand is a subtask of CoNLL-2010 shared task. Our solution consists of two classifiers, a statistical one and a CRF model, and a simple combination schema that combines their predictions. We report in detail on each component of our system and discuss the results. We also show that a more sophisticated combination schema could improve the F-score significantly. 1 Problem definition The CoNLL-2010 Shared Task focused on the identification and localization of uncertain information and its scope in text. In the first task, a binary classification of sentences had to be performed, based on whether they are uncertain or not. The second task concentrated on the identification of the source of uncertainty – specifying the keyword/phrase that makes its context uncertain –, and the localization of its scope. The organizers provided training data from two application domains: biomedical texts and Wikipedia articles. For more details see the overview paper by the organizers (Farkas et al., 2010). We focused on task 1 and worked with biomedical texts exclusively. The biomedical training corpus contains selected abstracts and full text articles from the Bio- Scope corpus (Vincze et al., 2008). The corpus manually annotated for cues the phrase level. Sentences containing at least one cue are considered as uncertain, while sentences with no cues are considered as factual. Though cue tagging was given in the training data, their marking in the submission was not mandatory. The evaluation of systems at task 1 was performed on the sentence level with the F-measure of the uncertain class being the official evaluation metric. For evaluation, corpora also from both domains were provided that allowed for in-domain and cross-domain experiments as well. Nevertheless, we restricted the scope of our system to the in-domain biomedical subtask. 2 Background Automatic information extraction methods may incorrectly extract facts that are mentioned in a negated or speculative context. If aiming at high accuracy, it is therefore crucial to be able to classify assertions to avoid such false positives. The importance of assertion classification has been recently recognized by the text mining community, which yielded several text-mining challenges covering this task. For example, the main task of Obesity Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Even when classifying sentences, the most common approach is to look for below the sentence-level and Radev, 2009). The common in these approaches is that they use a text representation richer than bag-ofwords, usually tokens from a fixed-width window with additional surface features. Evaluation of assertion classification is mostly performed at the sentence level, where state-ofthe-art systems have been reported to achieve an F-measure of 83–85% for hedge detection in 144 of the Fourteenth Conference on Computational Natural Language Learning: Shared pages Sweden, 15-16 July 2010. Association for Computational Linguistics biomedical literature (Medlock and Briscoe, 2007; Szarvas, 2008). 3 Methods Although the problem itself is a binary categorization problem, we approach the problem at the token/phrase level. We search for hedge cues and used the decision model also applied by the annotators of the training corpus: when a sentence contains at least one uncertainty cue then it is uncertain, otherwise factual. We applied two different models to identify hedge cues: a model creates a candidate list of cue words/phrases from the training samples, and cuts off the list based on the precision measured on the trial set; a sequence tagger trained again with hedge cues using various feature sets. Finally, we combined the outputs of the methods at the sentence level. Here we applied two very simple ways of combination: the aggressive one assigns a sentence to the uncertain class if any of the models finds a cue phrase therein (OR merger), while the conservative only if both models predict the sentence as uncertain (AND merger). We submitted the version which produced better result on the trial set. The overview of our system is depicted on Figure 1. 3.1 Preprocessing The biomedical corpus was provided in two train/trial pairs (abstracts and full texts), see also Table 1. Because the ratio of uncertain sentences is similar in both train and trial sets, we merged the two train sets and the two trial sets, respectively, to obtain a single train/trial pair. Since the trial set was originally included also in the train set, we removed the elements of the merged trial set from the merged train set. In the following, we to them as All data (train, trial, evaluation) were given as separate sentences; therefore no sentence segmentation had to be performed. Merging train and trial sets was also motivated by the sparsity of data and the massively different train/trial ratio observed for the two types of biomedical texts (Table 1). Therefore building separate models for abstracts and full texts may Figure 1: System overview only yield overfitting, particularly because such a distinction is not available for the evaluation set. 3.2 Statistical model The statistical model considers a sentence uncertain, if it contains at least one cue from a validated set of cue phrases. To determine the set of cue phrases to be used, we first collected all annotated cues from the training data. From this candidate cue set we retained those ones that had a precision over a predefined threshold. To this end we measured on the training set the precision of each cue phrase. We depicted on Figure 2 the precision, recall and F-measure values obtained on the trial set with different cue phrase precision thresholds. The candidate cue set contains 186 cue phrases, among which 83 has precision 1.0 and 141 has precision greater or equal 0.5. Best cue phrases words/phrases like + verb phrase, hypothesis, indicate, may, no(t) + verb/noun, raise + noun, seem, suggest, whether while low cues are, e.g., not fully undernot, or, prediction, 3.3 CRF model Identifying entities such as speculation cues can be efficiently solved by training conditional random field (CRF) models. As a general sequence tagger, a CRF can be naturally extended to incorporate token features and features of neighboring tokens. The trained CRF model is then applied to unseen Input Statistical model CRF model classification Statistical classification classification 145 Train set Trial set Evaluation set sentences uncertain ratio sentences uncertain ratio sentences uncertain ratio Abstract 11832 2091 17.7% 39 10 25.6% – – – Full text 2 442 468 19.2% 228 51 22.4% – – – Total 14 274 2 559 17.9% 267 61 22.9% 5 003 790 15.8% Table 1: Basic statistics of the provided train, trial, and evaluation sets Threshold Figure 2: Cue phrase threshold selection text, whenever a speculation cue is found the containing sentence is annotated as being speculative. In our experiments, we used MALLET (McCallum, 2002) to train CRF models using custom tokenization (Section 3.3.1) and feature sets (Section 3.3.2). We included features of 2–2 neighboring tokens in each direction, not surpassing the sentence limits. 3.3.1 Tokenization We split text into tokens using punctuation and white-space tokenization, keeping punctuation symbols as separate tokens. 3.3.2 Feature sets We experimented with the following binary surface features: 1. token text 2. token text in lowercase 3. stem of token in lowercase 4. indicator of the token being all lowercase 5. indicator whether the token is in sentence case (first character upper-, others lowercase) 6. indicator whether the token contains at least one digit 7. indicator of token being a punctuation symbol These features were evaluated both in isolation and in combination on the trial set. The best performing combination was then used to train the final model. 3.3.3 Feature selection Evaluating all combinations of the above features, we found that the combination of features 2 and 4 produced the best results on the trial set. For computational efficiency, when selecting the best performing feature subset, we considered lower feature count to overrule a slight increase in performance. 4 Results Table 2 and Table 3 summarize the results for the statistical and CRF models and their AND and OR combinations on the trial and on the evaluation sets, respectively. For the latter, we used naturally all available labeled data (train and trial sets) for training. Numbers shown correspond to the output of the official evaluation tool. Results on the combination OR represent our official shared task evaluation. 5 Discussion In the development scenario (Table 2), the main difference between the statistical and CRF model was that the former was superior in recall while the latter in precision. It was thus unclear which of the combinations OR and AND would perform better, we chose OR, the combination method which performed better on the trial set. Unfortunately, the rank of combination methods was different when measured on the evaluation set (Table 3). A possible explanation for this non-extrapolability is the different prior probability of speculative sentences in each set, e.g., 17.9% on the train set while 22.9% on the trial set and 15.8% on the evaluation set. While using only a minimal amount of features, both of our models were on par with other participants’ solutions. Overfitting was observed by the statistical model only (14% drop in precision on the evaluation set), the CRF model showed more consistent behavior across the datasets.</abstract>
<note confidence="0.806110321428571">0.00 0.20 0.40 0.60 0.80 1.00 Percentage 100 90 70 40 30 80 60 50 85.71 86.40 73.62 78.57 F-Measure Precision Recall 146 Model Statistical CRF Combination AND Combination OR Precision (%) 84.4 92.3 93.9 83.6 Recall (%) 88.6 78.7 75.4 91.8 F-measure (%) 86.4 85.0 83.6 87.5 Table 2: Results on trial set (development) Model Statistical CRF Combination AND Combination OR Precision (%) 70.5 87.0 88.0 70.1 Recall (%) 89.4 82.7 81.0 91.0 F-measure (%) 78.8 84.8 84.4 79.2</note>
<abstract confidence="0.952383882352941">Table 3: Results on evaluation set 6 Conclusion We presented our method to identify hedging in biomedical literature, and its evaluation at the CoNLL-2010 shared task. We solved the sentence level assertion classification problem by using an ensemble of statistical and CRF models that identify speculation cue phrases. The non-extrapolability of the combination methods’ performance observed emphasizes the sensitivity of ensemble methods to the distributions of the datasets they are applied to. While using only a minimal set of standard surface features, our CRF model was on par with participants’ systems. Acknowledgement D. Tikk was supported by the Alexander-von- Humboldt Foundation.</abstract>
<title confidence="0.699154">References</title>
<note confidence="0.8573515">A simple algorithm for identifying negated findings diseases in discharge summaries. of 2001:34–301. Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL- 2010 Shared Task: Learning to Detect Hedges and Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared pages 1–12, Uppsala, Sweden. ACL. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 shared task on event extraction. In ’09: Proc. of the Workshop on pages 1–9, Morristown, NJ, USA. ACL. Andrew K. McCallum. 2002. MALLET: A Ma-</note>
<title confidence="0.906331">Learning for Language Toolkit.</title>
<author confidence="0.908604">Weakly super-</author>
<abstract confidence="0.657482833333333">vised learning for hedge classification in scientific In of the 45th Annual Meeting of Association of Computational pages 992–999, Prague, Czech Republic, June. ACL. and Dragomir R. Radev. 2009. Detecting speculations and their scopes in scientific text.</abstract>
<note confidence="0.912356181818182">09: Proc. of Conf. on Empirical Methin Natural Language pages 1398– 1407, Morristown, NJ, USA. ACL. Gy¨orgy Szarvas. 2008. Hedge Classification in Biomedical Texts with a Weakly Supervised Selecof Keywords. In of ACL-08: pages 281–289, Columbus, Ohio, June. ACL. Uzuner, Xiaoran Zhang, and Tawanda Sibanda. Machine Learning and Rule-based Apto Assertion Classification. the American Medical Informatics 16(1):109–115. Uzuner. 2008. Second i2b2 workshop on natural language processing challenges for clinical In Annual Symposium pages 1252–3. Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The Bio- Scope corpus: biomedical texts annotated for uncernegation and their scopes. Bioinformat- 9(Suppl 11):S9. 147</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wendy W Chapman</author>
<author>Will Bridewell</author>
<author>Paul Hanbury</author>
<author>Gregory F Cooper</author>
<author>Bruce G Buchanan</author>
</authors>
<title>A simple algorithm for identifying negated findings and diseases in discharge summaries.</title>
<date>2001</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>2001--34</pages>
<contexts>
<context position="3180" citStr="Chapman et al., 2001" startWordPosition="495" endWordPosition="498">ication has been recently recognized by the text mining community, which yielded several text-mining challenges covering this task. For example, the main task of Obesity Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Even when classifying sentences, the most common approach is to look for cues below the sentence-level (¨Ozg¨ur and Radev, 2009). The common in these approaches is that they use a text representation richer than bag-ofwords, usually tokens from a fixed-width window with additional surface features. Evaluation of assertion classification is mostly performed at the sentence level, where state-ofthe-art systems have been reporte</context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>Wendy W. Chapman, Will Bridewell, Paul Hanbury, Gregory F. Cooper, and Bruce G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of Biomedical Informatics, 2001:34–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1–12, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proc. of the Workshop on BioNLP,</booktitle>
<pages>1--9</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2930" citStr="Kim et al., 2009" startWordPosition="459" endWordPosition="462">methods may incorrectly extract facts that are mentioned in a negated or speculative context. If aiming at high accuracy, it is therefore crucial to be able to classify assertions to avoid such false positives. The importance of assertion classification has been recently recognized by the text mining community, which yielded several text-mining challenges covering this task. For example, the main task of Obesity Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Even when classifying sentences, the most common approach is to look for cues below the sentence-level (¨Ozg¨ur and Radev, 2009). The common in these approaches is that they use a</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 shared task on event extraction. In BioNLP ’09: Proc. of the Workshop on BioNLP, pages 1–9, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<note>http:// mallet.cs.umass.edu.</note>
<contexts>
<context position="8145" citStr="McCallum, 2002" startWordPosition="1314" endWordPosition="1316">R merger CRF classification Statistical classification Final classification 145 Train set Trial set Evaluation set sentences uncertain ratio sentences uncertain ratio sentences uncertain ratio Abstract 11832 2091 17.7% 39 10 25.6% – – – Full text 2 442 468 19.2% 228 51 22.4% – – – Total 14 274 2 559 17.9% 267 61 22.9% 5 003 790 15.8% Table 1: Basic statistics of the provided train, trial, and evaluation sets Threshold Figure 2: Cue phrase threshold selection text, whenever a speculation cue is found the containing sentence is annotated as being speculative. In our experiments, we used MALLET (McCallum, 2002) to train CRF models using custom tokenization (Section 3.3.1) and feature sets (Section 3.3.2). We included features of 2–2 neighboring tokens in each direction, not surpassing the sentence limits. 3.3.1 Tokenization We split text into tokens using punctuation and white-space tokenization, keeping punctuation symbols as separate tokens. 3.3.2 Feature sets We experimented with the following binary surface features: 1. token text 2. token text in lowercase 3. stem of token in lowercase 4. indicator of the token being all lowercase 5. indicator whether the token is in sentence case (first charac</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http:// mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>992--999</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="3262" citStr="Medlock and Briscoe, 2007" startWordPosition="506" endWordPosition="509">ded several text-mining challenges covering this task. For example, the main task of Obesity Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Even when classifying sentences, the most common approach is to look for cues below the sentence-level (¨Ozg¨ur and Radev, 2009). The common in these approaches is that they use a text representation richer than bag-ofwords, usually tokens from a fixed-width window with additional surface features. Evaluation of assertion classification is mostly performed at the sentence level, where state-ofthe-art systems have been reported to achieve an F-measure of 83–85% for hedge detection in 144 Proceedings of the </context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999, Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan ¨Ozg¨ur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Detecting speculations and their scopes in scientific text.</title>
<date>2009</date>
<booktitle>In EMNLP ’09: Proc. of Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1398--1407</pages>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<marker>¨Ozg¨ur, Radev, 2009</marker>
<rawString>Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting speculations and their scopes in scientific text. In EMNLP ’09: Proc. of Conf. on Empirical Methods in Natural Language Processing, pages 1398– 1407, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge Classification in Biomedical Texts with a Weakly Supervised Selection of Keywords.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>281--289</pages>
<publisher>ACL.</publisher>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3216" citStr="Szarvas, 2008" startWordPosition="502" endWordPosition="503"> text mining community, which yielded several text-mining challenges covering this task. For example, the main task of Obesity Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Even when classifying sentences, the most common approach is to look for cues below the sentence-level (¨Ozg¨ur and Radev, 2009). The common in these approaches is that they use a text representation richer than bag-ofwords, usually tokens from a fixed-width window with additional surface features. Evaluation of assertion classification is mostly performed at the sentence level, where state-ofthe-art systems have been reported to achieve an F-measure of 83–85% </context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Gy¨orgy Szarvas. 2008. Hedge Classification in Biomedical Texts with a Weakly Supervised Selection of Keywords. In Proceedings of ACL-08: HLT, pages 281–289, Columbus, Ohio, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem Uzuner</author>
<author>Xiaoran Zhang</author>
<author>Tawanda Sibanda</author>
</authors>
<title>Machine Learning and Rule-based Approaches to Assertion Classification.</title>
<date>2009</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="3349" citStr="Uzuner et al., 2009" startWordPosition="522" endWordPosition="525">Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Even when classifying sentences, the most common approach is to look for cues below the sentence-level (¨Ozg¨ur and Radev, 2009). The common in these approaches is that they use a text representation richer than bag-ofwords, usually tokens from a fixed-width window with additional surface features. Evaluation of assertion classification is mostly performed at the sentence level, where state-ofthe-art systems have been reported to achieve an F-measure of 83–85% for hedge detection in 144 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 14</context>
</contexts>
<marker>Uzuner, Zhang, Sibanda, 2009</marker>
<rawString>¨Ozlem Uzuner, Xiaoran Zhang, and Tawanda Sibanda. 2009. Machine Learning and Rule-based Approaches to Assertion Classification. Journal of the American Medical Informatics Association, 16(1):109–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozlem Uzuner</author>
</authors>
<title>Second i2b2 workshop on natural language processing challenges for clinical records.</title>
<date>2008</date>
<booktitle>In AMIA Annual Symposium Proceedings,</booktitle>
<pages>1252--3</pages>
<contexts>
<context position="2753" citStr="Uzuner, 2008" startWordPosition="426" endWordPosition="427">and cross-domain experiments as well. Nevertheless, we restricted the scope of our system to the in-domain biomedical subtask. 2 Background Automatic information extraction methods may incorrectly extract facts that are mentioned in a negated or speculative context. If aiming at high accuracy, it is therefore crucial to be able to classify assertions to avoid such false positives. The importance of assertion classification has been recently recognized by the text mining community, which yielded several text-mining challenges covering this task. For example, the main task of Obesity Challenge (Uzuner, 2008) was to identify based on a free text medical record whether a patient is known to, speculated to or known not to have a disease; in the BioNLP’09 Shared Task (Kim et al., 2009), mentions of bio-molecular events had to be classified as either positive or negative statements or speculations. Approaches to tackle assertion classification can be roughly organized into following classes: rule based models (Chapman et al., 2001), statistical models (Szarvas, 2008), machine learning (Medlock and Briscoe, 2007), though most contributions can be seen as a combination of these (Uzuner et al., 2009). Ev</context>
</contexts>
<marker>Uzuner, 2008</marker>
<rawString>¨Ozlem Uzuner. 2008. Second i2b2 workshop on natural language processing challenges for clinical records. In AMIA Annual Symposium Proceedings, pages 1252–3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<pages>11--9</pages>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>