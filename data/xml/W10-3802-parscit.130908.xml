<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.996872">
A Systematic Comparison between Inversion Transduction Grammar
and Linear Transduction Grammar for Word Alignment
</title>
<author confidence="0.758957">
Markus Saers and Joakim Nivre Dekai Wu
</author>
<affiliation confidence="0.70037275">
Dept. of Linguistics &amp; Philology HKUST
Uppsala University Human Language Technology Center
first.last@lingfil.uu.se Dept. of Computer Science &amp; Engineering
Hong Kong Univ. of Science &amp; Technology
</affiliation>
<email confidence="0.854382">
dekai@cs.ust.hk
</email>
<sectionHeader confidence="0.995962" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999868095238095">
We present two contributions to gram-
mar driven translation. First, since both
Inversion Transduction Grammar and
Linear Inversion Transduction Gram-
mars have been shown to produce bet-
ter alignments then the standard word
alignment tool, we investigate how the
trade-off between speed and end-to-end
translation quality extends to the choice
of grammar formalism. Second, we
prove that Linear Transduction Gram-
mars (LTGs) generate the same transduc-
tions as Linear Inversion Transduction
Grammars, and present a scheme for ar-
riving at LTGs by bilingualizing Linear
Grammars. We also present a method for
obtaining Inversion Transduction Gram-
mars from Linear (Inversion) Transduc-
tion Grammars, which can speed up
grammar induction from parallel corpora
dramatically.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994085">
In this paper we introduce Linear Transduction
Grammars (LTGs), which are the bilingual case
of Linear Grammars (LGs). We also show that
LTGs are equal to Linear Inversion Transduction
Grammars (Saers et al., 2010). To be able to in-
duce transduction grammars directly from par-
allel corpora an approximate search for parses is
needed. The trade-off between speed and end-to-
end translation quality is investigated and com-
pared to Inversion Transduction Grammars (Wu,
1997) and the standard tool for word alignment,
GIZA++ (Brown et al., 1993; Vogel et al., 1996;
Och and Ney, 2003). A heuristic for converting
stochastic bracketing LTGs into stochastic brack-
eting ITGs is presented, and fitted into the speed–
quality trade-off.
In section 3 we give an overview of transduc-
tion grammars, introduce LTGs and show that
they are equal to LITGs. In section 4 we give
a short description of the rational for the trans-
duction grammar pruning used. In section 5 we
describe a way of seeding a stochastic bracketing
ITG with the rules and probabilities of a stochas-
tic bracketing LTG. Section 6 describes the setup,
and results are given in section 7. Finally, some
conclusions are offered in section 8
</bodyText>
<sectionHeader confidence="0.995478" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999937684210526">
Any form of automatic translation that relies on
generalizations of observed translations needs to
align these translations on a sub-sentential level.
The standard way of doing this is by aligning
words, which works well for languages that use
white space separators between words. The stan-
dard method is a combination of the family of
IBM-models (Brown et al., 1993) and Hidden
Markov Models (Vogel et al., 1996). These
methods all arrive at a function (A) from lan-
guage 1 (F) to language 2 (E). By running the
process in both directions, two functions can be
estimated and then combined to form an align-
ment. The simplest of these combinations are in-
tersection and union, but usually, the intersection
is heuristically extended. Transduction gram-
mars on the other hand, impose a shared struc-
ture on the sentence pairs, thus forcing a consis-
tent alignment in both directions. This method
</bodyText>
<page confidence="0.986176">
10
</page>
<note confidence="0.8535105">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18,
COLING 2010, Beijing, August 2010.
</note>
<bodyText confidence="0.983333333333333">
has proved successful in the settings it has been
tried (Zhang et al., 2008; Saers and Wu, 2009;
Haghighi et al., 2009; Saers et al., 2009; Saers
et al., 2010). Most efforts focus on cutting down
time complexity so that larger data sets than toy-
examples can be processed.
</bodyText>
<sectionHeader confidence="0.994965" genericHeader="method">
3 Transduction Grammars
</sectionHeader>
<bodyText confidence="0.999976636363636">
Transduction grammars were first introduced in
Lewis and Stearns (1968), and further devel-
oped in Aho and Ullman (1972). The origi-
nal notation called for regular CFG-rules in lan-
guage F with rephrased E productions, either in
curly brackets, or comma separated. The bilin-
gual version of CFGs is called Syntax-Directed
Transduction Grammars (SDTGs). To differenti-
ate identical nonterminal symbols, indices were
used (the bag of nonterminals for the two pro-
ductions are equal by definition).
</bodyText>
<equation confidence="0.982988">
A → B(1) a B(2) {x B(1) B(2)}
= A → B(1) a B(2), x B(1) B(2)
</equation>
<bodyText confidence="0.9999547">
The semantics of the rules is that one nontermi-
nal rewrites into a bag of nonterminals that is dis-
tributed independently in the two languages, and
interspersed with any number of terminal sym-
bols in the respective languages. As with CFGs,
the terminal symbols can be factored out into
preterminals with the added twist that they are
shared between the two languages, since preter-
minals are formally nonterminals. The above
rule can thus be rephrased as
</bodyText>
<equation confidence="0.9984825">
A → B(1) Xa/x B(2), Xa/x B(1) B(2)
Xa/x → a, x
</equation>
<bodyText confidence="0.999910333333333">
In this way, rules producing nonterminals and
rules producing terminals can be separated.
Since only nonterminals are allowed to move,
their movement can be represented as the orig-
inal sequence of nonterminals and a permutation
vector as follows:
</bodyText>
<equation confidence="0.700757">
A → B Xa/x B ; 1, 0, 2
Xa/x → a, x
</equation>
<bodyText confidence="0.999807833333333">
To keep the reordering as monotone as possible,
the terminals a and x can be produced separately,
but doing so eliminates any possibility of param-
eterizing their lexical relationship. Instead, the
individual terminals are pair up with the empty
string (ǫ).
</bodyText>
<equation confidence="0.993918333333333">
A → Xx B Xa B ; 0, 1, 2,3
Xa → a, ǫ
Xx → ǫ, x
</equation>
<bodyText confidence="0.999917256410256">
Lexical rules involving the empty string are re-
ferred to as singletons. Whenever a preterminal
is used to pair up two terminal symbols, we refer
to that pair of terminals as a biterminal, which
will be written as e/f.
Any SDTG can be rephrased to contain per-
muted nonterminal productions and biterminal
productions only, and we will call this the nor-
mal form of SDTGs. Note that it is not possi-
ble to produce a two-normal form for SDTGs,
as there are some rules that are not binarizable
(Wu, 1997; Huang et al., 2009). This is an
important point to make, since efficient parsing
for CFGs is based on either restricting parsing
to only handle binary grammars (Cocke, 1969;
Kasami, 1965; Younger, 1967), or rely on on-
the-fly binarization (Earley, 1970). When trans-
lating with a grammar, parsing only has to be
done in F, which is binarizable (since it is a
CFG), and can therefor be computed in polyno-
mial time (O(n3)). Once there is a parse tree
for F, the corresponding tree for E can be eas-
ily constructed. When inducing a grammar from
examples, however, biparsing (finding an anal-
ysis that is consistent across a sentence pair) is
needed. The time complexity for biparsing with
SDTGs is O(n2n+2), which is clearly intractable.
Inversion Transduction Grammars or ITGs
(Wu, 1997) are transduction grammars that have
a two-normal form, thus guaranteeing binariz-
ability. Defining the rank of a rule as the number
of nonterminals in the production, and the rank
of a grammar as the highest ranking rule in the
rule set, ITGs are a) any SDTG of rank two, b)
any SDTG of rank three or c) any SDTG where no
rule has a permutation vector other than identity
permutation or inversion permutation. It follows
from this definition that ITGs have a two-normal
form, which is usually expressed as SDTG rules,
</bodyText>
<page confidence="0.998557">
11
</page>
<bodyText confidence="0.9826621875">
with brackets around the production to distin-
guish the different kinds of rules from each other.
Graphically, we will represent LTG rules as pro-
duction rules with biterminals:
A → B C ; 0,1 = A → [ B C ]
A → B C ; 1,0 = A → h B C i
A → e/f = A → e/f
By guaranteeing binarizability, biparsing time
complexity becomes O(n6).
There is an even more restricted version of
SDTGs called Simple Transduction Grammar
(STG), where no permutation at all is allowed,
which can also biparse a sentence pair in O(n6)
time.
A Linear Transduction Grammar (LTG) is a
bilingual version of a Linear Grammar (LG).
</bodyText>
<equation confidence="0.590494">
Definition 1. An LG in normal form is a tuple
GL = hN, E, R, Si
</equation>
<bodyText confidence="0.99844475">
Where N is a finite set of nonterminal symbols,
E is a finite set of terminal symbols, R is a finite
set of rules and S ∈ N is the designated start
symbol. The rule set is constrained so that
</bodyText>
<equation confidence="0.928422">
R ⊆ N × (E ∪ {E})N(E ∪ {E}) ∪ {E}
Where E is the empty string.
</equation>
<bodyText confidence="0.9960208">
To bilingualize a linear grammar, we will take
the same approach as taken when a finite-state
automaton is bilingualized into a finite-state
transducer. That is: to replace all terminal sym-
bols with biterminal symbols.
</bodyText>
<construct confidence="0.523186">
Definition 2. An LTG in normal form is a tuple
TGL = hN, E, A, R, Si
</construct>
<bodyText confidence="0.996897166666667">
Where N is a finite set of nonterminal symbols,
E is a finite set of terminal symbols in language
E, A is a finite set of terminal symbols in lan-
guage F, R is a finite set of linear transduction
rules and S ∈ N is the designated start symbol.
The rule set is constrained so that
</bodyText>
<equation confidence="0.890917">
R ⊆ N × IFNIF ∪ {hE, Ei}
</equation>
<bodyText confidence="0.999316736842105">
Where I F= E∪{E}×A∪{E} and E is the empty
string.
hA, hx,piBhy, qii = A → x/p B y/q
hA, hE, Eii = B → E/E
Like STGs, LTGs do not allow any reordering,
and are monotone, but because they are linear,
this has no impact on expressiveness, as we shall
see later.
Linear Inversion Transduction Grammars
(LITGs) were introduced in Saers et al. (2010),
and represent ITGs that are allowed to have at
most one nonterminal symbol in each produc-
tion. These are attractive because they can bi-
parse a sentence pair in O(n4) time, which can
be further reduced to linear time by severely
pruning the search space. This makes them
tractable for large parallel corpora, and a viable
way to induce transduction grammars from large
parallel corpora.
</bodyText>
<construct confidence="0.6184415">
Definition 3. An LITG in normal form is a tuple
TGLI = hN, E, A, R, Si
</construct>
<bodyText confidence="0.994310666666667">
Where N is a finite set of nonterminal symbols,
E is a finite set of terminal symbols from lan-
guage E, A is a finite set of terminal symbols
from language F, R is a set of rules and S ∈ N
is the designated start symbol. The rule set is
constrained so that
</bodyText>
<equation confidence="0.898003">
R ⊆ N × {[], hi} × IFN ∪ NIF ∪ {hE, Ei}
</equation>
<bodyText confidence="0.999678076923077">
Where [] represents identity permutation and hi
represents inversion permutation, IF = E∪{E}×
A ∪ {E} is a possibly empty biterminal, and E is
the empty string.
Graphically, a rule will be represented as an ITG
rule:
hA, [],Bhe, fii = A → [ B e/f ]
hA, hi, he, fiBi = A → h e/f B i
hA, [], hE, Eii = A → E/E
As with ITGs, productions with only biterminals
will be represented without their permutation, as
any such rule can be trivially rewritten into in-
verted or identity form.
</bodyText>
<page confidence="0.993104">
12
</page>
<bodyText confidence="0.99038425">
Definition 4. An E-free LITG is an LITG where
no rule may rewrite one nonterminal into another
nonterminal only. Formally, the rule set is con-
strained so that
</bodyText>
<equation confidence="0.952695">
R ∩ N × {[], hi} × ({hE, Ei}B ∪ B{hE, Ei}) = ∅
</equation>
<bodyText confidence="0.9950904">
The LITG presented in Saers et al. (2010) is
thus an E-free LITG in normal form, since it has
the following thirteen rule forms (of which 8 are
meaningful, 1 is only used to terminate genera-
tion and 4 are redundant):
</bodyText>
<figure confidence="0.995243666666667">
A → [ e/f B ]
A → h e/f B i
A → [ B e/f ]
A → h B e/f i
A → [ e/E B ]  |A → h e/E B i
A → [ B e/E ]  |A → h B e/E i
A → [ E/f B ]  |A → h B E/f i
A → [ B E/f ]  |A → h E/f B i
A → E/E
</figure>
<bodyText confidence="0.905536428571429">
All the singleton rules can be expressed either in
straight or inverted form, but the result of apply-
ing the two rules are the same.
Lemma 1. Any LITG in normal form can be ex-
pressed as an LTG in normal form.
Proof. The above LITG can be rewritten in LTG
form as follows:
</bodyText>
<figure confidence="0.867950083333334">
A → [ e/f B ] = A → e/f B
A → h e/f B i = A → e/E B E/f
A → [ B e/f ] = A → B e/f
A → h B e/f i = A → E/f B e/E
A → [ e/E B ] = A → e/E B
A → [ B e/E ] = A → B e/E
A → [ E/f B ] = A → E/f B
A → [ B E/f ] = A → B E/f
A → E/E = A → E/E
To account for all LITGs in normal form, the fol-
lowing two non-E-free rules also needs to be ac-
counted for:
A → [ B ] = A → B
A → h B i = A → B
Lemma 2. Any LTG in normal form can be ex-
pressed as an LITG in normal form.
Proof. An LTG in normal form has two rules,
which can be rewritten in LITG form, either as
straight or inverted rules as follows
A → x/p B y/q = A → [ x/p B]
B¯→ [ B y/q ]
= A → h x/q B¯i
B¯→ h B y/p i
A → E/E = A → E/E
</figure>
<figureCaption confidence="0.715734">
Theorem 1. LTGs in normal form and LITGs in
normal form express the same class of transduc-
tions.
</figureCaption>
<bodyText confidence="0.8714005">
Proof. Follows from lemmas 1 and 2.
By theorem 1 everything concerning LTGs is also
applicable to LITGs, and an LTG can be expressed
in LITG form when convenient, and vice versa.
</bodyText>
<sectionHeader confidence="0.994815" genericHeader="method">
4 Pruning the Alignment Space
</sectionHeader>
<bodyText confidence="0.999851347826087">
The alignment space for a transduction grammar
is the combinations of the parse spaces of the
sentence pair. Let e be the E sentence, and f
be the F sentence. The parse spaces would be
O(|e|2) and O(|f|2) respectively, and the com-
bination of these spaces would be O(|e|2 ×|f|2),
or O(n4) if we assume n to be proportional
to the sentence lengths. In the case of LTGs,
this space is searched linearly, giving time com-
plexity O(n4), and in the case of ITGs there
is branching within both parse spaces, adding
an order of magnitude each, giving a total time
complexity of O(n6). There is, in other words,
a tight connection between the alignment space
and the time complexity of the biparsing al-
gorithm. Furthermore, most of this alignment
space is clearly useless. Consider the case where
the entire F sentence is deleted, and the entire E
sentence is simply inserted. Although it is pos-
sible that it is allowed by the grammar, it should
have a negligible probability (since it is clearly a
translation strategy that generalize poorly), and
could, for all practical reasons, be ignored.
</bodyText>
<page confidence="0.991657">
13
</page>
<table confidence="0.999756">
Language pair Bisentences Tokens
Spanish–English 108,073 1,466,132
French–English 95,990 1,340,718
German–English 115,323 1,602,781
</table>
<tableCaption confidence="0.999872">
Table 1: Size of training data.
</tableCaption>
<bodyText confidence="0.999598307692308">
Saers et al. (2009) present a scheme for prun-
ing away most of the points in the alignment
space. Parse items are binned according to cov-
erage (the total number of words covered), and
each bin is restricted to carry a maximum of b
items. Any items that do not fit in the bins are
excluded from further analysis. To decide which
items to keep, inside probability is used. This
pruning scheme effectively linearizes the align-
ment space, as is will be of size O(nb), regard-
less of what type grammar is used. An ITG can
thus be biparsed in cubic time, and an LTG in lin-
ear time.
</bodyText>
<sectionHeader confidence="0.895332" genericHeader="method">
5 Seeding an ITG with an LTG
</sectionHeader>
<bodyText confidence="0.999853833333333">
Since LTGs are a subclass of ITGs, it would be
possible to convert an LTG to a ITG. This could
save a lot of time, since LTGs are much faster to
induce from corpora than ITGs.
Converting a BLTG to a BITG is fairly straight
forward. Consider the BLTG rule
</bodyText>
<equation confidence="0.983952">
X -+ [ e/f X ]
</equation>
<bodyText confidence="0.9990675">
To convert it to BITG in two-normal form, the
biterminal has to be factored out. Replacing
the biterminal with a temporary symbol ¯X, and
introducing a rule that rewrites this temporary
symbol to the replaced biterminal produces two
rules:
</bodyText>
<equation confidence="0.999866">
X -+ [ XX ]
X¯ -+ e/f
</equation>
<bodyText confidence="0.9996373125">
This is no longer a bracketing grammar since
there are two nonterminals, but equating X¯ to X
restores this property. An analogous procedure
can be applied in the case where the nonterminal
comes before the biterminal, as well as for the
inverting cases.
When converting stochastic LTGs, the proba-
bility mass of the SLTG rule has to be distributed
to two SITG rules. The fact that the LTG rule
X -+ E/E lacks correspondence in ITGs has to
be weighted in as well. In this paper we took the
maximum entropy approach and distributed the
probability mass uniformly. This means defin-
ing the probability mass function p′ for the new
SBITG from the probability mass function p of
the original SBLTG such that:
</bodyText>
<equation confidence="0.999619947368421">
qp(X→[ e/f X ])
1−p(X→ǫ/ǫ)
+
qp(X→[ X e/f ])
1−p(X→ǫ/ǫ)
qp(X→h e/f X i)
1−p(X→ǫ/ǫ)
+
qp(X→h X e/f i)
1−p(X→ǫ/ǫ)
qp(X→[ e/f X ]) 1−p(X→ǫ/ǫ)
+
qp(X→[ X e/f ])
1−p(X→ǫ/ǫ)
qp(X→h e/f X i)
+ 1−p(X→ǫ/ǫ)
+
qp(X→h X e/f i)
1−p(X→ǫ/ǫ)
</equation>
<sectionHeader confidence="0.992456" genericHeader="method">
6 Setup
</sectionHeader>
<bodyText confidence="0.999968722222222">
The aim of this paper is to compare the align-
ments from SBITG and SBLTG to those from
GIZA++, and to study the impact of pruning
on efficiency and translation quality. Initial
grammars will be estimated by counting cooc-
currences in the training corpus, after which
expectation-maximization (ENI) will be used to
refine the initial estimate. At the last iteration,
the one-best parse of each sentence will be con-
sidered as the word alignment of that sentence.
In order to keep the experiments comparable,
relatively small corpora will be used. If larger
corpora were used, it would not be possible to get
any results for unpruned SBITGs because of the
prohibitive time complexity. The Europarl cor-
pus (Koehn, 2005) was used as a starting point,
and then all sentence pairs where one of the sen-
tences were longer than 10 tokens were filtered
</bodyText>
<equation confidence="0.974295142857143">
X
p′(X -+ [ X X ]) =
e/f
X
p′(X -+ ( X X )) =
e/f
p′(X -+ e/f) =
</equation>
<figure confidence="0.809410583333333">

  

  

            






</figure>
<page confidence="0.809508">
14
</page>
<figureCaption confidence="0.982808">
Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in
seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination.
</figureCaption>
<table confidence="0.999557153846154">
Beam size
System 1 10 25 50 75 100 ∞
BLEU
SBITG 0.1234 0.2608 0.2655 0.2653 0.2661 0.2671 0.2663
SBLTG 0.2574 0.2645 0.2631 0.2624 0.2625 0.2633 0.2628
GIZA++ 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597 0.2597
NIST
SBITG 3.9705 6.6439 6.7312 6.7101 6.7329 6.7445 6.6793
SBLTG 6.6023 6.6800 6.6657 6.6637 6.6714 6.6863 6.6765
GIZA++ 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464
Training times
SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00
SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59
</table>
<tableCaption confidence="0.996472">
Table 2: Results for the Spanish–English translation task.
</tableCaption>
<bodyText confidence="0.999198066666667">
out (see table 1). The GIZA++ system was built
according to the instructions for creating a base-
line system for the Fifth Workshop on Statistical
Machine Translation (WMT’10),1 but the above
corpora were used instead of those supplied by
the workshop. This includes word alignment
with GIZA++, a 5-gram language model built
with SRILM (Stolcke, 2002) and parameter tun-
ing with MERT (Och, 2003). To carry out the ac-
tual translations, Moses (Koehn et al., 2007) was
used. The SBITG and SBLTG systems were built
in exactly the same way, except that the align-
ments from GIZA++ were replaced by those from
the respective grammars.
In addition to trying out exhaustive biparsing
</bodyText>
<footnote confidence="0.948026">
1http://www.statmt.org/wmt10/
</footnote>
<bodyText confidence="0.999656888888889">
for SBITGs and SBLTGs on three different trans-
lation tasks, several different levels of pruning
were tried (1, 10, 25, 50, 75 and 100). We also
used the grammar induced from SBLTGs with a
beam size of 25 to seed SBITGs (see section 5),
which were then run for an additional iteration
of EM, also with beam size 25.
All systems are evaluated with BLEU (Pap-
ineni et al., 2002) and NIST (Doddington, 2002).
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999976333333333">
The results for the three different translation
tasks are presented in Tables 2, 3 and 4. It is
interesting to note that the trend they portray is
quite similar. When the beam is very narrow,
GIZA++ is better, but already at beam size 10,
both transduction grammars are superior. Con-
</bodyText>
<page confidence="0.989178">
15
</page>
<table confidence="0.999888769230769">
Beam size
System 1 10 25 50 75 100 00
BLEU
SBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663
SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649
GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603
NIST
SBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151
SBLTG 6.6814 6.7608 6.7656 6.7992 6.8020 6.7925 6.7784
GIZA++ 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907 6.6907
Training times
SBITG 03:25 17:00 42:00 1:25:00 2:10:00 2:45:00 3:10:00
SBLTG 31 1:41 3:25 7:06 9:35 13:56 10:52
</table>
<tableCaption confidence="0.945299">
Table 3: Results for the French–English translation task.
</tableCaption>
<table confidence="0.999927538461539">
Beam size
System 1 10 25 50 75 100 00
BLEU
SBITG 0.0926 0.2050 0.2091 0.2090 0.2091 0.2094 0.2113
SBLTG 0.2015 0.2067 0.2066 0.2073 0.2080 0.2066 0.2088
GIZA++ 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059
NIST
SBITG 3.4297 5.8743 5.9292 5.8947 5.8955 5.9086 5.9380
SBLTG 5.7799 5.8819 5.8882 5.8963 5.9252 5.8757 5.9311
GIZA++ 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668
Training times
SBITG 03:20 17:00 41:00 1:25:00 2:10:00 2:45:00 3:40:00
SBLTG 38 1:58 4:52 8:08 11:42 16:05 13:32
</table>
<tableCaption confidence="0.998757">
Table 4: Results for the German–English translation task.
</tableCaption>
<bodyText confidence="0.999830884615385">
sistent with Saers et al. (2009), SBITG has a sharp
rise in quality going from beam size 1 to 10,
and then a gentle slope up to beam size 25, af-
ter which it levels out. SBLTG, on the other hand
start out at a respectable level, and goes up a gen-
tle slope from beam size 1 to 10, after which is
level out. This is an interesting observation, as it
suggests that SBLTG reaches its optimum with a
lower beam size (although that optimum is lower
than that of SBITG). The trade-off between qual-
ity and time can now be extended beyond beam
size to include grammar choice. In Figure 1, run
times are plotted against BLEU scores to illus-
trate this trade-off. It is clear that SBLTGs are
indeed much faster than SBITGs, the only excep-
tion is when SBITGs are run with b = 1, but then
the BLEU score is so low that is is not worth con-
sidering.
The time may seem inconsistent between b =
100 and b = 00 for SBLTG, but the extra time
for the tighter beam is because of beam manage-
ment, which the exhaustive search doesn’t bother
with.
In table 5 we compare the pure approaches
to one where an LTG was trained during 10 it-
erations of EM and then used to seed (see sec-
</bodyText>
<page confidence="0.994905">
16
</page>
<table confidence="0.9998671">
Translation task System BLEU NIST Total time
Spanish–English SBLTG 0.2631 6.6657 36:40
SBITG 0.2655 6.7312 6:20:00
Both 0.2660 6.7124 1:14:40
French–English SBLTG 0.2651 6.7656 34:10
SBITG 0.2654 6.7913 7:00:00
Both 0.2625 6.7609 1:16:10
German–English SBLTG 0.2066 5.8882 48:52
SBITG 0.2091 5.9292 6:50:00
Both 0.2095 5.9224 1:29:40
</table>
<tableCaption confidence="0.960119">
Table 5: Results for seeding an SBITG with an SBLTG (Both) compared to the pure approach. Total
</tableCaption>
<bodyText confidence="0.977558666666667">
time refers to 10 iterations of EM training for SBITG and SBLTG respectively, and 10 iterations of
SBLTG and one iteration of SBITG training for the combined system.
tion 5) an SBITG, which was then trained for
one iteration of EM. Although the differences
are fairly small, German–English and Spanish–
English seem to reach the level of SBITG,
whereas French–English is actually hurt. The
big difference is in time, since the combined sys-
tem needs about a fifth of the time the SBITG-
based system needs. This phenomenon needs to
be more thoroughly examined.
It is also worth noting that GIZA++ was beaten
by an aligner that used less than 20 minutes (less
than 2 minutes per iteration and at most 10 itera-
tions) to align the corpus.
</bodyText>
<sectionHeader confidence="0.999312" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999674555555556">
In this paper we have introduced the bilingual
version of linear grammar: Linear Transduc-
tion Grammars, and found that they generate the
same class of transductions as Linear Inversion
Transduction Grammars. We have also com-
pared Stochastic Bracketing versions of ITGs and
LTGs to GIZA++ on three word alignment tasks.
The efficiency issues with transduction gram-
mars have been addressed by pruning, and the
conclusion is that there is a trade-off between
run time and translation quality. A part of the
trade-off is choosing which grammar framework
to use, as LTGs are faster but not as good as ITGs.
It also seems possible to take a short-cut in this
trade-off by starting out with an LTG and convert-
ing it to an ITG. We have also showed that it is
possible to beat the translation quality of GIZA++
with a quite fast transduction grammar.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999987466666667">
This work was funded by the Swedish Na-
tional Graduate School of Language Technol-
ogy (GSLT), the Defense Advanced Research
Projects Agency (DARPA) under GALE Con-
tracts No. HR0011-06-C-0022 and No. HR0011-
06-C-0023, and the Hong Kong Research
Grants Council (RGC) under research grants
GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and
conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views ofthe Defense Ad-
vanced Research Projects Agency. The computa-
tions were performed on UPPMAX resources un-
der project p2007020.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989942">
Aho, Alfred V. Ullman, Jeffrey D. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice-
Halll, Inc., Upper Saddle River, NJ.
Brown, Peter F., Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Cocke, John. 1969. Programming languages and
their compilers: Preliminary notes. Courant Insti-
</reference>
<page confidence="0.995332">
17
</page>
<reference confidence="0.998786739583333">
tute of Mathematical Sciences, New York Univer-
sity.
Doddington, George. 2002. Automatic eval-
uation of machine translation quality using n-
gram co-occurrence statistics. In Proceedings of
Human Language Technology conference (HLT-
2002), San Diego, California.
Earley, Jay. 1970. An efficient context-free parsing
algorithm. Communications of the Association for
Comuter Machinery, 13(2):94–102.
Haghighi, Aria, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
923–931, Suntec, Singapore, August.
Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin
Knight. 2009. Binarization of synchronous
context-free grammars. Computational Linguis-
tics, 35(4):559–595.
Kasami, Tadao. 1965. An efficient recognition
and syntax analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-00143, Air
Force Cambridge Research Laboratory.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions, pages 177–180,
Prague, Czech Republic, June.
Koehn, Philipp. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, Phuket, Thailand, Septem-
ber.
Lewis, Philip M. and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the Asso-
ciation for Computing Machinery, 15(3):465–488.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In 41st Annual
Meeting of the Association for Computational Lin-
guistics, pages 160–167, Sapporo, Japan, July.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311–
318, Philadelphia, Pennsylvania, July.
Saers, Markus and Dekai Wu. 2009. Improving
phrase-based translation via word alignments from
Stochastic Inversion Transduction Grammars. In
Proceedings of the Third Workshop on Syntax
and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009, pages 28–36, Boulder, Col-
orado,June.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2009.
Learning Stochastic Bracketing Inversion Trans-
duction Grammars with a cubic time biparsing al-
gorithm. In Proceedings of the 11th International
Conference on Parsing Technologies (IWPT’09),
pages 29–32, Paris, France, October.
Saers, Markus, Joakim Nivre, and Dekai Wu. 2010.
Word alignment with Stochastic Bracketing Linear
Inversion Transduction Grammar. In Proceedings
of Human Language Technologies: The 11th An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
Los Angeles, California, June.
Stolcke, Andreas. 2002. SRILM – an extensible
language modeling toolkit. In International Con-
ference on Spoken Language Processing, Denver,
Colorado, September.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in sta-
tistical translation. In Proceedings of the 16th con-
ference on Computational linguistics, pages 836–
841, Morristown, New Jersey.
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377–403.
Younger, Daniel H. 1967. Recognition and parsing
of context-free languages in time n3. Information
and Control, 10(2):189–208.
Zhang, Hao, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing.
In Proceedings of ACL-08: HLT, pages 97–105,
Columbus, Ohio, June.
</reference>
<page confidence="0.999293">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441665">
<title confidence="0.962325">A Systematic Comparison between Inversion Transduction Grammar and Linear Transduction Grammar for Word Alignment</title>
<author confidence="0.885551">Saers Nivre Dekai</author>
<affiliation confidence="0.880400333333333">Dept. of Linguistics &amp; Philology Uppsala University Human Language Technology Dept. of Computer Science &amp; Hong Kong Univ. of Science &amp;</affiliation>
<email confidence="0.973165">dekai@cs.ust.hk</email>
<abstract confidence="0.982911045454545">We present two contributions to grammar driven translation. First, since both Inversion Transduction Grammar and Linear Inversion Transduction Grammars have been shown to produce better alignments then the standard word alignment tool, we investigate how the trade-off between speed and end-to-end translation quality extends to the choice of grammar formalism. Second, we prove that Linear Transduction Gramgenerate the same transductions as Linear Inversion Transduction Grammars, and present a scheme for arat by bilingualizing Linear Grammars. We also present a method for obtaining Inversion Transduction Grammars from Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Ullman Aho</author>
<author>D Jeffrey</author>
</authors>
<title>The Theory of Parsing, Translation, and Compiling. PrenticeHalll, Inc., Upper Saddle River,</title>
<date>1972</date>
<location>NJ.</location>
<marker>Aho, Jeffrey, 1972</marker>
<rawString>Aho, Alfred V. Ullman, Jeffrey D. 1972. The Theory of Parsing, Translation, and Compiling. PrenticeHalll, Inc., Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1691" citStr="Brown et al., 1993" startWordPosition="245" endWordPosition="248"> speed up grammar induction from parallel corpora dramatically. 1 Introduction In this paper we introduce Linear Transduction Grammars (LTGs), which are the bilingual case of Linear Grammars (LGs). We also show that LTGs are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, GIZA++ (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). A heuristic for converting stochastic bracketing LTGs into stochastic bracketing ITGs is presented, and fitted into the speed– quality trade-off. In section 3 we give an overview of transduction grammars, introduce LTGs and show that they are equal to LITGs. In section 4 we give a short description of the rational for the transduction grammar pruning used. In section 5 we describe a way of seeding a stochastic bracketing ITG with the rules and probabilities of a stochastic bracketing LTG. Section 6 describes the setup, and results are given in section </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Cocke</author>
</authors>
<title>Programming languages and their compilers: Preliminary notes.</title>
<date>1969</date>
<booktitle>Courant Institute of Mathematical Sciences,</booktitle>
<location>New York University.</location>
<contexts>
<context position="5991" citStr="Cocke, 1969" startWordPosition="991" endWordPosition="992"> singletons. Whenever a preterminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f. Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F, which is binarizable (since it is a CFG), and can therefor be computed in polynomial time (O(n3)). Once there is a parse tree for F, the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a sentence pair) is needed. The time complexity for biparsing with SDTGs is O(n2n+2), which is clearly intractable. Inversion Transduction Grammars or ITGs (</context>
</contexts>
<marker>Cocke, 1969</marker>
<rawString>Cocke, John. 1969. Programming languages and their compilers: Preliminary notes. Courant Institute of Mathematical Sciences, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using ngram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology conference (HLT2002),</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="18195" citStr="Doddington, 2002" startWordPosition="3365" endWordPosition="3366">G systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1http://www.statmt.org/wmt10/ for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 7 Results The results for the three different translation tasks are presented in Tables 2, 3 and 4. It is interesting to note that the trend they portray is quite similar. When the beam is very narrow, GIZA++ is better, but already at beam size 10, both transduction grammars are superior. Con15 Beam size System 1 10 25 50 75 100 00 BLEU SBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663 SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649 GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 NIST SBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151 SBLTG 6.6814 6.7608 6.7656 6.7992</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>Doddington, George. 2002. Automatic evaluation of machine translation quality using ngram co-occurrence statistics. In Proceedings of Human Language Technology conference (HLT2002), San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Comuter Machinery,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="6071" citStr="Earley, 1970" startWordPosition="1003" endWordPosition="1004">e refer to that pair of terminals as a biterminal, which will be written as e/f. Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F, which is binarizable (since it is a CFG), and can therefor be computed in polynomial time (O(n3)). Once there is a parse tree for F, the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a sentence pair) is needed. The time complexity for biparsing with SDTGs is O(n2n+2), which is clearly intractable. Inversion Transduction Grammars or ITGs (Wu, 1997) are transduction grammars that have a two-normal form, thus guaranteei</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Earley, Jay. 1970. An efficient context-free parsing algorithm. Communications of the Association for Comuter Machinery, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>923--931</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3507" citStr="Haghighi et al., 2009" startWordPosition="549" endWordPosition="552">directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars (SDTGs). To differentiate identical nonterminal symbols, indices were used (the ba</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Haghighi, Aria, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 923–931, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Binarization of synchronous context-free grammars.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="5838" citStr="Huang et al., 2009" startWordPosition="964" endWordPosition="967">vidual terminals are pair up with the empty string (ǫ). A → Xx B Xa B ; 0, 1, 2,3 Xa → a, ǫ Xx → ǫ, x Lexical rules involving the empty string are referred to as singletons. Whenever a preterminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f. Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F, which is binarizable (since it is a CFG), and can therefor be computed in polynomial time (O(n3)). Once there is a parse tree for F, the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a se</context>
</contexts>
<marker>Huang, Zhang, Gildea, Knight, 2009</marker>
<rawString>Huang, Liang, Hao Zhang, Daniel Gildea, and Kevin Knight. 2009. Binarization of synchronous context-free grammars. Computational Linguistics, 35(4):559–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadao Kasami</author>
</authors>
<title>An efficient recognition and syntax analysis algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AFCRL-65-00143,</tech>
<institution>Air Force Cambridge Research Laboratory.</institution>
<contexts>
<context position="6005" citStr="Kasami, 1965" startWordPosition="993" endWordPosition="994">Whenever a preterminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f. Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F, which is binarizable (since it is a CFG), and can therefor be computed in polynomial time (O(n3)). Once there is a parse tree for F, the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a sentence pair) is needed. The time complexity for biparsing with SDTGs is O(n2n+2), which is clearly intractable. Inversion Transduction Grammars or ITGs (Wu, 1997) are </context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>Kasami, Tadao. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-00143, Air Force Cambridge Research Laboratory.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17549" citStr="Koehn et al., 2007" startWordPosition="3252" endWordPosition="3255">mes SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00 SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1http://www.statmt.org/wmt10/ for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU (Papi</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<location>Phuket, Thailand,</location>
<contexts>
<context position="16099" citStr="Koehn, 2005" startWordPosition="2991" endWordPosition="2992">study the impact of pruning on efficiency and translation quality. Initial grammars will be estimated by counting cooccurrences in the training corpus, after which expectation-maximization (ENI) will be used to refine the initial estimate. At the last iteration, the one-best parse of each sentence will be considered as the word alignment of that sentence. In order to keep the experiments comparable, relatively small corpora will be used. If larger corpora were used, it would not be possible to get any results for unpruned SBITGs because of the prohibitive time complexity. The Europarl corpus (Koehn, 2005) was used as a starting point, and then all sentence pairs where one of the sentences were longer than 10 tokens were filtered X p′(X -+ [ X X ]) = e/f X p′(X -+ ( X X )) = e/f p′(X -+ e/f) =                             14 Figure 1: Trade-offs between translation quality (as measured by BLEU) and biparsing time (in seconds plotted on a logarithmic scale) for SBLTGs, SBITGs and the combination. Beam size System 1 10 25 50 75 100 ∞ BLEU SBITG 0.1234 0.2608 0.2655 0.2653 0.2661 0.2671 0.2663 SBLTG 0.2574 0.2645 0.2631 0.2624 0.2625 0.2633 0.2628 GIZA++ </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, Philipp. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine Translation Summit X, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M Lewis</author>
<author>Richard E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="3756" citStr="Lewis and Stearns (1968)" startWordPosition="589" endWordPosition="592">pose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars (SDTGs). To differentiate identical nonterminal symbols, indices were used (the bag of nonterminals for the two productions are equal by definition). A → B(1) a B(2) {x B(1) B(2)} = A → B(1) a B(2), x B(1) B(2) The semantics of the rules is that one nonterminal rewrites into a bag of nonterminals that is distributed independently</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>Lewis, Philip M. and Richard E. Stearns. 1968. Syntax-directed transduction. Journal of the Association for Computing Machinery, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1731" citStr="Och and Ney, 2003" startWordPosition="253" endWordPosition="256">l corpora dramatically. 1 Introduction In this paper we introduce Linear Transduction Grammars (LTGs), which are the bilingual case of Linear Grammars (LGs). We also show that LTGs are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, GIZA++ (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). A heuristic for converting stochastic bracketing LTGs into stochastic bracketing ITGs is presented, and fitted into the speed– quality trade-off. In section 3 we give an overview of transduction grammars, introduce LTGs and show that they are equal to LITGs. In section 4 we give a short description of the rational for the transduction grammar pruning used. In section 5 we describe a way of seeding a stochastic bracketing ITG with the rules and probabilities of a stochastic bracketing LTG. Section 6 describes the setup, and results are given in section 7. Finally, some conclusions are offered</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz Josef and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="17483" citStr="Och, 2003" startWordPosition="3242" endWordPosition="3243">464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 Training times SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00 SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1http://www.statmt.org/wmt10/ for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM,</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz Josef. 2003. Minimum error rate training in statistical machine translation. In 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="18167" citStr="Papineni et al., 2002" startWordPosition="3358" endWordPosition="3362">007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1http://www.statmt.org/wmt10/ for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were then run for an additional iteration of EM, also with beam size 25. All systems are evaluated with BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). 7 Results The results for the three different translation tasks are presented in Tables 2, 3 and 4. It is interesting to note that the trend they portray is quite similar. When the beam is very narrow, GIZA++ is better, but already at beam size 10, both transduction grammars are superior. Con15 Beam size System 1 10 25 50 75 100 00 BLEU SBITG 0.1268 0.2632 0.2654 0.2669 0.2668 0.2655 0.2663 SBLTG 0.2600 0.2638 0.2651 0.2668 0.2672 0.2662 0.2649 GIZA++ 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 0.2603 NIST SBITG 4.0849 6.7136 6.7913 6.8065 6.8068 6.8088 6.8151 SBLTG</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311– 318, Philadelphia, Pennsylvania, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrase-based translation via word alignments from Stochastic Inversion Transduction Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009,</booktitle>
<pages>28--36</pages>
<location>Boulder, Colorado,June.</location>
<contexts>
<context position="3484" citStr="Saers and Wu, 2009" startWordPosition="545" endWordPosition="548">the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars (SDTGs). To differentiate identical nonterminal symbols, in</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>Saers, Markus and Dekai Wu. 2009. Improving phrase-based translation via word alignments from Stochastic Inversion Transduction Grammars. In Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation (SSST-3) at NAACL HLT 2009, pages 28–36, Boulder, Colorado,June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning Stochastic Bracketing Inversion Transduction Grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>29--32</pages>
<location>Paris, France,</location>
<contexts>
<context position="3527" citStr="Saers et al., 2009" startWordPosition="553" endWordPosition="556">ns can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars (SDTGs). To differentiate identical nonterminal symbols, indices were used (the bag of nonterminals fo</context>
<context position="13333" citStr="Saers et al. (2009)" startWordPosition="2485" endWordPosition="2488"> the time complexity of the biparsing algorithm. Furthermore, most of this alignment space is clearly useless. Consider the case where the entire F sentence is deleted, and the entire E sentence is simply inserted. Although it is possible that it is allowed by the grammar, it should have a negligible probability (since it is clearly a translation strategy that generalize poorly), and could, for all practical reasons, be ignored. 13 Language pair Bisentences Tokens Spanish–English 108,073 1,466,132 French–English 95,990 1,340,718 German–English 115,323 1,602,781 Table 1: Size of training data. Saers et al. (2009) present a scheme for pruning away most of the points in the alignment space. Parse items are binned according to coverage (the total number of words covered), and each bin is restricted to carry a maximum of b items. Any items that do not fit in the bins are excluded from further analysis. To decide which items to keep, inside probability is used. This pruning scheme effectively linearizes the alignment space, as is will be of size O(nb), regardless of what type grammar is used. An ITG can thus be biparsed in cubic time, and an LTG in linear time. 5 Seeding an ITG with an LTG Since LTGs are a</context>
<context position="19626" citStr="Saers et al. (2009)" startWordPosition="3594" endWordPosition="3597">he French–English translation task. Beam size System 1 10 25 50 75 100 00 BLEU SBITG 0.0926 0.2050 0.2091 0.2090 0.2091 0.2094 0.2113 SBLTG 0.2015 0.2067 0.2066 0.2073 0.2080 0.2066 0.2088 GIZA++ 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 0.2059 NIST SBITG 3.4297 5.8743 5.9292 5.8947 5.8955 5.9086 5.9380 SBLTG 5.7799 5.8819 5.8882 5.8963 5.9252 5.8757 5.9311 GIZA++ 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 5.8668 Training times SBITG 03:20 17:00 41:00 1:25:00 2:10:00 2:45:00 3:40:00 SBLTG 38 1:58 4:52 8:08 11:42 16:05 13:32 Table 4: Results for the German–English translation task. sistent with Saers et al. (2009), SBITG has a sharp rise in quality going from beam size 1 to 10, and then a gentle slope up to beam size 25, after which it levels out. SBLTG, on the other hand start out at a respectable level, and goes up a gentle slope from beam size 1 to 10, after which is level out. This is an interesting observation, as it suggests that SBLTG reaches its optimum with a lower beam size (although that optimum is lower than that of SBITG). The trade-off between quality and time can now be extended beyond beam size to include grammar choice. In Figure 1, run times are plotted against BLEU scores to illustra</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Saers, Markus, Joakim Nivre, and Dekai Wu. 2009. Learning Stochastic Bracketing Inversion Transduction Grammars with a cubic time biparsing algorithm. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT’09), pages 29–32, Paris, France, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Word alignment with Stochastic Bracketing Linear Inversion Transduction Grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles, California,</location>
<contexts>
<context position="1366" citStr="Saers et al., 2010" startWordPosition="193" endWordPosition="196">prove that Linear Transduction Grammars (LTGs) generate the same transductions as Linear Inversion Transduction Grammars, and present a scheme for arriving at LTGs by bilingualizing Linear Grammars. We also present a method for obtaining Inversion Transduction Grammars from Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 1 Introduction In this paper we introduce Linear Transduction Grammars (LTGs), which are the bilingual case of Linear Grammars (LGs). We also show that LTGs are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, GIZA++ (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). A heuristic for converting stochastic bracketing LTGs into stochastic bracketing ITGs is presented, and fitted into the speed– quality trade-off. In section 3 we give an overview of transduction grammars, introduce LTGs and show that</context>
<context position="3548" citStr="Saers et al., 2010" startWordPosition="557" endWordPosition="560">and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars (SDTGs). To differentiate identical nonterminal symbols, indices were used (the bag of nonterminals for the two productions</context>
<context position="8970" citStr="Saers et al. (2010)" startWordPosition="1552" endWordPosition="1555">nterminal symbols, E is a finite set of terminal symbols in language E, A is a finite set of terminal symbols in language F, R is a finite set of linear transduction rules and S ∈ N is the designated start symbol. The rule set is constrained so that R ⊆ N × IFNIF ∪ {hE, Ei} Where I F= E∪{E}×A∪{E} and E is the empty string. hA, hx,piBhy, qii = A → x/p B y/q hA, hE, Eii = B → E/E Like STGs, LTGs do not allow any reordering, and are monotone, but because they are linear, this has no impact on expressiveness, as we shall see later. Linear Inversion Transduction Grammars (LITGs) were introduced in Saers et al. (2010), and represent ITGs that are allowed to have at most one nonterminal symbol in each production. These are attractive because they can biparse a sentence pair in O(n4) time, which can be further reduced to linear time by severely pruning the search space. This makes them tractable for large parallel corpora, and a viable way to induce transduction grammars from large parallel corpora. Definition 3. An LITG in normal form is a tuple TGLI = hN, E, A, R, Si Where N is a finite set of nonterminal symbols, E is a finite set of terminal symbols from language E, A is a finite set of terminal symbols </context>
<context position="10454" citStr="Saers et al. (2010)" startWordPosition="1845" endWordPosition="1848">sibly empty biterminal, and E is the empty string. Graphically, a rule will be represented as an ITG rule: hA, [],Bhe, fii = A → [ B e/f ] hA, hi, he, fiBi = A → h e/f B i hA, [], hE, Eii = A → E/E As with ITGs, productions with only biterminals will be represented without their permutation, as any such rule can be trivially rewritten into inverted or identity form. 12 Definition 4. An E-free LITG is an LITG where no rule may rewrite one nonterminal into another nonterminal only. Formally, the rule set is constrained so that R ∩ N × {[], hi} × ({hE, Ei}B ∪ B{hE, Ei}) = ∅ The LITG presented in Saers et al. (2010) is thus an E-free LITG in normal form, since it has the following thirteen rule forms (of which 8 are meaningful, 1 is only used to terminate generation and 4 are redundant): A → [ e/f B ] A → h e/f B i A → [ B e/f ] A → h B e/f i A → [ e/E B ] |A → h e/E B i A → [ B e/E ] |A → h B e/E i A → [ E/f B ] |A → h B E/f i A → [ B E/f ] |A → h E/f B i A → E/E All the singleton rules can be expressed either in straight or inverted form, but the result of applying the two rules are the same. Lemma 1. Any LITG in normal form can be expressed as an LTG in normal form. Proof. The above LITG can be rewrit</context>
</contexts>
<marker>Saers, Nivre, Wu, 2010</marker>
<rawString>Saers, Markus, Joakim Nivre, and Dekai Wu. 2010. Word alignment with Stochastic Bracketing Linear Inversion Transduction Grammar. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="17440" citStr="Stolcke, 2002" startWordPosition="3234" endWordPosition="3235">0 6.6657 6.6637 6.6714 6.6863 6.6765 GIZA++ 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 6.6464 Training times SBITG 03:10 17:00 38:00 1:20:00 2:00:00 2:40:00 3:20:00 SBLTG 35 1:49 3:40 7:33 9:44 12:13 11:59 Table 2: Results for the Spanish–English translation task. out (see table 1). The GIZA++ system was built according to the instructions for creating a baseline system for the Fifth Workshop on Statistical Machine Translation (WMT’10),1 but the above corpora were used instead of those supplied by the workshop. This includes word alignment with GIZA++, a 5-gram language model built with SRILM (Stolcke, 2002) and parameter tuning with MERT (Och, 2003). To carry out the actual translations, Moses (Koehn et al., 2007) was used. The SBITG and SBLTG systems were built in exactly the same way, except that the alignments from GIZA++ were replaced by those from the respective grammars. In addition to trying out exhaustive biparsing 1http://www.statmt.org/wmt10/ for SBITGs and SBLTGs on three different translation tasks, several different levels of pruning were tried (1, 10, 25, 50, 75 and 100). We also used the grammar induced from SBLTGs with a beam size of 25 to seed SBITGs (see section 5), which were </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM – an extensible language modeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>836--841</pages>
<location>Morristown, New Jersey.</location>
<contexts>
<context position="1711" citStr="Vogel et al., 1996" startWordPosition="249" endWordPosition="252">duction from parallel corpora dramatically. 1 Introduction In this paper we introduce Linear Transduction Grammars (LTGs), which are the bilingual case of Linear Grammars (LGs). We also show that LTGs are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, GIZA++ (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). A heuristic for converting stochastic bracketing LTGs into stochastic bracketing ITGs is presented, and fitted into the speed– quality trade-off. In section 3 we give an overview of transduction grammars, introduce LTGs and show that they are equal to LITGs. In section 4 we give a short description of the rational for the transduction grammar pruning used. In section 5 we describe a way of seeding a stochastic bracketing ITG with the rules and probabilities of a stochastic bracketing LTG. Section 6 describes the setup, and results are given in section 7. Finally, some con</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Vogel, Stephan, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based word alignment in statistical translation. In Proceedings of the 16th conference on Computational linguistics, pages 836– 841, Morristown, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1622" citStr="Wu, 1997" startWordPosition="235" endWordPosition="236">ars from Linear (Inversion) Transduction Grammars, which can speed up grammar induction from parallel corpora dramatically. 1 Introduction In this paper we introduce Linear Transduction Grammars (LTGs), which are the bilingual case of Linear Grammars (LGs). We also show that LTGs are equal to Linear Inversion Transduction Grammars (Saers et al., 2010). To be able to induce transduction grammars directly from parallel corpora an approximate search for parses is needed. The trade-off between speed and end-toend translation quality is investigated and compared to Inversion Transduction Grammars (Wu, 1997) and the standard tool for word alignment, GIZA++ (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). A heuristic for converting stochastic bracketing LTGs into stochastic bracketing ITGs is presented, and fitted into the speed– quality trade-off. In section 3 we give an overview of transduction grammars, introduce LTGs and show that they are equal to LITGs. In section 4 we give a short description of the rational for the transduction grammar pruning used. In section 5 we describe a way of seeding a stochastic bracketing ITG with the rules and probabilities of a stochastic bracketing </context>
<context position="5817" citStr="Wu, 1997" startWordPosition="962" endWordPosition="963">, the individual terminals are pair up with the empty string (ǫ). A → Xx B Xa B ; 0, 1, 2,3 Xa → a, ǫ Xx → ǫ, x Lexical rules involving the empty string are referred to as singletons. Whenever a preterminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f. Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F, which is binarizable (since it is a CFG), and can therefor be computed in polynomial time (O(n3)). Once there is a parse tree for F, the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is c</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Wu, Dekai. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="6021" citStr="Younger, 1967" startWordPosition="995" endWordPosition="996">terminal is used to pair up two terminal symbols, we refer to that pair of terminals as a biterminal, which will be written as e/f. Any SDTG can be rephrased to contain permuted nonterminal productions and biterminal productions only, and we will call this the normal form of SDTGs. Note that it is not possible to produce a two-normal form for SDTGs, as there are some rules that are not binarizable (Wu, 1997; Huang et al., 2009). This is an important point to make, since efficient parsing for CFGs is based on either restricting parsing to only handle binary grammars (Cocke, 1969; Kasami, 1965; Younger, 1967), or rely on onthe-fly binarization (Earley, 1970). When translating with a grammar, parsing only has to be done in F, which is binarizable (since it is a CFG), and can therefor be computed in polynomial time (O(n3)). Once there is a parse tree for F, the corresponding tree for E can be easily constructed. When inducing a grammar from examples, however, biparsing (finding an analysis that is consistent across a sentence pair) is needed. The time complexity for biparsing with SDTGs is O(n2n+2), which is clearly intractable. Inversion Transduction Grammars or ITGs (Wu, 1997) are transduction gra</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Younger, Daniel H. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="3464" citStr="Zhang et al., 2008" startWordPosition="541" endWordPosition="544">e 2 (E). By running the process in both directions, two functions can be estimated and then combined to form an alignment. The simplest of these combinations are intersection and union, but usually, the intersection is heuristically extended. Transduction grammars on the other hand, impose a shared structure on the sentence pairs, thus forcing a consistent alignment in both directions. This method 10 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 10–18, COLING 2010, Beijing, August 2010. has proved successful in the settings it has been tried (Zhang et al., 2008; Saers and Wu, 2009; Haghighi et al., 2009; Saers et al., 2009; Saers et al., 2010). Most efforts focus on cutting down time complexity so that larger data sets than toyexamples can be processed. 3 Transduction Grammars Transduction grammars were first introduced in Lewis and Stearns (1968), and further developed in Aho and Ullman (1972). The original notation called for regular CFG-rules in language F with rephrased E productions, either in curly brackets, or comma separated. The bilingual version of CFGs is called Syntax-Directed Transduction Grammars (SDTGs). To differentiate identical non</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Zhang, Hao, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In Proceedings of ACL-08: HLT, pages 97–105, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>