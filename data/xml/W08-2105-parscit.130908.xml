<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.972966">
Transforming Meaning Representation Grammars to Improve Semantic
Parsing
</title>
<author confidence="0.949135">
Rohit J. Kate
</author>
<affiliation confidence="0.967084">
Department of Computer Sciences*
The University of Texas at Austin
1 University Station C0500
</affiliation>
<address confidence="0.608162">
Austin, TX 78712-0233, USA
</address>
<email confidence="0.997844">
rjkate@cs.utexas.edu
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922095238095">
A semantic parser learning system learns
to map natural language sentences into
their domain-specific formal meaning rep-
resentations, but if the constructs of the
meaning representation language do not
correspond well with the natural language
then the system may not learn a good se-
mantic parser. This paper presents ap-
proaches for automatically transforming a
meaning representation grammar (MRG)
to conform it better with the natural lan-
guage semantics. It introduces grammar
transformation operators and meaning rep-
resentation macros which are applied in an
error-driven manner to transform an MRG
while training a semantic parser learning
system. Experimental results show that the
automatically transformed MRGs lead to
better learned semantic parsers which per-
form comparable to the semantic parsers
learned using manually engineered MRGs.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891333333333">
Semantic parsing is the task of converting natural
language (NL) sentences into their meaning repre-
sentations (MRs) which a computer program can
execute to perform some domain-specific task, like
controlling a robot, answering database queries
etc. These MRs are expressed in a formal mean-
ing representation language (MRL) unique to the
domain to suit the application, like some specific
command language to control a robot or some
</bodyText>
<note confidence="0.5351038">
*Alumnus at the time of submission.
@ 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</note>
<bodyText confidence="0.999964710526316">
query language to execute database queries. A ma-
chine learning system for semantic parsing takes
NL sentences paired with their respective MRs as
training data and induces a semantic parser which
can then map novel NL sentences into their MRs.
The grammar of an MRL, which we will call
meaning representation grammar (MRG), is as-
sumed to be deterministic and context-free which
is true for grammars of almost all the computer
executable languages. A semantic parsing learn-
ing system typically exploits the given MRG of the
MRL to learn a semantic parser (Kate and Mooney,
2006; Wong and Mooney, 2006). Although in dif-
ferent ways, but the systems presented in these pa-
pers learn how the NL phrases relate to the pro-
ductions of the MRG, and using this information
they parse a test sentence to compositionally gen-
erate its best MR. In order to learn a good seman-
tic parser, it is necessary that the productions of
the MRG accurately represent the semantics be-
ing expressed by the natural language. However,
an MRL and its MRG are typically designed to
best suit the application with little consideration
for how well they correspond to the semantics of
a natural language.
Some other semantic parser learning systems
which need MRL in the form of Prolog (Tang
and Mooney, 2001) or A-calculus (Zettlemoyer and
Collins, 2007; Wong and Mooney, 2007) do not
use productions of the MRG but instead use pred-
icates of the MRL. However, in order to learn a
good semantic parser, they still require that these
predicates correspond well with the semantics of
the natural language. There are also systems which
learn semantic parsers from more detailed train-
ing data in the form of semantically augmented
parse trees of NL sentences in which each inter-
nal node has a syntactic and a semantic label (Ge
</bodyText>
<page confidence="0.982594">
33
</page>
<note confidence="0.9072942">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 33–40
Manchester, August 2008
(a) NL: If the ball is in our midfield then player 5 should go to (-5,0). REGION —. ( rec POINT POINT)
MR:(bpos (rec (pt -32 -35)(pt 0 35))
(do (player our {5})(pos (pt -5 0))))
(b) NL: Which is the longest river in Texas? POINT —. ( pt NUM NUM) POINT —. ( pt NUM NUM)
MR: answer(longest(river(loc_2(stateid(’Texas’)))))
NUM —. 0 NUM —. 35
NUM —. -32 NUM —. -35
(c) NL: Which is the longest river in Texas?
</note>
<bodyText confidence="0.5402765">
MR: select river.name from river where
river.traverse=’Texas’ and river.length=
(select max(river.length) from river
where river.traverse=’Texas’);
</bodyText>
<figureCaption confidence="0.963646333333333">
Figure 1: Examples of NL sentences and their MRs from
(a) the CLANG domain (b) GEOQUERY domain with func-
tional MRL (c) GEOQUERY domain with SQL.
</figureCaption>
<bodyText confidence="0.9999222">
and Mooney, 2005; Nguyen et al., 2006). For these
systems to work well, it is also necessary that the
semantic labels of the MRL correspond well with
natural language semantics.
If the MRG of a domain-specific MRL does not
correspond well with natural language semantics
then manually re-engineering the MRG to work
well for semantic parsing is a tedious task and re-
quires considerable domain expertise. In this pa-
per, we present methods to automatically trans-
form a given MRG to make it more suitable for
learning semantic parsers. No previous work ad-
dresses this issue to our best knowledge. We intro-
duce grammar transformation operators and mean-
ing representation macros to transform an MRG.
We describe how these are applied in an error-
driven manner using the base semantic parsing
learning algorithm presented in (Kate and Mooney,
2006) resulting in a better learned semantic parser.
Our approach, however, is general enough to im-
prove any semantic parser learning system which
uses productions of the MRG. We present exper-
imental results with three very different MRLs to
show how these grammar transformations improve
the semantic parsing performance.
</bodyText>
<sectionHeader confidence="0.976307" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.998228428571429">
The following subsection gives some examples of
semantic parsing domains and their corresponding
MRLs and illustrates why incompatibility between
MRGs and natural language could hurt semantic
parsing. The next subsection then briefly describes
a base semantic parser learning system which we
use in our experiments.
</bodyText>
<subsectionHeader confidence="0.950885">
2.1 MRLs and MRGs for Semantic Parsing
</subsectionHeader>
<figureCaption confidence="0.8947615">
Figure 1 (a) gives an example of a natural lan-
guage sentence and its corresponding MR in an
MRL called CLANG which is a formal declar-
Figure 2: The parse for the CLANG expression “(rec (pt
-32 -35) (pt 0 35))” corresponding to the natural language ut-
terance “our midfield” using its original MRG.
</figureCaption>
<bodyText confidence="0.999890717948718">
ative language with LISP-like prefix notation
designed to instruct simulated soccer players in
the RoboCup1 Coach Competition. The MRL
and its MRG was designed by the Coach Com-
petition community (Chen et al., 2003) to suit
the requirements of their application independent
of how well the MRG conforms with the natural
language semantics. They were, in fact, not
aware that later (Kate et al., 2005) this will be
introduced as a test domain for learning semantic
parsers. In this original MRG for CLANG, there
are several constructs which do not correspond
well with their meanings in the natural language.
For example, the MR expression of the rectangle
(rec (pt -32 -35) (pt 0 35)) from
the example MR in Figure 1 (a), whose parse ac-
cording to the original MRG is shown in Figure 2,
corresponds to the NL utterance “our midfield”. In
the parse tree, the nodes are the MRG productions
and the tokens in upper-case are non-terminals
of the MRG while the tokens in lower-case are
terminals of the MRG, this convention will be
used throughout the paper. As can be seen,
the numbers as well as the productions in the
parse of the MR expression do not correspond to
anything in its natural language utterance. It is
also impossible to derive a semantic parse tree
of this MR expression over its natural language
utterance because there are not enough words in
it to cover all the productions present in the MR
parse at the lowest level. To alleviate this problem,
the provided MRG was manually modified (Kate
et al., 2005) to make it correspond better with
the natural language by replacing such long MR
expressions for soccer regions by shorter expres-
sions like (midfield our)2. This new MRG
was used in all the previous work which uses the
CLANG corpus. In the next sections of the paper,
we will present methods to automatically obtain a
</bodyText>
<footnote confidence="0.99980325">
1http://www.robocup.org
2The names for the new tokens introduced were chosen for
readability and their similarity to the natural language words
is inconsequential for learning semantic parsers.
</footnote>
<page confidence="0.998429">
34
</page>
<figure confidence="0.9349654">
(a) ANSWER —. answer ( RIVER )
RIVER —. longest ( RIVER )
RIVER —. river ( LOCATIONS )
LOCATIONS —. loc 2 ( STATE )
STATE —. STATEID
STATEID —. stateid ( ‘Texas’ )
(b) ANSWER —. answer ( RIVER )
RIVER —. QUALIFIER ( RIVER )
QUALIFIER —. longest RIVER —. river ( LOCATIONS )
STATEID —. stateid ( ‘Texas’ )
</figure>
<figureCaption confidence="0.999183">
Figure 3: Different parse trees obtained for the MR
</figureCaption>
<construct confidence="0.511137666666667">
“answer(longest(river(loc 2(stateid(‘Texas’)))))” correspond-
ing to the NL sentence “Which is the longest river in Texas?”
using (a) a simple MRG (b) a manually designed MRG.
</construct>
<bodyText confidence="0.999536675">
better MRG which corresponds well with the NL
semantics.
Figure 1 (b) shows an NL sentence and its MR
from the GEOQUERY domain (Zelle and Mooney,
1996) which consists of a database of U.S. geo-
graphical facts about which a user can query. The
MRL used for GEOQUERY in some of the previ-
ous work is a variable-free functional query lan-
guage, that was constructed from the original MRs
in Prolog (Kate et al., 2005). From this MRL, the
MRG was then manually written so that its pro-
ductions were compatible with the semantics ex-
pressible in natural language. This MRG was dif-
ferent from some simple MRG one would other-
wise design for the MRL. Figure 3 (a) shows the
parse tree obtained using a simple MRG for the
MR shown in Figure 1 (b). The MR parse ob-
tained using the simple MRG is more like a linear
chain which means that in a semantic parse of the
NL sentence each production will have to corre-
spond to the entire sentence. But ideally, different
productions should correspond to the meanings of
different substrings of the sentence. Figure 3 (b)
shows a parse tree obtained using the manually de-
signed MRG in which the productions QUALIFIER
→ longest and LOC 2 → loc 2 would correspond to
the semantic concepts of “longest” and “located
in” that are expressible in natural language.
Finally, Figure 1 (c) shows the same NL sen-
tence from the GEOQUERY domain but the MR
in SQL which is the standard database query lan-
guage. The inner expression finds the length of the
longest river in Texas and then the outer expres-
sion finds the river in Texas which has that length.
Due to space restriction, we are not showing the
parse tree for this SQL MR, but its incompatibil-
ity with the NL sentence can be seen from the MR
itself because part of the query repeats itself with
’Texas’ appearing twice while in the NL sen-
tence everything is said only once.
</bodyText>
<subsectionHeader confidence="0.9931">
2.2 KRISP: A Semantic Parser Learning
System
</subsectionHeader>
<bodyText confidence="0.999971461538462">
We very briefly describe the semantic parser learn-
ing system, KRISP (Kate and Mooney, 2006),
which we will use as a base system for transform-
ing MRGs, we however note that the MRG trans-
formation methods presented in this paper are gen-
eral enough to work with any system which learns
semantic parser using MRGs. KRISP (Kernel-
based Robust Interpretation for Semantic Parsing)
is a supervised learning system for semantic pars-
ing which takes NL sentences paired with their
MRs as training data. The productions of the MRG
are treated like semantic concepts. For each of
these productions, a Support-Vector Machine clas-
sifier is trained using string similarity as the ker-
nel (Lodhi et al., 2002). Each classifier can then
estimate the probability of any NL substring rep-
resenting the semantic concept for its production.
During semantic parsing, the classifiers are called
to estimate probabilities on different substrings of
the sentence to compositionally build the most
probable MR parse over the entire sentence with
its productions covering different substrings of the
sentence. KRISP was shown to perform competi-
tively with other existing semantic parser learning
systems and was shown to be particularly robust to
noisy NL input.
</bodyText>
<sectionHeader confidence="0.974263" genericHeader="method">
3 Transforming MRGs Using Operators
</sectionHeader>
<bodyText confidence="0.999927818181818">
This section describes an approach to transform
an MRG using grammar transformation operators
to conform it better with the NL semantics. The
following section will present another approach
for transforming an MRG using macros which is
sometimes more directly applicable.
The MRLs used for semantic parsing are always
assumed to be context-free which is true for al-
most all executable computer languages. There
has been some work in learning context-free gram-
mars (CFGs) for a language given several exam-
</bodyText>
<figure confidence="0.333991">
LOCATIONS —. LOC 2 ( STATE)
LOC 2 —. loc 2 STATE —. STATEID
</figure>
<page confidence="0.984281">
35
</page>
<bodyText confidence="0.9999534">
ples of its expressions (Lee, 1996). Most of the
approaches directly learn a grammar from the ex-
pressions but there also have been approaches that
first start with a simple grammar and then trans-
form it using suitable operators to a better gram-
mar (Langley and Stromsten, 2000). The goodness
for a grammar is typically measured in terms of its
simplicity and coverage. Langley and Stromsten
(2000) transform syntactic grammars for NL sen-
tences. To our best knowledge, there is no previous
work on transforming MRGs for semantic parsing.
For this task, since an initial MRG is always given
with the MRL, there is no need to first learn it from
its MRs. The next subsection describes the opera-
tors our method uses to transform an initial MRG.
The subsection following that then describes how
and when the operators are applied to transform the
MRG during training. Our criteria for goodness of
an MRG is the performance of the semantic parser
learned using that MRG.
</bodyText>
<subsectionHeader confidence="0.997283">
3.1 Transformation Operators
</subsectionHeader>
<bodyText confidence="0.9999059">
We describe five transformation operators which
are used to transform an MRG. Each of these op-
erators preserves the coverage of the grammar,
i.e. after application of the operator, the trans-
formed grammar generates the same language that
the previous grammar generated3. The MRs do
not change but only the way they are parsed may
change because of grammar transformations. This
is important because the MRs are to be used in an
application and hence should not be changed.
</bodyText>
<listItem confidence="0.7962365">
1. Create Non-terminal from a Terminal
(CreateNT): Given a terminal symbol t in the
</listItem>
<bodyText confidence="0.997773076923077">
grammar, this operator adds a new production
T → t to it and replaces all the occurrences of
the terminal t in all the other productions by the
new non-terminal T. In the context of seman-
tic parsing learning algorithm, this operator intro-
duces a new semantic concept the previous gram-
mar was not explicit about. For example, this oper-
ator may introduce a production (a semantic con-
cept) LONGEST → longest to the simple grammar
whose parse was shown in Figure 3 (a). This is
close to the production QUALIFIER → longest of the
manual grammar used in the parse shown in Fig-
ure 3 (b).
</bodyText>
<listItem confidence="0.778959333333333">
2. Merge Non-terminals (MergeNT): This op-
erator merges n non-terminals T1, T2, ..., Tn, by
introducing n productions T → T1, T → T2, ...,
</listItem>
<footnote confidence="0.628991">
3This is also known as weak equivalence of grammars.
</footnote>
<bodyText confidence="0.99496685">
T → Tn where T is a new non-terminal. All the
occurrences of the merged non-terminals on the
right-hand-side (RHS) of all the remaining produc-
tions are then replaced by the non-terminal T. In
order to ensure that this operator preserves the cov-
erage of the grammar, before applying it, it is made
sure that if one of these non-terminals, say T1, oc-
curs on the RHS of a production Tr1 then there also
exist productions Tr2, ..., Trn which are same as Tr1
except that T2, ..., Tn respectively occur in them
in place of T1. If this condition is violated for any
production of any of the n non-terminals then this
operator is not applicable. This operator enables
generalization of some non-terminals which occur
in similar contexts leading to generalization of pro-
ductions in which they occur on the RHS. For ex-
ample, this operator may generalize non-terminals
LONGEST and SHORTEST in GEOQUERY MRG to
form QUALIFIER4 → LONGEST and QUALIFIER →
SHORTEST productions.
</bodyText>
<listItem confidence="0.9889343">
3. Combine Two Non-terminals (Combi-
neNT): This operator combines two non-terminals
T1 and T2 into one new non-terminal T by intro-
ducing a new production T → T1 T2. All the
instances of T1 and T2 occurring adjacent in this
order on the RHS (with at least one more non-
terminal5) of all the other productions are replaced
by the new non-terminal T. For example, the pro-
duction A → a B T1 T2 will be changed to A → a
B T. This operator will not eliminate other occur-
</listItem>
<bodyText confidence="0.884227888888889">
rences of T1 and T2 on the RHS of other produc-
tions in which they do not occur adjacent to each
other. In the context of semantic parsing, this op-
erator adds an extra level in the MR parses which
does not seem to be useful in itself, but later if
the non-terminals T1 and T2 get eliminated (by the
application of the DeleteProd operator described
shortly), this operator will be combining the con-
cepts represented by the two non-terminals.
</bodyText>
<listItem confidence="0.989006125">
4. Remove Duplicate Non-terminals (Re-
moveDuplNT): If a production has the same non-
terminal appearing twice on its RHS then this op-
erator adds an additional production which differs
from the first production in that it has only one oc-
currence of that non-terminal. For example, if a
production is A → b C D C, then this operator
will introduce a new production A → b C D re-
</listItem>
<footnote confidence="0.9277012">
4A system generated name will be given to the new non-
terminal.
5Without the presence of an extra non-terminal on the
RHS, this change will merely add redundancy to the parse
trees using this production.
</footnote>
<page confidence="0.997381">
36
</page>
<bodyText confidence="0.996207558823529">
moving the second occurrence of the non-terminal
C. This operator is applied only when the subtrees
under the duplicate non-terminals of the produc-
tion are often found to be the same in the parse
trees of the MRs in the training data. As such this
operator will change the MRL the new MRG will
generate, but this can be easily reverted by appro-
priately duplicating the subtrees in its generated
MR parses in accordance to the original produc-
tion. This operator is useful during learning a se-
mantic parser because it eliminates the type of in-
compatibility between MRs and NL sentences il-
lustrated with Figure 1 (c) in Subsection 2.1.
5. Delete Production (DeleteProd): This last
operator deletes a production and replaces the oc-
currences of its left-hand-side (LHS) non-terminal
with its RHS in the RHS of all the other produc-
tions. In terms of semantic parsing, this operator
eliminates the need to learn a semantic concept. It
can undo the transformations obtained by the other
operators by deleting the new productions they in-
troduce.
We note that the CombineNT and MergeNT op-
erators are same as the two operators used by Lan-
gley and Stromsten (2000) to search a good syntac-
tic grammar for natural language sentences from
the space of its possible grammars. We also note
that the applications of CreateNT and CombineNT
operators can reduce a CFG to its Chomsky nor-
mal form6, and conversely, because of the reverse
transformations achieved by the DeleteProd opera-
tor, a Chomsky normal form of a CFG can be con-
verted into any other CFG which accepts the same
language.
</bodyText>
<subsectionHeader confidence="0.999458">
3.2 Applying Transformation Operators
</subsectionHeader>
<bodyText confidence="0.999945076923077">
In order to transform an MRG to improve semantic
parsing, since a simple hill-climbing type approach
to search the space of all possible MRGs will be
computationally very intensive, we use the follow-
ing error-driven heuristic search which is faster al-
though less thorough.
First, using the provided MRG and the training
data, a semantic parser is trained using KRISP. The
trained semantic parser is applied to each of the
training NL sentences. Next, for each production 7r
in the MRG, two values total, and incorrect, are
computed. The value total, counts how many MR
parses from the training examples use the produc-
</bodyText>
<footnote confidence="0.619385">
6In which all the productions are of the form A → a or
A→ B C.
</footnote>
<bodyText confidence="0.999843583333333">
tion 7r. The value incorrect, counts the number
of training examples for which the semantic parser
incorrectly uses the production 7r, i.e. it either did
not include the production 7r in the parse of the MR
it produces when the correct MR’s parse included
it, or it included the production 7r when it was not
present in the correct MR’s parse. These two statis-
tics for a production indicate how well the seman-
tic parser was able to use the production in seman-
tic parsing. If it was not able to use a production 7r
well, then the ratio incorrect,/total,, which we
call mistakeRatio,, will be high indicating that
some change needs to be made to that production.
After computing these values for all the produc-
tions, the procedure described below for applying
the first type of operator is followed. After this,
the MRs in the training data are re-parsed using
the new MRG, the semantic parser is re-trained and
the total, and incorrect, values are re-computed.
Next, the procedure for applying the next operator
is followed and so on. The whole process is re-
peated for a specified number of iterations. In the
experiments, we found that the performance does
not improve much after two iterations.
</bodyText>
<listItem confidence="0.928403727272727">
1. Apply CreateNT: For each terminal t in the
grammar, totals and incorrects values are com-
puted by summing up the corresponding values for
all the productions in which t occurs on the RHS
with at least one non-terminal7. If totals is greater
than Q (a parameter) and mistakeRatios =
incorrects/totals is greater than α (another pa-
rameter), then the CreateNT operator is applied,
provided the production T —* t is not already
present.
2. Apply MergeNT: All the non-terminals oc-
</listItem>
<bodyText confidence="0.922415777777778">
curring on the RHS of all those productions 7r are
collected whose mistakeRatio, value is greater
than α and whose total, value is greater than Q.
The set of these non-terminals is then partitioned
such that the criteria for applying the MergeNT
is satisfied by the non-terminals in each partition
with size at least two. The MergeNT operator is
then applied to the non-terminals in each partition
with size at least two.
</bodyText>
<sectionHeader confidence="0.302471" genericHeader="method">
3. Apply CombineNT: For every non-terminal
</sectionHeader>
<bodyText confidence="0.6835565">
pair T1 and T2, totalT1T2 and incorrectT1T2 val-
ues are computed by summing their correspond-
ing values for the productions in which the two
non-terminals are adjacent in the RHS in the
</bodyText>
<footnote confidence="0.734231333333333">
7Without a non-terminal on the RHS, the operator will
only add a redundant level to the parses which use this pro-
duction.
</footnote>
<page confidence="0.999194">
37
</page>
<bodyText confidence="0.9786676">
presence of at least one more non-terminal. If
mistakeRatioT1T2 = incorectT1T2/totalT1T2 is
greater than α and totalT1T2 is greater than Q, then
the CombineNT operator is applied to these two
non-terminals.
</bodyText>
<listItem confidence="0.993874285714286">
4. Apply RemoveDuplNT: If a production
7r has duplicate non-terminals on the RHS under
which the same subtrees are found in the MR parse
trees of the training data more than once then this
operator is applied provided its mistakeRatio, is
greater than α and total, is greater than Q.
5. Apply DeleteProd: The DeleteProd opera-
</listItem>
<bodyText confidence="0.898085285714286">
tor is applied to all the productions 7r and whose
mistakeRatio, is greater than α and total, is
greater than Q. This step simply deletes the pro-
ductions which are mostly incorrectly used.
For the experiments, we set the α parameter to
0.75 and Q parameter to 5, these values were de-
termined through pilot experiments.
</bodyText>
<sectionHeader confidence="0.979035" genericHeader="method">
4 Transforming MRGs Using Macros
</sectionHeader>
<bodyText confidence="0.999975285714286">
As was illustrated with Figure 2 in Subsection 2.1,
sometimes there can be large parses for MR ex-
pressions which do not correspond well with their
semantics in the natural language. While it is pos-
sible to transform the MRG using the operators
described in the previous section to reduce a sub-
tree of the parse to just one production which will
then correspond directly to its meaning in the nat-
ural language, it will require a particular sequence
of transformation operators to achieve this which
may rarely happen during the heuristic search used
in the MRG transformation algorithm. In this sec-
tion, we describe a more direct way of obtaining
such transformations using macros.
</bodyText>
<subsectionHeader confidence="0.999769">
4.1 Meaning Representation Macros
</subsectionHeader>
<bodyText confidence="0.961906421052632">
A meaning representation macro for an MRG is a
production formed by combining two or more ex-
isting productions of the MRG. For example, for
the CLANG example shown in Figure 2, the pro-
duction REGION → (rec(pt -32 -35)(pt 0 35)) is a mean-
ing representation macro. There could also be non-
terminals on its RHS. From an MR parse drawn
with non-terminals at the internal nodes (instead of
productions), a macro can be derived from a sub-
tree8 rooted at any of the internal nodes by making
its root as the LHS non-terminal and the left-to-
right sequence formed by its leaves (which could
8Each node of a subtree must either include all the chil-
dren nodes of the corresponding node from the original tree
or none of them.
be non-terminals) as the RHS. We use the follow-
ing error-driven procedure to introduce macros in
the MRG in order to improve the performance of
semantic parsing.
</bodyText>
<subsectionHeader confidence="0.9960065">
4.2 Learning Meaning Representation
Macros
</subsectionHeader>
<bodyText confidence="0.999962909090909">
A semantic parser is first learned from the train-
ing data using KRISP and the given MRG. The
learned semantic parser is then applied to the train-
ing sentences and if the system can not produce
any parse for a sentence then the parse tree of its
corresponding MR is included in a set called failed
parse trees. Common subtrees in these failed parse
trees are likely to be good candidates for introduc-
ing macros. Then a set of candidate trees is cre-
ated as follows. This set is first initialized to the
set of failed parse trees. The largest common sub-
tree of every pair of trees in the candidate trees is
then also included in this set if it is not an empty
tree. The process continues with the newly added
trees until no new tree can be included. This pro-
cess is similar to the repeated bottom-up general-
ization of clauses used in the inductive logic pro-
gramming system GOLEM (Muggleton and Feng,
1992). Next, the trees in this set are sorted based
on the number of failed parse trees of which they
are a subtree. The trees which are part of fewer
than Q subtrees are removed. Then in highest to
lowest order, the trees are selected one-by-one to
form macros, provided their height is greater than
two (otherwise it will be an already existing MRG
production) and an already selected tree is not its
subtree. A macro is formed from a tree by mak-
ing the non-terminal root of the tree as its LHS
non-terminal and the left-to-right sequence of the
leaves as its RHS.
These newly formed macros (productions) are
then included in the MRG. The MRs in the train-
ing data are re-parsed and the semantic parser is
re-trained using the new MRG. In order to delete
the macros which were not found useful, a pro-
cedure similar to the application of DeleteProd is
used. The total, and incorrect, values for all the
macros are computed in a manner similar to de-
scribed in the previous section. The macros for
which mistakeRatio, = total,/incorrect, is
greater than α and total, is greater than Q are re-
moved. This whole procedure of adding and delet-
ing macros is repeated a specified number of it-
erations. In the experiments, we found that two
</bodyText>
<page confidence="0.994045">
38
</page>
<figure confidence="0.994903">
0 10 20 30 40 50 60 70 80 90 100
Recall
</figure>
<figureCaption confidence="0.989581666666667">
Figure 4: The results comparing the performances of the
learned semantic parsers on the GEOUQERY domain with the
functional query language using different MRGs.
</figureCaption>
<figure confidence="0.9913035">
0 10 20 30 40 50 60 70 80 90 100
Recall
</figure>
<figureCaption confidence="0.756900333333333">
Figure 5: The results comparing the performances of
the learned semantic parsers on the GEOUQERY domain with
SQL as the MRL using different MRGs.
</figureCaption>
<bodyText confidence="0.745477">
iterations are usually sufficient.
</bodyText>
<sectionHeader confidence="0.998867" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998681045454546">
We tested our MRG transformation methods with
MRGs of three different MRLs which were de-
scribed in the Background section. In each case,
we first transformed the given MRG using macros
and then using grammar transformation operators.
The training and testing was done using standard
10-fold cross-validation and the performance was
measured in terms of precision (the percentage of
generated MRs that were correct) and recall (the
percentage of all sentences for which correct MRs
were obtained). Since we wanted to evaluate how
the grammar transformation changes the perfor-
mance on the semantic parsing task, in each of
the experiments, we used the same system, KRISP,
and compared how it performs when trained using
different MRGs for the same MRL. Since KRISP
assigns confidences to the MRs it generates, an en-
tire range of precision-recall trade-off was plotted
by measuring precision and recall values at various
confidence levels.
Figure 4 shows the results on the GEOQUERY
domain using the functional query language whose
</bodyText>
<figure confidence="0.955784">
0 10 20 30 40 50 60 70 80 90 100
Recall
</figure>
<figureCaption confidence="0.988611333333333">
Figure 6: The results comparing the performances of the
learned semantic parsers on the CLANG corpus using different
MRGs.
</figureCaption>
<bodyText confidence="0.999851078947368">
corpus contained total 880 NL-MR pairs. As can
be seen, the performance of the semantic parser
that KRISP learns when trained using the initial
simple MRG for the MRL is not good. But
when that MRG is transformed, the performance
of the semantic parser dramatically improves and
is very close to the performance obtained with the
manually-engineered grammar. The macro trans-
formations did not help improve the performance
with this MRG, and most of the the performance
gain was obtained because of the CreateNT and
DeleteProd operators.
We next tested our MRG transformation algo-
rithm on SQL as the MRL for the GEOQUERY do-
main. This corpus contains 700 NL-MR pairs in
which the NL sentences were taken from the orig-
inal 880 examples. This corpus was previously
used to evaluate the PRECISION system (Popescu
et al., 2003), but since that system is not a machine
learning system, its results cannot be directly com-
pared with ours. The initial MRG we used con-
tained the basic SQL productions. Figure 5 shows
that results improve by a large amount after MRG
transformations. We did not have any manually-
engineered MRG for SQL for this domain avail-
able to us. With this MRG, most of the improve-
ment was obtained using the macros and the Re-
moveDuplNT transformation operator.
Finally, we tested our MRG transformation al-
gorithm on the CLANG domain using its origi-
nal MRG in which all the chief regions of the
soccer field were in the form of numeric MR ex-
pressions which do not correspond to their mean-
ings in the natural language. Its corpus contains
300 examples of NL-MR pairs. Figure 6 shows
the results. After applying the MRG transforma-
tions the performance improved by a large margin.
The gain was due to transformations obtained us-
</bodyText>
<figure confidence="0.999098606060606">
Manual grammar
Transformed grammar
Initial grammar
Precision
100
80
50
40
30
90
70
60
Transformed grammar
Initial grammar
Manual grammar
Transformed grammar
Initial grammar
Precision 100
90
80
70
60
50
40
30
Precision 100
90
80
70
60
50
40
30
</figure>
<page confidence="0.997581">
39
</page>
<bodyText confidence="0.999982666666667">
ing macros while the grammar transformation op-
erators did not help with this MRG. Although the
precision was lower for low recall values, the re-
call increased by a large quantity and the best F-
measure improved from 50% to 63%. But the per-
formance still lagged behind that obtained using
the manually-engineered MRG. The main reason
for this is that the manual MRG introduced some
domain specific expressions, like left, right,
left-quarter etc., which correspond directly
to their meanings in the natural language. On
the other hand, the only way to specify “left” of
a region using the original CLANG MRG is by
specifying the coordinates of the left region, like
(rec(pt -32 -35)(pt 0 0)) is the left of
(rec (pt -32 -35) (pt 0 35)) etc. It
is not possible to learn the concept of “left” from
such expressions even with MRG transformations.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999950416666667">
A meaning representation grammar which does not
correspond well with the natural language seman-
tics can lead to a poor performance by a learned
semantic parser. This paper presented grammar
transformation operators and meaning representa-
tion macros using which the meaning representa-
tion grammar can be transformed to make it better
conform with the semantics of natural language.
Experimental results on three different grammars
demonstrated that the performance on semantic
parsing task can be improved by large amounts by
transforming the grammars.
</bodyText>
<sectionHeader confidence="0.998225" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999869">
I would like to thank Raymond Mooney for many
useful discussions regarding the work described in
this paper.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999326822222223">
Chen et al. 2003. Users manual: RoboCup soccer server manual for soc-
cer server version 7.07 and later. Available at http://sourceforge.
net/projects/sserver/.
Ge, R. and R. J. Mooney. 2005. A statistical semantic parser that integrates
syntax and semantics. In Proc. of CoNLL-2005, pages 9–16, Ann Arbor,
MI.
Kate, R. J. and R. J. Mooney. 2006. Using string-kernels for learning se-
mantic parsers. In Proc. of COLING/ACL-2006, pages 913–920, Sydney,
Australia.
Kate, R. J., Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural
to formal languages. In Proc. ofAAAI-2005, pages 1062–1068, Pittsburgh,
PA.
Langley, Pat and Sean Stromsten. 2000. Learning context-free gramamr with a
simplicity bias. In Proc. of ECML-2000, pages 220–228, Barcelona, Spain.
Lee, Lillian. 1996. Learning of context-free languages: A survey of the lit-
erature. Technical Report TR-12-96, Center for Research in Computing
Technology, Harvard University.
Lodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris
Watkins. 2002. Text classification using string kernels. Journal ofMachine
Learning Research, 2:419–444.
Muggleton, Stephen and C. Feng. 1992. Efficient induction of logic programs.
In Muggleton, Stephen, editor, Inductive Logic Programming, pages 281–
297. Academic Press, New York.
Nguyen, Le-Minh, Akira Shimazu, and Xuan-Hieu Phan. 2006. Semantic
parsing with structured SVM ensemble classification models. In Proc. of
COLING/ACL 2006 Main Conf. Poster Sessions, pages 619–626, Sydney,
Australia.
Popescu, Ana-Maria, Oren Etzioni, and Henry Kautz. 2003. Towards a theory
of natural language interfaces to databases. In Proc. of IUI-2003, pages
149–157, Miami, FL.
Tang, L. R. and R. J. Mooney. 2001. Using multiple clause constructors in in-
ductive logic programming for semantic parsing. In Proc. of ECML-2001,
pages 466–477, Freiburg, Germany.
Wong, Y. W. and R. Mooney. 2006. Learning for semantic parsing with statis-
tical machine translation. In Proc. of HLT/NAACL-2006, pages 439–446,
New York City, NY.
Wong, Y. W. and R. J. Mooney. 2007. Learning synchronous grammars for
semantic parsing with lambda calculus. In Proc. ofACL-2007, pages 960–
967, Prague, Czech Republic.
Zelle, J. M. and R. J. Mooney. 1996. Learning to parse database queries using
inductive logic programming. In Proc. of AAAI-1996, pages 1050–1055,
Portland, OR.
Zettlemoyer, Luke S. and Michael Collins. 2007. Online learning of relaxed
CCG grammars for parsing to logical form. In Proc. of EMNLP-CoNLL-
2007, pages 678–687, Prague, Czech Republic.
</reference>
<page confidence="0.998637">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521060">
<title confidence="0.9990615">Transforming Meaning Representation Grammars to Improve Semantic Parsing</title>
<author confidence="0.999445">J Rohit</author>
<affiliation confidence="0.88150925">of Computer The University of Texas at 1 University Station Austin, TX 78712-0233,</affiliation>
<email confidence="0.999835">rjkate@cs.utexas.edu</email>
<abstract confidence="0.999509727272727">A semantic parser learning system learns to map natural language sentences into their domain-specific formal meaning representations, but if the constructs of the meaning representation language do not correspond well with the natural language then the system may not learn a good semantic parser. This paper presents approaches for automatically transforming a meaning representation grammar (MRG) to conform it better with the natural language semantics. It introduces grammar transformation operators and meaning representation macros which are applied in an error-driven manner to transform an MRG while training a semantic parser learning system. Experimental results show that the automatically transformed MRGs lead to better learned semantic parsers which perform comparable to the semantic parsers learned using manually engineered MRGs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chen</author>
</authors>
<title>Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http://sourceforge.</title>
<date>2003</date>
<note>net/projects/sserver/.</note>
<marker>Chen, 2003</marker>
<rawString>Chen et al. 2003. Users manual: RoboCup soccer server manual for soccer server version 7.07 and later. Available at http://sourceforge. net/projects/sserver/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>A statistical semantic parser that integrates syntax and semantics.</title>
<date>2005</date>
<booktitle>In Proc. of CoNLL-2005,</booktitle>
<pages>9--16</pages>
<location>Ann Arbor, MI.</location>
<marker>Ge, Mooney, 2005</marker>
<rawString>Ge, R. and R. J. Mooney. 2005. A statistical semantic parser that integrates syntax and semantics. In Proc. of CoNLL-2005, pages 9–16, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>R J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL-2006,</booktitle>
<pages>913--920</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2322" citStr="Kate and Mooney, 2006" startWordPosition="339" endWordPosition="342">censes/by-nc-sa/3.0/). Some rights reserved. query language to execute database queries. A machine learning system for semantic parsing takes NL sentences paired with their respective MRs as training data and induces a semantic parser which can then map novel NL sentences into their MRs. The grammar of an MRL, which we will call meaning representation grammar (MRG), is assumed to be deterministic and context-free which is true for grammars of almost all the computer executable languages. A semantic parsing learning system typically exploits the given MRG of the MRL to learn a semantic parser (Kate and Mooney, 2006; Wong and Mooney, 2006). Although in different ways, but the systems presented in these papers learn how the NL phrases relate to the productions of the MRG, and using this information they parse a test sentence to compositionally generate its best MR. In order to learn a good semantic parser, it is necessary that the productions of the MRG accurately represent the semantics being expressed by the natural language. However, an MRL and its MRG are typically designed to best suit the application with little consideration for how well they correspond to the semantics of a natural language. Some </context>
<context position="5207" citStr="Kate and Mooney, 2006" startWordPosition="827" endWordPosition="830">not correspond well with natural language semantics then manually re-engineering the MRG to work well for semantic parsing is a tedious task and requires considerable domain expertise. In this paper, we present methods to automatically transform a given MRG to make it more suitable for learning semantic parsers. No previous work addresses this issue to our best knowledge. We introduce grammar transformation operators and meaning representation macros to transform an MRG. We describe how these are applied in an errordriven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. Our approach, however, is general enough to improve any semantic parser learning system which uses productions of the MRG. We present experimental results with three very different MRLs to show how these grammar transformations improve the semantic parsing performance. 2 Background The following subsection gives some examples of semantic parsing domains and their corresponding MRLs and illustrates why incompatibility between MRGs and natural language could hurt semantic parsing. The next subsection then briefly describes a base semantic parser le</context>
<context position="10738" citStr="Kate and Mooney, 2006" startWordPosition="1785" endWordPosition="1788"> the MR in SQL which is the standard database query language. The inner expression finds the length of the longest river in Texas and then the outer expression finds the river in Texas which has that length. Due to space restriction, we are not showing the parse tree for this SQL MR, but its incompatibility with the NL sentence can be seen from the MR itself because part of the query repeats itself with ’Texas’ appearing twice while in the NL sentence everything is said only once. 2.2 KRISP: A Semantic Parser Learning System We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. KRISP (Kernelbased Robust Interpretation for Semantic Parsing) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. The productions of the MRG are treated like semantic concepts. For each of these productions, a Support-Vector Machine classifier is trained using string similarity as the kernel (Lodhi et al., 2002)</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Kate, R. J. and R. J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proc. of COLING/ACL-2006, pages 913–920, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Kate</author>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages. In</title>
<date>2005</date>
<booktitle>Proc. ofAAAI-2005,</booktitle>
<pages>1062--1068</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="6588" citStr="Kate et al., 2005" startWordPosition="1048" endWordPosition="1051">nding MR in an MRL called CLANG which is a formal declarFigure 2: The parse for the CLANG expression “(rec (pt -32 -35) (pt 0 35))” corresponding to the natural language utterance “our midfield” using its original MRG. ative language with LISP-like prefix notation designed to instruct simulated soccer players in the RoboCup1 Coach Competition. The MRL and its MRG was designed by the Coach Competition community (Chen et al., 2003) to suit the requirements of their application independent of how well the MRG conforms with the natural language semantics. They were, in fact, not aware that later (Kate et al., 2005) this will be introduced as a test domain for learning semantic parsers. In this original MRG for CLANG, there are several constructs which do not correspond well with their meanings in the natural language. For example, the MR expression of the rectangle (rec (pt -32 -35) (pt 0 35)) from the example MR in Figure 1 (a), whose parse according to the original MRG is shown in Figure 2, corresponds to the NL utterance “our midfield”. In the parse tree, the nodes are the MRG productions and the tokens in upper-case are non-terminals of the MRG while the tokens in lower-case are terminals of the MRG</context>
<context position="9165" citStr="Kate et al., 2005" startWordPosition="1500" endWordPosition="1503"> parse trees obtained for the MR “answer(longest(river(loc 2(stateid(‘Texas’)))))” corresponding to the NL sentence “Which is the longest river in Texas?” using (a) a simple MRG (b) a manually designed MRG. better MRG which corresponds well with the NL semantics. Figure 1 (b) shows an NL sentence and its MR from the GEOQUERY domain (Zelle and Mooney, 1996) which consists of a database of U.S. geographical facts about which a user can query. The MRL used for GEOQUERY in some of the previous work is a variable-free functional query language, that was constructed from the original MRs in Prolog (Kate et al., 2005). From this MRL, the MRG was then manually written so that its productions were compatible with the semantics expressible in natural language. This MRG was different from some simple MRG one would otherwise design for the MRL. Figure 3 (a) shows the parse tree obtained using a simple MRG for the MR shown in Figure 1 (b). The MR parse obtained using the simple MRG is more like a linear chain which means that in a semantic parse of the NL sentence each production will have to correspond to the entire sentence. But ideally, different productions should correspond to the meanings of different subs</context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Kate, R. J., Y. W. Wong, and R. J. Mooney. 2005. Learning to transform natural to formal languages. In Proc. ofAAAI-2005, pages 1062–1068, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pat Langley</author>
<author>Sean Stromsten</author>
</authors>
<title>Learning context-free gramamr with a simplicity bias.</title>
<date>2000</date>
<booktitle>In Proc. of ECML-2000,</booktitle>
<pages>220--228</pages>
<location>Barcelona,</location>
<contexts>
<context position="12760" citStr="Langley and Stromsten, 2000" startWordPosition="2114" endWordPosition="2117">ing macros which is sometimes more directly applicable. The MRLs used for semantic parsing are always assumed to be context-free which is true for almost all executable computer languages. There has been some work in learning context-free grammars (CFGs) for a language given several examLOCATIONS —. LOC 2 ( STATE) LOC 2 —. loc 2 STATE —. STATEID 35 ples of its expressions (Lee, 1996). Most of the approaches directly learn a grammar from the expressions but there also have been approaches that first start with a simple grammar and then transform it using suitable operators to a better grammar (Langley and Stromsten, 2000). The goodness for a grammar is typically measured in terms of its simplicity and coverage. Langley and Stromsten (2000) transform syntactic grammars for NL sentences. To our best knowledge, there is no previous work on transforming MRGs for semantic parsing. For this task, since an initial MRG is always given with the MRL, there is no need to first learn it from its MRs. The next subsection describes the operators our method uses to transform an initial MRG. The subsection following that then describes how and when the operators are applied to transform the MRG during training. Our criteria f</context>
<context position="18400" citStr="Langley and Stromsten (2000)" startWordPosition="3107" endWordPosition="3111">es the type of incompatibility between MRs and NL sentences illustrated with Figure 1 (c) in Subsection 2.1. 5. Delete Production (DeleteProd): This last operator deletes a production and replaces the occurrences of its left-hand-side (LHS) non-terminal with its RHS in the RHS of all the other productions. In terms of semantic parsing, this operator eliminates the need to learn a semantic concept. It can undo the transformations obtained by the other operators by deleting the new productions they introduce. We note that the CombineNT and MergeNT operators are same as the two operators used by Langley and Stromsten (2000) to search a good syntactic grammar for natural language sentences from the space of its possible grammars. We also note that the applications of CreateNT and CombineNT operators can reduce a CFG to its Chomsky normal form6, and conversely, because of the reverse transformations achieved by the DeleteProd operator, a Chomsky normal form of a CFG can be converted into any other CFG which accepts the same language. 3.2 Applying Transformation Operators In order to transform an MRG to improve semantic parsing, since a simple hill-climbing type approach to search the space of all possible MRGs wil</context>
</contexts>
<marker>Langley, Stromsten, 2000</marker>
<rawString>Langley, Pat and Sean Stromsten. 2000. Learning context-free gramamr with a simplicity bias. In Proc. of ECML-2000, pages 220–228, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Learning of context-free languages: A survey of the literature.</title>
<date>1996</date>
<tech>Technical Report TR-12-96,</tech>
<institution>Center for Research in Computing Technology, Harvard University.</institution>
<contexts>
<context position="12518" citStr="Lee, 1996" startWordPosition="2074" endWordPosition="2075">erators This section describes an approach to transform an MRG using grammar transformation operators to conform it better with the NL semantics. The following section will present another approach for transforming an MRG using macros which is sometimes more directly applicable. The MRLs used for semantic parsing are always assumed to be context-free which is true for almost all executable computer languages. There has been some work in learning context-free grammars (CFGs) for a language given several examLOCATIONS —. LOC 2 ( STATE) LOC 2 —. loc 2 STATE —. STATEID 35 ples of its expressions (Lee, 1996). Most of the approaches directly learn a grammar from the expressions but there also have been approaches that first start with a simple grammar and then transform it using suitable operators to a better grammar (Langley and Stromsten, 2000). The goodness for a grammar is typically measured in terms of its simplicity and coverage. Langley and Stromsten (2000) transform syntactic grammars for NL sentences. To our best knowledge, there is no previous work on transforming MRGs for semantic parsing. For this task, since an initial MRG is always given with the MRL, there is no need to first learn </context>
</contexts>
<marker>Lee, 1996</marker>
<rawString>Lee, Lillian. 1996. Learning of context-free languages: A survey of the literature. Technical Report TR-12-96, Center for Research in Computing Technology, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huma Lodhi</author>
<author>Craig Saunders</author>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
<author>Chris Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="11338" citStr="Lodhi et al., 2002" startWordPosition="1886" endWordPosition="1889">e and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. KRISP (Kernelbased Robust Interpretation for Semantic Parsing) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. The productions of the MRG are treated like semantic concepts. For each of these productions, a Support-Vector Machine classifier is trained using string similarity as the kernel (Lodhi et al., 2002). Each classifier can then estimate the probability of any NL substring representing the semantic concept for its production. During semantic parsing, the classifiers are called to estimate probabilities on different substrings of the sentence to compositionally build the most probable MR parse over the entire sentence with its productions covering different substrings of the sentence. KRISP was shown to perform competitively with other existing semantic parser learning systems and was shown to be particularly robust to noisy NL input. 3 Transforming MRGs Using Operators This section describes</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>Lodhi, Huma, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. 2002. Text classification using string kernels. Journal ofMachine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Muggleton</author>
<author>C Feng</author>
</authors>
<title>Efficient induction of logic programs.</title>
<date>1992</date>
<booktitle>Inductive Logic Programming,</booktitle>
<pages>281--297</pages>
<editor>In Muggleton, Stephen, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="25401" citStr="Muggleton and Feng, 1992" startWordPosition="4321" endWordPosition="4324">a set called failed parse trees. Common subtrees in these failed parse trees are likely to be good candidates for introducing macros. Then a set of candidate trees is created as follows. This set is first initialized to the set of failed parse trees. The largest common subtree of every pair of trees in the candidate trees is then also included in this set if it is not an empty tree. The process continues with the newly added trees until no new tree can be included. This process is similar to the repeated bottom-up generalization of clauses used in the inductive logic programming system GOLEM (Muggleton and Feng, 1992). Next, the trees in this set are sorted based on the number of failed parse trees of which they are a subtree. The trees which are part of fewer than Q subtrees are removed. Then in highest to lowest order, the trees are selected one-by-one to form macros, provided their height is greater than two (otherwise it will be an already existing MRG production) and an already selected tree is not its subtree. A macro is formed from a tree by making the non-terminal root of the tree as its LHS non-terminal and the left-to-right sequence of the leaves as its RHS. These newly formed macros (productions</context>
</contexts>
<marker>Muggleton, Feng, 1992</marker>
<rawString>Muggleton, Stephen and C. Feng. 1992. Efficient induction of logic programs. In Muggleton, Stephen, editor, Inductive Logic Programming, pages 281– 297. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le-Minh Nguyen</author>
<author>Akira Shimazu</author>
<author>Xuan-Hieu Phan</author>
</authors>
<title>Semantic parsing with structured SVM ensemble classification models.</title>
<date>2006</date>
<booktitle>In Proc. of COLING/ACL 2006 Main Conf. Poster Sessions,</booktitle>
<pages>619--626</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="4404" citStr="Nguyen et al., 2006" startWordPosition="695" endWordPosition="698">pt -32 -35)(pt 0 35)) (do (player our {5})(pos (pt -5 0)))) (b) NL: Which is the longest river in Texas? POINT —. ( pt NUM NUM) POINT —. ( pt NUM NUM) MR: answer(longest(river(loc_2(stateid(’Texas’))))) NUM —. 0 NUM —. 35 NUM —. -32 NUM —. -35 (c) NL: Which is the longest river in Texas? MR: select river.name from river where river.traverse=’Texas’ and river.length= (select max(river.length) from river where river.traverse=’Texas’); Figure 1: Examples of NL sentences and their MRs from (a) the CLANG domain (b) GEOQUERY domain with functional MRL (c) GEOQUERY domain with SQL. and Mooney, 2005; Nguyen et al., 2006). For these systems to work well, it is also necessary that the semantic labels of the MRL correspond well with natural language semantics. If the MRG of a domain-specific MRL does not correspond well with natural language semantics then manually re-engineering the MRG to work well for semantic parsing is a tedious task and requires considerable domain expertise. In this paper, we present methods to automatically transform a given MRG to make it more suitable for learning semantic parsers. No previous work addresses this issue to our best knowledge. We introduce grammar transformation operator</context>
</contexts>
<marker>Nguyen, Shimazu, Phan, 2006</marker>
<rawString>Nguyen, Le-Minh, Akira Shimazu, and Xuan-Hieu Phan. 2006. Semantic parsing with structured SVM ensemble classification models. In Proc. of COLING/ACL 2006 Main Conf. Poster Sessions, pages 619–626, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
<author>Henry Kautz</author>
</authors>
<title>Towards a theory of natural language interfaces to databases.</title>
<date>2003</date>
<booktitle>In Proc. of IUI-2003,</booktitle>
<pages>149--157</pages>
<location>Miami, FL.</location>
<contexts>
<context position="29079" citStr="Popescu et al., 2003" startWordPosition="4953" endWordPosition="4956"> transformed, the performance of the semantic parser dramatically improves and is very close to the performance obtained with the manually-engineered grammar. The macro transformations did not help improve the performance with this MRG, and most of the the performance gain was obtained because of the CreateNT and DeleteProd operators. We next tested our MRG transformation algorithm on SQL as the MRL for the GEOQUERY domain. This corpus contains 700 NL-MR pairs in which the NL sentences were taken from the original 880 examples. This corpus was previously used to evaluate the PRECISION system (Popescu et al., 2003), but since that system is not a machine learning system, its results cannot be directly compared with ours. The initial MRG we used contained the basic SQL productions. Figure 5 shows that results improve by a large amount after MRG transformations. We did not have any manuallyengineered MRG for SQL for this domain available to us. With this MRG, most of the improvement was obtained using the macros and the RemoveDuplNT transformation operator. Finally, we tested our MRG transformation algorithm on the CLANG domain using its original MRG in which all the chief regions of the soccer field were</context>
</contexts>
<marker>Popescu, Etzioni, Kautz, 2003</marker>
<rawString>Popescu, Ana-Maria, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language interfaces to databases. In Proc. of IUI-2003, pages 149–157, Miami, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tang</author>
<author>R J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Proc. of ECML-2001,</booktitle>
<pages>466--477</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="3021" citStr="Tang and Mooney, 2001" startWordPosition="461" endWordPosition="464">ed in these papers learn how the NL phrases relate to the productions of the MRG, and using this information they parse a test sentence to compositionally generate its best MR. In order to learn a good semantic parser, it is necessary that the productions of the MRG accurately represent the semantics being expressed by the natural language. However, an MRL and its MRG are typically designed to best suit the application with little consideration for how well they correspond to the semantics of a natural language. Some other semantic parser learning systems which need MRL in the form of Prolog (Tang and Mooney, 2001) or A-calculus (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) do not use productions of the MRG but instead use predicates of the MRL. However, in order to learn a good semantic parser, they still require that these predicates correspond well with the semantics of the natural language. There are also systems which learn semantic parsers from more detailed training data in the form of semantically augmented parse trees of NL sentences in which each internal node has a syntactic and a semantic label (Ge 33 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language L</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Tang, L. R. and R. J. Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Proc. of ECML-2001, pages 466–477, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL-2006,</booktitle>
<pages>439--446</pages>
<location>New York City, NY.</location>
<contexts>
<context position="2346" citStr="Wong and Mooney, 2006" startWordPosition="343" endWordPosition="346">Some rights reserved. query language to execute database queries. A machine learning system for semantic parsing takes NL sentences paired with their respective MRs as training data and induces a semantic parser which can then map novel NL sentences into their MRs. The grammar of an MRL, which we will call meaning representation grammar (MRG), is assumed to be deterministic and context-free which is true for grammars of almost all the computer executable languages. A semantic parsing learning system typically exploits the given MRG of the MRL to learn a semantic parser (Kate and Mooney, 2006; Wong and Mooney, 2006). Although in different ways, but the systems presented in these papers learn how the NL phrases relate to the productions of the MRG, and using this information they parse a test sentence to compositionally generate its best MR. In order to learn a good semantic parser, it is necessary that the productions of the MRG accurately represent the semantics being expressed by the natural language. However, an MRL and its MRG are typically designed to best suit the application with little consideration for how well they correspond to the semantics of a natural language. Some other semantic parser le</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Wong, Y. W. and R. Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proc. of HLT/NAACL-2006, pages 439–446, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y W Wong</author>
<author>R J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proc. ofACL-2007,</booktitle>
<pages>960--967</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3090" citStr="Wong and Mooney, 2007" startWordPosition="471" endWordPosition="474"> of the MRG, and using this information they parse a test sentence to compositionally generate its best MR. In order to learn a good semantic parser, it is necessary that the productions of the MRG accurately represent the semantics being expressed by the natural language. However, an MRL and its MRG are typically designed to best suit the application with little consideration for how well they correspond to the semantics of a natural language. Some other semantic parser learning systems which need MRL in the form of Prolog (Tang and Mooney, 2001) or A-calculus (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) do not use productions of the MRG but instead use predicates of the MRL. However, in order to learn a good semantic parser, they still require that these predicates correspond well with the semantics of the natural language. There are also systems which learn semantic parsers from more detailed training data in the form of semantically augmented parse trees of NL sentences in which each internal node has a syntactic and a semantic label (Ge 33 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 33–40 Manchester, August 2008 (a) NL: If the ball is i</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Wong, Y. W. and R. J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proc. ofACL-2007, pages 960– 967, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Zelle</author>
<author>R J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proc. of AAAI-1996,</booktitle>
<pages>1050--1055</pages>
<location>Portland, OR.</location>
<contexts>
<context position="8905" citStr="Zelle and Mooney, 1996" startWordPosition="1452" endWordPosition="1455">IVER —. river ( LOCATIONS ) LOCATIONS —. loc 2 ( STATE ) STATE —. STATEID STATEID —. stateid ( ‘Texas’ ) (b) ANSWER —. answer ( RIVER ) RIVER —. QUALIFIER ( RIVER ) QUALIFIER —. longest RIVER —. river ( LOCATIONS ) STATEID —. stateid ( ‘Texas’ ) Figure 3: Different parse trees obtained for the MR “answer(longest(river(loc 2(stateid(‘Texas’)))))” corresponding to the NL sentence “Which is the longest river in Texas?” using (a) a simple MRG (b) a manually designed MRG. better MRG which corresponds well with the NL semantics. Figure 1 (b) shows an NL sentence and its MR from the GEOQUERY domain (Zelle and Mooney, 1996) which consists of a database of U.S. geographical facts about which a user can query. The MRL used for GEOQUERY in some of the previous work is a variable-free functional query language, that was constructed from the original MRs in Prolog (Kate et al., 2005). From this MRL, the MRG was then manually written so that its productions were compatible with the semantics expressible in natural language. This MRG was different from some simple MRG one would otherwise design for the MRL. Figure 3 (a) shows the parse tree obtained using a simple MRG for the MR shown in Figure 1 (b). The MR parse obta</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>Zelle, J. M. and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proc. of AAAI-1996, pages 1050–1055, Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL2007,</booktitle>
<pages>678--687</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3066" citStr="Zettlemoyer and Collins, 2007" startWordPosition="467" endWordPosition="470">rases relate to the productions of the MRG, and using this information they parse a test sentence to compositionally generate its best MR. In order to learn a good semantic parser, it is necessary that the productions of the MRG accurately represent the semantics being expressed by the natural language. However, an MRL and its MRG are typically designed to best suit the application with little consideration for how well they correspond to the semantics of a natural language. Some other semantic parser learning systems which need MRL in the form of Prolog (Tang and Mooney, 2001) or A-calculus (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) do not use productions of the MRG but instead use predicates of the MRL. However, in order to learn a good semantic parser, they still require that these predicates correspond well with the semantics of the natural language. There are also systems which learn semantic parsers from more detailed training data in the form of semantically augmented parse trees of NL sentences in which each internal node has a syntactic and a semantic label (Ge 33 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 33–40 Manchester, August 2008 </context>
</contexts>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Zettlemoyer, Luke S. and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proc. of EMNLP-CoNLL2007, pages 678–687, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>