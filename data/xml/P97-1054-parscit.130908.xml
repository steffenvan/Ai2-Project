<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.946801">
Co-evolution of Language and of the Language Acquisition Device
</title>
<author confidence="0.988111">
Ted Briscoe
e jb@c1 . cam . ac . uk
</author>
<affiliation confidence="0.9947955">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.781285">
Pembroke Street
Cambridge CB2 3QG, UK
</address>
<sectionHeader confidence="0.937168" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999940785714286">
A new account of parameter setting dur-
ing grammatical acquisition is presented in
terms of Generalized Categorial Grammar
embedded in a default inheritance hierar-
chy, providing a natural partial ordering
on the setting of parameters. Experiments
show that several experhnentally effective
learners can be defined in this framework.
Evolutionary simulations suggest that a
learner with default initial settings for pa-
rameters will emerge, provided that learn-
ing is memory limited and the environment
of linguistic adaptation contains an appro-
priate language.
</bodyText>
<sectionHeader confidence="0.981258" genericHeader="method">
1 Theoretical Background
</sectionHeader>
<bodyText confidence="0.999922171428571">
Grammatical acquisition proceeds on the basis of a
partial genotypic specification of (universal) gram-
mar (UG) complemented with a learning procedure
enabling the child to complete this specification ap-
propriately. The parameter setting framework of
Chomsky (1981) claims that learning involves fix-
ing the values of a finite set of finite-valued param-
eters to select a single fully-specified grammar from
within the space defined by the genotypic specifi-
cation of UG. Formal accounts of parameter set-
ting have been developed for small fragments but
even in these search spaces contain local maxima
and subset-superset relations which may cause a
learner to converge to an incorrect grammar (Clark,
1992; Gibson and Wexler, 1994; Niyogi and Berwick,
1995). The solution to these problems involves defin-
ing default, unmarked initial values for (some) pa-
rameters iird/or ordering the setting of parameters
during learning.
Bickerton (1984) argues for the Bioprogram Hy-
pothesis as an explanation for universal similarities
between historically unrelated creoles, and for the
rapid increase in grammatical complexity accompa-
nying the transition from pidgin to creole languages.
From the perspective of the parameters framework,
the Bioprogram Hypothesis claims that children are
endowed genetically with a UG which, by default,
specifies the stereotypical core creole grammar, with
right-branching syntax and subject-verb-object or-
der, as in Saramaccan. Others working within the
parameters framework have proposed unmarked, de-
fault parameters (e.g. Lightfoot, 1991), but the Bio-
program Hypothesis can be interpreted as towards
one end of a continuum of proposals ranging from all
parameters initially unset to all set to default values.
</bodyText>
<sectionHeader confidence="0.965178" genericHeader="method">
2 The Language Acquisition Device
</sectionHeader>
<bodyText confidence="0.9999585">
A model of the Language Acquisition Device (LAD)
incorporates a UG with associated parameters, a
parser, and an algorithm for updating initial param-
eter settings on parse failure during learning.
</bodyText>
<subsectionHeader confidence="0.948745">
2.1 The Grammar (set)
</subsectionHeader>
<bodyText confidence="0.997069263157895">
Basic categorial grammar (CG) uses one rule of ap-
plication which combines a functor category (con-
taining a slash) with an argument category to form
a derived category (with one less slashed argument
category). Grammatical constraints of order and
agreement are captured by only allowing directed
application to adjacent matching categories. Gener-
alized Categorial Grammar (GCG) extends CG with
further rule schemata.&apos; The rules of FA, BA, gen-
eralized weak permutation (P) and backward and
forward composition (FC, BC) are given in Fig-
ure 1 (where X, Y and Z are category variables,
I is a variable over slash and backslash, and ...
denotes zero or more further functor arguments).
Once permutation is included, several semantically
&apos;Wood (1993) is a general introduction to Categorial
Grammar and extensions to the basic theory. The most
closely related theories to that presented here are those
of Steedman (e.g. 1988) and Hoffman (1995).
</bodyText>
<page confidence="0.997673">
418
</page>
<note confidence="0.98124925">
Forward Application:
X/Y Y X A y [X(y)] (y) = X(Y)
Backward Application:
Y X \Y X Y [X(y)] (y) = X(Y)
Forward Composition:
X/Y Y/Z = X/Z A y [X(y)] A z [Y(z)] A z [X(Y(z))]
Backward Composition:
Y\Z X\Y = X\Z A z [Y(z)] A y [X(y)] A z [X(Y(z))]
</note>
<figure confidence="0.5178345">
(Generalized Weak) Permutation:
(XIY1). lYn (XlYn)ilri ... A y„ ...,y1 [X(yi ...,y77)] = A 371,37,2 [X(Yi
</figure>
<figureCaption confidence="0.997597">
Figure 1: GCG Rule Schemata
</figureCaption>
<figure confidence="0.9869528">
Kim loves Sandy
NP (S\NP)/NP NP
kim&apos; A y,x [love&apos;(x y)] sandy&apos;
(S/NP)\NP
A x,y [love/(x y)]
BA
S/NP
A y [love&apos;(kim&apos; y)]
FA
love&apos;(kim&apos; sandy&apos;)
</figure>
<figureCaption confidence="0.999733">
Figure 2: GCG Derivation for Kim loves Sandy
</figureCaption>
<bodyText confidence="0.970798711538462">
equivalent derivations for Kim loves Sandy become
available, Figure 2 shows the non-conventional left-
branching one. Composition also allows alterna-
tive non-conventional semantically equivalent (left-
branching) derivations.
GCG as presented is inadequate as art account of
UG or of any individual grammar. In particular,
the definition of atomic categories needs extending
to deal with featural variation (e.g. Bouma and van
Noord, 1994), and the rule schemata, especially com-
position and weak permutation, must be restricted
in various parametric ways so that overgeneration
is prevented for specific languages. Nevertheless,
GCG does represent a plausible kernel of UG; Hoff-
man (1995, 1996) explores the descriptive power of a
very similar system, in which generalized weak per-
mutation is not required because functor arguments
are interpreted as multisets. She demonstrates that
this system can handle (long-distance) scrambling
elegantly and generates mildly context-sensitive lan-
guages (Joshi et al, 1991).
The relationship between GCG as a theory of UG
(GCUG) and as a the specification of a particu-
lar grammar is captured by embedding the theory
in a default inheritance hierarchy. This is repre-
sented as a lattice of typed default feature structures
(TDFSs) representing subsumption and default in-
heritance relationships (Lascarides et al, 1996; Las-
carides and Copestake, 1996). The lattice defines
intensionally the set of possible categories and rule
schemata via type declarations on nodes. For ex-
ample, an intransitive verb might be treated as a
subtype of verb, inheriting subject directionality by
default from a type gendir (for general direction).
For English, gendir is default right but the node of
the (intransitive) functor category, where the direc-
tionality of subject arguments is specified, overrides
this to left, reflecting the fact that English is pre-
dominantly right-branching, though subjects appear
to the left of the verb. A transitive verb would in-
herit structure from the type for intransitive verbs
and an extra NP argument with default directional-
ity specified by gendir, and so forth.2
For the purposes of the evolutionary simulation
described in §3, GC(U)Gs are represented as a se-
quence of p-settings (where p denotes principles or
parameters) based on a flat (ternary) sequential en-
coding of such default inheritance lattices. The in-
2Bouma and van Noord (1994) and others demon-
strate that CGs can be embedded in a constraint-based
representation. Briscoe (1997a,b) gives further details of
the encoding of GCG in TDFSs.
</bodyText>
<page confidence="0.997436">
419
</page>
<table confidence="0.968284">
NP N S gen-dir subj-dir applic
AT AT AT DR DL DT
NP gendir applic S N subj-dir
AT DR DT AT AT DL
applic NP N gen-dir subj-dir S
DT AT AT DR DL AT
...
</table>
<figureCaption confidence="0.994401">
Figure 3: Sequential encodings of the grammar fragment
</figureCaption>
<bodyText confidence="0.962845811764706">
heritance hierarchy provides a partial ordering on
parameters, which is exploited in the learning pro-
cedure. For example, the atomic categories, N,
NP and S are each represented by a parameter en-
coding the presence/absence or lack of specification
(T/F /?) of the category in the (U)G. Since they will
be unordered in the lattice their ordering in the se-
quential coding is arbitrary. However, the ordering
of the directional types gendir and subjdir (with
values L/R) is significant as the latter is a more spe-
cific type. The distinctions between absolute, de-
fault or unset specifications also form part of the
encoding (A/D/?). Figure 3 shows several equiva-
lent and equally correct sequential encodings of the
fragment of the English type system outlined above.
A set of grammars based on typological distinc-
tions defined by basic constituent order (e.g. Green-
berg, 1966; Hawkins, 1994) was constructed as a
(partial) GCUG with independently varying binary-
valued parameters. The eight basic language fami-
lies are defined in terms of the unmarked order of
verb (V), subject (S) and objects (0) in clauses.
Languages within families further specify the order
of modifiers and specifiers in phrases, the order of ad-
positions and further phrasal-level ordering param-
eters. Figure 4 list the language-specific ordering
parameters used to define the full set of grammars
in (partial) order of generality, and gives examples
of settings based on familiar languages such as &amp;quot;En-
glish&amp;quot;, &amp;quot;German&amp;quot; and &amp;quot;Japanese&amp;quot; .3 &amp;quot;English&amp;quot; de-
fines an SVO language, with prepositions in which
specifiers, complementizers and some modifiers pre-
cede heads of phrases. There are other grammars in
the SVO family in which all modifers follow heads,
there are postpositions, and so forth. Not all combi-
nations of parameter settings correspond to attested
languages and one entire language family (OVS) is
unattested. &amp;quot;Japanese&amp;quot; is an SOV language with
3Throughout double quotes around language names
are used as convenient mnemonics for familiar combina-
tions of parameters. Since not all aspects of these actual
languages are represented in the grammars, conclusions
about actual languages must be made with care.
postpositions in which specifiers and modifiers follow
heads. There are other languages in the SOV family
with less consistent left-branching syntax in which
specifiers and/or modifiers precede phrasal heads,
some of which are attested. &amp;quot;German&amp;quot; is a more
complex SOV language in which the parameter verb-
second (v2) ensures that the surface order in main
clauses is usually SV0.4
There are 20 p-settings which determine the rule
schemata available, the atomic category set, and so
forth. In all, this CGUG defines just under 300
grammars. Not all of the resulting languages are
(stringset) distinct and some are proper subsets of
other languages. &amp;quot;English&amp;quot; without the rule of per-
mutation results in a stringset-identical language,
but the grammar assigns different derivations to
some strings, though the associated logical forms are
identical. &amp;quot;English&amp;quot; without composition results in
a subset language. Some combinations of p-settings
result in &apos;impossible&apos; grammars (or UGs). Others
yield equivalent grammars, for example, different
combinations of default settings (for types and their
subtypes) can define an identical category set.
The grammars defined generate (usually infinite)
stringsets of lexical syntactic categories. These
strings are sentence types since each is equivalent
to a finite set of grammatical sentences formed by
selecting a lexical instance of each lexical category.
Languages are represented as a finite subset of sen-
tence types generated by the associated grammar.
These represent a sample of degree-1 learning trig-
gers for the language (e.g. Lightfoot, 1991). Subset
languages are represented by 3-9 sentence types and
&apos;full&apos; languages by 12 sentence types. The construc-
tions exemplified by each sentence type and their
length are equivalent across all the languages defined
by the grammar set, but the sequences of lexical cat-
egories can differ. For example, two SOY language
renditions of The man who Bill likes gave Fred a
4Representation of the vl/v2 parameter(s) in terms
of a type constraint determining allowable functor cate-
gories is discussed in more detail in Briscoe (1997b).
</bodyText>
<page confidence="0.986143">
420
</page>
<figure confidence="0.58555375">
gen vi n subj obj v2 mod spec relcl adpos compl
Engl R F RL R F R R R R R
Ger R F RL L TR L R R R R
Jap L F LL L F L L L ?
</figure>
<figureCaption confidence="0.999676">
Figure 4: The Grammar Set — Ordering Parameters
</figureCaption>
<bodyText confidence="0.935507">
present, one with premodifying and the other post-
modifying relative clauses, both with a relative pro-
noun at the right boundary of the relative clause, are
shown below with the differing category highlighted.
</bodyText>
<figure confidence="0.406213833333333">
Bill likes who the-man a-present Fred gave
NP, (S \ NP,)\NP„ R,c \ (S \ NP0) NPARc NP02
NPoi ((S\NP5)\NP02)\NP01
The-man Bill likes who a-present Fred gave
NPs/Rc NPs (S \ NPs) \ NP0 RcVS \ NP0) NP02
NP01 ((S\NP5)\NP02)\NP01
</figure>
<subsectionHeader confidence="0.998576">
2.2 The Parser
</subsectionHeader>
<bodyText confidence="0.99960464516129">
The parser is a deterministic, bounded-context
stack-based shift-reduce algorithm. The parser op-
erates on two data structures, an input buffer or
queue, and a stack or push down store. The algo-
rithm for the parser working with a GCG which in-
cludes application, composition and permutation is
given in Figure 5. This algorithm finds the most left-
branching derivation for a sentence type because Re-
duce is ordered before Shift. The category sequences
representing the sentence types in the data for the
entire language set are designed to be unambiguous
relative to this &apos;greedy&apos;, deterministic algorithm, so
it will always assign the appropriate logical form to
each sentence type. However, there are frequently al-
ternative less left-branching derivations of the same
logical form.
The parser is augmented with an algorithm which
computes working memory load during an analy-
sis (e.g. Baddeley, 1992). Limitations of working
memory are modelled in the parser by associating a
cost with each stack cell occupied during each step
of a derivation, and recency and depth of process-
ing effects are modelled by resetting this cost each
time a reduction occurs: the working memory load
(WML) algorithm is given in Figure 6. Figure 7 gives
the right-branching derivation for Kim loves Sandy,
found by the parser utilising a grammar without per-
mutation. The WML at each step is shown for this
derivation. The overall WML (16) is higher than for
the left-branching derivation (9).
The WML algorithm ranks sentence types, and
</bodyText>
<listItem confidence="0.911787181818182">
1. The Reduce Step: if the top 2 cells of the
stack are occupied,
then try
a) Application, if match, then apply and goto
1), else b),
b) Combination, if match then apply and goto
1), else c),
c) Permutation, if match then apply and goto
1), else goto 2)
2. The Shift Step: if the first cell of the Input
Buffer is occupied,
</listItem>
<bodyText confidence="0.819323571428571">
then pop it and move it onto the Stack to-
gether with its associated lexical syntactic cat-
egory and goto 1),
else goto 3)
3. The Halt Step: if only the top cell of the Stack
is occupied by a constituent of category S,
then return Success,
else return Fail
The Match and Apply operation: if a binary
rule schema matches the categories of the top 2 cells
of the Stack, then they are popped from the Stack
and the new category formed by applying the rule
schema is pushed onto the Stack.
The Permutation operation: each time step 1c)
is visited during the Reduce step, permutation is ap-
plied to one of the categories in the top 2 cells of the
Stack until all possible permutations of the 2 cate-
gories have been tried using the binary rules. The
number of possible permutation operations is finite
and bounded by the maximum number of arguments
of any functor category in the grammar.
</bodyText>
<figureCaption confidence="0.997109">
Figure 5: The Parsing Algorithm
</figureCaption>
<page confidence="0.975952">
421
</page>
<table confidence="0.998797454545455">
Stack Input Buffer Operation Step WML
Kim loves Sandy 0 0
Kim:NP:kim&apos; loves Sandy Shift 1 1
loves:(S \ NP)/NP:A y,x(love&apos; x, y) Sandy Shift 2 3
Kim:NP:kim&apos;
Sandy:NP:sandy&apos; Shift 3 6
loves:(S \ NP)/NP:A y,x(love&apos; x, y)
Kim:NP :kim&apos;
loves Sandy:S/NP:A x(love&apos; x, sandy&apos;) Reduce (A) 4 5
Kim:NP:kim&apos;
Kim loves Sandy:S:(love&apos; kim&apos;, sandy&apos;) Reduce (A) 5 1
</table>
<figureCaption confidence="0.727779">
Figure 7: WML for Kim loves Sandy
</figureCaption>
<bodyText confidence="0.322759">
After each parse step (Shift, Reduce, Halt (see
Fig 5):
</bodyText>
<listItem confidence="0.977744166666667">
1. Assign any new Stack entry in the top cell (in-
troduced by Shift or Reduce) a WML value of
0
2. Increment every Stack cell&apos;s WML value by 1
3. Push the sum of the WML values of each Stack
cell onto the WML-record
</listItem>
<bodyText confidence="0.9143135">
When the parser halts, return the sum of the WML-
record gives the total WML for a derivation
</bodyText>
<figureCaption confidence="0.999376">
Figure 6: The WML Algorithm
</figureCaption>
<bodyText confidence="0.999825461538462">
thus indirectly languages, by parsing each sentence
type from the exemplifying data with the associ-
ated grammar and then taking the mean of the
WML obtained for these sentence types. &amp;quot;En-
glish&amp;quot; with Permutation has a lower mean WML
than &amp;quot;English&amp;quot; without Permutation, though they
are stringset-identical, whilst a hypothetical mix-
ture of &amp;quot;Japanese&amp;quot; SOV clausal order with &amp;quot;En-
glish&amp;quot; phrasal syntax has a mean WML which is 25%
worse than that for &amp;quot;English&amp;quot;. The WML algorithm
is in accord with existing (psycholinguistically-
motivated) theories of parsing complexity (e.g. Gib-
son, 1991; Hawkins, 1994; Rambow and Joshi, 1994).
</bodyText>
<subsectionHeader confidence="0.999223">
2.3 The Parameter Setting Algorithm
</subsectionHeader>
<bodyText confidence="0.999008814814815">
The parameter setting algorithm is an extension of
Gibson and Wexler&apos;s (1994) Trigger Learning Al-
gorithm (TLA) to take account of the inheritance-
based partial ordering and the role of memory in
learning. The TLA is error-driven — parameter set-
tings are altered in constrained ways when a learner
cannot parse trigger input. Trigger input is de-
fined as primary linguistic data which, because of
its structure or context of use, is determinately un-
parsable with the correct interpretation (e.g. Light-
foot, 1991). In this model, the issue of ambigu-
ity and triggers does not arise because all sentence
types are treated as triggers represented by p-setting
schemata. The TLA is memoryless in the sense that
a history of parameter (re)settings is not maintained,
in principle, allowing the learner to revisit previous
hypotheses. This is what allows Niyogi and Berwick
(1995) to formalize parameter setting as a Markov
process. However, as Brent (1996) argues, the psy-
chological plausibility of this algorithm is doubt-
ful — there is no evidence that children (randomly)
move between neighbouring grammars along paths
that revisit previous hypotheses. Therefore, each
parameter can only be reset once during the learn-
ing process. Each step for a learner can be defined
in terms of three functions: P-SETTING, GRAMMAR
and PARSER, as:
</bodyText>
<equation confidence="0.73175">
PARSERi(GRAMMARi(P-SETTING2(SentenCe3)))
</equation>
<bodyText confidence="0.999674466666667">
A p-setting defines a grammar which in turn defines
a parser (where the subscripts indicate the output of
each function given the previous trigger). A param-
eter is updated on parse failure and, if this results
in a parse, the new setting is retained. The algo-
rithm is summarized in Figure 8. Working mem-
ory grows through childhood (e.g. Baddeley, 1992),
and this may assist learning by ensuring that trigger
sentences gradually increase in complexity through
the acquisition period (e.g. Elman, 1993) by forcing
the learner to ignore more complex potential triggers
that occur early in the learning process. The WML
of a sentence type can be used to determine whether
it can function as a trigger at a particular stage in
learning.
</bodyText>
<page confidence="0.957913">
422
</page>
<figure confidence="0.554193583333333">
Data: IS1, S2, • • • SO
unless
PARSER, (GRAMMAR (P-SETTING, (Si))) = Success
then
p-settingj = UPDATE(p-setting)
unless
PARSER3 (GRAMMAR i (P-SETTING3 (Si))) = Success
then
RETURN p-settings,
else
RETURN p-settingsj
Update:
</figure>
<bodyText confidence="0.974588333333333">
Reset the first (most general) default or unset pa-
rameter in a left-to-right search of the p-set accord-
ing to the following table:
</bodyText>
<equation confidence="0.591698">
Input: Dl DO ? ?
Output: R 0 R 1 ? 1/0 (random)
= T/L and 0 = F/R)
</equation>
<figureCaption confidence="0.99894">
Figure 8: The Learning Algorithm
</figureCaption>
<sectionHeader confidence="0.997404" genericHeader="method">
3 The Simulation Model
</sectionHeader>
<bodyText confidence="0.999346178571429">
The computational simulation supports the evolu-
tion of a population of Language Agents (LAgts),
similar to Holland&apos;s (1993) Echo agents. LAgts gen-
erate and parse sentences compatible with their cur-
rent p-setting. They participate in linguistic inter-
actions which are successful if their p-settings are
compatible. The relative fitness of a LAgt is a func-
tion of the proportion of its linguistic interactions
which have been successful, the expressivity of the
language(s) spoken, and, optionally, of the mean
WML for parsing during a cycle of interactions. An
interaction cycle consists of a prespecified number
of individual random interactions between LAgts,
with generating and parsing agents also selected ran-
domly. LAgts which have a history of mutually suc-
cessful interaction and high fitness can &apos;reproduce&apos;.
A LAgt can &apos;live&apos; for up to ten interaction cycles,
but may &apos;die&apos; earlier if its fitness is relatively low. It
is possible for a population to become extinct (for
example, if all the initial LAgts go through ten in-
teraction cycles without any successful interaction
occurring), and successful populations tend to grow
at a modest rate (to ensure a reasonable proportion
of adult speakers is always present). LAgts learn
during a critical period from ages 1-3 and reproduce
from 4-10, parsing and/or generating any language
learnt throughout their life.
During learning a LAgt can reset genuine param-
</bodyText>
<table confidence="0.889886375">
Variables Typical Values
Population Size 32
Interaction Cycle 2K Interactions
Simulation Run 50 Cycles
Crossover Probability 0.9
Mutation Probability 0
Learning memory limited yes
critical period yes
</table>
<figureCaption confidence="0.999777">
Figure 9: The Simulation Options
</figureCaption>
<bodyText confidence="0.948103666666667">
(Cost/Benefits per sentence (1-6); summed for each
LAgt at end of an interaction cycle and used to cal-
culate fitness functions (7-8)):
</bodyText>
<listItem confidence="0.99938">
1. Generate cost: 1 (GC)
2. Parse cost: 1 (PC)
3. Generate subset language cost: 1 (GSC)
4. Parse failure cost: 1 (PF)
5. Parse memory cost: WML(st)
6. Interaction success benefit: 1 (SI)
7. Fitness(WML): GC
</listItem>
<figure confidence="0.9557465">
GC+PC GC+GSC X
SI GC
8. Fitness(-,WML)-
&apos; GC+PC GC+GSC
</figure>
<figureCaption confidence="0.996974">
Figure 10: Fitness Functions
</figureCaption>
<bodyText confidence="0.999705">
eters which either were unset or had default settings
&apos;at birth&apos;. However, p-settings with an absolute
value (principles) cannot be altered during the life-
time of an LAgt. Successful LAgts reproduce at the
end of interaction cycles by one-point crossover of
(and, optionally, single point mutation of) their ini-
tial p-settings, ensuring neo-Darwinian rather than
Lamarckian inheritance. The encoding of p-settings
allows the deterministic recovery of the initial set-
ting. Fitness-based reproduction ensures that suc-
cessful and somewhat compatible p-settings are pre-
served in the population and randomly sampled in
the search for better versions of universal grammar,
including better initial settings of genuine parame-
ters. Thus, although the learning algorithm per se
is fixed, a range of alternative learning procedures
can be explored based on the definition of the inital
set of parameters and their initial settings. Figure 9
summarizes crucial options in the simulation giving
the values used in the experiments reported in §4
and Figure 10 shows the fitness functions.
</bodyText>
<equation confidence="0.4175435">
(where 1
P&amp;quot; C&apos; F!&apos; F )
</equation>
<page confidence="0.998517">
423
</page>
<sectionHeader confidence="0.99821" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999841">
4.1 Effectiveness of Learning Procedures
</subsectionHeader>
<bodyText confidence="0.99995164">
Two learning procedures were predefined — a default
learner and an unset learner. These LAgts were ini-
tialized with p-settings consistent with a minimal in-
herited CGUG consisting of application with NP and
S atomic categories. All the remaining p-settings
were genuine parameters for both learners. The un-
set learner was initialized with all unset, whilst the
default learner had default settings for the parame-
ters gendir and subjdir and argorder which spec-
ify a minimal SVO right-branching grammar, as well
as default (off) settings for comp and perm which
determine the availability of Composition and Per-
mutation, respectively. The unset learner represents
a &apos;pure&apos; principles-and-parameters learner. The de-
fault learner is modelled on Bickerton&apos;s bioprogram
learner.
Each learner was tested against an adult LAgt
initialized to generate one of seven full lan-
guages in the set which are close to an at-
tested language; namely, &amp;quot;English&amp;quot; (SVO, predom-
inantly right-branching), &amp;quot;Welsh&amp;quot; (SV0v1, mixed
order), &amp;quot;Malagasy&amp;quot; (VOS, right-branching), &amp;quot;Taga-
log&amp;quot; (VSO, right-branching), &amp;quot;Japanese&amp;quot; (SOV,
left-branching), &amp;quot;German&amp;quot; (SOVv2, predominantly
right-branching), &amp;quot;Hixkaryana&amp;quot; (OVS, mixed or-
der), and an unattested full OSV language with left-
branching syntax. In these tests, a single learner in-
teracted with a single adult. After every ten interac-
tions, in which the adult randomly generated a sen-
tence type and the learner attempted to parse and
learn from it, the state of the learner&apos;s p-settings was
examined to determine whether the learner had con-
verged on the same grammar as the adult. Table 1
shows the number of such interaction cycles (i.e. the
number of input sentences to within ten) required by
each type of learner to converge on each of the eight
languages. These figures are each calculated from
100 trials to a 1% error rate; they suggest that, in
general, the default learner is more effective than
the unset learner. However, for the OVS language
(OVS languages represent 1.24% of the world&apos;s lan-
guages, Tomlin, 1986), and for the unattested OSV
language, the default (SVO) learner is less effective.
So, there are at least two learning procedures in the
space defined by the model which can converge with
some presentation orders on some of the grammars
in this set. Stronger conclusions require either ex-
haustive experimentation or theoretical analysis of
the model of the type undertaken by Gibson and
Wexler (1994) and Niyogi and Berwick (1995).
</bodyText>
<table confidence="0.988753666666667">
Unset Default None
WML 15 39 26
—MML 34 17 29
</table>
<tableCaption confidence="0.994608">
Table 2: Overall preferences for parameter types
</tableCaption>
<subsectionHeader confidence="0.997716">
4.2 Evolution of Learning Procedures
</subsectionHeader>
<bodyText confidence="0.999982466666667">
In order to test the preference for default versus un-
set parameters under different conditions, the five
parameters which define the difference between the
two learning procedures were tracked through an-
other series of 50 cycle runs initialized with either 16
default learning adult speakers and 16 unset learning
adult speakers, with or without memory-limitations
during learning and parsing, speaking one of the
eight languages described above. Each condition was
run ten times. In the memory limited runs, default
parameters came to dominate some but not all pop-
ulations. In a few runs all unset parameters dis-
appeared altogether. In all runs with populations
initialized to speak &amp;quot;English&amp;quot; (SVO) or &amp;quot;Malagasy&amp;quot;
(VOS) the preference for default settings was 100%.
In 8 runs with &amp;quot;Tagalog&amp;quot; (VSO) the same preference
emerged, in one there was a preference for unset pa-
rameters and in the other no clear preference. How-
ever, for the remaining five languages there was no
strong preference.
The results for the runs without memory limita-
tions are different, with an increased preference for
unset parameters across all languages but no clear
100% preference for any individual language. Ta-
ble 2 shows the pattern of preferences which emerged
across 160 runs and how this was affected by the
presence or absence of memory limitations.
To test whether it was memory limitations during
learning or during parsing which were affecting the
results, another series of runs for &amp;quot;English&amp;quot; was per-
formed with either memory limitations during learn-
ing but not parsing enabled, or vice versa. Memory
limitations during learning are creating the bulk of
the preference for a default learner, though there
appears to be an additive effect. In seven of the
ten runs with memory limitations only in learning, a
clear preference for default learners emerged. In five
of the runs with memory limitations only in parsing
there appeared to be a slight preference for defaults
emerging. Default learners may have a fitness ad-
vantage when the number of interactions required to
learn successfully is greater because they will tend to
converge faster, at least to a subset language. This
will tend to increase their fitness over unset learners
who do not speak any language until further into the
</bodyText>
<page confidence="0.996952">
424
</page>
<table confidence="0.99933175">
Learner Language
SVO SV0v1 VOS VSO SOV SOVv2 OVS OSV
Unset 60 80 70 80 70 70 70 70
Default 60 60 60 60 60 60 80 70
</table>
<tableCaption confidence="0.999902">
Table 1: Effectiveness of Two Learning Procedures
</tableCaption>
<bodyText confidence="0.996289368421053">
learning period.
The precise linguistic environment of adaptation
determines the initial values of default parameters
which evolve. For example, in the runs initialized
with 16 unset learning &amp;quot;Malagasy&amp;quot; VOS adults and
16 default (SVO) learning VOS adults, the learn-
ing procedure which dominated the population was
a variant VOS default learner in which the value
for subjdir was reversed to reflect the position of
the subject in this language. In some of these
runs, the entire population evolved a default sub-
jdir &apos;right&apos; setting, though some LAgts always re-
tained unset settings for the other two ordering pa-
rameters, gendir and argo, as is illustrated in Fig-
ure 11. This suggests that if the human language fac-
ulty has evolved to be a right-branching SVO default
learner, then the environment of linguistic adapta-
tion must have contained a dominant language fully
compatible with this (minimal) grammar.
</bodyText>
<subsectionHeader confidence="0.999498">
4.3 Emergence of Language and Learners
</subsectionHeader>
<bodyText confidence="0.999992394366197">
To explore the emergence and persistence of struc-
tured language, and consequently the emergence of
effective learners, (pseudo) random initialization was
used. A series of simulation runs of 500 cycles were
performed with random initialization of 32 LAgts&apos;
p-settings for any combination of p-setting values,
with a probability of 0.25 that a setting would be an
absolute principle, and 0.75 a parameter with unbi-
ased allocation for default or unset parameters and
for values of all settings. All LAgts were initialized
to be age 1 with a critical period of 3 interaction
cycles of 2000 random interactions for learning, a
maximum age of 10, and the ability to reproduce by
crossover (0.9 probability) and mutation (0.01 prob-
ability) from 4-10. In around 5% of the runs, lan-
guage(s) emerged and persisted to the end of the
run.
Languages with close to optimal WML scores typi-
cally came to dominate the population quite rapidly.
However, sometimes sub-optimal languages were ini-
tially selected and occasionally these persisted de-
spite the later appearance of a more optimal lan-
guage, but with few speakers. Typically, a minimal
subset language dominated — although full and inter-
mediate languages did appear briefly, they did not
survive against less expressive subset languages with
a lower mean WML. Figure 12 is a typical plot of
the emergence (and extinction) of languages in one
of these runs. In this run, around 10 of the initial
population converged on a minimal OVS language
and 3 others on a VOS language. The latter is more
optimal with respect to WML and both are of equal
expressivity so, as expected, the VOS language ac-
quired more speakers over the next few cycles. A few
speakers also converged on VOS-N, a more expres-
sive but higher WML extension of VSO-N-GWP-
COMP. However, neither this nor the OVS language
survived beyond cycle 14. Instead a VSO language
emerged at cycle 10, which has the same minimal
expressivity of the VOS language but a lower WML
(by virtue of placing the subject before the object)
and this language dominated rapidly and eclipsed all
others by cycle 40.
In all these runs, the population settled on sub-
set languages of low expressivity, whilst the percent-
age of absolute principles and default parameters in-
creased relative to that of unset parameters (mean
% change from beginning to end of runs: +4.7, +1.5
and -6.2, respectively). So a second identical set of
ten was undertaken, except that the initial popula-
tion now contained two SOV-V2 &amp;quot;German&amp;quot; speak-
ing unset learner LAgts. In seven of these runs, the
population fixed on a full SOV-V2 language, in two
on the intermediate subset language SOV-V2-N, and
in one on the minimal subset language SOV-V2-N-
GWP-COMP. These runs suggest that if a full lan-
guage defines the environment of adaptation then
a population of randomly initialized LAgts is more
likely to converge on a (related) full language. Thus,
although the simulation does not model the devel-
opment of expressivity well, it does appear that it
can model the emergence of effective learning pro-
cedures for (some) full languages. The pattern of
language emergence and extinction followed that of
the previous series of runs: lower mean WML lan-
guages were selected from those that emerged during
the run. However, often the initial optimal SVO-V2
itself was lost before enough LAgts evolved capable
of learning this language. In these runs, changes
in the percentages of absolute, default or unset p-
settings in the population show a marked difference:
</bodyText>
<page confidence="0.996824">
425
</page>
<figure confidence="0.99626625">
100
&amp;quot;GOgE;ricfir&amp;quot;
&amp;quot;GOargo&amp;quot; -----
&amp;quot;GOsubjdir&amp;quot;
</figure>
<figureCaption confidence="0.997979666666667">
Figure 11: Percentage of each default ordering pa-
rameter
Figure 12: Emergence of language(s)
</figureCaption>
<figure confidence="0.996819375">
80
60
40
20
0 10 20 30 40 50 60 70
Interaction Cycles
No. of Speakers
50
20 25 30 35 40
Interaction Cycles
45
5 10 15
&amp;quot;G8-141.&amp;quot;
!G4-,vos-N% -------
&amp;quot;(48-VOS-N-dWP-COMP&amp;quot;
&amp;quot;(18-VSON=GWP-COMP&amp;quot; -
45
40
35
30
25
20
15
10
</figure>
<bodyText confidence="0.998005875">
the mean number of absolute principles declined by
6.1% and unset parameters by 17.8%, so the num-
ber of default parameters rose by 23.9% on average
between the beginning and end of the 10 runs. This
may reflect the more complex linguistic environment
in which (incorrect) absolute settings are more likely
to handicap, rather than simply be irrelevant to, the
performance of the LAgt.
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999972333333334">
Partially ordering the updating of parameters can
result in (experimentally) effective learners with a
more complex parameter system than that studied
previously. Experimental comparison of the default
(SVO) learner and the unset learner suggests that
the default learner is more efficient on typologically
more common constituent orders. Evolutionary sim-
ulation predicts that a learner with default param-
eters is likely to emerge, though this is dependent
both on the type of language spoken and the pres-
ence of memory limitations during learning and pars-
ing. Moreover, a SVO bioprogram learner is only
likely to evolve if the environment contains a domi-
nant SVO language.
The evolution of a bioprogram learner is a man-
ifestation of the Baldwin Effect (Baldwin, 1896) —
genetic assimilation of aspects of the linguistic envi-
ronment during the period of evolutionary adapta-
tion of the language learning procedure. In the case
of grammar learning this is a co-evolutionary process
in which languages (and their associated grammars)
are also undergoing selection. The WML account of
parsing complexity predicts that a right-branching
SVO language would be a near optimal selection at
a stage in grammatical development when complex
rules of reordering such as extraposition, scrambling
or mixed order strategies such as vi and v2 had
not evolved. Briscoe (1997a) reports further exper-
iments which demonstrate language selection in the
model.
Though, simulation can expose likely evolution-
ary pathways under varying conditions, these might
have been blocked by accidental factors, such as ge-
netic drift or bottlenecks, causing premature fixa-
tion of alleles in the genotype (roughly correspond-
ing to certain p-setting values). The value of the
simulation is to, firstly, show that a bioprogram
learner could have emerged via adaptation, and sec-
ondly, to clarify experimentally the precise condi-
tions required for its emergence. Since in many
cases these conditions will include the presence of
constraints (working memory limitations, expressiv-
ity, the learning algorithm etc.) which will remain
causally manifest, further testing of any conclusions
drawn must concentrate on demonstrating the ac-
</bodyText>
<page confidence="0.996935">
426
</page>
<bodyText confidence="0.99961825">
curacy of the assumptions made about such con-
straints. Briscoe (1997b) evaluates the psychological
plausibility of the account of parsing and working
memory.
</bodyText>
<sectionHeader confidence="0.996849" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999316356321839">
Baddeley, A. (1992) &apos;Working Memory: the interface
between memory and cognition&apos;, J. of Cognitive
Neuroscience, vol.4.3, 281-288.
Baldwin, J.M. (1896) &apos;A new factor in evolution&apos;,
American Naturalist, vol.30, 441-451.
Bickerton, D. (1984) &apos;The language bioprogram hy-
pothesis&apos;, The Behavioral and Brain Sciences,
vol.7.2, 173-222.
Bouma, G. and van Noord, G (1994) &apos;Constraint-
based categorial grammar&apos;, Proceedings of the
32nd Assoc. for Computational Linguistics, Las
Cruces, NM, pp. 147-154.
Brent, M. (1996) &apos;Advances in the computational
study of language acquisition&apos;, Cognition, vol. 61,
1-38.
Briscoe, E.J. (1997a, submitted) &apos;Language Acquisi-
tion: the Bioprogram Hypothesis and the Bald-
win Effect&apos;, Language,
Briscoe, E.J. (1997b, in prep.) Working memory and
its influence on the development of human lan-
guages and the human language faculty, Univer-
sity of Cambridge, Computer Laboratory, m.s..
Chomsky, N. (1981) Government and Binding, Foris,
Dordrecht.
Clark, R. (1992) &apos;The selection of syntactic knowl-
edge&apos;, Language Acquisition, vol.2.2, 83-149.
Elman, J. (1993) &apos;Learning and development in neu-
ral networks: the importance of starting small&apos;,
Cognition, vol.48, 71-99.
Gibson, E. (1991) A Copmutational Theory of Hu-
man Linguistic Processing: Memory Limitations
and Processing Breakdown, Doctoral disserta-
tion, Carnegie Mellon University.
Gibson, E. and Wexler, K. (1994) &apos;Triggers&apos;, Lin-
guistic Inquiry, vol.25.3, 407-454.
Greenberg, J. (1966) &apos;Some universals of grammar
with particular reference to the order of mean-
ingful elements&apos; in J. Greenberg (ed.), Univer-
sals of Grammar, MIT Press, Cambridge, Ma.,
pp. 73-113.
Hawkins, J.A. (1994) A Performance Theory of
Order and Constituency, Cambridge University
Press, Cambridge.
Hoffman, B. (1995) The Computational Analysis of
the Syntax and Interpretation of &apos;Free&apos; Word Or-
der in Turkish, PhD dissertation, University of
Pennsylvania.
Hoffman, B. (1996) &apos;The formal properties of syn-
chronous CCGs&apos;, Proceedings of the ESSLLI For-
mal Grammar Conference, Prague.
Holland, J.H. (1993) Echoing emergence: objectives,
rough definitions and speculations for echo-class
models, Santa Fe Institute, Technical Report 93-
04-023.
Joshi, A., Vijay-Shanker, K. and Weir, D. (1991)
&apos;The convergence of mildly context-sensitive
grammar formalisms&apos; in Sells, P., Shieber, S. and
Wasow, T. (ed.), Foundational Issues in Natural
Language Processing, MIT Press, pp. 31-82.
Lascarides, A., Briscoe E.J. , Copestake A.A and
Asher, N. (1995) &apos;Order-independent and persis-
tent default unification&apos;, Linguistics and Philos-
ophy, vol.19.1, 1-89.
Lascarides, A. and Copestake A.A. (1996, submit-
ted) &apos;Order-independent typed default unifica-
tion&apos;, Computational Linguistics,
Lightfoot, D. (1991) How to Set Parameters: Argu-
ments from language Change, MIT Press, Cam-
bridge, Ma..
Niyogi, P. and Berwick, R.C. (1995) &apos;A markov
language learning model for finite parameter
spaces&apos;, Proceedings of the 33rd Annual Meet-
ing of the Association for Computational Lin-
guistics, MIT, Cambridge, Ma..
Rambow, 0. and Joshi, A. (1994) &apos;A processing
model of free word order languages&apos; in C. Clifton,
L. Frazier and K. Rayner (ed.), Perspectives on
Sentence Processing, Lawrence Erlbaum, Hills-
dale, NJ., pp. 267-301.
Steedman, M. (1988) &apos;Combinators and grammars&apos;
in R. Oehrle, E. Bach and D. Wheeler (ed.), Cat-
egorial Grammars and Natural Language Struc-
tures, Reidel, Dordrecht, pp. 417-442.
Tomlin, R.. (1986) Basic Word Order: Functional
Principles, Routledge, London.
Wood, M.M. (1993) Categorial- Grammars, Rout-
ledge, London.
</reference>
<page confidence="0.998549">
427
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336904">
<title confidence="0.998935">Co-evolution of Language and of the Language Acquisition Device</title>
<author confidence="0.768165">cam</author>
<affiliation confidence="0.999973">Computer Laboratory University of Cambridge</affiliation>
<address confidence="0.8792935">Pembroke Street Cambridge CB2 3QG, UK</address>
<abstract confidence="0.996889563380282">A new account of parameter setting during grammatical acquisition is presented in terms of Generalized Categorial Grammar embedded in a default inheritance hierarchy, providing a natural partial ordering on the setting of parameters. Experiments show that several experhnentally effective learners can be defined in this framework. Evolutionary simulations suggest that a learner with default initial settings for parameters will emerge, provided that learning is memory limited and the environment of linguistic adaptation contains an appropriate language. 1 Theoretical Background Grammatical acquisition proceeds on the basis of a partial genotypic specification of (universal) grammar (UG) complemented with a learning procedure enabling the child to complete this specification appropriately. The parameter setting framework of Chomsky (1981) claims that learning involves fixing the values of a finite set of finite-valued parameters to select a single fully-specified grammar from within the space defined by the genotypic specification of UG. Formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset-superset relations which may cause a learner to converge to an incorrect grammar (Clark, 1992; Gibson and Wexler, 1994; Niyogi and Berwick, 1995). The solution to these problems involves defining default, unmarked initial values for (some) parameters iird/or ordering the setting of parameters during learning. Bickerton (1984) argues for the Bioprogram Hypothesis as an explanation for universal similarities between historically unrelated creoles, and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages. From the perspective of the parameters framework, the Bioprogram Hypothesis claims that children are endowed genetically with a UG which, by default, specifies the stereotypical core creole grammar, with right-branching syntax and subject-verb-object order, as in Saramaccan. Others working within the parameters framework have proposed unmarked, default parameters (e.g. Lightfoot, 1991), but the Bioprogram Hypothesis can be interpreted as towards one end of a continuum of proposals ranging from all parameters initially unset to all set to default values. 2 The Language Acquisition Device A model of the Language Acquisition Device (LAD) incorporates a UG with associated parameters, a parser, and an algorithm for updating initial parameter settings on parse failure during learning. 2.1 The Grammar (set) Basic categorial grammar (CG) uses one rule of application which combines a functor category (containing a slash) with an argument category to form a derived category (with one less slashed argument category). Grammatical constraints of order and agreement are captured by only allowing directed application to adjacent matching categories. Generalized Categorial Grammar (GCG) extends CG with further rule schemata.&apos; The rules of FA, BA, generalized weak permutation (P) and backward and forward composition (FC, BC) are given in Figure 1 (where X, Y and Z are category variables, I is a variable over slash and backslash, and ... denotes zero or more further functor arguments).</abstract>
<intro confidence="0.97105">Once permutation is included, several semantically</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Baddeley</author>
</authors>
<title>Working Memory: the interface between memory and cognition&apos;,</title>
<date>1992</date>
<journal>J. of Cognitive Neuroscience,</journal>
<volume>4</volume>
<pages>281--288</pages>
<contexts>
<context position="12858" citStr="Baddeley, 1992" startWordPosition="2042" endWordPosition="2043">ation is given in Figure 5. This algorithm finds the most leftbranching derivation for a sentence type because Reduce is ordered before Shift. The category sequences representing the sentence types in the data for the entire language set are designed to be unambiguous relative to this &apos;greedy&apos;, deterministic algorithm, so it will always assign the appropriate logical form to each sentence type. However, there are frequently alternative less left-branching derivations of the same logical form. The parser is augmented with an algorithm which computes working memory load during an analysis (e.g. Baddeley, 1992). Limitations of working memory are modelled in the parser by associating a cost with each stack cell occupied during each step of a derivation, and recency and depth of processing effects are modelled by resetting this cost each time a reduction occurs: the working memory load (WML) algorithm is given in Figure 6. Figure 7 gives the right-branching derivation for Kim loves Sandy, found by the parser utilising a grammar without permutation. The WML at each step is shown for this derivation. The overall WML (16) is higher than for the left-branching derivation (9). The WML algorithm ranks sente</context>
<context position="17867" citStr="Baddeley, 1992" startWordPosition="2899" endWordPosition="2900"> paths that revisit previous hypotheses. Therefore, each parameter can only be reset once during the learning process. Each step for a learner can be defined in terms of three functions: P-SETTING, GRAMMAR and PARSER, as: PARSERi(GRAMMARi(P-SETTING2(SentenCe3))) A p-setting defines a grammar which in turn defines a parser (where the subscripts indicate the output of each function given the previous trigger). A parameter is updated on parse failure and, if this results in a parse, the new setting is retained. The algorithm is summarized in Figure 8. Working memory grows through childhood (e.g. Baddeley, 1992), and this may assist learning by ensuring that trigger sentences gradually increase in complexity through the acquisition period (e.g. Elman, 1993) by forcing the learner to ignore more complex potential triggers that occur early in the learning process. The WML of a sentence type can be used to determine whether it can function as a trigger at a particular stage in learning. 422 Data: IS1, S2, • • • SO unless PARSER, (GRAMMAR (P-SETTING, (Si))) = Success then p-settingj = UPDATE(p-setting) unless PARSER3 (GRAMMAR i (P-SETTING3 (Si))) = Success then RETURN p-settings, else RETURN p-settingsj </context>
</contexts>
<marker>Baddeley, 1992</marker>
<rawString>Baddeley, A. (1992) &apos;Working Memory: the interface between memory and cognition&apos;, J. of Cognitive Neuroscience, vol.4.3, 281-288.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J M Baldwin</author>
</authors>
<title>(1896) &apos;A new factor in evolution&apos;,</title>
<journal>American Naturalist,</journal>
<volume>30</volume>
<pages>441--451</pages>
<marker>Baldwin, </marker>
<rawString>Baldwin, J.M. (1896) &apos;A new factor in evolution&apos;, American Naturalist, vol.30, 441-451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bickerton</author>
</authors>
<title>The language bioprogram hypothesis&apos;,</title>
<date>1984</date>
<journal>The Behavioral and Brain Sciences,</journal>
<volume>7</volume>
<pages>173--222</pages>
<contexts>
<context position="1710" citStr="Bickerton (1984)" startWordPosition="255" endWordPosition="256">nite set of finite-valued parameters to select a single fully-specified grammar from within the space defined by the genotypic specification of UG. Formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset-superset relations which may cause a learner to converge to an incorrect grammar (Clark, 1992; Gibson and Wexler, 1994; Niyogi and Berwick, 1995). The solution to these problems involves defining default, unmarked initial values for (some) parameters iird/or ordering the setting of parameters during learning. Bickerton (1984) argues for the Bioprogram Hypothesis as an explanation for universal similarities between historically unrelated creoles, and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages. From the perspective of the parameters framework, the Bioprogram Hypothesis claims that children are endowed genetically with a UG which, by default, specifies the stereotypical core creole grammar, with right-branching syntax and subject-verb-object order, as in Saramaccan. Others working within the parameters framework have proposed unmarked, default paramete</context>
</contexts>
<marker>Bickerton, 1984</marker>
<rawString>Bickerton, D. (1984) &apos;The language bioprogram hypothesis&apos;, The Behavioral and Brain Sciences, vol.7.2, 173-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G van Noord</author>
</authors>
<title>Constraintbased categorial grammar&apos;,</title>
<date>1994</date>
<booktitle>Proceedings of the 32nd Assoc. for Computational Linguistics, Las Cruces, NM,</booktitle>
<pages>147--154</pages>
<marker>Bouma, van Noord, 1994</marker>
<rawString>Bouma, G. and van Noord, G (1994) &apos;Constraintbased categorial grammar&apos;, Proceedings of the 32nd Assoc. for Computational Linguistics, Las Cruces, NM, pp. 147-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Advances in the computational study of language acquisition&apos;,</title>
<date>1996</date>
<journal>Cognition,</journal>
<volume>61</volume>
<pages>1--38</pages>
<contexts>
<context position="17094" citStr="Brent (1996)" startWordPosition="2777" endWordPosition="2778">ut is defined as primary linguistic data which, because of its structure or context of use, is determinately unparsable with the correct interpretation (e.g. Lightfoot, 1991). In this model, the issue of ambiguity and triggers does not arise because all sentence types are treated as triggers represented by p-setting schemata. The TLA is memoryless in the sense that a history of parameter (re)settings is not maintained, in principle, allowing the learner to revisit previous hypotheses. This is what allows Niyogi and Berwick (1995) to formalize parameter setting as a Markov process. However, as Brent (1996) argues, the psychological plausibility of this algorithm is doubtful — there is no evidence that children (randomly) move between neighbouring grammars along paths that revisit previous hypotheses. Therefore, each parameter can only be reset once during the learning process. Each step for a learner can be defined in terms of three functions: P-SETTING, GRAMMAR and PARSER, as: PARSERi(GRAMMARi(P-SETTING2(SentenCe3))) A p-setting defines a grammar which in turn defines a parser (where the subscripts indicate the output of each function given the previous trigger). A parameter is updated on pars</context>
</contexts>
<marker>Brent, 1996</marker>
<rawString>Brent, M. (1996) &apos;Advances in the computational study of language acquisition&apos;, Cognition, vol. 61, 1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
</authors>
<title>submitted) &apos;Language Acquisition: the Bioprogram Hypothesis and the Baldwin Effect&apos;,</title>
<date>1997</date>
<location>Language,</location>
<contexts>
<context position="6765" citStr="Briscoe (1997" startWordPosition="1047" endWordPosition="1048">ly right-branching, though subjects appear to the left of the verb. A transitive verb would inherit structure from the type for intransitive verbs and an extra NP argument with default directionality specified by gendir, and so forth.2 For the purposes of the evolutionary simulation described in §3, GC(U)Gs are represented as a sequence of p-settings (where p denotes principles or parameters) based on a flat (ternary) sequential encoding of such default inheritance lattices. The in2Bouma and van Noord (1994) and others demonstrate that CGs can be embedded in a constraint-based representation. Briscoe (1997a,b) gives further details of the encoding of GCG in TDFSs. 419 NP N S gen-dir subj-dir applic AT AT AT DR DL DT NP gendir applic S N subj-dir AT DR DT AT AT DL applic NP N gen-dir subj-dir S DT AT AT DR DL AT ... Figure 3: Sequential encodings of the grammar fragment heritance hierarchy provides a partial ordering on parameters, which is exploited in the learning procedure. For example, the atomic categories, N, NP and S are each represented by a parameter encoding the presence/absence or lack of specification (T/F /?) of the category in the (U)G. Since they will be unordered in the lattice t</context>
<context position="11332" citStr="Briscoe (1997" startWordPosition="1772" endWordPosition="1773">represent a sample of degree-1 learning triggers for the language (e.g. Lightfoot, 1991). Subset languages are represented by 3-9 sentence types and &apos;full&apos; languages by 12 sentence types. The constructions exemplified by each sentence type and their length are equivalent across all the languages defined by the grammar set, but the sequences of lexical categories can differ. For example, two SOY language renditions of The man who Bill likes gave Fred a 4Representation of the vl/v2 parameter(s) in terms of a type constraint determining allowable functor categories is discussed in more detail in Briscoe (1997b). 420 gen vi n subj obj v2 mod spec relcl adpos compl Engl R F RL R F R R R R R Ger R F RL L TR L R R R R Jap L F LL L F L L L ? Figure 4: The Grammar Set — Ordering Parameters present, one with premodifying and the other postmodifying relative clauses, both with a relative pronoun at the right boundary of the relative clause, are shown below with the differing category highlighted. Bill likes who the-man a-present Fred gave NP, (S \ NP,)\NP„ R,c \ (S \ NP0) NPARc NP02 NPoi ((S\NP5)\NP02)\NP01 The-man Bill likes who a-present Fred gave NPs/Rc NPs (S \ NPs) \ NP0 RcVS \ NP0) NP02 NP01 ((S\NP5</context>
<context position="33579" citStr="Briscoe (1997" startWordPosition="5446" endWordPosition="5447"> Effect (Baldwin, 1896) — genetic assimilation of aspects of the linguistic environment during the period of evolutionary adaptation of the language learning procedure. In the case of grammar learning this is a co-evolutionary process in which languages (and their associated grammars) are also undergoing selection. The WML account of parsing complexity predicts that a right-branching SVO language would be a near optimal selection at a stage in grammatical development when complex rules of reordering such as extraposition, scrambling or mixed order strategies such as vi and v2 had not evolved. Briscoe (1997a) reports further experiments which demonstrate language selection in the model. Though, simulation can expose likely evolutionary pathways under varying conditions, these might have been blocked by accidental factors, such as genetic drift or bottlenecks, causing premature fixation of alleles in the genotype (roughly corresponding to certain p-setting values). The value of the simulation is to, firstly, show that a bioprogram learner could have emerged via adaptation, and secondly, to clarify experimentally the precise conditions required for its emergence. Since in many cases these conditio</context>
</contexts>
<marker>Briscoe, 1997</marker>
<rawString>Briscoe, E.J. (1997a, submitted) &apos;Language Acquisition: the Bioprogram Hypothesis and the Baldwin Effect&apos;, Language,</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
</authors>
<title>in prep.) Working memory and its influence on the development of human languages and the human language faculty,</title>
<date>1997</date>
<institution>University of Cambridge, Computer Laboratory, m.s..</institution>
<contexts>
<context position="6765" citStr="Briscoe (1997" startWordPosition="1047" endWordPosition="1048">ly right-branching, though subjects appear to the left of the verb. A transitive verb would inherit structure from the type for intransitive verbs and an extra NP argument with default directionality specified by gendir, and so forth.2 For the purposes of the evolutionary simulation described in §3, GC(U)Gs are represented as a sequence of p-settings (where p denotes principles or parameters) based on a flat (ternary) sequential encoding of such default inheritance lattices. The in2Bouma and van Noord (1994) and others demonstrate that CGs can be embedded in a constraint-based representation. Briscoe (1997a,b) gives further details of the encoding of GCG in TDFSs. 419 NP N S gen-dir subj-dir applic AT AT AT DR DL DT NP gendir applic S N subj-dir AT DR DT AT AT DL applic NP N gen-dir subj-dir S DT AT AT DR DL AT ... Figure 3: Sequential encodings of the grammar fragment heritance hierarchy provides a partial ordering on parameters, which is exploited in the learning procedure. For example, the atomic categories, N, NP and S are each represented by a parameter encoding the presence/absence or lack of specification (T/F /?) of the category in the (U)G. Since they will be unordered in the lattice t</context>
<context position="11332" citStr="Briscoe (1997" startWordPosition="1772" endWordPosition="1773">represent a sample of degree-1 learning triggers for the language (e.g. Lightfoot, 1991). Subset languages are represented by 3-9 sentence types and &apos;full&apos; languages by 12 sentence types. The constructions exemplified by each sentence type and their length are equivalent across all the languages defined by the grammar set, but the sequences of lexical categories can differ. For example, two SOY language renditions of The man who Bill likes gave Fred a 4Representation of the vl/v2 parameter(s) in terms of a type constraint determining allowable functor categories is discussed in more detail in Briscoe (1997b). 420 gen vi n subj obj v2 mod spec relcl adpos compl Engl R F RL R F R R R R R Ger R F RL L TR L R R R R Jap L F LL L F L L L ? Figure 4: The Grammar Set — Ordering Parameters present, one with premodifying and the other postmodifying relative clauses, both with a relative pronoun at the right boundary of the relative clause, are shown below with the differing category highlighted. Bill likes who the-man a-present Fred gave NP, (S \ NP,)\NP„ R,c \ (S \ NP0) NPARc NP02 NPoi ((S\NP5)\NP02)\NP01 The-man Bill likes who a-present Fred gave NPs/Rc NPs (S \ NPs) \ NP0 RcVS \ NP0) NP02 NP01 ((S\NP5</context>
<context position="33579" citStr="Briscoe (1997" startWordPosition="5446" endWordPosition="5447"> Effect (Baldwin, 1896) — genetic assimilation of aspects of the linguistic environment during the period of evolutionary adaptation of the language learning procedure. In the case of grammar learning this is a co-evolutionary process in which languages (and their associated grammars) are also undergoing selection. The WML account of parsing complexity predicts that a right-branching SVO language would be a near optimal selection at a stage in grammatical development when complex rules of reordering such as extraposition, scrambling or mixed order strategies such as vi and v2 had not evolved. Briscoe (1997a) reports further experiments which demonstrate language selection in the model. Though, simulation can expose likely evolutionary pathways under varying conditions, these might have been blocked by accidental factors, such as genetic drift or bottlenecks, causing premature fixation of alleles in the genotype (roughly corresponding to certain p-setting values). The value of the simulation is to, firstly, show that a bioprogram learner could have emerged via adaptation, and secondly, to clarify experimentally the precise conditions required for its emergence. Since in many cases these conditio</context>
</contexts>
<marker>Briscoe, 1997</marker>
<rawString>Briscoe, E.J. (1997b, in prep.) Working memory and its influence on the development of human languages and the human language faculty, University of Cambridge, Computer Laboratory, m.s..</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Government and Binding,</title>
<date>1981</date>
<location>Foris, Dordrecht.</location>
<contexts>
<context position="1038" citStr="Chomsky (1981)" startWordPosition="150" endWordPosition="151"> Experiments show that several experhnentally effective learners can be defined in this framework. Evolutionary simulations suggest that a learner with default initial settings for parameters will emerge, provided that learning is memory limited and the environment of linguistic adaptation contains an appropriate language. 1 Theoretical Background Grammatical acquisition proceeds on the basis of a partial genotypic specification of (universal) grammar (UG) complemented with a learning procedure enabling the child to complete this specification appropriately. The parameter setting framework of Chomsky (1981) claims that learning involves fixing the values of a finite set of finite-valued parameters to select a single fully-specified grammar from within the space defined by the genotypic specification of UG. Formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset-superset relations which may cause a learner to converge to an incorrect grammar (Clark, 1992; Gibson and Wexler, 1994; Niyogi and Berwick, 1995). The solution to these problems involves defining default, unmarked initial values for (some) parameters iird</context>
</contexts>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. (1981) Government and Binding, Foris, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Clark</author>
</authors>
<title>The selection of syntactic knowledge&apos;,</title>
<date>1992</date>
<journal>Language Acquisition,</journal>
<volume>2</volume>
<pages>83--149</pages>
<contexts>
<context position="1476" citStr="Clark, 1992" startWordPosition="221" endWordPosition="222">ersal) grammar (UG) complemented with a learning procedure enabling the child to complete this specification appropriately. The parameter setting framework of Chomsky (1981) claims that learning involves fixing the values of a finite set of finite-valued parameters to select a single fully-specified grammar from within the space defined by the genotypic specification of UG. Formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset-superset relations which may cause a learner to converge to an incorrect grammar (Clark, 1992; Gibson and Wexler, 1994; Niyogi and Berwick, 1995). The solution to these problems involves defining default, unmarked initial values for (some) parameters iird/or ordering the setting of parameters during learning. Bickerton (1984) argues for the Bioprogram Hypothesis as an explanation for universal similarities between historically unrelated creoles, and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages. From the perspective of the parameters framework, the Bioprogram Hypothesis claims that children are endowed genetically with a U</context>
</contexts>
<marker>Clark, 1992</marker>
<rawString>Clark, R. (1992) &apos;The selection of syntactic knowledge&apos;, Language Acquisition, vol.2.2, 83-149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Elman</author>
</authors>
<title>Learning and development in neural networks: the importance of starting small&apos;,</title>
<date>1993</date>
<journal>Cognition,</journal>
<volume>48</volume>
<pages>71--99</pages>
<contexts>
<context position="18015" citStr="Elman, 1993" startWordPosition="2920" endWordPosition="2921">efined in terms of three functions: P-SETTING, GRAMMAR and PARSER, as: PARSERi(GRAMMARi(P-SETTING2(SentenCe3))) A p-setting defines a grammar which in turn defines a parser (where the subscripts indicate the output of each function given the previous trigger). A parameter is updated on parse failure and, if this results in a parse, the new setting is retained. The algorithm is summarized in Figure 8. Working memory grows through childhood (e.g. Baddeley, 1992), and this may assist learning by ensuring that trigger sentences gradually increase in complexity through the acquisition period (e.g. Elman, 1993) by forcing the learner to ignore more complex potential triggers that occur early in the learning process. The WML of a sentence type can be used to determine whether it can function as a trigger at a particular stage in learning. 422 Data: IS1, S2, • • • SO unless PARSER, (GRAMMAR (P-SETTING, (Si))) = Success then p-settingj = UPDATE(p-setting) unless PARSER3 (GRAMMAR i (P-SETTING3 (Si))) = Success then RETURN p-settings, else RETURN p-settingsj Update: Reset the first (most general) default or unset parameter in a left-to-right search of the p-set according to the following table: Input: Dl</context>
</contexts>
<marker>Elman, 1993</marker>
<rawString>Elman, J. (1993) &apos;Learning and development in neural networks: the importance of starting small&apos;, Cognition, vol.48, 71-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>A Copmutational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown, Doctoral dissertation,</title>
<date>1991</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="16069" citStr="Gibson, 1991" startWordPosition="2612" endWordPosition="2614">on Figure 6: The WML Algorithm thus indirectly languages, by parsing each sentence type from the exemplifying data with the associated grammar and then taking the mean of the WML obtained for these sentence types. &amp;quot;English&amp;quot; with Permutation has a lower mean WML than &amp;quot;English&amp;quot; without Permutation, though they are stringset-identical, whilst a hypothetical mixture of &amp;quot;Japanese&amp;quot; SOV clausal order with &amp;quot;English&amp;quot; phrasal syntax has a mean WML which is 25% worse than that for &amp;quot;English&amp;quot;. The WML algorithm is in accord with existing (psycholinguisticallymotivated) theories of parsing complexity (e.g. Gibson, 1991; Hawkins, 1994; Rambow and Joshi, 1994). 2.3 The Parameter Setting Algorithm The parameter setting algorithm is an extension of Gibson and Wexler&apos;s (1994) Trigger Learning Algorithm (TLA) to take account of the inheritancebased partial ordering and the role of memory in learning. The TLA is error-driven — parameter settings are altered in constrained ways when a learner cannot parse trigger input. Trigger input is defined as primary linguistic data which, because of its structure or context of use, is determinately unparsable with the correct interpretation (e.g. Lightfoot, 1991). In this mod</context>
</contexts>
<marker>Gibson, 1991</marker>
<rawString>Gibson, E. (1991) A Copmutational Theory of Human Linguistic Processing: Memory Limitations and Processing Breakdown, Doctoral dissertation, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
<author>K Wexler</author>
</authors>
<title>Triggers&apos;, Linguistic Inquiry,</title>
<date>1994</date>
<volume>25</volume>
<pages>407--454</pages>
<contexts>
<context position="1501" citStr="Gibson and Wexler, 1994" startWordPosition="223" endWordPosition="226">r (UG) complemented with a learning procedure enabling the child to complete this specification appropriately. The parameter setting framework of Chomsky (1981) claims that learning involves fixing the values of a finite set of finite-valued parameters to select a single fully-specified grammar from within the space defined by the genotypic specification of UG. Formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset-superset relations which may cause a learner to converge to an incorrect grammar (Clark, 1992; Gibson and Wexler, 1994; Niyogi and Berwick, 1995). The solution to these problems involves defining default, unmarked initial values for (some) parameters iird/or ordering the setting of parameters during learning. Bickerton (1984) argues for the Bioprogram Hypothesis as an explanation for universal similarities between historically unrelated creoles, and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages. From the perspective of the parameters framework, the Bioprogram Hypothesis claims that children are endowed genetically with a UG which, by default, spec</context>
<context position="24406" citStr="Gibson and Wexler (1994)" startWordPosition="3935" endWordPosition="3938">00 trials to a 1% error rate; they suggest that, in general, the default learner is more effective than the unset learner. However, for the OVS language (OVS languages represent 1.24% of the world&apos;s languages, Tomlin, 1986), and for the unattested OSV language, the default (SVO) learner is less effective. So, there are at least two learning procedures in the space defined by the model which can converge with some presentation orders on some of the grammars in this set. Stronger conclusions require either exhaustive experimentation or theoretical analysis of the model of the type undertaken by Gibson and Wexler (1994) and Niyogi and Berwick (1995). Unset Default None WML 15 39 26 —MML 34 17 29 Table 2: Overall preferences for parameter types 4.2 Evolution of Learning Procedures In order to test the preference for default versus unset parameters under different conditions, the five parameters which define the difference between the two learning procedures were tracked through another series of 50 cycle runs initialized with either 16 default learning adult speakers and 16 unset learning adult speakers, with or without memory-limitations during learning and parsing, speaking one of the eight languages descri</context>
</contexts>
<marker>Gibson, Wexler, 1994</marker>
<rawString>Gibson, E. and Wexler, K. (1994) &apos;Triggers&apos;, Linguistic Inquiry, vol.25.3, 407-454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Greenberg</author>
</authors>
<title>Some universals of grammar with particular reference to the order of meaningful elements&apos;</title>
<date>1966</date>
<booktitle>Universals of Grammar,</booktitle>
<pages>73--113</pages>
<editor>in J. Greenberg (ed.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Ma.,</location>
<contexts>
<context position="7905" citStr="Greenberg, 1966" startWordPosition="1245" endWordPosition="1247"> of the category in the (U)G. Since they will be unordered in the lattice their ordering in the sequential coding is arbitrary. However, the ordering of the directional types gendir and subjdir (with values L/R) is significant as the latter is a more specific type. The distinctions between absolute, default or unset specifications also form part of the encoding (A/D/?). Figure 3 shows several equivalent and equally correct sequential encodings of the fragment of the English type system outlined above. A set of grammars based on typological distinctions defined by basic constituent order (e.g. Greenberg, 1966; Hawkins, 1994) was constructed as a (partial) GCUG with independently varying binaryvalued parameters. The eight basic language families are defined in terms of the unmarked order of verb (V), subject (S) and objects (0) in clauses. Languages within families further specify the order of modifiers and specifiers in phrases, the order of adpositions and further phrasal-level ordering parameters. Figure 4 list the language-specific ordering parameters used to define the full set of grammars in (partial) order of generality, and gives examples of settings based on familiar languages such as &amp;quot;Eng</context>
</contexts>
<marker>Greenberg, 1966</marker>
<rawString>Greenberg, J. (1966) &apos;Some universals of grammar with particular reference to the order of meaningful elements&apos; in J. Greenberg (ed.), Universals of Grammar, MIT Press, Cambridge, Ma., pp. 73-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Hawkins</author>
</authors>
<title>A Performance Theory of Order and Constituency,</title>
<date>1994</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="7921" citStr="Hawkins, 1994" startWordPosition="1248" endWordPosition="1249">in the (U)G. Since they will be unordered in the lattice their ordering in the sequential coding is arbitrary. However, the ordering of the directional types gendir and subjdir (with values L/R) is significant as the latter is a more specific type. The distinctions between absolute, default or unset specifications also form part of the encoding (A/D/?). Figure 3 shows several equivalent and equally correct sequential encodings of the fragment of the English type system outlined above. A set of grammars based on typological distinctions defined by basic constituent order (e.g. Greenberg, 1966; Hawkins, 1994) was constructed as a (partial) GCUG with independently varying binaryvalued parameters. The eight basic language families are defined in terms of the unmarked order of verb (V), subject (S) and objects (0) in clauses. Languages within families further specify the order of modifiers and specifiers in phrases, the order of adpositions and further phrasal-level ordering parameters. Figure 4 list the language-specific ordering parameters used to define the full set of grammars in (partial) order of generality, and gives examples of settings based on familiar languages such as &amp;quot;English&amp;quot;, &amp;quot;German&amp;quot; </context>
<context position="16084" citStr="Hawkins, 1994" startWordPosition="2615" endWordPosition="2616">he WML Algorithm thus indirectly languages, by parsing each sentence type from the exemplifying data with the associated grammar and then taking the mean of the WML obtained for these sentence types. &amp;quot;English&amp;quot; with Permutation has a lower mean WML than &amp;quot;English&amp;quot; without Permutation, though they are stringset-identical, whilst a hypothetical mixture of &amp;quot;Japanese&amp;quot; SOV clausal order with &amp;quot;English&amp;quot; phrasal syntax has a mean WML which is 25% worse than that for &amp;quot;English&amp;quot;. The WML algorithm is in accord with existing (psycholinguisticallymotivated) theories of parsing complexity (e.g. Gibson, 1991; Hawkins, 1994; Rambow and Joshi, 1994). 2.3 The Parameter Setting Algorithm The parameter setting algorithm is an extension of Gibson and Wexler&apos;s (1994) Trigger Learning Algorithm (TLA) to take account of the inheritancebased partial ordering and the role of memory in learning. The TLA is error-driven — parameter settings are altered in constrained ways when a learner cannot parse trigger input. Trigger input is defined as primary linguistic data which, because of its structure or context of use, is determinately unparsable with the correct interpretation (e.g. Lightfoot, 1991). In this model, the issue o</context>
</contexts>
<marker>Hawkins, 1994</marker>
<rawString>Hawkins, J.A. (1994) A Performance Theory of Order and Constituency, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hoffman</author>
</authors>
<title>The Computational Analysis of the Syntax and Interpretation of &apos;Free&apos; Word Order</title>
<date>1995</date>
<institution>University of Pennsylvania.</institution>
<note>in Turkish, PhD dissertation,</note>
<contexts>
<context position="3697" citStr="Hoffman (1995)" startWordPosition="558" endWordPosition="559">hing categories. Generalized Categorial Grammar (GCG) extends CG with further rule schemata.&apos; The rules of FA, BA, generalized weak permutation (P) and backward and forward composition (FC, BC) are given in Figure 1 (where X, Y and Z are category variables, I is a variable over slash and backslash, and ... denotes zero or more further functor arguments). Once permutation is included, several semantically &apos;Wood (1993) is a general introduction to Categorial Grammar and extensions to the basic theory. The most closely related theories to that presented here are those of Steedman (e.g. 1988) and Hoffman (1995). 418 Forward Application: X/Y Y X A y [X(y)] (y) = X(Y) Backward Application: Y X \Y X Y [X(y)] (y) = X(Y) Forward Composition: X/Y Y/Z = X/Z A y [X(y)] A z [Y(z)] A z [X(Y(z))] Backward Composition: Y\Z X\Y = X\Z A z [Y(z)] A y [X(y)] A z [X(Y(z))] (Generalized Weak) Permutation: (XIY1). lYn (XlYn)ilri ... A y„ ...,y1 [X(yi ...,y77)] = A 371,37,2 [X(Yi Figure 1: GCG Rule Schemata Kim loves Sandy NP (S\NP)/NP NP kim&apos; A y,x [love&apos;(x y)] sandy&apos; (S/NP)\NP A x,y [love/(x y)] BA S/NP A y [love&apos;(kim&apos; y)] FA love&apos;(kim&apos; sandy&apos;) Figure 2: GCG Derivation for Kim loves Sandy equivalent derivations for K</context>
<context position="4952" citStr="Hoffman (1995" startWordPosition="764" endWordPosition="766">shows the non-conventional leftbranching one. Composition also allows alternative non-conventional semantically equivalent (leftbranching) derivations. GCG as presented is inadequate as art account of UG or of any individual grammar. In particular, the definition of atomic categories needs extending to deal with featural variation (e.g. Bouma and van Noord, 1994), and the rule schemata, especially composition and weak permutation, must be restricted in various parametric ways so that overgeneration is prevented for specific languages. Nevertheless, GCG does represent a plausible kernel of UG; Hoffman (1995, 1996) explores the descriptive power of a very similar system, in which generalized weak permutation is not required because functor arguments are interpreted as multisets. She demonstrates that this system can handle (long-distance) scrambling elegantly and generates mildly context-sensitive languages (Joshi et al, 1991). The relationship between GCG as a theory of UG (GCUG) and as a the specification of a particular grammar is captured by embedding the theory in a default inheritance hierarchy. This is represented as a lattice of typed default feature structures (TDFSs) representing subsum</context>
</contexts>
<marker>Hoffman, 1995</marker>
<rawString>Hoffman, B. (1995) The Computational Analysis of the Syntax and Interpretation of &apos;Free&apos; Word Order in Turkish, PhD dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hoffman</author>
</authors>
<title>The formal properties of synchronous CCGs&apos;,</title>
<date>1996</date>
<booktitle>Proceedings of the ESSLLI Formal Grammar Conference,</booktitle>
<location>Prague.</location>
<marker>Hoffman, 1996</marker>
<rawString>Hoffman, B. (1996) &apos;The formal properties of synchronous CCGs&apos;, Proceedings of the ESSLLI Formal Grammar Conference, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Holland</author>
</authors>
<title>Echoing emergence: objectives, rough definitions and speculations for echo-class models, Santa Fe Institute,</title>
<date>1993</date>
<tech>Technical Report 93-04-023.</tech>
<marker>Holland, 1993</marker>
<rawString>Holland, J.H. (1993) Echoing emergence: objectives, rough definitions and speculations for echo-class models, Santa Fe Institute, Technical Report 93-04-023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>The convergence of mildly context-sensitive grammar formalisms&apos;</title>
<date>1991</date>
<booktitle>Foundational Issues in Natural Language Processing,</booktitle>
<pages>31--82</pages>
<editor>in Sells, P., Shieber, S. and Wasow, T. (ed.),</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5277" citStr="Joshi et al, 1991" startWordPosition="809" endWordPosition="812">al variation (e.g. Bouma and van Noord, 1994), and the rule schemata, especially composition and weak permutation, must be restricted in various parametric ways so that overgeneration is prevented for specific languages. Nevertheless, GCG does represent a plausible kernel of UG; Hoffman (1995, 1996) explores the descriptive power of a very similar system, in which generalized weak permutation is not required because functor arguments are interpreted as multisets. She demonstrates that this system can handle (long-distance) scrambling elegantly and generates mildly context-sensitive languages (Joshi et al, 1991). The relationship between GCG as a theory of UG (GCUG) and as a the specification of a particular grammar is captured by embedding the theory in a default inheritance hierarchy. This is represented as a lattice of typed default feature structures (TDFSs) representing subsumption and default inheritance relationships (Lascarides et al, 1996; Lascarides and Copestake, 1996). The lattice defines intensionally the set of possible categories and rule schemata via type declarations on nodes. For example, an intransitive verb might be treated as a subtype of verb, inheriting subject directionality b</context>
</contexts>
<marker>Joshi, Vijay-Shanker, Weir, 1991</marker>
<rawString>Joshi, A., Vijay-Shanker, K. and Weir, D. (1991) &apos;The convergence of mildly context-sensitive grammar formalisms&apos; in Sells, P., Shieber, S. and Wasow, T. (ed.), Foundational Issues in Natural Language Processing, MIT Press, pp. 31-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A A Copestake</author>
<author>N Asher</author>
</authors>
<title>Order-independent and persistent default unification&apos;,</title>
<date>1995</date>
<journal>Linguistics and Philosophy,</journal>
<volume>19</volume>
<pages>1--89</pages>
<marker>Copestake, Asher, 1995</marker>
<rawString>Lascarides, A., Briscoe E.J. , Copestake A.A and Asher, N. (1995) &apos;Order-independent and persistent default unification&apos;, Linguistics and Philosophy, vol.19.1, 1-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lascarides</author>
<author>A A Copestake</author>
</authors>
<title>submitted) &apos;Order-independent typed default unification&apos;,</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<contexts>
<context position="5652" citStr="Lascarides and Copestake, 1996" startWordPosition="868" endWordPosition="872">hich generalized weak permutation is not required because functor arguments are interpreted as multisets. She demonstrates that this system can handle (long-distance) scrambling elegantly and generates mildly context-sensitive languages (Joshi et al, 1991). The relationship between GCG as a theory of UG (GCUG) and as a the specification of a particular grammar is captured by embedding the theory in a default inheritance hierarchy. This is represented as a lattice of typed default feature structures (TDFSs) representing subsumption and default inheritance relationships (Lascarides et al, 1996; Lascarides and Copestake, 1996). The lattice defines intensionally the set of possible categories and rule schemata via type declarations on nodes. For example, an intransitive verb might be treated as a subtype of verb, inheriting subject directionality by default from a type gendir (for general direction). For English, gendir is default right but the node of the (intransitive) functor category, where the directionality of subject arguments is specified, overrides this to left, reflecting the fact that English is predominantly right-branching, though subjects appear to the left of the verb. A transitive verb would inherit </context>
</contexts>
<marker>Lascarides, Copestake, 1996</marker>
<rawString>Lascarides, A. and Copestake A.A. (1996, submitted) &apos;Order-independent typed default unification&apos;, Computational Linguistics,</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lightfoot</author>
</authors>
<title>How to Set Parameters: Arguments from language Change,</title>
<date>1991</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Ma..</location>
<contexts>
<context position="2335" citStr="Lightfoot, 1991" startWordPosition="341" endWordPosition="342">or the Bioprogram Hypothesis as an explanation for universal similarities between historically unrelated creoles, and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages. From the perspective of the parameters framework, the Bioprogram Hypothesis claims that children are endowed genetically with a UG which, by default, specifies the stereotypical core creole grammar, with right-branching syntax and subject-verb-object order, as in Saramaccan. Others working within the parameters framework have proposed unmarked, default parameters (e.g. Lightfoot, 1991), but the Bioprogram Hypothesis can be interpreted as towards one end of a continuum of proposals ranging from all parameters initially unset to all set to default values. 2 The Language Acquisition Device A model of the Language Acquisition Device (LAD) incorporates a UG with associated parameters, a parser, and an algorithm for updating initial parameter settings on parse failure during learning. 2.1 The Grammar (set) Basic categorial grammar (CG) uses one rule of application which combines a functor category (containing a slash) with an argument category to form a derived category (with one</context>
<context position="10807" citStr="Lightfoot, 1991" startWordPosition="1687" endWordPosition="1688">UGs). Others yield equivalent grammars, for example, different combinations of default settings (for types and their subtypes) can define an identical category set. The grammars defined generate (usually infinite) stringsets of lexical syntactic categories. These strings are sentence types since each is equivalent to a finite set of grammatical sentences formed by selecting a lexical instance of each lexical category. Languages are represented as a finite subset of sentence types generated by the associated grammar. These represent a sample of degree-1 learning triggers for the language (e.g. Lightfoot, 1991). Subset languages are represented by 3-9 sentence types and &apos;full&apos; languages by 12 sentence types. The constructions exemplified by each sentence type and their length are equivalent across all the languages defined by the grammar set, but the sequences of lexical categories can differ. For example, two SOY language renditions of The man who Bill likes gave Fred a 4Representation of the vl/v2 parameter(s) in terms of a type constraint determining allowable functor categories is discussed in more detail in Briscoe (1997b). 420 gen vi n subj obj v2 mod spec relcl adpos compl Engl R F RL R F R R</context>
<context position="16656" citStr="Lightfoot, 1991" startWordPosition="2706" endWordPosition="2708">complexity (e.g. Gibson, 1991; Hawkins, 1994; Rambow and Joshi, 1994). 2.3 The Parameter Setting Algorithm The parameter setting algorithm is an extension of Gibson and Wexler&apos;s (1994) Trigger Learning Algorithm (TLA) to take account of the inheritancebased partial ordering and the role of memory in learning. The TLA is error-driven — parameter settings are altered in constrained ways when a learner cannot parse trigger input. Trigger input is defined as primary linguistic data which, because of its structure or context of use, is determinately unparsable with the correct interpretation (e.g. Lightfoot, 1991). In this model, the issue of ambiguity and triggers does not arise because all sentence types are treated as triggers represented by p-setting schemata. The TLA is memoryless in the sense that a history of parameter (re)settings is not maintained, in principle, allowing the learner to revisit previous hypotheses. This is what allows Niyogi and Berwick (1995) to formalize parameter setting as a Markov process. However, as Brent (1996) argues, the psychological plausibility of this algorithm is doubtful — there is no evidence that children (randomly) move between neighbouring grammars along pat</context>
</contexts>
<marker>Lightfoot, 1991</marker>
<rawString>Lightfoot, D. (1991) How to Set Parameters: Arguments from language Change, MIT Press, Cambridge, Ma..</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Niyogi</author>
<author>R C Berwick</author>
</authors>
<title>A markov language learning model for finite parameter spaces&apos;,</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>MIT, Cambridge, Ma..</location>
<contexts>
<context position="1528" citStr="Niyogi and Berwick, 1995" startWordPosition="227" endWordPosition="230">a learning procedure enabling the child to complete this specification appropriately. The parameter setting framework of Chomsky (1981) claims that learning involves fixing the values of a finite set of finite-valued parameters to select a single fully-specified grammar from within the space defined by the genotypic specification of UG. Formal accounts of parameter setting have been developed for small fragments but even in these search spaces contain local maxima and subset-superset relations which may cause a learner to converge to an incorrect grammar (Clark, 1992; Gibson and Wexler, 1994; Niyogi and Berwick, 1995). The solution to these problems involves defining default, unmarked initial values for (some) parameters iird/or ordering the setting of parameters during learning. Bickerton (1984) argues for the Bioprogram Hypothesis as an explanation for universal similarities between historically unrelated creoles, and for the rapid increase in grammatical complexity accompanying the transition from pidgin to creole languages. From the perspective of the parameters framework, the Bioprogram Hypothesis claims that children are endowed genetically with a UG which, by default, specifies the stereotypical cor</context>
<context position="17017" citStr="Niyogi and Berwick (1995)" startWordPosition="2763" endWordPosition="2766">ngs are altered in constrained ways when a learner cannot parse trigger input. Trigger input is defined as primary linguistic data which, because of its structure or context of use, is determinately unparsable with the correct interpretation (e.g. Lightfoot, 1991). In this model, the issue of ambiguity and triggers does not arise because all sentence types are treated as triggers represented by p-setting schemata. The TLA is memoryless in the sense that a history of parameter (re)settings is not maintained, in principle, allowing the learner to revisit previous hypotheses. This is what allows Niyogi and Berwick (1995) to formalize parameter setting as a Markov process. However, as Brent (1996) argues, the psychological plausibility of this algorithm is doubtful — there is no evidence that children (randomly) move between neighbouring grammars along paths that revisit previous hypotheses. Therefore, each parameter can only be reset once during the learning process. Each step for a learner can be defined in terms of three functions: P-SETTING, GRAMMAR and PARSER, as: PARSERi(GRAMMARi(P-SETTING2(SentenCe3))) A p-setting defines a grammar which in turn defines a parser (where the subscripts indicate the output</context>
<context position="24436" citStr="Niyogi and Berwick (1995)" startWordPosition="3940" endWordPosition="3943"> they suggest that, in general, the default learner is more effective than the unset learner. However, for the OVS language (OVS languages represent 1.24% of the world&apos;s languages, Tomlin, 1986), and for the unattested OSV language, the default (SVO) learner is less effective. So, there are at least two learning procedures in the space defined by the model which can converge with some presentation orders on some of the grammars in this set. Stronger conclusions require either exhaustive experimentation or theoretical analysis of the model of the type undertaken by Gibson and Wexler (1994) and Niyogi and Berwick (1995). Unset Default None WML 15 39 26 —MML 34 17 29 Table 2: Overall preferences for parameter types 4.2 Evolution of Learning Procedures In order to test the preference for default versus unset parameters under different conditions, the five parameters which define the difference between the two learning procedures were tracked through another series of 50 cycle runs initialized with either 16 default learning adult speakers and 16 unset learning adult speakers, with or without memory-limitations during learning and parsing, speaking one of the eight languages described above. Each condition was </context>
</contexts>
<marker>Niyogi, Berwick, 1995</marker>
<rawString>Niyogi, P. and Berwick, R.C. (1995) &apos;A markov language learning model for finite parameter spaces&apos;, Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, MIT, Cambridge, Ma..</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>A processing model of free word order languages&apos;</title>
<date>1994</date>
<booktitle>Perspectives on Sentence Processing, Lawrence Erlbaum,</booktitle>
<pages>267--301</pages>
<editor>in C. Clifton, L. Frazier and K. Rayner (ed.),</editor>
<location>Hillsdale, NJ.,</location>
<contexts>
<context position="16109" citStr="Joshi, 1994" startWordPosition="2619" endWordPosition="2620">rectly languages, by parsing each sentence type from the exemplifying data with the associated grammar and then taking the mean of the WML obtained for these sentence types. &amp;quot;English&amp;quot; with Permutation has a lower mean WML than &amp;quot;English&amp;quot; without Permutation, though they are stringset-identical, whilst a hypothetical mixture of &amp;quot;Japanese&amp;quot; SOV clausal order with &amp;quot;English&amp;quot; phrasal syntax has a mean WML which is 25% worse than that for &amp;quot;English&amp;quot;. The WML algorithm is in accord with existing (psycholinguisticallymotivated) theories of parsing complexity (e.g. Gibson, 1991; Hawkins, 1994; Rambow and Joshi, 1994). 2.3 The Parameter Setting Algorithm The parameter setting algorithm is an extension of Gibson and Wexler&apos;s (1994) Trigger Learning Algorithm (TLA) to take account of the inheritancebased partial ordering and the role of memory in learning. The TLA is error-driven — parameter settings are altered in constrained ways when a learner cannot parse trigger input. Trigger input is defined as primary linguistic data which, because of its structure or context of use, is determinately unparsable with the correct interpretation (e.g. Lightfoot, 1991). In this model, the issue of ambiguity and triggers </context>
</contexts>
<marker>Joshi, 1994</marker>
<rawString>Rambow, 0. and Joshi, A. (1994) &apos;A processing model of free word order languages&apos; in C. Clifton, L. Frazier and K. Rayner (ed.), Perspectives on Sentence Processing, Lawrence Erlbaum, Hillsdale, NJ., pp. 267-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Combinators and grammars&apos;</title>
<date>1988</date>
<booktitle>Categorial Grammars and Natural Language Structures,</booktitle>
<pages>417--442</pages>
<editor>in R. Oehrle, E. Bach and D. Wheeler (ed.),</editor>
<location>Reidel, Dordrecht,</location>
<marker>Steedman, 1988</marker>
<rawString>Steedman, M. (1988) &apos;Combinators and grammars&apos; in R. Oehrle, E. Bach and D. Wheeler (ed.), Categorial Grammars and Natural Language Structures, Reidel, Dordrecht, pp. 417-442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tomlin</author>
</authors>
<title>Basic Word Order: Functional Principles,</title>
<date>1986</date>
<location>Routledge, London.</location>
<contexts>
<context position="24005" citStr="Tomlin, 1986" startWordPosition="3872" endWordPosition="3873">attempted to parse and learn from it, the state of the learner&apos;s p-settings was examined to determine whether the learner had converged on the same grammar as the adult. Table 1 shows the number of such interaction cycles (i.e. the number of input sentences to within ten) required by each type of learner to converge on each of the eight languages. These figures are each calculated from 100 trials to a 1% error rate; they suggest that, in general, the default learner is more effective than the unset learner. However, for the OVS language (OVS languages represent 1.24% of the world&apos;s languages, Tomlin, 1986), and for the unattested OSV language, the default (SVO) learner is less effective. So, there are at least two learning procedures in the space defined by the model which can converge with some presentation orders on some of the grammars in this set. Stronger conclusions require either exhaustive experimentation or theoretical analysis of the model of the type undertaken by Gibson and Wexler (1994) and Niyogi and Berwick (1995). Unset Default None WML 15 39 26 —MML 34 17 29 Table 2: Overall preferences for parameter types 4.2 Evolution of Learning Procedures In order to test the preference for</context>
</contexts>
<marker>Tomlin, 1986</marker>
<rawString>Tomlin, R.. (1986) Basic Word Order: Functional Principles, Routledge, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Wood</author>
</authors>
<title>Categorial- Grammars,</title>
<date>1993</date>
<location>Routledge, London.</location>
<contexts>
<context position="3503" citStr="Wood (1993)" startWordPosition="527" endWordPosition="528">tegory to form a derived category (with one less slashed argument category). Grammatical constraints of order and agreement are captured by only allowing directed application to adjacent matching categories. Generalized Categorial Grammar (GCG) extends CG with further rule schemata.&apos; The rules of FA, BA, generalized weak permutation (P) and backward and forward composition (FC, BC) are given in Figure 1 (where X, Y and Z are category variables, I is a variable over slash and backslash, and ... denotes zero or more further functor arguments). Once permutation is included, several semantically &apos;Wood (1993) is a general introduction to Categorial Grammar and extensions to the basic theory. The most closely related theories to that presented here are those of Steedman (e.g. 1988) and Hoffman (1995). 418 Forward Application: X/Y Y X A y [X(y)] (y) = X(Y) Backward Application: Y X \Y X Y [X(y)] (y) = X(Y) Forward Composition: X/Y Y/Z = X/Z A y [X(y)] A z [Y(z)] A z [X(Y(z))] Backward Composition: Y\Z X\Y = X\Z A z [Y(z)] A y [X(y)] A z [X(Y(z))] (Generalized Weak) Permutation: (XIY1). lYn (XlYn)ilri ... A y„ ...,y1 [X(yi ...,y77)] = A 371,37,2 [X(Yi Figure 1: GCG Rule Schemata Kim loves Sandy NP (S</context>
</contexts>
<marker>Wood, 1993</marker>
<rawString>Wood, M.M. (1993) Categorial- Grammars, Routledge, London.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>