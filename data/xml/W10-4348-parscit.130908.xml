<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000150">
<title confidence="0.739605">
How to Drink from a Fire Hose:
One Person Can Annoscribe 693 Thousand Utterances in One Month
</title>
<author confidence="0.95047">
David Suendermann, Jackson Liscombe, Roberto Pieraccini
</author>
<affiliation confidence="0.897016">
SpeechCycle Labs
</affiliation>
<address confidence="0.962092">
New York, USA
</address>
<email confidence="0.984258">
fdavid, jackson, roberto}@speechcycle.com
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999765916666667">
Transcription and semantic annotation
.(annoscription) of utterances is crucial
part of speech performance analysis and
tuning of spoken dialog systems and other
natural language processing disciplines.
However, the fact that these are manual
tasks makes them expensive and slow. In
this paper, we will discuss how anno-
scription can be partially automated. We
will show that annoscription can reach a
throughput of 693 thousand utterances per
person month under certain assumptions.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999856">
Ever since spoken dialog systems entered the com-
mercial market in the mid 1990s, the caller’s
speech input is subject to collection, transcription,
and often also semantic annotation. Utterance
transcriptions and annotations (annoscriptions) are
used to measure speech recognition and spoken
language understanding performance of the appli-
cation. Furthermore, they are used to improve
speech recognition and application functionality
by tuning grammars, introducing new transitions
in the call flow to cover more of the callers’ de-
mands, or changing prompt wording or applica-
tion logic to influence the speech input. Anno-
scriptions are also crucial for training statistical
language models and utterance classifiers for call
routing or other unconstrained speech input con-
texts (Gorin et al., 1997). Since very recently, sta-
tistical methods are used to replace conventional
rule-based grammars in every recognition context
of commercial spoken dialog systems (Suender-
mann et al., 2009b). This replacement is only
possible by collecting massive amounts of anno-
scribed data from all contexts of an application.
To give the reader an idea of what massive means
in this case, in (Suendermann et al., 2009b), we
used 2,184,203 utterances to build a complex call
routing system. In (Suendermann et al., 2009a),
4,293,898 utterances were used to localize an En-
glish Internet troubleshooting application to Span-
ish.
Considering that professional service providers
may charge as much as 50 US cents for annoscrib-
ing a single utterance, the usage of these amounts
of data seems prohibitive since costs for such a
project could potentially add up to several million
US dollars. Furthermore, one has to consider the
average speed of annoscription which rarely ex-
ceeds 1000 utterances per hour and person. This
means that the turn-around of a project as men-
tioned above would be several years unless teams
of many people work simultaneously. However,
the integration of the work of a large team be-
comes the more tricky the more people are in-
volved. This is especially true for the annotation
portion since it requires a thorough understand-
ing of the spoken dialog system’s domain and de-
sign and very often can only be conducted under
close supervision by the interaction designer in
charge of the project. Furthermore, there are cru-
cial issues related to intra- and inter-labeler incon-
sistency becoming more critical the more people
work on the same or similar recognition contexts
of a given project.
This paper is to show how it is possible to au-
tomate large portions of both transcription and an-
notation while meeting human performance1 stan-
dards. As an example case, we show how the pro-
posed automation techniques can increase anno-
scription speed to nearly 693 thousand utterances
per person and month.
</bodyText>
<sectionHeader confidence="0.996203" genericHeader="method">
2 Automatic Transcription
</sectionHeader>
<subsectionHeader confidence="0.999784">
2.1 Two Fundamentals
</subsectionHeader>
<bodyText confidence="0.999749266666667">
Automatic transcription of spoken utterances may
not sound as something new to the reader. In
fact, the entire field of automatic speech recogni-
tion is about machine transcription. So, why is it
worth dedicating a full section to something well-
covered in research and industry for half a cen-
tury? The reason is the demand for achieving hu-
man performance as formulated in the introduc-
tion which, as is also well-known, cannot be satis-
fied by any of the large-vocabulary speech recog-
nizers ever developed. In order to demonstrate that
there is indeed a way to achieve human transcrip-
tion performance using automatic speech recogni-
tion, we would like to refer to two fundamental
observations on the performance of speech recog-
</bodyText>
<footnote confidence="0.952876">
1In this paper, performance stands for quality or accuracy
of transcription or annotation. It does not refer to speed or
throughput.
</footnote>
<affiliation confidence="0.487441">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257–260,
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.997551">
257
</page>
<listItem confidence="0.9657591">
nition:
(1) Speech recognition performance can be very
high for contexts of constrained vocabulary. An
example is the recognition of isolated letters in
the scope of a name spelling task as discussed
in (Waibel and Lee, 1990) that achieved a word
error rate of only 1.1%. In contrast, the word error
rate of large-vocabulary continuous speech recog-
nition can be as high as 40 to 65% on telephone
speech (Yuk and Flanagan, 1999).
(2) The positive dependence between speech
recognition performance and amount of data used
to train acoustic and language models, so far, did
not reach a saturation point even considering bil-
lions of training tokens (Och, 2006).
Both of these fundamentals can be applied to the
transcription task for utterances collected on spo-
ken dialog production systems as follows:
(1) The vocabulary of spoken dialog systems can
be rather complex. E.g., the caller utterances used
</listItem>
<bodyText confidence="0.678146181818182">
for the localization project mentioned in Section 1
distinguish more than 13,000 types. However,
the nature of commercial spoken dialog applica-
tions being mostly system-driven strongly con-
strains the vocabulary in many recognition con-
texts. E.g., when the prompt reads
You can say: recording problems, new
installation, frozen screen, or won’t turn
on
callers mostly respond things matching the pro-
posed phrases, occasionally altering the wording,
and only seldomly using completely unexpected
utterances.
(2) The continuous data feed available on high-
traffic spoken dialog systems in production pro-
cessing millions of calls per month can provide
large numbers of utterances for every possible
recognition context. Even if the context appears to
be of a simple nature, as for a yes/no question, the
continuous collection of more data will still have
an impact on the performance of a language model
built using this data.
</bodyText>
<subsectionHeader confidence="0.999603">
2.2 How to Achieve Human Performance
</subsectionHeader>
<bodyText confidence="0.999348603773585">
Even though we have suggested that the recog-
nition performance in many contexts of spoken
dialog systems may be very high, we have still
not shown how our observations can be utilized to
achieve human performance as demanded in Sec-
tion 1. How would a context-dependent speech
recognizer respond when the caller says some-
thing completely unexpected such as let’s wreck a
nice beach when asked for the cell phone number?
While a human transcriber may still be able to cor-
rectly transcribe this sentence, automatic speech
recognition will certainly fail even with the largest
possible training set. The answer to this question
is that the speech recognizer should not respond at
all in this case but admit that it had trouble rec-
ognizing this utterance. Rejection of hypotheses
based on confidence scores is common practice in
many speech and language processing tasks and
is heavily used in spoken dialog systems to avoid
mis-interpretation of user inputs.
So, we now know that we can limit automatic
transcriptions to hypotheses of a minimum relia-
bility. However, how do we prove that this limited
set resembles human performance? What is actu-
ally human performance? Does the human make
errors transcribing? And, if so, how do we mea-
sure human error? What do we compare it against?
To err is human. Accordingly, there is an error
associated with manual transcription which can
only be estimated by comparing somebody’s tran-
scription with somebody else’s due to a lack of
ground truth. Preferably, one should have a good
number of people transcribe the same speech ut-
terances and than compute the average word error
rate comparing every transcription batch with ev-
ery other producing a reliable estimate of the man-
ual error inherent to the transcription task of spo-
ken dialog system utterances. In order to do so,
we compared transcriptions of 258,843 utterances
collected from a variety of applications and recog-
nition contexts partially shared by up to six tran-
scribers and found that they averaged at an inter-
transcriber word error rate of WER0 = 1.3%.
Now, for every recognition context a language
model had been trained, we performed automatic
speech recognition on held-out test sets of N =
1000 utterances producing N hypotheses and their
associated confidence scores P = {p1, ... p�}.
Now, we determined that minimum confidence
threshold p0 for which the word error rate between
the set of hypotheses and manual reference tran-
scriptions was not statistically significantly greater
than WER0:
</bodyText>
<equation confidence="0.9999195">
WER(V(p)) � WER0, (1)
V (p) = {v1,... ,vK}: vk E {1,...,N},p„k ≥ p.
</equation>
<bodyText confidence="0.989499166666667">
Statistical significance was achieved when the
delta resulted in a p value greater than 0.05 using
the x2 calculus. For the number of test utterances,
1000, this point is reached when the word error
on the test set falls below WER1 = 2.2%. This
means that Equation 2.2’s ‘not statistically signifi-
cantly greater than’ sign can be replaced by a reg-
ular smaller-than sign as
WER WER0 q WER &lt; WER1. (2)
This essentially means that there is a chance that
the error produced by automatic transcription is
greater than that of manual transcription, however,
on the test set it could not be found to be of signifi-
cance. Requesting to lower the p value or even de-
manding that the test set performance falls below
the reported manual error can drastically lower the
automation rate and, in the latter case, is not even
reasonable—how can a machine possibly commit
</bodyText>
<equation confidence="0.8620225">
p0 = arg min
���
</equation>
<page confidence="0.935506">
258
</page>
<bodyText confidence="0.981842115384615">
training utterances
transcription automation rate
annotation automation rate
transcription automation rate = N (3)
where |V  |refers to the cardinality of the set V ,
i.e., the number of V ’s members.
The above example’s transcription automation
rate of 23.4% does not yet sound tremendously
high, so we should look at what can be done to
increase the automation rate as much as possible.
It is predictable that the two fundamentals formu-
lated in Section 2.1 have a large impact on recog-
nition performance and, hence, the transcription
automation rate:
(1) In large-scale experiments, we were able to
show a significant (negative) correlation between
the annotation automation rate and task complex-
ity. Since this study does not fit the present paper’s
scope, we will refrain from reporting on details at
this point.
(2) As an example which influence the amount of
training data can have on the transcription automa-
tion rate, Figure 1 shows statistics drawn from
twenty runs of language model training carried out
over the course of seven months while collecting
more and more data.
</bodyText>
<sectionHeader confidence="0.992951" genericHeader="method">
3 Automatic Annotation
</sectionHeader>
<bodyText confidence="0.99636105882353">
Semantic annotation of utterances into one of a fi-
nal set of classes is a task which may require pro-
found understanding of the application and recog-
nition context the specific utterances were col-
lected in. Examples include simple contexts such
as yes/no questions which may be easily manage-
able also by annotators unfamiliar with the ap-
plication, high-resolution open prompt contexts
with hundreds of technical and highly application-
specific classes, or number collection contexts al-
lowing for billions of classes. All these contexts
can benefit from two rules which help to signifi-
cantly reduce an annotator’s workload:
(A) Never do anything twice. This simple state-
ment means that there should be functionality built
into the annotation software or the underlying
database that
</bodyText>
<listItem confidence="0.938021">
• lets the annotator process multiple utterances
with identical transcription in a single step and
• makes sure that whenever a new utterance shows
up with a transcription identical to a formerly an-
notated one, the new utterance gets assigned the
same class automatically.
</listItem>
<bodyText confidence="0.993358571428571">
Figure 2 demonstrates the impact of Rule (A) with
two typical examples. The first is a yes/no context
allowing for the additional global commands help,
hold, agent, repeat, and i don’t know. The other is
an open prompt context distinguishing 79 classes.
When using the token/type distinction, the im-
pact of Rule (A) is that annotation effort becomes
linear with the number of types to work on. While
the ratio between types and tokens in a given cor-
pus can be very small (i.e., the automation rate is
very high, e.g., 95% in the above yes/no example),
this ratio reaches saturation at some point. In the
yes/no example, there is only a gradual difference
between the automation rates for 10 thousand and
1 million utterances. Hence, at a certain point, the
effort becomes virtually linear with the number of
tokens to be processed.
(B) Predict as much as possible. Most of the
recognition contexts for which utterances are tran-
scribed and annotated use grammars to implement
speech recognition functionality. Many of these
</bodyText>
<figure confidence="0.362013">
training utterances
</figure>
<figureCaption confidence="0.976717333333333">
Figure 2: Dependency between number of col-
lected utterances and annotation automation rate
based on Rule (A) for two different contexts
</figureCaption>
<figure confidence="0.58145">
training date
</figure>
<figureCaption confidence="0.997065">
Figure 1: Dependency between amount of training
data and transcription automation rate
</figureCaption>
<bodyText confidence="0.99623625">
less errors than a human being as it is trained on
human transcriptions?
As a proof of concept, we ran automatic tran-
scription against the same set of utterances used
to determine the manual transcription error, and
we found that the average word error rate between
manual and automatic annotation was as low as
1.1% for all utterances whose confidence score ex-
ceeded the context-dependent threshold trained as
described above. In this initial experiment, a total
of 60,608 utterances, i.e., 23.4%, had been auto-
mated.
</bodyText>
<subsectionHeader confidence="0.944941">
2.3 On Automation Rate
</subsectionHeader>
<bodyText confidence="0.9871155">
Formally, transcription automation rate is the ra-
tio of utterances whose confidence exeeded po in
</bodyText>
<equation confidence="0.8983605">
Equation 2.2:
|V (p0)
</equation>
<page confidence="0.99758">
259
</page>
<tableCaption confidence="0.9954855">
Table 1: Annotation automation rates for three dif-
ferent recognition contexts based on Rule (B)
</tableCaption>
<bodyText confidence="0.964290944444445">
grammar #symptoms ann. auto. rate
modem type 43 70.3%
blue/black/snow 10 77.0%
yes/no 10 88.6%
grammars will be rule-based grammars. Even if
the grammars are statistical, most often, earlier
in time, rule-based grammars had been used in
the same recognition context. Hence, we can as-
sume that we are given rule-based grammars for
many recognition contexts of the dialog system
in question. Per definition, rule-based grammars
shall contain canonical rules expressing the rela-
tionship between expected utterances in a given
context and the semantic classes these utterances
are to be associated with. Consequently, when-
ever for an utterance recorded in the context un-
der consideration there is a rule in the grammar,
it provides the correct class for this utterance, and
it can be excluded from annotation. These rules
can be strongly extended to allow for complex pre-
fix and suffix rules, repetitions, sub-grammars &amp;c.
making sure that the majority of utterances will
be covered by the rule-based grammars thereby
minimizing the annotation effort. Table 1 shows
three example grammars of different complex-
ity: One that collects the type of the caller’s mo-
dem, one for the identification of a TV set’s pic-
ture color (blue/black/snow), and a yes/no con-
text with global commands. Annotation automa-
tion rates for these grammars that were not specif-
ically tuned for maximizing automation but di-
rectly taken from the production dialog systems
varied between 70.3% and 88.6%.
To never ever touch a formerly annotated utter-
ance type again and to blindly rely on (mayby out-
dated or erroneous) rule-based grammars to pro-
vide baseline annotations may result in annota-
tion mistakes, possibly major ones when frequent
utterances are concerned. So, how do we make
sure that high annotation performance standards
are met?
To answer this question, the authors have de-
veloped a set of techniques called C7 taking care
of completeness, consistency, congruence, corre-
lation, confusion, coverage, and corpus size of an
annotation set (Suendermann et al., 2008). The
mentioned techniques are also useful in the fre-
quent event of changes to the number or scope of
annotation classes. This can happen e.g. due to
functional changes to the application, changes to
prompts, user behavior, or to contexts preceeding
the current annotation context. Another frequent
reason is the introduction of additional classes to
enlarge the scope of the current context2.
</bodyText>
<footnote confidence="0.901630666666667">
2In a specific context, callers may be asked whether they
want A, B, or C, but they may respond D. The introduc-
tion of a new class D which the application is able to handle
</footnote>
<sectionHeader confidence="0.75426" genericHeader="method">
4 693 Thousand Utterances
</sectionHeader>
<bodyText confidence="0.999998153846154">
Finally, we want to return to the initial statement
of this paper claiming that one person is able to
annoscribe 693 thousand utterances within one
month. An approximated automation rate of 80%
for transcription and 90% for annotation is possi-
ble when there is already a massive database of
annoscriptions available to be exploited for au-
tomation. These rates result in about 139 thou-
sand transcriptions and 69 thousand annotations
outstanding. At a pace of 1000 transcribed or 2000
annotated utterances per hour, the required time
would be 139 hours transcription and 35 hours an-
notation which averages at 40 hours per week3.
</bodyText>
<sectionHeader confidence="0.999568" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999924375">
This paper has demonstrated how automated
annoscription of utterances collected in the
production scope of spoken dialog systems can
effectively accelerate this conventionally entirely
manual effort. When allowing for some overtime,
we have shown that a single person is able to
produce 693 thousand annoscriptions within one
month.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998701458333333">
A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).
F. Och. 2006. Challenges in Machine Translation. In
Proc. of the TC-Star Workshop, Barcelona, Spain.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2008. C5.
In Proc. of the SLT, Goa, India.
D. Suendermann, J. Liscombe, K. Dayanidhi, and
R. Pieraccini. 2009a. Localization of Speech
Recognition in Spoken Dialog Systems: How Ma-
chine Translation Can Make Our Lives Easier. In
Proc. of the Interspeech, Brighton, UK.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009b. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.
A. Waibel and K.-F. Lee. 1990. Readings in Speech
Recognition. Morgan Kaufmann, San Francisco,
USA.
D. Yuk and J. Flanagan. 1999. Telephone Speech
Recognition Using Neural Networks and Hidden
Markov Models. In Proc. of the ICASSP, Phoenix,
USA.
</reference>
<bodyText confidence="0.7374937">
requires the re-annotation of all utterances falling into D’s
scope.
3The original title of this paper claimed that one person
could annoscribe even one million utterances in a month.
However, after receiving multiple complaints about the un-
lawfulness of a 58-hour workweek, we had to change the title
accordingly to avoid disputes with the Department of Labor.
Furthermore, as discussed earlier, at the starting point of an
annoscription project, automation rates are much lower than
later.
</bodyText>
<page confidence="0.993892">
260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439580">
<title confidence="0.9982245">How to Drink from a Fire Hose: One Person Can Annoscribe 693 Thousand Utterances in One Month</title>
<author confidence="0.995477">David Suendermann</author>
<author confidence="0.995477">Jackson Liscombe</author>
<author confidence="0.995477">Roberto</author>
<affiliation confidence="0.632355">SpeechCycle</affiliation>
<address confidence="0.99598">New York, USA</address>
<email confidence="0.701077">jackson,</email>
<abstract confidence="0.999607384615385">Transcription and semantic annotation of utterances is crucial part of speech performance analysis and tuning of spoken dialog systems and other natural language processing disciplines. However, the fact that these are manual tasks makes them expensive and slow. In this paper, we will discuss how annoscription can be partially automated. We will show that annoscription can reach a throughput of 693 thousand utterances per person month under certain assumptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Gorin</author>
<author>G Riccardi</author>
<author>J Wright</author>
</authors>
<title>How May I Help You? Speech Communication,</title>
<date>1997</date>
<pages>23--1</pages>
<contexts>
<context position="1528" citStr="Gorin et al., 1997" startWordPosition="218" endWordPosition="221">ion. Utterance transcriptions and annotations (annoscriptions) are used to measure speech recognition and spoken language understanding performance of the application. Furthermore, they are used to improve speech recognition and application functionality by tuning grammars, introducing new transitions in the call flow to cover more of the callers’ demands, or changing prompt wording or application logic to influence the speech input. Annoscriptions are also crucial for training statistical language models and utterance classifiers for call routing or other unconstrained speech input contexts (Gorin et al., 1997). Since very recently, statistical methods are used to replace conventional rule-based grammars in every recognition context of commercial spoken dialog systems (Suendermann et al., 2009b). This replacement is only possible by collecting massive amounts of annoscribed data from all contexts of an application. To give the reader an idea of what massive means in this case, in (Suendermann et al., 2009b), we used 2,184,203 utterances to build a complex call routing system. In (Suendermann et al., 2009a), 4,293,898 utterances were used to localize an English Internet troubleshooting application to</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. Gorin, G. Riccardi, and J. Wright. 1997. How May I Help You? Speech Communication, 23(1/2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Challenges in Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of the TC-Star Workshop,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5309" citStr="Och, 2006" startWordPosition="837" endWordPosition="838"> be very high for contexts of constrained vocabulary. An example is the recognition of isolated letters in the scope of a name spelling task as discussed in (Waibel and Lee, 1990) that achieved a word error rate of only 1.1%. In contrast, the word error rate of large-vocabulary continuous speech recognition can be as high as 40 to 65% on telephone speech (Yuk and Flanagan, 1999). (2) The positive dependence between speech recognition performance and amount of data used to train acoustic and language models, so far, did not reach a saturation point even considering billions of training tokens (Och, 2006). Both of these fundamentals can be applied to the transcription task for utterances collected on spoken dialog production systems as follows: (1) The vocabulary of spoken dialog systems can be rather complex. E.g., the caller utterances used for the localization project mentioned in Section 1 distinguish more than 13,000 types. However, the nature of commercial spoken dialog applications being mostly system-driven strongly constrains the vocabulary in many recognition contexts. E.g., when the prompt reads You can say: recording problems, new installation, frozen screen, or won’t turn on calle</context>
</contexts>
<marker>Och, 2006</marker>
<rawString>F. Och. 2006. Challenges in Machine Translation. In Proc. of the TC-Star Workshop, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Suendermann</author>
<author>J Liscombe</author>
<author>K Evanini</author>
<author>K Dayanidhi</author>
<author>R Pieraccini</author>
</authors>
<date>2008</date>
<booktitle>C5. In Proc. of the SLT,</booktitle>
<location>Goa, India.</location>
<contexts>
<context position="16193" citStr="Suendermann et al., 2008" startWordPosition="2609" endWordPosition="2612">uction dialog systems varied between 70.3% and 88.6%. To never ever touch a formerly annotated utterance type again and to blindly rely on (mayby outdated or erroneous) rule-based grammars to provide baseline annotations may result in annotation mistakes, possibly major ones when frequent utterances are concerned. So, how do we make sure that high annotation performance standards are met? To answer this question, the authors have developed a set of techniques called C7 taking care of completeness, consistency, congruence, correlation, confusion, coverage, and corpus size of an annotation set (Suendermann et al., 2008). The mentioned techniques are also useful in the frequent event of changes to the number or scope of annotation classes. This can happen e.g. due to functional changes to the application, changes to prompts, user behavior, or to contexts preceeding the current annotation context. Another frequent reason is the introduction of additional classes to enlarge the scope of the current context2. 2In a specific context, callers may be asked whether they want A, B, or C, but they may respond D. The introduction of a new class D which the application is able to handle 4 693 Thousand Utterances Finally</context>
</contexts>
<marker>Suendermann, Liscombe, Evanini, Dayanidhi, Pieraccini, 2008</marker>
<rawString>D. Suendermann, J. Liscombe, K. Evanini, K. Dayanidhi, and R. Pieraccini. 2008. C5. In Proc. of the SLT, Goa, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Suendermann</author>
<author>J Liscombe</author>
<author>K Dayanidhi</author>
<author>R Pieraccini</author>
</authors>
<title>Localization of Speech Recognition in Spoken Dialog Systems: How Machine Translation Can Make Our Lives Easier.</title>
<date>2009</date>
<booktitle>In Proc. of the Interspeech,</booktitle>
<location>Brighton, UK.</location>
<contexts>
<context position="1714" citStr="Suendermann et al., 2009" startWordPosition="244" endWordPosition="248">they are used to improve speech recognition and application functionality by tuning grammars, introducing new transitions in the call flow to cover more of the callers’ demands, or changing prompt wording or application logic to influence the speech input. Annoscriptions are also crucial for training statistical language models and utterance classifiers for call routing or other unconstrained speech input contexts (Gorin et al., 1997). Since very recently, statistical methods are used to replace conventional rule-based grammars in every recognition context of commercial spoken dialog systems (Suendermann et al., 2009b). This replacement is only possible by collecting massive amounts of annoscribed data from all contexts of an application. To give the reader an idea of what massive means in this case, in (Suendermann et al., 2009b), we used 2,184,203 utterances to build a complex call routing system. In (Suendermann et al., 2009a), 4,293,898 utterances were used to localize an English Internet troubleshooting application to Spanish. Considering that professional service providers may charge as much as 50 US cents for annoscribing a single utterance, the usage of these amounts of data seems prohibitive sinc</context>
</contexts>
<marker>Suendermann, Liscombe, Dayanidhi, Pieraccini, 2009</marker>
<rawString>D. Suendermann, J. Liscombe, K. Dayanidhi, and R. Pieraccini. 2009a. Localization of Speech Recognition in Spoken Dialog Systems: How Machine Translation Can Make Our Lives Easier. In Proc. of the Interspeech, Brighton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Suendermann</author>
<author>J Liscombe</author>
<author>K Evanini</author>
<author>K Dayanidhi</author>
<author>R Pieraccini</author>
</authors>
<title>From Rule-Based to Statistical Grammars: Continuous Improvement of Large-Scale Spoken Dialog Systems.</title>
<date>2009</date>
<booktitle>In Proc. of the ICASSP,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1714" citStr="Suendermann et al., 2009" startWordPosition="244" endWordPosition="248">they are used to improve speech recognition and application functionality by tuning grammars, introducing new transitions in the call flow to cover more of the callers’ demands, or changing prompt wording or application logic to influence the speech input. Annoscriptions are also crucial for training statistical language models and utterance classifiers for call routing or other unconstrained speech input contexts (Gorin et al., 1997). Since very recently, statistical methods are used to replace conventional rule-based grammars in every recognition context of commercial spoken dialog systems (Suendermann et al., 2009b). This replacement is only possible by collecting massive amounts of annoscribed data from all contexts of an application. To give the reader an idea of what massive means in this case, in (Suendermann et al., 2009b), we used 2,184,203 utterances to build a complex call routing system. In (Suendermann et al., 2009a), 4,293,898 utterances were used to localize an English Internet troubleshooting application to Spanish. Considering that professional service providers may charge as much as 50 US cents for annoscribing a single utterance, the usage of these amounts of data seems prohibitive sinc</context>
</contexts>
<marker>Suendermann, Liscombe, Evanini, Dayanidhi, Pieraccini, 2009</marker>
<rawString>D. Suendermann, J. Liscombe, K. Evanini, K. Dayanidhi, and R. Pieraccini. 2009b. From Rule-Based to Statistical Grammars: Continuous Improvement of Large-Scale Spoken Dialog Systems. In Proc. of the ICASSP, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Waibel</author>
<author>K-F Lee</author>
</authors>
<title>Readings in Speech Recognition.</title>
<date>1990</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, USA.</location>
<contexts>
<context position="4878" citStr="Waibel and Lee, 1990" startWordPosition="763" endWordPosition="766">ns on the performance of speech recog1In this paper, performance stands for quality or accuracy of transcription or annotation. It does not refer to speed or throughput. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257–260, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 257 nition: (1) Speech recognition performance can be very high for contexts of constrained vocabulary. An example is the recognition of isolated letters in the scope of a name spelling task as discussed in (Waibel and Lee, 1990) that achieved a word error rate of only 1.1%. In contrast, the word error rate of large-vocabulary continuous speech recognition can be as high as 40 to 65% on telephone speech (Yuk and Flanagan, 1999). (2) The positive dependence between speech recognition performance and amount of data used to train acoustic and language models, so far, did not reach a saturation point even considering billions of training tokens (Och, 2006). Both of these fundamentals can be applied to the transcription task for utterances collected on spoken dialog production systems as follows: (1) The vocabulary of spok</context>
</contexts>
<marker>Waibel, Lee, 1990</marker>
<rawString>A. Waibel and K.-F. Lee. 1990. Readings in Speech Recognition. Morgan Kaufmann, San Francisco, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yuk</author>
<author>J Flanagan</author>
</authors>
<title>Telephone Speech Recognition Using Neural Networks and Hidden Markov Models.</title>
<date>1999</date>
<booktitle>In Proc. of the ICASSP,</booktitle>
<location>Phoenix, USA.</location>
<contexts>
<context position="5080" citStr="Yuk and Flanagan, 1999" startWordPosition="799" endWordPosition="802">he 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257–260, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 257 nition: (1) Speech recognition performance can be very high for contexts of constrained vocabulary. An example is the recognition of isolated letters in the scope of a name spelling task as discussed in (Waibel and Lee, 1990) that achieved a word error rate of only 1.1%. In contrast, the word error rate of large-vocabulary continuous speech recognition can be as high as 40 to 65% on telephone speech (Yuk and Flanagan, 1999). (2) The positive dependence between speech recognition performance and amount of data used to train acoustic and language models, so far, did not reach a saturation point even considering billions of training tokens (Och, 2006). Both of these fundamentals can be applied to the transcription task for utterances collected on spoken dialog production systems as follows: (1) The vocabulary of spoken dialog systems can be rather complex. E.g., the caller utterances used for the localization project mentioned in Section 1 distinguish more than 13,000 types. However, the nature of commercial spoken</context>
</contexts>
<marker>Yuk, Flanagan, 1999</marker>
<rawString>D. Yuk and J. Flanagan. 1999. Telephone Speech Recognition Using Neural Networks and Hidden Markov Models. In Proc. of the ICASSP, Phoenix, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>