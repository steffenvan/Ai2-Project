<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022797">
<title confidence="0.996262">
UNITOR: Combining Syntactic and Semantic Kernels for
Twitter Sentiment Analysis
</title>
<author confidence="0.995842">
Giuseppe Castellucci(†), Simone Filice(f), Danilo Croce(*), Roberto Basili(*)
</author>
<affiliation confidence="0.9528575">
(†) Dept. of Electronic Engineering
(�) Dept. of Civil Engineering and Computer Science Engineering
(*) Dept. of Enterprise Engineering
University of Rome, Tor Vergata
</affiliation>
<address confidence="0.680577">
Rome, Italy
</address>
<email confidence="0.996607">
{castellucci,filice,croce,basili}@info.uniroma2.it
</email>
<sectionHeader confidence="0.995601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999716647058824">
In this paper, the UNITOR system participat-
ing in the SemEval-2013 Sentiment Analysis
in Twitter task is presented. The polarity de-
tection of a tweet is modeled as a classifica-
tion task, tackled through a Multiple Kernel
approach. It allows to combine the contribu-
tion of complex kernel functions, such as the
Latent Semantic Kernel and Smoothed Par-
tial Tree Kernel, to implicitly integrate syn-
tactic and lexical information of annotated ex-
amples. In the challenge, UNITOR system
achieves good results, even considering that
no manual feature engineering is performed
and no manually coded resources are em-
ployed. These kernels in-fact embed distri-
butional models of lexical semantics to deter-
mine expressive generalization of tweets.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999383846153846">
Web 2.0 and Social Networks technologies allow
users to generate contents on blogs, forums and new
forms of communication (such as micro-blogging)
writing their opinion about facts, things, events. The
analysis of this information is crucial for companies,
politicians or other users in order to learn what peo-
ple think, and consequently to adjust their strategies.
In such a scenario, the interest in the analysis of the
sentiment expressed by people is rapidly growing.
Twitter1 represents an intriguing source of informa-
tion as it is used to share opinions and sentiments
about brands, products, or situations (Jansen et al.,
2009).
</bodyText>
<footnote confidence="0.975471">
1http://www.twitter.com
</footnote>
<bodyText confidence="0.9993914">
On the other hand, tweet analysis represents a
challenging task for natural language processing
systems. Let us consider the following tweets, evok-
ing a positive (1), and negative (2) polarity, respec-
tively.
</bodyText>
<equation confidence="0.308487">
Porto amazing as the sun sets... http://bit.ly/c28w (1)
@knickfan82 Nooooo ;( they delayed the knicks game
until Monday! (2)
</equation>
<bodyText confidence="0.999775615384615">
Tweets are short, informal and characterized by
their own particular language with “Twitter syntax”,
e.g. retweets (“RT”), user references (“@”), hash-
tags (“#”) or other typical web abbreviations, such
as emoticons or acronyms.
Classical approaches to sentiment analysis (Pang
et al., 2002; Pang and Lee, 2008) are not directly ap-
plicable to tweets: most of them focus on relatively
large texts, e.g. movie or product reviews, and per-
formance drops are experimented in tweets scenario.
Some recent works tried to model the sentiment in
tweets (Go et al., 2009; Pak and Paroubek, 2010;
Kouloumpis et al., 2011; Davidov et al., 2010; Bifet
and Frank, 2010; Croce and Basili, 2012; Barbosa
and Feng, 2010; Agarwal et al., 2011). Specific ap-
proaches and feature modeling are used to achieve
good accuracy levels in tweet polarity recognition.
For example, the use of n-grams, POS tags, polar-
ity lexicon and tweet specific features (e.g. hash-
tag, retweet) are some of the component exploited
by these works in combination with different ma-
chine learning algorithms (e.g. Naive Bayes (Pak
and Paroubek, 2010), k-NN strategies (Davidov et
al., 2010), SVM and Tree Kernels (Agarwal et al.,
2011)).
In this paper, the UNITOR system participating
</bodyText>
<page confidence="0.985111">
369
</page>
<bodyText confidence="0.995988783783784">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 369–374, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
in the SemEval-2013 Sentiment Analysis in Twit-
ter task (Wilson et al., 2013) models the senti-
ment analysis stage as a classification task. A Sup-
port Vector Machine (SVM) classifier learns the as-
sociation between short texts and polarity classes
(i.e. positive, negative, neutral). Different kernel
functions (Shawe-Taylor and Cristianini, 2004) have
been used: each kernel aims at capturing specific as-
pects of the semantic similarity between two tweets,
according to syntactic and lexical information. In
particular, in line with the idea of using convolu-
tion tree kernels to model complex semantic tasks,
e.g. (Collins and Duffy, 2001; Moschitti et al., 2008;
Croce et al., 2011), we adopted the Smoothed Par-
tial Tree Kernel (Croce et al., 2011) (SPTK). It is
a state-of-the-art convolution kernel that allows to
measure the similarity between syntactic structures,
which are partially similar and whose nodes can dif-
fer but are nevertheless semantically related. More-
over, a Bag-of-Word and a Latent Semantic Kernel
(Cristianini et al., 2002) are also combined with the
SPTK in a multi-kernel approach.
Our aim is to design a system that exhibits wide
applicability and robustness. This objective is pur-
sued by adopting an approach that avoids the use
of any manually coded resource (e.g. a polarity
lexicon), but mainly exploits distributional analysis
of unlabeled corpora: the generalization of words
meaning is achieved through the construction of a
Word Space (Sahlgren, 2006), which provides an ef-
fective distributional model of lexical semantics.
In the rest of the paper, in Section 2 we will
deeply explain our approach. In Section 3 the re-
sults achieved by our system in the SemEval-2013
challenge are described and discussed.
</bodyText>
<sectionHeader confidence="0.973848" genericHeader="introduction">
2 System Description
</sectionHeader>
<bodyText confidence="0.99986025">
This section describes the approach behind the
UNITOR system. Tweets pre-processing and lin-
guistic analysis is described in Section 2.1, while the
core modeling is described in 2.2.
</bodyText>
<subsectionHeader confidence="0.991101">
2.1 Tweet Preprocessing
</subsectionHeader>
<bodyText confidence="0.9985314">
Tweets are noisy texts and a pre-processing phase is
required to reduce data sparseness and improve the
generalization capability of the learning algorithms.
The following set of actions is performed before ap-
plying the natural language processing chain:
</bodyText>
<listItem confidence="0.872111884615385">
• fully capitalized words are converted in their
lowercase counterparts;
• reply marks are replaced with the pseudo-token
USER, and POS tag is set to $USR;
• hyperlinks are replaced by the token LINK,
whose POS is $URL;
• hashtags are replaced by the pseudo-token
HASHTAG, whose POS is imposed to $HTG;
• characters consecutively repeated more than
three times are cleaned as they cause high lev-
els of lexical data sparseness (e.g. “nooo!!!!!”
and “nooooo!!!” are both converted into
“noo!!”);
• all emoticons are replaced by SML CLS, where
CLS is an element of a list of classified emoti-
cons (113 emoticons in 13 classes).
For example, the tweet in the example 2 is nor-
malized in ‘user noo sml cry they delayed the knicks
game until monday’. Then, we apply an almost stan-
dard NLP chain with Chaos (Basili and Zanzotto,
2002). In particular, we process each tweet to pro-
duce chunks. We adapt the POS Tagging and Chunk-
ing phases in order to correctly manage the pseudo-
tokens introduced in the normalization step. This is
necessary because tokens like SML SAD are tagged
as nouns, and they influence the chunking quality.
</listItem>
<subsectionHeader confidence="0.999179">
2.2 Modeling Kernel Functions
</subsectionHeader>
<bodyText confidence="0.999959388888889">
Following a summary of the employed kernel func-
tions is provided.
Bag of Word Kernel (BOWK) A basic kernel func-
tion that reflects the lexical overlap between tweets.
Each text is represented as a vector whose dimen-
sions correspond to different words. Each dimen-
sion represents a boolean indicator of the presence
or not of a word in the text. The kernel function is
the cosine similarity between vector pairs.
Lexical Semantic Kernel (LSK) A kernel function
is obtained to generalize the lexical information of
tweets, without exploiting any manually coded re-
source. Basic lexical information is obtained by
a co-occurrence Word Space built accordingly to
the methodology described in (Sahlgren, 2006) and
(Croce and Previtali, 2010). A word-by-context ma-
trix M is obtained through a large scale corpus anal-
ysis. Then the Latent Semantic Analysis (Lan-
</bodyText>
<page confidence="0.984077">
370
</page>
<bodyText confidence="0.978448407407408">
dauer and Dumais, 1997) technique is applied as fol-
lows. The matrix M is decomposed through Singu-
lar Value Decomposition (SVD) (Golub and Kahan,
1965) into the product of three new matrices: U, S,
and V so that S is diagonal and M = USVT . M is
then approximated by Mk = UkSkVkT, where only
the first k columns of U and V are used, correspond-
ing to the first k greatest singular values. The orig-
inal statistical information about M is captured by
the new k-dimensional space, which preserves the
global structure while removing low-variant dimen-
sions, i.e. distribution noise. The result is that every
word is projected in the reduced Word Space and
an entire tweet is represented by applying an addi-
tive linear combination. Finally, the resulting ker-
nel function is the cosine similarity between vector
pairs, in line with (Cristianini et al., 2002).
Smoothed Partial Tree Kernel (SPTK) In order
to exploit the syntactic information of tweets, the
Smoothed Partial Tree Kernel proposed in (Croce et
al., 2011) is adopted. Tree kernels exploit syntactic
similarity through the idea of convolutions among
substructures. Any tree kernel evaluates the number
of common substructures between two trees T1 and
T2 without explicitly considering the whole frag-
ment space. Its general equation is reported here-
after:
</bodyText>
<equation confidence="0.9918855">
TK(T1,T2) = � E A(n1, n2), (3)
n1ENT1 n2ENT2
</equation>
<bodyText confidence="0.999989882352941">
where NT, and NT, are the sets of the T1’s and
T2’s nodes, respectively and A(n1, n2) is equal to
the number of common fragments rooted in the n1
and n2 nodes. The function A determines the na-
ture of the kernel space. In the SPTK formulation
(Croce et al., 2011) this function emphasizes lexical
nodes. It computes the similarity between lexical
nodes as the similarity between words in the Word
Space. So, this kernel allows a generalization both
from the syntactic and the lexical point of view.
However, tree kernel methods are biased by pars-
ing accuracy and standard NLP parsers suffer accu-
racy loss in this scenario (Foster et al., 2011). It
is mainly due to the complexities of the language
adopted in tweets. In this work, we do not use a
representation that depends on full parse trees. A
syntactic representation derived from tweets chunk-
ing (Tjong Kim Sang and Buchholz, 2000) is here
adopted, as shown in Figure 1.
Notice that no explicit manual feature engineering
is applied. On the contrary we expect that discrim-
inative lexical and syntactic information (e.g. nega-
tion) is captured by the kernel in the implicit feature
space, as discussed in (Collins and Duffy, 2001).
A multiple kernel approach Kernel methods are
appealing as they can be integrated in various ma-
chine learning algorithms, such as SVM. Moreover
a combination of kernels is still a kernel function
(Shawe-Taylor and Cristianini, 2004). We employed
a linear combination αBOWK + βLSK + γSPTK
in order to exploit the lexical properties captured by
BOWK (and generalized by LSK) and the syntac-
tic information of the SPTK. In our experiments, the
kernel weights α, β and γ are set to 1.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="background">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9991125">
In this section experimental results of the UNITOR
system are reported.
</bodyText>
<subsectionHeader confidence="0.991162">
3.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.99998372">
In the Sentiment Analysis in Twitter task, two
subtasks are defined: Contextual Polarity
Disambiguation (Task A), and Message
Polarity Classification (Task B). The for-
mer deals with the polarity classification (positive,
negative or neutral) of a marked occurrence of a
word or phrase in a tweet context. For example
the adjective “amazing” in example 1 expresses a
positive marked word. The latter deals with the
classification of an entire tweet with respect to
the three classes positive, negative and neutral. In
both subtasks, we computed a fixed (80%-20%)
split of the training data for classifiers parameter
tuning. Tuned parameters are the regularization
parameter and the cost factor (Morik et al., 1999)
of the SVM formulation. The former represents the
trade off between a training error and the margin.
The latter controls the trade off between positive
and negative examples. The learning phase is made
available by an extended version of SVM-LightTK2,
implementing the smooth matching between tree
nodes.
We built a Word Space based on about 1.5 mil-
lion of tweets downloaded during the challenge pe-
riod using the topic name from the trial material as
</bodyText>
<footnote confidence="0.967821">
2http://disi.unitn.it/moschitti/Tree-Kernel.htm
</footnote>
<page confidence="0.994428">
371
</page>
<figure confidence="0.998271846153846">
TW
Prep
NN
punt
.::.
VerFin
VBZ
set::v
.
LINK
$URL
link::$
sun
</figure>
<page confidence="0.977041">
372
</page>
<subsectionHeader confidence="0.712893">
3.3 Results over Message Polarity
Classification
</subsectionHeader>
<bodyText confidence="0.9996185">
A multi-kernel approach is adopted for this task too,
as described in the following Equation 5:
</bodyText>
<equation confidence="0.999945666666667">
k(t1,t2) = SPTK(0B(t1),0B(t2))
+ BOWK(0B(t1), 0B(t2))
+ LSK(TB(t1),TB(t2)) (5)
</equation>
<bodyText confidence="0.999318666666667">
The OB(x) function extracts a tree representation of
x. In this case no nodes in the trees are marked.
The OB(x) function extracts Bag-of-Word vectors
for all the words in the tweet x, while TB(x) extracts
the linear combination of vectors in the Word Space
for adjectives, nouns, verbs and special tokens (e.g.
hashtag, smiles) of the words in x. Again, a One-Vs-
All strategy (Rifkin and Klautau, 2004) is applied.
Constrained run. Tables 3 and 4 report the result
in the constrained case. In the sms dataset our sys-
tem suffers more with respect to the tweet one. In
both cases, the system shows a performance drop
on the negative class. It seems that the multi-kernel
approach needs more examples to correctly disam-
biguate elements within this class. Indeed, nega-
tive class cardinality was about 15% of the training
data, while the positive and neutral classes approxi-
mately equally divided the remaining 85%. More-
over, it seems that our system confuses polarized
classes with the neutral one. For example, the tweet
“going Hilton hotel on Thursday for #cantwait” is
classified as neutral (the gold label is positive). In
this case, the hashtag is the sentiment bearer, and
our model is not able to capture this information.
</bodyText>
<table confidence="0.999624">
Rank 13/29 class precision recall f1
positive .5224 .7358 .6110
Avg-F1 .5122 negative .6019 .3147 .4133
neutral .7883 .7798 .7840
</table>
<tableCaption confidence="0.9906325">
Table 3: Task B results for the sms dataset in the
constrained case
</tableCaption>
<table confidence="0.99982025">
Rank 13/36 class precision recall f1
positive .7394 .6514 .6926
Avg-F1 .5827 negative .6366 .3760 .4728
neutral .6397 .8085 .7142
</table>
<tableCaption confidence="0.9881315">
Table 4: Task B results for the twitter dataset in the
constrained case
</tableCaption>
<bodyText confidence="0.997506">
Unconstrained run. In the unconstrained case we
trained our system adding 2000 positive examples
and 2000 negative examples to the provided training
set. These additional tweets were downloaded from
Twitter during the challenge period using positive
and negative emoticons as query terms. The under-
lying hypothesis is that the polarity of the emoticons
can be extended to the tweet (Pak and Paroubek,
2010; Croce and Basili, 2012). In tables 5 and 6
performance measures in this setting are reported.
</bodyText>
<table confidence="0.99968575">
Rank 10/15 class precision recall f1
positive .4337 .7317 .5446
Avg-F1 .4888 negative .3294 .6320 .4330
neutral .8524 .3584 .5047
</table>
<tableCaption confidence="0.9839785">
Table 5: Task B results for the sms dataset in the
unconstrained case
</tableCaption>
<table confidence="0.99990025">
Rank 5/15 class precision recall f1
positive .7375 .6399 .6853
Avg-F1 .5950 negative .5729 .4509 .5047
neutral .6478 .7805 .7080
</table>
<tableCaption confidence="0.958318">
Table 6: Task B results for the twitter dataset in the
unconstrained case
</tableCaption>
<bodyText confidence="0.99981705882353">
In this scenario, sms performances are again
lower than the twitter case. This is probably due to
the fact that the sms context is quite different from
the twitter one. This is not true for Task A: polar ex-
pressions are more similar in sms and tweets. Again,
we report a performance drop on the negative class.
However, using more negative tweets seems to be
beneficial. The F1 for this class increased of about
3 points for both datasets. Our approach thus needs
more examples to better generalize from data.
In the future, we should check the redundancy and
novelty of the downloaded material, as early dis-
cussed in (Zanzotto et al., 2011). Moreover, we will
explore the possibility to automatically learn the ker-
nel linear combination coefficients in order to op-
timize the balancing between kernel contributions
(G¨onen and Alpaydin, 2011).
</bodyText>
<sectionHeader confidence="0.996459" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.93947975">
This work has been partially funded by the Ital-
ian Ministry of Industry within the “Industria
2015” Framework, under the project DIVINO
(MI01 00234).
</bodyText>
<page confidence="0.998843">
373
</page>
<sectionHeader confidence="0.98994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999434669811321">
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis of
twitter data. In Proceedings of the Workshop on Lan-
guages in Social Media, pages 30–38, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING, pages 36–44, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Roberto Basili and Fabio Massimo Zanzotto. 2002. Pars-
ing engineering and empirical robustness. Nat. Lang.
Eng., 8(3):97–120, June.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, pages 1–15, Berlin, Heidelberg. Springer-
Verlag.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems (NIPS’2001), pages
625–632.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2002. Latent semantic kernels. J. Intell. Inf. Syst.,
18(2-3):127–152.
Danilo Croce and Roberto Basili. 2012. Grammatical
feature engineering for fine-grained ir tasks. In IIR,
pages 133–143.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: an empirical investigation. In GEMS 2010,
pages 7–16, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In COLING, pages 241–249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: Pos tagging and parsing the twitterverse. In
Analyzing Microtext.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
sentiment classification using distant supervision.
G. Golub and W. Kahan. 1965. Calculating the singular
values and pseudo-inverse of a matrix. Journal of the
Society for Industrial and Applied Mathematics: Se-
ries B, Numerical Analysis, 2(2):pp. 205–224.
Mehmet G¨onen and Ethem Alpaydin. 2011. Multi-
ple kernel learning algorithms. Journal of Machine
Learning Research, 12:2211–2268.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169–2188, November.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the omg! In ICWSM.
Tom Landauer and Sue Dumais. 1997. A solution to
plato’s problem: The latent semantic analysis theory
of acquisition, induction and representation of knowl-
edge. Psychological Review, 104.
Katharina Morik, Peter Brockhausen, and Thorsten
Joachims. 1999. Combining statistical learning with a
knowledge-based approach - a case study in intensive
care monitoring. In ICML, pages 268–277, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Alessandro Moschitti, Daniele Pighin, and Robert Basili.
2008. Tree kernels for semantic role labeling. Com-
putational Linguistics, 34.
Alexander Pak and Patrick Paroubek. 2010. Twitter as a
corpus for sentiment analysis and opinion mining. In
LREC.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–
135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In EMNLP, volume 10,
pages 79–86, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of
one-vs-all classification. J. Mach. Learn. Res., 5:101–
141, December.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the conll-2000 shared task: chunking. In
ConLL ’00, pages 127–132, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan
Ritter, Sara Rosenthal, and Veselin Stoyonov. 2013.
Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the 7th International Workshop on
Semantic Evaluation. Association for Computational
Linguistics.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis. 2011. Linguistic redundancy
in twitter. In EMNLP, pages 659–669.
</reference>
<page confidence="0.999023">
374
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.931045">
<title confidence="0.994144">UNITOR: Combining Syntactic and Semantic Kernels Twitter Sentiment Analysis</title>
<author confidence="0.998921">Simone Danilo Roberto</author>
<affiliation confidence="0.991994">Dept. of Electronic Dept. of Civil Engineering and Computer Science Dept. of Enterprise University of Rome, Tor</affiliation>
<address confidence="0.990836">Rome, Italy</address>
<abstract confidence="0.999024444444444">this paper, the participatin the SemEval-2013 Analysis Twitter is presented. The polarity detection of a tweet is modeled as a classification task, tackled through a Multiple Kernel approach. It allows to combine the contribution of complex kernel functions, such as the Latent Semantic Kernel and Smoothed Partial Tree Kernel, to implicitly integrate syntactic and lexical information of annotated ex- In the challenge, achieves good results, even considering that no manual feature engineering is performed and no manually coded resources are employed. These kernels in-fact embed distributional models of lexical semantics to determine expressive generalization of tweets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment analysis of twitter data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media,</booktitle>
<pages>30--38</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2865" citStr="Agarwal et al., 2011" startWordPosition="428" endWordPosition="431">g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the UNITOR system participating 369 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Se</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment analysis of twitter data. In Proceedings of the Workshop on Languages in Social Media, pages 30–38, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data. In</title>
<date>2010</date>
<booktitle>COLING,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2842" citStr="Barbosa and Feng, 2010" startWordPosition="424" endWordPosition="427">ith “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the UNITOR system participating 369 Second Joint Conference on Lexical and Computational Semanti</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In COLING, pages 36–44, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Parsing engineering and empirical robustness.</title>
<date>2002</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="6680" citStr="Basili and Zanzotto, 2002" startWordPosition="1031" endWordPosition="1034">, whose POS is $URL; • hashtags are replaced by the pseudo-token HASHTAG, whose POS is imposed to $HTG; • characters consecutively repeated more than three times are cleaned as they cause high levels of lexical data sparseness (e.g. “nooo!!!!!” and “nooooo!!!” are both converted into “noo!!”); • all emoticons are replaced by SML CLS, where CLS is an element of a list of classified emoticons (113 emoticons in 13 classes). For example, the tweet in the example 2 is normalized in ‘user noo sml cry they delayed the knicks game until monday’. Then, we apply an almost standard NLP chain with Chaos (Basili and Zanzotto, 2002). In particular, we process each tweet to produce chunks. We adapt the POS Tagging and Chunking phases in order to correctly manage the pseudotokens introduced in the normalization step. This is necessary because tokens like SML SAD are tagged as nouns, and they influence the chunking quality. 2.2 Modeling Kernel Functions Following a summary of the employed kernel functions is provided. Bag of Word Kernel (BOWK) A basic kernel function that reflects the lexical overlap between tweets. Each text is represented as a vector whose dimensions correspond to different words. Each dimension represent</context>
</contexts>
<marker>Basili, Zanzotto, 2002</marker>
<rawString>Roberto Basili and Fabio Massimo Zanzotto. 2002. Parsing engineering and empirical robustness. Nat. Lang. Eng., 8(3):97–120, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 13th international conference on Discovery science,</booktitle>
<pages>1--15</pages>
<publisher>SpringerVerlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="2794" citStr="Bifet and Frank, 2010" startWordPosition="416" endWordPosition="419">haracterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the UNITOR system participating 369 Second Joint</context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Proceedings of the 13th international conference on Discovery science, pages 1–15, Berlin, Heidelberg. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems (NIPS’2001),</booktitle>
<pages>625--632</pages>
<contexts>
<context position="4269" citStr="Collins and Duffy, 2001" startWordPosition="639" endWordPosition="642">l-2013 Sentiment Analysis in Twitter task (Wilson et al., 2013) models the sentiment analysis stage as a classification task. A Support Vector Machine (SVM) classifier learns the association between short texts and polarity classes (i.e. positive, negative, neutral). Different kernel functions (Shawe-Taylor and Cristianini, 2004) have been used: each kernel aims at capturing specific aspects of the semantic similarity between two tweets, according to syntactic and lexical information. In particular, in line with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011), we adopted the Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK). It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ but are nevertheless semantically related. Moreover, a Bag-of-Word and a Latent Semantic Kernel (Cristianini et al., 2002) are also combined with the SPTK in a multi-kernel approach. Our aim is to design a system that exhibits wide applicability and robustness. This objective is pursued by adopting an approach t</context>
<context position="10397" citStr="Collins and Duffy, 2001" startWordPosition="1653" endWordPosition="1656">nd standard NLP parsers suffer accuracy loss in this scenario (Foster et al., 2011). It is mainly due to the complexities of the language adopted in tweets. In this work, we do not use a representation that depends on full parse trees. A syntactic representation derived from tweets chunking (Tjong Kim Sang and Buchholz, 2000) is here adopted, as shown in Figure 1. Notice that no explicit manual feature engineering is applied. On the contrary we expect that discriminative lexical and syntactic information (e.g. negation) is captured by the kernel in the implicit feature space, as discussed in (Collins and Duffy, 2001). A multiple kernel approach Kernel methods are appealing as they can be integrated in various machine learning algorithms, such as SVM. Moreover a combination of kernels is still a kernel function (Shawe-Taylor and Cristianini, 2004). We employed a linear combination αBOWK + βLSK + γSPTK in order to exploit the lexical properties captured by BOWK (and generalized by LSK) and the syntactic information of the SPTK. In our experiments, the kernel weights α, β and γ are set to 1. 3 Results and Discussion In this section experimental results of the UNITOR system are reported. 3.1 Experimental setu</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proceedings of Neural Information Processing Systems (NIPS’2001), pages 625–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
<author>Huma Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2002</date>
<journal>J. Intell. Inf. Syst.,</journal>
<pages>18--2</pages>
<contexts>
<context position="4678" citStr="Cristianini et al., 2002" startWordPosition="703" endWordPosition="706">tic similarity between two tweets, according to syntactic and lexical information. In particular, in line with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011), we adopted the Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK). It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ but are nevertheless semantically related. Moreover, a Bag-of-Word and a Latent Semantic Kernel (Cristianini et al., 2002) are also combined with the SPTK in a multi-kernel approach. Our aim is to design a system that exhibits wide applicability and robustness. This objective is pursued by adopting an approach that avoids the use of any manually coded resource (e.g. a polarity lexicon), but mainly exploits distributional analysis of unlabeled corpora: the generalization of words meaning is achieved through the construction of a Word Space (Sahlgren, 2006), which provides an effective distributional model of lexical semantics. In the rest of the paper, in Section 2 we will deeply explain our approach. In Section 3</context>
<context position="8710" citStr="Cristianini et al., 2002" startWordPosition="1371" endWordPosition="1374">and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. The original statistical information about M is captured by the new k-dimensional space, which preserves the global structure while removing low-variant dimensions, i.e. distribution noise. The result is that every word is projected in the reduced Word Space and an entire tweet is represented by applying an additive linear combination. Finally, the resulting kernel function is the cosine similarity between vector pairs, in line with (Cristianini et al., 2002). Smoothed Partial Tree Kernel (SPTK) In order to exploit the syntactic information of tweets, the Smoothed Partial Tree Kernel proposed in (Croce et al., 2011) is adopted. Tree kernels exploit syntactic similarity through the idea of convolutions among substructures. Any tree kernel evaluates the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its general equation is reported hereafter: TK(T1,T2) = � E A(n1, n2), (3) n1ENT1 n2ENT2 where NT, and NT, are the sets of the T1’s and T2’s nodes, respectively and A(n1, n2) is equal t</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2002</marker>
<rawString>Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2002. Latent semantic kernels. J. Intell. Inf. Syst., 18(2-3):127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Roberto Basili</author>
</authors>
<title>Grammatical feature engineering for fine-grained ir tasks.</title>
<date>2012</date>
<booktitle>In IIR,</booktitle>
<pages>133--143</pages>
<contexts>
<context position="2818" citStr="Croce and Basili, 2012" startWordPosition="420" endWordPosition="423">wn particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the UNITOR system participating 369 Second Joint Conference on Lexical a</context>
<context position="14563" citStr="Croce and Basili, 2012" startWordPosition="2326" endWordPosition="2329">6 class precision recall f1 positive .7394 .6514 .6926 Avg-F1 .5827 negative .6366 .3760 .4728 neutral .6397 .8085 .7142 Table 4: Task B results for the twitter dataset in the constrained case Unconstrained run. In the unconstrained case we trained our system adding 2000 positive examples and 2000 negative examples to the provided training set. These additional tweets were downloaded from Twitter during the challenge period using positive and negative emoticons as query terms. The underlying hypothesis is that the polarity of the emoticons can be extended to the tweet (Pak and Paroubek, 2010; Croce and Basili, 2012). In tables 5 and 6 performance measures in this setting are reported. Rank 10/15 class precision recall f1 positive .4337 .7317 .5446 Avg-F1 .4888 negative .3294 .6320 .4330 neutral .8524 .3584 .5047 Table 5: Task B results for the sms dataset in the unconstrained case Rank 5/15 class precision recall f1 positive .7375 .6399 .6853 Avg-F1 .5950 negative .5729 .4509 .5047 neutral .6478 .7805 .7080 Table 6: Task B results for the twitter dataset in the unconstrained case In this scenario, sms performances are again lower than the twitter case. This is probably due to the fact that the sms contex</context>
</contexts>
<marker>Croce, Basili, 2012</marker>
<rawString>Danilo Croce and Roberto Basili. 2012. Grammatical feature engineering for fine-grained ir tasks. In IIR, pages 133–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Daniele Previtali</author>
</authors>
<title>Manifold learning for the semi-supervised induction of framenet predicates: an empirical investigation.</title>
<date>2010</date>
<booktitle>In GEMS 2010,</booktitle>
<pages>7--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7738" citStr="Croce and Previtali, 2010" startWordPosition="1202" endWordPosition="1205">function that reflects the lexical overlap between tweets. Each text is represented as a vector whose dimensions correspond to different words. Each dimension represents a boolean indicator of the presence or not of a word in the text. The kernel function is the cosine similarity between vector pairs. Lexical Semantic Kernel (LSK) A kernel function is obtained to generalize the lexical information of tweets, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space built accordingly to the methodology described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Lan370 dauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. The original statistical information about M is captured by the new k-dimensional space, wh</context>
</contexts>
<marker>Croce, Previtali, 2010</marker>
<rawString>Danilo Croce and Daniele Previtali. 2010. Manifold learning for the semi-supervised induction of framenet predicates: an empirical investigation. In GEMS 2010, pages 7–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Croce</author>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="4314" citStr="Croce et al., 2011" startWordPosition="647" endWordPosition="650"> et al., 2013) models the sentiment analysis stage as a classification task. A Support Vector Machine (SVM) classifier learns the association between short texts and polarity classes (i.e. positive, negative, neutral). Different kernel functions (Shawe-Taylor and Cristianini, 2004) have been used: each kernel aims at capturing specific aspects of the semantic similarity between two tweets, according to syntactic and lexical information. In particular, in line with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011), we adopted the Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK). It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ but are nevertheless semantically related. Moreover, a Bag-of-Word and a Latent Semantic Kernel (Cristianini et al., 2002) are also combined with the SPTK in a multi-kernel approach. Our aim is to design a system that exhibits wide applicability and robustness. This objective is pursued by adopting an approach that avoids the use of any manually coded reso</context>
<context position="8870" citStr="Croce et al., 2011" startWordPosition="1396" endWordPosition="1399">original statistical information about M is captured by the new k-dimensional space, which preserves the global structure while removing low-variant dimensions, i.e. distribution noise. The result is that every word is projected in the reduced Word Space and an entire tweet is represented by applying an additive linear combination. Finally, the resulting kernel function is the cosine similarity between vector pairs, in line with (Cristianini et al., 2002). Smoothed Partial Tree Kernel (SPTK) In order to exploit the syntactic information of tweets, the Smoothed Partial Tree Kernel proposed in (Croce et al., 2011) is adopted. Tree kernels exploit syntactic similarity through the idea of convolutions among substructures. Any tree kernel evaluates the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space. Its general equation is reported hereafter: TK(T1,T2) = � E A(n1, n2), (3) n1ENT1 n2ENT2 where NT, and NT, are the sets of the T1’s and T2’s nodes, respectively and A(n1, n2) is equal to the number of common fragments rooted in the n1 and n2 nodes. The function A determines the nature of the kernel space. In the SPTK formulation (Croce et al.,</context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Danilo Croce, Alessandro Moschitti, and Roberto Basili. 2011. Structured lexical similarity via convolution kernels on dependency trees. In Proceedings of EMNLP, Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In COLING,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2771" citStr="Davidov et al., 2010" startWordPosition="412" endWordPosition="415"> short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the UNITOR system partici</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In COLING, pages 241–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>¨Ozlem C¸etinoglu</author>
<author>Joachim Wagner</author>
<author>Joseph Le Roux</author>
<author>Stephen Hogan</author>
<author>Joakim Nivre</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>hardtoparse: Pos tagging and parsing the twitterverse. In Analyzing Microtext.</title>
<date>2011</date>
<marker>Foster, C¸etinoglu, Wagner, Le Roux, Hogan, Nivre, Hogan, van Genabith, 2011</marker>
<rawString>Jennifer Foster, ¨Ozlem C¸etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre Hogan, and Josef van Genabith. 2011. #hardtoparse: Pos tagging and parsing the twitterverse. In Analyzing Microtext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<contexts>
<context position="2700" citStr="Go et al., 2009" startWordPosition="400" endWordPosition="403">ooooo ;( they delayed the knicks game until Monday! (2) Tweets are short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Ke</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Golub</author>
<author>W Kahan</author>
</authors>
<title>Calculating the singular values and pseudo-inverse of a matrix.</title>
<date>1965</date>
<journal>Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>205--224</pages>
<contexts>
<context position="8010" citStr="Golub and Kahan, 1965" startWordPosition="1248" endWordPosition="1251">rity between vector pairs. Lexical Semantic Kernel (LSK) A kernel function is obtained to generalize the lexical information of tweets, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space built accordingly to the methodology described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Lan370 dauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. The original statistical information about M is captured by the new k-dimensional space, which preserves the global structure while removing low-variant dimensions, i.e. distribution noise. The result is that every word is projected in the reduced Word Space and an entire tweet is represented by applying an additive linear combination. Finally, the resulting ke</context>
</contexts>
<marker>Golub, Kahan, 1965</marker>
<rawString>G. Golub and W. Kahan. 1965. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics: Series B, Numerical Analysis, 2(2):pp. 205–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehmet G¨onen</author>
<author>Ethem Alpaydin</author>
</authors>
<title>Multiple kernel learning algorithms.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2211</pages>
<marker>G¨onen, Alpaydin, 2011</marker>
<rawString>Mehmet G¨onen and Ethem Alpaydin. 2011. Multiple kernel learning algorithms. Journal of Machine Learning Research, 12:2211–2268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>J. Am. Soc. Inf. Sci. Technol.,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="1782" citStr="Jansen et al., 2009" startWordPosition="259" endWordPosition="262">cial Networks technologies allow users to generate contents on blogs, forums and new forms of communication (such as micro-blogging) writing their opinion about facts, things, events. The analysis of this information is crucial for companies, politicians or other users in order to learn what people think, and consequently to adjust their strategies. In such a scenario, the interest in the analysis of the sentiment expressed by people is rapidly growing. Twitter1 represents an intriguing source of information as it is used to share opinions and sentiments about brands, products, or situations (Jansen et al., 2009). 1http://www.twitter.com On the other hand, tweet analysis represents a challenging task for natural language processing systems. Let us consider the following tweets, evoking a positive (1), and negative (2) polarity, respectively. Porto amazing as the sun sets... http://bit.ly/c28w (1) @knickfan82 Nooooo ;( they delayed the knicks game until Monday! (2) Tweets are short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical app</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. J. Am. Soc. Inf. Sci. Technol., 60(11):2169–2188, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the omg!</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="2749" citStr="Kouloumpis et al., 2011" startWordPosition="408" endWordPosition="411">il Monday! (2) Tweets are short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna Moore. 2011. Twitter sentiment analysis: The good the bad and the omg! In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Landauer</author>
<author>Sue Dumais</author>
</authors>
<title>A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Tom Landauer and Sue Dumais. 1997. A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Morik</author>
<author>Peter Brockhausen</author>
<author>Thorsten Joachims</author>
</authors>
<title>Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring.</title>
<date>1999</date>
<booktitle>In ICML,</booktitle>
<pages>268--277</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="11712" citStr="Morik et al., 1999" startWordPosition="1864" endWordPosition="1867">biguation (Task A), and Message Polarity Classification (Task B). The former deals with the polarity classification (positive, negative or neutral) of a marked occurrence of a word or phrase in a tweet context. For example the adjective “amazing” in example 1 expresses a positive marked word. The latter deals with the classification of an entire tweet with respect to the three classes positive, negative and neutral. In both subtasks, we computed a fixed (80%-20%) split of the training data for classifiers parameter tuning. Tuned parameters are the regularization parameter and the cost factor (Morik et al., 1999) of the SVM formulation. The former represents the trade off between a training error and the margin. The latter controls the trade off between positive and negative examples. The learning phase is made available by an extended version of SVM-LightTK2, implementing the smooth matching between tree nodes. We built a Word Space based on about 1.5 million of tweets downloaded during the challenge period using the topic name from the trial material as 2http://disi.unitn.it/moschitti/Tree-Kernel.htm 371 TW Prep NN punt .::. VerFin VBZ set::v . LINK $URL link::$ sun 372 3.3 Results over Message Pola</context>
</contexts>
<marker>Morik, Brockhausen, Joachims, 1999</marker>
<rawString>Katharina Morik, Peter Brockhausen, and Thorsten Joachims. 1999. Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring. In ICML, pages 268–277, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Robert Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<contexts>
<context position="4293" citStr="Moschitti et al., 2008" startWordPosition="643" endWordPosition="646"> in Twitter task (Wilson et al., 2013) models the sentiment analysis stage as a classification task. A Support Vector Machine (SVM) classifier learns the association between short texts and polarity classes (i.e. positive, negative, neutral). Different kernel functions (Shawe-Taylor and Cristianini, 2004) have been used: each kernel aims at capturing specific aspects of the semantic similarity between two tweets, according to syntactic and lexical information. In particular, in line with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011), we adopted the Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK). It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ but are nevertheless semantically related. Moreover, a Bag-of-Word and a Latent Semantic Kernel (Cristianini et al., 2002) are also combined with the SPTK in a multi-kernel approach. Our aim is to design a system that exhibits wide applicability and robustness. This objective is pursued by adopting an approach that avoids the use of an</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Robert Basili. 2008. Tree kernels for semantic role labeling. Computational Linguistics, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="2724" citStr="Pak and Paroubek, 2010" startWordPosition="404" endWordPosition="407">ayed the knicks game until Monday! (2) Tweets are short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet specific features (e.g. hashtag, retweet) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2</context>
<context position="14538" citStr="Pak and Paroubek, 2010" startWordPosition="2322" endWordPosition="2325">nstrained case Rank 13/36 class precision recall f1 positive .7394 .6514 .6926 Avg-F1 .5827 negative .6366 .3760 .4728 neutral .6397 .8085 .7142 Table 4: Task B results for the twitter dataset in the constrained case Unconstrained run. In the unconstrained case we trained our system adding 2000 positive examples and 2000 negative examples to the provided training set. These additional tweets were downloaded from Twitter during the challenge period using positive and negative emoticons as query terms. The underlying hypothesis is that the polarity of the emoticons can be extended to the tweet (Pak and Paroubek, 2010; Croce and Basili, 2012). In tables 5 and 6 performance measures in this setting are reported. Rank 10/15 class precision recall f1 positive .4337 .7317 .5446 Avg-F1 .4888 negative .3294 .6320 .4330 neutral .8524 .3584 .5047 Table 5: Task B results for the sms dataset in the unconstrained case Rank 5/15 class precision recall f1 positive .7375 .6399 .6853 Avg-F1 .5950 negative .5729 .4509 .5047 neutral .6478 .7805 .7080 Table 6: Task B results for the twitter dataset in the unconstrained case In this scenario, sms performances are again lower than the twitter case. This is probably due to the</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter as a corpus for sentiment analysis and opinion mining. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="2451" citStr="Pang and Lee, 2008" startWordPosition="357" endWordPosition="360"> analysis represents a challenging task for natural language processing systems. Let us consider the following tweets, evoking a positive (1), and negative (2) polarity, respectively. Porto amazing as the sun sets... http://bit.ly/c28w (1) @knickfan82 Nooooo ;( they delayed the knicks game until Monday! (2) Tweets are short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarity lexicon and tweet s</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1– 135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP,</booktitle>
<volume>10</volume>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2430" citStr="Pang et al., 2002" startWordPosition="353" endWordPosition="356">e other hand, tweet analysis represents a challenging task for natural language processing systems. Let us consider the following tweets, evoking a positive (1), and negative (2) polarity, respectively. Porto amazing as the sun sets... http://bit.ly/c28w (1) @knickfan82 Nooooo ;( they delayed the knicks game until Monday! (2) Tweets are short, informal and characterized by their own particular language with “Twitter syntax”, e.g. retweets (“RT”), user references (“@”), hashtags (“#”) or other typical web abbreviations, such as emoticons or acronyms. Classical approaches to sentiment analysis (Pang et al., 2002; Pang and Lee, 2008) are not directly applicable to tweets: most of them focus on relatively large texts, e.g. movie or product reviews, and performance drops are experimented in tweets scenario. Some recent works tried to model the sentiment in tweets (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Davidov et al., 2010; Bifet and Frank, 2010; Croce and Basili, 2012; Barbosa and Feng, 2010; Agarwal et al., 2011). Specific approaches and feature modeling are used to achieve good accuracy levels in tweet polarity recognition. For example, the use of n-grams, POS tags, polarit</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In EMNLP, volume 10, pages 79–86, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Rifkin</author>
<author>Aldebaro Klautau</author>
</authors>
<title>In defense of one-vs-all classification.</title>
<date>2004</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>5</volume>
<pages>141</pages>
<contexts>
<context position="12909" citStr="Rifkin and Klautau, 2004" startWordPosition="2055" endWordPosition="2058">.3 Results over Message Polarity Classification A multi-kernel approach is adopted for this task too, as described in the following Equation 5: k(t1,t2) = SPTK(0B(t1),0B(t2)) + BOWK(0B(t1), 0B(t2)) + LSK(TB(t1),TB(t2)) (5) The OB(x) function extracts a tree representation of x. In this case no nodes in the trees are marked. The OB(x) function extracts Bag-of-Word vectors for all the words in the tweet x, while TB(x) extracts the linear combination of vectors in the Word Space for adjectives, nouns, verbs and special tokens (e.g. hashtag, smiles) of the words in x. Again, a One-VsAll strategy (Rifkin and Klautau, 2004) is applied. Constrained run. Tables 3 and 4 report the result in the constrained case. In the sms dataset our system suffers more with respect to the tweet one. In both cases, the system shows a performance drop on the negative class. It seems that the multi-kernel approach needs more examples to correctly disambiguate elements within this class. Indeed, negative class cardinality was about 15% of the training data, while the positive and neutral classes approximately equally divided the remaining 85%. Moreover, it seems that our system confuses polarized classes with the neutral one. For exa</context>
</contexts>
<marker>Rifkin, Klautau, 2004</marker>
<rawString>Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-vs-all classification. J. Mach. Learn. Res., 5:101– 141, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Stockholm University.</institution>
<contexts>
<context position="5117" citStr="Sahlgren, 2006" startWordPosition="774" endWordPosition="775">hich are partially similar and whose nodes can differ but are nevertheless semantically related. Moreover, a Bag-of-Word and a Latent Semantic Kernel (Cristianini et al., 2002) are also combined with the SPTK in a multi-kernel approach. Our aim is to design a system that exhibits wide applicability and robustness. This objective is pursued by adopting an approach that avoids the use of any manually coded resource (e.g. a polarity lexicon), but mainly exploits distributional analysis of unlabeled corpora: the generalization of words meaning is achieved through the construction of a Word Space (Sahlgren, 2006), which provides an effective distributional model of lexical semantics. In the rest of the paper, in Section 2 we will deeply explain our approach. In Section 3 the results achieved by our system in the SemEval-2013 challenge are described and discussed. 2 System Description This section describes the approach behind the UNITOR system. Tweets pre-processing and linguistic analysis is described in Section 2.1, while the core modeling is described in 2.2. 2.1 Tweet Preprocessing Tweets are noisy texts and a pre-processing phase is required to reduce data sparseness and improve the generalizatio</context>
<context position="7706" citStr="Sahlgren, 2006" startWordPosition="1199" endWordPosition="1200">BOWK) A basic kernel function that reflects the lexical overlap between tweets. Each text is represented as a vector whose dimensions correspond to different words. Each dimension represents a boolean indicator of the presence or not of a word in the text. The kernel function is the cosine similarity between vector pairs. Lexical Semantic Kernel (LSK) A kernel function is obtained to generalize the lexical information of tweets, without exploiting any manually coded resource. Basic lexical information is obtained by a co-occurrence Word Space built accordingly to the methodology described in (Sahlgren, 2006) and (Croce and Previtali, 2010). A word-by-context matrix M is obtained through a large scale corpus analysis. Then the Latent Semantic Analysis (Lan370 dauer and Dumais, 1997) technique is applied as follows. The matrix M is decomposed through Singular Value Decomposition (SVD) (Golub and Kahan, 1965) into the product of three new matrices: U, S, and V so that S is diagonal and M = USVT . M is then approximated by Mk = UkSkVkT, where only the first k columns of U and V are used, corresponding to the first k greatest singular values. The original statistical information about M is captured by</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model. Ph.D. thesis, Stockholm University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3977" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="592" endWordPosition="595">er, the UNITOR system participating 369 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 369–374, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics in the SemEval-2013 Sentiment Analysis in Twitter task (Wilson et al., 2013) models the sentiment analysis stage as a classification task. A Support Vector Machine (SVM) classifier learns the association between short texts and polarity classes (i.e. positive, negative, neutral). Different kernel functions (Shawe-Taylor and Cristianini, 2004) have been used: each kernel aims at capturing specific aspects of the semantic similarity between two tweets, according to syntactic and lexical information. In particular, in line with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., 2011), we adopted the Smoothed Partial Tree Kernel (Croce et al., 2011) (SPTK). It is a state-of-the-art convolution kernel that allows to measure the similarity between syntactic structures, which are partially similar and whose nodes can differ but are nevertheless </context>
<context position="10631" citStr="Shawe-Taylor and Cristianini, 2004" startWordPosition="1689" endWordPosition="1692">parse trees. A syntactic representation derived from tweets chunking (Tjong Kim Sang and Buchholz, 2000) is here adopted, as shown in Figure 1. Notice that no explicit manual feature engineering is applied. On the contrary we expect that discriminative lexical and syntactic information (e.g. negation) is captured by the kernel in the implicit feature space, as discussed in (Collins and Duffy, 2001). A multiple kernel approach Kernel methods are appealing as they can be integrated in various machine learning algorithms, such as SVM. Moreover a combination of kernels is still a kernel function (Shawe-Taylor and Cristianini, 2004). We employed a linear combination αBOWK + βLSK + γSPTK in order to exploit the lexical properties captured by BOWK (and generalized by LSK) and the syntactic information of the SPTK. In our experiments, the kernel weights α, β and γ are set to 1. 3 Results and Discussion In this section experimental results of the UNITOR system are reported. 3.1 Experimental setup In the Sentiment Analysis in Twitter task, two subtasks are defined: Contextual Polarity Disambiguation (Task A), and Message Polarity Classification (Task B). The former deals with the polarity classification (positive, negative or</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2004</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2004. Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: chunking.</title>
<date>2000</date>
<journal>In ConLL</journal>
<volume>00</volume>
<pages>127--132</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10100" citStr="Sang and Buchholz, 2000" startWordPosition="1604" endWordPosition="1607"> this function emphasizes lexical nodes. It computes the similarity between lexical nodes as the similarity between words in the Word Space. So, this kernel allows a generalization both from the syntactic and the lexical point of view. However, tree kernel methods are biased by parsing accuracy and standard NLP parsers suffer accuracy loss in this scenario (Foster et al., 2011). It is mainly due to the complexities of the language adopted in tweets. In this work, we do not use a representation that depends on full parse trees. A syntactic representation derived from tweets chunking (Tjong Kim Sang and Buchholz, 2000) is here adopted, as shown in Figure 1. Notice that no explicit manual feature engineering is applied. On the contrary we expect that discriminative lexical and syntactic information (e.g. negation) is captured by the kernel in the implicit feature space, as discussed in (Collins and Duffy, 2001). A multiple kernel approach Kernel methods are appealing as they can be integrated in various machine learning algorithms, such as SVM. Moreover a combination of kernels is still a kernel function (Shawe-Taylor and Cristianini, 2004). We employed a linear combination αBOWK + βLSK + γSPTK in order to e</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: chunking. In ConLL ’00, pages 127–132, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyonov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3709" citStr="Wilson et al., 2013" startWordPosition="554" endWordPosition="557">t) are some of the component exploited by these works in combination with different machine learning algorithms (e.g. Naive Bayes (Pak and Paroubek, 2010), k-NN strategies (Davidov et al., 2010), SVM and Tree Kernels (Agarwal et al., 2011)). In this paper, the UNITOR system participating 369 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 369–374, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics in the SemEval-2013 Sentiment Analysis in Twitter task (Wilson et al., 2013) models the sentiment analysis stage as a classification task. A Support Vector Machine (SVM) classifier learns the association between short texts and polarity classes (i.e. positive, negative, neutral). Different kernel functions (Shawe-Taylor and Cristianini, 2004) have been used: each kernel aims at capturing specific aspects of the semantic similarity between two tweets, according to syntactic and lexical information. In particular, in line with the idea of using convolution tree kernels to model complex semantic tasks, e.g. (Collins and Duffy, 2001; Moschitti et al., 2008; Croce et al., </context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyonov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyonov. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Kostas Tsioutsiouliklis</author>
</authors>
<title>Linguistic redundancy in twitter.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>659--669</pages>
<marker>Zanzotto, Pennacchiotti, Tsioutsiouliklis, 2011</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in twitter. In EMNLP, pages 659–669.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>