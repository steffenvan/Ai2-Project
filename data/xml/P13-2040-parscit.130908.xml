<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000087">
<title confidence="0.989272">
Semantic Neighborhoods as Hypergraphs
</title>
<author confidence="0.974482">
Chris Quirk and Pallavi Choudhury
</author>
<affiliation confidence="0.951424">
Microsoft Research
</affiliation>
<address confidence="0.933394">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.999734">
{chrisq,pallavic}@microsoft.com
</email>
<sectionHeader confidence="0.997409" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999590210526316">
Ambiguity preserving representations
such as lattices are very useful in a num-
ber of NLP tasks, including paraphrase
generation, paraphrase recognition, and
machine translation evaluation. Lattices
compactly represent lexical variation, but
word order variation leads to a combina-
torial explosion of states. We advocate
hypergraphs as compact representations
for sets of utterances describing the same
event or object. We present a method
to construct hypergraphs from sets of
utterances, and evaluate this method on
a simple recognition task. Given a set of
utterances that describe a single object or
event, we construct such a hypergraph,
and demonstrate that it can recognize
novel descriptions of the same event with
high accuracy.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999088076923077">
Humans can construct a broad range of descrip-
tions for almost any object or event. In this paper,
we will refer to such objects or events as ground-
ings, in the sense of grounded semantics. Exam-
ples of groundings include pictures (Rashtchian et
al., 2010), videos (Chen and Dolan, 2011), transla-
tions of a sentence from another language (Dreyer
and Marcu, 2012), or even paraphrases of the same
sentence (Barzilay and Lee, 2003).
One crucial problem is recognizing whether
novel utterances are relevant descriptions of those
groundings. In the case of machine translation,
this is the evaluation problem; for images and
videos, this is recognition and retrieval. Generat-
ing descriptions of events is also often an interest-
ing task: we might like to find a novel paraphrase
for a given sentence, or generate a description of a
grounding that meets certain criteria (e.g., brevity,
use of a restricted vocabulary).
Much prior work has used lattices to compactly
represent a range of lexical choices (Pang et al.,
2003). However, lattices cannot compactly repre-
sent alternate word orders, a common occurrence
in linguistic descriptions. Consider the following
excerpts from a video description corpus (Chen
and Dolan, 2011):
</bodyText>
<listItem confidence="0.9934585">
• A man is sliding a cat on the floor.
• A boy is cleaning the floor with the cat.
• A cat is being pushed across the floor by a
man.
</listItem>
<bodyText confidence="0.9896206">
Ideally we would like to recognize that the fol-
lowing utterance is also a valid description of that
event: A cat is being pushed across the floor by a
boy. That is difficult with lattice representations.
Consider the following context free grammar:
</bodyText>
<equation confidence="0.824220125">
S → X0 X1
 |X2 X3
a man  |a boy
is sliding X2 on X4
is cleaning X4 with X2
a cat  |the cat
is being pushed across X4 by X0
the floor
</equation>
<bodyText confidence="0.999899357142857">
This grammar compactly captures many lexical
and syntactic variants of the input set. Note how
the labels act as a kind of multiple-sequence-
alignment allowing reordering: spans of tokens
covered by the same label are, in a sense, aligned.
This hypergraph or grammar represents a seman-
tic neighborhood: a set of utterances that describe
the same entity in a semantic space.
Semantic neighborhoods are defined in terms of
a grounding. Two utterances are neighbors with
respect to some grounding (semantic event) if they
are both descriptions of that grounding. Para-
phrases, in contrast, may be defined over all pos-
sible groundings. That is, two words or phrases
</bodyText>
<figure confidence="0.726048833333333">
X0 →
X1 →
|
X2 →
X3 →
X4 →
</figure>
<page confidence="0.963243">
222
</page>
<bodyText confidence="0.925395736842105">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 222–227,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
are considered paraphrases if there exists some
grounding that they both describe. The para-
phrase relation is more permissive than the seman-
tic neighbor relation in that regard. We believe that
it is much easier to define and evaluate semantic
neighbors. Human annotators may have difficulty
separating paraphrases from unrelated or merely
related utterances, and this line may not be con-
sistent between judges. Annotating whether an ut-
terance clearly describes a grounding is a much
easier task.
This paper describes a simple method for con-
structing hypergraph-shaped Semantic Neighbor-
hoods from sets of expressions describing the
same grounding. The method is evaluated in
a paraphrase recognition task, inspired by a
CAPTCHA task (Von Ahn et al., 2003).
</bodyText>
<sectionHeader confidence="0.992903" genericHeader="method">
2 Inducing neighborhoods
</sectionHeader>
<bodyText confidence="0.999931782608696">
Constructing a hypergraph to capture a set of utter-
ances is a variant of grammar induction. Given a
sample of positive examples, we infer a compact
and accurate description of the underlying lan-
guage. Conventional grammar induction attempts
to define the set of grammatical sentences in the
language. Here, we search for a grammar over the
fluent and adequate descriptions of a particular in-
put. Many of the same techniques still apply.
Rather than starting from scratch, we bootstrap
from an existing English parser. We begin by pars-
ing the set of input utterances. This parsed set of
utterances acts as a sort of treebank. Reading off a
grammar from this treebank produces a grammar
that can generate not only the seed sentences, but
also a broad range of nearby sentences. In the case
above with cat, man, and boy, we would be able
to generate cases legitimate variants where man
was replaced by boy as well as undesired variants
where man is replaced by cat or floor. This initial
grammar captures a large neighborhood of nearby
utterances including many such undesirable ones.
Therefore, we refine the grammar.
Refinements have been in common use in syn-
tactic parsing for years now. Inspired by the re-
sult that manual annotations of Treebank cate-
gories can substantially increase parser accuracy
(Klein and Manning, 2003), several approaches
have been introduced to automatically induce la-
tent symbols on existing trees. We use the split-
merge method commonly used in syntactic pars-
ing (Petrov et al., 2006). In its original setting,
the refinements captured details beyond that of the
original Penn Treebank symbols. Here, we cap-
ture both syntactic and semantic regularities in the
descriptions of a given grounding.
As we perform more rounds of refinement, the
grammar becomes tightly constrained to the orig-
inal sentences. Indeed, if we iterated to a fixed
point, the resulting grammar would parse only the
original sentences. This is a common dilemma in
paraphrase learning: the safest meaning preserv-
ing rewrite is to change nothing. We optimize the
number of split-merge rounds for task-accuracy;
two or three rounds works well in practice. Fig-
ure 1 illustrates the process.
</bodyText>
<subsectionHeader confidence="0.943923">
2.1 Split-merge induction
</subsectionHeader>
<bodyText confidence="0.999771914285714">
We begin with a set of utterances that describe
a specific grounding. They are parsed with a
conventional Penn Treebank parser (Quirk et al.,
2012) to produce a type of treebank. Unlike con-
ventional treebanks which are annotated by human
experts, the trees here are automatically created
and thus are more likely to contain errors. This
treebank is the input to the split-merge process.
Split: Given an input treebank, we propose re-
finements of the symbols in hopes of increasing
the likelihood of the data. For each original sym-
bol in the grammar such as NP, we consider two la-
tent refinements: NP0 and NP1. Each binary rule
then produces 8 possible variants, since the par-
ent, left child, and right child now have two possi-
ble refinements. The parameters of this grammar
are then optimized using EM. Although we do not
know the correct set of latent annotations, we can
search for the parameters that optimize the likeli-
hood of the given treebank. We initialize the pa-
rameters of this refined grammar with the counts
from the original grammar along with a small ran-
dom number. This randomness prevents EM from
starting on a saddle point by breaking symmetries;
Petrov et al. describe this in more detail.
Merge: After EM has run to completion, we
have a new grammar with twice as many symbols
and eight times as many rules. Many of these sym-
bols may not be necessary, however. For instance,
nouns may require substantial refinement to dis-
tinguish a number of different actors and objects,
where determiners might not require much refine-
ment at all. Therefore, we discard the splits that
led to the least increase in likelihood, and then
reestimate the grammar once again.
</bodyText>
<page confidence="0.988339">
223
</page>
<listItem confidence="0.904256">
(a) Input:
• the man plays the piano
• the guy plays the keyboard
(b) Parses:
• (S (NP (DT the) (NN man))
(VP (VBZ plays)
(NP (DT the) (NN piano)))
• (S (NP (DT the) (NN guy))
(VP (VBZ plays)
(NP (DT the) (NN keyboard)))
(c) Parses with latent annotations:
• (S (NP0 (DT the) (NN0 man))
(VP (VBZ plays)
(NP1 (DT the) (NN1 piano)))
• (S (NP0 (DT the) (NN0 guy))
(VP (VBZ plays)
(NP1 (DT the) (NN1 keyboard)))
</listItem>
<figure confidence="0.951474666666667">
(d) Refined grammar:
S NP0 VP
NP0 DT NN0
NP1 DT NN1
NP VBZ NP1
DT the
NN0 man  |guy
NN1 piano  |keyboard
VBZ plays
</figure>
<figureCaption confidence="0.886237666666667">
Figure 1: Example of hypergraph induction. First
a conventional Treebank parser converts input ut-
terances (a) into parse trees (b). A grammar could
</figureCaption>
<bodyText confidence="0.8821785">
be directly read from this small treebank, but it
would conflate all phrases of the same type. In-
stead we induce latent refinements of this small
treebank (c). The resulting grammar (d) can match
and generate novel variants of these inputs, such
as the man plays the keyboard and the buy plays
the piano. While this simplified example sug-
gests a single hard assignment of latent annota-
tions to symbols, in practice we maintain a dis-
tribution over these latent annotations and extract
a weighted grammar.
Iteration: We run this process in series. First
the original grammar is split, then some of the
least useful splits are discarded. This refined
grammar is then split again, with the least useful
splits discarded once again. We repeat for a num-
ber of iterations based on task accuracy.
Final grammar estimation: The EM proce-
dure used during split and merge assigns fractional
counts c(· · · ) to each refined symbol Xi and each
production Xi → Yj Zk. We estimate the final
grammar using these fractional counts.
</bodyText>
<equation confidence="0.996048333333333">
c(Xi,Yj, Zk)
P(Xi → Yj Zk) =
c(Xi)
</equation>
<bodyText confidence="0.9990855">
In Petrov et al., these latent refinements are later
discarded as the goal is to find the best parse with
the original coarse symbols. Here, we retain the
latent refinements during parsing, since they dis-
tinguish semantically related utterances from un-
related utterances. Note in Figure 1 how NN0
and NN1 refer to different objects; were we to ig-
nore that distinction, the parser would recognize
semantically different utterances such as the piano
plays the piano.
</bodyText>
<subsectionHeader confidence="0.998466">
2.2 Pruning and smoothing
</subsectionHeader>
<bodyText confidence="0.999964055555556">
For both speed and accuracy, we may also prune
the resulting rules. Pruning low probability rules
increases the speed of parsing, and tends to in-
crease the precision of the matching operation at
the cost of recall. Here we only use an absolute
threshold; we vary this threshold and inspect the
impact on task accuracy. Once the fully refined
grammar has been trained, we only retain those
rules with a probability above some threshold. By
varying this threshold t we can adjust precision
and recall: as the low probability rules are re-
moved from the grammar, precision tends to in-
crease and recall tends to decrease.
Another critical issue, especially in these small
grammars, is smoothing. When parsing with a
grammar obtained from only 20 to 50 sentences,
we are very likely to encounter words that have
never been seen before. We may reasonably re-
ject such sentences under the assumption that they
are describing words not present in the training
corpus. However, this may be overly restrictive:
we might see additional adjectives, for instance.
In this work, we perform a very simple form of
smoothing. If the fractional count of a word given
a pre-terminal symbol falls below a threshold k,
then we consider that instance rare and reserve a
fraction of its probability mass for unseen words.
This accounts for lexical variation of the ground-
ing, especially in the least consistently used words.
Substantial speedups could be attained by us-
ing finite state approximations of this grammar:
matching complexity drops to cubic to linear in
the length of the input. A broad range of approxi-
mations are available (Nederhof, 2000). Since the
small grammars in our evaluation below seldom
exhibit self-embedding (latent state identification
</bodyText>
<page confidence="0.995912">
224
</page>
<bodyText confidence="0.998029">
tends to remove recursion), these approximations
would often be tight.
</bodyText>
<sectionHeader confidence="0.995956" genericHeader="method">
3 Experimental evaluation
</sectionHeader>
<bodyText confidence="0.999986363636364">
We explore a task in description recognition.
Given a large set of videos and a number of de-
scriptions for each video (Chen and Dolan, 2011),
we build a system that can recognize fluent and
accurate descriptions of videos. Such a recognizer
has a number of uses. One example currently in
evaluation is a novel CAPTCHAs: to differentiate
a human from a bot, a video is presented, and the
response must be a reasonably accurate and fluent
description of this video.
We split the above data into training and test.
From the training sets, we build a set of recogniz-
ers. Then we present these recognizers with a se-
ries of inputs, some of which are from the held out
set of correct descriptions of this video, and some
of which are from descriptions of other videos.
Based on discussions with authors of CAPTCHA
systems, a ratio of actual users to spammers of 2:1
seemed reasonable, so we selected one negative
example for every two positives. This simulates
the accuracy of the system when presented with a
simple bot that supplies random, well-formed text
as CAPTCHA answers.1
As a baseline, we compare against a simple tf-
idf approach. In this baseline we first pool all
the training descriptions of the video into a sin-
gle virtual document. We gather term frequen-
cies and inverse document frequencies across the
whole corpus. An incoming utterance to be classi-
fied is scored by computing the dot product of its
counted terms with each document; it is assigned
to the document with the highest dot product (co-
sine similarity).
Table 2 demonstrates that a baseline tf-idf ap-
proach is a reasonable starting point. An oracle
selection from among the top three is the best per-
formance – clearly this is a reasonable approach.
That said, grammar based approach shows im-
provements over the baseline tf-idf, especially in
recall. Recall is crucial in a CAPTCHA style task:
if we fail to recognize utterances provided by hu-
mans, we risk frustration or abandonment of the
service protected by the CAPTCHA. The relative
importance of false positives versus false negatives
</bodyText>
<footnote confidence="0.99206625">
1A bot might perform object recognition on the videos and
supply a stream of object names. We might simulate this by
classifying utterances consisting of appropriate object words
but without appropriate syntax or function words.
</footnote>
<table confidence="0.999293857142857">
Total videos 2,029
Training descriptions 22,198
types 5,497
tokens 159,963
Testing descriptions 15,934
types 4,075
tokens 114,399
</table>
<tableCaption confidence="0.886393">
Table 1: Characteristics of the evaluation data.
The descriptions from the video description cor-
pus are randomly partitioned into training and test.
</tableCaption>
<table confidence="0.983286518518518">
(a)
Algorithm S k Prec Rec F-0
tf-idf 99.9 46.6 63.6
tf-idf (top 3 oracle) 99.9 65.3 79.0
grammar 2 1 86.6 51.5 64.6
2 4 80.2 62.6 70.3
2 16 74.2 74.2 74.2
2 32 73.5 76.4 74.9
3 1 91.1 43.9 59.2
3 4 83.7 54.4 65.9
3 16 77.3 65.7 71.1
3 32 76.4 68.1 72.0
4 1 94.1 39.7 55.8
4 4 85.5 51.1 64.0
4 16 79.1 61.5 69.2
4 32 78.2 63.9 70.3
(b)
t S Prec Rec F-0
&gt; 4.5 x 10−5 2 74.8 73.9 74.4
&gt; 4.5 x 10−5 3 79.6 60.9 69.0
&gt; 4.5 x 10−5 4 82.5 53.2 64.7
&gt; 3.1 x 10−7 2 74.2 75.0 74.6
&gt; 3.1 x 10−7 3 78.1 64.6 70.7
&gt; 3.1 x 10−7 4 80.7 58.8 68.1
&gt; 0 2 73.4 76.4 74.9
&gt; 0 3 76.4 68.1 72.0
&gt; 0 4 78.2 63.9 70.3
</table>
<tableCaption confidence="0.994886">
Table 2: Experimental results. (a) Comparison of
</tableCaption>
<bodyText confidence="0.929979555555555">
tf-idf baseline against grammar based approach,
varying several free parameters. An oracle checks
if the correct video is in the top three. For the
grammar variants, the number of splits S and the
smoothing threshold k are varied. (b) Variations
on the rule pruning threshold t and number of
split-merge rounds S. &gt; 0 indicates that all rules
are retained. Here the smoothing threshold k is
fixed at 32.
</bodyText>
<page confidence="0.994219">
225
</page>
<listItem confidence="0.931706866666667">
(a) Input descriptions:
• A cat pops a bunch of little balloons that are on the groung.
• A dog attacks a bunch of balloons.
• A dog is biting balloons and popping them.
• A dog is playing balloons.
• A dog is playing with balloons.
• A dog is playing with balls.
• A dog is popping balloons with its teeth.
• A dog is popping balloons.
• A dog is popping balloons.
• A dog plays with a bunch of balloons.
• A small dog is attacking balloons.
• The dog enjoyed popping balloons.
• The dog popped the balloons.
(b) Top ranked yields from the resulting grammar:
</listItem>
<bodyText confidence="0.487505461538462">
+0.085 A dog is popping balloons.
+0.062 A dog is playing with balloons.
+0.038 A dog is playing balloons.
0.038 A dog is attacking balloons.
+0.023 A dog plays with a bunch of balloons.
+0.023 A dog attacks a bunch of balloons.
0.023 A dog pops a bunch of balloons.
0.023 A dog popped a bunch of balloons.
0.023 A dog enjoyed a bunch of balloons.
0.018 The dog is popping balloons.
0.015 A dog is biting balloons.
0.015 A dog is playing with them.
0.015 A dog is playing with its teeth.
</bodyText>
<figureCaption confidence="0.62477325">
Figure 2: Example yields from a small grammar. The descriptions in (a) were parsed as-is (including the
typographical error “groung”), and a refined grammar was trained with 4 splits. The top k yields from
this grammar along with the probability of that derivation are listed in (b). A ‘+’ symbol indicates that
the yield was in the training set. No smoothing or pruning was performed on this grammar.
</figureCaption>
<bodyText confidence="0.999887833333333">
may vary depending on the underlying resource.
Adjusting the free parameters of this method al-
lows us to achieve different thresholds. We can
see that rule pruning does not have a large impact
on overall results, though it does allow yet another
means of tradiing off precision vs. recall.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999993323529412">
We have presented a method for automatically
constructing compact representations of linguis-
tic variation. Although the initial evaluation only
explored a simple recognition task, we feel the
underlying approach is relevant to many linguis-
tic tasks including machine translation evalua-
tion, and natural language command and con-
trol systems. The induction procedure is rather
simple but effective, and addresses some of the
reordering limitations associated with prior ap-
proaches.(Barzilay and Lee, 2003) In effect, we
are performing a multiple sequence alignment that
allows reordering operations. The refined symbols
of the grammar act as a correspondence between
related inputs.
The quality of the input parser is crucial. This
method only considers one possible parse of the
input. A straightforward extension would be to
consider an n-best list or packed forest of input
parses, which would allow the method to move
past errors in the first input process. Perhaps also
this reliance on symbols from the original Tree-
bank is not ideal. We could merge away some or
all of the original distinctions, or explore different
parameterizations of the grammar that allow more
flexibility in parsing.
The handling of unseen words is very simple.
We are investigating means of including addi-
tional paraphrase resources into the training to in-
crease the effective lexical knowledge of the sys-
tem. It is inefficient to learn each grammar inde-
pendently. By sharing parameters across different
groundings, we should be able to identify Seman-
tic Neighborhoods with fewer training instances.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999565">
We would like to thank William Dolan and the
anonymous reviewers for their valuable feedback.
</bodyText>
<sectionHeader confidence="0.999546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998566772727273">
Regina Barzilay and Lillian Lee. 2003. Learn-
ing to paraphrase: An unsupervised approach us-
ing multiple-sequence alignment. In Proceedings of
NAACL-HLT.
David Chen and William Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 190–200, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation eval-
uation. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 162–171, Montr´eal, Canada, June.
Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics, pages 423–430, Sapporo,
</reference>
<page confidence="0.981097">
226
</page>
<reference confidence="0.999703275">
Japan, July. Association for Computational Linguis-
tics.
Mark-Jan Nederhof. 2000. Practical experiments with
regular approximation of context-free languages.
Computational Linguistics, 26(1):17–44, March.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami
Suzuki, Kristina Toutanova, Michael Gamon, Wen-
tau Yih, Colin Cherry, and Lucy Vanderwende.
2012. Msr splat, a language analysis toolkit. In
Proceedings of the Demonstration Session at the
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 21–24, Montr´eal,
Canada, June. Association for Computational Lin-
guistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon’s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, pages 139–147, Los Angeles, June. Asso-
ciation for Computational Linguistics.
Luis Von Ahn, Manuel Blum, Nicholas J. Hopper, and
John Langford. 2003. Captcha: Using hard ai prob-
lems for security. In Eli Biham, editor, Advances in
Cryptology – EUROCRYPT 2003, volume 2656 of
Lecture Notes in Computer Science, pages 294–311.
Springer Berlin Heidelberg.
</reference>
<page confidence="0.997938">
227
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.694880">
<title confidence="0.99952">Semantic Neighborhoods as Hypergraphs</title>
<author confidence="0.924654">Quirk</author>
<affiliation confidence="0.921806">Microsoft</affiliation>
<address confidence="0.879922">One Microsoft Redmond, WA 98052,</address>
<abstract confidence="0.99926545">Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="1352" citStr="Barzilay and Lee, 2003" startWordPosition="201" endWordPosition="204">of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy. 1 Introduction Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of a restricted vocabulary). Much prior work has used lattices to compactly represent a range of lexical choices (Pang et al., 2003). However, l</context>
<context position="18135" citStr="Barzilay and Lee, 2003" startWordPosition="3086" endWordPosition="3090">e impact on overall results, though it does allow yet another means of tradiing off precision vs. recall. 4 Conclusions We have presented a method for automatically constructing compact representations of linguistic variation. Although the initial evaluation only explored a simple recognition task, we feel the underlying approach is relevant to many linguistic tasks including machine translation evaluation, and natural language command and control systems. The induction procedure is rather simple but effective, and addresses some of the reordering limitations associated with prior approaches.(Barzilay and Lee, 2003) In effect, we are performing a multiple sequence alignment that allows reordering operations. The refined symbols of the grammar act as a correspondence between related inputs. The quality of the input parser is crucial. This method only considers one possible parse of the input. A straightforward extension would be to consider an n-best list or packed forest of input parses, which would allow the method to move past errors in the first input process. Perhaps also this reliance on symbols from the original Treebank is not ideal. We could merge away some or all of the original distinctions, or</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chen</author>
<author>William Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>190--200</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1210" citStr="Chen and Dolan, 2011" startWordPosition="178" endWordPosition="181">t. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy. 1 Introduction Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of</context>
<context position="12397" citStr="Chen and Dolan, 2011" startWordPosition="2063" endWordPosition="2066">lly in the least consistently used words. Substantial speedups could be attained by using finite state approximations of this grammar: matching complexity drops to cubic to linear in the length of the input. A broad range of approximations are available (Nederhof, 2000). Since the small grammars in our evaluation below seldom exhibit self-embedding (latent state identification 224 tends to remove recursion), these approximations would often be tight. 3 Experimental evaluation We explore a task in description recognition. Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. Such a recognizer has a number of uses. One example currently in evaluation is a novel CAPTCHAs: to differentiate a human from a bot, a video is presented, and the response must be a reasonably accurate and fluent description of this video. We split the above data into training and test. From the training sets, we build a set of recognizers. Then we present these recognizers with a series of inputs, some of which are from the held out set of correct descriptions of this video, and some of which are from descript</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 190–200, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Daniel Marcu</author>
</authors>
<title>Hyter: Meaning-equivalent semantics for translation evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>162--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1285" citStr="Dreyer and Marcu, 2012" startWordPosition="190" endWordPosition="193">and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy. 1 Introduction Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of a restricted vocabulary). Much prior work has used lattices to compactly r</context>
</contexts>
<marker>Dreyer, Marcu, 2012</marker>
<rawString>Markus Dreyer and Daniel Marcu. 2012. Hyter: Meaning-equivalent semantics for translation evaluation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 162–171, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo,</location>
<contexts>
<context position="5645" citStr="Klein and Manning, 2003" startWordPosition="916" endWordPosition="919"> not only the seed sentences, but also a broad range of nearby sentences. In the case above with cat, man, and boy, we would be able to generate cases legitimate variants where man was replaced by boy as well as undesired variants where man is replaced by cat or floor. This initial grammar captures a large neighborhood of nearby utterances including many such undesirable ones. Therefore, we refine the grammar. Refinements have been in common use in syntactic parsing for years now. Inspired by the result that manual annotations of Treebank categories can substantially increase parser accuracy (Klein and Manning, 2003), several approaches have been introduced to automatically induce latent symbols on existing trees. We use the splitmerge method commonly used in syntactic parsing (Petrov et al., 2006). In its original setting, the refinements captured details beyond that of the original Penn Treebank symbols. Here, we capture both syntactic and semantic regularities in the descriptions of a given grounding. As we perform more rounds of refinement, the grammar becomes tightly constrained to the original sentences. Indeed, if we iterated to a fixed point, the resulting grammar would parse only the original sen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo,</rawString>
</citation>
<citation valid="false">
<authors>
<author>July Japan</author>
</authors>
<title>Association for Computational Linguistics.</title>
<marker>Japan, </marker>
<rawString>Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Practical experiments with regular approximation of context-free languages.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="12046" citStr="Nederhof, 2000" startWordPosition="2011" endWordPosition="2012">dditional adjectives, for instance. In this work, we perform a very simple form of smoothing. If the fractional count of a word given a pre-terminal symbol falls below a threshold k, then we consider that instance rare and reserve a fraction of its probability mass for unseen words. This accounts for lexical variation of the grounding, especially in the least consistently used words. Substantial speedups could be attained by using finite state approximations of this grammar: matching complexity drops to cubic to linear in the length of the input. A broad range of approximations are available (Nederhof, 2000). Since the small grammars in our evaluation below seldom exhibit self-embedding (latent state identification 224 tends to remove recursion), these approximations would often be tight. 3 Experimental evaluation We explore a task in description recognition. Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. Such a recognizer has a number of uses. One example currently in evaluation is a novel CAPTCHAs: to differentiate a human from a bot, a video is presented, and the re</context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Mark-Jan Nederhof. 2000. Practical experiments with regular approximation of context-free languages. Computational Linguistics, 26(1):17–44, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<contexts>
<context position="1940" citStr="Pang et al., 2003" startWordPosition="295" endWordPosition="298">tence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e.g., brevity, use of a restricted vocabulary). Much prior work has used lattices to compactly represent a range of lexical choices (Pang et al., 2003). However, lattices cannot compactly represent alternate word orders, a common occurrence in linguistic descriptions. Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): • A man is sliding a cat on the floor. • A boy is cleaning the floor with the cat. • A cat is being pushed across the floor by a man. Ideally we would like to recognize that the following utterance is also a valid description of that event: A cat is being pushed across the floor by a boy. That is difficult with lattice representations. Consider the following context free grammar: S → X0 X1 |</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="5830" citStr="Petrov et al., 2006" startWordPosition="946" endWordPosition="949">aced by boy as well as undesired variants where man is replaced by cat or floor. This initial grammar captures a large neighborhood of nearby utterances including many such undesirable ones. Therefore, we refine the grammar. Refinements have been in common use in syntactic parsing for years now. Inspired by the result that manual annotations of Treebank categories can substantially increase parser accuracy (Klein and Manning, 2003), several approaches have been introduced to automatically induce latent symbols on existing trees. We use the splitmerge method commonly used in syntactic parsing (Petrov et al., 2006). In its original setting, the refinements captured details beyond that of the original Penn Treebank symbols. Here, we capture both syntactic and semantic regularities in the descriptions of a given grounding. As we perform more rounds of refinement, the grammar becomes tightly constrained to the original sentences. Indeed, if we iterated to a fixed point, the resulting grammar would parse only the original sentences. This is a common dilemma in paraphrase learning: the safest meaning preserving rewrite is to change nothing. We optimize the number of split-merge rounds for task-accuracy; two </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chris Quirk</author>
<author>Pallavi Choudhury</author>
<author>Jianfeng Gao</author>
<author>Hisami Suzuki</author>
<author>Kristina Toutanova</author>
<author>Michael Gamon</author>
<author>Wentau Yih</author>
<author>Colin Cherry</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Msr splat, a language analysis toolkit.</title>
<date>2012</date>
<booktitle>In Proceedings of the Demonstration Session at the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>21--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="6677" citStr="Quirk et al., 2012" startWordPosition="1081" endWordPosition="1084">e rounds of refinement, the grammar becomes tightly constrained to the original sentences. Indeed, if we iterated to a fixed point, the resulting grammar would parse only the original sentences. This is a common dilemma in paraphrase learning: the safest meaning preserving rewrite is to change nothing. We optimize the number of split-merge rounds for task-accuracy; two or three rounds works well in practice. Figure 1 illustrates the process. 2.1 Split-merge induction We begin with a set of utterances that describe a specific grounding. They are parsed with a conventional Penn Treebank parser (Quirk et al., 2012) to produce a type of treebank. Unlike conventional treebanks which are annotated by human experts, the trees here are automatically created and thus are more likely to contain errors. This treebank is the input to the split-merge process. Split: Given an input treebank, we propose refinements of the symbols in hopes of increasing the likelihood of the data. For each original symbol in the grammar such as NP, we consider two latent refinements: NP0 and NP1. Each binary rule then produces 8 possible variants, since the parent, left child, and right child now have two possible refinements. The p</context>
</contexts>
<marker>Quirk, Choudhury, Gao, Suzuki, Toutanova, Gamon, Yih, Cherry, Vanderwende, 2012</marker>
<rawString>Chris Quirk, Pallavi Choudhury, Jianfeng Gao, Hisami Suzuki, Kristina Toutanova, Michael Gamon, Wentau Yih, Colin Cherry, and Lucy Vanderwende. 2012. Msr splat, a language analysis toolkit. In Proceedings of the Demonstration Session at the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 21–24, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>139--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles,</location>
<contexts>
<context position="1179" citStr="Rashtchian et al., 2010" startWordPosition="173" endWordPosition="176">describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy. 1 Introduction Humans can construct a broad range of descriptions for almost any object or event. In this paper, we will refer to such objects or events as groundings, in the sense of grounded semantics. Examples of groundings include pictures (Rashtchian et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. In the case of machine translation, this is the evaluation problem; for images and videos, this is recognition and retrieval. Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain </context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 139–147, Los Angeles, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Von Ahn</author>
<author>Manuel Blum</author>
<author>Nicholas J Hopper</author>
<author>John Langford</author>
</authors>
<title>Captcha: Using hard ai problems for security.</title>
<date>2003</date>
<booktitle>Advances in Cryptology – EUROCRYPT 2003,</booktitle>
<volume>2656</volume>
<pages>294--311</pages>
<editor>In Eli Biham, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Von Ahn, Blum, Hopper, Langford, 2003</marker>
<rawString>Luis Von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford. 2003. Captcha: Using hard ai problems for security. In Eli Biham, editor, Advances in Cryptology – EUROCRYPT 2003, volume 2656 of Lecture Notes in Computer Science, pages 294–311. Springer Berlin Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>