<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.806919">
Speech to Speech translation for Nurse Patient Interaction
</title>
<note confidence="0.659757666666667">
Karen Sudre
TeleNav, Inc./1130 Kifer Road,
Sunnyvale CA, 94086
</note>
<email confidence="0.929856">
karens@telenav.com
</email>
<note confidence="0.70494925">
Farzad Ehsani, Jim Kimzey, Elaine
Zuber, Demitrios Master
Fluential Inc./ 1153 Bordeaux Dr.,
Sunnyvale, CA 94089
</note>
<email confidence="0.7820695">
{farzad,jkimzey,elaine,dlm}
@fluentialinc.com
</email>
<sectionHeader confidence="0.992661" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999329285714286">
S-MINDS is a speech translation system,
which allows an English speaker to commu-
nicate with a limited English proficiency
speaker easily within a question-and-answer,
interview-style format. It can handle dialogs
in specific settings such as nurse-patient in-
teraction, or medical triage. We have built
and tested an English-Spanish system for ena-
bling nurse-patient interaction in a number of
domains in Kaiser Permanente achieving a to-
tal translation accuracy of 92.8% (for both
English and Spanish). We will give an over-
view of the system as well as the quantitative
and qualitatively system performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976117647059">
There has been a lot of work in the area of
speech to speech translation by CMU, IBM, SRI,
University of Geneva and others. In a health care
setting, this technology has the potential to give
nurses and other clinical staff immediate access
to consistent, easy-to-use, and accurate medical
interpretation for routine patient encounters. This
could greatly improve safety and quality of care
for patients who speak a different language from
that of the healthcare provider.
This paper describes the building and testing of a
speech translation system, S-MINDS (Speaking
Multilingual Interactive Natural Dialog System),
built in less than 3 months with Kaiser Perma-
nente Hospital in San Francisco, CA. The sys-
tem was able to gain fairly robust results for the
domains that it was designed for, and we believe
</bodyText>
<footnote confidence="0.926878">
© 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
</footnote>
<bodyText confidence="0.977850333333333">
that it does demonstrate that building and deploy-
ing a successful speech translation system is be-
coming possible and even commercially viable.
</bodyText>
<sectionHeader confidence="0.987528" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.993146272727273">
The number of people in the U.S. who speak a
language other than English is large and grow-
ing, and Spanish is the most commonly spoken
language next to English. According to the 2000
census, 18% of the U.S. population over age 5
(47 million people) did not speak English at
home—a 48% increase from 1990. In 2000, 8%
of the population (21 million) was LEP (Limited
English Proficiency), with more than 65% of that
population (almost 14 million people) speaking
Spanish.
A body of research shows that language barriers
impede access to care, compromise quality, and
increase the risk of adverse outcomes. Although
trained medical interpreters and bilingual health-
care providers are effective in overcoming such
language barriers, the use of semi-fluent health-
care professionals and ad hoc interpreters (such
as family members and friends) cause more in-
terpreter errors and lower quality of care (Flores
2005).
When friends and family interpret, they are prone
to omit, add, and substitute information. Often
they inject their own opinions and observations,
or impose their own values and judgments, rather
than interpreting what the patient actually said.
Frequently these ad hoc interpreters have limited
English capabilities themselves and are
unfamiliar with medical terminology.
Furthermore, many patients are reluctant to
disclose private or sensitive information in front
of a family member, thus giving the doctor an
incomplete picture; this sometimes prevents a
</bodyText>
<page confidence="0.996046">
54
</page>
<note confidence="0.604604">
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 54–59
</note>
<bodyText confidence="0.939867">
Manchester, August 2008
correct diagnosis. For example, a battered
woman is unlikely to reveal the true cause of her
injuries if her husband is being used as the
interpreter.
The California Academy of Family Physicians
Foundation conducted practice visits in 2006 and
found that, “Although they realize the use of
family members or friends as interpreters is
probably not the best means of interpretation, all
practice teams use them.” (Chen et al 2007)
</bodyText>
<sectionHeader confidence="0.989209" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.996829166666667">
Fluential’s speech translation system, S-
MINDS1, has a hybrid architecture (Figure 1)
that combines multiple ASR engines and multi-
ple translation engines. This approach only
slightly increases the development cost of new
translation applications, but it greatly improves
the accuracy and the coverage of the system by
leveraging the strengths of both statistical and
grammar/rules-based systems. Furthermore, this
hybrid approach enables rapid integration of new
speech recognition and translation engines as
they become available.
</bodyText>
<figureCaption confidence="0.990765">
Figure 1. The hybrid system architecture of S-
MINDS combines multiple ASR engines with an in-
terpretation engine and an SMT engine. Note that this
figure describes the interaction in English to-second
language direction only. The 2nd language-to-English
direction has only the small ASR engine and the in-
terpretation engine.
</figureCaption>
<subsectionHeader confidence="0.958172">
3.1 Components of Speech Translation
System
</subsectionHeader>
<bodyText confidence="0.999288333333333">
S-MINDS has a modular architecture with the
components described below. All of these com-
ponents already exist, so they will not need to be
</bodyText>
<footnote confidence="0.2751005">
1 Speaking Multilingual Interactive Natural Dialog
System
</footnote>
<bodyText confidence="0.9817455">
developed to conduct the research proposed in
Phase I.
</bodyText>
<subsectionHeader confidence="0.850756">
3.1.1 ASR Engine
</subsectionHeader>
<bodyText confidence="0.999987777777778">
S-MINDS employs multiple acoustic engines so
the best engine can be chosen for each language.
Within each language, two separate language
models are active at the same time, telling the
ASR engines which words and phrases to recog-
nize. A smaller, more directed language model
with higher accuracy is used to capture important
and frequently used concepts. For less frequently
used concepts, a larger language model that gen-
erally has broader coverage but somewhat lower
accuracy is used. The combination of these two
provides high accuracy for responses that can be
anticipated and slightly lower accuracy but
broader coverage for everything else. This
method also allows development of new domains
with very little data—for each domain, only a
new domain-specific small language model
needs to be built.
</bodyText>
<subsectionHeader confidence="0.71031">
3.1.2 Interpretation Engine
</subsectionHeader>
<bodyText confidence="0.999985684210527">
Fluential has created an interpretation engine that
is an alternative to an SMT engine. The S-
MINDS interpretation engine uses information
extracted from the output of the ASR engine and
then performs a paraphrase translation in seman-
tic space. This process is similar to what human
interpreters do when they convey the essential
meaning without providing a literal translation.
The advantage of an interpretation engine is that
new domains can be added more quickly and
with less data than is possible with an SMT en-
gine. For high–volume, routine interactions, an
interpretation engine can be extremely fast and
highly accurate; however, the translation may
lose some of the nuance. Again, this means that
highly accurate target applications can be built
with very little data—only a few examples of
each concept are needed to train the interpreta-
tion engine.
</bodyText>
<subsectionHeader confidence="0.67463">
3.1.3 Statistical Machine Translation En-
gine
</subsectionHeader>
<bodyText confidence="0.886669857142857">
For the S-MINDS SMT engine, Fluential is de-
veloping a novel approach that has generally im-
proved the accuracy of speech translation sys-
tems.2 This approach capitalizes on the intuition
that language is broadly divided into two levels:
2 This effort is ongoing; it has not yet been fully
implemented.
</bodyText>
<page confidence="0.993278">
55
</page>
<bodyText confidence="0.999961210526316">
structure and vocabulary. Traditional statistical
approaches force the system to learn both types
of information simultaneously. However, if the
acquisition of structural information is kept sepa-
rate from the acquisition of vocabulary, the re-
sulting system should learn both levels more ef-
ficiently. And by modifying the existing corpus
to separate structure and vocabulary, we have
been able to take full advantage of all the infor-
mation in the bilingual corpus, producing higher-
quality MT without requiring large bodies of
training data. The most recent modification to
this approach was the use of distance-based or-
dering (Zens and Ney, 2003) and lexicalized or-
dering (Tillmann and Zhang, 2005) to allow for
multiple language models, including non-word
models such as part-of-speech improved search
algorithm, in order to improve its speed and effi-
ciency.
</bodyText>
<sectionHeader confidence="0.677058" genericHeader="method">
3.1.4 VUI+GUI System
</sectionHeader>
<bodyText confidence="0.999979307692308">
S-MINDS has a flexible user interface that can
be configured to use VUI only or VUI+GUI for
either the English speaker or the second-
language speaker. Also, the English speaker can
experience a different user interface than the sec-
ond-language speaker. The system has the flexi-
bility to use multiple types of microphones, in-
cluding open microphones, headsets, and tele-
phone headsets. Speech recognition can be con-
firmed by VUI, GUI, or both, and it can be con-
figured to verify all utterances, no utterances, or
just utterances that fall below a certain confi-
dence level.
</bodyText>
<subsectionHeader confidence="0.796628">
3.1.5 Synthesis Engine
</subsectionHeader>
<bodyText confidence="0.999854636363636">
S-MINDS can use text-to-speech (TTS) synthesis
throughout the system; alternatively, it can use
TTS in its SMT-based system and chunk-based
recordings that are spliced together in its inter-
pretation engine. Fluential licenses its TTS tech-
nology from Cepstral, and other vendors. In
general we do not expect to be doing any re-
search and development activities in this area, as
Cepstral can easily create good synthesis models
from the 10 hours of provided speech data
(Schultz and Black, 2006, Peterson, 2007).
</bodyText>
<sectionHeader confidence="0.993697" genericHeader="method">
4 System Building
</sectionHeader>
<bodyText confidence="0.968499333333333">
Fluential conducted fiver activities in order to
build the system. They included: (1) Defining
the task, (2) Collecting speech data to model
nurse-patient interactions, (3) Building and test-
ing a speech translation system in English and
Spanish, (4) Using the system with patients and
nurses and collecting data to measure system
performance, and (5) Analyzing the results.
To define the task, Fluential conducted a two-
hour focus group with six registered nurses from
Med/Surg unit of Kaiser Medical Center in San
Francisco. In this focus group, the nurses identi-
fied six nurse-patient encounter types that they
wanted to use for the evaluation. These were:
(1) Greeting/Goodbye, (2) Vital Signs, (3) Pain
Assessment, (4) Respiratory Assessment, (5)
Blood Sugar, (6) Placement of an I.V.
Fluential then collected speech data over a four-
week period by recording nurse-patient interac-
tions involving 11 nurses and 25 patients. Fluen-
tial also recruited 59 native Spanish speakers
who provided speech data using an automated
system that walked them through hypothetical
scenarios and elicited their responses in Spanish.
The English recognizer had a vocabulary of
2,003 and it was trained with 9,683 utterances.
The Spanish recognizer had a vocabulary of 822,
and it was trained with 1,556 utterances. We
suspect that the vocabulary size in Spanish would
have been much bigger if we had more data.
</bodyText>
<sectionHeader confidence="0.989266" genericHeader="method">
5 System Evaluation
</sectionHeader>
<bodyText confidence="0.999943684210526">
After building and testing the speech translation
system, Fluential conducted a two-hour training
session for each of the nurses before using the
system with patients. A bilingual research assis-
tant explained the study to patients, obtained
their consent, and trained them for less than five
minutes on the system. Nurses then used the sys-
tem with Spanish-speaking patients for the six
nurse-patient encounters that were built into the
system. The system was used by three nurses
with eleven patients for a total of 95 nurse-
patient encounters creating a total of 500 conver-
sation segments.3
To protect patients from a mistranslation, each
encounter was remotely monitored by a bilingual
interpreter, who immediately notified the nurse
any time the system mistranslated. Each encoun-
ter was recorded, transcribed, and translated by a
human.
</bodyText>
<subsectionHeader confidence="0.996346">
3.1 Scoring Accuracy
</subsectionHeader>
<sectionHeader confidence="0.46113" genericHeader="method">
3 A conversation segment is a single con-
</sectionHeader>
<bodyText confidence="0.481715">
tinuous sequence of speech in a single language
plus the translation of what was said.
</bodyText>
<page confidence="0.993296">
56
</page>
<bodyText confidence="0.937191909090909">
The human translations were compared to the
system’s translations and given a score using the
Laws Methodology of either Good, Fair, Poor,
Mistranslated, or Not Translated. (Laws, 2004).
If a translation were scored as Good or Fair, it
was considered accurate. If the translation were
scored as Poor, Mistranslated, or Not Translated,
it was considered inaccurate.
Table 2 and 3 give examples of how we have
used Law’s method to grade actual interaction
results from nurses and patients.
</bodyText>
<table confidence="0.991246880952381">
Table 2: Nurse Scoring Examples
What S-MINDSTM Human S-MINDS
Nurse Said Translation Translation Accuracy
I will give Voy a colo- Voy a colo- Good
you an I.V. carle un carle un
cateter para cateter de
liquidos liquidos
intraveno- intravenosos.
sos.
Let me Dejeme Permitame Fair
check if I chequear si reviso si
can give puedo darle puedo darle
you medi- algun medi- algun medi-
cation for camento. camento para
that. eso.
I will Yo voy a Voy a revisar Poor
check revisarle los su ...
your... vendajes
Did some- LLe tomare ¿Alguien Mis-
one take sus signos tomo sus translated
your vi- vitals? signos vi-
tals? tals?
Your heart --- Su frecuen- Not
rate is cia cardiaca Trans-
normal. es normal. lated
Table 3: Patient Scoring Examples
What S-MINDS Human S-MINDS
Patient Translation Translation Accuracy
Said
No, no I don&apos;t have No, I don&apos;t Good
tengo difficulty have diffi-
dificultad breathing. culty breath-
en respi- ing.
rar.
En la The lower In the lower Fair
parte baja part of my part of my
del esto- stomach. stomach.
mago.
N/A N/A N/A Poor
N/A N/A N/A Mistranslated
Los --- My bones. Not Trans-
huesos. lated
</table>
<sectionHeader confidence="0.999749" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.985444142857143">
Our internal milestones for Phase I was to
achieve 80% accuracy using the Laws Method-
ology. Out of 500 conversation segments, the
speech translation system had an overall accu-
racy rate of 93% combining both nurse- and pa-
tient-conversation segments,
1.00% 0.60% 4.20% 2.40%
</bodyText>
<subsectionHeader confidence="0.469828">
Good Fair Poor Mistranslated Not Translated
</subsectionHeader>
<figureCaption confidence="0.924432">
Figure 2: Total results for both nurses and
patients.
</figureCaption>
<subsectionHeader confidence="0.996476">
6.1 Nurse Translation Results
</subsectionHeader>
<bodyText confidence="0.9995304">
Looking at just nurse conversation segments, the
speech translation system had higher accuracy
than for patient segments. Out of 404 nurse seg-
ments, the speech translation system had a 94%
accuracy rate.
</bodyText>
<figure confidence="0.993637083333333">
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Good Fair Poor Mistranslated Not Translated
</figure>
<figureCaption confidence="0.857407">
Figure 3: Accuracy for Nurse Conversational
Segments
</figureCaption>
<bodyText confidence="0.999992538461538">
The biggest problem with system performance
with nurses was with mistranslations. When
nurses tried to say things that were not in the sys-
tem, the system tried to map their utterances to
something that was in the system. In each case of
mistranslation, the system told the nurse what it
was about to translate, gave the nurse a chance to
stop the translation, and then translated the
wrong thing when the nurse did not respond. We
believe that system performance can be greatly
improved in by collecting more speech data from
patients and nurses, making changes to the user
interface, and improving our training program.
</bodyText>
<figure confidence="0.999752583333333">
100%
91.80%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
</figure>
<page confidence="0.97782">
57
</page>
<subsectionHeader confidence="0.994106">
6.2 Patient Translation Results
</subsectionHeader>
<bodyText confidence="0.9929466">
Looking at just patient conversation segments,
the speech translation system had lower overall
accuracy than for nurse segments. Out of 96 pa-
tient segments, the speech translation system had
a 90% accuracy rate.
</bodyText>
<figure confidence="0.997240833333333">
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
Good Fair Poor Mistranslated Not Translated
</figure>
<figureCaption confidence="0.999889">
Figure 4: Results for Patients
</figureCaption>
<bodyText confidence="0.9999756">
All of the problems with system performance
with patients were with responses that the system
was not able to translate. The system never gave
a Poor translation or Mistranslated. So there were
times when the nurse knew that the patient tried
to say something that the system could not trans-
late, but there was never a time when the system
gave the nurse false information. However, this
percentage is quite high, and in a large context, it
might cause additional problems.
</bodyText>
<subsectionHeader confidence="0.994244">
6.3 Nurse Survey Results
</subsectionHeader>
<bodyText confidence="0.8672475625">
After each time using the system, the nurses
completed a user satisfaction survey that had five
statements and asked them assign a 1-to-5 Likert
score to each statement with 1 meaning
“Strongly Disagree” and 5 meaning “Strongly
Agree.” Average scores for each question were:
4.7 The speech translator was easy
to use.
4.5 The English voice was fluent
and easy to understand.
4.4 I understood the patient better
because of the speech translator.
4.5 I feel that I am providing better
medical care because of the speech translator.
4.7 I would like to use the speech
translator with my patients in the future.
</bodyText>
<subsectionHeader confidence="0.990058">
6.4 Patient Survey Results
</subsectionHeader>
<bodyText confidence="0.6645655">
The patients also completed a similar user satis-
faction survey, translated to Spanish, after using
the system. Their average scores for each ques-
tion were:
4.6 The speech translator was easy
to use.
4.8 The Spanish voice was fluent
and easy to understand.
4.7 I understood my nurse better be-
cause of the speech translator.
5.0 I feel that I am receiving better
medical care because of the speech translator.
4.9 I would like to use the speech
translator with my nurse in the future.
</bodyText>
<subsectionHeader confidence="0.886359">
6.5 ANOVA Testing
</subsectionHeader>
<bodyText confidence="0.9997664">
We conducted Analysis of Variance (ANOVA)
testing to evaluate whether there were any sig-
nificant variations in translation accuracy by pa-
tient, nurse, or encounter type. There were no
significant differences.
</bodyText>
<sectionHeader confidence="0.999763" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999949875">
We were able to build and evaluate a system in 3
months and show its utility by nurses and pa-
tients in clinical setting. The system seemed to
work and was liked by both nurses and patients.
The next question is whether such a system can
scale and cover a much larger domain, and how
much data and training is required to accomplish
this.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999701055555556">
Chen A., et al. (2007), Addressing Language and
Culture—A Practice Assessment for Health
Care Professionals, p3.
Flores Glenn, (2005), “The Impact of Medical
Interpreter Services on the Quality of Health
Care: A Systematic Review,” Medical Care
Research and Review, Vol. 62, No. 3, pp. 255-
29
Laws, MB, Rachel Heckscher, Sandra Mayo,
Wenjun. Li, Ira Wilson, (2004), “A New
Method for Evaluating the Quality of Medical
Interpretation,” Medical Care. 42(1):71-80,
January 2004
Peterson Kay (2007). Senior Linguist, Cepstral
LLC, Personal Communication.
Schultz, Tanja and A. W Black (2006),
“Challenges with Rapid Adaptation of Speech
Translation Systems to New Language Pairs”
</reference>
<page confidence="0.983435">
58
</page>
<reference confidence="0.9997328125">
Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal
Processing (ICASSP-2006), Toulouse, France,
May 15-19, 2006.
Tillmann, Christoph and T. Zhang, (2005), “A
localized prediction model for statistical
machine translation,” in Proceedings of the
43rd Annual Meeting of the ACL, pp. 557-564,
Ann Arbor, June 2005.
Zens, Richard, and H. Ney, (2003), “A compara-
tive study of reordering constraints in statisti-
cal machine translation,” in Proceedings of the
41st Annual Meetings of the ACL, pp. 144-
151, Sapporo, Japan, July 2003Association for
Computing Machinery. 1983. Computing Re-
views, 24(11):503-512.
</reference>
<page confidence="0.999261">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.266925">
<title confidence="0.998408">Speech to Speech translation for Nurse Patient Interaction</title>
<author confidence="0.992798">Karen Sudre</author>
<address confidence="0.865771">TeleNav, Inc./1130 Kifer Sunnyvale CA, 94086</address>
<email confidence="0.999389">karens@telenav.com</email>
<author confidence="0.988772">Farzad Ehsani</author>
<author confidence="0.988772">Jim Kimzey</author>
<affiliation confidence="0.682814">Zuber, Demitrios Fluential Inc./ 1153 Bordeaux</affiliation>
<address confidence="0.999707">Sunnyvale, CA 94089</address>
<email confidence="0.939705">farzad@fluentialinc.com</email>
<email confidence="0.939705">jkimzey@fluentialinc.com</email>
<email confidence="0.939705">elaine@fluentialinc.com</email>
<email confidence="0.939705">dlm@fluentialinc.com</email>
<abstract confidence="0.999637933333333">S-MINDS is a speech translation system, which allows an English speaker to communicate with a limited English proficiency speaker easily within a question-and-answer, interview-style format. It can handle dialogs in specific settings such as nurse-patient interaction, or medical triage. We have built and tested an English-Spanish system for enabling nurse-patient interaction in a number of domains in Kaiser Permanente achieving a total translation accuracy of 92.8% (for both English and Spanish). We will give an overview of the system as well as the quantitative and qualitatively system performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Chen</author>
</authors>
<title>Addressing Language and Culture—A Practice Assessment for Health Care Professionals,</title>
<date>2007</date>
<pages>3</pages>
<marker>Chen, 2007</marker>
<rawString>Chen A., et al. (2007), Addressing Language and Culture—A Practice Assessment for Health Care Professionals, p3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Flores Glenn</author>
</authors>
<title>The Impact of Medical Interpreter Services on the Quality of Health Care:</title>
<date>2005</date>
<journal>A Systematic Review,” Medical Care Research and Review,</journal>
<volume>62</volume>
<pages>255--29</pages>
<marker>Glenn, 2005</marker>
<rawString>Flores Glenn, (2005), “The Impact of Medical Interpreter Services on the Quality of Health Care: A Systematic Review,” Medical Care Research and Review, Vol. 62, No. 3, pp. 255-29</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ira Wilson Li</author>
</authors>
<date>2004</date>
<journal>A New Method for Evaluating the Quality of Medical Interpretation,” Medical Care.</journal>
<pages>42--1</pages>
<marker>Li, 2004</marker>
<rawString>Laws, MB, Rachel Heckscher, Sandra Mayo, Wenjun. Li, Ira Wilson, (2004), “A New Method for Evaluating the Quality of Medical Interpretation,” Medical Care. 42(1):71-80, January 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peterson Kay</author>
</authors>
<date>2007</date>
<institution>Senior Linguist, Cepstral LLC, Personal Communication.</institution>
<marker>Kay, 2007</marker>
<rawString>Peterson Kay (2007). Senior Linguist, Cepstral LLC, Personal Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanja Schultz</author>
<author>A W Black</author>
</authors>
<title>Challenges with Rapid Adaptation of Speech Translation Systems to New Language Pairs”</title>
<date>2006</date>
<booktitle>Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2006),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="9324" citStr="Schultz and Black, 2006" startWordPosition="1436" endWordPosition="1439">ured to verify all utterances, no utterances, or just utterances that fall below a certain confidence level. 3.1.5 Synthesis Engine S-MINDS can use text-to-speech (TTS) synthesis throughout the system; alternatively, it can use TTS in its SMT-based system and chunk-based recordings that are spliced together in its interpretation engine. Fluential licenses its TTS technology from Cepstral, and other vendors. In general we do not expect to be doing any research and development activities in this area, as Cepstral can easily create good synthesis models from the 10 hours of provided speech data (Schultz and Black, 2006, Peterson, 2007). 4 System Building Fluential conducted fiver activities in order to build the system. They included: (1) Defining the task, (2) Collecting speech data to model nurse-patient interactions, (3) Building and testing a speech translation system in English and Spanish, (4) Using the system with patients and nurses and collecting data to measure system performance, and (5) Analyzing the results. To define the task, Fluential conducted a twohour focus group with six registered nurses from Med/Surg unit of Kaiser Medical Center in San Francisco. In this focus group, the nurses identi</context>
</contexts>
<marker>Schultz, Black, 2006</marker>
<rawString>Schultz, Tanja and A. W Black (2006), “Challenges with Rapid Adaptation of Speech Translation Systems to New Language Pairs” Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP-2006), Toulouse, France, May 15-19, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A localized prediction model for statistical machine translation,”</title>
<date>2005</date>
<booktitle>in Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>557--564</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="8057" citStr="Tillmann and Zhang, 2005" startWordPosition="1231" endWordPosition="1234">ystem to learn both types of information simultaneously. However, if the acquisition of structural information is kept separate from the acquisition of vocabulary, the resulting system should learn both levels more efficiently. And by modifying the existing corpus to separate structure and vocabulary, we have been able to take full advantage of all the information in the bilingual corpus, producing higherquality MT without requiring large bodies of training data. The most recent modification to this approach was the use of distance-based ordering (Zens and Ney, 2003) and lexicalized ordering (Tillmann and Zhang, 2005) to allow for multiple language models, including non-word models such as part-of-speech improved search algorithm, in order to improve its speed and efficiency. 3.1.4 VUI+GUI System S-MINDS has a flexible user interface that can be configured to use VUI only or VUI+GUI for either the English speaker or the secondlanguage speaker. Also, the English speaker can experience a different user interface than the second-language speaker. The system has the flexibility to use multiple types of microphones, including open microphones, headsets, and telephone headsets. Speech recognition can be confirme</context>
</contexts>
<marker>Tillmann, Zhang, 2005</marker>
<rawString>Tillmann, Christoph and T. Zhang, (2005), “A localized prediction model for statistical machine translation,” in Proceedings of the 43rd Annual Meeting of the ACL, pp. 557-564, Ann Arbor, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>H Ney</author>
</authors>
<title>A comparative study of reordering constraints in statistical machine translation,”</title>
<date>2003</date>
<journal>Computing Reviews,</journal>
<booktitle>in Proceedings of the 41st Annual Meetings of the ACL,</booktitle>
<pages>144--151</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="8005" citStr="Zens and Ney, 2003" startWordPosition="1223" endWordPosition="1226">Traditional statistical approaches force the system to learn both types of information simultaneously. However, if the acquisition of structural information is kept separate from the acquisition of vocabulary, the resulting system should learn both levels more efficiently. And by modifying the existing corpus to separate structure and vocabulary, we have been able to take full advantage of all the information in the bilingual corpus, producing higherquality MT without requiring large bodies of training data. The most recent modification to this approach was the use of distance-based ordering (Zens and Ney, 2003) and lexicalized ordering (Tillmann and Zhang, 2005) to allow for multiple language models, including non-word models such as part-of-speech improved search algorithm, in order to improve its speed and efficiency. 3.1.4 VUI+GUI System S-MINDS has a flexible user interface that can be configured to use VUI only or VUI+GUI for either the English speaker or the secondlanguage speaker. Also, the English speaker can experience a different user interface than the second-language speaker. The system has the flexibility to use multiple types of microphones, including open microphones, headsets, and te</context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Zens, Richard, and H. Ney, (2003), “A comparative study of reordering constraints in statistical machine translation,” in Proceedings of the 41st Annual Meetings of the ACL, pp. 144-151, Sapporo, Japan, July 2003Association for Computing Machinery. 1983. Computing Reviews, 24(11):503-512.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>