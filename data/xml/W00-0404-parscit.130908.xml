<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9989605">
Extracting Key Paragraph based on Topic and Event Detection
Towards Multi-Document Summarization
</title>
<author confidence="0.982142">
Fumiyo Fukumoto and Yoshimi Suzukit
</author>
<affiliation confidence="0.998047">
Department of Computer Science and Media Engineering,.
Yamanashi University
</affiliation>
<address confidence="0.956807">
4-3-11 Takeda, Kofu 400-8511 Japan
</address>
<email confidence="0.578932">
IfakumotoAskye.esk ysuzuki Oalps1.641.yamanashi.ac.jp
</email>
<sectionHeader confidence="0.971186" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967111111111">
This paper proposes a method for extracting key
paragraph for multi-document summarization based
on distinction between a topic and an event. A topic
and an event are identified using a simple criterion
called domain dependency of words. The methog
was tested on the TDT1 corpus which has been de-
veloped by the TDT Pilot Study and the result can
be regarded as promising the idea of domain depen-
dency of words effectively employed.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957428571429">
As the volume of online documents has drastically
increased, summarization techniques have become
very important in IR and NLP studies. Most of the
summarization work has focused on a single docu-
ment. This paper focuses on multi-document sum-
marization: broadcast news documents about the
same topic. One of the major problems in the multi-
document summarization task is how to identify dif-
ferences and similarities across documents. This can
be interpreted as a question of how to make a clear
distinction between an event and a topic in docu,.
ments. Here, an event is the subject of a document
itself, i.e. a writer wants to express, in other words,
notions of who, what, where, when, why and how in
a document. On the other hand, a topic in this paper
is some unique thing that happens at some specific
time and place, and the unavoidable consequences.
It&apos; becomes background among documents. For ex-
ample, in the documents of &apos;Kobe Japan quake&apos;, the
event includes early reports of damage, location and
nature of quake, rescue efforts, consequences of the
quake, and on-site reports, while the topic is Kobe
Japan quake. The well-known past experience from
IR that notions of who, what, where, when, why
and how may not make a great contribution to the
topic detection and tracking task (Allan and Papka,
1998) causes this fact, i.e. a topic and an event are
different from each other&apos; .
</bodyText>
<footnote confidence="0.99096675">
1 Some topic words can also be an event. For instance,
in the document shown in Figure 1, &apos;Japan&apos; and &apos;quake&apos; are
topic words and also event words in the document. However,
we regarded these words as a topic, i.e. not be an event..
</footnote>
<bodyText confidence="0.999232538461538">
In this paper, we propose a. method for extract-
ing key paragraph for nmlti-document summariza-
tion based on distinction between a topic and an
event. We use a simple criterion called domain de-
pendency of words as a solution and present how the
idea of domain dependency of words can be utilized
effectively to identify a topic and an event, and thus
allow multi-document summarization.
The basic idea of our approach is that whether a
word appeared in a document is a topic (an event)
or not, depends on the domain to which the docu-
ment belongs. Let us take a look at the following
document from the TDT1 corpus.
</bodyText>
<listItem confidence="0.979229583333333">
(1-2) Two Americans known dead in Japan quake
1. The number of [Americans] known to have been
killed in Tuesday&apos;s earthquake in Japan has risen to
two, the [State] [Department] said Thursday.
2. The first was named Wednesday as Voni Lynn
Wong, a teacher from California. [State] [De-
partment] spokswoman Christine Shelly declined
to name the second, saying formalities of notifying
the family had not been completed.
3. With the death toll still mounting, at least 4,000
people were killed in the earthquake which devas-
tated the Japanese city of Kobe.
4. [U.S.] diplomats were trying to locate the several
thousand-strong [U.S.] community in the area, and
some [Americans] who had been made homeless
were found shelter in the [U.S.] consulate there,
which was only lightly damaged in the quake.
5. Shelly said an emergency [State] [Department]
telephone number in Washington to provide infor-
mation about private [American] citizens in Japan
had received over 6,000 calls, more than half orfhn
seeking direct assistance.
6. The Pentagon has agreed to send 57,000 blankets
to Japan and [U.S.] ambassador to Tokyo Walter
</listItem>
<bodyText confidence="0.966682333333333">
Mondale has donated a $25,000 discretionary fund
for emergencies to the Japanese Red Cross, Shelly
said.
7. Japan has also agreed to a visit by a team of [U.S.]
experts headed by Richard Witt, national director
of the Federal Emergency Management Agency.
</bodyText>
<figureCaption confidence="0.97520525">
Figure 1: The document titled &apos;Two Americans
known dead in Japan quake&apos;
Figure 1 is the document whose topic is &apos;Kobe Japan
quake&apos;, and the subject of the document (event
</figureCaption>
<page confidence="0.99969">
31
</page>
<bodyText confidence="0.999567588235294">
words) is &apos;Two Americans known dead in Japan
quake&apos;. Underlined words denote a topic, and the
words marked with &apos;[ ]&apos; are events. of Figure
1 is paragraph id. Like Lulm&apos;s technique of keyword
extraction, our method assumes that an event asso-
ciated with a document appears throughout para-
graphs (Luhn, 1958), but a topic does not: This is
because an event is the subject of a document. itself,
while a topic is an event, along with all directly re-
lated events. In Figure 1, event words &apos;Americans&apos;
and &apos;U.S.&apos;, for instance, appears across paragraphs,
while a topic word, for example, &apos;Kobe&apos; appears only
the third paragraph. Let us consider further a broad
coverage domain which consists of a small number of
sample news documents about the same topic, &apos;Kobe
Japan quake&apos;. Figure 2 and 3 are documents with
&apos;Kobe Japan quake&apos;.
</bodyText>
<listItem confidence="0.980003444444444">
(1-1) Quake collapses buildings in central Japan
1. At least two people died and dozens were injured
when a powerful earthquake rolled through central
Japan Tuesday morning, collapsing buildings and
setting off fires in the cities of Kobe and Osaka.
2. The Japan Meteorological Agency said the
earthquake, which measured 7.2 on the open-ended
Richter sc.ale, rumbled across Honshu Island from
the Pacific Ocean to the Japan Sea.
</listItem>
<figureCaption confidence="0.952083">
Figure 2: The document titled &apos;Quake collapses
buildings in central Japan&apos;
</figureCaption>
<listItem confidence="0.809266875">
(1-3) Kobe quake leaves questions about medical system
1. The earthquake that devastated Kobe in January
raised serious questions about the efficiency of
Japan&apos;s emergency medical system, a government
report released on Tuesday said.
2. &apos;The earthquake exposed many issues in terms
of quantity, quality, promptness and efficiency of
Japan&apos;s medical care in time of disaster,&apos; the report
</listItem>
<bodyText confidence="0.873649">
onThalth and welfare said.
.....................
</bodyText>
<figureCaption confidence="0.992399">
Figure 3: The document titled &apos;Kobe quake leaves
questions about medical system&apos;
</figureCaption>
<bodyText confidence="0.9997013">
Underlined words in Figure 2 and 3 show the topic
of these documents. In these two documents, &apos;Kobe&apos;
which is a topic appears in every document, while
&apos;Americans&apos; and &apos;U.S.&apos; which are events of the docu-
ment shown in Figure 1, does not appear. Our tech-
nique for making the distinction between a topic and
an event explicitly exploits this feature of the domain
dependency of words: how strongly a word features
a given set of data.
The rest of the paper is organized as follows.
The next section provides domain dependency of
words which is used to identify a topic and an event
for broadcast news documents. We then present a
method for extracting topic and event words, and de-
scribe a paragraph-based summarization algorithm
using the result of topic and event extraction. Fi-
nally, we report some experiments using the TDT1
corpus which has been developed by the TDT (Topic
Detection and Tracking) Pilot Study (Allan and
Carbonell, 1998) with a discussion of evaluation.
</bodyText>
<sectionHeader confidence="0.907759" genericHeader="method">
2 Domain Dependency of Words
</sectionHeader>
<bodyText confidence="0.99986652">
The domain dependency of words that how strongly
a word features a given set of data (documents) con-
tributes to event extraction, as we previously re-
ported (Fukumoto et al., 1997). In the study, we
hypothesised that the articles from the Wall Street
Journal corpus can be structured by three levels, i.e.
Domain, Article and Paragraph. If a word is an event
in a given article, it satisfies the two conditions: (1)
The dispersion value of the word in the Paragraph
level is smaller than that of the Article, since the
word appears throughout paragraphs in the Para-
graph level rather than articles in the Article level.
(2) The dispersion value of the word in the Arti-
cle is smaller than that of the Domain, as the word
appears across articles rather than domains.
However, there are two problems to adapt it to
multi-document summarization task. The .first is
that the method extracts only events in the docu-
ment. Because the goal of the study is to summarize
a single document, and thus there is no answer to
the question of how to identify differences and sim-
ilarities across documents. The second is that the
performance of the method greatly depends on the
structure of a given data itself. Like the Wall Street
Journal corpus, (i) if a given data can be structured
by three levels, Paragraph, Article and Domain, each
of which consists of several paragraphs, articles and
domains, respectively, and (ii) if Domain consists of
different subject domains, such as &apos;aerospace&apos;, &apos;en-
vironment&apos; and &apos;stock market&apos;, the method can be
done with satisfactory accuracy. However, there is
no guarantee to make such an appropriate structure
from a given set of documents in the multi-document
summarization task. •
The purpose of this paper is to define domain
dependency of words for a number of sample doc-
uments about the same topic, and thus for multi-
document summarization task. Figure 4 illustrates
the structure of broadcast news documents which
have been developed by the TDT (Topic Detection
and Tracking) Pilot Study (Allan and Carbonell,
1998). It consists of two levels, Paragraph and Doc-
ument. In Document level, there is a small number
of sample news documents about the same topic.
These documents are arranged in chronological or-
der such as, &apos;(1-1) Quake collapses buildings in cen-
tral Japan (Figure 2)&apos;, &apos;(1-2) Two Americans known
dead in Japan quake (Figure 1)&apos; and &apos;(1-3) Kobe
quake leaves questions about medical system (Fig-
ure 3)&apos;. A particular document consists of several
</bodyText>
<page confidence="0.998609">
32
</page>
<bodyText confidence="0.999374">
paragraphs. We call it Paragraph level. Let words
within a document be an event, a topic, or among
others (We call it a general word).
</bodyText>
<figure confidence="0.959103125">
(1.1)
- • -
Ox 0
6 • ;
• 6 0
0
0
•
</figure>
<figureCaption confidence="0.972329">
Figure 4: The structure of broadcast news documents
(event extraction)
</figureCaption>
<bodyText confidence="0.999944166666667">
Given the structure shown in Figure 4, how can we
identify every word in document (1-2) with an event,
a topic or a general word? Our method assumes that
an event associated with a document appears across
paragraphs, but a topic word does not. Then, we use
domain dependency of words to extract event and
topic words in document (1-2). Domain dependency
of words is a measure showing how greatly each word
features a given set of data.
In Figure 4, let `(:)&apos;, &apos;A&apos; and &apos; x denote a topic,
, an event and a general word in document (1-2), re-
spectively. We recall the example shown in Figure 1.
</bodyText>
<listItem confidence="0.929024">
• &apos;A&apos;, for instance, &apos;U.S.&apos; appears across paragraphs.
</listItem>
<bodyText confidence="0.874123294117647">
&apos; However, in the Document level, &apos;A&apos; frequently ap-
pears in document, (1-2) itself. On the basis of this
example, we hypothesize that if word i is an event,
it &apos;satisfies the following condition:
[1] Word i greatly depends on a particular
document in the Document level rather
than a particular paragraph in the Para-
graph.
Next, we turn to identify the remains (words) with
a topic, or a general word. In Figure 5, a topic of
documents (1-1) &amp;quot;, (1-3), for instance, &apos;Kobe&apos; ap-
pears in a particular paragraph in each level of Para-
graph&apos;, Paragraph2 and Paragraph3. Here, (1-1), (1-
2) and (1-3) corresponds to Paragraphl, Paragraph2
and Paragraph3, respectively. On the other hand,
in Document level, a topic frequently appears across
documents. Then, we hypothesize that if word i is a
</bodyText>
<table confidence="0.991265333333333">
(1-1) (1-2) (1-3) jorn
O o
Document level i=2
C. C.
h1
C x
° o x
_
Paragraph I C.
level C 0.
j=2
° x
Paragraph 2
level
•
O. topic word
x: general word
; 1 x
x 0 .
Paragraph3
; level 1!
</table>
<figureCaption confidence="0.98884">
Figure 5: The structure of broadcast news documents
(topic extraction)
</figureCaption>
<bodyText confidence="0.9748742">
topic, it satisfies the following condition:
[2] Word i greatly depends on a particu-.
lar paragraph in each Paragraph level
rather than a particular document in
Document.
</bodyText>
<sectionHeader confidence="0.995405" genericHeader="method">
3 Topic and Event Extraction
</sectionHeader>
<bodyText confidence="0.9998505">
We hypothesized that the domain dependency of
words is a key clue to make a distinction between
a topic and an event. This can be broken down into
two observations: (1) whether a word appears across
paragraphs (documents), (ii) whether or not a word
appears frequently. We represented the former by
using dispersion value, and the latter by deviation
value. Topic and event words are extracted by using
these values.
The first step to extract topic and event words is
to assign weight to the individual word in a docu-
ment. We applied TF*IDF to each level of the Doc-
ument and Paragraph, i.e. Paragraph&apos;, Paragraph2
and Paragraph3.
</bodyText>
<equation confidence="0.977976">
wdzi = TFclii* log Ndt (1)
</equation>
<bodyText confidence="0.999805125">
Wdit in formula (1) is TF*IDF of term tin the i-th
document. In a similar way, Wpit denotes TF*IDF
of the term t in the i-th paragraph. TFclit in (1)
denotes term frequency of tin the i-th document. N
is the number of documents and Ndt is the number
of documents where t occurs. The second step is to
calculate domain dependency of words. We defined
it by using formula (2) and (3).
</bodyText>
<figure confidence="0.8948899375">
(1-2) (1.3)
, •
• Document level .
0. topic word (1.1) &apos;Maim collapses buildmgs in central Japan&apos;
6: event word (1-2) Two Americans known dead in Japan quake&apos;
X: general word (1-3) &apos;Kobe quake leaves questions about medical system&apos;
CO
Paragraph level ; a
, X A ;
P2
33
Eal (Wait — mean1)2
171
(W d1 — meant)
*10 + 50
DispDt
</figure>
<bodyText confidence="0.979381">
Formula (2) is dispersion value of term t in the level
of Document which consists of m documents, and
denotes how frequently t appears across documents.
hi a similar way, DispPt denotes dispersion of term
I in the level of Paragraph. Formula (3) is the devia-
tion value of t in the i-th document and denotes how
frequently it appears in a particular document, the
i-th document. Detipit is deviation of term t in the
i-th paragraph. In (2) and (3), meant is the mean
of the total TF*IDF values of term t in the level of
Document.
The last step is to extract a topic and an event
using formula (2) and (3). We recall that if t is an
event, it satisfies [1] described in section 2. This is
shown by using formula (4) and (5).
</bodyText>
<equation confidence="0.7915025">
DispPt &lt; DispDt (4)
for all pi E di Devpit &lt; Devclit (5)
</equation>
<bodyText confidence="0.99927">
Formula (4) shows that. t frequently appears across
paragraphs rather than documents. In formula (5),
di is the i-th document and consists of the number
of n paragraphs (see Figure 4). pi is an element of
di. (5) shows that t frequently appears in the i-th
document di rather than paragraphs pi ( 1 &lt; j &lt;
n). On the other hand, if t satisfies formula (6) and
(7), then propose t as a topic.
</bodyText>
<subsubsectionHeader confidence="0.592154">
DispPt &gt; DispDt (6)
</subsubsectionHeader>
<bodyText confidence="0.848112833333333">
for all di E D.
pit exists such that Devpit &gt; Devdit (7)
In formula (7), D consists of the number of rn doc-
uments (see Figure 5). (7) denotes that t frequently
appears in the particular paragraph pi rather than
the document di which includes pi.
</bodyText>
<sectionHeader confidence="0.991663" genericHeader="method">
4 Key Paragraph Extraction
</sectionHeader>
<bodyText confidence="0.9999166">
The summarization task in this paper is paragraph-
based extraction (Stein et al., 1999). Basically, para-
graphs which include not only event words but also
topic words are considered to be significant .para-
graphs. The basic algorithm works as follows:
</bodyText>
<listItem confidence="0.9962146">
1. For each document, extract topic and event
words.
2. Determine the paragraph weights for all para-
graphs in the documents:
(a) Compute the sum of topic weights over the
total number of topic words for each para-
graph.
(b) Compute the sum of event weights over the
total number of event words for each path-
graph.
</listItem>
<bodyText confidence="0.8597215">
A topic and an event weights are calculated
by using Detidit in formula (3). Here, t is a
topic or an event and i is the i-th document
in the documents.
</bodyText>
<listItem confidence="0.9978355">
(c) Compute the sum of (a) and (h) for each
paragraph.
3. Sort the paragraphs according to their weights
and extract the N highest weighted paragraphs
in documents in order to yield summarization
of the documents.
4. When their weights are the same, Compute the
sum of all the topic and event word weights.
Select a paragraph whose weight is higher than
the others.
</listItem>
<sectionHeader confidence="0.997131" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999867571428571">
Evaluation of extracting key paragraph based on
multi-document is difficult. First, we have not found
an existing collection of summaries of multiple doc-
uments. Second, the manual effort needed to judge
system output is far more extensive than for single
document summarization. Consequently, we focused
on the TDT1 corpus. This is because (i) events have
been defined to support the TDT study effort, (ii)
it was completely annotated with respect to these
events (Allan and Carbonell, 1997). Therefore, we
do not need the manual effort to collect documents
which discuss about the target event.
We report the results of three experiments. The
first experiment, Event Extraction, is concerned with
event extraction technique. In the second experi-
ment, Tracking Task, we applied the extracted top-
ics to tracking task (Allan and Carbonell, 1998).
The third experiment, Key Paragraph Extraction is
conducted to evaluate how the extracted topic and
event words can be used effectively to extract key
paragraph.
</bodyText>
<subsectionHeader confidence="0.769241">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.993813142857143">
The TDT1 corpus comprises a set of documents
(15,863) that includes both newswire (Reuters)
7,965 and a manual transcription of the broadcast
news speech (CNN) 7,898 documents. A set of 25
target events were defined 2 .
All documents were tagged by the tagger (Brill,
1992). We used nouns in the documents.
</bodyText>
<page confidence="0.506445">
2 littp://morphidc.upenn.edu/TDT
</page>
<figure confidence="0.8282755">
• DispDt =
Devdit
</figure>
<page confidence="0.96151">
34
</page>
<subsectionHeader confidence="0.964174">
5.2 Event Extraction
</subsectionHeader>
<bodyText confidence="0.999987653846154">
We collected 300 documents from the TDT1 corpus,
each of which is annotated with respect to one of 25
events. The result is shown in Table 1.
In Table 1, &apos;Event type&apos; illustrates the target events
defined by the TDT Pilot Study. &apos;Doc&apos; denotes the
number of documents. &apos;Rec&apos; (Recall) is the num-
ber of correct events divided by the total number
of events which are selected by a human, and &apos;Prec&apos;
(Precision) stands for the number of correct -events
divided by the number of events which are selected
by our method. The denominator &apos;Rec&apos; is made by
a human judge. &apos;Accuracy&apos; in Table 1 is the total
average ratio.
In Table 1, recall and precision values range from
55.0/47.0 to 83.3/84.2, the average being 71.0/72.2.
The worst result of recall and precision was when
event type was &apos;Serbs violate Bihac&apos; (55.0/59.3). We
currently hypothesize that this drop of accuracy is ,
due to the fact that some documents are against our
assumption of an event. Examining the documents
whose event type is &apos;Serbs violate Bihac&apos;, 3 ( one
from CNN and two from Reuters) .out of 16 docu-
ments has discussed the same event, i.e. &apos;Bosnian
Muslim enclave hit by heavy shelling&apos;. As a result,
the event appears across these three documents. Fu-
ture research will shed more light on that.
</bodyText>
<subsectionHeader confidence="0.938299">
5.3 Tracking Task
</subsectionHeader>
<bodyText confidence="0.999741083333333">
Tracking task in the TDT project is starting from
a few sample documents and finding all subsequent
documents that discuss the same event (Allan and
Carbonell, 1998), (Carbonell et al., 1999). The cor-
pus is divided into two parts: training set and test
set. Each of the documents is flagged as to whether
it discusses the target event, and these flags (&apos;ITES&apos;,
&apos;NO&apos;) are the only information used for training the
..system to correctly classify the target event. We ap-
plied the extracted topic to the tracking task under
these conditions. The basic algorithm used in the
experiment is as follows:
</bodyText>
<listItem confidence="0.5310425">
1. Create a single document Sfp and represent it as
a term vector
</listItem>
<bodyText confidence="0.994706">
For the results of topic extraction, all the docu-
ments that belong to the same topic are bundled
into a single document Stp and represent it by
a term vector as follows:
</bodyText>
<figure confidence="0.7725266">
[ttpl I
ttp2 f (t,„i) if It&amp;quot; is a topic
St,,= . S.t. ttpj =-* of St p
• 0 otherwise
ttpn
</figure>
<listItem confidence="0.573893333333333">
f(w) denotes term frequency of word w.
2. Represent other training and test documents as
term vectors
</listItem>
<bodyText confidence="0.998985142857143">
Let .52, •, S,„ be all the other training docu-
ments (where 7Th is the number of training doc-
uments which does not belong to the target
event) and Sz be a test document which should
.be classified as to whether or not it discusses the
target event. S1, • S„, and Sx are represented
by term vectors as follows:
</bodyText>
<listItem confidence="0.979935375">
3. Compute the similarity between a training docu-
ment and a test document
Given a vector representation of documents Si
• • St., Stp and Sz, a similarity between two
documents St (1 &lt; i &lt; in, tp) and the test doc-
ument Si would be obtained by using formula
(8), i.e. the inner product of their normalized
vectors.
</listItem>
<equation confidence="0.994611666666667">
Si • Sy
Sim(S,, S.) -= (8)
I si II sx I
</equation>
<bodyText confidence="0.955373666666667">
The greater the value of Sim(S,, Sz) is, the
more similar St and S. are. If the similarity
value between the test document Sz and the
document Stp is largest among all the other
pairs of documents, i.e. (Si, S.), - - (ST„, Sr),
Sz is judged to be a document that discusses
the target event.
We used the standard TDT evaluation measure 3 .
Table 2 illustrates the result.
</bodyText>
<tableCaption confidence="0.996842">
Table 2: The results of tracking task
</tableCaption>
<table confidence="0.999622142857143">
Nt %Miss 7i.F/A Fl %Rec %Prec
1 32.5 0.16 0.68 67.5 70.0
2 23.7 0.06 0.80 76.3 87.8
4 23.1 0.05 0.81 76.9 90.1
8 12.0 0.08 0.87 88.0 91.4
16 13.7 0.06 0.89 86.3 93.6
Avg 21.0 0.08 0.76 79.0 86.6
</table>
<bodyText confidence="0.7897485">
In Table 2, &apos;AT1&apos; denotes the number of positive train-
ing documents where Nt takes on values 1, 2, 4, 8
</bodyText>
<figure confidence="0.882121583333333">
3 ttp://www.nist .govispeech/tdt98.htm
1 s.t. txj = { 0
if ti, (1 &lt; i &lt; nt)
appears in Si and
not be a topic of Stp
otherwise
if txj appears hi S.
otherwise
[ 112
Si =
111 1 If(t1)
0
</figure>
<page confidence="0.992369">
35
</page>
<tableCaption confidence="0.999653">
Table 1: The reSults of event words extraction
</tableCaption>
<table confidence="0.998022666666666">
Event type Doc Avg Rec/Avg Prec Event type Doc Avg R,ec/Avg Prec
Aldrich Ames 8 61.7/70.5 Karrigan/Harding 2 64.7/55.5 -
Carlos the Jackal 8 60.7/73.3 Kobe Japan quake 16 74.5/75.0
Carter in Bosnia 16 76.3/79.1 Lost in Iraq 16 75.7/68.8
Cessna on White House 8 65.7/80.0 NYC Subway bombing 16 68.0/84.2
Clinic Murders 16 75.9/80.0 OK-City bombing 16 78.8/47.0 •
Comet into Jupiter 16 65.2/61.9 Pentium chip flaw 4 81.1/72.9
Cuban riot in Panama 2 65.2/73.9 Quayle lung clot 8 63.6/74.4
Death of Kim Jong 16 83.3/71.4 Serbians down F-16 16 78.6/75.0
DNA in OJ trial 16 78.7/72.9 Serbs violate Bihac 16 55.0/59.3
Haiti ousts observers 8 62.0/74.0 Shannon Faulker 4 71.4/82.4
Hall&apos;s copter 16 78.5/75.0_ USAir 427 crash 16 72.6/86.3
Humble, TX, flooding 16 80.4/70.2 WTC Bombing trial 16 62.6/70.1
Justice-to-be Breyer . 8 75.9/72.2
Accuracy . 71.0/72.2
</table>
<bodyText confidence="0.898967695652174">
and 16. &apos;Miss&apos; means Miss rate, which is the ra-
tio of the documents that were judged as YES but
were not evaluated as YES for the run in question.
&apos;F/A&apos; shows false alarm rate and T1&apos; is a measure
that balances recall and precision. &apos;Rec&apos; denotes the
ratio of the documents judged YES that were also
evaluated as YES, and &apos;Prec&apos; is the percent of the
documents that were evaluated as YES which corre-
spond to documents actually judged as YES.
Table 2 shows that more training data helps the
performance, as the best result was when we used
Art = 16.
Table 3 illustrates the extracted topic and event
words in a sample document. The topic is &apos;Kobe
Japan quake&apos; and the number of positive training
documents is 4. `Devclit&apos; , DispP t&apos; and
DispDt&apos; denote values calculated by using formula
(2) and (3).
Topic word Dev pit Devdit DispPt DisPDt
earthquake 53.5 50.0 12.3 10.3
Japan 69.8 50.0 13.3 9.8
Kobe 56.6 50.0 8.6 6.4
fire 57.0 46.4 2.3 1.5
</bodyText>
<figureCaption confidence="0.946607">
In Table 3, &apos;Event&apos; denotes event words in the first
document in chronological order from Nt = 4, and
the title of the document is &apos;Emergency Work Con-
tinues After Earthquake in Japan&apos;. Table 3 clearly
demonstrates that the criterion, domain dependency
of words effectively employed.
Figure 6 illustrates the DET (Detection Evalua-
tion Tradeoff) curves for a sample event (event type
is &apos;Comet into Jupiter&apos;) runs at several values of N.
Figure 6: DET curve for a sample tracking runs
</figureCaption>
<bodyText confidence="0.999495">
Overall, the curves also show that more training
helps the performance, while there is no significant
difference among Nt = 2, 4 and 8.
</bodyText>
<subsectionHeader confidence="0.902147">
5.4 Key Paragraph Extraction
</subsectionHeader>
<bodyText confidence="0.997055">
We used 4 different sets as a test data. Each set con-
sists of 2, 4, 8 and 16 documents. For each set, we
</bodyText>
<figure confidence="0.866001958333333">
.,Table 3: Topic and event words in &apos;Kobe Japan
quake&apos;
Event word Dev pi t Devdit DispP f DiAPDt
emergency 50.0 74.7 0.9 1.5
area 40.6 50.0 0.6 1.0
worker 50.0 66.1 0.4 1.0
rescue 43.3 50.0 2.3 3.4
tandem ped °mance
t.k1
Evaluated at •
Evaluated at -
N=4
Evaluated at
Ned
Evaluated at
14.18 -
Evaluated at •
.1 4.
20 i•
10
5 I&apos;m)*
4
2 6 10 20 40
False Alarm Probabifity (in %)
</figure>
<page confidence="0.974887">
36
</page>
<subsectionHeader confidence="0.97524">
5.2 Event Extraction
</subsectionHeader>
<bodyText confidence="0.999989692307692">
We collected 300 documents from the TDT1 corpus,
each of which is annotated with respect to one of 25
events. The result is shown in Table 1.
In Table 1, &apos;Event type&apos; illustrates the target events
defined by the TDT Pilot Study. &apos;Doc&apos; denotes the
number of documents. &apos;Rec&apos; (Recall) is the num-
ber of correct events divided by the total number
of events which are selected by a human, and &apos;Pre&amp;
(Precision) stands for the number of correct -events
divided by the number of events which are selected
by our method. The denominator &apos;Re&amp; is made by
a human judge. &apos;Accuracy&apos; in Table 1 is the total
average ratio.
In Table 1, recall and precision values range from
55.0/47.0 to 83.3/84.2, the average being 71.0/72.2.
The worst result of recall and precision was when
event type was &apos;Serbs violate Bihac&apos; (55.0/59.3). We
currently hypothesize that this drop of accuracy is
due to the fact that some documents are against our &apos;
assumption of an event. Examining the documents
whose event type is &apos;Serbs violate Bihac&apos;, 3 ( one
from CNN and two from Reuters) out of 16 docu-
ments has discussed the same event, i.e. &apos;Bosnian
Muslim enclave hit by heavy shelling&apos;. As a result,
the event appears across these three documents. Fu-
ture research will shed more light on that.
</bodyText>
<subsectionHeader confidence="0.950803">
5.3 Tracking Task
</subsectionHeader>
<bodyText confidence="0.998569363636364">
Tracking task in the TDT project is starting from
a few sample documents and finding all subsequent
documents that discuss the same event (Allan and
Carbonell, 1998), (Carbonell et al., 1999). The cor-
pus is divided into two parts: training set and test
set. Each of the documents is flagged as to whether
it discusses the target event, and these flags (`YES&apos;,
&apos;NO&apos;) are the only information used for training the
, system to correctly classify the target event. We ap-
plied the extracted topic to the tracking task under
. these conditions. The basic algorithm used in the
</bodyText>
<listItem confidence="0.981255">
• experiment is as follows:
1. Create a single document SO and represent it as
a term vector
</listItem>
<bodyText confidence="0.9345673">
For the results of topic extraction, all the docu-
ments that belong to the same topic are bundled
into a single document Stp and represent it by
a term vector as follows:
[ ttpl 1 if itpj is a topic
ttp2 1 f(tepi) of Stp
SA. tipj --= otherwise
0 •
ttpn
f (w) denotes term frequency of word w.
</bodyText>
<listItem confidence="0.8137305">
2. Represent other training and test documents as
term vectors
</listItem>
<bodyText confidence="0.998100571428571">
Let SI, S„2 be all the other training docu-
ments (where in is the number of training doc-
uments which does not belong to the target
event) and S, be a test document which should
be classified as to whether or not it discusses the
target event. SI, • • S,„ and Sr are represented -
by term vectors as follows:
</bodyText>
<equation confidence="0.922327">
=
t.1
= tx2 s.t.t.i =
t 0 otherwise
txn
</equation>
<listItem confidence="0.97635025">
3. Compute the similarity between a training docu-
ment and a test document
Given a vector representation of documents Se,
• • S„,, Stp and S. a similarity between two
documents Si (1 &lt;i &lt;7n, tp) and the test doc-
ument Sr would be obtained by using formula
(8), i.e. the inner product of their normalized
vectors.
</listItem>
<equation confidence="0.949239666666667">
Si Sx
Sim (Si, Sx) = (8)
I Si II Sx
</equation>
<bodyText confidence="0.985705777777778">
The greater the value of Sim(Si, Sr) is, the
more similar Si and Sr are. If the similarity
value between the test document Sr and the
document Sip is largest among all the other
pairs of documents, i.e. (Si, Sr), • (Sm, Sr),
S, is judged to be a document that discusses
the target event.
We used the standard TDT evaluation measure 3 .
Table 2 illustrates the result.
</bodyText>
<tableCaption confidence="0.99701">
Table 2: The results of tracking task
</tableCaption>
<table confidence="0.999435714285714">
Ni %Miss %F/A Fl %Rec %Prec
1 32.5 0.16 0.68 67.5 70.0
2 23.7 0.06 0.80 76.3 87.8
4 23.1 0.05 0.81 76.9 90.1
8 12.0 0.08 0.87 88.0 91.4
16 13.7 0.06 0.89 86.3 93.6
Avg 21.0 0.08 0.76 79.0 86.6
</table>
<bodyText confidence="0.5116015">
In Table 2, Re&apos; denotes the number of positive train-
ing documents where Are takes on values 1, 2, 4, 8
</bodyText>
<footnote confidence="0.805545">
3 http://www.nist.gov/speech/tdt98.htm
</footnote>
<table confidence="0.7563248">
if (1 &lt; i &lt; In)
appears in Si and
not he a topic of Stp
otherwise
if t,, appears in St
</table>
<page confidence="0.987681">
35
</page>
<tableCaption confidence="0.999804">
Table 1: The reSults of event words extraction
</tableCaption>
<table confidence="0.99949192">
Event type Doc Avg Rec/Avg Prec Event type Doc _ Avg Rec/Avg Prec
Aldrich Ames 8 61.7/70.5 Karrigan/Harding 9 64.7/55.5 .
Carlos the Jackal 8 60.7/73.3 Kobe Japan quake 16 74.5/75.0
Carter in Bosnia 16 76.3/79.1 Lost in Iraq 16 75.7/68.8
Cessna on White House 8 65.7/80.0 NYC Subway bombing 16 68.0/84.2
Clinic Murders 16 75.9/80.0 OK-City bombing 16 78.8/47.0
Comet into Jupiter 16 65.2/61.9 Pentium chip flaw 4 81.1/72.9
Cuban riot in Panama 2 65.2/73.9 Quayle lung clot 8 63.6/74.4
Death of Kim Jong 16 83.3/71.4 Serbians down F-16 16 78.6/75.0
DNA in OJ trial 16 78.7/72.9 Serbs violate Bihac 16 55.0/59.3
Haiti ousts observers 8 62.0/74.0 Shannon Faulker 4 71.4/82.4
Hall&apos;s copter 16 78.5/75.0 USAir 427 crash 16 72.6/86.3
Humble, TX, flooding 16 80.4/70.2 WTC Bombing trial 16 62.6/70.1
Justice-to-be Breyer 8 75.9/72.2
Accuracy 71.0/72.2
Topic word Devpit Devdit DispPt DispDt
earthquake 53.5 50.0 12.3 10.3
Japan 69.8 50.0 13.3 9.8
Kobe 56.6 50.0 8.6 6.4
fire_ 57.0 46.4 2.3 1.5
Event word Devpit Devdit DispPt DisPDt
emergency 50.0 74.7 0.9 1.5
area 40.6 50.0 0.6 1.0
worker 50.0 66.1 0.4 1.0
rescue 43.3 50.0 2.3 3.4
</table>
<bodyText confidence="0.955353931034483">
and 16. &apos;Miss&apos; means Miss rate, which is the ra-
tio of the documents that were, judged as YES but
were not evaluated as YES for the run in question.
`F/A&apos; shows false alarm rate and &apos;Fr is a measure
that balances recall and precision. am&apos; denotes the
ratio of the documents judged YES that were also
evaluated as YES, and &apos;Prec&apos; is the percent of the
documents that were evaluated as YES which corre-
spond to documents actually judged as YES.
Table 2 shows that more training data helps the
performance, as the best result was when we used
Art = 16.
Table 3 illustrates the extracted topic and event
words in a sample document. The topic is &apos;Kobe
Japan quake&apos; and the number of positive training
documents is 4. `Devpit&apos;, `Devdit&apos;, `DispPt&apos; and
`DispDt&apos; denote values calculated by using formula
(2) and (3).
Table 3: Topic and event words in &apos;Kobe Japan
&apos;quake&apos;
In Table 3, &apos;Event&apos; denotes event words in the first
document in chronological order from Art = 4, and
the title of the document is &apos;Emergency Work Con-
tinues After Earthquake in Japan&apos;. Table • 3 clearly
demonstrates that the criterion, domain dependency
of words effectively employed.
Figure 6 illustrates the DET (Detection Evalua-
tion Tradeoff) curves for a sample event (event type
is &apos;Comet into Jupiter&apos;) runs at several values of Nt.
</bodyText>
<figureCaption confidence="0.991565">
Figure 6: DET curve for a sample tracking runs
</figureCaption>
<bodyText confidence="0.999032">
&apos; Overall, the curves also show that more training
helps the performance, while there is no significant
difference among Art = 2, 4 and 8.
</bodyText>
<subsectionHeader confidence="0.994011">
5.4 Key Paragraph Extraction
</subsectionHeader>
<bodyText confidence="0.999887">
We used 4 different sets as a test data. Each set con-
sists of 2, 4, 8 and 16 documents. For each set, we
</bodyText>
<figure confidence="0.992999266666667">
random performance -
aN1 4
Evaluated at
afr-2
Evaluated at
N=4
Evaluated at
N.8
Evaluated at c
1,418 -
Evaluated at •
?
0.5 2 S 10 20 40 80 90
False Alarm Probability (in %)
90
</figure>
<page confidence="0.995284">
36
</page>
<bodyText confidence="0.999575058823529">
extracted 10% and 20% of the full-documents para-
graph length Ping et al., 1998). Table 4 illustrates
the result.
In Table 4, &apos;Num&apos; denotes the number of documents
in a set. 10 and 20% indica.te the extraction ratio.
&apos;Para&apos; denotes the number of paragraphs extracted
by a human judge, and &apos;Correct&apos; shows the accuracy
of the method.
The best result was 77.7% (the extraction ratio is
20% and the number of documents is 2).
We now turn our attention to the main question:
how was the contribution of making the distinction
between a topic and an event for summarization
task? Figure 7 illustrates the results of the methods
which used (i) the extracted topic and event words,
i.e. our method, and (ii) only the extracted event
words.
</bodyText>
<figureCaption confidence="0.999246">
Figure 7: Accuracy with each method
</figureCaption>
<bodyText confidence="0.99853495">
In Figure 7, &apos;(10%)&apos; and &apos;(20%)&apos; denote the ex:
tracted paragraph ratio. &apos;Event&apos; is the result when
we used only the extracted event words. Figure 7
shows that our method consistently outperforms the
method which used only the extracted events. To
summarize the evaluation:
t. Event extraction effectively employed when
each document discusses different subject about
the same topic. This shows that the method will
be applicable to other genres of corpora which
consist of different subjects.
2. The result of tracking task (79.0% average recall
and 86.6% average precision) is comparable to
the existing tracking techniques which tested on
the TDT1 corpus (Allan and Carbonell, 1998).
3. Distinction between a topic and an event im-
proved the results of key paragraph extrac-
tion, as our method consistently outperforms
the method which used only the extracted event
words (see Figure 7).
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999835267857143">
The majority of techniques for summarization fall
within two broad categories: Those that rely on tem-
plate instantiation and those that rely on passage
extraction.
Work in the former approach is the DARPA-
sponsored TIPSTER program and, in particular, the
message understanding conferences has provided fer-
tile ground for such work, by placing the emphasis
of document analysis to the identification and ex-
traction of certain core entities and facts in a doc-
ument, while work on template-driven, knowledge-
based summarization to date is hardly domain or
genre-independent (Boguraev and Kennedy, 1997).
The alternative approach largely escapes this con-
straint, by viewing the task as one of identifying
certain passages(typically sentences) which, by some
metric, are deemed to be the most representative of
the document&apos;s content. A variety of approaches ex-
ist for determining the salient sentences in the text:
statistical techniques based on word distribution
(Kupiec et al., 1995), (Zechner, 1996), (Salton et
al., 1991), (Teufell and Moens, 1997), symbolic tech-
niques based on discourse structure (Marcu, 1997)
and semantic relations between words (Barzilay and
Elhadad, 1997). All of their results demonstrate that
passage extraction techniques are a useful first step
in document summarization, although most of them
have focused on a single document.
Some researchers have started to apply a
single-document summarization technique to multi-
document. Stein et. al. proposed a method for
summarizing multi-document using single-document
summarizer (Stralkowsik et al., 1998), (Stralkowski
et al., 1999). Their method first summarizes each
document of multi-document, then groups the sum-
maries in clusters and finally, orders these summaries
in a logical way (Stein et al., 1999). Their technique
seems sensible. However, as she admits, (i) the order
the information should not only depend on topic cov-
ered, (ii) background information that helps clarify
related information should be placed first. More seri-
ously, as Barzilay and Mani claim, summarization of
multiple documents requires information about sim-
ilarities and differences across documents. There-
fore it is difficult to identify these information using
a single-document summarizer technique (Mani and
Bloedorn, 1997), (Barzilay et al., 1999).
A method proposed by Mani et. al. deal with
the problem, i.e. they tried to detect the similar-
ities and differences in information content among
documents (Mani and Bloedorn, 1997). They used
a spreading activation algorithm and graph match-
ing in order to identify similarities and differences
across documents. The output is presented as a set
of paragraphs with similar and unique words high-
lighted. However, if the same information is men-
</bodyText>
<figure confidence="0.9069235">
4 8 16
Num
</figure>
<page confidence="0.995772">
37
</page>
<tableCaption confidence="0.999563">
Table 4: The results of Key Paragraph Extraction
</tableCaption>
<table confidence="0.9992375">
Num Accuracy
%10 %20 Total
Para Correct(%) Para Correct(%) Para Correct(%)
2 58 44(75.8) 117 91(77.7) 175 135(77.1)
4 107 80(74.7) 214 160(74.7) 321 240(74.7)
8 202 138(68.3) 404 278(68.8) 606 416(68.6)
16 281 175(62.2) 563 361(64.1) 844 536(63.5)
Total 648 437(67.4) 1,298 890(68.5) 1,946 1,327(68.1)
</table>
<bodyText confidence="0.999411606060606">
-tioned several times in different documents, much of
the summary will be redundant.
Allan et. al. also address the problem and pro-
posed a method for event tracking using cornmoTz
words and surprising features by supplementing the
corpus statistics (Allan and Papka, 1998) (Papka et
al., 1999). One of the purpose of this study is to
make a distinction between an event and an event
class using surprising features. Here event class fea-
tures are broad news areas such as politics, death,
destruction and warfare. The idea is considered to
be necessary to obtain high accuracy, while Allan
claims that the surprising words do not provide a
broad enough coverage to capture all documents on
the event.
A more recent approach dealing with this problem
is Barzilay et. al&apos;s approach (Barzilay et al., 1999).
They used paraphrasing ndes which are manually
derived from the result of syntactic analysis to iden-
tify theme intersection and used language generation
to reformulate them as a coherent summary. While
promising to obtain high accuracy, the result of sum-
marization task has not been reported.
Like Mani and Barzilay&apos;s techniques, our ap-
proach focuses on the problem that how to identify
differences and similarities across documents, rather
than the problem that how to form the actual suin-
mary (Sparck, 1993), (McKeown and Radev, 1995),
(Radev and McKeown, 1998). However, while Barzi-
lay&apos;s approach used paraphrasing rules to eliminate
redundancy in a summary, we proposed domain de-
pendency of words to address robustness of the tech-
nique.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999982947368421">
In this paper, we proposed a method for extract-
ing key paragraph for summarization based on dis-
tinction between. a topic and an event. The results
showed that the average accuracy was 68.1% when
we used the TDT1 corpus. TIPSTER Text Sum-
marization Evaluation (SUMMAC) proposed vari-
ous methods for evaluating document summariza-
tion and tasks (Mani et al., 1999). Of these, par-
ticipants submitted two summaries: a fixed-length
summary limited to 10% of the length of the source,
and a summary which was not limited in length. Fu-
ture work includes quantitative and qualitative eval-
uation. In addition, our method used single words
rather than phrases. These phrases, however, would
be helpful to resolve ambiguity and reduce a lot of
noise, i.e. yield much better accuracy. We plan to
apply our method to phrase-based topic and event
extraction, then turn to focus on the problem that
how to form the actual summary.
</bodyText>
<sectionHeader confidence="0.999075" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999048">
The authors would like to thank the reviewers
for their valuable comments. This work was sup-
ported by the Grant-in-aid for the Japan Society for
the Promotion of Science(JSPS, No.11780258) and
Tateisi Science and Technology Foundation.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998914904761905">
J. Allan and J. Carbonell. 1997. The tdt pilot study
corpus documentation. In TDT . Study. Corpus,
V1.3.doc.
J. Allan and J. Carbonell. 1998. Topic detection
and tracking pilot study: Final report. In Proc.
of the DARPA Broadcast News Transcription and
Understanding Workshop.
J. Allan and R. Papka. 1998. On-line new event de-
tection and tracking. In Proc. of 21st Annual In-
ternational ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
37-45.
R. Barzilay and M. Elhadad. 1997. Using lexical
chains for text summarization. In Proc. of ACL
Workshop on Intelligent Scalable Text Summa-
rization, pages 10-17.
R. Barzilay, K. R. McKeown, and M. Elhadad.
1999. Information fusion in the context of multi-
document summarization. In Proc. of 37th An-
nual Meeting of Association for Computational
Linguistics, pages 550-557.
</reference>
<page confidence="0.985303">
38
</page>
<reference confidence="0.9998677375">
B. Boguraev and C. Kennedy. 1997. Salience-based
content characterization of text documents. In
Proc. of ACL Workshop on Intelligent Scalable
Text Summarization, pages 2-9.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. of the 3rd Conference on Applied
Natural Language Processing, pages 152-155.
J. Carbonell, Y. Yang, and J. Lafferty. 1999. CMU
report on TDT-2: Segmentation, detection and
tracking. In Proc. of the DARPA Broadcast News
Workshop.
F. Fukumoto, Y. Suzuki, and J. Fukumoto. 1997.
An automatic extraction of key paragraphs based
on context dependency. In Proc. of the 5th Con-
ference on Applied Natural Language Processing,
pages 291-298.
H. Jing, R. Barzilay, K. R. McKeown, and M. El-
hadad. 1998. Summarization evaluation methods:
Experiments and analysis, intelligent text sum-,
marization. In Proc. of 1998 American Associa-
tion for Artificial Intelligence Spring Symposium,
pages 51-59.
J. Kupiec, Pedersen, and F. . Chen. 1995. A
trainable document summarizer. In Proc. of the
18th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 68-73.
H. P. Luhn. 1958. The automatic creation of litera-
ture abstracts. IBM journal, 2(1):159-165.
I. Mani and E. Bloedorn. 1997. Multi-document
summarization by graph search and matching. In
Proc. of the 15th National Conference on Artifi-
cial Intelligence, pages 622-628.
I. Mani, T. Firmin, and B. Sundheim. 1999. The
TIPSTER SUMMAC text summarization evalu-
ation. In Proc. of Ninth Conference of the Eu-
ropean Chapter of the Association for Computa-
tional Linguistics, pages 77-85.
D. Marcu. 1997. From discourse structures to text
summaries. In Proc. of ACL Workshop on Intel-
ligent Scalable Text Summarization, pages 82-88.
K. R. McKeown and D. R.. Radev. 1995. Generating
summaries of multiple news articles. In Proc. of
the 18th Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 74-82.
R. Papka, J. Allan, and V. Lavrenko. 1999. UMASS
approaches to detection and tracking at TDT2. In
Proc. of the DARPA Broadcast News Workshop.
D. R. Radev and K. R. McKeown. 1998. Gen-
erating natural language summaries from multi-
ple on-line sources. Computational Linguistics,
24(3):469-500.
G. Salton, J. Allan, C. Buckley, and A. Singhal.
1991. Automatic analysis, theme generation, and
summarization of machine-readable texts. Sci-
ence, 164:1421-1426.
K. J. Sparck. 1993. What might be in a summary?
In Proc. of Information Retrieval93, pages 9-26.
G. C. Stein, T. Strzalkowski, and G. B. Wise. 1999.
Summarizing multiple documents using text ex-
traction and interactive clustering. In Proc. of
the Pacific Association for Computational Lin-
guistics1999, pages 200-208.
T. Stralkowsik, G. C. Stein, and G. B. Wise. 1998.
A text-extraction based summarizer. In Proc. of
Tipster Workshop.
T. Stralkowski, G. C. Stein, and G. B. Wise. 1999.
Getracker: A robust, lightweight topic tracking
system. In Proc. of the DARPA Broadcast News
Workshop.
S. Teufell and M. Moens. 1997. Sentence extraction
as a classification task. In Proc. of ACL Workshop
on Intelligent Scalable Text Summarization, pages
58-65.
K. Zechner. 1996. Fast generation of abstracts from
general domain text corpora by extracting rele-
vant sentences. In Proc. of the 16th International
Conference on Computational Linguistics, pages
986-989.
</reference>
<page confidence="0.999527">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.179330">
<title confidence="0.999449">Extracting Key Paragraph based on Topic and Event Detection Towards Multi-Document Summarization</title>
<author confidence="0.992465">Fumiyo Fukumoto</author>
<author confidence="0.992465">Yoshimi</author>
<affiliation confidence="0.7253975">Department of Computer Science and Media Yamanashi</affiliation>
<address confidence="0.683176">4-3-11 Takeda, Kofu 400-8511</address>
<note confidence="0.575636">IfakumotoAskye.esk ysuzuki Oalps1.641.yamanashi.ac.jp</note>
<abstract confidence="0.9932175">This paper proposes a method for extracting key paragraph for multi-document summarization based on distinction between a topic and an event. A topic and an event are identified using a simple criterion called domain dependency of words. The methog was tested on the TDT1 corpus which has been developed by the TDT Pilot Study and the result can be regarded as promising the idea of domain dependency of words effectively employed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Carbonell</author>
</authors>
<title>The tdt pilot study corpus documentation.</title>
<date>1997</date>
<booktitle>In TDT . Study. Corpus, V1.3.doc.</booktitle>
<contexts>
<context position="16416" citStr="Allan and Carbonell, 1997" startWordPosition="2830" endWordPosition="2833">ame, Compute the sum of all the topic and event word weights. Select a paragraph whose weight is higher than the others. 5 Experiments Evaluation of extracting key paragraph based on multi-document is difficult. First, we have not found an existing collection of summaries of multiple documents. Second, the manual effort needed to judge system output is far more extensive than for single document summarization. Consequently, we focused on the TDT1 corpus. This is because (i) events have been defined to support the TDT study effort, (ii) it was completely annotated with respect to these events (Allan and Carbonell, 1997). Therefore, we do not need the manual effort to collect documents which discuss about the target event. We report the results of three experiments. The first experiment, Event Extraction, is concerned with event extraction technique. In the second experiment, Tracking Task, we applied the extracted topics to tracking task (Allan and Carbonell, 1998). The third experiment, Key Paragraph Extraction is conducted to evaluate how the extracted topic and event words can be used effectively to extract key paragraph. 5.1 Data The TDT1 corpus comprises a set of documents (15,863) that includes both ne</context>
</contexts>
<marker>Allan, Carbonell, 1997</marker>
<rawString>J. Allan and J. Carbonell. 1997. The tdt pilot study corpus documentation. In TDT . Study. Corpus, V1.3.doc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>J Carbonell</author>
</authors>
<title>Topic detection and tracking pilot study: Final report.</title>
<date>1998</date>
<booktitle>In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="7319" citStr="Allan and Carbonell, 1998" startWordPosition="1207" endWordPosition="1210"> explicitly exploits this feature of the domain dependency of words: how strongly a word features a given set of data. The rest of the paper is organized as follows. The next section provides domain dependency of words which is used to identify a topic and an event for broadcast news documents. We then present a method for extracting topic and event words, and describe a paragraph-based summarization algorithm using the result of topic and event extraction. Finally, we report some experiments using the TDT1 corpus which has been developed by the TDT (Topic Detection and Tracking) Pilot Study (Allan and Carbonell, 1998) with a discussion of evaluation. 2 Domain Dependency of Words The domain dependency of words that how strongly a word features a given set of data (documents) contributes to event extraction, as we previously reported (Fukumoto et al., 1997). In the study, we hypothesised that the articles from the Wall Street Journal corpus can be structured by three levels, i.e. Domain, Article and Paragraph. If a word is an event in a given article, it satisfies the two conditions: (1) The dispersion value of the word in the Paragraph level is smaller than that of the Article, since the word appears throug</context>
<context position="9427" citStr="Allan and Carbonell, 1998" startWordPosition="1562" endWordPosition="1565">consists of different subject domains, such as &apos;aerospace&apos;, &apos;environment&apos; and &apos;stock market&apos;, the method can be done with satisfactory accuracy. However, there is no guarantee to make such an appropriate structure from a given set of documents in the multi-document summarization task. • The purpose of this paper is to define domain dependency of words for a number of sample documents about the same topic, and thus for multidocument summarization task. Figure 4 illustrates the structure of broadcast news documents which have been developed by the TDT (Topic Detection and Tracking) Pilot Study (Allan and Carbonell, 1998). It consists of two levels, Paragraph and Document. In Document level, there is a small number of sample news documents about the same topic. These documents are arranged in chronological order such as, &apos;(1-1) Quake collapses buildings in central Japan (Figure 2)&apos;, &apos;(1-2) Two Americans known dead in Japan quake (Figure 1)&apos; and &apos;(1-3) Kobe quake leaves questions about medical system (Figure 3)&apos;. A particular document consists of several 32 paragraphs. We call it Paragraph level. Let words within a document be an event, a topic, or among others (We call it a general word). (1.1) - • - Ox 0 6 • </context>
<context position="16768" citStr="Allan and Carbonell, 1998" startWordPosition="2885" endWordPosition="2888">is far more extensive than for single document summarization. Consequently, we focused on the TDT1 corpus. This is because (i) events have been defined to support the TDT study effort, (ii) it was completely annotated with respect to these events (Allan and Carbonell, 1997). Therefore, we do not need the manual effort to collect documents which discuss about the target event. We report the results of three experiments. The first experiment, Event Extraction, is concerned with event extraction technique. In the second experiment, Tracking Task, we applied the extracted topics to tracking task (Allan and Carbonell, 1998). The third experiment, Key Paragraph Extraction is conducted to evaluate how the extracted topic and event words can be used effectively to extract key paragraph. 5.1 Data The TDT1 corpus comprises a set of documents (15,863) that includes both newswire (Reuters) 7,965 and a manual transcription of the broadcast news speech (CNN) 7,898 documents. A set of 25 target events were defined 2 . All documents were tagged by the tagger (Brill, 1992). We used nouns in the documents. 2 littp://morphidc.upenn.edu/TDT • DispDt = Devdit 34 5.2 Event Extraction We collected 300 documents from the TDT1 corp</context>
<context position="18772" citStr="Allan and Carbonell, 1998" startWordPosition="3226" endWordPosition="3229">ently hypothesize that this drop of accuracy is , due to the fact that some documents are against our assumption of an event. Examining the documents whose event type is &apos;Serbs violate Bihac&apos;, 3 ( one from CNN and two from Reuters) .out of 16 documents has discussed the same event, i.e. &apos;Bosnian Muslim enclave hit by heavy shelling&apos;. As a result, the event appears across these three documents. Future research will shed more light on that. 5.3 Tracking Task Tracking task in the TDT project is starting from a few sample documents and finding all subsequent documents that discuss the same event (Allan and Carbonell, 1998), (Carbonell et al., 1999). The corpus is divided into two parts: training set and test set. Each of the documents is flagged as to whether it discusses the target event, and these flags (&apos;ITES&apos;, &apos;NO&apos;) are the only information used for training the ..system to correctly classify the target event. We applied the extracted topic to the tracking task under these conditions. The basic algorithm used in the experiment is as follows: 1. Create a single document Sfp and represent it as a term vector For the results of topic extraction, all the documents that belong to the same topic are bundled into </context>
<context position="25626" citStr="Allan and Carbonell, 1998" startWordPosition="4484" endWordPosition="4487">rently hypothesize that this drop of accuracy is due to the fact that some documents are against our &apos; assumption of an event. Examining the documents whose event type is &apos;Serbs violate Bihac&apos;, 3 ( one from CNN and two from Reuters) out of 16 documents has discussed the same event, i.e. &apos;Bosnian Muslim enclave hit by heavy shelling&apos;. As a result, the event appears across these three documents. Future research will shed more light on that. 5.3 Tracking Task Tracking task in the TDT project is starting from a few sample documents and finding all subsequent documents that discuss the same event (Allan and Carbonell, 1998), (Carbonell et al., 1999). The corpus is divided into two parts: training set and test set. Each of the documents is flagged as to whether it discusses the target event, and these flags (`YES&apos;, &apos;NO&apos;) are the only information used for training the , system to correctly classify the target event. We applied the extracted topic to the tracking task under . these conditions. The basic algorithm used in the • experiment is as follows: 1. Create a single document SO and represent it as a term vector For the results of topic extraction, all the documents that belong to the same topic are bundled int</context>
<context position="32434" citStr="Allan and Carbonell, 1998" startWordPosition="5712" endWordPosition="5715">agraph ratio. &apos;Event&apos; is the result when we used only the extracted event words. Figure 7 shows that our method consistently outperforms the method which used only the extracted events. To summarize the evaluation: t. Event extraction effectively employed when each document discusses different subject about the same topic. This shows that the method will be applicable to other genres of corpora which consist of different subjects. 2. The result of tracking task (79.0% average recall and 86.6% average precision) is comparable to the existing tracking techniques which tested on the TDT1 corpus (Allan and Carbonell, 1998). 3. Distinction between a topic and an event improved the results of key paragraph extraction, as our method consistently outperforms the method which used only the extracted event words (see Figure 7). 6 Related Work The majority of techniques for summarization fall within two broad categories: Those that rely on template instantiation and those that rely on passage extraction. Work in the former approach is the DARPAsponsored TIPSTER program and, in particular, the message understanding conferences has provided fertile ground for such work, by placing the emphasis of document analysis to th</context>
</contexts>
<marker>Allan, Carbonell, 1998</marker>
<rawString>J. Allan and J. Carbonell. 1998. Topic detection and tracking pilot study: Final report. In Proc. of the DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>R Papka</author>
</authors>
<title>On-line new event detection and tracking.</title>
<date>1998</date>
<booktitle>In Proc. of 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>37--45</pages>
<contexts>
<context position="2066" citStr="Allan and Papka, 1998" startWordPosition="328" endWordPosition="331">and how in a document. On the other hand, a topic in this paper is some unique thing that happens at some specific time and place, and the unavoidable consequences. It&apos; becomes background among documents. For example, in the documents of &apos;Kobe Japan quake&apos;, the event includes early reports of damage, location and nature of quake, rescue efforts, consequences of the quake, and on-site reports, while the topic is Kobe Japan quake. The well-known past experience from IR that notions of who, what, where, when, why and how may not make a great contribution to the topic detection and tracking task (Allan and Papka, 1998) causes this fact, i.e. a topic and an event are different from each other&apos; . 1 Some topic words can also be an event. For instance, in the document shown in Figure 1, &apos;Japan&apos; and &apos;quake&apos; are topic words and also event words in the document. However, we regarded these words as a topic, i.e. not be an event.. In this paper, we propose a. method for extracting key paragraph for nmlti-document summarization based on distinction between a topic and an event. We use a simple criterion called domain dependency of words as a solution and present how the idea of domain dependency of words can be utili</context>
<context position="36038" citStr="Allan and Papka, 1998" startWordPosition="6265" endWordPosition="6268">able 4: The results of Key Paragraph Extraction Num Accuracy %10 %20 Total Para Correct(%) Para Correct(%) Para Correct(%) 2 58 44(75.8) 117 91(77.7) 175 135(77.1) 4 107 80(74.7) 214 160(74.7) 321 240(74.7) 8 202 138(68.3) 404 278(68.8) 606 416(68.6) 16 281 175(62.2) 563 361(64.1) 844 536(63.5) Total 648 437(67.4) 1,298 890(68.5) 1,946 1,327(68.1) -tioned several times in different documents, much of the summary will be redundant. Allan et. al. also address the problem and proposed a method for event tracking using cornmoTz words and surprising features by supplementing the corpus statistics (Allan and Papka, 1998) (Papka et al., 1999). One of the purpose of this study is to make a distinction between an event and an event class using surprising features. Here event class features are broad news areas such as politics, death, destruction and warfare. The idea is considered to be necessary to obtain high accuracy, while Allan claims that the surprising words do not provide a broad enough coverage to capture all documents on the event. A more recent approach dealing with this problem is Barzilay et. al&apos;s approach (Barzilay et al., 1999). They used paraphrasing ndes which are manually derived from the resu</context>
</contexts>
<marker>Allan, Papka, 1998</marker>
<rawString>J. Allan and R. Papka. 1998. On-line new event detection and tracking. In Proc. of 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 37-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proc. of ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<contexts>
<context position="33830" citStr="Barzilay and Elhadad, 1997" startWordPosition="5927" endWordPosition="5930"> genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 19</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>R. Barzilay and M. Elhadad. 1997. Using lexical chains for text summarization. In Proc. of ACL Workshop on Intelligent Scalable Text Summarization, pages 10-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Information fusion in the context of multidocument summarization.</title>
<date>1999</date>
<booktitle>In Proc. of 37th Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>550--557</pages>
<contexts>
<context position="34958" citStr="Barzilay et al., 1999" startWordPosition="6091" endWordPosition="6094">e summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 1999). Their technique seems sensible. However, as she admits, (i) the order the information should not only depend on topic covered, (ii) background information that helps clarify related information should be placed first. More seriously, as Barzilay and Mani claim, summarization of multiple documents requires information about similarities and differences across documents. Therefore it is difficult to identify these information using a single-document summarizer technique (Mani and Bloedorn, 1997), (Barzilay et al., 1999). A method proposed by Mani et. al. deal with the problem, i.e. they tried to detect the similarities and differences in information content among documents (Mani and Bloedorn, 1997). They used a spreading activation algorithm and graph matching in order to identify similarities and differences across documents. The output is presented as a set of paragraphs with similar and unique words highlighted. However, if the same information is men4 8 16 Num 37 Table 4: The results of Key Paragraph Extraction Num Accuracy %10 %20 Total Para Correct(%) Para Correct(%) Para Correct(%) 2 58 44(75.8) 117 9</context>
<context position="36568" citStr="Barzilay et al., 1999" startWordPosition="6356" endWordPosition="6359">words and surprising features by supplementing the corpus statistics (Allan and Papka, 1998) (Papka et al., 1999). One of the purpose of this study is to make a distinction between an event and an event class using surprising features. Here event class features are broad news areas such as politics, death, destruction and warfare. The idea is considered to be necessary to obtain high accuracy, while Allan claims that the surprising words do not provide a broad enough coverage to capture all documents on the event. A more recent approach dealing with this problem is Barzilay et. al&apos;s approach (Barzilay et al., 1999). They used paraphrasing ndes which are manually derived from the result of syntactic analysis to identify theme intersection and used language generation to reformulate them as a coherent summary. While promising to obtain high accuracy, the result of summarization task has not been reported. Like Mani and Barzilay&apos;s techniques, our approach focuses on the problem that how to identify differences and similarities across documents, rather than the problem that how to form the actual suinmary (Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). However, while Barzilay&apos;s approac</context>
</contexts>
<marker>Barzilay, McKeown, Elhadad, 1999</marker>
<rawString>R. Barzilay, K. R. McKeown, and M. Elhadad. 1999. Information fusion in the context of multidocument summarization. In Proc. of 37th Annual Meeting of Association for Computational Linguistics, pages 550-557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>C Kennedy</author>
</authors>
<title>Salience-based content characterization of text documents.</title>
<date>1997</date>
<booktitle>In Proc. of ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>2--9</pages>
<contexts>
<context position="33250" citStr="Boguraev and Kennedy, 1997" startWordPosition="5841" endWordPosition="5844">see Figure 7). 6 Related Work The majority of techniques for summarization fall within two broad categories: Those that rely on template instantiation and those that rely on passage extraction. Work in the former approach is the DARPAsponsored TIPSTER program and, in particular, the message understanding conferences has provided fertile ground for such work, by placing the emphasis of document analysis to the identification and extraction of certain core entities and facts in a document, while work on template-driven, knowledgebased summarization to date is hardly domain or genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their resul</context>
</contexts>
<marker>Boguraev, Kennedy, 1997</marker>
<rawString>B. Boguraev and C. Kennedy. 1997. Salience-based content characterization of text documents. In Proc. of ACL Workshop on Intelligent Scalable Text Summarization, pages 2-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proc. of the 3rd Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<contexts>
<context position="17214" citStr="Brill, 1992" startWordPosition="2960" endWordPosition="2961">n, is concerned with event extraction technique. In the second experiment, Tracking Task, we applied the extracted topics to tracking task (Allan and Carbonell, 1998). The third experiment, Key Paragraph Extraction is conducted to evaluate how the extracted topic and event words can be used effectively to extract key paragraph. 5.1 Data The TDT1 corpus comprises a set of documents (15,863) that includes both newswire (Reuters) 7,965 and a manual transcription of the broadcast news speech (CNN) 7,898 documents. A set of 25 target events were defined 2 . All documents were tagged by the tagger (Brill, 1992). We used nouns in the documents. 2 littp://morphidc.upenn.edu/TDT • DispDt = Devdit 34 5.2 Event Extraction We collected 300 documents from the TDT1 corpus, each of which is annotated with respect to one of 25 events. The result is shown in Table 1. In Table 1, &apos;Event type&apos; illustrates the target events defined by the TDT Pilot Study. &apos;Doc&apos; denotes the number of documents. &apos;Rec&apos; (Recall) is the number of correct events divided by the total number of events which are selected by a human, and &apos;Prec&apos; (Precision) stands for the number of correct -events divided by the number of events which are s</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. In Proc. of the 3rd Conference on Applied Natural Language Processing, pages 152-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>Y Yang</author>
<author>J Lafferty</author>
</authors>
<title>CMU report on TDT-2: Segmentation, detection and tracking.</title>
<date>1999</date>
<booktitle>In Proc. of the DARPA Broadcast News Workshop.</booktitle>
<contexts>
<context position="18798" citStr="Carbonell et al., 1999" startWordPosition="3230" endWordPosition="3233">rop of accuracy is , due to the fact that some documents are against our assumption of an event. Examining the documents whose event type is &apos;Serbs violate Bihac&apos;, 3 ( one from CNN and two from Reuters) .out of 16 documents has discussed the same event, i.e. &apos;Bosnian Muslim enclave hit by heavy shelling&apos;. As a result, the event appears across these three documents. Future research will shed more light on that. 5.3 Tracking Task Tracking task in the TDT project is starting from a few sample documents and finding all subsequent documents that discuss the same event (Allan and Carbonell, 1998), (Carbonell et al., 1999). The corpus is divided into two parts: training set and test set. Each of the documents is flagged as to whether it discusses the target event, and these flags (&apos;ITES&apos;, &apos;NO&apos;) are the only information used for training the ..system to correctly classify the target event. We applied the extracted topic to the tracking task under these conditions. The basic algorithm used in the experiment is as follows: 1. Create a single document Sfp and represent it as a term vector For the results of topic extraction, all the documents that belong to the same topic are bundled into a single document Stp and </context>
<context position="25652" citStr="Carbonell et al., 1999" startWordPosition="4488" endWordPosition="4491">drop of accuracy is due to the fact that some documents are against our &apos; assumption of an event. Examining the documents whose event type is &apos;Serbs violate Bihac&apos;, 3 ( one from CNN and two from Reuters) out of 16 documents has discussed the same event, i.e. &apos;Bosnian Muslim enclave hit by heavy shelling&apos;. As a result, the event appears across these three documents. Future research will shed more light on that. 5.3 Tracking Task Tracking task in the TDT project is starting from a few sample documents and finding all subsequent documents that discuss the same event (Allan and Carbonell, 1998), (Carbonell et al., 1999). The corpus is divided into two parts: training set and test set. Each of the documents is flagged as to whether it discusses the target event, and these flags (`YES&apos;, &apos;NO&apos;) are the only information used for training the , system to correctly classify the target event. We applied the extracted topic to the tracking task under . these conditions. The basic algorithm used in the • experiment is as follows: 1. Create a single document SO and represent it as a term vector For the results of topic extraction, all the documents that belong to the same topic are bundled into a single document Stp an</context>
</contexts>
<marker>Carbonell, Yang, Lafferty, 1999</marker>
<rawString>J. Carbonell, Y. Yang, and J. Lafferty. 1999. CMU report on TDT-2: Segmentation, detection and tracking. In Proc. of the DARPA Broadcast News Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>Y Suzuki</author>
<author>J Fukumoto</author>
</authors>
<title>An automatic extraction of key paragraphs based on context dependency.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>291--298</pages>
<contexts>
<context position="7561" citStr="Fukumoto et al., 1997" startWordPosition="1248" endWordPosition="1251"> topic and an event for broadcast news documents. We then present a method for extracting topic and event words, and describe a paragraph-based summarization algorithm using the result of topic and event extraction. Finally, we report some experiments using the TDT1 corpus which has been developed by the TDT (Topic Detection and Tracking) Pilot Study (Allan and Carbonell, 1998) with a discussion of evaluation. 2 Domain Dependency of Words The domain dependency of words that how strongly a word features a given set of data (documents) contributes to event extraction, as we previously reported (Fukumoto et al., 1997). In the study, we hypothesised that the articles from the Wall Street Journal corpus can be structured by three levels, i.e. Domain, Article and Paragraph. If a word is an event in a given article, it satisfies the two conditions: (1) The dispersion value of the word in the Paragraph level is smaller than that of the Article, since the word appears throughout paragraphs in the Paragraph level rather than articles in the Article level. (2) The dispersion value of the word in the Article is smaller than that of the Domain, as the word appears across articles rather than domains. However, there </context>
</contexts>
<marker>Fukumoto, Suzuki, Fukumoto, 1997</marker>
<rawString>F. Fukumoto, Y. Suzuki, and J. Fukumoto. 1997. An automatic extraction of key paragraphs based on context dependency. In Proc. of the 5th Conference on Applied Natural Language Processing, pages 291-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Barzilay</author>
<author>K R McKeown</author>
<author>M Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and analysis, intelligent text sum-, marization.</title>
<date>1998</date>
<booktitle>In Proc. of</booktitle>
<pages>51--59</pages>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>H. Jing, R. Barzilay, K. R. McKeown, and M. Elhadad. 1998. Summarization evaluation methods: Experiments and analysis, intelligent text sum-, marization. In Proc. of 1998 American Association for Artificial Intelligence Spring Symposium, pages 51-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>Pedersen</author>
<author>F</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Proc. of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<contexts>
<context position="33633" citStr="Kupiec et al., 1995" startWordPosition="5899" endWordPosition="5902">cument analysis to the identification and extraction of certain core entities and facts in a document, while work on template-driven, knowledgebased summarization to date is hardly domain or genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (S</context>
</contexts>
<marker>Kupiec, Pedersen, F, 1995</marker>
<rawString>J. Kupiec, Pedersen, and F. . Chen. 1995. A trainable document summarizer. In Proc. of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 68-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM journal,</journal>
<pages>2--1</pages>
<contexts>
<context position="4840" citStr="Luhn, 1958" startWordPosition="804" endWordPosition="805">greed to a visit by a team of [U.S.] experts headed by Richard Witt, national director of the Federal Emergency Management Agency. Figure 1: The document titled &apos;Two Americans known dead in Japan quake&apos; Figure 1 is the document whose topic is &apos;Kobe Japan quake&apos;, and the subject of the document (event 31 words) is &apos;Two Americans known dead in Japan quake&apos;. Underlined words denote a topic, and the words marked with &apos;[ ]&apos; are events. of Figure 1 is paragraph id. Like Lulm&apos;s technique of keyword extraction, our method assumes that an event associated with a document appears throughout paragraphs (Luhn, 1958), but a topic does not: This is because an event is the subject of a document. itself, while a topic is an event, along with all directly related events. In Figure 1, event words &apos;Americans&apos; and &apos;U.S.&apos;, for instance, appears across paragraphs, while a topic word, for example, &apos;Kobe&apos; appears only the third paragraph. Let us consider further a broad coverage domain which consists of a small number of sample news documents about the same topic, &apos;Kobe Japan quake&apos;. Figure 2 and 3 are documents with &apos;Kobe Japan quake&apos;. (1-1) Quake collapses buildings in central Japan 1. At least two people died and</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H. P. Luhn. 1958. The automatic creation of literature abstracts. IBM journal, 2(1):159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Multi-document summarization by graph search and matching.</title>
<date>1997</date>
<booktitle>In Proc. of the 15th National Conference on Artificial Intelligence,</booktitle>
<pages>622--628</pages>
<contexts>
<context position="34933" citStr="Mani and Bloedorn, 1997" startWordPosition="6087" endWordPosition="6090">ti-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 1999). Their technique seems sensible. However, as she admits, (i) the order the information should not only depend on topic covered, (ii) background information that helps clarify related information should be placed first. More seriously, as Barzilay and Mani claim, summarization of multiple documents requires information about similarities and differences across documents. Therefore it is difficult to identify these information using a single-document summarizer technique (Mani and Bloedorn, 1997), (Barzilay et al., 1999). A method proposed by Mani et. al. deal with the problem, i.e. they tried to detect the similarities and differences in information content among documents (Mani and Bloedorn, 1997). They used a spreading activation algorithm and graph matching in order to identify similarities and differences across documents. The output is presented as a set of paragraphs with similar and unique words highlighted. However, if the same information is men4 8 16 Num 37 Table 4: The results of Key Paragraph Extraction Num Accuracy %10 %20 Total Para Correct(%) Para Correct(%) Para Corre</context>
</contexts>
<marker>Mani, Bloedorn, 1997</marker>
<rawString>I. Mani and E. Bloedorn. 1997. Multi-document summarization by graph search and matching. In Proc. of the 15th National Conference on Artificial Intelligence, pages 622-628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>T Firmin</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC text summarization evaluation.</title>
<date>1999</date>
<booktitle>In Proc. of Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>77--85</pages>
<contexts>
<context position="37683" citStr="Mani et al., 1999" startWordPosition="6535" endWordPosition="6538">(Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). However, while Barzilay&apos;s approach used paraphrasing rules to eliminate redundancy in a summary, we proposed domain dependency of words to address robustness of the technique. 7 Conclusion In this paper, we proposed a method for extracting key paragraph for summarization based on distinction between. a topic and an event. The results showed that the average accuracy was 68.1% when we used the TDT1 corpus. TIPSTER Text Summarization Evaluation (SUMMAC) proposed various methods for evaluating document summarization and tasks (Mani et al., 1999). Of these, participants submitted two summaries: a fixed-length summary limited to 10% of the length of the source, and a summary which was not limited in length. Future work includes quantitative and qualitative evaluation. In addition, our method used single words rather than phrases. These phrases, however, would be helpful to resolve ambiguity and reduce a lot of noise, i.e. yield much better accuracy. We plan to apply our method to phrase-based topic and event extraction, then turn to focus on the problem that how to form the actual summary. Acknowledgments The authors would like to than</context>
</contexts>
<marker>Mani, Firmin, Sundheim, 1999</marker>
<rawString>I. Mani, T. Firmin, and B. Sundheim. 1999. The TIPSTER SUMMAC text summarization evaluation. In Proc. of Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 77-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From discourse structures to text summaries.</title>
<date>1997</date>
<booktitle>In Proc. of ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<contexts>
<context position="33764" citStr="Marcu, 1997" startWordPosition="5920" endWordPosition="5921">edgebased summarization to date is hardly domain or genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and </context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D. Marcu. 1997. From discourse structures to text summaries. In Proc. of ACL Workshop on Intelligent Scalable Text Summarization, pages 82-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
<author>D R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proc. of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<contexts>
<context position="37106" citStr="McKeown and Radev, 1995" startWordPosition="6441" endWordPosition="6444">proach dealing with this problem is Barzilay et. al&apos;s approach (Barzilay et al., 1999). They used paraphrasing ndes which are manually derived from the result of syntactic analysis to identify theme intersection and used language generation to reformulate them as a coherent summary. While promising to obtain high accuracy, the result of summarization task has not been reported. Like Mani and Barzilay&apos;s techniques, our approach focuses on the problem that how to identify differences and similarities across documents, rather than the problem that how to form the actual suinmary (Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). However, while Barzilay&apos;s approach used paraphrasing rules to eliminate redundancy in a summary, we proposed domain dependency of words to address robustness of the technique. 7 Conclusion In this paper, we proposed a method for extracting key paragraph for summarization based on distinction between. a topic and an event. The results showed that the average accuracy was 68.1% when we used the TDT1 corpus. TIPSTER Text Summarization Evaluation (SUMMAC) proposed various methods for evaluating document summarization and tasks (Mani et al., 1999). Of these, participant</context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>K. R. McKeown and D. R.. Radev. 1995. Generating summaries of multiple news articles. In Proc. of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 74-82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Papka</author>
<author>J Allan</author>
<author>V Lavrenko</author>
</authors>
<title>UMASS approaches to detection and tracking at TDT2.</title>
<date>1999</date>
<booktitle>In Proc. of the DARPA Broadcast News Workshop.</booktitle>
<contexts>
<context position="36059" citStr="Papka et al., 1999" startWordPosition="6269" endWordPosition="6272">ey Paragraph Extraction Num Accuracy %10 %20 Total Para Correct(%) Para Correct(%) Para Correct(%) 2 58 44(75.8) 117 91(77.7) 175 135(77.1) 4 107 80(74.7) 214 160(74.7) 321 240(74.7) 8 202 138(68.3) 404 278(68.8) 606 416(68.6) 16 281 175(62.2) 563 361(64.1) 844 536(63.5) Total 648 437(67.4) 1,298 890(68.5) 1,946 1,327(68.1) -tioned several times in different documents, much of the summary will be redundant. Allan et. al. also address the problem and proposed a method for event tracking using cornmoTz words and surprising features by supplementing the corpus statistics (Allan and Papka, 1998) (Papka et al., 1999). One of the purpose of this study is to make a distinction between an event and an event class using surprising features. Here event class features are broad news areas such as politics, death, destruction and warfare. The idea is considered to be necessary to obtain high accuracy, while Allan claims that the surprising words do not provide a broad enough coverage to capture all documents on the event. A more recent approach dealing with this problem is Barzilay et. al&apos;s approach (Barzilay et al., 1999). They used paraphrasing ndes which are manually derived from the result of syntactic analy</context>
</contexts>
<marker>Papka, Allan, Lavrenko, 1999</marker>
<rawString>R. Papka, J. Allan, and V. Lavrenko. 1999. UMASS approaches to detection and tracking at TDT2. In Proc. of the DARPA Broadcast News Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>K R McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--3</pages>
<contexts>
<context position="37133" citStr="Radev and McKeown, 1998" startWordPosition="6445" endWordPosition="6448">oblem is Barzilay et. al&apos;s approach (Barzilay et al., 1999). They used paraphrasing ndes which are manually derived from the result of syntactic analysis to identify theme intersection and used language generation to reformulate them as a coherent summary. While promising to obtain high accuracy, the result of summarization task has not been reported. Like Mani and Barzilay&apos;s techniques, our approach focuses on the problem that how to identify differences and similarities across documents, rather than the problem that how to form the actual suinmary (Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). However, while Barzilay&apos;s approach used paraphrasing rules to eliminate redundancy in a summary, we proposed domain dependency of words to address robustness of the technique. 7 Conclusion In this paper, we proposed a method for extracting key paragraph for summarization based on distinction between. a topic and an event. The results showed that the average accuracy was 68.1% when we used the TDT1 corpus. TIPSTER Text Summarization Evaluation (SUMMAC) proposed various methods for evaluating document summarization and tasks (Mani et al., 1999). Of these, participants submitted two summaries: </context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>D. R. Radev and K. R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469-500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>J Allan</author>
<author>C Buckley</author>
<author>A Singhal</author>
</authors>
<title>Automatic analysis, theme generation, and summarization of machine-readable texts.</title>
<date>1991</date>
<journal>Science,</journal>
<pages>164--1421</pages>
<contexts>
<context position="33673" citStr="Salton et al., 1991" startWordPosition="5905" endWordPosition="5908">d extraction of certain core entities and facts in a document, while work on template-driven, knowledgebased summarization to date is hardly domain or genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method f</context>
</contexts>
<marker>Salton, Allan, Buckley, Singhal, 1991</marker>
<rawString>G. Salton, J. Allan, C. Buckley, and A. Singhal. 1991. Automatic analysis, theme generation, and summarization of machine-readable texts. Science, 164:1421-1426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Sparck</author>
</authors>
<title>What might be in a summary?</title>
<date>1993</date>
<booktitle>In Proc. of Information Retrieval93,</booktitle>
<pages>9--26</pages>
<contexts>
<context position="37079" citStr="Sparck, 1993" startWordPosition="6439" endWordPosition="6440">A more recent approach dealing with this problem is Barzilay et. al&apos;s approach (Barzilay et al., 1999). They used paraphrasing ndes which are manually derived from the result of syntactic analysis to identify theme intersection and used language generation to reformulate them as a coherent summary. While promising to obtain high accuracy, the result of summarization task has not been reported. Like Mani and Barzilay&apos;s techniques, our approach focuses on the problem that how to identify differences and similarities across documents, rather than the problem that how to form the actual suinmary (Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). However, while Barzilay&apos;s approach used paraphrasing rules to eliminate redundancy in a summary, we proposed domain dependency of words to address robustness of the technique. 7 Conclusion In this paper, we proposed a method for extracting key paragraph for summarization based on distinction between. a topic and an event. The results showed that the average accuracy was 68.1% when we used the TDT1 corpus. TIPSTER Text Summarization Evaluation (SUMMAC) proposed various methods for evaluating document summarization and tasks (Mani et al., 1</context>
</contexts>
<marker>Sparck, 1993</marker>
<rawString>K. J. Sparck. 1993. What might be in a summary? In Proc. of Information Retrieval93, pages 9-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G C Stein</author>
<author>T Strzalkowski</author>
<author>G B Wise</author>
</authors>
<title>Summarizing multiple documents using text extraction and interactive clustering.</title>
<date>1999</date>
<booktitle>In Proc. of the Pacific Association for Computational Linguistics1999,</booktitle>
<pages>200--208</pages>
<contexts>
<context position="14915" citStr="Stein et al., 1999" startWordPosition="2573" endWordPosition="2576">r of n paragraphs (see Figure 4). pi is an element of di. (5) shows that t frequently appears in the i-th document di rather than paragraphs pi ( 1 &lt; j &lt; n). On the other hand, if t satisfies formula (6) and (7), then propose t as a topic. DispPt &gt; DispDt (6) for all di E D. pit exists such that Devpit &gt; Devdit (7) In formula (7), D consists of the number of rn documents (see Figure 5). (7) denotes that t frequently appears in the particular paragraph pi rather than the document di which includes pi. 4 Key Paragraph Extraction The summarization task in this paper is paragraphbased extraction (Stein et al., 1999). Basically, paragraphs which include not only event words but also topic words are considered to be significant .paragraphs. The basic algorithm works as follows: 1. For each document, extract topic and event words. 2. Determine the paragraph weights for all paragraphs in the documents: (a) Compute the sum of topic weights over the total number of topic words for each paragraph. (b) Compute the sum of event weights over the total number of event words for each pathgraph. A topic and an event weights are calculated by using Detidit in formula (3). Here, t is a topic or an event and i is the i-</context>
<context position="34433" citStr="Stein et al., 1999" startWordPosition="6015" endWordPosition="6018">d Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 1999). Their technique seems sensible. However, as she admits, (i) the order the information should not only depend on topic covered, (ii) background information that helps clarify related information should be placed first. More seriously, as Barzilay and Mani claim, summarization of multiple documents requires information about similarities and differences across documents. Therefore it is difficult to identify these information using a single-document summarizer technique (Mani and Bloedorn, 1997), (Barzilay et al., 1999). A method proposed by Mani et. al. deal with the problem, i.e. they tried </context>
</contexts>
<marker>Stein, Strzalkowski, Wise, 1999</marker>
<rawString>G. C. Stein, T. Strzalkowski, and G. B. Wise. 1999. Summarizing multiple documents using text extraction and interactive clustering. In Proc. of the Pacific Association for Computational Linguistics1999, pages 200-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Stralkowsik</author>
<author>G C Stein</author>
<author>G B Wise</author>
</authors>
<title>A text-extraction based summarizer.</title>
<date>1998</date>
<booktitle>In Proc. of Tipster Workshop.</booktitle>
<contexts>
<context position="34229" citStr="Stralkowsik et al., 1998" startWordPosition="5983" endWordPosition="5986">ibution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 1999). Their technique seems sensible. However, as she admits, (i) the order the information should not only depend on topic covered, (ii) background information that helps clarify related information should be placed first. More seriously, as Barzilay and Mani claim, summarization of multiple documents requires information about similarities and differences across documents. Therefore it is diffic</context>
</contexts>
<marker>Stralkowsik, Stein, Wise, 1998</marker>
<rawString>T. Stralkowsik, G. C. Stein, and G. B. Wise. 1998. A text-extraction based summarizer. In Proc. of Tipster Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Stralkowski</author>
<author>G C Stein</author>
<author>G B Wise</author>
</authors>
<title>Getracker: A robust, lightweight topic tracking system.</title>
<date>1999</date>
<booktitle>In Proc. of the DARPA Broadcast News Workshop.</booktitle>
<contexts>
<context position="34257" citStr="Stralkowski et al., 1999" startWordPosition="5987" endWordPosition="5990">), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method first summarizes each document of multi-document, then groups the summaries in clusters and finally, orders these summaries in a logical way (Stein et al., 1999). Their technique seems sensible. However, as she admits, (i) the order the information should not only depend on topic covered, (ii) background information that helps clarify related information should be placed first. More seriously, as Barzilay and Mani claim, summarization of multiple documents requires information about similarities and differences across documents. Therefore it is difficult to identify these inform</context>
</contexts>
<marker>Stralkowski, Stein, Wise, 1999</marker>
<rawString>T. Stralkowski, G. C. Stein, and G. B. Wise. 1999. Getracker: A robust, lightweight topic tracking system. In Proc. of the DARPA Broadcast News Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufell</author>
<author>M Moens</author>
</authors>
<title>Sentence extraction as a classification task.</title>
<date>1997</date>
<booktitle>In Proc. of ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="33700" citStr="Teufell and Moens, 1997" startWordPosition="5909" endWordPosition="5912"> core entities and facts in a document, while work on template-driven, knowledgebased summarization to date is hardly domain or genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al., 1999). Their method first summarizes each docume</context>
</contexts>
<marker>Teufell, Moens, 1997</marker>
<rawString>S. Teufell and M. Moens. 1997. Sentence extraction as a classification task. In Proc. of ACL Workshop on Intelligent Scalable Text Summarization, pages 58-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
</authors>
<title>Fast generation of abstracts from general domain text corpora by extracting relevant sentences.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>986--989</pages>
<contexts>
<context position="33650" citStr="Zechner, 1996" startWordPosition="5903" endWordPosition="5904">identification and extraction of certain core entities and facts in a document, while work on template-driven, knowledgebased summarization to date is hardly domain or genre-independent (Boguraev and Kennedy, 1997). The alternative approach largely escapes this constraint, by viewing the task as one of identifying certain passages(typically sentences) which, by some metric, are deemed to be the most representative of the document&apos;s content. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Kupiec et al., 1995), (Zechner, 1996), (Salton et al., 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzilay and Elhadad, 1997). All of their results demonstrate that passage extraction techniques are a useful first step in document summarization, although most of them have focused on a single document. Some researchers have started to apply a single-document summarization technique to multidocument. Stein et. al. proposed a method for summarizing multi-document using single-document summarizer (Stralkowsik et al., 1998), (Stralkowski et al.</context>
</contexts>
<marker>Zechner, 1996</marker>
<rawString>K. Zechner. 1996. Fast generation of abstracts from general domain text corpora by extracting relevant sentences. In Proc. of the 16th International Conference on Computational Linguistics, pages 986-989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>