<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023012">
<bodyText confidence="0.482095">
‘How was your day?’ An architecture for multimodal ECA systems
</bodyText>
<note confidence="0.9922926">
Raúl Santos de la Markku Turunen Jaakko Hakulinen Debora Field
Cámara Univ. of Tampere Univ. of Tampere Computer Science
Telefónica I+D Kanslerinrinne 1 Kanslerinrinne 1 Univ. of Sheffield
C/ Emilio Vargas 6 FI-33014, Finland FI-33014, Finland S1 4DP, UK
28043 Madrid, Spain mturunen@ jh@cs.uta.fi d.field@shef.
</note>
<email confidence="0.64684">
e.rsai@tid.es cs.uta.fi ac.uk
</email>
<sectionHeader confidence="0.987747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999828625">
Multimodal conversational dialogue sys-
tems consisting of numerous software
components create challenges for the un-
derlying software architecture and devel-
opment practices. Typically, such sys-
tems are built on separate, often pre-
existing components developed by dif-
ferent organizations and integrated in a
highly iterative way. The traditional dia-
logue system pipeline is not flexible
enough to address the needs of highly in-
teractive systems, which include parallel
processing of multimodal input and out-
put. We present an architectural solution
for a multimodal conversational social
dialogue system.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954930232558">
Multimodal conversational dialogue applica-
tions with embodied conversational agents
(ECas) are complex software systems consisting
of multiple software components. They require
much of architectural solutions and development
approaches compared to traditional spoken dia-
logue systems. These systems are mostly assem-
bled from separate, often pre-existing compo-
nents developed by different organizations. For
such systems, the simple pipeline architecture is
not a viable choice. When multimodal systems
are built, software architecture should be flexible
enough to enable the system to support natural
interaction with features such as continuous and
timely multimodal feedback and interruptions by
both participants. Such features require parallel
processing components and flexible communica-
tion between the components. Furthermore, the
architecture should provide an open sandbox,
where the components can be efficiently com-
bined and experimented with during the iterative
development process.
The HWYD (‘How was your day?’) Compan-
ion system is a multimodal virtual companion
capable of affective social dialogue and for
which we have developed a custom novel archi-
tecture. The application features an ECA which
exhibits facial expressions and bodily move-
ments and gestures. The system is rendered on a
HD screen with the avatar being presented as
roughly life-size. The user converses with the
ECA using a wireless microphone. A demonstra-
tion video of the virtual companion in action is
available online1.
The application is capable of long social con-
versations about events that take place during a
user’s working day. The system monitors the
user’s emotional state on acoustic and linguistic
levels, generates affective spoken responses, and
attempts to positively influence the user’s emo-
tional state. The system allows for user initiative,
it asks questions, makes comments and sugges-
tions, gives warnings, and offers advice.
</bodyText>
<sectionHeader confidence="0.983881" genericHeader="method">
2 Communications framework
</sectionHeader>
<bodyText confidence="0.999825266666667">
The HWYD Companion system architecture em-
ploys Inamode, a loosely coupled multi-hub
framework which facilitates a loose, non-
hierarchical connection between any number of
components. Every component in the system is
connected to a repeating hub which broadcasts
all messages sent to it to all connected compo-
nents. The hub and the components connected to
it form a single domain. Facilitators are used to
forward messages between different domains
according to filtering rules. During development,
we have experimented with a number of Facilita-
tors to create efficient and simple domains to
overcome problems associated with single-hub
systems. For example, multiple hubs allow the
</bodyText>
<footnote confidence="0.978997">
1 http://www.youtube.com/
watch?v=BmDMNguQUmM
</footnote>
<affiliation confidence="0.4881445">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 47–50,
The University of Tokyo, September 24-25, 2010. p@c 2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999811">
47
</page>
<bodyText confidence="0.999765666666666">
reduction of broadcast messages, which is for
example used in the audio processing pipeline,
where a dedicated hub allows very rapid message
broadcast (nearly 100 messages per second are
exchanged) without compromising the stability
of the system by flooding the common pipeline.
For communication between components, a
lightweight communication protocol is used to
support components implemented in various
programming languages. A common XML mes-
sage “envelope” specifies the basic format of
message headers as seen in Figure 1.
</bodyText>
<figureCaption confidence="0.9962">
Figure 1: System message XML format
</figureCaption>
<bodyText confidence="0.975807833333333">
.
Mandatory elements in the envelope (top
block) are necessary so other modules can iden-
tify the purpose of the message and its contents
upon a shallow inspection. These include the
sender component and a unique message id. Ad-
ditional envelope fields elements include: mes-
sage type, turn id, dialogue segment identifier,
recipient identifier, and a list of message identi-
fiers corresponding to the previous messages in
the current processing sequence.
For system-wide and persistent knowledge
management, a central XML-database allows the
system to have inter-session and intra-session
‘memory’ of past events and dialogues. This da-
tabase (KB) includes information such the user
and dialogue models, processing status of mod-
ules, and other system-wide information.
</bodyText>
<sectionHeader confidence="0.891158" genericHeader="method">
3 Data flow in the architecture
</sectionHeader>
<bodyText confidence="0.999495571428572">
To maximize the naturalness of the ECA’s inter-
action, the system implements parallel process-
ing paths. It also makes use of a special module,
the Interruption Manager (IM), to control
components in situations where regular process-
ing procedure must be deviated from. In addi-
tion, there are ‘long’ and ‘short’ processing se-
quences from user input to system output. Both
‘loops’ operate simultaneously. The Main Dia-
logue (‘long’) Loop, which is the normal proc-
essing path, is indicated by the bold arrows in
Fig. 2, and includes all system components ex-
cept the IM. The dotted arrows signal the devia-
tions to this main path that are introduced by the
</bodyText>
<figure confidence="0.9955246">
Multimodal Fission
Manager (MFM)
Natural Language
Generation (NLG)
Dialogue Act
Tagger (DAT)
Sentiment
Analyzer (SA)
Dialogue
Manager(DM)
</figure>
<figureCaption confidence="0.995871">
Figure 2:HWYD Companion main modules
</figureCaption>
<bodyText confidence="0.999986384615385">
interruption management and feedback loops.
The system has an activity detector in the input
subsystem that is active permanently and analy-
ses user input in real-time. If there is a detection
of user input at the same time as the ECA is talk-
ing, this module triggers a signal that is captured
by the IM. The IM, which tracks the activity of
the rest of the modules in the system, has a set of
heuristics that are examined each time this trig-
gering signal is detected. If any heuristic
matches, the system decides there has been a
proper user interruption and decides upon a se-
ries of actions to recover from the interruption.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="method">
4 Module Processing Procedure
</sectionHeader>
<bodyText confidence="0.99515945">
The first stage in the processing is the acoustic
processing. User speech is processed by the
Acoustic Analyzer, the Automatic Speech Rec-
ognizer, and the Acoustic Emotion Classifier
simultaneously for maximum responsiveness.
The Acoustic Analyzer (AA) extracts low-
level features (pitch, intensity and the probability
that the input was from voiced speech) from the
acoustic signal at frequent time intervals (typi-
cally 10 milliseconds). Features are passed to the
Acoustic Turn-Taking Detector in larger buffers
(a few hundred milliseconds) together with time-
stamps. AA is implemented in TCL using Snack
toolkit (http://www.speech.kth.se/snack/).
The Acoustic Turn-Taking detector (ATT)
is a Java module, which estimates when the user
has finished a turn by comparing intensity pause
lengths and pitch information of user speech to
configurable empirical thresholds. ATT also de-
cides whether the user has interrupted the system
</bodyText>
<figure confidence="0.99923780952381">
Affective Strategy
(ASM)
Text-to-Speech
(TTS)
Avatar
(ECA)
Natural Language
Understanding (NLU)
Kne.W.
Ba= UM
Interruption
Manager(IM)
Automatic Speech
Recognition (ASR)
Emotional
Model (EM)
Acoustic
Analyzer (AA)
Acoustic Turn-
Taking (ATT)
Acoustic Emotion Classifier (AEC)
</figure>
<page confidence="0.996962">
48
</page>
<bodyText confidence="0.99856203773585">
(‘barge-in’), while ignoring shorter backchannel-
ling phrases (Crook et al. (2010)). Interruption
messages are passed to the Interruption Manager.
ATT receives a message from the ECA module
when the system starts or stops speaking.
Dragon Naturally Speaking Automatic
Speech Recognition (ASR) system is used to
provide real-time large vocabulary speech recog-
nition. Per-user acoustic adaptation is used to
improve recognition rates. ASR provides N-best
lists, confidence scores, and phrase hypotheses.
The Acoustic Emotion Classifier (AEC)
component (EmoVoice (Vogt et al. (2008)) cate-
gorizes segments of user speech into five va-
lence+arousal categories, also applying a confi-
dence score. The Interruption Manager monitors
the messages of the AEC to include emotion-
related information into feedback loop messages
sent to the ECA subsystem. This allows rapid
reactions to the user mood.
The Sentiment Analyzer (SA) labels ASR
output strings with sentiment information at
word and sentence levels using valence catego-
ries positive, neutral and negative. The SA uses
the AFFECTiS Sentiment Server, which is a gen-
eral purpose .NET SOAP XML service for
analysis and scoring of author sentiment.
The Emotional Model (EM), written in Lisp,
fuses information from the AEC and SA. It
stores a globally accessible emotional representa-
tion of the user for other system modules to
make use of. Affective fusion is rule-based, pre-
fers the SA’s valence information, and outputs
the same five valence+arousal categories as used
in the AEC. The EM can also serve as a basis for
temporal integration (mood representation) as
part of the affective content of the User Model. It
also combines the potentially different segmenta-
tions by the ASR and AEC.
The User Model (UM) stores facts about the
user as objects and associated attributes. The in-
formation contained in the User Model is used by
other system modules, in particular by Dialogue
Manager and Affective Strategy Module.
The Dialogue Act Tagger and Segmenter
(DAT), written in C under Linux, uses the ATT
results to compile all ASR results corresponding
to each user turn. DAT then segments the com-
bined results into semantic units and labels each
with a dialogue act (DA) tag (from a subset of
SWBD-DAMSL (Jurafsky et al. (2001)). A Sto-
chastic Machine Learning model combining
Hidden Markov Model (HMM) and N-grams is
used in a manner analogous to Martínez-
Hinarejos et al. (2006). The N-grams yield the
probability of a possible DA tag given the previ-
ous ones. The Viterbi algorithm is used to find
the most likely sequence of DA tags.
The Natural Language Understanding
(NLU) component, implemented in Prolog, pro-
duces a logical form representing the semantic
meaning of a user turn. The NLU consists of a
part-of-speech tagger, a Noun Phrase and Verb
Group chunker, a named-entity classification
component (rule-based), and a set of pattern-
matching rules which recognize major gram-
matical relationships (subject, direct object, etc.)
The resulting shallow-parsed text is further proc-
essed using pattern-matching rules. These recog-
nize configurations of entity and relation relevant
to the templates needed by the Dialogue Man-
ager, the EM, and the Affective Strategy Module.
The Dialogue Manager (DM), written in Java
and Prolog, combines the SA and NLU results,
decides on the system&apos;s next utterance and identi-
fies salient objects for the Affective Strategy
Module. The DM maintains an information state
containing information about concepts under dis-
cussion, as well as the system&apos;s agenda of current
conversational goals.
One of the main features of the HWYD Com-
panion is its ability to positively influence the
user’s mood through its Affective Strategy
Module (ASM). This module appraises the
user’s situation, considering the events reported
in the user turn and its (bi-modal) affective ele-
ments. From this appraisal, the ASM generates a
long multi-utterance turn. Each utterance imple-
ments communicative acts constitutive of the
strategy. ASM generates influence operators
which are passed to the Natural Language Gen-
eration module. ASM output is triggered when
the system has learned enough about a particular
event to warrant affective influence. As input,
ASM takes information extraction templates de-
scribing events, together with the emotional data
attached. ASM is a Hierarchical Task Network
(HTN) Planner implemented in Lisp.
The Natural Language Generator (NLG),
written in Lisp, produces linguistic surface forms
from influence operators produced by the ASM.
These operators correspond to communicative
actions taking the form of performatives. NLG
uses specific rhetorical structures and constructs
associated with humour, and uses emotional TTS
expressions through specific lexical choice.
</bodyText>
<page confidence="0.999492">
49
</page>
<sectionHeader confidence="0.993964" genericHeader="method">
5 Multimodal ECA Control
</sectionHeader>
<bodyText confidence="0.999959977777778">
Multimodal control of the ECA, which consists
of a tightly-synchronized naturalistic avatar and
affective Text-To-Speech (TTS) generation, is
highly challenging from an architectural view-
point, since the coordinating component needs to
be properly synchronized with the rest of the sys-
tem, including both the main dialogue loop and
the feedback and interruption loops.
The system Avatar is in charge of generating a
three-dimensional, human-like character to serve
as the system’s ‘face’. The avatar is connected to
the TTS, and the speech is synchronized with the
lip movements. The prototype is currently using
the HaptekTM 3D avatar engine running inside a
web browser. The Haptek engine provides a talk-
ing head and torso along with a low level API to
control its interaction with any SAPI-compliant
TTS subsystem, and also allows some manipula-
tion of the character animation. An intermediate
layer consisting of a Java applet and Javascript
code embeds the rendered avatar in a web page
and provides connectivity with the Multimodal
Fission Manager. We intend to replace the cur-
rent avatar with a photorealistic avatar under de-
velopment within the project consortium.
LoquendoTM TTS SAPI synthesizer is used to
vocalize system turns. The TTS engine works in
close connection with the ECA software using
the SAPI interface. TTS includes custom para-
linguistic events for producing expressive
speech. TTS is based on the concatenative tech-
nique with variable length acoustic units.
The Multimodal Fission Manager (MFM) con-
trols the Avatar and the TTS engine, enabling the
system to construct complex communicative acts
that chain together series of utterances and ges-
tures. It offers FML-standard-based syntax to
make the avatar perform a series of body and
facial gestures.
The system features a template-based input
mode in which a module can call ECA to per-
form actions without having to build a full FML-
based XML message. This is intended to be used
in the feedback loops, for example, to convey the
impression that the ECA is paying attention.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999957666666667">
We have presented an advanced multimodal dia-
logue system that challenges the usual pipeline-
based implementation. To do so, it leverages on
an architecture that provides the means for a
flexible component interconnection, that can ac-
comodate the needs of a system using more than
one processing path for its data. We have shown
how this has enabled us to implement complex
behavior such as interrupt and short loop han-
dling. We are currently expanding coverage and
will carry out an evaluation with real users this
September.
</bodyText>
<sectionHeader confidence="0.993037" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.86096675">
This work was funded by Companions, a Euro-
pean Commission Sixth Framework Programme
Information Society Technologies Integrated
Project (IST-34434).
</bodyText>
<sectionHeader confidence="0.988317" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999760282051282">
Vogt, T., André, E. and Bee, N. 2008. EmoVoice – A
framework for online recognition of emotions from
voice. In: Proc. Workshop on Perception and In-
teractive Technologies for Speech-Based Systems,
Springer, Kloster Irsee, Germany.
Cavazza, M., Smith, C., Charlton, D., Crook, N.,
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive
Dialogue based on a Narrative Theory: an ECA
Implementation, Proc. 5th Int. Conf. on Persuasive
Technology (to appear).
Hernández, A., López, B., Pardo, D., Santos, R.,
Hernández, L., Relaiio Gil, J. and Rodríguez, M.C.
2008 Modular definition of multimodal ECA
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp,
S., Marsella, S., Pelachaud, C., and Vilhjálmsson,
H. (Eds.), AAMAS 2008 Workshop on Functional
Markup Language.
Crook, N., Smith, C., Cavazza, M., Pulman, S.,
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent. In
Proc. AAMAS 2010.
Wagner J., André, E., and Jung, F. 2009 Smart sensor
integration: A framework for multimodal emotion
recognition in real-time. In Affective Computing
and Intelligent Interaction 2009.
Cavazza, M., Pizzi, D., Charles, F., Vogt, T. André,
E. 2009 Emotional input for character-based in-
teractive storytelling AAMAS (1) 2009: 313-320.
Jurafsky, D. Shriberg, E., Biasca, D. 2001
Switchboard swbd-damsl shallow- discourse-
function annotation coders manual. Tech. Rep. 97
-01, University of Colorado Institute of Cognitive
Science
Martínez-Hinarejos, C.D., Granell, R., Benedí, J.M.
2006. Segmented and unsegmented dialogue-act
annotation with statistical dialogue models. Proc.
COLING/ACL Sydney, Australia, pp. 563-570.
</reference>
<page confidence="0.997683">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753419">
<title confidence="0.999657">How was your day?’ An architecture for multimodal ECA systems</title>
<author confidence="0.99662">Raúl Santos de_la Markku Turunen Jaakko Hakulinen Debora Field</author>
<affiliation confidence="0.923861">Cámara Univ. of Tampere Univ. of Tampere Computer Science Telefónica I+D Kanslerinrinne 1 Kanslerinrinne 1 Univ. of Sheffield</affiliation>
<address confidence="0.972807">C/ Emilio Vargas 6 FI-33014, Finland FI-33014, Finland S1 4DP, UK 28043 Madrid, Spain mturunen@ jh@cs.uta.fi d.field@shef.</address>
<email confidence="0.912299">e.rsai@tid.escs.uta.fiac.uk</email>
<abstract confidence="0.999343882352941">Multimodal conversational dialogue systems consisting of numerous software components create challenges for the underlying software architecture and development practices. Typically, such systems are built on separate, often preexisting components developed by different organizations and integrated in a highly iterative way. The traditional dialogue system pipeline is not flexible enough to address the needs of highly interactive systems, which include parallel processing of multimodal input and output. We present an architectural solution for a multimodal conversational social dialogue system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Vogt</author>
<author>E André</author>
<author>N Bee</author>
</authors>
<title>EmoVoice – A framework for online recognition of emotions from voice. In:</title>
<date>2008</date>
<booktitle>Proc. Workshop on Perception and Interactive Technologies for Speech-Based Systems,</booktitle>
<publisher>Springer,</publisher>
<location>Kloster Irsee, Germany.</location>
<contexts>
<context position="8569" citStr="Vogt et al. (2008)" startWordPosition="1273" endWordPosition="1276">T) Acoustic Emotion Classifier (AEC) 48 (‘barge-in’), while ignoring shorter backchannelling phrases (Crook et al. (2010)). Interruption messages are passed to the Interruption Manager. ATT receives a message from the ECA module when the system starts or stops speaking. Dragon Naturally Speaking Automatic Speech Recognition (ASR) system is used to provide real-time large vocabulary speech recognition. Per-user acoustic adaptation is used to improve recognition rates. ASR provides N-best lists, confidence scores, and phrase hypotheses. The Acoustic Emotion Classifier (AEC) component (EmoVoice (Vogt et al. (2008)) categorizes segments of user speech into five valence+arousal categories, also applying a confidence score. The Interruption Manager monitors the messages of the AEC to include emotionrelated information into feedback loop messages sent to the ECA subsystem. This allows rapid reactions to the user mood. The Sentiment Analyzer (SA) labels ASR output strings with sentiment information at word and sentence levels using valence categories positive, neutral and negative. The SA uses the AFFECTiS Sentiment Server, which is a general purpose .NET SOAP XML service for analysis and scoring of author </context>
</contexts>
<marker>Vogt, André, Bee, 2008</marker>
<rawString>Vogt, T., André, E. and Bee, N. 2008. EmoVoice – A framework for online recognition of emotions from voice. In: Proc. Workshop on Perception and Interactive Technologies for Speech-Based Systems, Springer, Kloster Irsee, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cavazza</author>
<author>C Smith</author>
<author>D Charlton</author>
<author>N Crook</author>
<author>J Boye</author>
<author>S Pulman</author>
<author>K Moilanen</author>
<author>D Pizzi</author>
<author>Santos de la Camara</author>
<author>R Turunen</author>
<author>M</author>
</authors>
<title>Persuasive Dialogue based on a Narrative Theory: an ECA Implementation,</title>
<date>2010</date>
<booktitle>Proc. 5th Int. Conf. on Persuasive Technology</booktitle>
<note>(to appear).</note>
<marker>Cavazza, Smith, Charlton, Crook, Boye, Pulman, Moilanen, Pizzi, Camara, Turunen, M, 2010</marker>
<rawString>Cavazza, M., Smith, C., Charlton, D., Crook, N., Boye, J., Pulman, S., Moilanen, K., Pizzi, D., Santos de la Camara, R., Turunen, M. 2010 Persuasive Dialogue based on a Narrative Theory: an ECA Implementation, Proc. 5th Int. Conf. on Persuasive Technology (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hernández</author>
<author>B López</author>
<author>D Pardo</author>
<author>R Santos</author>
<author>L Hernández</author>
<author>Relaiio Gil</author>
<author>J</author>
<author>M C Rodríguez</author>
</authors>
<title>Modular definition of multimodal ECA communication acts to improve dialogue robustness and depth of intention. In: Heylen,</title>
<date>2008</date>
<booktitle>AAMAS 2008 Workshop on Functional Markup Language.</booktitle>
<marker>Hernández, López, Pardo, Santos, Hernández, Gil, J, Rodríguez, 2008</marker>
<rawString>Hernández, A., López, B., Pardo, D., Santos, R., Hernández, L., Relaiio Gil, J. and Rodríguez, M.C. 2008 Modular definition of multimodal ECA communication acts to improve dialogue robustness and depth of intention. In: Heylen, D., Kopp, S., Marsella, S., Pelachaud, C., and Vilhjálmsson, H. (Eds.), AAMAS 2008 Workshop on Functional Markup Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Crook</author>
<author>C Smith</author>
<author>M Cavazza</author>
<author>S Pulman</author>
<author>R Moore</author>
<author>J Boye</author>
</authors>
<title>Handling User Interruptions in an Embodied Conversational Agent.</title>
<date>2010</date>
<booktitle>In Proc. AAMAS</booktitle>
<contexts>
<context position="8072" citStr="Crook et al. (2010)" startWordPosition="1203" endWordPosition="1206">king detector (ATT) is a Java module, which estimates when the user has finished a turn by comparing intensity pause lengths and pitch information of user speech to configurable empirical thresholds. ATT also decides whether the user has interrupted the system Affective Strategy (ASM) Text-to-Speech (TTS) Avatar (ECA) Natural Language Understanding (NLU) Kne.W. Ba= UM Interruption Manager(IM) Automatic Speech Recognition (ASR) Emotional Model (EM) Acoustic Analyzer (AA) Acoustic TurnTaking (ATT) Acoustic Emotion Classifier (AEC) 48 (‘barge-in’), while ignoring shorter backchannelling phrases (Crook et al. (2010)). Interruption messages are passed to the Interruption Manager. ATT receives a message from the ECA module when the system starts or stops speaking. Dragon Naturally Speaking Automatic Speech Recognition (ASR) system is used to provide real-time large vocabulary speech recognition. Per-user acoustic adaptation is used to improve recognition rates. ASR provides N-best lists, confidence scores, and phrase hypotheses. The Acoustic Emotion Classifier (AEC) component (EmoVoice (Vogt et al. (2008)) categorizes segments of user speech into five valence+arousal categories, also applying a confidence </context>
</contexts>
<marker>Crook, Smith, Cavazza, Pulman, Moore, Boye, 2010</marker>
<rawString>Crook, N., Smith, C., Cavazza, M., Pulman, S., Moore, R., and Boye, J. 2010 Handling User Interruptions in an Embodied Conversational Agent. In Proc. AAMAS 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wagner</author>
<author>E André</author>
<author>F Jung</author>
</authors>
<title>Smart sensor integration: A framework for multimodal emotion recognition in real-time.</title>
<date>2009</date>
<booktitle>In Affective Computing and Intelligent Interaction</booktitle>
<marker>Wagner, André, Jung, 2009</marker>
<rawString>Wagner J., André, E., and Jung, F. 2009 Smart sensor integration: A framework for multimodal emotion recognition in real-time. In Affective Computing and Intelligent Interaction 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cavazza</author>
<author>D Pizzi</author>
<author>F Charles</author>
<author>T André Vogt</author>
<author>E</author>
</authors>
<title>Emotional input for character-based interactive storytelling AAMAS</title>
<date>2009</date>
<pages>313--320</pages>
<marker>Cavazza, Pizzi, Charles, Vogt, E, 2009</marker>
<rawString>Cavazza, M., Pizzi, D., Charles, F., Vogt, T. André, E. 2009 Emotional input for character-based interactive storytelling AAMAS (1) 2009: 313-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shriberg Jurafsky</author>
<author>E Biasca</author>
<author>D</author>
</authors>
<title>Switchboard swbd-damsl shallow- discoursefunction annotation coders manual.</title>
<date>2001</date>
<tech>Tech. Rep. 97 -01,</tech>
<institution>University of Colorado Institute of Cognitive Science</institution>
<contexts>
<context position="10260" citStr="Jurafsky et al. (2001)" startWordPosition="1551" endWordPosition="1554">r Model. It also combines the potentially different segmentations by the ASR and AEC. The User Model (UM) stores facts about the user as objects and associated attributes. The information contained in the User Model is used by other system modules, in particular by Dialogue Manager and Affective Strategy Module. The Dialogue Act Tagger and Segmenter (DAT), written in C under Linux, uses the ATT results to compile all ASR results corresponding to each user turn. DAT then segments the combined results into semantic units and labels each with a dialogue act (DA) tag (from a subset of SWBD-DAMSL (Jurafsky et al. (2001)). A Stochastic Machine Learning model combining Hidden Markov Model (HMM) and N-grams is used in a manner analogous to MartínezHinarejos et al. (2006). The N-grams yield the probability of a possible DA tag given the previous ones. The Viterbi algorithm is used to find the most likely sequence of DA tags. The Natural Language Understanding (NLU) component, implemented in Prolog, produces a logical form representing the semantic meaning of a user turn. The NLU consists of a part-of-speech tagger, a Noun Phrase and Verb Group chunker, a named-entity classification component (rule-based), and a </context>
</contexts>
<marker>Jurafsky, Biasca, D, 2001</marker>
<rawString>Jurafsky, D. Shriberg, E., Biasca, D. 2001 Switchboard swbd-damsl shallow- discoursefunction annotation coders manual. Tech. Rep. 97 -01, University of Colorado Institute of Cognitive Science</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Martínez-Hinarejos</author>
<author>R Granell</author>
<author>J M Benedí</author>
</authors>
<title>Segmented and unsegmented dialogue-act annotation with statistical dialogue models.</title>
<date>2006</date>
<booktitle>Proc. COLING/ACL</booktitle>
<pages>563--570</pages>
<location>Sydney, Australia,</location>
<marker>Martínez-Hinarejos, Granell, Benedí, 2006</marker>
<rawString>Martínez-Hinarejos, C.D., Granell, R., Benedí, J.M. 2006. Segmented and unsegmented dialogue-act annotation with statistical dialogue models. Proc. COLING/ACL Sydney, Australia, pp. 563-570.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>