<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.143566">
<title confidence="0.990843">
Generating Image Captions using Topic Focused Multi-document
Summarization
</title>
<author confidence="0.985155">
Robert Gaizauskas
</author>
<affiliation confidence="0.832459">
Natural Language Processing Group
Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello, Sheffield, S1 4DP, UK
</affiliation>
<email confidence="0.987439">
R.Gaizauskas@sheffield.ac.uk
</email>
<bodyText confidence="0.999989176470588">
In the near future digital cameras will come
standardly equipped with GPS and compass and
will automatically add global position and direc-
tion information to the metadata of every picture
taken. Can we use this information, together with
information from geographical information sys-
tems and the Web more generally, to caption im-
ages automatically?
This challenge is being pursued in the TRIPOD
project (http://tripod.shef.ac.uk/) and in this talk
I will address one of the subchallenges this topic
raises: given a set of toponyms automatically gen-
erated from geo-data associated with an image, can
we use these toponyms to retrieve documents from
the Web and to generate an appropriate caption for
the image?
We begin assuming the toponyms name the prin-
cipal objects or scene contents in the image. Using
web resources (e.g. Wikipedia) we attempt to de-
termine the types of these things – is this a picture
of church? a mountain? a city? We have con-
structed a taxonomy of such image content types
using on-line collections of captioned images and
for each type in the taxonomy we have constructed
several collections of texts describing that type.
For example, we have a collection of captions de-
scribing churches and a collection of Wiki pages
describing churches. The intuition here is that
these collections are examples of, e.g. the sorts
of things people say in captions or in descriptions
of churches. These collections can then be used to
derive models of objects or scene types which in
turn can be used to bias or focus multi-document
summaries of new images of things of the same
</bodyText>
<note confidence="0.457886">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.971415333333333">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.9964729375">
type.
In the talk I report results of work we have
carried out to explore the hypothesis underlying
this approach, namely that brief multi-document
summaries generated as image captions by using
models of object/scene types to bias or focus con-
tent selection will be superior to generic multi-
document summaries generated for this purpose.
I describe how we have constructed an image con-
tent taxonomy, how we have derived text collec-
tions for object/scene types, how we have derived
object/scene type models from these collections
and how these have been used in multi-document
summarization. I also discuss the issue of how to
evaluate the resulting captions and present prelim-
inary results from one sort of evaluation.
</bodyText>
<page confidence="0.936905">
1
</page>
<note confidence="0.74143">
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, page 1
Manchester, August 2008
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.091823">
<title confidence="0.999254">Generating Image Captions using Topic Focused Summarization</title>
<author confidence="0.889275">Robert</author>
<affiliation confidence="0.744586">Natural Language Processing Department of Computer Science, University of Regent Court, 211 Portobello, Sheffield, S1 4DP,</affiliation>
<email confidence="0.823923">R.Gaizauskas@sheffield.ac.uk</email>
<abstract confidence="0.999735055555556">In the near future digital cameras will come standardly equipped with GPS and compass and will automatically add global position and direction information to the metadata of every picture taken. Can we use this information, together with information from geographical information systems and the Web more generally, to caption images automatically? This challenge is being pursued in the TRIPOD project (http://tripod.shef.ac.uk/) and in this talk I will address one of the subchallenges this topic raises: given a set of toponyms automatically generated from geo-data associated with an image, can we use these toponyms to retrieve documents from the Web and to generate an appropriate caption for the image? We begin assuming the toponyms name the principal objects or scene contents in the image. Using web resources (e.g. Wikipedia) we attempt to determine the types of these things – is this a picture of church? a mountain? a city? We have constructed a taxonomy of such image content types using on-line collections of captioned images and for each type in the taxonomy we have constructed several collections of texts describing that type. For example, we have a collection of captions describing churches and a collection of Wiki pages describing churches. The intuition here is that these collections are examples of, e.g. the sorts of things people say in captions or in descriptions of churches. These collections can then be used to derive models of objects or scene types which in turn can be used to bias or focus multi-document summaries of new images of things of the same Licensed under the Commons Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. type. In the talk I report results of work we have carried out to explore the hypothesis underlying this approach, namely that brief multi-document summaries generated as image captions by using models of object/scene types to bias or focus content selection will be superior to generic multidocument summaries generated for this purpose. I describe how we have constructed an image content taxonomy, how we have derived text collections for object/scene types, how we have derived object/scene type models from these collections and how these have been used in multi-document summarization. I also discuss the issue of how to evaluate the resulting captions and present preliminary results from one sort of evaluation.</abstract>
<note confidence="0.5662095">1 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, 1</note>
<address confidence="0.648866">Manchester, August 2008</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>