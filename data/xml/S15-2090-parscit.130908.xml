<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023565">
<title confidence="0.9989625">
Gradiant-Analytics: Training Polarity Shifters with CRFs for Message Level
Polarity Detection
</title>
<author confidence="0.984538">
H´ector Cerezo-Costas, Diego Celix-Salgado
</author>
<affiliation confidence="0.810981">
Gradiant - Galician Research and Development Center in Advanced Telecommunications
</affiliation>
<address confidence="0.924897">
Edificio CITEXVI, local 14
Vigo, Pontevedra 36310, SPA
</address>
<email confidence="0.997737">
{hcerezo, dcelix}@gradiant.org
</email>
<sectionHeader confidence="0.995813" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994048642857143">
In this paper we present our solution for ob-
taining sentiment at message-level of short
sentences. The system combines the use of
polarity dictionaries and Conditional Random
Fields to obtain syntactic and semantic fea-
tures, which are afterwards fed to a statistical
classifier in order to obtain the sentence polar-
ity. To improve results, an ensemble of clas-
sifiers was employed by combining the indi-
vidual outputs with majority voting strategy.
Our solution was evaluated in the SemEval
2015 Task 10, subtask B: Sentiment Analysis
in Twitter, achieving competitive performance
in all testsets.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999771163265306">
Sentiment Analysis (SA) is a hot-topic in the aca-
demic world, and also in the industry. In SA, a la-
bel is automatically assigned to a piece of content
carrying the polarity of the composition. The rel-
evance for the web industry is clear, as new ser-
vices promote content sharing among users. The
number of registers generated by these services is
paramount, discouraging manual analysis. Hence,
automatic systems capable of processing this infor-
mation have great value for the industry. Many ser-
vices, such as web advertisement, recommendation,
and mail campaigns (to name a few) could benefit
from the information gathered with polarity analysis
of user content.
This work is focused on message-level sentiment
analysis, that is, the objective is the assignment of
polarity to a small piece a text, typically one or two
sentences with less than 140 characters. This restric-
tion is motivated by the popularity of microblogging
services such as Twitter. Here, users write messages
of up to 140 characters to share information, their
opinions or their feelings with other users. Those
messages are shared in real time, and are a sample
of the public opinion. Therefore, these small compo-
sitions published in microblogging sites can be ana-
lyzed to deduce the opinions about any topic of in-
terest.
Nevertheless, automatic systems are not perfect.
The results of the sentiment analysis in short sen-
tences is not completely reliable. State of the art
solutions are still far from being comparable to hu-
man performance, though very promising results
were obtained recently using deep learning systems
(Socher et al., 2013; Tang et al., 2014), and a careful
selection of features with Support Vector Machines
(SVM) (Zhu et al., 2014) or other statistical classi-
fiers (Go et al., 2009).
This paper describes our sentiment classifier for
short sentences and the results in our participation
in the SemEval 2015 competition. We have imple-
mented a supervised solution for learning the polar-
ity of short messages. We made extensive use of se-
quential Conditional Random Fields (CRFs) in order
to obtain the scope of polarity modifiers and shifters
(e.g. negation, intensification). Although a complete
explanation of CRFs is out of scope of this paper, the
reader can obtain comprehensive information about
it in the literature (Lafferty et al., 2001; Sutton and
McCallum, 2011).
</bodyText>
<page confidence="0.978473">
539
</page>
<note confidence="0.604822">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 539–544,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.944747" genericHeader="method">
2 The SA System
</sectionHeader>
<bodyText confidence="0.999928866666666">
This section presents a detailed overview of our sys-
tem for sentiment tagging of short sentences. Our
system performs several steps over each register to
determine the polarity of the sentence. Initially, each
register is preprocessed to obtain a normalized rep-
resentation of the data. Next, syntactic information
is extracted generating high-level features. As a final
step, the features extracted in previous analysis are
fed to a statistical classifier, obtaining the polarity of
the register.
This is a supervised system, and therefore it needs
a learning phase where data are tagged manually.
The supervised models are trained only with the data
provided by the organization, and therefore it can be
considered a constraint solution.
</bodyText>
<subsectionHeader confidence="0.982901">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.991839586206897">
The sort of language used in microblogging services
is colloquial style, with misspelled words and gram-
matical and syntactic errors. In order to solve this
problem, basic normalization is performed as the
first step. The actions executed in this stage are the
substitution of emoticons for equivalent words and
the substitution of frequent abbreviations. By lack of
space, a complete Lookup Table of emoticons can-
not be displayed in this paper but a sample of rele-
vant transformations are in Table 1. We divided the
emoticons in twelve categories: angry, bad, boring,
complicity, happy, laugh, love, neutral, sad shy, sur-
prise and worried.
One kind of specific language artefacts appearing
in Twitter are hashtags. Hashtags are small pieces
of text which usually contain valuable information
to extract the sense of a whole sentence. Users use
those chunks to voice those parts more relevant of
the message and, very frequently, they are opinion-
ated. Nevertheless, hashtags do not follow the gram-
mar rules (e.g. no case used, words are stuck to-
gether, incomplete sentences without subject, verb,
etc). To deal with the multiword problem of hash-
tags, we developed a module that uses CRFs with
character-level features to find word terminations in
hashtags. If more than one word is found, the system
handles them as separated words in following steps.
Table 2 contains different multi-word hashtags
that appear in the testsets provided in the SemEval
</bodyText>
<table confidence="0.999106769230769">
Emoticons Replacement
:), :-), :o), etc happy
XD, x-D, xD, etc laugh
:*, :ˆ *, etc love
;), ;-), ;D, etc complicity
:(, :’-(, :-[, etc sad
D;, DX, D:, etc worried
:@, :-||, etc angry
o O, o.O, o 0, etc surprise
:O, &gt;:O, :-O, etc boring
:-###.., etc bad
:$, ˆˆ;, etc shy
:#, :-#, :X, etc neutral
</table>
<tableCaption confidence="0.998439">
Table 1: Sample of Emoticon Transformations.
</tableCaption>
<table confidence="0.540706857142857">
Input Hashtags Output Words
#classicmovielotto classic movie lotto
#notupinhere not up in here
#Thatisall That is all
#shoptilwedrop shop til we drop
#whatabadass what a bad ass
#wordtomymuva word to my muva
</table>
<tableCaption confidence="0.963659">
Table 2: Inputs and Outputs of the Hashtag Splitter.
</tableCaption>
<bodyText confidence="0.9999708">
competition and the corresponding output of the
hashtag splitter. One of the hashtags, #whatabadass,
is wrongly processed but with no significant change
in meaning. In internal tests, 93% of words are cor-
rectly extracted by this approach.
</bodyText>
<subsectionHeader confidence="0.999171">
2.2 Word Features
</subsectionHeader>
<bodyText confidence="0.9994165">
Our system uses several dictionaries as an input
for different steps of the feature extraction process.
These dictionaries are used to extract labels that get
combined with features in the learning steps.
</bodyText>
<listItem confidence="0.9681656">
• Polar Dictionary: contains polar words, posi-
tive and negative, in English. This is a gen-
eral purpose dictionary and no adaptation to the
context of analysis was performed. If a word
is registered as a positive/negative word, it is
labelled with the corresponding polar tag. In
case of ambiguity (the word appears in both
dictionaries) the polarity label is not used for
this word. The baseline for this dictionary was
SentiWordNet (Baccianella et al., 2010), aug-
</listItem>
<page confidence="0.884699">
540
</page>
<bodyText confidence="0.615405">
mented after observation of training records.
</bodyText>
<listItem confidence="0.990147793103448">
• Denier particles: this dictionary contains par-
ticles that reverse the polarity of the words af-
fected by them (e.g. not, no, etc). Detecting the
scope of negation plays an important role in de-
tecting the polarity of a sentence. The academic
literature follows different approaches, such as
hand-crafted rules (Sidorov et al., 2013; Pang
et al., 2002; Athar, 2011), or CRFs (Lapponi et
al., 2012b; Nakagawa et al., 2010; Councill et
al., 2010).
• Reversal verbs: their behaviour is similar to de-
nier particles. Some verbs reverse the polarity
of the content under their scope of influence
(e.g. avoid, prevent, solve, etc). In order to
obtain the list of reversal verbs, basic syntactic
rules and a manual supervision was applied af-
terwards. A similar approach can be found in
(Choi and Cardie, 2008).
• Comparators and Superlatives: a dictionary
with comparatives and superlatives was built in
a similar way as the polar dictionary. There
is a bit of redundancy with this feature as the
morphological tagger used by the system gives
the same information. However, the syntactic
parser is not very reliable for informal style,
unless it is specifically trained, which is not the
case of our system. This information is needed
to track intensification and comparisons within
a sentence.
</listItem>
<subsectionHeader confidence="0.997112">
2.3 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999992076923077">
Several language constructions can act like polarity
shifters with those parts influenced by them. This
is the case of negation particles and some specific
verbs. Detecting the scope of this modifiers is a
hard task. Our system employs CRFs to obtain la-
bels of those part of sentences that can act as polarity
shifters, or that are influenced by polarity shifters. In
this sense, we consider the detection of these scopes
as an special case of a sequential labelling problem.
CRFs are supervised techniques and they learnt the
parameters of the system using manually labelled
examples. We have built training records using a
subset of the data available in the task.
</bodyText>
<sectionHeader confidence="0.512573" genericHeader="method">
Input Features
</sectionHeader>
<bodyText confidence="0.99905025">
words, word bigrams, word trigrams, stems,
stem bigrams, stem trigrams, PoS, PoS bigrams,
PoS trigrams, distance to denier particle,
distance to denier verb, distance to advers. particles
</bodyText>
<tableCaption confidence="0.990122">
Table 3: Input Features of CRFs.
</tableCaption>
<bodyText confidence="0.999984545454545">
Our system follows a similar approach to (Lap-
poni et al., 2012a) but it was enhanced to track in-
tensification, comparisons within a sentence, and the
effect of adversative clauses (e.g. sentences with but
particles). Figure 1 shows an example of the labels
assigned by the system to each word of a sentence.
Table 3 show the inputs and the combination of fea-
tures included in the CRFs. The particles of negation
(e.g. none, not), denier verbs (e.g. prevent, avoid)
and others present in internal dictionaries such as
more, very, less or but are marked as CUEs of nega-
tion, intensification and adversarial scopes respec-
tively.
Sentences are tagged to obtain morphosyntactic
data to use this information as input to the polar-
ity shifter modules. In our case, we use the Freel-
ing tool (Padr´o and Stanilovsky, 2012) for this stage.
Freeling is an open source suite with tools to anal-
yse textual data. It contains parsers with different
degree of complexity but to the purpose of our sys-
tem, only the Part of Speech (PoS) information was
needed. We do not use dependence parsing as input
feature in contrast to previous state of the art. The
approach followed could experience problems with
discontinuous scopes (e.g. when subordinate or par-
ticiple clauses are intermingled within a sentence),
but this problem is negligible due to the typically di-
rect and colloquial style of short sentences.
The labels gathered with the CRF modules are
used in conjunction with the Word, Stem, PoS and
polar dictionaries to generate high-level features
which serve as input to the classifier and thus to as-
sign the polarity to the message in the final step.
</bodyText>
<subsectionHeader confidence="0.98901">
2.4 Classification Algorithm
</subsectionHeader>
<bodyText confidence="0.999954">
All the characteristics from previous steps are in-
cluded as input features of a statistical classifier. The
lexical features (word, stem, word and stem bigrams
and flags extracted from the polar dictionaries) with
</bodyText>
<page confidence="0.997558">
541
</page>
<figureCaption confidence="0.996908">
Figure 1: Example sentence with the CRF label notation.
</figureCaption>
<table confidence="0.9862435">
Polarity Positives Neutral Negatives
N. Samples 3456 4468 1432
</table>
<tableCaption confidence="0.99971">
Table 4: Training vector.
</tableCaption>
<bodyText confidence="0.9980395">
PoS and the labels from the CRFs. The algorithm
employed for learning was a logistic regressor. Due
to the size of the feature space and their sparsity,
l1 (0.000001) and l2 (0.000001) regularization was
applied to learn the important features and discard
those with low relevance to the task.
</bodyText>
<subsectionHeader confidence="0.999548">
2.5 Ensemble of classifiers
</subsectionHeader>
<bodyText confidence="0.999987857142857">
It could be possible to use the whole training vec-
tor available in one unique classifier, but we chose a
different strategy that provided better results.
The ensemble was obtained by replicating the in-
dividual schema but using a small subset of the data
available for training. The final decision combines
the outputs of the classifiers using majority voting.
Despite the time complexity of the ensemble and
the lower precision of the individual classifiers, this
strategy yielded better results than the one-classifier
approach (between 1% and 3%) though it depended
on the individual execution. An ensemble of 15 to
30 classifiers performed reasonably well in the eval-
uation tests.
</bodyText>
<sectionHeader confidence="0.998841" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.989034">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.98059625">
To train and validate the system during development,
the SemEval organization provides the team com-
petitors with a) an index set of tweets (that should be
downloaded by teams), and b) a progress and input
</bodyText>
<table confidence="0.9994385">
Test F-score
LiveJournal2014 72.63
SMS 2013 61.97
Twitter 2013 65.29
Twitter 2014 66.87
Twitter 2014 sarcasm 59.11
Twitter 2015 60.62
Twitter 2015 sarcasm 56.45
</table>
<tableCaption confidence="0.99993">
Table 5: Performance in progress and input test.
</tableCaption>
<bodyText confidence="0.999805375">
test for fair comparison of the different approaches.
All the records that can be used as training vector
are labelled with a tag (positive, negative and neu-
tral). Due to cancellations of tweets that were not
available, our system employed a subset of training
of those provided by the organization. Table 4 shows
the distribution of the training vector used by our
system. A subset of those records are employed to
train the CRF models.
Finally, the performance of our system is evalu-
ated using a F-score that combines the F-score of
positive and negative tweet, whilst neutral records
are used to reckon the precision and recall of the pos-
itive/negative classes. We refer the reader to (Rosen-
thal et al., 2015) for a complete description of the
task and the evaluation process.
</bodyText>
<subsectionHeader confidence="0.953697">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999788">
Table 5 shows the results of our system in the
progress test of 2014 and the new input tests of 2015.
The system shows a distinguished performance in
nearly all the progress tests of 2014. It achieved
17th position in Twitter 2014 sentences, 1st in Twit-
</bodyText>
<page confidence="0.992799">
542
</page>
<bodyText confidence="0.999918142857143">
ter Sarcasm and 11th in LiveJournal2014. In SMS
2013 and in Twitter 2013 datasets we achieved also
a good result (21th and 22th respectively). Regard-
ing sarcasm detection in 2014 dataset, our system
had good results in tweets with hashtags (25 right
answers out of 35) whereas it was more prone to fail
when users expressed positive opinions over nega-
tive events. Paying more attention to these specific
constructions would lead to better results in the fu-
ture.
In the new tests of Twitter 2015, our system per-
formed in the 16th position of all competitors in both
sarcasm and normal datasets. There is a clear gap
of 6 points between the 2014 and 2015 Twitter F-
score and the new testset. Our system is supervised
and was only trained with the vector provided by the
SemEval community which could mean the gap be-
tween the training and test vectors has increased this
year. In this sense, it would be interesting to train
with external records to see if the performance over
the 2015 tests could be improved.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99998780952381">
This paper shows the solution developed by Gra-
diant (http://www.gradiant.org) for the Sentiment
Analysis Task 10 (subtask B) of SemEval 2015. The
system finished in a notable 16th position out of 40
participants. In general terms, our system exhibits
stability in all the different subtasks, achieving the
1st position in one of them, 2014 Tweet Sarcasm.
We emphasize the modularity of our solution as one
of the advantages of our approach. New function-
ality could be easily added to the current configura-
tion, tracking new aspects of polarity detection that
was left unattended in the current state of develop-
ment.
Despite the overall goodness of the system, there
is a generalized degradation in the evaluation results
between 2014 and 2015 Twitter datasets. This result
is very interesting and encouraging for future lines
of work, as there exists a clear need in research of
new models which provide better abstraction of the
data and improve the adaptation to new contexts that
differ substantially the training vectors.
</bodyText>
<sectionHeader confidence="0.986716" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999345431372549">
Awais Athar. 2011. Sentiment Analysis of Citations us-
ing Sentence Structure-Based Features. In Proceed-
ings of the ACL 2011 student session, pages 81–87.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An Enhanced Lexical
Resource for Sentiment Analysis and Opinion Mining.
In LREC, volume 10, pages 2200–2204.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 793–801.
Isaac G Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What’s Great and What’s Not: Learn-
ing to Classify the Scope of Negation for Improved
Sentiment Analysis. In Proceedings of the Workshop
on Negation and Speculation in Natural Language
Processing, pages 51–59.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter Sentiment Classification using Distant Supervision.
CS224N Project Report, Stanford, pages 1–12.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Proba-
bilistic models for Segmenting and Labeling Sequence
Data.
Emanuele Lapponi, Jonathon Read, and Lilja Ovre-
lid. 2012a. Representing and Resolving Negation
for Sentiment Analysis. In Data Mining Workshops
(ICDMW), 2012 IEEE 12th International Conference
on, pages 687–692.
Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and
Jonathon Read. 2012b. Uio 2: Sequence-Labeling
Negation Using Dependency Features. In Proceedings
of the First Joint Conference on Lexical and Compu-
tational Semantics-Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation, pages 319–327.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency Tree-Based Sentiment Classifica-
tion using crfs with Hidden Variables. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 786–794.
Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards Wider Multilinguality. In Proceedings
of the Language Resources and Evaluation Conference
(LREC 2012), Istanbul, Turkey, May.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment Classification using
Machine Learning Techniques. In Proceedings of the
</reference>
<page confidence="0.986559">
543
</page>
<reference confidence="0.999146757575758">
ACL-02 Conference on Empirical methods in Natural
Language Processing-Volume 10, pages 79–86.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 Task 10: Sentiment Anal-
ysis in Twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, June.
Grigori Sidorov, Sabino Miranda-Jim´enez, Francisco
Viveros-Jim´enez, Alexander Gelbukh, No´e Castro-
S´anchez, Francisco Vel´asquez, Ismael D´ıaz-Rangel,
Sergio Su´arez-Guerra, Alejandro Trevi˜no, and Juan
Gordon. 2013. Empirical Study of Machine Learn-
ing Based Approach for Opinion Mining in Tweets.
In Advances in Artificial Intelligence, pages 1–14.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive Deep Models
for Semantic Compositionality over a Sentiment Tree-
bank. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 1631–1642.
Charles Sutton and Andrew McCallum. 2011. An In-
troduction to Conditional Random Fields. Machine
Learning, 4(4):267–373.
Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming
Zhou. 2014. Coooolll: A Deep Learning System for
Twitter Sentiment Classification. SemEval 2014, page
208.
Xiaodan Zhu, Svetlana Kiritchenko, and Saif M Moham-
mad. 2014. Nrc-canada-2014: Recent Improvements
in the Sentiment Analysis of Tweets. SemEval 2014,
page 443.
</reference>
<page confidence="0.998264">
544
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.172696">
<title confidence="0.9983575">Gradiant-Analytics: Training Polarity Shifters with CRFs for Message Level Polarity Detection</title>
<author confidence="0.772235">H´ector Cerezo-Costas</author>
<author confidence="0.772235">Diego</author>
<affiliation confidence="0.376206">Gradiant - Galician Research and Development Center in Advanced</affiliation>
<address confidence="0.4662245">Edificio CITEXVI, local Vigo, Pontevedra 36310,</address>
<abstract confidence="0.999214333333333">In this paper we present our solution for obtaining sentiment at message-level of short sentences. The system combines the use of dictionaries and Random obtain syntactic and semantic features, which are afterwards fed to a statistical classifier in order to obtain the sentence polarity. To improve results, an ensemble of classifiers was employed by combining the individual outputs with majority voting strategy. Our solution was evaluated in the SemEval 2015 Task 10, subtask B: Sentiment Analysis in Twitter, achieving competitive performance in all testsets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Awais Athar</author>
</authors>
<title>Sentiment Analysis of Citations using Sentence Structure-Based Features.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>81--87</pages>
<contexts>
<context position="7642" citStr="Athar, 2011" startWordPosition="1213" endWordPosition="1214">olar tag. In case of ambiguity (the word appears in both dictionaries) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit of redundancy w</context>
</contexts>
<marker>Athar, 2011</marker>
<rawString>Awais Athar. 2011. Sentiment Analysis of Citations using Sentence Structure-Based Features. In Proceedings of the ACL 2011 student session, pages 81–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>2200--2204</pages>
<contexts>
<context position="7223" citStr="Baccianella et al., 2010" startWordPosition="1143" endWordPosition="1146"> an input for different steps of the feature extraction process. These dictionaries are used to extract labels that get combined with features in the learning steps. • Polar Dictionary: contains polar words, positive and negative, in English. This is a general purpose dictionary and no adaptation to the context of analysis was performed. If a word is registered as a positive/negative word, it is labelled with the corresponding polar tag. In case of ambiguity (the word appears in both dictionaries) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In LREC, volume 10, pages 2200–2204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>793--801</pages>
<contexts>
<context position="8078" citStr="Choi and Cardie, 2008" startWordPosition="1285" endWordPosition="1288">tant role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit of redundancy with this feature as the morphological tagger used by the system gives the same information. However, the syntactic parser is not very reliable for informal style, unless it is specifically trained, which is not the case of our system. This information is needed to track intensification and comparisons within a sentence. 2.3 Syntactic Features Several language constructions can act like polarity shifters with those parts influenced b</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 793–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isaac G Councill</author>
<author>Ryan McDonald</author>
<author>Leonid Velikovich</author>
</authors>
<title>What’s Great and What’s Not: Learning to Classify the Scope of Negation for Improved Sentiment Analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing,</booktitle>
<pages>51--59</pages>
<contexts>
<context position="7721" citStr="Councill et al., 2010" startWordPosition="1225" endWordPosition="1228">) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit of redundancy with this feature as the morphological tagger used by the system gives the same </context>
</contexts>
<marker>Councill, McDonald, Velikovich, 2010</marker>
<rawString>Isaac G Councill, Ryan McDonald, and Leonid Velikovich. 2010. What’s Great and What’s Not: Learning to Classify the Scope of Negation for Improved Sentiment Analysis. In Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 51–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter Sentiment Classification using Distant Supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="2701" citStr="Go et al., 2009" startWordPosition="418" endWordPosition="421">refore, these small compositions published in microblogging sites can be analyzed to deduce the opinions about any topic of interest. Nevertheless, automatic systems are not perfect. The results of the sentiment analysis in short sentences is not completely reliable. State of the art solutions are still far from being comparable to human performance, though very promising results were obtained recently using deep learning systems (Socher et al., 2013; Tang et al., 2014), and a careful selection of features with Support Vector Machines (SVM) (Zhu et al., 2014) or other statistical classifiers (Go et al., 2009). This paper describes our sentiment classifier for short sentences and the results in our participation in the SemEval 2015 competition. We have implemented a supervised solution for learning the polarity of short messages. We made extensive use of sequential Conditional Random Fields (CRFs) in order to obtain the scope of polarity modifiers and shifters (e.g. negation, intensification). Although a complete explanation of CRFs is out of scope of this paper, the reader can obtain comprehensive information about it in the literature (Lafferty et al., 2001; Sutton and McCallum, 2011). 539 Procee</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification using Distant Supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<contexts>
<context position="3261" citStr="Lafferty et al., 2001" startWordPosition="506" endWordPosition="509">al., 2014) or other statistical classifiers (Go et al., 2009). This paper describes our sentiment classifier for short sentences and the results in our participation in the SemEval 2015 competition. We have implemented a supervised solution for learning the polarity of short messages. We made extensive use of sequential Conditional Random Fields (CRFs) in order to obtain the scope of polarity modifiers and shifters (e.g. negation, intensification). Although a complete explanation of CRFs is out of scope of this paper, the reader can obtain comprehensive information about it in the literature (Lafferty et al., 2001; Sutton and McCallum, 2011). 539 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 539–544, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 The SA System This section presents a detailed overview of our system for sentiment tagging of short sentences. Our system performs several steps over each register to determine the polarity of the sentence. Initially, each register is preprocessed to obtain a normalized representation of the data. Next, syntactic information is extracted generating high-level features. As a fin</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for Segmenting and Labeling Sequence Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Lapponi</author>
<author>Jonathon Read</author>
<author>Lilja Ovrelid</author>
</authors>
<title>Representing and Resolving Negation for Sentiment Analysis.</title>
<date>2012</date>
<booktitle>In Data Mining Workshops (ICDMW), 2012 IEEE 12th International Conference on,</booktitle>
<pages>687--692</pages>
<contexts>
<context position="7673" citStr="Lapponi et al., 2012" startWordPosition="1217" endWordPosition="1220">biguity (the word appears in both dictionaries) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit of redundancy with this feature as the morphol</context>
<context position="9552" citStr="Lapponi et al., 2012" startWordPosition="1522" endWordPosition="1526"> polarity shifters. In this sense, we consider the detection of these scopes as an special case of a sequential labelling problem. CRFs are supervised techniques and they learnt the parameters of the system using manually labelled examples. We have built training records using a subset of the data available in the task. Input Features words, word bigrams, word trigrams, stems, stem bigrams, stem trigrams, PoS, PoS bigrams, PoS trigrams, distance to denier particle, distance to denier verb, distance to advers. particles Table 3: Input Features of CRFs. Our system follows a similar approach to (Lapponi et al., 2012a) but it was enhanced to track intensification, comparisons within a sentence, and the effect of adversative clauses (e.g. sentences with but particles). Figure 1 shows an example of the labels assigned by the system to each word of a sentence. Table 3 show the inputs and the combination of features included in the CRFs. The particles of negation (e.g. none, not), denier verbs (e.g. prevent, avoid) and others present in internal dictionaries such as more, very, less or but are marked as CUEs of negation, intensification and adversarial scopes respectively. Sentences are tagged to obtain morph</context>
</contexts>
<marker>Lapponi, Read, Ovrelid, 2012</marker>
<rawString>Emanuele Lapponi, Jonathon Read, and Lilja Ovrelid. 2012a. Representing and Resolving Negation for Sentiment Analysis. In Data Mining Workshops (ICDMW), 2012 IEEE 12th International Conference on, pages 687–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Lapponi</author>
<author>Erik Velldal</author>
<author>Lilja Øvrelid</author>
<author>Jonathon Read</author>
</authors>
<title>Uio 2: Sequence-Labeling Negation Using Dependency Features.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>319--327</pages>
<contexts>
<context position="7673" citStr="Lapponi et al., 2012" startWordPosition="1217" endWordPosition="1220">biguity (the word appears in both dictionaries) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit of redundancy with this feature as the morphol</context>
<context position="9552" citStr="Lapponi et al., 2012" startWordPosition="1522" endWordPosition="1526"> polarity shifters. In this sense, we consider the detection of these scopes as an special case of a sequential labelling problem. CRFs are supervised techniques and they learnt the parameters of the system using manually labelled examples. We have built training records using a subset of the data available in the task. Input Features words, word bigrams, word trigrams, stems, stem bigrams, stem trigrams, PoS, PoS bigrams, PoS trigrams, distance to denier particle, distance to denier verb, distance to advers. particles Table 3: Input Features of CRFs. Our system follows a similar approach to (Lapponi et al., 2012a) but it was enhanced to track intensification, comparisons within a sentence, and the effect of adversative clauses (e.g. sentences with but particles). Figure 1 shows an example of the labels assigned by the system to each word of a sentence. Table 3 show the inputs and the combination of features included in the CRFs. The particles of negation (e.g. none, not), denier verbs (e.g. prevent, avoid) and others present in internal dictionaries such as more, very, less or but are marked as CUEs of negation, intensification and adversarial scopes respectively. Sentences are tagged to obtain morph</context>
</contexts>
<marker>Lapponi, Velldal, Øvrelid, Read, 2012</marker>
<rawString>Emanuele Lapponi, Erik Velldal, Lilja Øvrelid, and Jonathon Read. 2012b. Uio 2: Sequence-Labeling Negation Using Dependency Features. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 319–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency Tree-Based Sentiment Classification using crfs with Hidden Variables.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>786--794</pages>
<contexts>
<context position="7697" citStr="Nakagawa et al., 2010" startWordPosition="1221" endWordPosition="1224">rs in both dictionaries) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit of redundancy with this feature as the morphological tagger used by th</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency Tree-Based Sentiment Classification using crfs with Hidden Variables. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 786–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>Freeling 3.0: Towards Wider Multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC 2012),</booktitle>
<location>Istanbul, Turkey,</location>
<marker>Padr´o, Stanilovsky, 2012</marker>
<rawString>Llu´ıs Padr´o and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards Wider Multilinguality. In Proceedings of the Language Resources and Evaluation Conference (LREC 2012), Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Conference on Empirical methods in Natural Language Processing-Volume 10,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7628" citStr="Pang et al., 2002" startWordPosition="1209" endWordPosition="1212">the corresponding polar tag. In case of ambiguity (the word appears in both dictionaries) the polarity label is not used for this word. The baseline for this dictionary was SentiWordNet (Baccianella et al., 2010), aug540 mented after observation of training records. • Denier particles: this dictionary contains particles that reverse the polarity of the words affected by them (e.g. not, no, etc). Detecting the scope of negation plays an important role in detecting the polarity of a sentence. The academic literature follows different approaches, such as hand-crafted rules (Sidorov et al., 2013; Pang et al., 2002; Athar, 2011), or CRFs (Lapponi et al., 2012b; Nakagawa et al., 2010; Councill et al., 2010). • Reversal verbs: their behaviour is similar to denier particles. Some verbs reverse the polarity of the content under their scope of influence (e.g. avoid, prevent, solve, etc). In order to obtain the list of reversal verbs, basic syntactic rules and a manual supervision was applied afterwards. A similar approach can be found in (Choi and Cardie, 2008). • Comparators and Superlatives: a dictionary with comparatives and superlatives was built in a similar way as the polar dictionary. There is a bit o</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: Sentiment Classification using Machine Learning Techniques. In Proceedings of the ACL-02 Conference on Empirical methods in Natural Language Processing-Volume 10, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 Task 10: Sentiment Analysis in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="13680" citStr="Rosenthal et al., 2015" startWordPosition="2202" endWordPosition="2206"> training vector are labelled with a tag (positive, negative and neutral). Due to cancellations of tweets that were not available, our system employed a subset of training of those provided by the organization. Table 4 shows the distribution of the training vector used by our system. A subset of those records are employed to train the CRF models. Finally, the performance of our system is evaluated using a F-score that combines the F-score of positive and negative tweet, whilst neutral records are used to reckon the precision and recall of the positive/negative classes. We refer the reader to (Rosenthal et al., 2015) for a complete description of the task and the evaluation process. 3.2 Results Table 5 shows the results of our system in the progress test of 2014 and the new input tests of 2015. The system shows a distinguished performance in nearly all the progress tests of 2014. It achieved 17th position in Twitter 2014 sentences, 1st in Twit542 ter Sarcasm and 11th in LiveJournal2014. In SMS 2013 and in Twitter 2013 datasets we achieved also a good result (21th and 22th respectively). Regarding sarcasm detection in 2014 dataset, our system had good results in tweets with hashtags (25 right answers out o</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 Task 10: Sentiment Analysis in Twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grigori Sidorov</author>
<author>Sabino Miranda-Jim´enez</author>
<author>Francisco Viveros-Jim´enez</author>
<author>Alexander Gelbukh</author>
<author>No´e CastroS´anchez</author>
<author>Francisco Vel´asquez</author>
<author>Ismael D´ıaz-Rangel</author>
<author>Sergio Su´arez-Guerra</author>
<author>Alejandro Trevi˜no</author>
<author>Juan Gordon</author>
</authors>
<title>Empirical Study of Machine Learning Based Approach for Opinion Mining in Tweets.</title>
<date>2013</date>
<booktitle>In Advances in Artificial Intelligence,</booktitle>
<pages>1--14</pages>
<marker>Sidorov, Miranda-Jim´enez, Viveros-Jim´enez, Gelbukh, CastroS´anchez, Vel´asquez, D´ıaz-Rangel, Su´arez-Guerra, Trevi˜no, Gordon, 2013</marker>
<rawString>Grigori Sidorov, Sabino Miranda-Jim´enez, Francisco Viveros-Jim´enez, Alexander Gelbukh, No´e CastroS´anchez, Francisco Vel´asquez, Ismael D´ıaz-Rangel, Sergio Su´arez-Guerra, Alejandro Trevi˜no, and Juan Gordon. 2013. Empirical Study of Machine Learning Based Approach for Opinion Mining in Tweets. In Advances in Artificial Intelligence, pages 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="2539" citStr="Socher et al., 2013" startWordPosition="390" endWordPosition="393">aracters to share information, their opinions or their feelings with other users. Those messages are shared in real time, and are a sample of the public opinion. Therefore, these small compositions published in microblogging sites can be analyzed to deduce the opinions about any topic of interest. Nevertheless, automatic systems are not perfect. The results of the sentiment analysis in short sentences is not completely reliable. State of the art solutions are still far from being comparable to human performance, though very promising results were obtained recently using deep learning systems (Socher et al., 2013; Tang et al., 2014), and a careful selection of features with Support Vector Machines (SVM) (Zhu et al., 2014) or other statistical classifiers (Go et al., 2009). This paper describes our sentiment classifier for short sentences and the results in our participation in the SemEval 2015 competition. We have implemented a supervised solution for learning the polarity of short messages. We made extensive use of sequential Conditional Random Fields (CRFs) in order to obtain the scope of polarity modifiers and shifters (e.g. negation, intensification). Although a complete explanation of CRFs is out</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>An Introduction to Conditional Random Fields.</title>
<date>2011</date>
<booktitle>Machine Learning,</booktitle>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="3289" citStr="Sutton and McCallum, 2011" startWordPosition="510" endWordPosition="513">tistical classifiers (Go et al., 2009). This paper describes our sentiment classifier for short sentences and the results in our participation in the SemEval 2015 competition. We have implemented a supervised solution for learning the polarity of short messages. We made extensive use of sequential Conditional Random Fields (CRFs) in order to obtain the scope of polarity modifiers and shifters (e.g. negation, intensification). Although a complete explanation of CRFs is out of scope of this paper, the reader can obtain comprehensive information about it in the literature (Lafferty et al., 2001; Sutton and McCallum, 2011). 539 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 539–544, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics 2 The SA System This section presents a detailed overview of our system for sentiment tagging of short sentences. Our system performs several steps over each register to determine the polarity of the sentence. Initially, each register is preprocessed to obtain a normalized representation of the data. Next, syntactic information is extracted generating high-level features. As a final step, the features extrac</context>
</contexts>
<marker>Sutton, McCallum, 2011</marker>
<rawString>Charles Sutton and Andrew McCallum. 2011. An Introduction to Conditional Random Fields. Machine Learning, 4(4):267–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Ming Zhou</author>
</authors>
<title>Coooolll: A Deep Learning System for Twitter Sentiment Classification. SemEval</title>
<date>2014</date>
<pages>208</pages>
<contexts>
<context position="2559" citStr="Tang et al., 2014" startWordPosition="394" endWordPosition="397">ormation, their opinions or their feelings with other users. Those messages are shared in real time, and are a sample of the public opinion. Therefore, these small compositions published in microblogging sites can be analyzed to deduce the opinions about any topic of interest. Nevertheless, automatic systems are not perfect. The results of the sentiment analysis in short sentences is not completely reliable. State of the art solutions are still far from being comparable to human performance, though very promising results were obtained recently using deep learning systems (Socher et al., 2013; Tang et al., 2014), and a careful selection of features with Support Vector Machines (SVM) (Zhu et al., 2014) or other statistical classifiers (Go et al., 2009). This paper describes our sentiment classifier for short sentences and the results in our participation in the SemEval 2015 competition. We have implemented a supervised solution for learning the polarity of short messages. We made extensive use of sequential Conditional Random Fields (CRFs) in order to obtain the scope of polarity modifiers and shifters (e.g. negation, intensification). Although a complete explanation of CRFs is out of scope of this pa</context>
</contexts>
<marker>Tang, Wei, Qin, Liu, Zhou, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming Zhou. 2014. Coooolll: A Deep Learning System for Twitter Sentiment Classification. SemEval 2014, page 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
</authors>
<title>Nrc-canada-2014: Recent Improvements in the Sentiment Analysis of Tweets. SemEval</title>
<date>2014</date>
<pages>443</pages>
<contexts>
<context position="2650" citStr="Zhu et al., 2014" startWordPosition="409" endWordPosition="412">al time, and are a sample of the public opinion. Therefore, these small compositions published in microblogging sites can be analyzed to deduce the opinions about any topic of interest. Nevertheless, automatic systems are not perfect. The results of the sentiment analysis in short sentences is not completely reliable. State of the art solutions are still far from being comparable to human performance, though very promising results were obtained recently using deep learning systems (Socher et al., 2013; Tang et al., 2014), and a careful selection of features with Support Vector Machines (SVM) (Zhu et al., 2014) or other statistical classifiers (Go et al., 2009). This paper describes our sentiment classifier for short sentences and the results in our participation in the SemEval 2015 competition. We have implemented a supervised solution for learning the polarity of short messages. We made extensive use of sequential Conditional Random Fields (CRFs) in order to obtain the scope of polarity modifiers and shifters (e.g. negation, intensification). Although a complete explanation of CRFs is out of scope of this paper, the reader can obtain comprehensive information about it in the literature (Lafferty e</context>
</contexts>
<marker>Zhu, Kiritchenko, Mohammad, 2014</marker>
<rawString>Xiaodan Zhu, Svetlana Kiritchenko, and Saif M Mohammad. 2014. Nrc-canada-2014: Recent Improvements in the Sentiment Analysis of Tweets. SemEval 2014, page 443.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>