<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.986854">
Time Mapping with Hypergraphs
</title>
<author confidence="0.990908">
Jan W. Amtrup Volker Weber
</author>
<affiliation confidence="0.924054">
Computing Research Laboratory University of Hamburg,
New Mexico State University Computer Science Department,
</affiliation>
<address confidence="0.816525">
Las Cruces, NM 88003,USA Vogt-K011n-Str. 30, D-22527 Hamburg, Germany
</address>
<email confidence="0.904123">
email: j amtrup@crl . nmsu . edu email: weber@inf ormat ik . uni-hamburg . de
</email>
<sectionHeader confidence="0.995888" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902777777778">
Word graphs are able to represent a large num-
ber of different utterance hypotheses in a very
compact manner. However, usually they con-
tain a huge amount of redundancy in terms
of word hypotheses that cover almost identi-
cal intervals in time. We address this problem
by introducing hypergraphs for speech process-
ing. Hypergraphs can be classified as an ex-
tension to word graphs and charts, their edges
possibly having several start and end vertices.
By converting ordinary word graphs to hyper-
graphs one can reduce the number of edges
considerably. We define hypergraphs formally,
present an algorithm to convert word graphs
into hypergraphs and state consistency proper-
ties for edges and their combination. Finally, we
present some empirical results concerning graph
size and parsing efficiency.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994497368421052">
The interface between a word recognizer and
language processing modules is a crucial issue
with modern speech processing systems. Given
a sufficiently high word recognition rate, it suf-
fices to transmit the most probable word se-
quence from the recognizer to a subsequent
module (e.g. a parser). A slight extension over
this best chain mode would be to deliver n-best
chains to improve language processing results.
However, it is usually not enough to deliver
just the best 10 or 20 utterances, at least not
for reasonable sized applications given todays
speech recognition technology. To overcome this
problem, in most current systems word graphs
are used as speech-language interface. Word
graphs offer a simple and efficient means to rep-
resent a very high number of utterance hypothe-
ses in a extremely compact way (Oerder and
Ney, 1993; Aubert and Ney, 1995).
</bodyText>
<figureCaption confidence="0.994889">
Figure 1: Two families of edges in a word graph
</figureCaption>
<bodyText confidence="0.9998956875">
Although they are compact, the use of word
graphs leads to problems by itself. One of them
is the current lack of a reasonable measure for
word graph size and evaluation of their contents
(Amtrup et al., 1997). The problem we want to
address in this paper is the presence of a large
number of almost identical word hypotheses. By
almost identical we mean that the start and end
vertices of edges differ only slightly.
Consider figure 1 as an example section of a
word graph. There are several word hypothe-
ses representing the words unci (and) and dann
(then). The start and end points of them differ
by small numbers of frames, each of them 10ms
long. The reasons for the existence of these fam-
ilies of edges are at least twofold:
</bodyText>
<listItem confidence="0.874628384615385">
• Standard HMM-based word recognizers try
to start (and finish) word models at each
individual frame. Since the resolution is
quite high (10ms, in many cases shorter
than the word onset), a word model may
have boundaries at several points in time.
• Natural speech (and in particular spon-
taneously produced speech) tends to blur
word boundaries. This effect is in part re-
sponsible for the dramatic decrease in word
recognition rate, given fluent speech as in-
put in contrast to isolated words as in-
put. Figure 1 demonstrates the inaccuracy
</listItem>
<page confidence="0.99777">
55
</page>
<bodyText confidence="0.999093451612903">
of word boundaries by containing several
meeting points between und and dann, em-
phasized by the fact that both words end
resp. start with the same consonant.
Thus, for most words, there is a whole set
of word hypotheses in a word graph which re-
sults in several meets between two sets of hy-
potheses. Both facts are disadvantageous for
speech processing: Many word edges result in
a high number of lexical lookups and basic op-
erations (e.g. bottom-up proposals of syntactic
categories); many meeting points between edges
result in a high number of possibly complex op-
erations (like unifications in a parser).
The most obvious way to reduce the number
of neighboring, identically labeled edges is to
reduce the time resolution provided by a word
recognizer (Weber, 1992). If a word edge is
to be processed, the start and end vertices are
mapped to the more coarse grained points in
time used by linguistic modules and a redun-
dancy check is carried out in order to prevent
multiple copies of edges. This can be easily
done, but one has to face the drawback on in-
troducing many more paths through the graph
due to artificially constructed overlaps. Fur-
thermore, it is not simple to choose a correct
resolution, as the intervals effectively appearing
with word onsets and offsets change consider-
ably with words spoken. Also, the introduction
of cycles has to be avoided.
A more sophisticated schema would use in-
terval graphs to encode word graphs. Edges of
interval graphs do not have individual start and
end vertices, but instead use intervals to denote
the range of applicability of an edge. The major
problem with interval graphs lies with the com-
plexity of edge access methods. However, many
formal statements shown below will use interval
arithmetics, as the argument will be easier to
follow.
The approach we take in this paper is to use
hypergraphs as representation medium for word
graphs. What one wants is to carry out oper-
ations only once and record the fact that there
are several start and end points of words. Hy-
pergraphs (Gondran and Minoux, 1984, p. 30)
are generalizations of ordinary graphs that al-
low multiple start and end vertices of edges.
We extend the approach of H. Weber (Weber,
1995) for time mapping. Weber considered sets
of edges with identical start vertices but slightly
different end vertices, for which the notion fam-
ily was introduced. We use full hypergraphs as
representation and thus additionally allow sev-
eral start vertices, which results in a further de-
crease of 6% in terms of resulting chart edges
while parsing (cf. section 3). Figure 2 shows
the example section using hyperedges for the
two families of edges. We adopt the way of
dealing with different acoustical scores of word
hypotheses from Weber.
</bodyText>
<figureCaption confidence="0.636267">
Figure 2: Two hyperedges representing families
of edges
</figureCaption>
<subsectionHeader confidence="0.390607">
2 Word Graphs and Hypergraphs
</subsectionHeader>
<bodyText confidence="0.826176875">
As described in the introduction, word graphs
consist of edges representing word hypotheses
generated by a word recognizer. The start and
end point of edges usually denote points in time.
Formally, a word graph is a directed, acyclic,
weighted, labeled graph with distinct root and
end vertices. It is a quadruple G =(V,E,W,G)
with the following components:
</bodyText>
<listItem confidence="0.953486277777778">
• A nonempty set of graph vertices V =
{v1, • • • ,vn}• To associate vertices with
points in time, we use a function t: V --*
N that returns the frame number for a
given vertex.
• A nonempty set of weighted, labeled, di-
rected edges e = fel, ,e,} C V xVx
W x G. To access the components of an
edge e = (v, v&apos;, w, 1), we use functions a,
w and 1, which return the start vertex
(a(e) = v), the end vertex (13(e) = v&apos;), the
weight (w(e) = w) and the label (1(e) = 1)
of an edge, respectively.
• A nonempty set of edge weights W =
{w1, , wp}. Edge weights normally rep-
resent a the acoustic score assigned to the
word hypothesis by a HMM based word
recognizer.
</listItem>
<page confidence="0.948249">
56
</page>
<figure confidence="0.9891752">
argmax{t(v)jv E V}
argmin{t(v)jv E VI}
argmax{t(v)Iv E VI}
[t(a&lt;(e)),t(a&gt; (e))]
[t(/3&lt; (e)), 4)3&gt; (e))]
</figure>
<bodyText confidence="0.996639857142857">
In contrast to interval graphs, we do not re-
quire the sets of start and end vertices to be con-
tiguous, i.e. there may be vertices that fall in
the range of the start or end vertices of an edge
which are not members of that set. If we are not
interested in the individual members of a(e) or
#(e), we merely talk about interval graphs.
</bodyText>
<subsectionHeader confidence="0.978114">
2.2 Edge Consistency
</subsectionHeader>
<bodyText confidence="0.99849025">
Just like word graphs, we demand that hyper-
graphs are acyclic, i.e. Vv 4 w : v w.
In terms of edges, this corresponds to Ve :
t(a&gt;(e)) &lt; t(0&lt;(e)).
</bodyText>
<subsectionHeader confidence="0.987982">
2.3 Adding Edges to Hypergraphs
</subsectionHeader>
<bodyText confidence="0.999061222222222">
Adding a simple word edge to a hypergraph is a
simplification of merging two hyperedges bear-
ing the same label into a new hyperedge. There-
fore we are going to explain the more general
case for hyperedge merging first. We analyze
which edges of a hypergraph may be merged to
form a new hyperedge without loss of linguistic
information. This process has to follow three
main principles:
</bodyText>
<listItem confidence="0.950221">
• Edge labels have to be identical
• Edge weights (scores) have to be combined
to a single value
• Edges have to be compatible in their start
</listItem>
<bodyText confidence="0.908889571428571">
and end vertices and must not introduce
cycles to the resulting graph
Simple Rule Set for Edge Merging
Let el, e2 E S be two hyperedges to be checked
for merging, where el = (V1, VI&apos;, w1, li) and e2 =
(172, .q,102,12). Then el and e2 could be merged
into a new hyperedge e3 = (V3, w3, /3) if
</bodyText>
<equation confidence="0.9941585">
/(el) = 4e2) (10)
min(t(/3‹ (e0), t (3&lt; (e2))) &gt;
max(t(a&gt; (el)), t(et&gt; (€2)))
where e3 is:
/3 = /1 (=12) (12)
W3 = scorejoin(wi, w2)2 (13)
</equation>
<listItem confidence="0.901755333333333">
• A nonempty set of Labels G = {11,... , /0},
which represents information attached to
an edge, usually words.
</listItem>
<bodyText confidence="0.994630428571429">
We define the relation of reachability for ver-
tices (-0 as Vv, w E V:v w &lt;#, 3e E :
a(e) = v A 0(e) = w
The transitive hull of the reachability relation
is denoted by 4.
We already stated that a word graph is acyclic
and distinctly rooted and ended.
</bodyText>
<subsectionHeader confidence="0.990314">
2.1 Hypergraphs
</subsectionHeader>
<bodyText confidence="0.9994791">
Hypergraphs differ from graphs by allowing sev-
eral start and end vertices for a single edge. In
order to apply this property to word graphs, the
definition of edges has to be changed. The set of
edges S becomes a nonempty set of weighted, la-
beled, directed hyperedges S = {el, ,em} C
V*V6 x V*Vh x W x G.
Several notions and functions defined for ordi-
nary word graphs have to be adapted to reflect
edges having sets of start and end vertices.
</bodyText>
<listItem confidence="0.774738454545455">
• The accessor functions for start and end
vertices have to be adapted to return sets of
vertices. Consider an edge e (V,V&apos;,w, /),
then we redefine
: 6 —4 V*, a(e) := V (1)
: V* , 0(e) := V&apos; (2)
• Two hyperedges e, e&apos; are adjacent, if they
share a common vertex:
0(e) n a(e) 0 0 (3)
• The reachability relation is now Vv, w E V:
v-÷w&lt;=&gt;BeES:vEa(e)AwEf3(e)
</listItem>
<bodyText confidence="0.999868375">
Additionally, we define accessor functions for
the first and last start and end vertex of an
edge. We recur to the association of vertices
with frame numbers, which is a slight simplifi-
cation (in general, there is no need for a total
ordering on the vertices in a word graph)&apos;. Fur-
thermore, the intervals covered by start and end
vertices are defined.
</bodyText>
<equation confidence="0.849922">
a&lt;(e) argmin{t(v)Iv E V} (4)
</equation>
<bodyText confidence="0.7001265">
&apos;The total ordering on vertices is naturally given
through the linearity of speech.
</bodyText>
<page confidence="0.909933">
57
</page>
<equation confidence="0.9959885">
V3 ---- V1uV2 (14)
= 17 (15)
</equation>
<bodyText confidence="0.998782">
ei and 62 have to be removed from the hyper-
graph while e3 has to be inserted.
</bodyText>
<subsectionHeader confidence="0.890208">
Sufficiency of the Rule-Set
</subsectionHeader>
<bodyText confidence="0.921093485714286">
Why is this set of two conditions sufficient for
hyperedge merging? First of all it is clear that
we can merge only hyperedges with the same
label (this is prescribed by condition 10). Con-
dition 11 gives advice which hyperedges could
be combined and prohibits cycles to be intro-
duced in the hypergraph. An analysis of the
occuring cases shows that this condition is rea-
sonable. Without loss of generality, we assume
that t(0&gt;(ei)) &lt; t(0&gt;(e2))•
1. an(ei) n (e2) 0 V au(e2) n (30(ei) 0:
This is the case where either the start ver-
tices of el and the end vertices of e2 or the
start vertices of e2 and end vertices of el
overlap each other. The merge of two such
hyperedges of this case would result in a
hyperedge e3 where t(a&gt;(e3)) &gt; t(13&lt;(e3)).
This could introduce cycles to the hyper-
graph. So this case is excluded by condi-
tion 11.
2. an(e1) fl )30(e2) = 0 A co(e2) C1/30(ei) = 0:
This is the complementary case to 1.
(a) t(a&lt;(e2)) t(0&gt;(e1))
This is the case where all vertices of
hyperedge el occur before all vertices
of hyperedge 62 or in other words the
case where two individual independent
word hypotheses with same label oc-
cur in the word graph. This case must
also not result in an edge merge since
)30(ei) c [t(a&lt;(e1)),t(a&gt;(e2))] in the
merged edge. This merge is prohib-
ited by condition 11 since all vertices
of 0(e1) have to be smaller than all
vertices of a(e2).
</bodyText>
<equation confidence="0.795523833333333">
(b) ga&lt;(e2)) &lt; t(f3&gt;(e1))
This is the complementary case to (a).
i. t(a&lt;(ei)) t(13&gt;(e2))
This is only a theoretical case be-
cause t(a&lt;(ei)) &lt; t(a&gt;(e1)) &lt;
t(0&lt;(e1)) &lt; t(0&lt;(e2)) &lt; t(f3&gt;(e2))
</equation>
<bodyText confidence="0.98981732">
2Examples for the scorejoin operation are given later
in the paragraph about score normalization.
is required (62 contains the last end
vertex).
t(a&lt;(ei)) &lt; 4/3&gt; (e2))
This is the complementary case to
i. As a result of the empty in-
tersections and the cases (b) and
ii we get t(a&gt;(ei)) &lt; t(13&lt;(e2))
and t(a&gt;.(e2)) &lt; t(i3.&lt;(ei)). That
is in other words Vta E an(el) U
cro(e2), to E )30(ei) U it(e2) : t0 &lt;
to and just the case demanded by
condition 2.
After analyzing all cases of merging of inter-
sections between start and end vertices of two
hyperedges we turn to insertion of word hy-
potheses to a hypergraph. Of course, a word
hypothesis could be seen as interval edge with
trivial intervals or as a hyperedge with only one
start and one end vertex. Since this case of
adding an edge to a hypergraph is rather easy
to depict and is heavily used while parsing word
graphs incrementally we discuss it in more de-
tail.
</bodyText>
<figureCaption confidence="0.992244">
Figure 3: Cases for adding an edge to the graph
</figureCaption>
<bodyText confidence="0.999942777777778">
The speech decoder we use delivers word hy-
potheses incrementally and ordered by the time
stamps of their end vertices. For practical rea-
sons we further sort the start vertices with equal
end vertex of a hypergraph by time. Under this
precondition we get the cases shown in figure 3.
The situation is such that eg is a hyperedge al-
ready constructed and el — e5 are candidates for
insertion.
</bodyText>
<page confidence="0.992445">
58
</page>
<figure confidence="0.745447391304348">
function AddEdge(G:Hypergraph,en:Wordhypothesie)
HyperGraph
begin
[1] If 3ek E e(G) with
l(ea) = 1(en) A t(/3&lt;(ek )) &gt; t(o(en)) then
[2] Modify edge ek
(o(ek )U {a(e,)},
13(ek)U {f9(ex )}.
max(w(ek),
normalize(w(e,,), a(e,,),0(en))),
1(ek))
return
:= (V U {a(en), d(e” )1,
e\ek U
\ w(ea) U w(ek )1, Z)
[3] dee
[4] Add edge
normalize(w(e,,), ci(en d(en )),
4e,))
return
:= (Vu fa(ea), d(en )1, U {4),
W U {w(en )), U ft(e:)p
end
</figure>
<figureCaption confidence="0.9836265">
Figure 4: An algorithm for adding edges to hy-
pergraphs
</figureCaption>
<bodyText confidence="0.999843">
of a matching hyperdege. In practice this is not
needed and we could check a smaller amount of
vertices. We do this by introducing a maximal
time gap which gives us advice how far (in mea-
sures of time) we look backwards from the start
vertex of a new edge to be inserted into the hy-
pergraph to determine a compatible hyperedge
of the hypergraph.
</bodyText>
<subsectionHeader confidence="0.512958">
Additional Paths
</subsectionHeader>
<figureCaption confidence="0.947002">
Figure 5: Additional paths by time mapping
</figureCaption>
<bodyText confidence="0.9999468">
It is not possible to add el and e2 to the hy-
peredge eg since they would introduce an over-
lap between the sets of start and end vertices
of the potential new hyperedge. The resulting
hyperedges of adding e3 —65 are depicted below.
</bodyText>
<subsectionHeader confidence="0.898865">
Score Normalization
</subsectionHeader>
<bodyText confidence="0.98860865">
Score normalization is a necessary means if
one wants to compare hypotheses of different
lengths. Thus, edges in word graphs are as-
signed normalized scores that account for words
of different extensions in time. The usual mea-
sure is the score per frame, which is computed
Score per word
by taking
Length of word in frames •
When combining several word edges as we
do by constructing hyperedges, the combina-
tion should be assigned a single value that re-
flects a certain useful aspect of the originating
edges. In order not to exclude certain hypothe-
ses from consideration in score-driven language
processing modules, the score of the hyperedge
is inherited from the best-rated word hypothe-
sis (cf. (Weber, 1995)). We use the minimum of
the source acoustic scores, which corresponds to
a highest recognition probability.
</bodyText>
<subsectionHeader confidence="0.983248">
Introducing a Maximal Time Gap
</subsectionHeader>
<bodyText confidence="0.999990388888889">
The algorithm depicted in figure 4 can be
speeded up for practical reasons. Each vertex
between the graph root and the start vertex of
the new edge could be one of the start vertices
It is possible to introduce additional paths
into a graph by performing time mapping. Con-
sider fig. 5 as an example. Taken as a normal
word graph, it contains two label sequences,
namely a-c-d and b-c-e. However, if time
mapping is performed for the edges labelled c,
two additional sequences are introduced: a-c-e
and b-c-d. Thus, time mapping by hyper-
graphs is not information preserving in a strong
sense. For practical applications this does not
present any problems. The situations in which
additional label sequences are introduced are
quite rare, we did not observe any linguistic dif-
ference in our experiments.
</bodyText>
<subsectionHeader confidence="0.998696">
2.4 Edge Combination
</subsectionHeader>
<bodyText confidence="0.999570818181818">
Besides merging the combination of hyperedges,
to construct edges with new content is an impor-
tant task within any speech processing module,
e.g. for parsing. The assumption we will adopt
here is that two hyperedges el, e2 E E may be
combined if they are adjacent, i.e. they share a
common vertex: 0(ei ) n a(e2) 0 0. The label
of the new edge en (which may be used to rep-
resent linguistic content) is determined by the
component building it, whereas the start and
end vertices are determined by
</bodyText>
<equation confidence="0.999938">
a(en) := a(ei) (16)
0(en) := 0(62) (17)
</equation>
<page confidence="0.987851">
59
</page>
<bodyText confidence="0.999897083333333">
This approach is quite analogous to edge com-
bination methods for normal graphs, e.g. in
chart parsing, where two edges are equally re-
quired to have a meeting point. However, score
computation for hyperedge combination is more
difficult. The goal is to determine a score per
frame by selecting the smallest possible score
under all possible meeting vertices. It is de-
rived by examining all possible connecting ver-
tices (all elements of / := )3(ei) n a(e2)) and
computing the resulting score of the new edge:
If w(el) &lt; w(e2), we use
</bodyText>
<equation confidence="0.998443">
w( e.) w(ei)-(t&lt; —t(a&gt; (1)))4-w(e2).(t(0&gt;(e2))—t&lt;)
t(13&gt;(e2))—t(a&gt; (et))
</equation>
<bodyText confidence="0.994718">
where t&lt; = minIt(v)Iv E /1. If, on the other
hand, w(ei) &gt; w(e2), we use
</bodyText>
<equation confidence="0.982816">
w ( en ) tu(ei ).(t&gt; -t(ct&lt; )))+w(e2).(t(O&lt; (e2))-t&gt; )
</equation>
<bodyText confidence="0.983854">
where t&gt; = max{t(v) iv E I}.
</bodyText>
<sectionHeader confidence="0.984186" genericHeader="method">
3 Experiments with Hypergraphs
</sectionHeader>
<bodyText confidence="0.999144444444445">
The method of converting word graphs to hy-
pergraphs has been used in two experiments so
far. One of them is devoted to the study of
connectionist unification in speech applications
(Weber, forthcoming). The other one, from
which the performance figures in this section
are drawn, is an experimental speech transla-
tion system focusing on incremental operation
and uniform representation (Amtrup, 1997).
</bodyText>
<figure confidence="0.99437625">
2000 -
1000 -
1000 2000 3000 4000 5000
# Edges
</figure>
<figureCaption confidence="0.999885">
Figure 6: Word edge reduction
</figureCaption>
<bodyText confidence="0.999994730769231">
We want to show the effect of hypergraphs re-
garding edge reduction and parsing effort. In or-
der to provide real-world figures, we used word
graphs produced by the Hamburg speech recog-
nition system (Huebener et al., 1996). The
test data consisted of one dialogue within the
Verbmobil domain. There were 41 turns with
an average length of 4.65s speaking time per
turn. The word graphs contained 1828 edges
on the average. Figure 6 shows the amount of
reduction in the number of edges by converting
the graphs into hypergraphs. On the average,
1671 edges were removed (mapped), leaving 157
edges in hypergraphs, approximately 91% less
than the original word graphs.
Next, we used both sets of graphs (the orig-
inal word graphs and hypergraphs) as input
to the speech parser used in (Amtrup, 1997).
This parser is an incremental active chart parser
which uses a typed feature formalism to describe
linguistic entities. The grammar is focussed on
partial parsing and contains rules mainly for
noun phrases, prepositional phrases and such.
The integration of complete utterances is ne-
glected. Figure 7 shows the reduction in terms
of chart edges at completion time.
</bodyText>
<figure confidence="0.963588">
-4o- Without mapping
With mapping
0 2000 4000
# Edges
</figure>
<figureCaption confidence="0.999953">
Figure 7: Chart edge reduction
</figureCaption>
<bodyText confidence="0.999577777777778">
The amount of reduction concerning parsing
effort is much less impressive than pure edge
reduction. On the average, parsing of complete
graphs resulted in 15547 chart edges, while pars-
ing of hypergraphs produced 3316 chart edges,
a reduction of about 79%. Due to edge combi-
nations, one could have expected a much higher
value. The reason for this fact lies mainly with
the redundancy test used in the parser. There
</bodyText>
<figure confidence="0.996116714285714">
4000 -
3000 -
Fo
.c
ax 1000 7
-at- Maximum gaps 100
10000
</figure>
<page confidence="0.979244">
60
</page>
<bodyText confidence="0.999491916666667">
are many instances of edges which are not in-
serted into the chart at all, because identical
hypotheses are already present.
Consequently, the amount of reduction in
parse time is within the same bounds. Pars-
ing ordinary graphs took 87.7s, parsing of hy-
pergraphs 6.4s, a reduction of 93%. There are
some extreme cases of word graphs, where hy-
pergraph parsing was 94 times faster than word
graph parsing. One of the turns had to be ex-
cluded from the test set, because it could not
be fully parsed as word graph.
</bodyText>
<figure confidence="0.996529">
10001
100
L.7 10
A. 1.00
0.10
0.01
-40- Without mapping
-a- With mapping
2000 40100 6063
# Edges
</figure>
<figureCaption confidence="0.999847">
Figure 8: Parsing time reduction
</figureCaption>
<sectionHeader confidence="0.999445" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999778">
In this paper, we have proposed the application
of hypergraph techniques to word graph pars-
ing. Motivated by linguistic properties of spon-
taneously spoken speech, we argued that bun-
dles of edges in word graphs should be treated in
an integrated manner. We introduced interval
graphs and directed hypergraphs as representa-
tion devices. Directed hypergraphs extend the
notion of a family of edges in that they are able
to represent edges having several start and end
vertices.
We gave a formal definition of word graphs
and the necessary extensions to cover hyper-
graphs. The conditions that have to be fulfilled
in order to merge two hyperedges and to com-
bine two adjacent hyperedges were stated in a
formal way; an algorithm to integrate a word
hypothesis into a hypergraph was presented.
We proved the applicability of our mecha-
nisms by parsing one dialogue of real-world spo-
ken utterances. Using hypergraphs resulted in a
91% reduction of initial edges in the graph and
a 79% reduction in the total number of chart
edges. Parsing hypergraphs instead of ordinary
word graphs reduced the parsing time by 93%.
</bodyText>
<sectionHeader confidence="0.99943" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887809523809">
Jan W. Amtrup, Henrik Heine, and Uwe Jost.
1997. What&apos;s in a Word Graph — Evaluation
and Enhancement of Word Lattices. In Proc.
of Eurospeech 1997, Rhodes, Greece, Septem-
ber.
Jan W. Amtrup. 1997. Layered Charts for
Speech Translation. In Proceedings of the
Seventh International Conference on Theo-
retical and Methodological Issues in Machine
Translation, TMI &apos;97, Santa Fe, NM, July.
Xavier Aubert and Hermann Ney. 1995. Large
Vocabulary Continuous Speech Recognition
Using Word Graphs. In ICASSP 95.
Michel Gondran and Michel Minoux. 1984.
Graphs and algorithms. Wiley-Interscience
Series in Discrete Mathematics. John Wiley
&amp; Sons, Chichester.
Kai Huebener, Uwe Jost, and Henrik Heine.
1996. Speech Recognition for Spontaneously
Spoken German Dialogs. In ICSLP96,
Philadelphia.
Martin Oerder and Hermann Ney. 1993.
Word Graphs: An Efficient Interface Be-
tween Continuous-Speech Recognition and
Language Understanding. In Proceedings
of the 1993 IEEE International Conference
on Acoustics, Speech eY Signal Processing,
ICASSP, pages I1/119-11/122, Minneapolis,
MN.
Hans Weber. 1992. Chartparsing in ASL-Nord:
Berichte zu den Arbeitspaketen PI bis P9.
Technical Report ASL-TR-28-92/UER, Uni-
versitat Erlangen-Niirnberg, Erlangen, De-
cember.
Hans Weber. 1995. LR-inkrementelles, Pro b-
abilistisches Chartparsing von Worth ypothe-
sengraphen mit Unifikationsgrammatiken:
Eine Enge Kopplung von Suche und Analyse.
Ph.D. thesis, Universitat Hamburg.
Volker Weber. forthcoming. Funktionales Kon-
nektionistisches Unifikationsbasiertes Pars-
ing. Ph.D. thesis, Univ. Hamburg.
</reference>
<page confidence="0.999273">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.785771">
<title confidence="0.999917">Time Mapping with Hypergraphs</title>
<author confidence="0.999967">Jan W Amtrup Volker Weber</author>
<affiliation confidence="0.9992355">Computing Research Laboratory University of Hamburg, New Mexico State University Computer Science Department,</affiliation>
<address confidence="0.995925">Las Cruces, NM 88003,USA Vogt-K011n-Str. 30, D-22527 Hamburg, Germany</address>
<email confidence="0.813027">j.nmsu.eduormatik.uni-hamburg.de</email>
<abstract confidence="0.99847952631579">Word graphs are able to represent a large number of different utterance hypotheses in a very compact manner. However, usually they contain a huge amount of redundancy in terms of word hypotheses that cover almost identical intervals in time. We address this problem by introducing hypergraphs for speech processing. Hypergraphs can be classified as an extension to word graphs and charts, their edges possibly having several start and end vertices. By converting ordinary word graphs to hypergraphs one can reduce the number of edges considerably. We define hypergraphs formally, present an algorithm to convert word graphs into hypergraphs and state consistency properties for edges and their combination. Finally, we present some empirical results concerning graph size and parsing efficiency.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jan W Amtrup</author>
<author>Henrik Heine</author>
<author>Uwe Jost</author>
</authors>
<title>What&apos;s in a Word Graph — Evaluation and Enhancement of Word Lattices.</title>
<date>1997</date>
<booktitle>In Proc. of Eurospeech</booktitle>
<location>Rhodes, Greece,</location>
<contexts>
<context position="2254" citStr="Amtrup et al., 1997" startWordPosition="359" endWordPosition="362">ast not for reasonable sized applications given todays speech recognition technology. To overcome this problem, in most current systems word graphs are used as speech-language interface. Word graphs offer a simple and efficient means to represent a very high number of utterance hypotheses in a extremely compact way (Oerder and Ney, 1993; Aubert and Ney, 1995). Figure 1: Two families of edges in a word graph Although they are compact, the use of word graphs leads to problems by itself. One of them is the current lack of a reasonable measure for word graph size and evaluation of their contents (Amtrup et al., 1997). The problem we want to address in this paper is the presence of a large number of almost identical word hypotheses. By almost identical we mean that the start and end vertices of edges differ only slightly. Consider figure 1 as an example section of a word graph. There are several word hypotheses representing the words unci (and) and dann (then). The start and end points of them differ by small numbers of frames, each of them 10ms long. The reasons for the existence of these families of edges are at least twofold: • Standard HMM-based word recognizers try to start (and finish) word models at</context>
</contexts>
<marker>Amtrup, Heine, Jost, 1997</marker>
<rawString>Jan W. Amtrup, Henrik Heine, and Uwe Jost. 1997. What&apos;s in a Word Graph — Evaluation and Enhancement of Word Lattices. In Proc. of Eurospeech 1997, Rhodes, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan W Amtrup</author>
</authors>
<title>Layered Charts for Speech Translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation, TMI &apos;97,</booktitle>
<location>Santa Fe, NM,</location>
<contexts>
<context position="18005" citStr="Amtrup, 1997" startWordPosition="3176" endWordPosition="3177">&lt;) t(13&gt;(e2))—t(a&gt; (et)) where t&lt; = minIt(v)Iv E /1. If, on the other hand, w(ei) &gt; w(e2), we use w ( en ) tu(ei ).(t&gt; -t(ct&lt; )))+w(e2).(t(O&lt; (e2))-t&gt; ) where t&gt; = max{t(v) iv E I}. 3 Experiments with Hypergraphs The method of converting word graphs to hypergraphs has been used in two experiments so far. One of them is devoted to the study of connectionist unification in speech applications (Weber, forthcoming). The other one, from which the performance figures in this section are drawn, is an experimental speech translation system focusing on incremental operation and uniform representation (Amtrup, 1997). 2000 - 1000 - 1000 2000 3000 4000 5000 # Edges Figure 6: Word edge reduction We want to show the effect of hypergraphs regarding edge reduction and parsing effort. In order to provide real-world figures, we used word graphs produced by the Hamburg speech recognition system (Huebener et al., 1996). The test data consisted of one dialogue within the Verbmobil domain. There were 41 turns with an average length of 4.65s speaking time per turn. The word graphs contained 1828 edges on the average. Figure 6 shows the amount of reduction in the number of edges by converting the graphs into hypergrap</context>
</contexts>
<marker>Amtrup, 1997</marker>
<rawString>Jan W. Amtrup. 1997. Layered Charts for Speech Translation. In Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation, TMI &apos;97, Santa Fe, NM, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Aubert</author>
<author>Hermann Ney</author>
</authors>
<title>Large Vocabulary Continuous Speech Recognition Using Word Graphs.</title>
<date>1995</date>
<booktitle>In ICASSP 95.</booktitle>
<contexts>
<context position="1995" citStr="Aubert and Ney, 1995" startWordPosition="311" endWordPosition="314"> the recognizer to a subsequent module (e.g. a parser). A slight extension over this best chain mode would be to deliver n-best chains to improve language processing results. However, it is usually not enough to deliver just the best 10 or 20 utterances, at least not for reasonable sized applications given todays speech recognition technology. To overcome this problem, in most current systems word graphs are used as speech-language interface. Word graphs offer a simple and efficient means to represent a very high number of utterance hypotheses in a extremely compact way (Oerder and Ney, 1993; Aubert and Ney, 1995). Figure 1: Two families of edges in a word graph Although they are compact, the use of word graphs leads to problems by itself. One of them is the current lack of a reasonable measure for word graph size and evaluation of their contents (Amtrup et al., 1997). The problem we want to address in this paper is the presence of a large number of almost identical word hypotheses. By almost identical we mean that the start and end vertices of edges differ only slightly. Consider figure 1 as an example section of a word graph. There are several word hypotheses representing the words unci (and) and dan</context>
</contexts>
<marker>Aubert, Ney, 1995</marker>
<rawString>Xavier Aubert and Hermann Ney. 1995. Large Vocabulary Continuous Speech Recognition Using Word Graphs. In ICASSP 95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Gondran</author>
<author>Michel Minoux</author>
</authors>
<title>Graphs and algorithms. Wiley-Interscience Series in Discrete Mathematics.</title>
<date>1984</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>Chichester.</location>
<contexts>
<context position="5374" citStr="Gondran and Minoux, 1984" startWordPosition="899" endWordPosition="902">d graphs. Edges of interval graphs do not have individual start and end vertices, but instead use intervals to denote the range of applicability of an edge. The major problem with interval graphs lies with the complexity of edge access methods. However, many formal statements shown below will use interval arithmetics, as the argument will be easier to follow. The approach we take in this paper is to use hypergraphs as representation medium for word graphs. What one wants is to carry out operations only once and record the fact that there are several start and end points of words. Hypergraphs (Gondran and Minoux, 1984, p. 30) are generalizations of ordinary graphs that allow multiple start and end vertices of edges. We extend the approach of H. Weber (Weber, 1995) for time mapping. Weber considered sets of edges with identical start vertices but slightly different end vertices, for which the notion family was introduced. We use full hypergraphs as representation and thus additionally allow several start vertices, which results in a further decrease of 6% in terms of resulting chart edges while parsing (cf. section 3). Figure 2 shows the example section using hyperedges for the two families of edges. We ado</context>
</contexts>
<marker>Gondran, Minoux, 1984</marker>
<rawString>Michel Gondran and Michel Minoux. 1984. Graphs and algorithms. Wiley-Interscience Series in Discrete Mathematics. John Wiley &amp; Sons, Chichester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Huebener</author>
<author>Uwe Jost</author>
<author>Henrik Heine</author>
</authors>
<title>Speech Recognition for Spontaneously Spoken German Dialogs. In</title>
<date>1996</date>
<booktitle>ICSLP96,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="18304" citStr="Huebener et al., 1996" startWordPosition="3228" endWordPosition="3231">riments so far. One of them is devoted to the study of connectionist unification in speech applications (Weber, forthcoming). The other one, from which the performance figures in this section are drawn, is an experimental speech translation system focusing on incremental operation and uniform representation (Amtrup, 1997). 2000 - 1000 - 1000 2000 3000 4000 5000 # Edges Figure 6: Word edge reduction We want to show the effect of hypergraphs regarding edge reduction and parsing effort. In order to provide real-world figures, we used word graphs produced by the Hamburg speech recognition system (Huebener et al., 1996). The test data consisted of one dialogue within the Verbmobil domain. There were 41 turns with an average length of 4.65s speaking time per turn. The word graphs contained 1828 edges on the average. Figure 6 shows the amount of reduction in the number of edges by converting the graphs into hypergraphs. On the average, 1671 edges were removed (mapped), leaving 157 edges in hypergraphs, approximately 91% less than the original word graphs. Next, we used both sets of graphs (the original word graphs and hypergraphs) as input to the speech parser used in (Amtrup, 1997). This parser is an incremen</context>
</contexts>
<marker>Huebener, Jost, Heine, 1996</marker>
<rawString>Kai Huebener, Uwe Jost, and Henrik Heine. 1996. Speech Recognition for Spontaneously Spoken German Dialogs. In ICSLP96, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Oerder</author>
<author>Hermann Ney</author>
</authors>
<title>Word Graphs: An Efficient Interface Between Continuous-Speech Recognition and Language Understanding.</title>
<date>1993</date>
<booktitle>In Proceedings of the 1993 IEEE International Conference on Acoustics, Speech eY Signal Processing, ICASSP,</booktitle>
<pages>1--119</pages>
<location>Minneapolis, MN.</location>
<contexts>
<context position="1972" citStr="Oerder and Ney, 1993" startWordPosition="307" endWordPosition="310">ble word sequence from the recognizer to a subsequent module (e.g. a parser). A slight extension over this best chain mode would be to deliver n-best chains to improve language processing results. However, it is usually not enough to deliver just the best 10 or 20 utterances, at least not for reasonable sized applications given todays speech recognition technology. To overcome this problem, in most current systems word graphs are used as speech-language interface. Word graphs offer a simple and efficient means to represent a very high number of utterance hypotheses in a extremely compact way (Oerder and Ney, 1993; Aubert and Ney, 1995). Figure 1: Two families of edges in a word graph Although they are compact, the use of word graphs leads to problems by itself. One of them is the current lack of a reasonable measure for word graph size and evaluation of their contents (Amtrup et al., 1997). The problem we want to address in this paper is the presence of a large number of almost identical word hypotheses. By almost identical we mean that the start and end vertices of edges differ only slightly. Consider figure 1 as an example section of a word graph. There are several word hypotheses representing the w</context>
</contexts>
<marker>Oerder, Ney, 1993</marker>
<rawString>Martin Oerder and Hermann Ney. 1993. Word Graphs: An Efficient Interface Between Continuous-Speech Recognition and Language Understanding. In Proceedings of the 1993 IEEE International Conference on Acoustics, Speech eY Signal Processing, ICASSP, pages I1/119-11/122, Minneapolis, MN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Weber</author>
</authors>
<title>Chartparsing in ASL-Nord: Berichte zu den Arbeitspaketen PI bis P9.</title>
<date>1992</date>
<tech>Technical Report ASL-TR-28-92/UER,</tech>
<institution>Universitat Erlangen-Niirnberg,</institution>
<location>Erlangen,</location>
<contexts>
<context position="4085" citStr="Weber, 1992" startWordPosition="677" endWordPosition="678">t. Thus, for most words, there is a whole set of word hypotheses in a word graph which results in several meets between two sets of hypotheses. Both facts are disadvantageous for speech processing: Many word edges result in a high number of lexical lookups and basic operations (e.g. bottom-up proposals of syntactic categories); many meeting points between edges result in a high number of possibly complex operations (like unifications in a parser). The most obvious way to reduce the number of neighboring, identically labeled edges is to reduce the time resolution provided by a word recognizer (Weber, 1992). If a word edge is to be processed, the start and end vertices are mapped to the more coarse grained points in time used by linguistic modules and a redundancy check is carried out in order to prevent multiple copies of edges. This can be easily done, but one has to face the drawback on introducing many more paths through the graph due to artificially constructed overlaps. Furthermore, it is not simple to choose a correct resolution, as the intervals effectively appearing with word onsets and offsets change considerably with words spoken. Also, the introduction of cycles has to be avoided. A </context>
</contexts>
<marker>Weber, 1992</marker>
<rawString>Hans Weber. 1992. Chartparsing in ASL-Nord: Berichte zu den Arbeitspaketen PI bis P9. Technical Report ASL-TR-28-92/UER, Universitat Erlangen-Niirnberg, Erlangen, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Weber</author>
</authors>
<title>LR-inkrementelles, Pro babilistisches Chartparsing von Worth ypothesengraphen mit Unifikationsgrammatiken: Eine Enge Kopplung von Suche und Analyse.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Hamburg.</institution>
<contexts>
<context position="5523" citStr="Weber, 1995" startWordPosition="927" endWordPosition="928">major problem with interval graphs lies with the complexity of edge access methods. However, many formal statements shown below will use interval arithmetics, as the argument will be easier to follow. The approach we take in this paper is to use hypergraphs as representation medium for word graphs. What one wants is to carry out operations only once and record the fact that there are several start and end points of words. Hypergraphs (Gondran and Minoux, 1984, p. 30) are generalizations of ordinary graphs that allow multiple start and end vertices of edges. We extend the approach of H. Weber (Weber, 1995) for time mapping. Weber considered sets of edges with identical start vertices but slightly different end vertices, for which the notion family was introduced. We use full hypergraphs as representation and thus additionally allow several start vertices, which results in a further decrease of 6% in terms of resulting chart edges while parsing (cf. section 3). Figure 2 shows the example section using hyperedges for the two families of edges. We adopt the way of dealing with different acoustical scores of word hypotheses from Weber. Figure 2: Two hyperedges representing families of edges 2 Word </context>
<context position="15310" citStr="Weber, 1995" startWordPosition="2720" endWordPosition="2721">Thus, edges in word graphs are assigned normalized scores that account for words of different extensions in time. The usual measure is the score per frame, which is computed Score per word by taking Length of word in frames • When combining several word edges as we do by constructing hyperedges, the combination should be assigned a single value that reflects a certain useful aspect of the originating edges. In order not to exclude certain hypotheses from consideration in score-driven language processing modules, the score of the hyperedge is inherited from the best-rated word hypothesis (cf. (Weber, 1995)). We use the minimum of the source acoustic scores, which corresponds to a highest recognition probability. Introducing a Maximal Time Gap The algorithm depicted in figure 4 can be speeded up for practical reasons. Each vertex between the graph root and the start vertex of the new edge could be one of the start vertices It is possible to introduce additional paths into a graph by performing time mapping. Consider fig. 5 as an example. Taken as a normal word graph, it contains two label sequences, namely a-c-d and b-c-e. However, if time mapping is performed for the edges labelled c, two addit</context>
</contexts>
<marker>Weber, 1995</marker>
<rawString>Hans Weber. 1995. LR-inkrementelles, Pro babilistisches Chartparsing von Worth ypothesengraphen mit Unifikationsgrammatiken: Eine Enge Kopplung von Suche und Analyse. Ph.D. thesis, Universitat Hamburg.</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Funktionales Konnektionistisches Unifikationsbasiertes Parsing.</title>
<tech>Ph.D. thesis,</tech>
<institution>Univ. Hamburg.</institution>
<marker>forthcoming, </marker>
<rawString>Volker Weber. forthcoming. Funktionales Konnektionistisches Unifikationsbasiertes Parsing. Ph.D. thesis, Univ. Hamburg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>