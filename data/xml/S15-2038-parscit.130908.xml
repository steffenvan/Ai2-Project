<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005392">
<title confidence="0.992432">
JAIST: Combining multiple features for Answer Selection in Community
Question Answering
</title>
<author confidence="0.999132">
Quan Hung Tran1, Vu Duc Tran1, Tu Thanh Vu2, Minh Le Nguyen1, Son Bao Pham2
</author>
<affiliation confidence="0.9963995">
1Japan Advanced Institute of Science and Technology
2University of Engineering and Technology, Vietnam National University, Hanoi
</affiliation>
<email confidence="0.924978">
1{ quanth,vu.tran,nguyenml}@jaist.ac.jp
2{ tuvt,sonpb}@vnu.edu.vn
</email>
<sectionHeader confidence="0.998334" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996031363636364">
In this paper, we describe our system for
SemEval-2015 Task 3: Answer Selection in
Community Question Answering. In this task,
the systems are required to identify the good
or potentially good answers from the answer
thread in Community Question Answering
collections. Our system combines 16 features
belong to 5 groups to predict answer quality.
Our final model achieves the best result in sub-
task A for English, both in accuracy and F1-
score.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986116">
Nowadays, community question answering (cQA)
websites like Yahoo! Answers play a crucial role
in supporting people to seek desired information.
Users can post their questions on these sites for find-
ing help as well as personal advice. However, the
quality of these answers varies greatly. Typically,
only a few of the answers in an answer thread are
useful to the users and it may take a lot of efforts to
identify them manually. Thus, a system that auto-
matically identifies answer quality is much needed.
The task of identifying answer quality has been
studied by many researchers in the field of Question
Answering. Many methods have been proposed:
web redundancy information (Magnini et al., 2002),
non-textual features (Jeon et al., 2006), textual en-
tailment (Wang and Neumann, 2007), syntactic fea-
tures (Crrundstr¨om and Nugues, 2014). However,
most of these works used independent dataset and
evaluation metrics; thus it is difficult to compare
the results of these methods. The SEMEVAL task
3 (M`arquez et al., 2015) addresses this problem by
providing a common framework to compare differ-
ent methods in multiple languages.
Our system incorporates a range of features:
word-matching features, special component fea-
tures, topic-modeling-based features, translation-
based features and non-textual features to achieve
the best performance in subtask A (M`arquez et al.,
2015). In the remainder of the paper, we will de-
scribe our system with the focus on the features.
</bodyText>
<sectionHeader confidence="0.980521" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.99995675">
For extracting the features, we first preprocess the
questions and the answers then build a number of
models based on training data or other sources (Fig-
ure 1).
</bodyText>
<subsectionHeader confidence="0.995064">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999943875">
All the questions and the answers are preprocessed
through the following steps: Tokenization, POS-
tagging, Syntactic parsing, Dependency parsing,
Lemmatization, Stopword removal, Name-Entity
recognition. These preprocessing steps are com-
pleted using The Stanford CoreNLP Natural Lan-
guage Processing Toolkit (Manning et al., 2014).
Because of the noisy nature of community data, the
syntactic parsing, dependency parsing and Name-
Entity recognition steps do not produce highly ac-
curate results. Thus, we rely mainly on the bag-of-
word representation of text. Removing stopwords or
lemmatization can alter the meaning of the text, so in
the system, we keep both the original version and the
processed version of the text. The choice between
using the two versions is made using experiments in
</bodyText>
<page confidence="0.988241">
215
</page>
<note confidence="0.7509755">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 215–219,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999759">
Figure 1: System components
</figureCaption>
<bodyText confidence="0.836986">
development set.
</bodyText>
<subsectionHeader confidence="0.999692">
2.2 Building models from data
</subsectionHeader>
<bodyText confidence="0.999974533333333">
In this section, we describe the resources we use,
or build for extracting features, these resources are:
Translation models, LDA models, Word vector rep-
resentation models, Word Lists. The translation
models are built to bridge the lexical chasm be-
tween the questions and the answers (Surdeanu et
al., 2008). In previous works (Jeon et al., 2005;
Zhou et al., 2011), monolingual translation mod-
els between questions have been successfully used
in finding similar questions in Question Answering
archive. We adapt the idea and build translation
models between the questions and their answers us-
ing the training data and the Qatar Living forum
data. We treat the question-answer pairs similar to
dual language sentence pairs in machine translation.
First, each question-answer pair is tokenized and all
special characters are removed. In the process, if any
answer has too few tokens (less than two tokens), it
is removed from the training data. Then the trans-
lation probabilities are calculated by IBM Model 1
(Brown et al., 1993) and Hidden Markov Model.
Each model is trained with 200 iterations. The cal-
culated translation probabilities help us to calculate
the probability that an answer is the translation of
the question. The translation feature will be detailed
in Section 2.3.
We build two topic models, the first one is trained
in the training data, the second one is trained in
Wikipedia data1 using Gensim toolkit (ˇReh˚uˇrek and
Sojka, 2010) and Mallet toolkit (McCallum, 2002).
</bodyText>
<footnote confidence="0.9966675">
1The compressed version of all article from Wikipedia
downloaded at http://dumps.wikimedia.org/enwiki/
</footnote>
<bodyText confidence="0.999423">
These LDA models have 100 topics. The choice be-
tween which model will be used is based on experi-
ments in the development set.
We experiment with two word vector represen-
tation models built using Word2Vec tool (Mikolov
et al., 2013), the first one is pre-trained word2vec
model provided by the authors, and the second one
is trained from the Qatar Living forum data. Our
Word2Vec model was built with word vector size of
300, window size of 3 (n-skip-gram, n=3) and mini-
mum word frequency of 1. In Section 2.3, we detail
how to extract feature using these models.
We also build several word lists from the training
set to extract features:
</bodyText>
<listItem confidence="0.887037666666667">
• The words that usually appear on each type of
answers (Good, Bad, Potential).
• The words pairs (one from the question, one
</listItem>
<bodyText confidence="0.77519625">
from the good answers) that have high fre-
quency in the training set. We aim to extract the
information about word collocations through
this list.
</bodyText>
<subsectionHeader confidence="0.97205">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.993271">
Word-matching feature group: This feature group
exploits the surface word-based similarity between
the Question and the Answer to assign score:
</bodyText>
<listItem confidence="0.996729">
• Cosine similarity:
</listItem>
<equation confidence="0.97683225">
�n ui×vi
cosine sim = i=1 (1)
(ui)2 [� (vi)2
i=1 i=1
</equation>
<bodyText confidence="0.999923666666667">
With u and v are binary bag of words vectors
(with stopwords are removed), ui is the i-th di-
mension of vector u and n is vector size. This
</bodyText>
<page confidence="0.993466">
216
</page>
<bodyText confidence="0.993041">
feature returns the cosine similarity between
question vector and answer vector.
</bodyText>
<listItem confidence="0.991387684210526">
• Dependency cosine similarity: We represent
the questions and the answers as bag of word-
dependency, with words are associated with
their dependency label in the dependency tree.
For example: a dependency arc in the depen-
dency tree: prep(buy-4, for-7) will generate the
following word-dependency: prep-by-for. We
consider the sentence to be the collection of
these word-dependencies. The cosine similar-
ity score is calculated similar to bag-of-word
cosine similarity.
• Word alignment: We also use the Meteor
toolkit (Denkowski and Lavie, 2014) to align
the words from the question and the answers,
and use the alignment score returned as a fea-
ture in the feature space
• Noun match: This feature is similar to Cosine
similarity feature, however; only nouns are re-
tained in the bag-of-word.
</listItem>
<bodyText confidence="0.772022">
Special-component feature group: This feature
group identifies the special characteristics of the an-
swers that show the answer quality:
</bodyText>
<listItem confidence="0.879603125">
• Special words feature: This feature identifies if
an answer contains some of the special tokens
(question marks, laugh symbols). Typically, the
posts that contains this type of tokens are not
a serious answer (laugh symbols), or a further
question (question marks). The laugh symbols
are identified using a regular expression.
• Typical words feature: This feature identifies if
an answer contains some specific words that are
typical for an answer quality class (good, bad,
potential). The typical word lists are built using
training data and described in the previous sec-
tion. After the experiment step, however, only
the typical word list for bad answers was found
to be effective and was used in the final version
of the system.
</listItem>
<bodyText confidence="0.749333666666667">
Non-textual feature group: This feature group
exploits some non-textual information of the posts
in the answer thread to assign answer quality:
</bodyText>
<listItem confidence="0.886898545454545">
• Question author feature: This feature identifies
if an answer in the answer thread belongs to the
author of the question. If a post belongs to the
author of the question, it is very unlikely to be
an answer.
• Question category: We also include the ques-
tion category (27 categories) in the feature
space because we found out that the quality dis-
tribution of different types of question are very
different.
• The number of posts from the same user: We
</listItem>
<bodyText confidence="0.93946428">
include the number of posts from the same user
as a feature because we observe that if a user
has a large number of posts, most of them will
be non-informative, irrelevant to the original
question.
Topic model based feature: We use the previ-
ously mentioned LDA models to transform ques-
tions and answers to topic vectors and calculate the
cosine similarity between the topic vectors of the
question and its answers. We use this feature be-
cause a question and its correct answer should be
about similar topics. After experimenting on the de-
velopment set, only the LDA model built from train-
ing data is effective and thus, it is used in the final
system.
Word Vector representation based feature: We
use the word vector representation to model the
relevance between the question and the answer.
All the questions and answers are tokenized and
the words are transformed to vector using the pre-
trained word2vec model. Each word in the ques-
tion will then be aligned to the word in the answer
that has the highest vector cosine similarity. The re-
turned value will be the sum of the scores of these
alignments normalized by the question’s length:
</bodyText>
<equation confidence="0.998358">
align(wi) = max (cosine(wi, w&apos;�)) (2)
0&lt;j≤m 7
word2vec sim = En
i=1 align(wi) (3)
n
</equation>
<bodyText confidence="0.9970166">
With cosine(wi,w�)) is the cosine similarity of two
vector representations of i-th word in the question
with the j-th word in the answer. n and m are the
length (in number of words) of the question and the
answer respectively.
</bodyText>
<page confidence="0.994686">
217
</page>
<bodyText confidence="0.999278">
Translation based feature: We use the previ-
ously mentioned translation models to find the word
to word alignments between the question and the an-
swer. This feature is calculated similar to the Word
Vector representation based feature. Each word in
the question will be aligned with the word in the an-
swer with the highest translation score. The feature
value will be the sum of translation scores normal-
ized by question’ length.
</bodyText>
<subsectionHeader confidence="0.999548">
2.4 System run configuration
</subsectionHeader>
<bodyText confidence="0.999958909090909">
The straightforward way to identify the quality
classes for answers is using a classification model.
However, the classification model has problem in
identifying the Potential class. In our experiments,
the classification model ignores the Potential class
entirely. This problem may be caused by our feature
design as the features actually aim to identify either
good or bad answers.
To solve this problem, we use another approach.
As we observe the data, most of the Potential an-
swers can be considered “Not good enough” and
“Not bad enough”. An answer which is not quite
good nor quite bad can be considered “Potential”,
thus using a regression model2 to score the quality
of the answer would probably be better. In our ex-
periment with the development data, the regression
model outperforms the classification model by 3.4
F-measure score.
Features are extracted from the answers (with
their questions treated as the context), and then
the feature values are passed through a regression
model. However, the provided data only has qual-
ity classes but not regression value, thus we need to
assign the regression value for each answer quality
class: 1.0 for Good answers, 0.5 for Potential an-
swers, and 0.0 for Bad answers.
Our system runs are different in the feature space.
Our best run (JAIST-contrastive1) uses all the fea-
tures described above. Our main run (JAIST-
primary) excludes the topic-modeling based feature
while the third run (JAIST-contrastive2) includes
several other experimental features that did not have
contribution when tested on the development set.
</bodyText>
<footnote confidence="0.6298075">
2We use SVM-regression model in WEKA toolkit (Hall et
al., 2009)
</footnote>
<tableCaption confidence="0.997923">
Table 1: System performance
</tableCaption>
<table confidence="0.9994975">
Runs F1-score Accuracy Rank
primary 57.19 (%) 72.52 (%) 2
contrastive1 57.29 (%) 72.67 (%) 1
contrastive2 46.96 (%) 57.74 (%) 18
</table>
<tableCaption confidence="0.713413">
Table 2: Detail Class F1-score
</tableCaption>
<table confidence="0.990604">
Runs F1-score
Good 78.96 (%)
Bad 78.24 (%)
Potential 14.36 (%)
</table>
<sectionHeader confidence="0.994265" genericHeader="method">
3 Result and Discussion
</sectionHeader>
<bodyText confidence="0.999989125">
We only take part in subtask A for English. Our
system has the best accuracy and F1-score in sub-
task A (primary runs) shown in Table 1. Classifying
the Potential class is quite difficult (M`arquez et al.,
2015) and our system only achieve 14.36 % F1 score
on this class. Although the use of regression model
partly solves this problem, our feature space is not
adequate for identifying this class reliably (Table 2)
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999905545454545">
In this paper, we present our approach for the sub-
task A - English of the SEMEVAL 2015 task 3 - An-
swer Selection in Community Question Answering.
We propose an Answer quality scoring based ap-
proach for classifying answers in Community Ques-
tion Answering. Our system achieves high results
in the task, however, does not handle the Potential
class well. A possible explanation is that we still rely
heavily on the bag-of-word representation of text. In
the future, other semantically rich representations of
text would be employed to improve performance.
</bodyText>
<sectionHeader confidence="0.999315" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9975428">
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. The Mathematics of
Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311, June
1993.
</reference>
<page confidence="0.991344">
218
</page>
<reference confidence="0.999495264705882">
Jakob Grundstr¨om and Pierre Nugues. Using Syntactic
Features in Answer Reranking. In AAAI 2014 Work-
shop on Cognitive Computing for Augmented Human
Intelligence, pages 13–19, 2014.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten. The
WEKA Data Mining Software: An Update. SIGKDD
Explor. Newsl., 11(1):10–18, November 2009.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. Find-
ing Similar Questions in Large Question and Answer
Archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, CIKM ’05, pages 84–90, New York,
NY, USA, 2005.
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. A Framework to Predict the Quality of Answers
with Non-textual Features. In Proceedings of the 29th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval, SI-
GIR ’06, pages 228–235, New York, NY, USA, 2006.
Michael Denkowski Alon Lavie. Meteor Universal: Lan-
guage Specific Translation Evaluation for Any Target
Language. ACL 2014, page 376, 2014.
Bernardo Magnini, Matteo Negri, Roberto Prevete, and
Hristo Tanev. Is it the right answer?: exploiting web
redundancy for Answer Validation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 425–432, 2002.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David McClosky.
The Stanford CoreNLP natural language processing
toolkit. In Proceedings of 52nd Annual Meeting of
the Association for Computational Linguistics: System
Demonstrations, pages 55–60, 2014.
Andrew Kachites McCallum. MALLET: A
Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu, 2002.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efficient Estimation of Word Representations
in Vector Space. CoRR, abs/1301.3781, 2013.
Llu´ıs M`arquez, James Glass, Walid Magdy, Alessan-
dro Moschitti, Preslav Nakov, and Bilal Randeree.
SemEval-2015 Task 3: Answer Selection in Commu-
nity Question Answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval 2015).
Radim ˇReh˚uˇrek and Petr Sojka. Software Framework for
Topic Modelling with Large Corpora. In Proceedings
of the LREC 2010 Workshop on New Challenges for
NLP Frameworks, pages 45–50, Valletta, Malta, May
2010.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. Learning to rank answers on large online
QA collections. In In Proceedings of the 46th An-
nual Meeting for the Association for Computational
Linguistics: Human Language Technologies (ACL-08:
HLT, pages 719–727, 2008.
Rui Wang and G¨unter Neumann. DFKILT at AVE 2007:
Using Recognizing Textual Entailment for Answer
Validation. In In online proceedings of CLEF 2007
Working Notes, ISBN, pages 2–912335, 2007.
Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu.
Phrase-based Translation Model for Question Re-
trieval in Community Question Answer Archives. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 653–
662, Stroudsburg, PA, USA, 2011.
</reference>
<page confidence="0.999216">
219
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858434">
<title confidence="0.99394">JAIST: Combining multiple features for Answer Selection in Question Answering</title>
<author confidence="0.998901">Hung Vu Duc Tu Thanh Minh Le_Son Bao</author>
<affiliation confidence="0.9948855">Advanced Institute of Science and of Engineering and Technology, Vietnam National University,</affiliation>
<abstract confidence="0.989261">In this paper, we describe our system for SemEval-2015 Task 3: Answer Selection in Community Question Answering. In this task, the systems are required to identify the good or potentially good answers from the answer thread in Community Question Answering collections. Our system combines 16 features belong to 5 groups to predict answer quality. Our final model achieves the best result in subtask A for English, both in accuracy and F1score.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4580" citStr="Brown et al., 1993" startWordPosition="702" endWordPosition="705">tions have been successfully used in finding similar questions in Question Answering archive. We adapt the idea and build translation models between the questions and their answers using the training data and the Qatar Living forum data. We treat the question-answer pairs similar to dual language sentence pairs in machine translation. First, each question-answer pair is tokenized and all special characters are removed. In the process, if any answer has too few tokens (less than two tokens), it is removed from the training data. Then the translation probabilities are calculated by IBM Model 1 (Brown et al., 1993) and Hidden Markov Model. Each model is trained with 200 iterations. The calculated translation probabilities help us to calculate the probability that an answer is the translation of the question. The translation feature will be detailed in Section 2.3. We build two topic models, the first one is trained in the training data, the second one is trained in Wikipedia data1 using Gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) and Mallet toolkit (McCallum, 2002). 1The compressed version of all article from Wikipedia downloaded at http://dumps.wikimedia.org/enwiki/ These LDA models have 100 topics. Th</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311, June 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Grundstr¨om</author>
<author>Pierre Nugues</author>
</authors>
<title>Using Syntactic Features in Answer Reranking.</title>
<date>2014</date>
<booktitle>In AAAI 2014 Workshop on Cognitive Computing for Augmented Human Intelligence,</booktitle>
<pages>13--19</pages>
<marker>Grundstr¨om, Nugues, 2014</marker>
<rawString>Jakob Grundstr¨om and Pierre Nugues. Using Syntactic Features in Answer Reranking. In AAAI 2014 Workshop on Cognitive Computing for Augmented Human Intelligence, pages 13–19, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12316" citStr="Hall et al., 2009" startWordPosition="1988" endWordPosition="1991"> data only has quality classes but not regression value, thus we need to assign the regression value for each answer quality class: 1.0 for Good answers, 0.5 for Potential answers, and 0.0 for Bad answers. Our system runs are different in the feature space. Our best run (JAIST-contrastive1) uses all the features described above. Our main run (JAISTprimary) excludes the topic-modeling based feature while the third run (JAIST-contrastive2) includes several other experimental features that did not have contribution when tested on the development set. 2We use SVM-regression model in WEKA toolkit (Hall et al., 2009) Table 1: System performance Runs F1-score Accuracy Rank primary 57.19 (%) 72.52 (%) 2 contrastive1 57.29 (%) 72.67 (%) 1 contrastive2 46.96 (%) 57.74 (%) 18 Table 2: Detail Class F1-score Runs F1-score Good 78.96 (%) Bad 78.24 (%) Potential 14.36 (%) 3 Result and Discussion We only take part in subtask A for English. Our system has the best accuracy and F1-score in subtask A (primary runs) shown in Table 1. Classifying the Potential class is quite difficult (M`arquez et al., 2015) and our system only achieve 14.36 % F1 score on this class. Although the use of regression model partly solves th</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. The WEKA Data Mining Software: An Update. SIGKDD Explor. Newsl., 11(1):10–18, November 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding Similar Questions in Large Question and Answer Archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05,</booktitle>
<pages>84--90</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="3896" citStr="Jeon et al., 2005" startWordPosition="593" endWordPosition="596">iments in 215 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 215–219, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: System components development set. 2.2 Building models from data In this section, we describe the resources we use, or build for extracting features, these resources are: Translation models, LDA models, Word vector representation models, Word Lists. The translation models are built to bridge the lexical chasm between the questions and the answers (Surdeanu et al., 2008). In previous works (Jeon et al., 2005; Zhou et al., 2011), monolingual translation models between questions have been successfully used in finding similar questions in Question Answering archive. We adapt the idea and build translation models between the questions and their answers using the training data and the Qatar Living forum data. We treat the question-answer pairs similar to dual language sentence pairs in machine translation. First, each question-answer pair is tokenized and all special characters are removed. In the process, if any answer has too few tokens (less than two tokens), it is removed from the training data. T</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. Finding Similar Questions in Large Question and Answer Archives. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05, pages 84–90, New York, NY, USA, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A Framework to Predict the Quality of Answers with Non-textual Features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’06,</booktitle>
<pages>228--235</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="1572" citStr="Jeon et al., 2006" startWordPosition="239" endWordPosition="242">esired information. Users can post their questions on these sites for finding help as well as personal advice. However, the quality of these answers varies greatly. Typically, only a few of the answers in an answer thread are useful to the users and it may take a lot of efforts to identify them manually. Thus, a system that automatically identifies answer quality is much needed. The task of identifying answer quality has been studied by many researchers in the field of Question Answering. Many methods have been proposed: web redundancy information (Magnini et al., 2002), non-textual features (Jeon et al., 2006), textual entailment (Wang and Neumann, 2007), syntactic features (Crrundstr¨om and Nugues, 2014). However, most of these works used independent dataset and evaluation metrics; thus it is difficult to compare the results of these methods. The SEMEVAL task 3 (M`arquez et al., 2015) addresses this problem by providing a common framework to compare different methods in multiple languages. Our system incorporates a range of features: word-matching features, special component features, topic-modeling-based features, translationbased features and non-textual features to achieve the best performance </context>
</contexts>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. A Framework to Predict the Quality of Answers with Non-textual Features. In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’06, pages 228–235, New York, NY, USA, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
</authors>
<title>Alon Lavie. Meteor Universal: Language Specific Translation Evaluation for Any Target Language.</title>
<date>2014</date>
<booktitle>ACL 2014,</booktitle>
<pages>376</pages>
<marker>Denkowski, 2014</marker>
<rawString>Michael Denkowski Alon Lavie. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. ACL 2014, page 376, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Matteo Negri</author>
</authors>
<title>Roberto Prevete, and Hristo Tanev. Is it the right answer?: exploiting web redundancy for Answer Validation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>425--432</pages>
<marker>Magnini, Negri, 2002</marker>
<rawString>Bernardo Magnini, Matteo Negri, Roberto Prevete, and Hristo Tanev. Is it the right answer?: exploiting web redundancy for Answer Validation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 425–432, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="2829" citStr="Manning et al., 2014" startWordPosition="426" endWordPosition="429">. In the remainder of the paper, we will describe our system with the focus on the features. 2 System Description For extracting the features, we first preprocess the questions and the answers then build a number of models based on training data or other sources (Figure 1). 2.1 Preprocessing All the questions and the answers are preprocessed through the following steps: Tokenization, POStagging, Syntactic parsing, Dependency parsing, Lemmatization, Stopword removal, Name-Entity recognition. These preprocessing steps are completed using The Stanford CoreNLP Natural Language Processing Toolkit (Manning et al., 2014). Because of the noisy nature of community data, the syntactic parsing, dependency parsing and NameEntity recognition steps do not produce highly accurate results. Thus, we rely mainly on the bag-ofword representation of text. Removing stopwords or lemmatization can alter the meaning of the text, so in the system, we keep both the original version and the processed version of the text. The choice between using the two versions is made using experiments in 215 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 215–219, Denver, Colorado, June 4-5, 2015. c�</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu,</location>
<contexts>
<context position="5039" citStr="McCallum, 2002" startWordPosition="778" endWordPosition="779">w tokens (less than two tokens), it is removed from the training data. Then the translation probabilities are calculated by IBM Model 1 (Brown et al., 1993) and Hidden Markov Model. Each model is trained with 200 iterations. The calculated translation probabilities help us to calculate the probability that an answer is the translation of the question. The translation feature will be detailed in Section 2.3. We build two topic models, the first one is trained in the training data, the second one is trained in Wikipedia data1 using Gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) and Mallet toolkit (McCallum, 2002). 1The compressed version of all article from Wikipedia downloaded at http://dumps.wikimedia.org/enwiki/ These LDA models have 100 topics. The choice between which model will be used is based on experiments in the development set. We experiment with two word vector representation models built using Word2Vec tool (Mikolov et al., 2013), the first one is pre-trained word2vec model provided by the authors, and the second one is trained from the Qatar Living forum data. Our Word2Vec model was built with word vector size of 300, window size of 3 (n-skip-gram, n=3) and minimum word frequency of 1. I</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781,</location>
<contexts>
<context position="5375" citStr="Mikolov et al., 2013" startWordPosition="828" endWordPosition="831">lation of the question. The translation feature will be detailed in Section 2.3. We build two topic models, the first one is trained in the training data, the second one is trained in Wikipedia data1 using Gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) and Mallet toolkit (McCallum, 2002). 1The compressed version of all article from Wikipedia downloaded at http://dumps.wikimedia.org/enwiki/ These LDA models have 100 topics. The choice between which model will be used is based on experiments in the development set. We experiment with two word vector representation models built using Word2Vec tool (Mikolov et al., 2013), the first one is pre-trained word2vec model provided by the authors, and the second one is trained from the Qatar Living forum data. Our Word2Vec model was built with word vector size of 300, window size of 3 (n-skip-gram, n=3) and minimum word frequency of 1. In Section 2.3, we detail how to extract feature using these models. We also build several word lists from the training set to extract features: • The words that usually appear on each type of answers (Good, Bad, Potential). • The words pairs (one from the question, one from the good answers) that have high frequency in the training se</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. CoRR, abs/1301.3781, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>James Glass</author>
</authors>
<title>Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. SemEval-2015 Task 3: Answer Selection in Community Question Answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>M`arquez, Glass, 2015</marker>
<rawString>Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. SemEval-2015 Task 3: Answer Selection in Community Question Answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta, Malta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta, May 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting for the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT,</booktitle>
<pages>719--727</pages>
<contexts>
<context position="3858" citStr="Surdeanu et al., 2008" startWordPosition="586" endWordPosition="589"> using the two versions is made using experiments in 215 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 215–219, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: System components development set. 2.2 Building models from data In this section, we describe the resources we use, or build for extracting features, these resources are: Translation models, LDA models, Word vector representation models, Word Lists. The translation models are built to bridge the lexical chasm between the questions and the answers (Surdeanu et al., 2008). In previous works (Jeon et al., 2005; Zhou et al., 2011), monolingual translation models between questions have been successfully used in finding similar questions in Question Answering archive. We adapt the idea and build translation models between the questions and their answers using the training data and the Qatar Living forum data. We treat the question-answer pairs similar to dual language sentence pairs in machine translation. First, each question-answer pair is tokenized and all special characters are removed. In the process, if any answer has too few tokens (less than two tokens), i</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. Learning to rank answers on large online QA collections. In In Proceedings of the 46th Annual Meeting for the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT, pages 719–727, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>G¨unter Neumann</author>
</authors>
<title>DFKILT at AVE 2007: Using Recognizing Textual Entailment for Answer Validation. In</title>
<date>2007</date>
<booktitle>In online proceedings of CLEF 2007 Working Notes, ISBN,</booktitle>
<pages>2--912335</pages>
<contexts>
<context position="1617" citStr="Wang and Neumann, 2007" startWordPosition="246" endWordPosition="249"> questions on these sites for finding help as well as personal advice. However, the quality of these answers varies greatly. Typically, only a few of the answers in an answer thread are useful to the users and it may take a lot of efforts to identify them manually. Thus, a system that automatically identifies answer quality is much needed. The task of identifying answer quality has been studied by many researchers in the field of Question Answering. Many methods have been proposed: web redundancy information (Magnini et al., 2002), non-textual features (Jeon et al., 2006), textual entailment (Wang and Neumann, 2007), syntactic features (Crrundstr¨om and Nugues, 2014). However, most of these works used independent dataset and evaluation metrics; thus it is difficult to compare the results of these methods. The SEMEVAL task 3 (M`arquez et al., 2015) addresses this problem by providing a common framework to compare different methods in multiple languages. Our system incorporates a range of features: word-matching features, special component features, topic-modeling-based features, translationbased features and non-textual features to achieve the best performance in subtask A (M`arquez et al., 2015). In the </context>
</contexts>
<marker>Wang, Neumann, 2007</marker>
<rawString>Rui Wang and G¨unter Neumann. DFKILT at AVE 2007: Using Recognizing Textual Entailment for Answer Validation. In In online proceedings of CLEF 2007 Working Notes, ISBN, pages 2–912335, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Li Cai</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
</authors>
<title>Phrase-based Translation Model for Question Retrieval in Community Question Answer Archives.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>653--662</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="3916" citStr="Zhou et al., 2011" startWordPosition="597" endWordPosition="600">edings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 215–219, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: System components development set. 2.2 Building models from data In this section, we describe the resources we use, or build for extracting features, these resources are: Translation models, LDA models, Word vector representation models, Word Lists. The translation models are built to bridge the lexical chasm between the questions and the answers (Surdeanu et al., 2008). In previous works (Jeon et al., 2005; Zhou et al., 2011), monolingual translation models between questions have been successfully used in finding similar questions in Question Answering archive. We adapt the idea and build translation models between the questions and their answers using the training data and the Qatar Living forum data. We treat the question-answer pairs similar to dual language sentence pairs in machine translation. First, each question-answer pair is tokenized and all special characters are removed. In the process, if any answer has too few tokens (less than two tokens), it is removed from the training data. Then the translation </context>
</contexts>
<marker>Zhou, Cai, Zhao, Liu, 2011</marker>
<rawString>Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. Phrase-based Translation Model for Question Retrieval in Community Question Answer Archives. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 653– 662, Stroudsburg, PA, USA, 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>