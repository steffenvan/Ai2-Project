<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009121">
<title confidence="0.978565">
Learning Summary Prior Representation for Extractive Summarization
</title>
<author confidence="0.98897">
Ziqiang Cao&apos;,2∗ Furu Wei3 Sujian Li&apos; ,2 Wenjie Li4 Ming Zhou3 Houfeng Wang&apos; ,2
</author>
<affiliation confidence="0.8673645">
&apos;Key Laboratory of Computational Linguistics, Peking University, MOE, China
2Collaborative Innovation Center for Language Ability, Xuzhou, Jiangsu, China
</affiliation>
<address confidence="0.78077">
3Microsoft Research, Beijing, China
4Computing Department, Hong Kong Polytechnic University, Hong Kong
</address>
<email confidence="0.9466605">
{ziqiangyeah, lisujian, wanghf}@pku.edu.cn
{furu, mingzhou}@microsoft.com cswjli@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.997264" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999598210526316">
In this paper, we propose the concept
of summary prior to define how much
a sentence is appropriate to be selected
into summary without consideration of
its context. Different from previous
work using manually compiled document-
independent features, we develop a novel
summary system called PriorSum, which
applies the enhanced convolutional neu-
ral networks to capture the summary
prior features derived from length-variable
phrases. Under a regression framework,
the learned prior features are concate-
nated with document-dependent features
for sentence ranking. Experiments on the
DUC generic summarization benchmarks
show that PriorSum can discover different
aspects supporting the summary prior and
outperform state-of-the-art baselines.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986633017857143">
Sentence ranking, the vital part of extractive
summarization, has been extensively investigated.
Regardless of ranking models (Osborne, 2002;
Galley, 2006; Conroy et al., 2004; Li et al.,
2007), feature engineering largely determines the
final summarization performance. Features often
fall into two types: document-dependent features
(e.g., term frequency or position) and document-
independent features (e.g., stopword ratio or word
polarity). The latter type of features take effects
due to the fact that, a sentence can often be judged
by itself whether it is appropriate to be included
in a summary no matter which document it lies in.
Take the following two sentences as an example:
1. Hurricane Emily slammed into Dominica on
September 22, causing 3 deaths with its wind
gusts up to 110 mph.
∗Contribution during internship at Microsoft Research
2. It was Emily, the hurricane which caused 3
deaths and armed with wind guests up to 110
mph, that slammed into Dominica on Tues-
day.
The first sentence describes the major information
of a hurricane. With similar meaning, the second
sentence uses an emphatic structure and is some-
what verbose. Obviously the first one should be
preferred for a news summary. In this paper, we
call such fact as summary prior nature1 and learn
document-independent features to reflect it.
In previous summarization systems, though not
well-studied, some widely-used sentence ranking
features such as the length and the ratio of stop-
words, can be seen as attempts to measure the
summary prior nature to a certain extent. Notably,
Hong and Nenkova (2014) built a state-of-the-art
summarization system through making use of ad-
vanced document-independent features. However,
these document-independent features are usually
hand-crafted, difficult to exhaust each aspect of
the summary prior nature. Meanwhile, items rep-
resenting the same feature may contribute differ-
ently to a summary. For example, “September 22”
and “Tuesday” are both indicators of time, but the
latter seldom occurs in a summary due to uncer-
tainty. In addition, to the best of our knowledge,
document-independent features beyond word level
(e.g., phrases) are seldom involved in current re-
search.
The CTSUM system developed by Wan and
Zhang (2014) is the most relevant to ours. It at-
tempted to explore a context-free measure named
certainty which is critical to ranking sentences in
summarization. To calculate the certainty score,
four dictionaries are manually built as features and
a corpus is annotated to train the feature weights
using Support Vector Regression (SVR). How-
</bodyText>
<footnote confidence="0.998238">
1In this paper, “summary prior features” and “document-
independent features” hold the same meaning.
</footnote>
<page confidence="0.790883">
829
</page>
<bodyText confidence="0.961523585365854">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 829–833,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
ever, a low certainty score does not always rep-
resent low quality of being a summary sentence.
For example, the sentence below is from a topic
about “Korea nuclear issue” in DUC 2004: Clin-
ton acknowledged that U.S. is not yet certain that
the suspicious underground construction project
in North Korea is nuclear related. The under-
lined phrases greatly reduce the certainty of this
sentence according to Wan and Zhang (2014)’s
model. But, in fact, this sentence can summarize
the government’s attitude and is salient enough in
the related documents. Thus, in our opinion, cer-
tainty can just be viewed as a specific aspect of the
summary prior nature.
To this end, we develop a novel summarization
system called PriorSum to automatically exploit
all possible semantic aspects latent in the sum-
mary prior nature. Since the Convolutional Neural
Networks (CNNs) have shown promising progress
in latent feature representation (Yih et al., 2014;
Shen et al., 2014; Zeng et al., 2014), PriorSum
applies CNNs with multiple filters to capture a
comprehensive set of document-independent fea-
tures derived from length-variable phrases. Then
we adopt a two-stage max-over-time pooling op-
eration to associate these filters since phrases
with different lengths may express the same as-
pect of summary prior. PriorSum generates the
document-independent features, and concatenates
them with document-dependent ones to work for
sentence regression (Section 2.1).
We conduct extensive experiments on the DUC
2001, 2002 and 2004 generic multi-document
summarization datasets. The experimental results
demonstrate that our model outperforms state-
of-the-art extractive summarization approaches.
Meanwhile, we analyze the different aspects sup-
porting the summary prior in Section 3.3.
</bodyText>
<sectionHeader confidence="0.994828" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999906578947368">
Our summarization system PriorSum follows the
traditional extractive framework (Carbonell and
Goldstein, 1998; Li et al., 2007). Specifically, the
sentence ranking process scores and ranks the sen-
tences from documents, and then the sentence se-
lection process chooses the top ranked sentences
to generate the final summary in accordance with
the length constraint and redundancy among the
selected sentences.
Sentence ranking aims to measure the saliency
score of a sentence with consideration of both
document-dependent and document-independent
features. In this study, we apply an enhanced ver-
sion of convolutional neural networks to automati-
cally generate document-independent features ac-
cording to the summary prior nature. Meanwhile,
some document-dependent features are extracted.
These two types of features are combined in the
sentence regression step.
</bodyText>
<subsectionHeader confidence="0.998577">
2.1 Sentence Ranking
</subsectionHeader>
<bodyText confidence="0.999980705882353">
PriorSum improves the standard convolutional
neural networks (CNNs) to learn the summary
prior since CNN is able to learn compressed rep-
resentation of n-grams effectively and tackle sen-
tences with variable lengths naturally. We first
introduce the standard CNNs, based on which
we design our improved CNNs for obtaining
document-independent features.
The standard CNNs contain a convolution oper-
ation over several word embeddings, followed by
a pooling operation. Let vi E Rk denote the k-
dimensional word embedding of the ith word in
the sentence. Assume vi:i+j to be the concatena-
tion of word embeddings vi, · · · , vi+j. A convo-
lution operation involves a filter Wht E Rl×hk,
which operates on a window of h words to pro-
duce a new feature with l dimensions:
</bodyText>
<equation confidence="0.864301">
ch i= f(Wht x vi:i+h−1) (1)
</equation>
<bodyText confidence="0.999831272727273">
where f is a non-linear function and tanh is used
like common practice. Here, the bias term is
ignored for simplicity. Then Wht is applied to
each possible window of h words in the sentence
of length N to produce a feature map: Ch =
[ch1, · · · , chN−h+1]. Next, we adopt the widely-
used max-over-time pooling operation (Collobert
et al., 2011) to obtain the final features ˆch from
Ch. That is, ˆch = max{Ch}. The idea behind
this pooling operation is to capture the most im-
portant features in a feature map.
In the standard CNNs, only the fixed-length
windows of words are considered to represent a
sentence. As we know, the variable-length phrases
composed of a sentence can better express the sen-
tence and disclose its summary prior nature. To
make full use of the phrase information, we design
an improved version of the standard CNNs, which
use multiple filters for different window sizes as
well as two max-over-time pooling operations to
get the final summary prior representation. Specif-
ically, let W1t , · · · , Wm t be m filters for window
</bodyText>
<page confidence="0.980187">
830
</page>
<bodyText confidence="0.999906333333333">
sizes from 1 to m, and correspondingly we can
obtain m feature maps C1, · · · , C&apos;. For each fea-
ture map CZ, We first adopt a max-over-time pool-
ing operation max{CZ} with the goal of capturing
the most salient features from each window size i.
Next, a second max-over-time pooling operation
is operated on all the windows to acquire the most
representative features. To formulate, the docu-
ment independent features xp can be generated by:
</bodyText>
<equation confidence="0.990117">
xp = max{max{C1}, · · · , max{C&apos;}}. (2)
</equation>
<bodyText confidence="0.997544166666667">
Kim (2014) also uses filters with varying win-
dow sizes for sentence-level classification tasks.
However, he reserves all the representations gen-
erated by filters to a fully connected output layer.
This practice greatly enlarges following parame-
ters and ignores the relation among phrases with
different lengths. Hence we use the two-stage
max-over-time pooling to associate all these fil-
ters.
Besides the features xp obtained through
the CNNs, we also extract several document-
dependent features notated as xe, shown in Table
1. In the end, xp is combined with xe to con-
duct sentence ranking. Here we follow the regres-
sion framework of Li et al. (2007). The sentence
saliency y is scored by ROUGE-2 (Lin, 2004)
(stopwords removed) and the model tries to esti-
mate this saliency.
</bodyText>
<equation confidence="0.9998415">
φ = [xp, xe] (3)
yˆ = wr × φ (4)
</equation>
<bodyText confidence="0.99345875">
where wr E Rl+|xe |is the regression weights.
We use linear transformation since it is convenient
to compare with regression baselines (see Section
3.2).
</bodyText>
<table confidence="0.65823">
Feature Description
POSITION The position of the sentence.
</table>
<tableCaption confidence="0.7663122">
AVG-TF The averaged term frequency values of
words in the sentence.
AVG-CF The averaged cluster frequency values of
words in the sentence.
Table 1: Extracted document-dependent features.
</tableCaption>
<subsectionHeader confidence="0.998512">
2.2 Sentence Selection
</subsectionHeader>
<bodyText confidence="0.999967538461539">
A summary is obliged to offer both informative
and non-redundant content. Here, we employ a
simple greedy algorithm to select sentences, simi-
lar to the MMR strategy (Carbonell and Goldstein,
1998). Firstly, we remove sentences less than 8
words (as in Erkan and Radev (2004)) and sort the
rest in descending order according to the estimated
saliency scores. Then, we iteratively dequeue one
sentence, and append it to the current summary if
it is non-redundant. A sentence is considered non-
redundant if it contains more new words compared
to the current summary content. We empirically
set the cut-off of new word ratio to 0.5.
</bodyText>
<sectionHeader confidence="0.999948" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999735">
3.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999982423076923">
In our work, we focus on the generic multi-
document summarization task and carry out ex-
periments on DUC 2001 2004 datasets. All the
documents are from newswires and grouped into
various thematic clusters. The summary length is
limited to 100 words (665 bytes for DUC 2004).
We use DUC 2003 data as the development set and
conduct a 3-fold cross-validation on DUC 2001,
2002 and 2004 datasets with two years of data as
training set and one year of data as test set.
We directly use the look-up table of 25-
dimensional word embeddings trained by the
model of Collobert et al. (2011). These small
word embeddings largely reduces model param-
eters. The dimension l of the hidden document-
independent features is experimented in the range
of [1, 40], and the window sizes are experimented
between 1 and 5. Through parameter experiments
on development set, we set l = 20 and m = 3 for
PriorSum. To update the weights W�h and wr, we
apply the diagonal variant of AdaGrad with mini-
batches (Duchi et al., 2011).
For evaluation, we adopt the widely-used auto-
matic evaluation metric ROUGE (Lin, 2004), and
take ROUGE-1 and ROUGE-2 as the main mea-
sures.
</bodyText>
<subsectionHeader confidence="0.99998">
3.2 Comparison with Baseline Methods
</subsectionHeader>
<bodyText confidence="0.986116636363636">
To evaluate the summarization performance of Pri-
orSum, we compare it with the best peer systems
(PeerT, Peer26 and Peer65 in Table 2) participat-
ing DUC evaluations. We also choose as baselines
those state-of-the-art summarization results on
DUC (2001, 2002, and 2004) data. To our knowl-
edge, the best reported results on DUC 2001,
2002 and 2004 are from R2N2 (Cao et al., 2015),
ClusterCMRW (Wan and Yang, 2008) and REG-
SUM2 (Hong and Nenkova, 2014) respectively.
R2N2 applies recursive neural networks to learn
</bodyText>
<footnote confidence="0.974015">
2REGSUM truncates a summary to 100 words.
</footnote>
<page confidence="0.998088">
831
</page>
<bodyText confidence="0.999921230769231">
feature combination. ClusterCMRW incorporates
the cluster-level information into the graph-based
ranking algorithm. REGSUM is a word regres-
sion approach based on some advanced features
such as word polarities (Wiebe et al., 2005) and
categories (Tausczik and Pennebaker, 2010). For
these three systems, we directly cite their pub-
lished results, marked with the sign “*” as in Ta-
ble 2. Meanwhile, LexRank (Erkan and Radev,
2004), a commonly-used graph-based summariza-
tion model, is introduced as an extra baseline.
Comparing with this baseline can demonstrate the
performance level of regression approaches. The
baseline StandardCNN means that we adopt the
standard CNNS with fixed window size for sum-
mary prior representation.
To explore the effects of the learned summary
prior representations, we design a baseline sys-
tem named Reg Manual which adopts manually-
compiled document-independent features such as
NUMBER (whether number exist), NENTITY
(whether named entities exist) and STOPRATIO
(the ratio of stopwords). Then we combine these
features with document-dependent features in Ta-
ble 1 and tune the feature weights through LIB-
LINEAR3 support vector regression.
From Table 2, we can see that PriorSum can
achieve a comparable performance to the state-
of-the-art summarization systems R2N2, Cluster-
CMRW and REGSUM. With respect to baselines,
PriorSum significantly4 outperforms Reg Manual
which uses manually compiled features and
the graph-based summarization system LexRank.
Meanwhile, PriorSum always enjoys a reasonable
increase over StandardCNN, which verifies the ef-
fects of the enhanced CNNs. It is noted that Stan-
dardCNN can also achieve the state-of-the-art per-
formance, indicating the summary prior represen-
tation really works.
</bodyText>
<subsectionHeader confidence="0.99962">
3.3 Analysis
</subsectionHeader>
<bodyText confidence="0.99971975">
In this section, we explore what PriorSum learns
according to the summary prior representations.
Since the convolution layer follows a linear regres-
sion output, we apply a simple strategy to measure
how much the learned document-independent fea-
tures contribute to the saliency estimation. Specif-
ically, for each sentence, we ignore its document-
dependent features through setting their values as
</bodyText>
<footnote confidence="0.993691">
3http://www.csie.ntu.edu.tw/˜cjlin/
liblinear/
4T-test with P-value ≤ 0.05
</footnote>
<table confidence="0.999968368421053">
Year System ROUGE-1 ROUGE-2
2001 PeerT 33.03 7.86
R2N2* 35.88 7.64
LexRank 33.43 6.09
Reg Manual 34.55 7.18
StandardCNN 35.19 7.63
PriorSum 35.98 7.89
2002 Peer26 35.15 7.64
ClusterCMRW* 38.55 8.65
LexRank 35.29 7.54
Reg Manual 34.81 8.12
StandardCNN 35.73 8.69
PriorSum 36.63 8.97
2004 Peer65 37.88 9.18
REGSUM* 38.57 9.75
LexRank 37.87 8.88
Reg Manual 37.05 9.34
StandardCNN 37.90 9.93
PriorSum 38.91 10.07
</table>
<tableCaption confidence="0.999648">
Table 2: Comparison results (%) on DUC datasets.
</tableCaption>
<bodyText confidence="0.9152715">
Meanwhile, Yugoslavia’s P.M. told an emer-
gency session Monday that the country is faced
with war.
high The rebels ethnic Tutsis, disenchanted members
scored of President Laurent Kabila’s army took up arms,
creating division among Congo’s 400 tribes.
The blast killed two assailants, wounded 21 Is-
raelis and prompted Israel to suspend implemen-
tation of the peace accord with the Palestinians.
The greatest need is that many, many of us have
been psychologically traumatized, and very, very
few are receiving help.
low Ruben Rivera: An impatient hitter who will
scored chase pitches out of the strike zone.
I think we should worry about tuberculosis and
the risk to the general population.
</bodyText>
<tableCaption confidence="0.703203">
Table 3: Example sentences selected by prior
scores.
</tableCaption>
<bodyText confidence="0.999852789473684">
zeros and then apply a linear transformation using
the weight wr to get a summary prior score xp.
The greater the score, the more possible a sentence
is to be included in a summary without context
consideration. We analyze what intuitive features
are hidden in the summary prior representation.
From Table 3, first we find that high-scored
sentences contains more named entities and num-
bers, which conforms to human intuition. By
contrast, the features NENTITY and NUMBER
in Reg Manual hold very small weights, only
2%,3% compared with the most significant fea-
ture AVG-CF. One possible reason is that named
entities or numbers are not independent features.
For example, “month + number” is a common
timestamp for an event whereas “number + a.m.”
is over-detailed and seldom appears in a summary.
We can also see that low-scored sentences are rel-
atively informal and fail to provide facts, which
</bodyText>
<page confidence="0.990595">
832
</page>
<bodyText confidence="0.9988286">
are difficult for human to generalize some spe-
cific features. For instance, informal sentences
seem to have more stopwords but the feature STO-
PRATIO holds a relatively large positive weight in
Reg Manual.
</bodyText>
<sectionHeader confidence="0.995921" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999609">
This paper proposes a novel summarization sys-
tem called PriorSum to automatically learn sum-
mary prior features for extractive summariza-
tion. Experiments on the DUC generic multi-
document summarization task show that our pro-
posed method outperforms state-of-the-art ap-
proaches. In addition, we demonstrate the dom-
inant sentences discovered by PriorSum, and the
results verify that our model can learn different as-
pects of summary prior.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999961875">
We thank all the anonymous reviewers for their in-
sightful comments. This work was partially sup-
ported by National Key Basic Research Program
of China (No. 2014CB340504), National Natural
Science Foundation of China (No. 61273278 and
61272291), and National Social Science Founda-
tion of China (No: 12&amp;ZD227). The correspon-
dence author of this paper is Sujian Li.
</bodyText>
<sectionHeader confidence="0.999088" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999531666666667">
Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming
Zhou. 2015. Ranking with recursive neural net-
works and its application to multi-document sum-
marization. In AAAI-2015.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR, pages 335–336.
Ronan Collobert, Jason Weston, Lon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
John M Conroy, Judith D Schlesinger, Jade Goldstein,
and Dianne P Oleary. 2004. Left-brain/right-brain
multi-document summarization. In Proceedings of
DUC.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
G¨unes Erkan and Dragomir R Radev. 2004.
Lexrank: Graph-based lexical centrality as salience
in text summarization. J. Artif. Intell. Res.(JAIR),
22(1):457–479.
Michel Galley. 2006. A skip-chain conditional random
field for ranking meeting utterances by importance.
In Proceedings of EMNLP, pages 364–372.
Kai Hong and Ani Nenkova. 2014. Improving
the estimation of word importance for news multi-
document summarization. In Proceedings of EACL.
Yoon Kim. 2014. Convolutional neural net-
works for sentence classification. arXiv preprint
arXiv:1408.5882.
Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007.
Multi-document summarization using support vec-
tor regression. In Proceedings of DUC.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
Workshop, pages 74–81.
Miles Osborne. 2002. Using maximum entropy for
sentence extraction. In Proceedings of ACL Work-
shop on Automatic Summarization, pages 1–8.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr´egoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
web search. In Companion publication of the 23rd
international conference on World wide web com-
panion, pages 373–374.
Yla R Tausczik and James W Pennebaker. 2010. The
psychological meaning of words: Liwc and comput-
erized text analysis methods. Journal of language
and social psychology, 29(1):24–54.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
summarization using cluster-based link analysis. In
Proceedings of SIGIR, pages 299–306.
Xiaojun Wan and Jianmin Zhang. 2014. Ctsum: ex-
tracting more certain summaries for news articles.
In Proceedings of the 37th international ACM SIGIR
conference on Research &amp; development in informa-
tion retrieval, pages 787–796. ACM.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language resources and evalua-
tion, 39(2-3):165–210.
Wen-tau Yih, Xiaodong He, and Christopher Meek.
2014. Semantic parsing for single-relation question
answering. In Proceedings of ACL.
Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,
and Jun Zhao. 2014. Relation classification via con-
volutional deep neural network. In Proceedings of
COLING, pages 2335–2344.
</reference>
<page confidence="0.999197">
833
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472136">
<title confidence="0.993742">Learning Summary Prior Representation for Extractive Summarization</title>
<affiliation confidence="0.7736815">Laboratory of Computational Linguistics, Peking University, MOE, Innovation Center for Language Ability, Xuzhou, Jiangsu,</affiliation>
<address confidence="0.9692755">Research, Beijing, China Department, Hong Kong Polytechnic University, Hong Kong</address>
<email confidence="0.9631825">lisujian,cswjli@comp.polyu.edu.hk</email>
<abstract confidence="0.999478">In this paper, we propose the concept of summary prior to define how much a sentence is appropriate to be selected into summary without consideration of its context. Different from work using manually compiled documentindependent features, we develop a novel summary system called PriorSum, which applies the enhanced convolutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ziqiang Cao</author>
<author>Furu Wei</author>
<author>Li Dong</author>
<author>Sujian Li</author>
<author>Ming Zhou</author>
</authors>
<title>Ranking with recursive neural networks and its application to multi-document summarization.</title>
<date>2015</date>
<booktitle>In AAAI-2015.</booktitle>
<contexts>
<context position="12651" citStr="Cao et al., 2015" startWordPosition="1994" endWordPosition="1997">gonal variant of AdaGrad with minibatches (Duchi et al., 2011). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004), and take ROUGE-1 and ROUGE-2 as the main measures. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn 2REGSUM truncates a summary to 100 words. 831 feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regression approach based on some advanced features such as word polarities (Wiebe et al., 2005) and categories (Tausczik and Pennebaker, 2010). For these three systems, we directly cite their published results, marked with the sign “*” as in Table 2. Meanwhile, LexRank (Erkan and Radev</context>
</contexts>
<marker>Cao, Wei, Dong, Li, Zhou, 2015</marker>
<rawString>Ziqiang Cao, Furu Wei, Li Dong, Sujian Li, and Ming Zhou. 2015. Ranking with recursive neural networks and its application to multi-document summarization. In AAAI-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>335--336</pages>
<contexts>
<context position="6108" citStr="Carbonell and Goldstein, 1998" startWordPosition="893" endWordPosition="896">ss the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental results demonstrate that our model outperforms stateof-the-art extractive summarization approaches. Meanwhile, we analyze the different aspects supporting the summary prior in Section 3.3. 2 Methodology Our summarization system PriorSum follows the traditional extractive framework (Carbonell and Goldstein, 1998; Li et al., 2007). Specifically, the sentence ranking process scores and ranks the sentences from documents, and then the sentence selection process chooses the top ranked sentences to generate the final summary in accordance with the length constraint and redundancy among the selected sentences. Sentence ranking aims to measure the saliency score of a sentence with consideration of both document-dependent and document-independent features. In this study, we apply an enhanced version of convolutional neural networks to automatically generate document-independent features according to the summ</context>
<context position="10630" citStr="Carbonell and Goldstein, 1998" startWordPosition="1646" endWordPosition="1649">wr × φ (4) where wr E Rl+|xe |is the regression weights. We use linear transformation since it is convenient to compare with regression baselines (see Section 3.2). Feature Description POSITION The position of the sentence. AVG-TF The averaged term frequency values of words in the sentence. AVG-CF The averaged cluster frequency values of words in the sentence. Table 1: Extracted document-dependent features. 2.2 Sentence Selection A summary is obliged to offer both informative and non-redundant content. Here, we employ a simple greedy algorithm to select sentences, similar to the MMR strategy (Carbonell and Goldstein, 1998). Firstly, we remove sentences less than 8 words (as in Erkan and Radev (2004)) and sort the rest in descending order according to the estimated saliency scores. Then, we iteratively dequeue one sentence, and append it to the current summary if it is non-redundant. A sentence is considered nonredundant if it contains more new words compared to the current summary content. We empirically set the cut-off of new word ratio to 0.5. 3 Experiments 3.1 Experiment Setup In our work, we focus on the generic multidocument summarization task and carry out experiments on DUC 2001 2004 datasets. All the do</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Lon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8011" citStr="Collobert et al., 2011" startWordPosition="1203" endWordPosition="1206">edding of the ith word in the sentence. Assume vi:i+j to be the concatenation of word embeddings vi, · · · , vi+j. A convolution operation involves a filter Wht E Rl×hk, which operates on a window of h words to produce a new feature with l dimensions: ch i= f(Wht x vi:i+h−1) (1) where f is a non-linear function and tanh is used like common practice. Here, the bias term is ignored for simplicity. Then Wht is applied to each possible window of h words in the sentence of length N to produce a feature map: Ch = [ch1, · · · , chN−h+1]. Next, we adopt the widelyused max-over-time pooling operation (Collobert et al., 2011) to obtain the final features ˆch from Ch. That is, ˆch = max{Ch}. The idea behind this pooling operation is to capture the most important features in a feature map. In the standard CNNs, only the fixed-length windows of words are considered to represent a sentence. As we know, the variable-length phrases composed of a sentence can better express the sentence and disclose its summary prior nature. To make full use of the phrase information, we design an improved version of the standard CNNs, which use multiple filters for different window sizes as well as two max-over-time pooling operations t</context>
<context position="11677" citStr="Collobert et al. (2011)" startWordPosition="1828" endWordPosition="1831">to 0.5. 3 Experiments 3.1 Experiment Setup In our work, we focus on the generic multidocument summarization task and carry out experiments on DUC 2001 2004 datasets. All the documents are from newswires and grouped into various thematic clusters. The summary length is limited to 100 words (665 bytes for DUC 2004). We use DUC 2003 data as the development set and conduct a 3-fold cross-validation on DUC 2001, 2002 and 2004 datasets with two years of data as training set and one year of data as test set. We directly use the look-up table of 25- dimensional word embeddings trained by the model of Collobert et al. (2011). These small word embeddings largely reduces model parameters. The dimension l of the hidden documentindependent features is experimented in the range of [1, 40], and the window sizes are experimented between 1 and 5. Through parameter experiments on development set, we set l = 20 and m = 3 for PriorSum. To update the weights W�h and wr, we apply the diagonal variant of AdaGrad with minibatches (Duchi et al., 2011). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004), and take ROUGE-1 and ROUGE-2 as the main measures. 3.2 Comparison with Baseline Methods To </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Lon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Jade Goldstein</author>
<author>Dianne P Oleary</author>
</authors>
<title>Left-brain/right-brain multi-document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of DUC.</booktitle>
<contexts>
<context position="1441" citStr="Conroy et al., 2004" startWordPosition="183" endWordPosition="186">es the enhanced convolutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines. 1 Introduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September 22, causing 3 deaths with its wind</context>
</contexts>
<marker>Conroy, Schlesinger, Goldstein, Oleary, 2004</marker>
<rawString>John M Conroy, Judith D Schlesinger, Jade Goldstein, and Dianne P Oleary. 2004. Left-brain/right-brain multi-document summarization. In Proceedings of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="12096" citStr="Duchi et al., 2011" startWordPosition="1902" endWordPosition="1905">4 datasets with two years of data as training set and one year of data as test set. We directly use the look-up table of 25- dimensional word embeddings trained by the model of Collobert et al. (2011). These small word embeddings largely reduces model parameters. The dimension l of the hidden documentindependent features is experimented in the range of [1, 40], and the window sizes are experimented between 1 and 5. Through parameter experiments on development set, we set l = 20 and m = 3 for PriorSum. To update the weights W�h and wr, we apply the diagonal variant of AdaGrad with minibatches (Duchi et al., 2011). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004), and take ROUGE-1 and ROUGE-2 as the main measures. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="10708" citStr="Erkan and Radev (2004)" startWordPosition="1660" endWordPosition="1663">since it is convenient to compare with regression baselines (see Section 3.2). Feature Description POSITION The position of the sentence. AVG-TF The averaged term frequency values of words in the sentence. AVG-CF The averaged cluster frequency values of words in the sentence. Table 1: Extracted document-dependent features. 2.2 Sentence Selection A summary is obliged to offer both informative and non-redundant content. Here, we employ a simple greedy algorithm to select sentences, similar to the MMR strategy (Carbonell and Goldstein, 1998). Firstly, we remove sentences less than 8 words (as in Erkan and Radev (2004)) and sort the rest in descending order according to the estimated saliency scores. Then, we iteratively dequeue one sentence, and append it to the current summary if it is non-redundant. A sentence is considered nonredundant if it contains more new words compared to the current summary content. We empirically set the cut-off of new word ratio to 0.5. 3 Experiments 3.1 Experiment Setup In our work, we focus on the generic multidocument summarization task and carry out experiments on DUC 2001 2004 datasets. All the documents are from newswires and grouped into various thematic clusters. The sum</context>
<context position="13258" citStr="Erkan and Radev, 2004" startWordPosition="2087" endWordPosition="2090">o et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn 2REGSUM truncates a summary to 100 words. 831 feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regression approach based on some advanced features such as word polarities (Wiebe et al., 2005) and categories (Tausczik and Pennebaker, 2010). For these three systems, we directly cite their published results, marked with the sign “*” as in Table 2. Meanwhile, LexRank (Erkan and Radev, 2004), a commonly-used graph-based summarization model, is introduced as an extra baseline. Comparing with this baseline can demonstrate the performance level of regression approaches. The baseline StandardCNN means that we adopt the standard CNNS with fixed window size for summary prior representation. To explore the effects of the learned summary prior representations, we design a baseline system named Reg Manual which adopts manuallycompiled document-independent features such as NUMBER (whether number exist), NENTITY (whether named entities exist) and STOPRATIO (the ratio of stopwords). Then we </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
</authors>
<title>A skip-chain conditional random field for ranking meeting utterances by importance.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>364--372</pages>
<contexts>
<context position="1420" citStr="Galley, 2006" startWordPosition="181" endWordPosition="182">m, which applies the enhanced convolutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines. 1 Introduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September 22, causing 3</context>
</contexts>
<marker>Galley, 2006</marker>
<rawString>Michel Galley. 2006. A skip-chain conditional random field for ranking meeting utterances by importance. In Proceedings of EMNLP, pages 364–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Hong</author>
<author>Ani Nenkova</author>
</authors>
<title>Improving the estimation of word importance for news multidocument summarization.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="2852" citStr="Hong and Nenkova (2014)" startWordPosition="411" endWordPosition="414"> Dominica on Tuesday. The first sentence describes the major information of a hurricane. With similar meaning, the second sentence uses an emphatic structure and is somewhat verbose. Obviously the first one should be preferred for a news summary. In this paper, we call such fact as summary prior nature1 and learn document-independent features to reflect it. In previous summarization systems, though not well-studied, some widely-used sentence ranking features such as the length and the ratio of stopwords, can be seen as attempts to measure the summary prior nature to a certain extent. Notably, Hong and Nenkova (2014) built a state-of-the-art summarization system through making use of advanced document-independent features. However, these document-independent features are usually hand-crafted, difficult to exhaust each aspect of the summary prior nature. Meanwhile, items representing the same feature may contribute differently to a summary. For example, “September 22” and “Tuesday” are both indicators of time, but the latter seldom occurs in a summary due to uncertainty. In addition, to the best of our knowledge, document-independent features beyond word level (e.g., phrases) are seldom involved in current</context>
<context position="12722" citStr="Hong and Nenkova, 2014" startWordPosition="2006" endWordPosition="2009">or evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004), and take ROUGE-1 and ROUGE-2 as the main measures. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn 2REGSUM truncates a summary to 100 words. 831 feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regression approach based on some advanced features such as word polarities (Wiebe et al., 2005) and categories (Tausczik and Pennebaker, 2010). For these three systems, we directly cite their published results, marked with the sign “*” as in Table 2. Meanwhile, LexRank (Erkan and Radev, 2004), a commonly-used graph-based summarization model, is introduced</context>
</contexts>
<marker>Hong, Nenkova, 2014</marker>
<rawString>Kai Hong and Ani Nenkova. 2014. Improving the estimation of word importance for news multidocument summarization. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</title>
<date>2014</date>
<contexts>
<context position="9211" citStr="Kim (2014)" startWordPosition="1419" endWordPosition="1420">erations to get the final summary prior representation. Specifically, let W1t , · · · , Wm t be m filters for window 830 sizes from 1 to m, and correspondingly we can obtain m feature maps C1, · · · , C&apos;. For each feature map CZ, We first adopt a max-over-time pooling operation max{CZ} with the goal of capturing the most salient features from each window size i. Next, a second max-over-time pooling operation is operated on all the windows to acquire the most representative features. To formulate, the document independent features xp can be generated by: xp = max{max{C1}, · · · , max{C&apos;}}. (2) Kim (2014) also uses filters with varying window sizes for sentence-level classification tasks. However, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe, shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujian Li</author>
<author>You Ouyang</author>
<author>Wei Wang</author>
<author>Bin Sun</author>
</authors>
<title>Multi-document summarization using support vector regression.</title>
<date>2007</date>
<booktitle>In Proceedings of DUC.</booktitle>
<contexts>
<context position="1459" citStr="Li et al., 2007" startWordPosition="187" endWordPosition="190">lutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines. 1 Introduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September 22, causing 3 deaths with its wind gusts up to 110 m</context>
<context position="6126" citStr="Li et al., 2007" startWordPosition="897" endWordPosition="900">rior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental results demonstrate that our model outperforms stateof-the-art extractive summarization approaches. Meanwhile, we analyze the different aspects supporting the summary prior in Section 3.3. 2 Methodology Our summarization system PriorSum follows the traditional extractive framework (Carbonell and Goldstein, 1998; Li et al., 2007). Specifically, the sentence ranking process scores and ranks the sentences from documents, and then the sentence selection process chooses the top ranked sentences to generate the final summary in accordance with the length constraint and redundancy among the selected sentences. Sentence ranking aims to measure the saliency score of a sentence with consideration of both document-dependent and document-independent features. In this study, we apply an enhanced version of convolutional neural networks to automatically generate document-independent features according to the summary prior nature. </context>
<context position="9852" citStr="Li et al. (2007)" startWordPosition="1521" endWordPosition="1524">arying window sizes for sentence-level classification tasks. However, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe, shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp, xe] (3) yˆ = wr × φ (4) where wr E Rl+|xe |is the regression weights. We use linear transformation since it is convenient to compare with regression baselines (see Section 3.2). Feature Description POSITION The position of the sentence. AVG-TF The averaged term frequency values of words in the sentence. AVG-CF The averaged cluster frequency values of words in the sentence. Table 1: Extracted document-dependent features. 2.2 Sentence Selection A summary is oblig</context>
</contexts>
<marker>Li, Ouyang, Wang, Sun, 2007</marker>
<rawString>Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007. Multi-document summarization using support vector regression. In Proceedings of DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="9910" citStr="Lin, 2004" startWordPosition="1533" endWordPosition="1534">ever, he reserves all the representations generated by filters to a fully connected output layer. This practice greatly enlarges following parameters and ignores the relation among phrases with different lengths. Hence we use the two-stage max-over-time pooling to associate all these filters. Besides the features xp obtained through the CNNs, we also extract several documentdependent features notated as xe, shown in Table 1. In the end, xp is combined with xe to conduct sentence ranking. Here we follow the regression framework of Li et al. (2007). The sentence saliency y is scored by ROUGE-2 (Lin, 2004) (stopwords removed) and the model tries to estimate this saliency. φ = [xp, xe] (3) yˆ = wr × φ (4) where wr E Rl+|xe |is the regression weights. We use linear transformation since it is convenient to compare with regression baselines (see Section 3.2). Feature Description POSITION The position of the sentence. AVG-TF The averaged term frequency values of words in the sentence. AVG-CF The averaged cluster frequency values of words in the sentence. Table 1: Extracted document-dependent features. 2.2 Sentence Selection A summary is obliged to offer both informative and non-redundant content. He</context>
<context position="12184" citStr="Lin, 2004" startWordPosition="1917" endWordPosition="1918">se the look-up table of 25- dimensional word embeddings trained by the model of Collobert et al. (2011). These small word embeddings largely reduces model parameters. The dimension l of the hidden documentindependent features is experimented in the range of [1, 40], and the window sizes are experimented between 1 and 5. Through parameter experiments on development set, we set l = 20 and m = 3 for PriorSum. To update the weights W�h and wr, we apply the diagonal variant of AdaGrad with minibatches (Duchi et al., 2011). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004), and take ROUGE-1 and ROUGE-2 as the main measures. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of the ACL Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Using maximum entropy for sentence extraction.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Automatic Summarization,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1406" citStr="Osborne, 2002" startWordPosition="179" endWordPosition="180"> called PriorSum, which applies the enhanced convolutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines. 1 Introduction Sentence ranking, the vital part of extractive summarization, has been extensively investigated. Regardless of ranking models (Osborne, 2002; Galley, 2006; Conroy et al., 2004; Li et al., 2007), feature engineering largely determines the final summarization performance. Features often fall into two types: document-dependent features (e.g., term frequency or position) and documentindependent features (e.g., stopword ratio or word polarity). The latter type of features take effects due to the fact that, a sentence can often be judged by itself whether it is appropriate to be included in a summary no matter which document it lies in. Take the following two sentences as an example: 1. Hurricane Emily slammed into Dominica on September</context>
</contexts>
<marker>Osborne, 2002</marker>
<rawString>Miles Osborne. 2002. Using maximum entropy for sentence extraction. In Proceedings of ACL Workshop on Automatic Summarization, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelong Shen</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Gr´egoire Mesnil</author>
</authors>
<title>Learning semantic representations using convolutional neural networks for web search.</title>
<date>2014</date>
<booktitle>In Companion publication of the 23rd international conference on World wide web companion,</booktitle>
<pages>373--374</pages>
<contexts>
<context position="5180" citStr="Shen et al., 2014" startWordPosition="768" endWordPosition="771">ases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. T</context>
</contexts>
<marker>Shen, He, Gao, Deng, Mesnil, 2014</marker>
<rawString>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr´egoire Mesnil. 2014. Learning semantic representations using convolutional neural networks for web search. In Companion publication of the 23rd international conference on World wide web companion, pages 373–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yla R Tausczik</author>
<author>James W Pennebaker</author>
</authors>
<title>The psychological meaning of words: Liwc and computerized text analysis methods.</title>
<date>2010</date>
<journal>Journal of language and social psychology,</journal>
<pages>29--1</pages>
<contexts>
<context position="13107" citStr="Tausczik and Pennebaker, 2010" startWordPosition="2060" endWordPosition="2063">e-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn 2REGSUM truncates a summary to 100 words. 831 feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regression approach based on some advanced features such as word polarities (Wiebe et al., 2005) and categories (Tausczik and Pennebaker, 2010). For these three systems, we directly cite their published results, marked with the sign “*” as in Table 2. Meanwhile, LexRank (Erkan and Radev, 2004), a commonly-used graph-based summarization model, is introduced as an extra baseline. Comparing with this baseline can demonstrate the performance level of regression approaches. The baseline StandardCNN means that we adopt the standard CNNS with fixed window size for summary prior representation. To explore the effects of the learned summary prior representations, we design a baseline system named Reg Manual which adopts manuallycompiled docum</context>
</contexts>
<marker>Tausczik, Pennebaker, 2010</marker>
<rawString>Yla R Tausczik and James W Pennebaker. 2010. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of language and social psychology, 29(1):24–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianwu Yang</author>
</authors>
<title>Multi-document summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="12685" citStr="Wan and Yang, 2008" startWordPosition="1999" endWordPosition="2002">nibatches (Duchi et al., 2011). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2004), and take ROUGE-1 and ROUGE-2 as the main measures. 3.2 Comparison with Baseline Methods To evaluate the summarization performance of PriorSum, we compare it with the best peer systems (PeerT, Peer26 and Peer65 in Table 2) participating DUC evaluations. We also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn 2REGSUM truncates a summary to 100 words. 831 feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regression approach based on some advanced features such as word polarities (Wiebe et al., 2005) and categories (Tausczik and Pennebaker, 2010). For these three systems, we directly cite their published results, marked with the sign “*” as in Table 2. Meanwhile, LexRank (Erkan and Radev, 2004), a commonly-used graph-bas</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>Xiaojun Wan and Jianwu Yang. 2008. Multi-document summarization using cluster-based link analysis. In Proceedings of SIGIR, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianmin Zhang</author>
</authors>
<title>Ctsum: extracting more certain summaries for news articles.</title>
<date>2014</date>
<booktitle>In Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval,</booktitle>
<pages>787--796</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3513" citStr="Wan and Zhang (2014)" startWordPosition="509" endWordPosition="512">ystem through making use of advanced document-independent features. However, these document-independent features are usually hand-crafted, difficult to exhaust each aspect of the summary prior nature. Meanwhile, items representing the same feature may contribute differently to a summary. For example, “September 22” and “Tuesday” are both indicators of time, but the latter seldom occurs in a summary due to uncertainty. In addition, to the best of our knowledge, document-independent features beyond word level (e.g., phrases) are seldom involved in current research. The CTSUM system developed by Wan and Zhang (2014) is the most relevant to ours. It attempted to explore a context-free measure named certainty which is critical to ranking sentences in summarization. To calculate the certainty score, four dictionaries are manually built as features and a corpus is annotated to train the feature weights using Support Vector Regression (SVR). How1In this paper, “summary prior features” and “documentindependent features” hold the same meaning. 829 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Sho</context>
</contexts>
<marker>Wan, Zhang, 2014</marker>
<rawString>Xiaojun Wan and Jianmin Zhang. 2014. Ctsum: extracting more certain summaries for news articles. In Proceedings of the 37th international ACM SIGIR conference on Research &amp; development in information retrieval, pages 787–796. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language resources and evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="13060" citStr="Wiebe et al., 2005" startWordPosition="2054" endWordPosition="2057"> also choose as baselines those state-of-the-art summarization results on DUC (2001, 2002, and 2004) data. To our knowledge, the best reported results on DUC 2001, 2002 and 2004 are from R2N2 (Cao et al., 2015), ClusterCMRW (Wan and Yang, 2008) and REGSUM2 (Hong and Nenkova, 2014) respectively. R2N2 applies recursive neural networks to learn 2REGSUM truncates a summary to 100 words. 831 feature combination. ClusterCMRW incorporates the cluster-level information into the graph-based ranking algorithm. REGSUM is a word regression approach based on some advanced features such as word polarities (Wiebe et al., 2005) and categories (Tausczik and Pennebaker, 2010). For these three systems, we directly cite their published results, marked with the sign “*” as in Table 2. Meanwhile, LexRank (Erkan and Radev, 2004), a commonly-used graph-based summarization model, is introduced as an extra baseline. Comparing with this baseline can demonstrate the performance level of regression approaches. The baseline StandardCNN means that we adopt the standard CNNS with fixed window size for summary prior representation. To explore the effects of the learned summary prior representations, we design a baseline system named</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language resources and evaluation, 39(2-3):165–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Xiaodong He</author>
<author>Christopher Meek</author>
</authors>
<title>Semantic parsing for single-relation question answering.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5161" citStr="Yih et al., 2014" startWordPosition="764" endWordPosition="767">The underlined phrases greatly reduce the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summar</context>
</contexts>
<marker>Yih, He, Meek, 2014</marker>
<rawString>Wen-tau Yih, Xiaodong He, and Christopher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daojian Zeng</author>
<author>Kang Liu</author>
<author>Siwei Lai</author>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
</authors>
<title>Relation classification via convolutional deep neural network.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>2335--2344</pages>
<contexts>
<context position="5200" citStr="Zeng et al., 2014" startWordPosition="772" endWordPosition="775"> the certainty of this sentence according to Wan and Zhang (2014)’s model. But, in fact, this sentence can summarize the government’s attitude and is salient enough in the related documents. Thus, in our opinion, certainty can just be viewed as a specific aspect of the summary prior nature. To this end, we develop a novel summarization system called PriorSum to automatically exploit all possible semantic aspects latent in the summary prior nature. Since the Convolutional Neural Networks (CNNs) have shown promising progress in latent feature representation (Yih et al., 2014; Shen et al., 2014; Zeng et al., 2014), PriorSum applies CNNs with multiple filters to capture a comprehensive set of document-independent features derived from length-variable phrases. Then we adopt a two-stage max-over-time pooling operation to associate these filters since phrases with different lengths may express the same aspect of summary prior. PriorSum generates the document-independent features, and concatenates them with document-dependent ones to work for sentence regression (Section 2.1). We conduct extensive experiments on the DUC 2001, 2002 and 2004 generic multi-document summarization datasets. The experimental resu</context>
</contexts>
<marker>Zeng, Liu, Lai, Zhou, Zhao, 2014</marker>
<rawString>Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING, pages 2335–2344.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>