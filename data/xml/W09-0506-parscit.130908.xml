<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000074">
<title confidence="0.987868">
Predicting Concept Types in User Corrections in Dialog
</title>
<author confidence="0.986054">
Svetlana Stoyanchev and Amanda Stent
</author>
<affiliation confidence="0.963658">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.832831">
Stony Brook, NY 11794-4400, USA
</address>
<email confidence="0.998545">
svetlana.stoyanchev@gmail.com, amanda.stent@stonybrook.edu
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998865">
Most dialog systems explicitly confirm
user-provided task-relevant concepts.
User responses to these system confirma-
tions (e.g. corrections, topic changes) may
be misrecognized because they contain
unrequested task-related concepts. In this
paper, we propose a concept-specific lan-
guage model adaptation strategy where
the language model (LM) is adapted to
the concept type(s) actually present in
the user’s post-confirmation utterance.
We evaluate concept type classification
and LM adaptation for post-confirmation
utterances in the Let’s Go! dialog system.
We achieve 93% accuracy on concept type
classification using acoustic, lexical and
dialog history features. We also show that
the use of concept type classification for
LM adaptation can lead to improvements
in speech recognition performance.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952692307692">
In most dialog systems, the system explicitly con-
firms user-provided task-relevant concepts. The
user’s response to a confirmation prompt such as
“leaving from Waterfront?” may consist of a sim-
ple confirmation (e.g. “yes”), a simple rejection
(e.g. “no”), a correction (e.g. “no, Oakland”) or a
topic change (e.g. “no, leave at 7” or “yes, and go
to Oakland”). Each type of utterance has implica-
tions for further processing. In particular, correc-
tions and topic changes are likely to contain un-
requested task-relevant concepts that are not well
represented in the recognizer’s post-confirmation
language model (LM)1. This means that they are
</bodyText>
<footnote confidence="0.895385">
1The word error rate on post-confirmation Let’s Go! utter-
ances containing a concept is 10% higher than on utterances
</footnote>
<bodyText confidence="0.981496194444444">
likely to be misrecognized, frustrating the user and
leading to cascading errors. Correct determina-
tion of the content of post-confirmation utterances
can lead to improved speech recognition, fewer
and shorter sequences of speech recognition er-
rors, and improved dialog system performance.
In this paper, we look at user responses to sys-
tem confirmation prompts CMU’s deployed Let’s
Go! dialog system. We adopt a two-pass recogni-
tion architecture (Young, 1994). In the first pass,
the input utterance is processed using a general-
purpose LM (e.g. specific to the domain, or spe-
cific to the dialog state). Recognition may fail
on concept words such as “Oakland” or “61C” ,
but is likely to succeed on closed-class words (e.g.
”yes”, ”no”, ”and”, ”but”, ”leaving”). If the ut-
terance follows a system confirmation prompt, we
then use acoustic, lexical and dialog history fea-
tures to determine the task-related concept type(s)
likely to be present in the utterance. In the second
recognition pass, any utterance containing a con-
cept type is re-processed using a concept-specific
LM. We show that: (1) it is possible to achieve
high accuracy in determining presence or absence
of particular concept types in a post-confirmation
utterance; and (2) 2-pass speech recognition with
concept type classification and language model
adaptation can lead to improved speech recogni-
tion performance for post-confirmation utterances.
The rest of this paper is structured as follows: In
Section 2 we discuss related work. In Section 3 we
describe our data. In Section 4 we present our con-
cept type classification experiment. In Section 5
we present our LM adaptation experiment. In Sec-
tion 6 we conclude and discuss future work.
without a concept.
</bodyText>
<note confidence="0.911923">
Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 42–49,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998537">
42
</page>
<sectionHeader confidence="0.99943" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930945945946">
When a dialog system requests a confirmation,
the user’s subsequent corrections and topic change
utterances are particularly likely to be misrecog-
nized. Considerable research has now been done
on the automatic detection of spoken corrections.
Linguistic cues to corrections include the num-
ber of words in the post-confirmation utterance
and the use of marked word order (Krahmer et
al., 2001). Prosodic cues include F0 max, RMS
max, RMS mean, duration, speech tempo, and
percentage of silent frames(Litman et al., 2006;
Hirschberg et al., 2004; Levow, 1998). Discourse
cues include the removal, repetition, addition or
modification of a concept, the system’s dialog act
type, and information about error rates in the dia-
log so far (Krahmer et al., 2001; et al., 2002; Lit-
man et al., 2006; Walker et al., 2000). In our ex-
periments, we use most of these features as well as
additional lexical features.
We can use knowledge of the type or content
of a user utterance to modify system behavior.
For example, in this paper we use the concept
type(s) in the user’s utterance to adapt the recog-
nizer’s LM. It is now common practice to adapt
the recognizer to the type, context or style of in-
put speech (Bellegarda, 2004). LM adaptation
has been used to improve automatic speech recog-
nition performance in automated meeting tran-
scription (Tur and Stolcke, 2007), speech-driven
question answering (Stenchikova et al., 2008),
broadcast news recognition (Gildea and Hofmann,
1999), and spoken dialog systems (Tur, 2005).
LMs in dialog systems can be adapted to the di-
alog state (e.g. (Riccardi and Gorin, 2000; Esteve
et al., 2001)), the topic (Iyer and Ostendorf, 1999;
Gildea and Hofmann, 1999), or the speaker (Tur,
2007).
</bodyText>
<sectionHeader confidence="0.996381" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999622818181818">
In this experiment we use annotated dialog tran-
scripts and speech from the Let’s Go! sys-
tem, a telephone-based spoken dialog system that
provides information about bus routes in Pitts-
burgh (Raux et al., 2005). The data we used comes
from the first two months of Let’s Go! system
operation in 2005 (2411 dialogs), and one month
in 2006 (1430 dialogs). This data has been tran-
scribed, and annotated by hand for concept types.
In order to provide the user with route in-
formation, Let’s Go! must elicit a departure
</bodyText>
<table confidence="0.926267857142857">
1 Sys Welcome to the CMU Let’s Go bus informa-
tion system. What can I do for you?
2 User I need to go from Oakland:p
ASR I need to go .from. can’t
3 Sys Where do you wanna leave from?
4 User to Waterfront:p
ASR told. .me. Waterfront
</table>
<figure confidence="0.755840333333333">
5 Sys Leaving from WATERFRONT . Did I get
that right?
6 User OAKLAND:p
ASR OAKLAND
7 Sys Leaving from OAKLAND . Did I get that
right?
</figure>
<figureCaption confidence="0.867278">
Figure 1: Dialog extract from Let’s Go! data. User
utterances are annotated with concept types (e.g.
:p for place)
</figureCaption>
<bodyText confidence="0.999939551724138">
location, a destination, a departure time, and
optionally a bus route number. Each concept
value provided by the user is explicitly con-
firmed by the system (see Figure 1). In the
annotated transcripts, the following concepts are
labeled: neighborhood, place, time,
hour, minute, time-of-day, and bus.
For our experiments we collapsed these concepts
into three concept types: time, place and bus.
Let’s Go! has five dialog states corresponding
to the type of user utterance it expects: first-query,
next-query, yes-no, place and time. Its speech
recognizer uses dialog state-specific n-gram LMs
trained on user utterances from the 2005 data.
We focus on user utterances in response to sys-
tem confirmation prompts (the yes-no state). Ta-
ble 1 shows statistics about yes-no state utterances
in Let’s Go!. Table 2 shows a confusion matrix
for confirmation prompt concept type and post-
confirmation utterance concept type. This table
indicates the potential for misrecognition of post-
confirmation utterances. For example, in the 2006
dataset after a system confirmation prompt for a
bus, a bus concept is used in only 64% of concept-
containing user utterances.
In our experiments, we used the 2006 data to
train concept type classifiers and for testing. We
used the 2005 data to build LMs for our speech
recognition experiment.
</bodyText>
<sectionHeader confidence="0.991904" genericHeader="method">
4 Concept Classification
</sectionHeader>
<subsectionHeader confidence="0.994439">
4.1 Method
</subsectionHeader>
<bodyText confidence="0.99647">
Our goal is to classify each post-confirmation user
utterance by the concept type(s) it contains (place,
time, bus or none) for later language-model adap-
tation (see Section 5). From the post-confirmation
user utterances in the 2006 dataset described in
</bodyText>
<page confidence="0.999767">
43
</page>
<table confidence="0.999598931034483">
Event 2005 2006
num % num %
Total dialogs 2411 1430
Total yes-no confirms 9098 100 9028 100
Yes-no confirms with 2194 24 1635 18.1
a concept
Dialog State
Total confirm place 5548 61 5347 59.2
utts
Total confirm bus utts 1763 19.4 1589 17.6
Total confirm time 1787 19.6 2011 22.3
utts
Concept Type Features
Yes-no utts with place 1416 15.6 1007 11.2
Yes-no utts with time 296 3.2 305 3.4
Yes-no utts with bus 584 6.4 323 3.6
Lexical Features
Yes-no utts with ‘yes’ 4395 48.3 3693 40.9
Yes-no utts with ‘no’ 2076 22.8 1564 17.3
Yes-no utts with ‘I’ 203 2.2 129 1.4
Yes-no utts with 114 1.3 185 2.1
‘from’
Yes-no utts with ‘to’ 204 2.2 237 2.6
Acoustic Features
feature mean stdev mean stdev
Duration (seconds) 1.341 1.097 1.365 1.242
RMS mean .037 .033 .055 .049
F0 mean 183.0 60.86 185.7 58.63
F0 max 289.8 148.5 296.9 146.5
</table>
<tableCaption confidence="0.99586">
Table 1: Statistics on post-confirmation utterances
</tableCaption>
<table confidence="0.999239222222222">
place bus time
2005 dataset
confirm place 0.86 0.13 0.01
confirm bus 0.18 0.81 0.01
confirm time 0.07 0.01 0.92
2006 dataset
confirm place 0.87 0.10 0.03
confirm bus 0.34 0.64 0.02
confirm time 0.15 0.13 0.71
</table>
<tableCaption confidence="0.999875">
Table 2: Confirmation state vs. user concept type
</tableCaption>
<bodyText confidence="0.98434611627907">
Section 3, we extracted the features described in
Section 4.2 below. To identify the correct concept
type(s) for each utterance, we used the human an-
notations provided with the data.
We performed a series of 10-fold cross-
validation experiments to examine the impact of
different types of feature on concept type classifi-
cation. We trained three binary classifiers for each
experiment, one for each concept type, i.e. we sep-
arately classified each post-confirmation utterance
as place + or place -, time + or time -, and bus + or
bus -. We used Weka’s implementation of the J48
decision tree classifier (Witten and Frank, 2005)2.
For each experiment, we report precision (pre+)
and recall (rec+) for determining presence of each
concept type, and overall classification accuracy
2J48 gave the highest classification accuracy compared to
other machine learning algorithms we tried on this data.
for each concept type (place, bus and time)3. We
also report overall pre+, rec+, f-measure (f+), and
classification accuracy across the three concept
types. Finally, we report the percentage of switch+
errors and switch errors. Switch+ errors are utter-
ances containing bus classified as time/place, time
as bus/place, and place as bus/time; these are the
errors most likely to cause decreases in speech
recognition accuracy after language model adap-
tation. Switch errors include utterances with no
concept classified as place, bus or time.
Only utterances classified as containing one of
the three concept types are subject to second-
pass recognition using a concept-specific language
model. Therefore, these are the only utterances on
which speech recognition performance may im-
prove. This means that we want to maximize rec+
(proportion of utterances containing a concept that
are classified correctly). On the other hand, utter-
ances that are incorrectly classified as containing a
particular concept type will be subject to second-
pass recognition using a poorly-chosen language
model. This may cause speech recognition per-
formance to suffer. This means that we want to
minimize switch+ errors.
</bodyText>
<subsectionHeader confidence="0.861813">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.9999858">
We used the features summarized in Table 3. All
of these features are available at run-time and so
may be used in a live system. Below we give ad-
ditional information about the RAW and LEX fea-
tures; the other feature sets are self-explanatory.
</bodyText>
<subsubsectionHeader confidence="0.656723">
4.2.1 Acoustic and Dialog History Features
</subsubsectionHeader>
<bodyText confidence="0.999897833333333">
The acoustic/prosodic and dialog history features
are adapted from those identified in previous work
on detecting speech recognition errors (particu-
larly (Litman et al., 2006)). We anticipated that
these features would help us distinguish correc-
tions and rejections from confirmations.
</bodyText>
<subsectionHeader confidence="0.511266">
4.2.2 Lexical Features
</subsectionHeader>
<bodyText confidence="0.849463727272727">
We used lexical features from the user’s current ut-
terance. Words in the output of first-pass ASR are
highly indicative both of concept presence or ab-
sence, and of the presence of particular concept
types; for example, going to suggests the pres-
ence of a place. We selected the most salient lexi-
3We do not report precision or recall for determining ab-
sence of each concept type. In our data set 82.2% of the ut-
terances do not contain any concepts (see Table 1). Conse-
quently, precision and recall for determining absence of each
concept type are above .9 in each of the experiments.
</bodyText>
<page confidence="0.986786">
44
</page>
<table confidence="0.9999365">
Feature type Feature source Features
System confirmation type system log System’s confirmation prompt concept type (confirm time,
(DIA) confirm place, or confirm bus)
Acoustic (RAV) raw speech F0 max; RMS max; RMS mean; Duration; Difference be-
tween F0 max in first half and in second half
Lexical (LEX) transcripts/ASR output Presence of specific lexical items; Number of tokens in utter-
ance; [transcribed speech only] String edit distance between
current and previous user utterances
Dialog history (DH1, DH3) 1-3 previous utterances System’s dialog states of previous utterances(place, bus,
time, confirm time, confirm place, or confirm bus); [tran-
scribed speech only] Concept(s) that occurred in user’s ut-
terances (YES/NO for each of the concepts place, bus, time)
ASR confidence score (ASR) ASR output Speech recognizer confidence score
Concept type match (CTM) transcripts/ASR output Presence of concept-specific lexical items
</table>
<tableCaption confidence="0.963181">
Table 3: Features for concept type classifiers
</tableCaption>
<bodyText confidence="0.999945103448276">
cal features (unigrams and bigrams) for each con-
cept type by computing the mutual information be-
tween potential features and concept types (Man-
ning et al., 2008). For each lexical feature t and
each concept type class c ∈ { place +, place -,
time +, time -, bus +, bus -}, we computed I:
where Ntc= number of utterances where t co-
occurs with c, N0c= number of utterances with c
but without t, Nto= number of utterances where t
occurs without c, Noo= number of utterances with
neither t nor c, Nt.= total number of utterances
containing t, N.c= total number of utterances con-
taining c, and N = total number of utterances.
To identify the most relevant lexical features,
we extracted from the data all the transcribed user
utterances. We removed all words that realize con-
cepts (e.g. “61C”, “Squirrel Hill”), as these are
likely to be misrecognized in a post-confirmation
utterance. We then extracted all word unigrams
and bigrams. We computed the mutual informa-
tion between each potential lexical feature and
concept type. We then selected the 30 features
with the highest mutual information which oc-
curred at least 20 times in the training data4.
For transcribed speech only, we also compute
the string edit distance between the current and
previous user utterances. This gives some indica-
tion of whether the current utterance is a correc-
tion or topic change (vs. a confirmation). How-
</bodyText>
<footnote confidence="0.89786">
4We aimed to select equal number of features for each
class with information measure in the top 25%. 30 was an
empirically derived threshold for the number of lexical fea-
tures to satisfy the desired condition.
</footnote>
<bodyText confidence="0.995965">
ever, for recognized speech recognition errors re-
duce the effectiveness of this feature (and of the
concept features in the dialog history feature set).
</bodyText>
<subsectionHeader confidence="0.966662">
4.3 Baseline
</subsectionHeader>
<bodyText confidence="0.999996454545455">
A simple baseline for this task, No-Concept, al-
ways predicts none in post-confirmation utter-
ances. This baseline achieves overall classifica-
tion accuracy of 82% but rec+ of 0. At the other
extreme, the Confirmation State baseline assigns
to each utterance the dialog system’s confirmation
prompt type (using the DIA feature). This base-
line achieves rec+ of .79, but overall classification
accuracy of only 14%. In all of the models used in
our experiments, we include the current confirma-
tion prompt type (DIA) feature.
</bodyText>
<subsectionHeader confidence="0.995002">
4.4 Experiment Results
</subsectionHeader>
<bodyText confidence="0.999936">
In this section we report the results of experiments
on concept type classification in which we exam-
ine the impact of the feature sets presented in Ta-
ble 3. We report performance separately for recog-
nized speech, which is available at runtime (Table
5); and for transcribed speech, which gives us an
idea of best possible performance (Table 4).
</bodyText>
<subsubsectionHeader confidence="0.885601">
4.4.1 Features from the Current Utterance
</subsubsectionHeader>
<bodyText confidence="0.999908166666667">
We first look at lexical (LEX) and prosodic (RAW)
features from the current utterance. For both rec-
ognized and transcribed speech, the LEX model
achieves significantly higher rec+ and overall ac-
curacy than the RAW model (p &lt; .001). For
recognized speech, however, the LEX model has
significantly more switch+ errors than the RAW
model (p &lt; .001). This is not surprising since the
majority of errors made by the RAW model are
labeling an utterance with a concept as none. Ut-
terances misclassified in this way are not subject to
second-pass recognition and do not increase WER.
</bodyText>
<equation confidence="0.75275375">
Ntc
I=
N
log2
N* Nt� No. N * N0c * + * log2 +
Nt. * N.c N N0. * N.c
Nt0 * lo 2 N * Nt0 + N00 * lo 2 N * N00
N g Nt. * N.0 N g N0. * N.0
</equation>
<page confidence="0.996461">
45
</page>
<table confidence="0.999799444444444">
Features Place Time Bus Overall
pre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switch
No Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0
Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3
RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03
LEX 0.81 0.88 0.96 0.77 0.48 0.98 0.83 0.59 0.98 0.87 0.72 0.79 0.93 7.32 3.22
LEX RAW 0.83 0.84 0.96 0.75 0.54 0.98 0.76 0.59 0.98 0.88 0.70 0.78 0.93 7.39 3.00
DH1 LEX 0.85 0.91 0.97 0.72 0.63 0.98 0.89 0.83 0.99 0.88 0.81 0.84 0.95 5.48 2.85
DH3 LEX 0.85 0.87 0.97 0.72 0.59 0.98 0.92 0.82 0.99 0.89 0.78 0.83 0.94 5.22 2.62
</table>
<tableCaption confidence="0.9815315">
Table 4: Concept type classification results: transcribed speech (all models include feature DIA). Best
overall values in each group are highlighted in bold.
</tableCaption>
<table confidence="0.9999212">
Features Place Time Bus Overall
pre+ rec+ acc pre+ rec+ acc pre+ rec+ acc pre+ rec+ f+ acc switch+ switch
No Concept 0 0 .86 0 0 0.81 0 0 .92 0 0 0 0.82 0 0
Confirmation State 0.87 0.85 0.86 0.64 0.54 0.58 0.71 0.87 0.78 0.14 0.79 0.24 0.14 17 72.3
RAW 0.65 0.53 0.92 0.25 0.01 0.96 0.38 0.07 0.96 0.67 0.34 0.45 0.85 6.43 4.03
LEX 0.70 0.70 0.93 0.67 0.15 0.97 0.65 0.62 0.98 0.75 0.56 0.64 0.89 9.94 4.93
LEX RAW 0.70 0.72 0.93 0.66 0.38 0.97 0.68 0.57 0.98 0.76 0.60 0.67 0.90 10.32 5.10
DH1 LEX RAW 0.71 0.68 0.93 0.68 0.38 0.97 0.78 0.63 0.98 0.77 0.60 0.67 0.90 8.15 4.55
DH3 LEX RAW 0.71 0.70 0.93 0.67 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.57
ASR DH3 LEX 0.71 0.70 0.93 0.69 0.42 0.97 0.79 0.63 0.98 0.77 0.62 0.68 0.90 7.20 4.54
RAW
CTM DH3 LEX 0.82 0.82 0.96 0.86 0.71 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 3.89 2.94
RAW
CTM ASR DH3 0.82 0.81 0.96 0.86 0.69 0.99 0.76 0.68 0.98 0.85 0.74 0.79 0.93 4.27 3.01
LEX RAW
</table>
<tableCaption confidence="0.967885">
Table 5: Concept type classification results: recognized speech (all models include feature DIA). Best
overall values in each group are highlighted in bold.
</tableCaption>
<bodyText confidence="0.999900588235294">
For transcribed speech, the LEX RAW model
does not perform significantly differently from the
LEX model in terms of overall accuracy, rec+, or
switch+ errors. However, for recognized speech,
LEX RAW achieves significantly higher rec+ and
overall accuracy than LEX (p &lt; .001). Lexical
content from transcribed speech is a very good in-
dicator of concept type. However, lexical content
from recognized speech is noisy, so concept type
classification from ASR output can be improved
by using acoustic/prosodic features.
We note that models containing only features
from the current utterance perform significantly
worse than the confirmation state baseline in terms
of rec+ (p &lt; .001). However, they have signif-
icantly better overall accuracy and fewer switch+
errors (p &lt; .001) .
</bodyText>
<subsubsectionHeader confidence="0.86022">
4.4.2 Features from the Dialog History
</subsubsectionHeader>
<bodyText confidence="0.998597866666667">
Next, we add features from the dialog history
to our best-performing models so far. For tran-
scribed speech, DH1 LEX performs significantly
better than LEX in terms of overall accuracy, rec+,
and switch+ errors (p &lt; .001). DH3 LEX per-
forms significantly worse than DH1 LEX in terms
of rec+ (p &lt; 0.05). For recognized speech,
neither DH1 LEX RAW nor DH3 LEX RAW is
significantly different from LEX RAW in terms
of rec+ or overall accuracy. However, both
DH1 LEX RAW and DH3 LEX RAW do per-
form significantly better than LEX RAW in terms
of switch+ errors (p &lt; .05). There are
no significant performance differences between
DH1 LEX RAW and DH3 LEX RAW.
</bodyText>
<subsubsectionHeader confidence="0.747237">
4.4.3 Features Specific to Recognized Speech
</subsubsectionHeader>
<bodyText confidence="0.999933666666667">
Finally, we add the ASR and CTM features to
models trained on recognized speech.
We hypothesized that the classifier can use the
recognizer’s confidence score to decide whether
an utterance is likely to have been misrecognized.
However, ASR DH3 LEX RAW is not signifi-
cantly different from DH3 LEX RAW in terms of
rec+, overall accuracy or switch+ errors.
We hypothesized that the CTM feature will im-
prove cases where a part of (but not the whole)
concept instance is recognized in first-pass recog-
nition5. The generic language model used in first-
pass recognition recognizes some concept-related
words. So, if in the utterance Madison avenue,
avenue (but not Madison), is recognized in the
first-pass recognition, the CTM feature can flag
the utterance with a partial match for place, help-
ing the classifier to correctly assign the place
</bodyText>
<footnote confidence="0.81901625">
5We do not try the CTM feature on transcribed speech be-
cause there is a one-to-one correspondence between presence
of the concept and the CTM feature, so it perfectly indicates
presence of a concept.
</footnote>
<page confidence="0.999444">
46
</page>
<bodyText confidence="0.999989272727273">
type to the utterance. Then, in the second-pass
recognition the utterance will be decoded with
a place concept-specific language model, poten-
tially improving speech recognition performance.
Adding the CTM feature to DH3 LEX RAW and
ASR DH3 LEX RAW leads to a large statistically
significant improvement in all measures: a 12%
absolute increase in rec+, a 3% absolute increase
in overall accuracy, and decreases in switch+ er-
rors (p &lt; .001). There are no statistically signifi-
cant differences between these two models.
</bodyText>
<subsectionHeader confidence="0.736588">
4.4.4 Summary and Discussion
</subsectionHeader>
<bodyText confidence="0.999991428571429">
In this section we evaluated different models for
concept type classification. The best perform-
ing transcribed speech model, DH1 LEX, signif-
icantly outperforms the Confirmation State base-
line on overall accuracy and on switch+ and switch
errors (p &lt; .001), and is not significantly different
on rec+. The best performing recognized speech
model, CTM DH3 LEX RAW, significantly out-
performs the Confirmation State baseline on
overall accuracy and on switch+ and switch er-
rors, but is significantly worse on rec+ (p &lt; .001).
The best transcribed speech model achieves signif-
icantly higher rec+ and overall accuracy than the
best recognized speech model (p &lt; .01).
</bodyText>
<sectionHeader confidence="0.992407" genericHeader="method">
5 Speech Recognition Experiment
</sectionHeader>
<bodyText confidence="0.9998055">
In this section we report the impact of concept type
prediction on recognition of post-confirmation ut-
terances in Let’s Go! system data. We hypothe-
sized that speech recognition performance for ut-
terances containing a concept can be improved
with the use of concept-specific LMs. We (1) com-
pare the existing dialog state-specific LM adap-
tation approach used in Let’s Go! with our pro-
posed concept-specific adaptation; (2) compare
two approaches to concept-specific adaptation (us-
ing the system’s confirmation prompt type and us-
ing our concept type classifiers); and (3) evaluate
the impact of different concept type classifiers on
concept-specific LM adaptation.
</bodyText>
<subsectionHeader confidence="0.997522">
5.1 Method
</subsectionHeader>
<bodyText confidence="0.999923928571429">
We used the PocketSphinx speech recognition en-
gine (et al., 2006) with gender-specific telephone-
quality acoustic models built for Communica-
tor (et al., 2000). We trained trigram LMs us-
ing 0.5 ratio discounting with the CMU language
modeling toolkit (Xu and Rudnicky, 2000)6. We
built state- and concept-specific hierarchical LMs
from the Let’s Go! 2005 data. The LMs are built
with [place], [time] and [bus] submodels.
We evaluate speech recognition performance
on the post-confirmation user utterances from the
2006 testing dataset. Each experiment varies in 1)
the LM used for the final recognition pass and 2)
the method of selecting a LM for use in decoding.
</bodyText>
<subsectionHeader confidence="0.561491">
5.1.1 Language models
</subsectionHeader>
<bodyText confidence="0.999973363636364">
We built seven LMs for these experiments. The
state-specific LM contains all utterances in the
training data that were produced in the yes-no di-
alog state. The confirm-place, confirm-bus and
confirm-time LMs contain all utterances produced
in the yes-no dialog state following confirm place,
confirm bus and confirm time system confirma-
tion prompts respectively. Finally, the concept-
place, concept-bus and concept-time LMs contain
all utterances produced in the yes-no dialog state
that contain a mention of a place, bus or time.
</bodyText>
<subsectionHeader confidence="0.52662">
5.1.2 Decoders
</subsectionHeader>
<bodyText confidence="0.999796333333333">
In the baseline, 1-pass general condition, we
use the state-specific LM to recognize all post-
confirmation utterances. In the 1-pass state ex-
perimental condition we use the confirm-place,
confirm-bus and confirm-time LMs to recog-
nize testing utterances produced following a con-
firm place, confirm bus and confirm time prompt
respectively7. In the 1-pass concept experimen-
tal condition we use the concept-place, concept-
bus and concept-time LMs to recognize testing ut-
terances produced following a confirm place, con-
firm bus and confirm time prompt respectively.
In the 2-pass conditions we perform first-pass
recognition using the general LM. Then, we clas-
sify the output of the first pass using a concept
type classifier. Finally, we perform second-pass
recognition using the concept-place, concept-bus
or concept-time LMs if the utterance was classi-
fied as place, bus or time respectively8. We used
the three classification models with highest overall
rec+: DH3 LEX RAW, ASR DH3 LEX RAW,
</bodyText>
<footnote confidence="0.998856625">
6We chose the same speech recognizer, acoustic models,
language modeling toolkit, and LM building parameters that
are used in the live Let’s Go! system (Raux et al., 2005).
7As we showed in Table 2, most, but not all, utterances in
a confirmation state contain the corresponding concept.
8We treat utterances classified as containing more than
concept type as none. In the 2006 data, only 5.6% of ut-
terances with a concept contain more than one concept type.
</footnote>
<page confidence="0.996812">
47
</page>
<table confidence="0.999531866666667">
Recognizer Concept type Language Overall Concept utterances
classifier model
WER WER Concept recall
1-pass general state-specific 38.49% 49.12% 50.75%
1-pass confirm state confirm-{place,bus,time} 38.83% 48.96% 51.36%
1-pass confirm state concept-{place,bus,time}, 46.47% 4 50.73% 46 52.9% *
state-specific
2-pass DH3 LEX RAW concept-{place,bus,time}, 38.48% 47.56% 4 53.2% *
state-specific
2-pass ASR DH3 LEX concept-{place,bus,time}, 38.51% 47.99% 46 52.7%
RAW state-specific
2-pass CTM ASR DH3 concept-{place,bus,time}, 38.42% 47.86% 46 52.6%
LEX RAW state-specific
2-pass oracle concept-{place,bus,time}, 37.85% 4 45.94% 4 54.91% 4
state-specific
</table>
<tableCaption confidence="0.959353">
Table 6: Speech recognition results. ♠ indicates significant difference (p&lt;.01). ♣ indicates significant
</tableCaption>
<bodyText confidence="0.904152142857143">
difference (p&lt;.05). * indicates near-significant trend in difference (p&lt;.07). Significance for WER is
computed as a paired t-test. Significance for concept recall is an inference on proportion.
and CTM ASR DH3 LEX RAW. To get an idea
of “best possible” performance, we also report 2-
pass oracle recognition results, assuming an oracle
classifier that always outputs the correct concept
type for an utterance.
</bodyText>
<subsectionHeader confidence="0.583073">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999993119047619">
In Table 6 we report average per-utterance word
error rate (WER) on post-confirmation utterances,
average per-utterance WER on post-confirmation
utterances containing a concept, and average con-
cept recall rate (percentage of correctly recog-
nized concepts) on post-confirmation utterances
containing a concept. In slot-filling dialog sys-
tems like Let’s Go!, the concept recall rate largely
determines the potential of the system to under-
stand user-provided information and continue the
dialog successfully. Our goal is to maximize con-
cept recall and minimize concept utterance WER,
without causing overall WER to decline.
As Table 6 shows, the 1-pass state and 1-pass
concept recognizers perform better than the 1-
pass general recognizer in terms of concept recall,
but worse in terms of overall WER. Most of these
differences are not statistically significant. How-
ever, the 1-pass concept recognizer has signifi-
cantly worse overall and concept utterance WER
than the 1-pass general recognizer (p &lt; .01).
All of the 2-pass recognizers that use au-
tomatic concept prediction achieve significantly
lower concept utterance WER than the 1-pass
general recognizer (p &lt; .05). Differences be-
tween these recognizers in overall WER and con-
cept recall are not significant.
The 2-pass oracle recognizer achieves signif-
icantly higher concept recall and significantly
lower overall and concept utterance WER than
the 1-pass general recognizer (p &lt; .01). It
also achieves significantly lower concept utterance
WER than any of the 2-pass recognizers that use
automatic concept prediction (p &lt; .01).
Our 2-pass concept results show that it is possi-
ble to use knowledge of the concepts in a user’s ut-
terance to improve speech recognition. Our 1-pass
concept results show that this cannot be effec-
tively done by assuming that the user will always
address the system’s question; instead, one must
consider the user’s actual utterance and the dis-
course history (as in our DH3 LEX RAW model).
</bodyText>
<sectionHeader confidence="0.998738" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999912428571429">
In this paper, we examined user responses to sys-
tem confirmation prompts in task-oriented spoken
dialog. We showed that these post-confirmation
utterances may contain unrequested task-relevant
concepts that are likely to be misrecognized. Us-
ing acoustic, lexical, dialog state and dialog his-
tory features, we were able to classify task-
relevant concepts in the ASR output for post-
confirmation utterances with 90% accuracy. We
showed that use of a concept type classifier can
lead to improvements in speech recognition per-
formance in terms of WER and concept recall.
Of course, any possible improvements in speech
recognition performance are dependent on (1) the
performance of concept type classification; (2)
the accuracy of the first-pass speech recognition;
and (3) the accuracy of the second-pass speech
recognition. For example, with our general lan-
guage model, we get a fairly high overall WER
of 38.49%. In future work, we will systematically
vary the WER of both the first- and second-pass
</bodyText>
<page confidence="0.997257">
48
</page>
<bodyText confidence="0.999982578947368">
speech recognizers to further explore the interac-
tion between speech recognition performance and
concept type classification.
The improvements our two-pass recognizers
achieve have quite small local effects (up to 3.18%
absolute improvement in WER on utterances con-
taining a concept, and less than 1% on post-
confirmation utterances overall) but may have
larger impact on dialog completion times and task
completion rates, as they reduce the number of
cascading recognition errors in the dialog (et al.,
2002). Furthermore, we could also use knowledge
of the concept type(s) contained in a user utterance
to improve dialog management and response plan-
ning (Bohus, 2007). In future work, we will look
at (1) extending the use of our concept-type clas-
sifiers to utterances following any system prompt;
and (2) the impact of these interventions on overall
metrics of dialog success.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999929">
We would like to thank the researchers at CMU
for providing the Let’s Go! data and additional
resources.
</bodyText>
<sectionHeader confidence="0.999114" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999952260273973">
J. R. Bellegarda. 2004. Statistical language model
adaptation: Review and perspectives. Speech Com-
munication Special Issue on Adaptation Methods for
Speech Recognition, 42:93–108.
D. Bohus. 2007. Error awareness and recovery in
task-oriented spoken dialog systems. Ph.D. thesis,
Carnegie Mellon University.
Y. Esteve, F. Bechet, A. Nasr, and R. Mori. 2001.
Stochastic finite state automata language model trig-
gered by dialogue states. In Proceedings of Eu-
rospeech.
A. Rudnicky et al. 2000. Task and domain specific
modelling in the Carnegie Mellon Communicator
system. In Proceedings ofICSLP.
J. Shin et al. 2002. Analysis of user behavior under
error conditions in spoken dialogs. In Proceedings
ofICSLP.
D. Huggins-Daines et al. 2006. Sphinx: A free, real-
time continuous speech recognition system for hand-
held devices. In Proceedings ofICASSP.
D. Gildea and T. Hofmann. 1999. Topic-based lan-
guage models using EM. In Proceedings of Eu-
rospeech.
J. Hirschberg, D. Litman, and M. Swerts. 2004.
Prosodic and other cues to speech recognition fail-
ures. Speech Communication, 43:155–175.
R. Iyer and M. Ostendorf. 1999. Modeling long dis-
tance dependencies in language: Topic mixtures ver-
sus dynamic cache model. IEEE Transactions on
Speech and Audio Processing, 7(1):30–39.
E. Krahmer, M. Swerts, M. Theune, and M. Weegels.
2001. Error detection in spoken human-machine in-
teraction. International Journal of Speech Technol-
ogy, 4(1).
G.-A. Levow. 1998. Characterizing and recognizing
spoken corrections in human-computer dialogue. In
Proceedings of COLING-ACL.
D. Litman, J.Hirschberg, and M. Swerts. 2006. Char-
acterizing and predicting corrections in spoken dia-
logue systems. Computational Linguistics, 32:417–
438.
C. D. Manning, P. Raghavan, and H. Sch¨utze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
A. Raux, B. Langner, A. Black, and M Eskenazi. 2005.
Let’s Go Public! Taking a spoken dialog system to
the real world. In Proceedings of Eurospeech.
G. Riccardi and A. L. Gorin. 2000. Stochastic lan-
guage adaptation over time and state in a natural spo-
ken dialog system. IEEE Transactions on Speech
and Audio Processing, 8(1):3–9.
S. Stenchikova, D. Hakkani-T¨ur, and G. Tur. 2008.
Name-aware speech recognition for interactive
question answering. In Proceedings ofICASSP.
G. Tur and A. Stolcke. 2007. Unsupervised language
model adaptation for meeting recognition. In Pro-
ceedings ofICASSP.
G. Tur. 2005. Model adaptation for spoken language
understanding. In Proceedings ofICASSP.
G. Tur. 2007. Extending boosting for large scale
spoken language understanding. Machine Learning,
69(1):55–74.
M. Walker, J. Wright, and I. Langkilde. 2000. Using
natural language processing and discourse features
to identify understanding errors in a spoken dialogue
system. In Proceedings ofICML.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
W. Xu and A. Rudnicky. 2000. Language modeling
for dialog system. In Proceedings ofICSLP.
S. Young. 1994. Detecting misrecognitions and out-
of-vocabulary words. In Proceedings ofICASSP.
</reference>
<page confidence="0.999545">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.250520">
<title confidence="0.999843">Predicting Concept Types in User Corrections in Dialog</title>
<author confidence="0.691254">Stoyanchev</author>
<affiliation confidence="0.841732">Department of Computer</affiliation>
<author confidence="0.514098">Stony Brook Stony Brook</author>
<author confidence="0.514098">NY</author>
<email confidence="0.999746">svetlana.stoyanchev@gmail.com,amanda.stent@stonybrook.edu</email>
<abstract confidence="0.998048857142857">Most dialog systems explicitly confirm user-provided task-relevant concepts. User responses to these system confirmations (e.g. corrections, topic changes) may be misrecognized because they contain unrequested task-related concepts. In this we propose a lanmodel adaptation strategy the language model (LM) is adapted to the concept type(s) actually present in the user’s post-confirmation utterance. We evaluate concept type classification and LM adaptation for post-confirmation in the Go! system. We achieve 93% accuracy on concept type classification using acoustic, lexical and dialog history features. We also show that the use of concept type classification for LM adaptation can lead to improvements in speech recognition performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: Review and perspectives. Speech Communication Special Issue on Adaptation Methods for Speech Recognition,</title>
<date>2004</date>
<pages>42--93</pages>
<contexts>
<context position="4970" citStr="Bellegarda, 2004" startWordPosition="767" endWordPosition="768">petition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system</context>
</contexts>
<marker>Bellegarda, 2004</marker>
<rawString>J. R. Bellegarda. 2004. Statistical language model adaptation: Review and perspectives. Speech Communication Special Issue on Adaptation Methods for Speech Recognition, 42:93–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
</authors>
<title>Error awareness and recovery in task-oriented spoken dialog systems.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<marker>Bohus, 2007</marker>
<rawString>D. Bohus. 2007. Error awareness and recovery in task-oriented spoken dialog systems. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Esteve</author>
<author>F Bechet</author>
<author>A Nasr</author>
<author>R Mori</author>
</authors>
<title>Stochastic finite state automata language model triggered by dialogue states.</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="5377" citStr="Esteve et al., 2001" startWordPosition="827" endWordPosition="830">le, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialogs). This data has been transcribed, and annotated by hand for concept types. In order to provide the user with route information, Let’s Go! must elicit</context>
</contexts>
<marker>Esteve, Bechet, Nasr, Mori, 2001</marker>
<rawString>Y. Esteve, F. Bechet, A. Nasr, and R. Mori. 2001. Stochastic finite state automata language model triggered by dialogue states. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
</authors>
<title>Task and domain specific modelling in the Carnegie Mellon Communicator system.</title>
<date>2000</date>
<booktitle>In Proceedings ofICSLP.</booktitle>
<contexts>
<context position="23533" citStr="Rudnicky, 2000" startWordPosition="3897" endWordPosition="3898">M adaptation approach used in Let’s Go! with our proposed concept-specific adaptation; (2) compare two approaches to concept-specific adaptation (using the system’s confirmation prompt type and using our concept type classifiers); and (3) evaluate the impact of different concept type classifiers on concept-specific LM adaptation. 5.1 Method We used the PocketSphinx speech recognition engine (et al., 2006) with gender-specific telephonequality acoustic models built for Communicator (et al., 2000). We trained trigram LMs using 0.5 ratio discounting with the CMU language modeling toolkit (Xu and Rudnicky, 2000)6. We built state- and concept-specific hierarchical LMs from the Let’s Go! 2005 data. The LMs are built with [place], [time] and [bus] submodels. We evaluate speech recognition performance on the post-confirmation user utterances from the 2006 testing dataset. Each experiment varies in 1) the LM used for the final recognition pass and 2) the method of selecting a LM for use in decoding. 5.1.1 Language models We built seven LMs for these experiments. The state-specific LM contains all utterances in the training data that were produced in the yes-no dialog state. The confirm-place, confirm-bus </context>
</contexts>
<marker>Rudnicky, 2000</marker>
<rawString>A. Rudnicky et al. 2000. Task and domain specific modelling in the Carnegie Mellon Communicator system. In Proceedings ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shin</author>
</authors>
<title>Analysis of user behavior under error conditions in spoken dialogs.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP.</booktitle>
<marker>Shin, 2002</marker>
<rawString>J. Shin et al. 2002. Analysis of user behavior under error conditions in spoken dialogs. In Proceedings ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Huggins-Daines</author>
</authors>
<title>Sphinx: A free, realtime continuous speech recognition system for handheld devices.</title>
<date>2006</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<marker>Huggins-Daines, 2006</marker>
<rawString>D. Huggins-Daines et al. 2006. Sphinx: A free, realtime continuous speech recognition system for handheld devices. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>T Hofmann</author>
</authors>
<title>Topic-based language models using EM.</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="5226" citStr="Gildea and Hofmann, 1999" startWordPosition="800" endWordPosition="803"> these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialog</context>
</contexts>
<marker>Gildea, Hofmann, 1999</marker>
<rawString>D. Gildea and T. Hofmann. 1999. Topic-based language models using EM. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
<author>D Litman</author>
<author>M Swerts</author>
</authors>
<title>Prosodic and other cues to speech recognition failures.</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<pages>43--155</pages>
<contexts>
<context position="4299" citStr="Hirschberg et al., 2004" startWordPosition="645" endWordPosition="648"> March 2009. c�2009 Association for Computational Linguistics 42 2 Related Work When a dialog system requests a confirmation, the user’s subsequent corrections and topic change utterances are particularly likely to be misrecognized. Considerable research has now been done on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the reco</context>
</contexts>
<marker>Hirschberg, Litman, Swerts, 2004</marker>
<rawString>J. Hirschberg, D. Litman, and M. Swerts. 2004. Prosodic and other cues to speech recognition failures. Speech Communication, 43:155–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iyer</author>
<author>M Ostendorf</author>
</authors>
<title>Modeling long distance dependencies in language: Topic mixtures versus dynamic cache model.</title>
<date>1999</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="5415" citStr="Iyer and Ostendorf, 1999" startWordPosition="833" endWordPosition="836">pt type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialogs). This data has been transcribed, and annotated by hand for concept types. In order to provide the user with route information, Let’s Go! must elicit a departure 1 Sys Welcome to the CMU </context>
</contexts>
<marker>Iyer, Ostendorf, 1999</marker>
<rawString>R. Iyer and M. Ostendorf. 1999. Modeling long distance dependencies in language: Topic mixtures versus dynamic cache model. IEEE Transactions on Speech and Audio Processing, 7(1):30–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Swerts</author>
<author>M Theune</author>
<author>M Weegels</author>
</authors>
<title>Error detection in spoken human-machine interaction.</title>
<date>2001</date>
<journal>International Journal of Speech Technology,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="4148" citStr="Krahmer et al., 2001" startWordPosition="621" endWordPosition="624">work. without a concept. Proceedings of EACL 2009 Workshop on Semantic Representation of Spoken Language - SRSL 2009, pages 42–49, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 42 2 Related Work When a dialog system requests a confirmation, the user’s subsequent corrections and topic change utterances are particularly likely to be misrecognized. Considerable research has now been done on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. </context>
</contexts>
<marker>Krahmer, Swerts, Theune, Weegels, 2001</marker>
<rawString>E. Krahmer, M. Swerts, M. Theune, and M. Weegels. 2001. Error detection in spoken human-machine interaction. International Journal of Speech Technology, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G-A Levow</author>
</authors>
<title>Characterizing and recognizing spoken corrections in human-computer dialogue.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="4313" citStr="Levow, 1998" startWordPosition="649" endWordPosition="650">iation for Computational Linguistics 42 2 Related Work When a dialog system requests a confirmation, the user’s subsequent corrections and topic change utterances are particularly likely to be misrecognized. Considerable research has now been done on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the </context>
</contexts>
<marker>Levow, 1998</marker>
<rawString>G.-A. Levow. 1998. Characterizing and recognizing spoken corrections in human-computer dialogue. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J Hirschberg</author>
<author>M Swerts</author>
</authors>
<title>Characterizing and predicting corrections in spoken dialogue systems.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<pages>438</pages>
<contexts>
<context position="4274" citStr="Litman et al., 2006" startWordPosition="641" endWordPosition="644">9, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 42 2 Related Work When a dialog system requests a confirmation, the user’s subsequent corrections and topic change utterances are particularly likely to be misrecognized. Considerable research has now been done on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common p</context>
<context position="11781" citStr="Litman et al., 2006" startWordPosition="1890" endWordPosition="1893">sing a poorly-chosen language model. This may cause speech recognition performance to suffer. This means that we want to minimize switch+ errors. 4.2 Features We used the features summarized in Table 3. All of these features are available at run-time and so may be used in a live system. Below we give additional information about the RAW and LEX features; the other feature sets are self-explanatory. 4.2.1 Acoustic and Dialog History Features The acoustic/prosodic and dialog history features are adapted from those identified in previous work on detecting speech recognition errors (particularly (Litman et al., 2006)). We anticipated that these features would help us distinguish corrections and rejections from confirmations. 4.2.2 Lexical Features We used lexical features from the user’s current utterance. Words in the output of first-pass ASR are highly indicative both of concept presence or absence, and of the presence of particular concept types; for example, going to suggests the presence of a place. We selected the most salient lexi3We do not report precision or recall for determining absence of each concept type. In our data set 82.2% of the utterances do not contain any concepts (see Table 1). Cons</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2006</marker>
<rawString>D. Litman, J.Hirschberg, and M. Swerts. 2006. Characterizing and predicting corrections in spoken dialogue systems. Computational Linguistics, 32:417– 438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>P Raghavan</author>
<author>H Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>C. D. Manning, P. Raghavan, and H. Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Raux</author>
<author>B Langner</author>
<author>A Black</author>
<author>M Eskenazi</author>
</authors>
<title>Let’s Go Public! Taking a spoken dialog system to the real world.</title>
<date>2005</date>
<booktitle>In Proceedings of Eurospeech.</booktitle>
<contexts>
<context position="5687" citStr="Raux et al., 2005" startWordPosition="879" endWordPosition="882">ing transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialogs). This data has been transcribed, and annotated by hand for concept types. In order to provide the user with route information, Let’s Go! must elicit a departure 1 Sys Welcome to the CMU Let’s Go bus information system. What can I do for you? 2 User I need to go from Oakland:p ASR I need to go .from. can’t 3 Sys Where do you wanna leave from? 4 User to Waterfront:p ASR told. .me. Waterfront 5 Sys Leaving from WATERFRONT . Did I get that right? 6 User OAKL</context>
<context position="25650" citStr="Raux et al., 2005" startWordPosition="4224" endWordPosition="4227">ompt respectively. In the 2-pass conditions we perform first-pass recognition using the general LM. Then, we classify the output of the first pass using a concept type classifier. Finally, we perform second-pass recognition using the concept-place, concept-bus or concept-time LMs if the utterance was classified as place, bus or time respectively8. We used the three classification models with highest overall rec+: DH3 LEX RAW, ASR DH3 LEX RAW, 6We chose the same speech recognizer, acoustic models, language modeling toolkit, and LM building parameters that are used in the live Let’s Go! system (Raux et al., 2005). 7As we showed in Table 2, most, but not all, utterances in a confirmation state contain the corresponding concept. 8We treat utterances classified as containing more than concept type as none. In the 2006 data, only 5.6% of utterances with a concept contain more than one concept type. 47 Recognizer Concept type Language Overall Concept utterances classifier model WER WER Concept recall 1-pass general state-specific 38.49% 49.12% 50.75% 1-pass confirm state confirm-{place,bus,time} 38.83% 48.96% 51.36% 1-pass confirm state concept-{place,bus,time}, 46.47% 4 50.73% 46 52.9% * state-specific 2-</context>
</contexts>
<marker>Raux, Langner, Black, Eskenazi, 2005</marker>
<rawString>A. Raux, B. Langner, A. Black, and M Eskenazi. 2005. Let’s Go Public! Taking a spoken dialog system to the real world. In Proceedings of Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Riccardi</author>
<author>A L Gorin</author>
</authors>
<title>Stochastic language adaptation over time and state in a natural spoken dialog system.</title>
<date>2000</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="5355" citStr="Riccardi and Gorin, 2000" startWordPosition="823" endWordPosition="826">system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialogs). This data has been transcribed, and annotated by hand for concept types. In order to provide the user with route information,</context>
</contexts>
<marker>Riccardi, Gorin, 2000</marker>
<rawString>G. Riccardi and A. L. Gorin. 2000. Stochastic language adaptation over time and state in a natural spoken dialog system. IEEE Transactions on Speech and Audio Processing, 8(1):3–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stenchikova</author>
<author>D Hakkani-T¨ur</author>
<author>G Tur</author>
</authors>
<title>Name-aware speech recognition for interactive question answering.</title>
<date>2008</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<marker>Stenchikova, Hakkani-T¨ur, Tur, 2008</marker>
<rawString>S. Stenchikova, D. Hakkani-T¨ur, and G. Tur. 2008. Name-aware speech recognition for interactive question answering. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>A Stolcke</author>
</authors>
<title>Unsupervised language model adaptation for meeting recognition.</title>
<date>2007</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="5110" citStr="Tur and Stolcke, 2007" startWordPosition="786" endWordPosition="789">Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used come</context>
</contexts>
<marker>Tur, Stolcke, 2007</marker>
<rawString>G. Tur and A. Stolcke. 2007. Unsupervised language model adaptation for meeting recognition. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
</authors>
<title>Model adaptation for spoken language understanding.</title>
<date>2005</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="5265" citStr="Tur, 2005" startWordPosition="808" endWordPosition="809">. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialogs). This data has been transcribed, and</context>
</contexts>
<marker>Tur, 2005</marker>
<rawString>G. Tur. 2005. Model adaptation for spoken language understanding. In Proceedings ofICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
</authors>
<title>Extending boosting for large scale spoken language understanding.</title>
<date>2007</date>
<booktitle>Machine Learning,</booktitle>
<volume>69</volume>
<issue>1</issue>
<contexts>
<context position="5470" citStr="Tur, 2007" startWordPosition="844" endWordPosition="845">s now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al., 2008), broadcast news recognition (Gildea and Hofmann, 1999), and spoken dialog systems (Tur, 2005). LMs in dialog systems can be adapted to the dialog state (e.g. (Riccardi and Gorin, 2000; Esteve et al., 2001)), the topic (Iyer and Ostendorf, 1999; Gildea and Hofmann, 1999), or the speaker (Tur, 2007). 3 Data In this experiment we use annotated dialog transcripts and speech from the Let’s Go! system, a telephone-based spoken dialog system that provides information about bus routes in Pittsburgh (Raux et al., 2005). The data we used comes from the first two months of Let’s Go! system operation in 2005 (2411 dialogs), and one month in 2006 (1430 dialogs). This data has been transcribed, and annotated by hand for concept types. In order to provide the user with route information, Let’s Go! must elicit a departure 1 Sys Welcome to the CMU Let’s Go bus information system. What can I do for you?</context>
</contexts>
<marker>Tur, 2007</marker>
<rawString>G. Tur. 2007. Extending boosting for large scale spoken language understanding. Machine Learning, 69(1):55–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>J Wright</author>
<author>I Langkilde</author>
</authors>
<title>Using natural language processing and discourse features to identify understanding errors in a spoken dialogue system.</title>
<date>2000</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="4565" citStr="Walker et al., 2000" startWordPosition="691" endWordPosition="694">one on the automatic detection of spoken corrections. Linguistic cues to corrections include the number of words in the post-confirmation utterance and the use of marked word order (Krahmer et al., 2001). Prosodic cues include F0 max, RMS max, RMS mean, duration, speech tempo, and percentage of silent frames(Litman et al., 2006; Hirschberg et al., 2004; Levow, 1998). Discourse cues include the removal, repetition, addition or modification of a concept, the system’s dialog act type, and information about error rates in the dialog so far (Krahmer et al., 2001; et al., 2002; Litman et al., 2006; Walker et al., 2000). In our experiments, we use most of these features as well as additional lexical features. We can use knowledge of the type or content of a user utterance to modify system behavior. For example, in this paper we use the concept type(s) in the user’s utterance to adapt the recognizer’s LM. It is now common practice to adapt the recognizer to the type, context or style of input speech (Bellegarda, 2004). LM adaptation has been used to improve automatic speech recognition performance in automated meeting transcription (Tur and Stolcke, 2007), speech-driven question answering (Stenchikova et al.,</context>
</contexts>
<marker>Walker, Wright, Langkilde, 2000</marker>
<rawString>M. Walker, J. Wright, and I. Langkilde. 2000. Using natural language processing and discourse features to identify understanding errors in a spoken dialogue system. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="9844" citStr="Witten and Frank, 2005" startWordPosition="1592" endWordPosition="1595">tion 3, we extracted the features described in Section 4.2 below. To identify the correct concept type(s) for each utterance, we used the human annotations provided with the data. We performed a series of 10-fold crossvalidation experiments to examine the impact of different types of feature on concept type classification. We trained three binary classifiers for each experiment, one for each concept type, i.e. we separately classified each post-confirmation utterance as place + or place -, time + or time -, and bus + or bus -. We used Weka’s implementation of the J48 decision tree classifier (Witten and Frank, 2005)2. For each experiment, we report precision (pre+) and recall (rec+) for determining presence of each concept type, and overall classification accuracy 2J48 gave the highest classification accuracy compared to other machine learning algorithms we tried on this data. for each concept type (place, bus and time)3. We also report overall pre+, rec+, f-measure (f+), and classification accuracy across the three concept types. Finally, we report the percentage of switch+ errors and switch errors. Switch+ errors are utterances containing bus classified as time/place, time as bus/place, and place as bu</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>A Rudnicky</author>
</authors>
<title>Language modeling for dialog system.</title>
<date>2000</date>
<booktitle>In Proceedings ofICSLP.</booktitle>
<contexts>
<context position="23533" citStr="Xu and Rudnicky, 2000" startWordPosition="3895" endWordPosition="3898">cific LM adaptation approach used in Let’s Go! with our proposed concept-specific adaptation; (2) compare two approaches to concept-specific adaptation (using the system’s confirmation prompt type and using our concept type classifiers); and (3) evaluate the impact of different concept type classifiers on concept-specific LM adaptation. 5.1 Method We used the PocketSphinx speech recognition engine (et al., 2006) with gender-specific telephonequality acoustic models built for Communicator (et al., 2000). We trained trigram LMs using 0.5 ratio discounting with the CMU language modeling toolkit (Xu and Rudnicky, 2000)6. We built state- and concept-specific hierarchical LMs from the Let’s Go! 2005 data. The LMs are built with [place], [time] and [bus] submodels. We evaluate speech recognition performance on the post-confirmation user utterances from the 2006 testing dataset. Each experiment varies in 1) the LM used for the final recognition pass and 2) the method of selecting a LM for use in decoding. 5.1.1 Language models We built seven LMs for these experiments. The state-specific LM contains all utterances in the training data that were produced in the yes-no dialog state. The confirm-place, confirm-bus </context>
</contexts>
<marker>Xu, Rudnicky, 2000</marker>
<rawString>W. Xu and A. Rudnicky. 2000. Language modeling for dialog system. In Proceedings ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
</authors>
<title>Detecting misrecognitions and outof-vocabulary words.</title>
<date>1994</date>
<booktitle>In Proceedings ofICASSP.</booktitle>
<contexts>
<context position="2283" citStr="Young, 1994" startWordPosition="327" endWordPosition="328">el (LM)1. This means that they are 1The word error rate on post-confirmation Let’s Go! utterances containing a concept is 10% higher than on utterances likely to be misrecognized, frustrating the user and leading to cascading errors. Correct determination of the content of post-confirmation utterances can lead to improved speech recognition, fewer and shorter sequences of speech recognition errors, and improved dialog system performance. In this paper, we look at user responses to system confirmation prompts CMU’s deployed Let’s Go! dialog system. We adopt a two-pass recognition architecture (Young, 1994). In the first pass, the input utterance is processed using a generalpurpose LM (e.g. specific to the domain, or specific to the dialog state). Recognition may fail on concept words such as “Oakland” or “61C” , but is likely to succeed on closed-class words (e.g. ”yes”, ”no”, ”and”, ”but”, ”leaving”). If the utterance follows a system confirmation prompt, we then use acoustic, lexical and dialog history features to determine the task-related concept type(s) likely to be present in the utterance. In the second recognition pass, any utterance containing a concept type is re-processed using a con</context>
</contexts>
<marker>Young, 1994</marker>
<rawString>S. Young. 1994. Detecting misrecognitions and outof-vocabulary words. In Proceedings ofICASSP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>