<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014825">
<title confidence="0.983011">
HeidelToul: A Baseline Approach for Cross-document Event Ordering
</title>
<author confidence="0.984204">
Bilel Moulahi1,3* and Jannik Str¨otgen2 and Michael Gertz2 and Lynda Tamine1
</author>
<affiliation confidence="0.995801666666667">
1 IRIT, University of Toulouse Paul Sabatier, France
2 Institute of Computer Science, Heidelberg University, Germany
3 Faculty of Science of Tunis, University of Tunis El-Manar, Tunisia
</affiliation>
<email confidence="0.991071">
{bilel.moulahi,lynda.lechani}@irit.fr
{stroetgen,gertz}@informatik.uni-heidelberg.de
</email>
<sectionHeader confidence="0.997313" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999822285714286">
In this paper, we give an overview of our
participation in the timeline generation task
of SemEval-2015 (task 4, TimeLine: Cross-
Document Event Ordering). The main goals
of this new track are, given a collection of
news articles and a so-called target entity, to
determine events that are relevant for the en-
tity, to resolve event coreferences, and to order
the events chronologically. We addressed the
sub-tasks, in which event mentions were pro-
vided, i.e., no additional event extraction was
required. For this, we developed an ad-hoc ap-
proach based on a temporal tagger and a coref-
erence resolution tool for entities. After de-
termining relevant sentences, relevant events
are extracted and anchored on a timeline. The
evaluation conducted on three collections of
news articles shows that our approach – de-
spite its simplicity – achieves reasonable re-
sults and opens several promising issues for
future work.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999472906976744">
Due to the tremendous amount of documents being
constantly published on the Internet, there is a need
for more enhanced search facilities to retrieve rel-
evant information. Consider, for example, a user
looking for information about the “Golden Globe
Awards”. It might be possible that the user’s in-
formation need is about the recent “72nd edition”.
However, it is also reasonable to assume that the user
*The work was done during an internship at Heidelberg
University.
would appreciate relevant information about previ-
ous editions. Thus, presenting search results for
time- and event-sensitive information needs in the
form of a complete and updatable timeline would
be a promising approach. While this issue is tack-
led by some applications, early techniques required
manual effort (Shahar and Musen, 1992) and recent
approaches rely on heavily structured information
such as Google’s entity-related search results, which
are based on Google’s knowledge graph (Singhal,
2012). However, instead of listing only structured
knowledge on a timeline, e.g., winners of the 71st
Golden Globes in our example, search results would
become much more valuable when adding tempo-
rally anchored event information extracted from text
documents (e.g., recent updates about the event).
In the SemEval task 4,1 the goal is to detect all
events in a document collection that are relevant for
a target entity, and to anchor these events on a time-
line. Thus, events are to be sorted chronologically,
and, if possible, specific dates are to be assigned to
the events. As in previous SemEval tasks addressing
temporal relation extraction, namely in the Temp-
Eval series (see, e.g., Verhagen et al., 2010), the
TimeML event definition is used. However, a special
focus is now put on the cross-document aspect, i.e.,
on cross-document event coreference resolution and
cross-document temporal relation extraction. While
the document collection contains news articles, tar-
get entities can be persons, organizations, products,
or financial entities.
The organizers offered the task in two tracks.
While the final goals of timeline construction are
</bodyText>
<footnote confidence="0.984734">
1http://alt.qcri.org/semeval2015/task4/
</footnote>
<page confidence="0.972675">
825
</page>
<bodyText confidence="0.867433555555556">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 825–829,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
identical in both tracks, systems addressing track A
had to extract event mentions, while event annota-
tions were provided to participants of track B. Fur-
thermore, both tracks were evaluated with and with-
out assigning explicit temporal information to the
events. Since we participated in track B, the main
challenges for our approach were to
</bodyText>
<listItem confidence="0.999949">
• filter events relevant for the target entities,
• assign date information to relevant events,
• determine cross-document event coreferences,
• and to construct a timeline for each entity.
</listItem>
<bodyText confidence="0.999846666666667">
In the following section, we describe our ap-
proach and give an example for cross-document
event ordering. In Section 3, we present and ana-
lyze the official evaluation results. Finally, we dis-
cuss open issues for future research in the context of
cross-document timeline construction.
</bodyText>
<sectionHeader confidence="0.951873" genericHeader="method">
2 Cross-document Event Ordering
</sectionHeader>
<bodyText confidence="0.999281">
Given a set of documents and a set of target entities,
the task is to build an event timeline for each entity.
Documents are provided with annotated sentences,
which may contain several event annotations.
</bodyText>
<subsectionHeader confidence="0.996613">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.99993275">
We implemented an ad-hoc approach for both the re-
trieval and the anchoring of relevant events. Figure 1
illustrates the general architecture of our approach.
Our system is based on five main components:
</bodyText>
<listItem confidence="0.947593">
• Matching: In this step, we identify sentences in
the document collection, in which parts of the
target entity name occur. Furthermore, we use
the cosine similarity matching function with a
threshold to not select sentences that contain
too few parts of the entity name. The result of
this step is a list of sentences with event candi-
dates for the timeline of the target entity.
• Coreference resolution: To avoid extracting
event candidates only from sentences in which
parts of the entity name occur explicitly, we
apply entity coreference resolution using the
Stanford CoreNLP tool (Lee et al., 2013; Man-
ning et al., 2014). Thus, sentences, in which
</listItem>
<bodyText confidence="0.714102666666667">
other terms, e.g., pronouns, are used to refer to
the target entity, can be added to the list of sen-
tences with event candidates.
</bodyText>
<figureCaption confidence="0.871322">
Figure 1: General architecture of our approach.
</figureCaption>
<listItem confidence="0.988834875">
• Temporal tagging: To extract and normalize
temporal expressions, HeidelTime (Str¨otgen
and Gertz, 2013) is applied. If a temporal ex-
pression cooccurs with an event candidate in a
sentence, the event is anchored at the respective
point in time. If no expressions are detected in
a sentence, we use the document creation time
as anchor date for respective events.
• Filtering: Since the first steps result in many
event candidates, we aim to filter out non rel-
evant events to improve the precision of our
approach. Using a threshold for the token dis-
tance between the event and the closest term re-
ferring to the target entity, we prune events for
which it is unlikly that the entity is involved.
• Cross-document event clustering: Finally, all
</listItem>
<bodyText confidence="0.89659825">
events anchored at the same point in time with
identical covered text are clustered.
We apply several filtering techniques in order to
prune non relevant sentences and events. These
thresholds were tuned using the trial data provided
by the organizers. Since the performance of our sys-
tem depends on these parameters, we submitted two
runs with different configurations:
</bodyText>
<listItem confidence="0.993964833333333">
• HeidelToul NonTolMatchPrune: The first run
uses a non-tolerant pruning setting with low
values for the thresholds and distances.
• HeidelToul TolMatchPrune: The second run
performs a more tolerant pruning of events and
sentences using quite high thresholds.
</listItem>
<page confidence="0.980079">
826
</page>
<table confidence="0.997354875">
anchor date event rel
1 1994 8983-13-flight 1
2 2000-02 4764-6-received 0
2 2000-02 4764-6-announced 1
3 2004-11-24 1173-6-negotiations 0
... ... ... ...
9 2008 8983-14-development 0
9 2008 8983-14-scheduled 0
</table>
<tableCaption confidence="0.841794">
Table 1: Timeline excerpt returned for Boeing 777.
Events are either relevant (1) or not (0).
Evaluation results are discussed in Section 3.
</tableCaption>
<subsectionHeader confidence="0.906339">
2.2 Timeline Construction Example
</subsectionHeader>
<bodyText confidence="0.998600121212121">
Table 1 shows some events of the timeline con-
structed by our system for the entity Boeing 777.
The listed events are extracted from the document
parts depicted in Figure 2. Events mentioned in the
timeline are surrounded by boxes, and (parts of) the
entity mentions are underlined. In the following, we
explain the timeline and the reasons for incorrectly
returned and anchored events.
The first column of the timeline refers to the rank
of the events, the second contains the dates in which
events are anchored, and the third corresponds to
the events that are detected as relevant for the tar-
get entity. Each event of the timeline is represented
by the document id and sentence id from which it
was extracted, and the covered text of the event men-
tion. For instance, our system correctly determines
the eventflight as chronologically first relevant event
(rank 1) occurring in 1994. It was extracted from
sentence 13 of document 8983 (c.f. Figure 2c).
If two events are simultaneous, they can be asso-
ciated with the same rank, as the second and third
event. If a systems fails to extract the anchor dates
of relevant events, these should be returned at rank 0
and are ignored in the evaluation.
Using the excerpts in Figure 2, we explain why
events in Table 1 have been selected as relevant for
Boeing 777. All sentences contain only substrings
of the target entity name, i.e., the full entity name
never occurs. For instance, sentence 13 of docu-
ment 8983 contains the string 777 while sentence 14
contains the string Boeing. As explained above, we
used substring matching with a threshold and coref-
erence resolution to increase the number of poten-
</bodyText>
<figure confidence="0.908748">
(a) Doc. #1173: Internal emails expose Boeing ...
</figure>
<figureCaption confidence="0.9766895">
Figure 2: Three document excerpts with sentences
containing events returned for entity Boeing 777.
</figureCaption>
<bodyText confidence="0.998664076923077">
tially relevant events. While for many target enti-
ties, it is important to not require a full entity name
(e.g., for persons), the term Boeing in the three doc-
ument excerpts never refers to Boeing 777, resulting
in some non-relevant events in our timeline. Note,
however, that relying on strict entity matching, no
event could be extracted from the sentences shown
in Figure 2, and that some events considered as not
relevant in the gold standard are at least debatable,
e.g., received: Although our anchor date is incorrect
(it should be the document creation time of the arti-
cle due to so far), the event is relevant for the target
entity since Boeing 777 is the subject of the orders.
</bodyText>
<sectionHeader confidence="0.932188" genericHeader="method">
3 Experimental Results and Discussion
</sectionHeader>
<bodyText confidence="0.9993">
The evaluation data consists of 3 sets of 30 doc-
uments from Wikinews annotated with event men-
</bodyText>
<listItem confidence="0.8425395">
(b) Doc. #4764: Boeing unveils long-range 777.
(c) Doc. #8983: Boeing secures $11bn of aircraft deals.
</listItem>
<page confidence="0.955926">
827
</page>
<table confidence="0.9998685">
track run MICRO-FSCORE details (overall)
corpus 1 corpus 2 corpus 3 overall precision recall
TrackB GPLSIUA 1 22.35 19.28 33.59 25.36 21.73 30.46
TrackB GPLSIUA 2 20.47 16.17 29.90 22.66 20.08 26.00
TrackB HeidelToul NTMP2 19.62 7.25 20.37 17.03 20.11 14.76
TrackB HeidelToul TMP3 16.50 10.94 25.89 18.34 13.58 28.22
SubTrackB GPLSIUA 1 18.35 20.48 32.08 23.15 18.90 29.85
SubTrackB GPLSIUA 2 15.93 14.44 27.48 19.18 16.19 23.52
SubTrackB HeidelToul NTMP 12.23 14.78 16.11 14.42 19.58 11.42
SubTrackB HeidelToul TMP 13.24 15.88 21.99 16.67 12.18 26.41
</table>
<tableCaption confidence="0.995619">
Table 2: Official results of participating groups in SemEval 2015 task 4. 2NonTolMatchPrune: non tolerant
matching and pruning setting; 3TolMatchPrune: tolerant matching and pruning setting (cf. Section 2.1).
</tableCaption>
<bodyText confidence="0.997252549019608">
tions and a total of 38 target entities. Our system
ranked second among only two participating groups.
While there have been a total of four teams
participating in the task, only two participated in
(sub)track B. Participants of (sub)track A addition-
ally performed event extraction so that a comparison
between results of all four participants is not possi-
ble. Thus, in Table 2, we only present the results of
the two teams that addressed (sub)track B.
Table 2 (left) reports the results by means of
Micro-FSCORE obtained by our runs and that of
the other participating group. As shown, our system
is outperformed by the system “GPLSIUA” for both
settings. The performance difference is most signif-
icant for corpus 2, especially within TrackB. How-
ever, we notice that our tolerant setting gives better
overall results than the non tolerant one. These im-
provements are less significant for corpora 1 and 2
than for corpus 3.
To get a deep understanding of the results, we re-
port in Table 2 (right) the overall precision and recall
values for our system configurations and that of the
other participating group. Our non tolerant setting is
slightly outperformed by the run ”GPLSIUA 1” in
terms of precision for trackB. However, it relatively
enhances the other runs within the SubTrackB. This
can be explained by the important number of rele-
vant retrieved events due to the high values of dis-
tances and thresholds used to prune the events. In
contrast, in terms of recall, our tolerant setting per-
forms better than the non tolerant one in both sub-
tracks. Actually, this is not surprising given that the
filtering techniques are not strict.
Interestingly, an in-depth analysis of the nature of
the target entities and the types of temporal expres-
sions in the documents for which our system fails
to provide good timeline, may help to improve the
overall performance of our system in the future. For
instance, for the target entities “Boeing 777” and
“Airbus A380” in corpus 1, we obtained the lowest
values in terms of MicroFSCORE among all target
entities. Clearly, this is due to the partial matching
technique we used, which results in the extraction of
many events related to other entities (e.g., “Boeing
787” instead of “Boeing 777”; cf. Table 1 and Fig-
ure 2). Moreover, all events that do not cooccur with
a temporal expression in the same sentence are an-
chored at the document creation time by our system.
This hurts the performance of our system in partic-
ular for TrackB, because many of those events are
placed at rank 0 in the gold standard.
</bodyText>
<sectionHeader confidence="0.99974" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999920333333333">
In this paper, we presented an overview of our
participation in the timeline generation task of
SemEval-2015. We proposed a baseline approach
for the extraction and anchoring of events. Our sys-
tem is evaluated using three corpora of news articles
and shows reasonable results.
Interesting future work to improve our approach
could include a fine tuning of the matching function
as well as the filtering parameters used to prune non
relevant events. In addition, more sophisticated en-
tity disambiguation could further improve the per-
formance of our system.
</bodyText>
<page confidence="0.997225">
828
</page>
<sectionHeader confidence="0.999545" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999749">
We thank the task organizers for their guidance and
prompt support in all organizational matters.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99963348">
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky.
2013. Deterministic Coreference Resolution Based
on Entity-centric, Precision-ranked Rules. Computu-
tational Linguistics, 39(4):885–916, December.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language Pro-
cessing Toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
Yuval Shahar and Mark A. Musen. 1992. A Temporal-
Abstraction System for Patient Monitoring. In Pro-
ceedings of the Annual Symposium on Computer Ap-
plication in Medical Care, pages 121–127.
Amit Singhal. 2012. Introducing the Knowledge Graph:
Things, not Strings”. Official Google Blog, May.
Jannik Str¨otgen and Michael Gertz. 2013. Multilingual
and Cross-domain Temporal Tagging. Language Re-
sources and Evaluation, 47(2):269–298.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation (SemEval ’10),
pages 57–62.
</reference>
<page confidence="0.998941">
829
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.534227">
<title confidence="0.996365">HeidelToul: A Baseline Approach for Cross-document Event Ordering</title>
<affiliation confidence="0.981603">University of Toulouse Paul Sabatier, of Computer Science, Heidelberg University, of Science of Tunis, University of Tunis El-Manar,</affiliation>
<abstract confidence="0.997974363636364">In this paper, we give an overview of our participation in the timeline generation task of SemEval-2015 (task 4, TimeLine: Cross- Document Event Ordering). The main goals of this new track are, given a collection of news articles and a so-called target entity, to determine events that are relevant for the entity, to resolve event coreferences, and to order the events chronologically. We addressed the sub-tasks, in which event mentions were provided, i.e., no additional event extraction was required. For this, we developed an ad-hoc approach based on a temporal tagger and a coreference resolution tool for entities. After determining relevant sentences, relevant events are extracted and anchored on a timeline. The evaluation conducted on three collections of news articles shows that our approach – despite its simplicity – achieves reasonable results and opens several promising issues for future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic Coreference Resolution Based on Entity-centric, Precision-ranked Rules.</title>
<date>2013</date>
<journal>Compututational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="5577" citStr="Lee et al., 2013" startWordPosition="849" endWordPosition="852"> components: • Matching: In this step, we identify sentences in the document collection, in which parts of the target entity name occur. Furthermore, we use the cosine similarity matching function with a threshold to not select sentences that contain too few parts of the entity name. The result of this step is a list of sentences with event candidates for the timeline of the target entity. • Coreference resolution: To avoid extracting event candidates only from sentences in which parts of the entity name occur explicitly, we apply entity coreference resolution using the Stanford CoreNLP tool (Lee et al., 2013; Manning et al., 2014). Thus, sentences, in which other terms, e.g., pronouns, are used to refer to the target entity, can be added to the list of sentences with event candidates. Figure 1: General architecture of our approach. • Temporal tagging: To extract and normalize temporal expressions, HeidelTime (Str¨otgen and Gertz, 2013) is applied. If a temporal expression cooccurs with an event candidate in a sentence, the event is anchored at the respective point in time. If no expressions are detected in a sentence, we use the document creation time as anchor date for respective events. • Filte</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic Coreference Resolution Based on Entity-centric, Precision-ranked Rules. Compututational Linguistics, 39(4):885–916, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="5600" citStr="Manning et al., 2014" startWordPosition="853" endWordPosition="857">ching: In this step, we identify sentences in the document collection, in which parts of the target entity name occur. Furthermore, we use the cosine similarity matching function with a threshold to not select sentences that contain too few parts of the entity name. The result of this step is a list of sentences with event candidates for the timeline of the target entity. • Coreference resolution: To avoid extracting event candidates only from sentences in which parts of the entity name occur explicitly, we apply entity coreference resolution using the Stanford CoreNLP tool (Lee et al., 2013; Manning et al., 2014). Thus, sentences, in which other terms, e.g., pronouns, are used to refer to the target entity, can be added to the list of sentences with event candidates. Figure 1: General architecture of our approach. • Temporal tagging: To extract and normalize temporal expressions, HeidelTime (Str¨otgen and Gertz, 2013) is applied. If a temporal expression cooccurs with an event candidate in a sentence, the event is anchored at the respective point in time. If no expressions are detected in a sentence, we use the document creation time as anchor date for respective events. • Filtering: Since the first s</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Shahar</author>
<author>Mark A Musen</author>
</authors>
<title>A TemporalAbstraction System for Patient Monitoring.</title>
<date>1992</date>
<booktitle>In Proceedings of the Annual Symposium on Computer Application in Medical Care,</booktitle>
<pages>121--127</pages>
<contexts>
<context position="2155" citStr="Shahar and Musen, 1992" startWordPosition="320" endWordPosition="323">r example, a user looking for information about the “Golden Globe Awards”. It might be possible that the user’s information need is about the recent “72nd edition”. However, it is also reasonable to assume that the user *The work was done during an internship at Heidelberg University. would appreciate relevant information about previous editions. Thus, presenting search results for time- and event-sensitive information needs in the form of a complete and updatable timeline would be a promising approach. While this issue is tackled by some applications, early techniques required manual effort (Shahar and Musen, 1992) and recent approaches rely on heavily structured information such as Google’s entity-related search results, which are based on Google’s knowledge graph (Singhal, 2012). However, instead of listing only structured knowledge on a timeline, e.g., winners of the 71st Golden Globes in our example, search results would become much more valuable when adding temporally anchored event information extracted from text documents (e.g., recent updates about the event). In the SemEval task 4,1 the goal is to detect all events in a document collection that are relevant for a target entity, and to anchor th</context>
</contexts>
<marker>Shahar, Musen, 1992</marker>
<rawString>Yuval Shahar and Mark A. Musen. 1992. A TemporalAbstraction System for Patient Monitoring. In Proceedings of the Annual Symposium on Computer Application in Medical Care, pages 121–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
</authors>
<title>Introducing the Knowledge Graph: Things, not Strings”. Official Google Blog,</title>
<date>2012</date>
<contexts>
<context position="2324" citStr="Singhal, 2012" startWordPosition="345" endWordPosition="346">s also reasonable to assume that the user *The work was done during an internship at Heidelberg University. would appreciate relevant information about previous editions. Thus, presenting search results for time- and event-sensitive information needs in the form of a complete and updatable timeline would be a promising approach. While this issue is tackled by some applications, early techniques required manual effort (Shahar and Musen, 1992) and recent approaches rely on heavily structured information such as Google’s entity-related search results, which are based on Google’s knowledge graph (Singhal, 2012). However, instead of listing only structured knowledge on a timeline, e.g., winners of the 71st Golden Globes in our example, search results would become much more valuable when adding temporally anchored event information extracted from text documents (e.g., recent updates about the event). In the SemEval task 4,1 the goal is to detect all events in a document collection that are relevant for a target entity, and to anchor these events on a timeline. Thus, events are to be sorted chronologically, and, if possible, specific dates are to be assigned to the events. As in previous SemEval tasks </context>
</contexts>
<marker>Singhal, 2012</marker>
<rawString>Amit Singhal. 2012. Introducing the Knowledge Graph: Things, not Strings”. Official Google Blog, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jannik Str¨otgen</author>
<author>Michael Gertz</author>
</authors>
<title>Multilingual and Cross-domain Temporal Tagging. Language Resources and Evaluation,</title>
<date>2013</date>
<volume>47</volume>
<issue>2</issue>
<marker>Str¨otgen, Gertz, 2013</marker>
<rawString>Jannik Str¨otgen and Michael Gertz. 2013. Multilingual and Cross-domain Temporal Tagging. Language Resources and Evaluation, 47(2):269–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval ’10),</booktitle>
<pages>57--62</pages>
<contexts>
<context position="3029" citStr="Verhagen et al., 2010" startWordPosition="459" endWordPosition="462">s of the 71st Golden Globes in our example, search results would become much more valuable when adding temporally anchored event information extracted from text documents (e.g., recent updates about the event). In the SemEval task 4,1 the goal is to detect all events in a document collection that are relevant for a target entity, and to anchor these events on a timeline. Thus, events are to be sorted chronologically, and, if possible, specific dates are to be assigned to the events. As in previous SemEval tasks addressing temporal relation extraction, namely in the TempEval series (see, e.g., Verhagen et al., 2010), the TimeML event definition is used. However, a special focus is now put on the cross-document aspect, i.e., on cross-document event coreference resolution and cross-document temporal relation extraction. While the document collection contains news articles, target entities can be persons, organizations, products, or financial entities. The organizers offered the task in two tracks. While the final goals of timeline construction are 1http://alt.qcri.org/semeval2015/task4/ 825 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 825–829, Denver, Colorado,</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. SemEval-2010 Task 13: TempEval-2. In Proceedings of the 5th International Workshop on Semantic Evaluation (SemEval ’10), pages 57–62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>