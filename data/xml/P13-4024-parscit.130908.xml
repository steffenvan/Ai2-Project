<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000361">
<title confidence="0.962056">
Linggle: a Web-scale Linguistic Search Engine for Words in Context
</title>
<author confidence="0.999311">
Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang*
</author>
<affiliation confidence="0.96051">
+Institute of Information Systems and Applications
*Department of Computer Science
National Tsing Hua University
</affiliation>
<address confidence="0.844458">
HsinChu, Taiwan, R.O.C. 30013
{joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang}
</address>
<email confidence="0.996737">
@gmail.com
</email>
<sectionHeader confidence="0.995607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999763571428571">
In this paper, we introduce a Web-scale lin-
guistics search engine, Linggle, that retrieves
lexical bundles in response to a given query.
The query might contain keywords, wildcards,
wild parts of speech (PoS), synonyms, and ad-
ditional regular expression (RE) operators. In
our approach, we incorporate inverted file in-
dexing, PoS information from BNC, and se-
mantic indexing based on Latent Dirichlet Al-
location with Google Web 1T. The method in-
volves parsing the query to transforming it in-
to several keyword retrieval commands. Word
chunks are retrieved with counts, further filter-
ing the chunks with the query as a RE, and fi-
nally displaying the results according to the
counts, similarities, and topics. Clusters of
synonyms or conceptually related words are
also provided. In addition, Linggle provides
example sentences from The New York Times
on demand. The current implementation of
Linggle is the most functionally comprehen-
sive, and is in principle language and dataset
independent. We plan to extend Linggle to
provide fast and convenient access to a wealth
of linguistic information embodied in Web
scale datasets including Google Web 1T and
Google Books Ngram for many major lan-
guages in the world.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983803921569">
As a non-native speaker writing in English, one
encounters many problems. Doubts concerning
the usage of a preposition, the mandatory presen-
ce of a determiner, the correctness of the associa-
tion of a verb with an object, or the need for syn-
onyms of a term in a given context are issues that
arise frequently. Printed collocation dictionaries
and reference tools based on compiled corpora
offer limited coverage of word usage while
knowledge of collocations is vital to acquire a
good level of linguistic competency. We propose
to address these limitations with a comprehen-
sive system aimed at helping the learners ‚Äúknow
a word by the company it keeps‚Äù (Firth, 1957).
Linggle (linggle.com). The system based on
Web-scaled datasets is designed to be a broad
coverage language reference tool for English
Second Language learners (ESL). It is conceived
to search information related to word usage in
context under various conditions.
First, we build an inverted file index for the
Google Web 1T n-grams to support queries with
RE-like patterns including PoS and synonym
matches. For example, for the query ‚Äú$V $D
+important role‚Äù, Linggle retrieves 4-grams that
start with a verb and a determiner followed by a
synonym of important and the keyword role (e.g.,
play a significant role 202,800). A natural lan-
guage interface is also available for users who
are less familiar with pattern-based searches. For
example, the question ‚ÄúHow can I describe a
beach?‚Äù would retrieve two word chunks such as
‚Äúsandy beach 413,300‚Äù and ‚Äúrocky beach
16,800‚Äù. The n-gram search implementation is
achieved through filtering, re-indexing, populat-
ing an HBase database with the Web 1T n-grams
and augmenting them with the most frequent PoS
for words (without disambiguation) derived from
the British National Corpus (BNC).
The n-grams returned for a query can then be
linked to examples extracted from the New York
Times Corpus (Sandhaus, 2008) in order to
provide full sentential context for more effective
learning.
In some situations, the user might need to search
for words in a specific syntactic relation (e.g.,
Verb-Object collocation). The query absorb $N
in n-grams display mode returns all the nouns
that follow the verb ordered by decreasing n-
gram counts. Some of these nouns might not be
objects of the verb absorb. In contrast, the same
</bodyText>
<page confidence="0.984966">
139
</page>
<bodyText confidence="0.844388111111111">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 139‚Äì144,
Sofia, Bulgaria, August 4-9 2013. cÔøΩ2013 Association for Computational Linguistics
query in cluster display mode will control that
two words have been labeled verb-object by a
parser. Moreover, n-grams grouped by object
topic/domain give the learner an overview of the
usage of the verb. For example the verb absorb
takes clusters of objects related to the topics liq-
uid, energy, money, knowledge, and population.
</bodyText>
<figureCaption confidence="0.536841">
Figure 1. An example Linggle search for the que-
ry ‚Äúabsorb $N.‚Äù
</figureCaption>
<bodyText confidence="0.999639423076923">
This tendency of predicates to prefer certain
classes of arguments is defined by Wilks (1978)
as selectional preferences and widely reported in
the literature. Erk and Pad‚Äî (2010) extend exper-
iments on selectional preference induction to in-
verse selectional preference, considering the re-
striction imposed on predicates. Inverse sectional
preference is also implemented in linggle (e.g.
‚Äú$V apple‚Äù).
Linggle presents clusters of synonymous col-
locates (adjectives, nouns and verbs) of a query
keyword. We obtained the clusters by building
on Lin and Pantel‚Äôs (2002) large-scale repository
of dependencies and word similarity scores. Us-
ing the method proposed by Ritter and Etzioni
(2010) we induce selectional preference with a
Latent Dirichlet Allocation (LDA) model to seed
the clusters.
The rest of the paper is organized as follows.
We review the related work in the next section.
Then we present the syntax of the queries and the
functionalities of the system (Section 3). We de-
scribe the details of implementation including the
indexing of the n-grams and the clustering algo-
rithm (Section 4) and draw perspective of devel-
opment of Web scale search engines (Section 5).
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999744431034483">
Web-scale Linguistic Search Engine (LSE) has
been an area of active research. Recently, the
state-of-the-art in LSE research has been re-
viewed in Fletcher (2012). We present in this
paper a linguistic search engine that provides a
more comprehensive and powerful set of query
features.
Kilgarriff et al. (2001) describe the implemen-
tation of the linguistic search engine Word
Sketch (2001) that displays collocations and de-
pendencies acquired from a large corpus such as
the BNC. Word Sketch is not as flexible as typi-
cal search engines, only supporting a fixed set of
queries.
Recently, researchers have been attempting to
go one step further and work with Web scale da-
tasets, but it is difficult for an academic institute
to crawl a dataset that is on par with the datasets
built by search engine companies. In 2006,
Google released the Web 1T for several major
languages of the world (trillion-word n-gram da-
tasets for English, Japanese, Chinese, and ten
European languages), to stimulate NLP research
in many areas. In 2008, Chang described a pro-
totype that enhances Google Web 1T bigrams
with PoS tags and supports search in the dataset
by wildcards (wild-PoS), to identify recurring
collocations. Wu, Witten and Franken (2010)
describe a more comprehensive system (FLAX)
that combines filtered Google data with text ex-
amples from the BNC for several learning activi-
ties.
In a way similar to Chang (2008) and Wu,
Witten and Franken (2010), Stein, Potthast, and
Trenkmann (2010) describe the implementation
and application of NetSpeak, a system that pro-
vides quick access to the Google Web 1T n-gram
with RE-like queries (alternator ‚Äú|‚Äù, one arbitrary
word ‚Äú*‚Äù, arbitrary number of words between
two specified words ‚Äú√â‚Äù). In contrast to Linggle,
NetSpeak does not support PoS wildcard or con-
ceptual clustering.
An important function in both Linggle and
NetSpeak is synonym query. NetSpeak uses
WordNet (Fellbaum 2010) synsets to support
synonym match. But WordNet synsets tend to
contain very little synonyms, leading to poor
coverage. Alternatively, one can use the distribu-
tional approach to similarity based on a very
large corpus. Lin and Pantel (2002) report efforts
to build a large repository of dependencies ex-
tracted from large corpora such as Wikipedia,
and provide similarity between words
(demo.patrickpantel.com). We use these results
both for handling synonym queries and to or-
ganize the n-grams into semantic classes.
More recently, Ritter and Etzioni (2010) pro-
pose to apply an LDA model (Blei et al. 2003) to
</bodyText>
<page confidence="0.98134">
140
</page>
<bodyText confidence="0.999477604651163">
the problem of inducing selectional preference.
The idea is to consider the verbs in a corpus as
the documents of a traditional LDA model. The
arguments of the verb that are encountered in the
corpus are treated as the words composing a
document in the traditional model. The model
seems to successfully infer the semantic classes
that correspond to the preferred arguments of a
verb. The topics are semi-automatically labeled
with WordNet classes to produce a repository of
human interpretable class-based selectional pref-
erence. This choice might be due to the fact that
if most LDA topic heads are usually reasonable
upon human inspection, some topics are also in-
coherent (Newman 2010) and lower frequency
words are not handled as successfully. We con-
trol the coherence of the topics and rearrange
them into human interpretable clusters using a
distributional similarity measure.
Microsoft Sempute Project (Sempute Team
2013) also explores core technologies and appli-
cations of semantic computing. As part of
Sempute project, NeedleSeek is aimed at auto-
matically extracting data to support general se-
mantic Web searches. While Linggle focuses on
n-gram information for language learning,
NeedleSeek also uses LDA to support question
answering (e.g., What were the Capitals of an-
cient China?) .
In contrast to the previous research in Web
scale linguistic search engines, we present a sys-
tem that supports queries with keywords, wild-
card words, POS, synonyms, and additional
regular expression (RE) operators and displays
the results according the count, similarity, and
topic with clusters of synonyms or conceptually
related words. We exploit and combine the
power of both LDA analysis and distributional
similarity to provide meaningful semantic classes
that are constrained with members of high simi-
larity. Distributional similarity (Lin 1998) and
LDA topics become two angles of attack to view
language usage and corpus patterns.
</bodyText>
<sectionHeader confidence="0.98992" genericHeader="method">
3 Linggle Functionalities
</sectionHeader>
<bodyText confidence="0.99984725">
The syntax of Linggle queries involves basic
regular expression of keywords enriched with
wildcard PoS and synonyms. Linggle queries can
be either pattern-based commands or natural lan-
guage questions. The natural language queries
are currently handled by simple string matching
based on a limited set of questions and command
pairs provided by a native speaker informant.
</bodyText>
<subsectionHeader confidence="0.997117">
3.1 Natural language queries
</subsectionHeader>
<bodyText confidence="0.999990071428571">
The handling of queries formulated in natural
language has been implemented with handcrafted
patterns refined from a corpus of questions found
on various websites. Additionally, we asked both
native and non-native speakers to use the system
for text edition and to write down all the ques-
tions that arise during the exercise.
Linggle transforms a question into commands
for further processing based on a set of canned
texts (e.g., ‚ÄúHow to describe a beach?‚Äù will be
converted to ‚Äú$A beach‚Äù). We are in the process
of gathering more examples of language-related
question and answer pairs from Answers.com to
improve the precision, versatility, and coverage.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2 Syntax of queries
</subsectionHeader>
<bodyText confidence="0.999989756756757">
The syntax of the patterns for n-grams is shown
in Table 1. The syntax supports two types of que-
ry functions: basic keyword search with regular
expression capability and semantic search.
Basic search operators enable the users to que-
ry zero, one or more arbitrary words up to five
words. For example, the query ‚Äúset off √â $N‚Äù is
intended to search for all nouns in the right con-
text of set off, within a maximum distance of
three words.
In addition, the ‚Äú?‚Äù operator in front of a word
represents a search for n-grams with or without
the word. For example, a user wanting to deter-
mine whether to use the word to between listen
and music can formulate the query ‚Äúlisten ?to
music.‚Äù
Yet another operation ‚Äú|‚Äù is provided to search
for information related to word choice. For ex-
ample the query ‚Äúbuild  |construct ... dream‚Äù can
be used to reveal that people build a dream much
more often than they construct a dream.
A set of PoS symbols (shown in Table 2) is
defined to support queries that need more preci-
sion than the symbol *. More work might be
needed to resolve PoS ambiguity for n-grams.
Currently, any word that has been labeled with
the requested PoS in the BNC more than 5% of
the time is displayed.
The ‚Äú+‚Äù operator is provided to support se-
mantic queries. Placed in front of a word, it is
intended to search for synonyms in the context.
For example the query ‚Äú+sandy beach‚Äù would
generate rocky beach, stony beach, barren beach
in the top three results. The query ‚Äú+abandoned
beach‚Äù generates deserted, destroyed and empty
beach at the top of the list. To support conceptual
clustering of collocational n-grams, we need to
</bodyText>
<page confidence="0.993368">
141
</page>
<bodyText confidence="0.999287833333333">
identify synonyms related to different senses of a
given word. Table 3 shows an example of the
result obtained for the ambiguous word bank as a
unigram query. We can see the two main senses
of the word (river bank and institution) as clus-
ters.
</bodyText>
<table confidence="0.998920714285714">
Operators Description
* Any Word
? With/without the word
√â Zero or more words
 |Alternator
$ Part of speech
+ Synonyms
</table>
<tableCaption confidence="0.999498">
Table 1: Operators in the Linggle queries
</tableCaption>
<table confidence="0.999879333333333">
Part of speech Description
N Noun
V Verb
A Adjective
R Adverb
PP Preposition
NP Proper Noun
PR Pronoun
D Determiner
</table>
<tableCaption confidence="0.99987">
Table 2: Part-of-speech in the Linggle queries
</tableCaption>
<bodyText confidence="0.978147470588235">
A cluster button on the interface activates or
cancels conceptual clustering. When Linggle is
switched into a cluster display mode, adjective-
nouns, verb-objects and subject-verb relations
can be browsed based on the induced conceptual
clusters (see Figure 1).
The New York Times Example Base
In order to display complete sentence examples
for users, the New York Times Corpus sentences
are indexed by word. When the user searches for
words in a specific syntactic relation, morpho-
logical query expansion is performed and pat-
terns are used to increase both the coverage and
the precision of the provided examples. For ex-
ample, the bi-gram kill bacteria will be associat-
ed with the example sentence ‚ÄúThe bacteria are
killed by high temperatures.‚Äù.
</bodyText>
<subsectionHeader confidence="0.997682">
3.3 Semantic Clusters
</subsectionHeader>
<bodyText confidence="0.92221552631579">
Two types of semantic clusters are provided in
Linggle: selectional preference and clusters of
synonyms. Selectional preference expresses for
example that an apple is more likely to be eaten
or cooked than to be killed or hanged. Different
classes of arguments for a predicate (or of predi-
cates for an argument) can be found automatical-
ly. The favorite class of objects for the verb drink
is LIQUID with the noun water ranked at the top.
Less frequent objects belonging to the same class
include liquor in the tail of the list. We aim at
grouping arguments and predicates into semantic
clusters for better readability.
valley mountain river lake hill bay plain north ridge
coast city district town area community municipality
country village land region
route highway road railway bridge crossing canal
railroad junction
stream creek tributary
</bodyText>
<tableCaption confidence="0.730925">
organization business institution company industry
organisation agency school department university
government court board
channel network affiliate outlet
supplier manufacturer distributor vendor retailer in-
vestor broker provider lender owner creditor share-
holder customer employer
Table 3: First two level-one clusters of synonyms for
the word ‚Äúbank‚Äù
</tableCaption>
<bodyText confidence="0.9999894">
We produce clusters with a two-layer structure.
Level one represents loose topical relatedness
roughly corresponding to broad domains, while
level two is aimed at grouping together closely
similar words. For example, among the objects
of the verb cultivate, the nouns tie and contact
belong to the same level-two cluster. Attitude and
spirit belong to another level-two cluster but
both pairs are in the same level-one cluster. The
nouns fruit and vegetable are clustered together
in another level-one cluster. This double-layer
representation is a solution to express at once
close synonymy and topic relatedness. The clus-
ters of symonyms displayed in Table 3 follow the
same representation.
</bodyText>
<sectionHeader confidence="0.747402" genericHeader="method">
4 Implementation of the system
</sectionHeader>
<bodyText confidence="0.999955833333333">
In this section, we describe the implementation
of Linggle, including how to index and store n-
grams for a fast access (Section 4.1) and
construction of the LDA models (Section 4.2).
We will describe the clustering method in more
details in section 5.
</bodyText>
<subsectionHeader confidence="0.993556">
4.1 N-grams preprocessing
</subsectionHeader>
<bodyText confidence="0.999925333333333">
The n-grams are first filtered keeping only the
words that are in WordNet and in the British Na-
tional Corpus, and then indexed by word and
position in the n-gram, in a way similar to the
rotated n-gram approach proposed by Lin et. al.
(2010). The files are then stored in an Apache
</bodyText>
<page confidence="0.995668">
142
</page>
<bodyText confidence="0.999852714285714">
HBase NoSQL base. The major advantages of
using a NoSQL database is the excellent perfor-
mance in querying the ability of storing large
amounts of data across several servers and the
capability to scale up when we have additional
entries in the dataset, or additional datasets to
add to the system.
</bodyText>
<subsectionHeader confidence="0.994672">
4.2 LDA models computations
</subsectionHeader>
<bodyText confidence="0.970815076923077">
Two types of LDA models are calculated for
Linggle. The first type is a selectional preference
model between heads and modifiers. Six models
are calculated in total for the subject-verb, the
verb-object and the adjective-noun relations done
in a similar way to Ritter and Etzioni‚Äôs (2010)
model with binary relations instead of triples.
The second is a word/synonyms model in which
a word is considered as a document in LDA and
its synonyms as the words of the document. This
second model has the effect of splitting the syno-
nyms of a word into different topics, as shown in
Table 3.
</bodyText>
<listItem confidence="0.972900166666667">
Seeds parameter: s1
1. Consider the m first topics for a verb v ac-
cording to the LDA per document-topic dis-
tribution (ùúÉ)
2. Consider S = o1,√â,on, a set of n objects of v.
3. Split S into m classes C1,..,Cm according to
</listItem>
<bodyText confidence="0.7987324">
their LDA per topic-word probability: oi is
assigned to the topic in which it has the
highest probability.
4. For each class Ci, move every object oj that
is not similar to any other ok of Ci , according
to a similarity threshold s1 into a new created
class.
Level 2 parameter: s2
While (Argmaxci ,cj Sim( ci , cj ) &gt; s2):
Merge Argmaxci ,cj Sim( ci , cj ) into one
class.
Level 1 parameter: s3
While (Argmaxci ,cj Sim(ci , cj ) &gt; s3):
Group Argmaxci ,cj Sim( ci ,cj ) under the
same level 1 cluster.
</bodyText>
<tableCaption confidence="0.9633705">
Table 4: Clustering Algorithm for the object of a giv-
en verb
</tableCaption>
<bodyText confidence="0.960405111111111">
The hyperparameters alpha, eta, that affect
the sparsity of the document-topic (theta) and the
topic-word (lambda) distributions are both set to
0.5 and the number of topics is set to 300. More
research would be necessary to optimize the val-
ue for the parameters in the perspective of the
clustering algorithm, as quickly discussed in the
next section.
Sim (ci, cj):
</bodyText>
<listItem confidence="0.995928">
1. Build the Cartesian product C = ci √ó cj
2. Get P the set of the similarity between all word pairs
in C
3. Return Sim(ci,cj) the mean of the scores in P
</listItem>
<tableCaption confidence="0.996815">
Table 5: Similarity between two classes ti and tj
</tableCaption>
<sectionHeader confidence="0.816777" genericHeader="method">
5 Clustering algorithm
</sectionHeader>
<bodyText confidence="0.999884377777778">
The clustering algorithm combines topic model-
ing results and a semantic similarity measure.
We use Pantel‚Äôs dependencies repository to
compute LDA models for subject-verbs, verbs-
objects and adjective-nouns relations in both di-
rections. Currently, we also use Pantel‚Äôs similari-
ty measure. It has a reasonable precision partly
because it relies on parser information instead of
bag of words windows. However the coverage of
the available scores is lower than what would be
needed for Linggle. We will address this issue in
the near future by extending it with similarity
scores computed from the n-grams.
We combine the two distributional semantics
approaches in a simple manner inspired by clus-
tering by committee algorithm (CBC). The simi-
larity measure is used to refine the LDA topics
and to generate finer grain clusters. Conversely,
LDA topics can also be seen as the seeds of our
clustering algorithm.
This algorithm intends to constrain the words
that belong to a final cluster more strictly than
LDA does in order to obtain clearly interpretable
clusters. The exact same algorithm is applied to
synonym models, for synonyms of nouns, adjec-
tives and verbs (shown in Table 3).
Table 4 shows the algorithm for constructing
double layer clusters for a set S of objects of a
verb v. The objects are first roughly split into
classes, attributing a single topic to every object
oi. The topic of a word oi is determined accord-
ing to its per topic-word probability. More exper-
iments could be done using the product of the per
document-topic and the per topic-word LDA
probabilities instead, in order to take into account
the specific verb when assigning a topic to the
object. Such a way of assigning topics should
also be more sensitive to the LDA hyperparame-
ters.
At this stage, some classes are incoherent and
that low frequency words that do not appear in
the head of any topic are often misclassified.
Words are rearranged between the classes and
create new classes if necessary using the simi-
larity measure. If any word of a class is not simi-
</bodyText>
<page confidence="0.997114">
143
</page>
<bodyText confidence="0.999938352941177">
lar to any other word in this class (the threshold
is set to s1 = 0.09), a new class is created for it.
Any two classes are then merged if their simi-
larity (computer accordingly to Table 5) is above
s2=0.06, forming the level 2 clusters. Classes are
then grouped together if the similarity between
them is above s3 = 0.02 forming the level 1 clus-
ters.
Finally, the classes that contain less than three
words are not displayed in Linggle and the predi-
cate-arguments counts in the Web 1T are re-
trieved using a few hand crafted RE and morpho-
logical expansion of the nouns and the verbs.
This algorithm appears to generate interpreta-
ble semantic classes and to be quite robust re-
garding the threshold parameters. More tests and
rigorous evaluation are left to future work.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999775">
There are many different directions in which
Linggle will be improved. The first one is to al-
low users to work with word forms and with
multiword expressions. The second one concerns
the extension of the coverage of the example
base with several large corpora such as Wikipe-
dia and the extension of the coverage of the simi-
larity measure. The third direction concerns the
development of automatic suggestions for text
edition, such as suggesting a better adjective or a
different preposition in the context of a sentence.
Finally, Linggle is currently being extended to
Chinese.
We presented a prototype that gives access
to Web Scale collocations. Linggle displays both
word usage and word similarity information.
Depending on the type of the input query, the
results are displayed under the form of lists or
clusters of n-grams. The system is designed to
become a multilingual platform for text edition
and can also become a valuable resource for
natural language processing research.
</bodyText>
<sectionHeader confidence="0.999284" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944390625">
David Blei, A. Ng, and M. Jordan. 2003. Latent Di-
richlet allocation. Journal of Machine Learning
Research, 3:993‚Äì1022, January 2003.
Jason S. Chang, 2008. Linggle: a web-scale language
reference search engine. Unpublished manuscript.
Katrin Erk and Sebastian Pad‚Äî. 2010. A Flexible,
Corpus-Driven Model of Regular and Inverse Se-
lectional Preferences. In Proceedings of ACL 2010.
Christiane Fellbaum. 2010. WordNet. MIT Press,
Cambridge, MA.
John Rupert Firth. 1957. The Semantics of Linguistics
Science. Papers in linguistics 1934-1951. London:
Oxford University Press.
William H Fletcher. 2012. Corpus analysis of the
world wide web.&amp;quot; In The Encyclopedia of Applied
Linguistics.
Adam Kilgarriff , and David Tugwell. 2001. Word
sketch: Extraction and display of significant collo-
cations for lexicography. In Proceedings of COL-
LOCTION: Computational Extraction, Analysis
and Exploitation workshop, 39th ACL and 10th
EACL, pp. 32-38.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics,
volume 2. Association for Computational Linguis-
tics, pp. 768-774.
Dekang Lin, and Patrick Pantel. 2002. Concept Dis-
covery from Text. In Proceedings of Conference on
Computational Linguistics (COLING-02). pp. 577-
583. Taipei, Taiwan.
Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi
Sekine, David Yarowsky, Shane Bergsma, Kailash
Patil, Emily Pitler, Rachel Lathbury, Vikram Rao,
Kapil Dalwani, Sushant Narsale. 2010. New tools
for web-scale n-grams. In Proceedings of LREC.
David Newman, Jey Han Lau, Karl Grieser and Timo-
thy Baldwin (2010). Automatic Evaluation of
Topic Coherence. In Proceedings of Human Lan-
guage Technologies, 11th NAACL HLT, Los Ange-
les, USA, pp. 100‚Äî108.
Evan Sandhaus. 2008. &amp;quot;New york times corpus: Cor-
pus overview.&amp;quot; LDC catalogue LDC2008T19.
Sempute Team. 2013. What is NeedleSeek?
http://needleseek.msra.cn/readme.htm
Benno Stein, Martin Potthast, and Martin Trenkmann.
2010. Retrieving customary Web language to assist
writers. Advances in Information Retrieval.
Springer Berlin Heidelberg, pp. 631-635.
Martin Potthast, Martin Trenkmann, and Benno Stein.
Using Web N-Grams to Help Second-Language
Speakers .2010. SIGIR 10 Web N-Gram Workshop,
pages 49-49.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A
Latent Dirichlet Allocation method for Selectional
Preferences. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (July 2010), pp. 424-434.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence 11(3), pp. 197-223.
Shaoqun Wu, Ian H. Witten and Margaret Franken
(2010). Utilizing lexical data from a web-derived
corpus to expand productive collocation
knowledge. ReCALL, 22(1), 83‚Äì102.
</reference>
<page confidence="0.998596">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.514434">
<title confidence="0.997183">Linggle: a Web-scale Linguistic Search Engine for Words in Context</title>
<author confidence="0.968019">Ting-Hui Jian-Cheng Tzu-His Jason S</author>
<affiliation confidence="0.817524666666667">of Information Systems and Applications of Computer National Tsing Hua</affiliation>
<address confidence="0.997152">HsinChu, Taiwan, R.O.C. 30013</address>
<email confidence="0.9171345">joanne.boisson@gmail.com</email>
<email confidence="0.9171345">maxis1718@gmail.com</email>
<email confidence="0.9171345">wujc86@gmail.com</email>
<email confidence="0.9171345">joseph.yen@gmail.com</email>
<email confidence="0.9171345">jason.jschang@gmail.com</email>
<abstract confidence="0.995555413793103">In this paper, we introduce a Web-scale linsearch engine, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and additional regular expression (RE) operators. In our approach, we incorporate inverted file indexing, PoS information from BNC, and semantic indexing based on Latent Dirichlet Allocation with Google Web 1T. The method involves parsing the query to transforming it into several keyword retrieval commands. Word chunks are retrieved with counts, further filtering the chunks with the query as a RE, and finally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are provided. In addition, sentences from New York Times on demand. The current implementation of the most functionally comprehensive, and is in principle language and dataset We plan to extend provide fast and convenient access to a wealth of linguistic information embodied in Web datasets including Web 1T Books Ngram many major languages in the world.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="8226" citStr="Blei et al. 2003" startWordPosition="1301" endWordPosition="1304">llbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distributional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies extracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to organize the n-grams into semantic classes. More recently, Ritter and Etzioni (2010) propose to apply an LDA model (Blei et al. 2003) to 140 the problem of inducing selectional preference. The idea is to consider the verbs in a corpus as the documents of a traditional LDA model. The arguments of the verb that are encountered in the corpus are treated as the words composing a document in the traditional model. The model seems to successfully infer the semantic classes that correspond to the preferred arguments of a verb. The topics are semi-automatically labeled with WordNet classes to produce a repository of human interpretable class-based selectional preference. This choice might be due to the fact that if most LDA topic h</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993‚Äì1022, January 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Chang</author>
</authors>
<title>Linggle: a web-scale language reference search engine.</title>
<date>2008</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="7109" citStr="Chang (2008)" startWordPosition="1128" endWordPosition="1129">e companies. In 2006, Google released the Web 1T for several major languages of the world (trillion-word n-gram datasets for English, Japanese, Chinese, and ten European languages), to stimulate NLP research in many areas. In 2008, Chang described a prototype that enhances Google Web 1T bigrams with PoS tags and supports search in the dataset by wildcards (wild-PoS), to identify recurring collocations. Wu, Witten and Franken (2010) describe a more comprehensive system (FLAX) that combines filtered Google data with text examples from the BNC for several learning activities. In a way similar to Chang (2008) and Wu, Witten and Franken (2010), Stein, Potthast, and Trenkmann (2010) describe the implementation and application of NetSpeak, a system that provides quick access to the Google Web 1T n-gram with RE-like queries (alternator ‚Äú|‚Äù, one arbitrary word ‚Äú*‚Äù, arbitrary number of words between two specified words ‚Äú√â‚Äù). In contrast to Linggle, NetSpeak does not support PoS wildcard or conceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synon</context>
</contexts>
<marker>Chang, 2008</marker>
<rawString>Jason S. Chang, 2008. Linggle: a web-scale language reference search engine. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad‚Äî</author>
</authors>
<title>A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Erk, Pad‚Äî, 2010</marker>
<rawString>Katrin Erk and Sebastian Pad‚Äî. 2010. A Flexible, Corpus-Driven Model of Regular and Inverse Selectional Preferences. In Proceedings of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>2010</date>
<publisher>WordNet. MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7621" citStr="Fellbaum 2010" startWordPosition="1207" endWordPosition="1208">data with text examples from the BNC for several learning activities. In a way similar to Chang (2008) and Wu, Witten and Franken (2010), Stein, Potthast, and Trenkmann (2010) describe the implementation and application of NetSpeak, a system that provides quick access to the Google Web 1T n-gram with RE-like queries (alternator ‚Äú|‚Äù, one arbitrary word ‚Äú*‚Äù, arbitrary number of words between two specified words ‚Äú√â‚Äù). In contrast to Linggle, NetSpeak does not support PoS wildcard or conceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distributional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies extracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to organize the n-grams into semantic classes. More recently, Ritter and Etzioni (2010) propose to apply an LDA model (Blei et al. </context>
</contexts>
<marker>Fellbaum, 2010</marker>
<rawString>Christiane Fellbaum. 2010. WordNet. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Rupert Firth</author>
</authors>
<title>The Semantics of Linguistics Science. Papers in linguistics 1934-1951. London:</title>
<date>1957</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2262" citStr="Firth, 1957" startWordPosition="351" endWordPosition="352">nters many problems. Doubts concerning the usage of a preposition, the mandatory presence of a determiner, the correctness of the association of a verb with an object, or the need for synonyms of a term in a given context are issues that arise frequently. Printed collocation dictionaries and reference tools based on compiled corpora offer limited coverage of word usage while knowledge of collocations is vital to acquire a good level of linguistic competency. We propose to address these limitations with a comprehensive system aimed at helping the learners ‚Äúknow a word by the company it keeps‚Äù (Firth, 1957). Linggle (linggle.com). The system based on Web-scaled datasets is designed to be a broad coverage language reference tool for English Second Language learners (ESL). It is conceived to search information related to word usage in context under various conditions. First, we build an inverted file index for the Google Web 1T n-grams to support queries with RE-like patterns including PoS and synonym matches. For example, for the query ‚Äú$V $D +important role‚Äù, Linggle retrieves 4-grams that start with a verb and a determiner followed by a synonym of important and the keyword role (e.g., play a si</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John Rupert Firth. 1957. The Semantics of Linguistics Science. Papers in linguistics 1934-1951. London: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Fletcher</author>
</authors>
<title>Corpus analysis of the world wide web.&amp;quot; In The Encyclopedia of Applied Linguistics.</title>
<date>2012</date>
<contexts>
<context position="5861" citStr="Fletcher (2012)" startWordPosition="921" endWordPosition="922">richlet Allocation (LDA) model to seed the clusters. The rest of the paper is organized as follows. We review the related work in the next section. Then we present the syntax of the queries and the functionalities of the system (Section 3). We describe the details of implementation including the indexing of the n-grams and the clustering algorithm (Section 4) and draw perspective of development of Web scale search engines (Section 5). 2 Related work Web-scale Linguistic Search Engine (LSE) has been an area of active research. Recently, the state-of-the-art in LSE research has been reviewed in Fletcher (2012). We present in this paper a linguistic search engine that provides a more comprehensive and powerful set of query features. Kilgarriff et al. (2001) describe the implementation of the linguistic search engine Word Sketch (2001) that displays collocations and dependencies acquired from a large corpus such as the BNC. Word Sketch is not as flexible as typical search engines, only supporting a fixed set of queries. Recently, researchers have been attempting to go one step further and work with Web scale datasets, but it is difficult for an academic institute to crawl a dataset that is on par wit</context>
</contexts>
<marker>Fletcher, 2012</marker>
<rawString>William H Fletcher. 2012. Corpus analysis of the world wide web.&amp;quot; In The Encyclopedia of Applied Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Tugwell</author>
</authors>
<title>Word sketch: Extraction and display of significant collocations for lexicography.</title>
<date>2001</date>
<booktitle>In Proceedings of COLLOCTION: Computational Extraction, Analysis and Exploitation workshop, 39th ACL and 10th EACL,</booktitle>
<pages>32--38</pages>
<marker>Tugwell, 2001</marker>
<rawString>Adam Kilgarriff , and David Tugwell. 2001. Word sketch: Extraction and display of significant collocations for lexicography. In Proceedings of COLLOCTION: Computational Extraction, Analysis and Exploitation workshop, 39th ACL and 10th EACL, pp. 32-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics, volume 2. Association for Computational Linguistics,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="10083" citStr="Lin 1998" startWordPosition="1591" endWordPosition="1592">ering (e.g., What were the Capitals of ancient China?) . In contrast to the previous research in Web scale linguistic search engines, we present a system that supports queries with keywords, wildcard words, POS, synonyms, and additional regular expression (RE) operators and displays the results according the count, similarity, and topic with clusters of synonyms or conceptually related words. We exploit and combine the power of both LDA analysis and distributional similarity to provide meaningful semantic classes that are constrained with members of high similarity. Distributional similarity (Lin 1998) and LDA topics become two angles of attack to view language usage and corpus patterns. 3 Linggle Functionalities The syntax of Linggle queries involves basic regular expression of keywords enriched with wildcard PoS and synonyms. Linggle queries can be either pattern-based commands or natural language questions. The natural language queries are currently handled by simple string matching based on a limited set of questions and command pairs provided by a native speaker informant. 3.1 Natural language queries The handling of queries formulated in natural language has been implemented with hand</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguistics, volume 2. Association for Computational Linguistics, pp. 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Concept Discovery from Text.</title>
<date>2002</date>
<booktitle>In Proceedings of Conference on Computational Linguistics (COLING-02).</booktitle>
<pages>577--583</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7860" citStr="Lin and Pantel (2002)" startWordPosition="1243" endWordPosition="1246"> a system that provides quick access to the Google Web 1T n-gram with RE-like queries (alternator ‚Äú|‚Äù, one arbitrary word ‚Äú*‚Äù, arbitrary number of words between two specified words ‚Äú√â‚Äù). In contrast to Linggle, NetSpeak does not support PoS wildcard or conceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distributional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies extracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to organize the n-grams into semantic classes. More recently, Ritter and Etzioni (2010) propose to apply an LDA model (Blei et al. 2003) to 140 the problem of inducing selectional preference. The idea is to consider the verbs in a corpus as the documents of a traditional LDA model. The arguments of the verb that are encountered in the corpus are treated as the words c</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>Dekang Lin, and Patrick Pantel. 2002. Concept Discovery from Text. In Proceedings of Conference on Computational Linguistics (COLING-02). pp. 577-583. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Ward Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, Sushant Narsale.</institution>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, Sushant Narsale. 2010. New tools for web-scale n-grams. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Jey Han Lau</author>
<author>Karl Grieser</author>
<author>Timothy Baldwin</author>
</authors>
<title>Automatic Evaluation of Topic Coherence.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies, 11th NAACL HLT,</booktitle>
<pages>100--108</pages>
<location>Los Angeles, USA,</location>
<marker>Newman, Lau, Grieser, Baldwin, 2010</marker>
<rawString>David Newman, Jey Han Lau, Karl Grieser and Timothy Baldwin (2010). Automatic Evaluation of Topic Coherence. In Proceedings of Human Language Technologies, 11th NAACL HLT, Los Angeles, USA, pp. 100‚Äî108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>New york times corpus: Corpus overview.&amp;quot; LDC catalogue LDC2008T19.</title>
<date>2008</date>
<contexts>
<context position="3519" citStr="Sandhaus, 2008" startWordPosition="550" endWordPosition="551">e interface is also available for users who are less familiar with pattern-based searches. For example, the question ‚ÄúHow can I describe a beach?‚Äù would retrieve two word chunks such as ‚Äúsandy beach 413,300‚Äù and ‚Äúrocky beach 16,800‚Äù. The n-gram search implementation is achieved through filtering, re-indexing, populating an HBase database with the Web 1T n-grams and augmenting them with the most frequent PoS for words (without disambiguation) derived from the British National Corpus (BNC). The n-grams returned for a query can then be linked to examples extracted from the New York Times Corpus (Sandhaus, 2008) in order to provide full sentential context for more effective learning. In some situations, the user might need to search for words in a specific syntactic relation (e.g., Verb-Object collocation). The query absorb $N in n-grams display mode returns all the nouns that follow the verb ordered by decreasing ngram counts. Some of these nouns might not be objects of the verb absorb. In contrast, the same 139 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 139‚Äì144, Sofia, Bulgaria, August 4-9 2013. cÔøΩ2013 Association for Computational Linguistics que</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. &amp;quot;New york times corpus: Corpus overview.&amp;quot; LDC catalogue LDC2008T19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sempute Team</author>
</authors>
<title>What is NeedleSeek?</title>
<date>2013</date>
<note>http://needleseek.msra.cn/readme.htm</note>
<contexts>
<context position="9162" citStr="Team 2013" startWordPosition="1452" endWordPosition="1453">ntic classes that correspond to the preferred arguments of a verb. The topics are semi-automatically labeled with WordNet classes to produce a repository of human interpretable class-based selectional preference. This choice might be due to the fact that if most LDA topic heads are usually reasonable upon human inspection, some topics are also incoherent (Newman 2010) and lower frequency words are not handled as successfully. We control the coherence of the topics and rearrange them into human interpretable clusters using a distributional similarity measure. Microsoft Sempute Project (Sempute Team 2013) also explores core technologies and applications of semantic computing. As part of Sempute project, NeedleSeek is aimed at automatically extracting data to support general semantic Web searches. While Linggle focuses on n-gram information for language learning, NeedleSeek also uses LDA to support question answering (e.g., What were the Capitals of ancient China?) . In contrast to the previous research in Web scale linguistic search engines, we present a system that supports queries with keywords, wildcard words, POS, synonyms, and additional regular expression (RE) operators and displays the </context>
</contexts>
<marker>Team, 2013</marker>
<rawString>Sempute Team. 2013. What is NeedleSeek? http://needleseek.msra.cn/readme.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benno Stein</author>
<author>Martin Potthast</author>
<author>Martin Trenkmann</author>
</authors>
<title>Retrieving customary Web language to assist writers. Advances in Information Retrieval.</title>
<date>2010</date>
<pages>631--635</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg,</location>
<marker>Stein, Potthast, Trenkmann, 2010</marker>
<rawString>Benno Stein, Martin Potthast, and Martin Trenkmann. 2010. Retrieving customary Web language to assist writers. Advances in Information Retrieval. Springer Berlin Heidelberg, pp. 631-635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Potthast</author>
<author>Martin Trenkmann</author>
<author>Benno Stein</author>
</authors>
<title>Using Web N-Grams to Help Second-Language Speakers</title>
<date>2010</date>
<booktitle>SIGIR 10 Web N-Gram Workshop,</booktitle>
<pages>49--49</pages>
<marker>Potthast, Trenkmann, Stein, 2010</marker>
<rawString>Martin Potthast, Martin Trenkmann, and Benno Stein. Using Web N-Grams to Help Second-Language Speakers .2010. SIGIR 10 Web N-Gram Workshop, pages 49-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A Latent Dirichlet Allocation method for Selectional Preferences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>424--434</pages>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A Latent Dirichlet Allocation method for Selectional Preferences. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (July 2010), pp. 424-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Making preferences more active.</title>
<date>1978</date>
<journal>Artificial Intelligence</journal>
<volume>11</volume>
<issue>3</issue>
<pages>197--223</pages>
<contexts>
<context position="4603" citStr="Wilks (1978)" startWordPosition="724" endWordPosition="725">Computational Linguistics, pages 139‚Äì144, Sofia, Bulgaria, August 4-9 2013. cÔøΩ2013 Association for Computational Linguistics query in cluster display mode will control that two words have been labeled verb-object by a parser. Moreover, n-grams grouped by object topic/domain give the learner an overview of the usage of the verb. For example the verb absorb takes clusters of objects related to the topics liquid, energy, money, knowledge, and population. Figure 1. An example Linggle search for the query ‚Äúabsorb $N.‚Äù This tendency of predicates to prefer certain classes of arguments is defined by Wilks (1978) as selectional preferences and widely reported in the literature. Erk and Pad‚Äî (2010) extend experiments on selectional preference induction to inverse selectional preference, considering the restriction imposed on predicates. Inverse sectional preference is also implemented in linggle (e.g. ‚Äú$V apple‚Äù). Linggle presents clusters of synonymous collocates (adjectives, nouns and verbs) of a query keyword. We obtained the clusters by building on Lin and Pantel‚Äôs (2002) large-scale repository of dependencies and word similarity scores. Using the method proposed by Ritter and Etzioni (2010) we ind</context>
</contexts>
<marker>Wilks, 1978</marker>
<rawString>Yorick Wilks. 1978. Making preferences more active. Artificial Intelligence 11(3), pp. 197-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaoqun Wu</author>
<author>Ian H Witten</author>
<author>Margaret Franken</author>
</authors>
<title>Utilizing lexical data from a web-derived corpus to expand productive collocation knowledge.</title>
<date>2010</date>
<journal>ReCALL,</journal>
<volume>22</volume>
<issue>1</issue>
<pages>83--102</pages>
<marker>Wu, Witten, Franken, 2010</marker>
<rawString>Shaoqun Wu, Ian H. Witten and Margaret Franken (2010). Utilizing lexical data from a web-derived corpus to expand productive collocation knowledge. ReCALL, 22(1), 83‚Äì102.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>