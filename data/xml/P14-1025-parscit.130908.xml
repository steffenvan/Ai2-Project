<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.71999">
Learning Word Sense Distributions, Detecting Unattested Senses and
Identifying Novel Senses Using Topic Models
</title>
<author confidence="0.675584">
Jey Han Lau,♠ Paul Cook,° Diana McCarthy,♦ Spandana Gella,° and Timothy Baldwin°
</author>
<affiliation confidence="0.782374666666667">
♠ Dept of Philosophy, King’s College London
♥ Dept of Computing and Information Systems, The University of Melbourne
♦ University of Cambridge
</affiliation>
<email confidence="0.9577345">
jeyhan.lau@gmail.com, paulcook@unimelb.edu.au,
diana@dianamccarthy.co.uk, spandanagella@gmail.com, tb@ldwin.net
</email>
<sectionHeader confidence="0.994754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999823636363636">
Unsupervised word sense disambiguation
(WSD) methods are an attractive approach
to all-words WSD due to their non-reliance
on expensive annotated data. Unsuper-
vised estimates of sense frequency have
been shown to be very useful for WSD due
to the skewed nature of word sense distri-
butions. This paper presents a fully unsu-
pervised topic modelling-based approach
to sense frequency estimation, which is
highly portable to different corpora and
sense inventories, in being applicable to
any part of speech, and not requiring a hi-
erarchical sense inventory, parsing or par-
allel text. We demonstrate the effective-
ness of the method over the tasks of pre-
dominant sense learning and sense distri-
bution acquisition, and also the novel tasks
of detecting senses which aren’t attested
in the corpus, and identifying novel senses
in the corpus which aren’t captured in the
sense inventory.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975352941176">
The automatic determination of word sense infor-
mation has been a long-term pursuit of the NLP
community (Agirre and Edmonds, 2006; Navigli,
2009). Word sense distributions tend to be Zip-
fian, and as such, a simple but surprisingly high-
accuracy back-off heuristic for word sense dis-
ambiguation (WSD) is to tag each instance of a
given word with its predominant sense (McCarthy
et al., 2007). Such an approach requires knowl-
edge of predominant senses; however, word sense
distributions — and predominant senses too —
vary from corpus to corpus. Therefore, meth-
ods for automatically learning predominant senses
and sense distributions for specific corpora are re-
quired (Koeling et al., 2005; Lapata and Brew,
2004).
In this paper, we propose a method which uses
topic models to estimate word sense distributions.
This method is in principle applicable to all parts
of speech, and moreover does not require a parser,
a hierarchical sense representation or parallel text.
Topic models have been used for WSD in a num-
ber of studies (Boyd-Graber et al., 2007; Li et
al., 2010; Lau et al., 2012; Preiss and Stevenson,
2013; Cai et al., 2007; Knopp et al., 2013), but
our work extends significantly on this earlier work
in focusing on the acquisition of prior word sense
distributions (and predominant senses).
Because of domain differences and the skewed
nature of word sense distributions, it is often the
case that some senses in a sense inventory will
not be attested in a given corpus. A system ca-
pable of automatically finding such senses could
reduce ambiguity, particularly in domain adapta-
tion settings, while retaining rare but nevertheless
viable senses. We further propose a method for ap-
plying our sense distribution acquisition system to
the task of finding unattested senses — i.e., senses
that are in the sense inventory but not attested in
a given corpus. In contrast to the previous work
of McCarthy et al. (2004a) on this topic which
uses the sense ranking score from McCarthy et
al. (2004b) to remove low-frequency senses from
WordNet, we focus on finding senses that are unat-
tested in the corpus on the premise that, given ac-
curate disambiguation, rare senses in a corpus con-
tribute to correct interpretation.
Corpus instances of a word can also correspond
to senses that are not present in a given sense in-
ventory. This can be due to, for example, words
taking on new meanings over time (e.g. the rela-
</bodyText>
<page confidence="0.975995">
259
</page>
<note confidence="0.830635">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999797388888889">
tively recent senses of tablet and swipe related to
touchscreen computers) or domain-specific terms
not being included in a more general-purpose
sense inventory. A system for automatically iden-
tifying such novel senses — i.e. senses that are
attested in the corpus but not in the sense inven-
tory — would be a very valuable lexicographi-
cal tool for keeping sense inventories up-to-date
(Cook et al., 2013). We further propose an appli-
cation of our proposed method to the identification
of such novel senses. In contrast to McCarthy et al.
(2004b), the use of topic models makes this possi-
ble, using topics as a proxy for sense (Brody and
Lapata, 2009; Yao and Durme, 2011; Lau et al.,
2012). Earlier work on identifying novel senses
focused on individual tokens (Erk, 2006), whereas
our approach goes further in identifying groups of
tokens exhibiting the same novel sense.
</bodyText>
<sectionHeader confidence="0.928827" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999955512195122">
There has been a considerable amount of research
on representing word senses and disambiguating
usages of words in context (WSD) as, in order
to produce computational systems that understand
and produce natural language, it is essential to
have a means of representing and disambiguat-
ing word sense. WSD algorithms require word
sense information to disambiguate token instances
of a given ambiguous word, e.g. in the form of
sense definitions (Lesk, 1986), semantic relation-
ships (Navigli and Velardi, 2005) or annotated
data (Zhong and Ng, 2010). One extremely use-
ful piece of information is the word sense prior
or expected word sense frequency distribution.
This is important because word sense distributions
are typically skewed (Kilgarriff, 2004), and sys-
tems do far better when they take bias into ac-
count (Agirre and Martinez, 2004).
Typically, word frequency distributions are esti-
mated with respect to a sense-tagged corpus such
as SemCor (Miller et al., 1993), a 220,000 word
corpus tagged with WordNet (Fellbaum, 1998)
senses. Due to the expense of hand tagging, and
sense distributions being sensitive to domain and
genre, there has been some work on trying to
estimate sense frequency information automati-
cally (McCarthy et al., 2004b; Chan and Ng, 2005;
Mohammad and Hirst, 2006; Chan and Ng, 2006).
Much of this work has been focused on ranking
word senses to find the predominant sense in a
given corpus (McCarthy et al., 2004b; Mohammad
and Hirst, 2006), which is a very powerful heuris-
tic approach to WSD. Most WSD systems rely upon
this heuristic for back-off in the absence of strong
contextual evidence (McCarthy et al., 2007). Mc-
Carthy et al. (2004b) proposed a method which
relies on distributionally similar words (nearest
neighbours) associated with the target word in
an automatically acquired thesaurus (Lin, 1998).
The distributional similarity scores of the nearest
neighbours are associated with the respective tar-
get word senses using a WordNet similarity mea-
sure, such as those proposed by Jiang and Conrath
(1997) and Banerjee and Pedersen (2002). The
word senses are ranked based on these similar-
ity scores, and the most frequent sense is selected
for the corpus that the distributional similarity the-
saurus was trained over.
As well as sense ranking for predominant sense
acquisition, automatic estimates of sense fre-
quency distribution can be very useful for WSD
for training data sampling purposes (Agirre and
Martinez, 2004), entropy estimation (Jin et al.,
2009), and prior probability estimates, all of which
can be integrated within a WSD system (Chan and
Ng, 2005; Chan and Ng, 2006; Lapata and Brew,
2004). Various approaches have been adopted,
such as normalizing sense ranking scores to ob-
tain a probability distribution (Jin et al., 2009), us-
ing subcategorisation information as an indication
of verb sense (Lapata and Brew, 2004) or alter-
natively using parallel text (Chan and Ng, 2005;
Chan and Ng, 2006; Agirre and Martinez, 2004).
The work of Boyd-Graber and Blei (2007) is
highly related in that it extends the method of Mc-
Carthy et al. (2004b) to provide a generative model
which assumes the words in a given document are
generated according to the topic distribution ap-
propriate for that document. They then predict the
most likely sense for each word in the document
based on the topic distribution and the words in
context (“corroborators”), each of which, in turn,
depends on the document’s topic distribution. Us-
ing this approach, they get comparable results to
McCarthy et al. when context is ignored (i.e. us-
ing a model with one topic), and at most a 1% im-
provement on SemCor when they use more topics
in order to take context into account. Since the
results do not improve on McCarthy et al. as re-
gards sense distribution acquisition irrespective of
context, we will compare our model with that pro-
posed by McCarthy et al.
</bodyText>
<page confidence="0.988847">
260
</page>
<bodyText confidence="0.999975266666667">
Recent work on finding novel senses has tended
to focus on comparing diachronic corpora (Sagi
et al., 2009; Cook and Stevenson, 2010; Gulor-
dava and Baroni, 2011) and has also considered
topic models (Lau et al., 2012). In a similar vein,
Peirsman et al. (2010) considered the identifica-
tion of words having a sense particular to one
language variety with respect to another (specif-
ically Belgian and Netherlandic Dutch). In con-
trast to these studies, we propose a model for com-
paring a corpus with a sense inventory. Carpuat
et al. (2013) exploit parallel corpora to identify
words in domain-specific monolingual corpora
with previously-unseen translations; the method
we propose does not require parallel data.
</bodyText>
<sectionHeader confidence="0.998684" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.995122709677419">
Our methodology is based on the WSI system
described in Lau et al. (2012),1 which has been
shown (Lau et al., 2012; Lau et al., 2013a; Lau et
al., 2013b) to achieve state-of-the-art results over
the WSI tasks from SemEval-2007 (Agirre and
Soroa, 2007), SemEval-2010 (Manandhar et al.,
2010) and SemEval-2013 (Navigli and Vannella,
2013; Jurgens and Klapaftis, 2013). The system
is built around a Hierarchical Dirichlet Process
(HDP: Teh et al. (2006)), a non-parametric variant
of a Latent Dirichlet Allocation topic model (Blei
et al., 2003) where the model automatically opti-
mises the number of topics in a fully-unsupervised
fashion over the training data.
To learn the senses of a target lemma, we train
a single topic model per target lemma. The sys-
tem reads in a collection of usages of that lemma,
and automatically induces topics (= senses) in the
form of a multinomial distribution over words, and
per-usage topic assignments (= probabilistic sense
assignments) in the form of a multinomial distri-
bution over topics. Following Lau et al. (2012),
we assign one topic to each usage by selecting the
topic that has the highest cumulative probability
density, based on the topic allocations of all words
in the context window for that usage.2 Note that in
their original work, Lau et al. (2012) experimented
with the use of features extracted from a depen-
dency parser. Due to the computational overhead
associated with these features, and the fact that the
empirical impact of the features was found to be
</bodyText>
<footnote confidence="0.93789425">
1Based on the implementation available at: https://
github.com/jhlau/hdp-wsi
2This includes all words in the usage sentence except
stopwords, which were filtered in the preprocessing step.
</footnote>
<bodyText confidence="0.9948015">
marginal, we make no use of parser-based features
in this paper.3
The induced topics take the form of word multi-
nomials, and are often represented by the top-N
words in descending order of conditional probabil-
ity. We interpret each topic as a sense of the target
lemma.4 To illustrate this, we give the example of
topics induced by the HDP model for network in
Table 1.
We refer to this method as HDP-WSI hence-
forth.5
In predominant sense acquisition, the task is to
learn, for each target lemma, the most frequently
occurring word sense in a particular domain or
corpus, relative to a predefined sense inventory.
The WSI system provides us with a topic alloca-
tion per usage of a given word, from which we can
derive a distribution of topics over usages and a
predominant topic. In order to map this onto the
predominant sense, we need to have some way of
aligning a topic with a sense. We design our topic–
sense alignment methodology with portability in
mind — it should be applicable to any sense in-
ventory. As such, our alignment methodology as-
sumes only that we have access to a conventional
sense gloss or definition for each sense, and does
not rely on ontological/structural knowledge (e.g.
the WordNet hierarchy).
To compute the similarity between a sense
and a topic, we first convert the words in the
gloss/definition into a multinomial distribution
over words, based on simple maximum likeli-
hood estimation.6 We then calculate the Jensen–
Shannon divergence between the multinomial dis-
tribution (over words) of the gloss and that of the
topic, and convert the divergence value into a sim-
ilarity score by subtracting it from 1. Formally, the
similarity sense si and topic tj is:
</bodyText>
<equation confidence="0.975729">
sim(si, tj) = 1 − JS(SIIT) (1)
</equation>
<bodyText confidence="0.992334">
where S and T are the multinomial distributions
</bodyText>
<footnote confidence="0.999304153846154">
3For hyper-parameters α and γ, we used 0.1 for both. We
did not tune the parameters, and opted to use the default pa-
rameters introduced in Teh et al. (2006).
4To avoid confusion, we will refer to the HDP-induced
topics as topics, and reserve the term sense to denote senses
in a sense inventory.
5The code used to learn predominant sense and run all
experiments described in this paper is available at: https:
//github.com/jhlau/predom_sense.
6Words are tokenised using OpenNLP and lemmatised
with Morpha (Minnen et al., 2001). We additionally remove
the target lemma, stopwords and words that are less than 3
characters in length.
</footnote>
<page confidence="0.997215">
261
</page>
<figure confidence="0.9817151">
Topic Num Top-10 Terms
1 network support @card@ information research service group development community member
2 service @card@ road company transport rail area government network public
3 network social model system family structure analysis form relationship neural
4 network @card@ computer system service user access internet datum server
5 system network management software support corp company service application product
6 @card@ radio news television show bbc programme call think film
7 police drug criminal terrorist intelligence network vodafone iraq attack cell
8 network atm manager performance craigavon group conference working modelling assistant
9 root panos comenius etd unipalm lse brazil telephone xxx discuss
</figure>
<tableCaption confidence="0.87699">
Table 1: An example to illustrate the topics induced for network by the HDP model. The top-10 highest
probability terms are displayed to represent each topic (@card@ denotes a tokenised cardinal number).
</tableCaption>
<bodyText confidence="0.99938325">
over words for sense si and topic tj, respectively,
and JS(XIIY) is the Jensen–Shannon divergence
for distribution X and Y .
To learn the predominant sense, we compute the
prevalence score of each sense and take the sense
with the highest prevalence score as the predom-
inant sense. The prevalence score for a sense is
computed by summing the product of its similar-
ity scores with each topic (i.e. sim(si, tj)) and the
prior probability of the topic in question (based
on maximum likelihood estimation). Formally, the
prevalence score of sense si is given as follows:
</bodyText>
<equation confidence="0.9863395">
(sim(si,tj) X P(tj)) (2)
�
sim(si,tj) X T(tj)
Ek f(tk)
</equation>
<bodyText confidence="0.999971571428571">
where f(tj) is the frequency of topic tj (i.e. the
number of usages assigned to topic tj), and T is
the number of topics.
The intuition behind the approach is that the
predominant sense should be the sense that has rel-
atively high similarity (in terms of lexical overlap)
with high-probability topic(s).
</bodyText>
<sectionHeader confidence="0.984047" genericHeader="method">
4 WordNet Experiments
</sectionHeader>
<bodyText confidence="0.99997803030303">
We first test the proposed method over the tasks
of predominant sense learning and sense distribu-
tion induction, using the WordNet-tagged dataset
of Koeling et al. (2005), which is made up of
3 collections of documents: a domain-neutral
corpus (BNC), and two domain-specific corpora
(SPORTS and FINANCE). For each domain,
annotators were asked to sense-annotate a ran-
dom selection of sentences for each of 40 target
nouns, based on WordNet v1.7. The predominant
sense and distribution across senses for each target
lemma was obtained by aggregating over the sense
annotations. The authors evaluated their method in
terms of WSD accuracy over a given corpus, based
on assigning all instances of a target word with the
predominant sense learned from that corpus. For
the remainder of the paper, we denote their system
as MKWC.
To compare our system (HDP-WSI) with
MKWC, we apply it to the three datasets of Koel-
ing et al. (2005). For each dataset, we use HDP
to induce topics for each target lemma, compute
the similarity between the topics and the WordNet
senses (Equation (1)), and rank the senses based
on the prevalence scores (Equation (2)). In addi-
tion to the WSD accuracy based on the predomi-
nant sense inferred from a particular corpus, we
additionally compute: (1) AccUB, the upper bound
for the first sense-based WSD accuracy (using the
gold standard predominant sense for disambigua-
tion);7 and (2) ERR, the error rate reduction be-
tween the accuracy for a given system (Acc) and
the upper bound (AccUB), calculated as follows:
</bodyText>
<equation confidence="0.7619635">
ERR = 1 AccUB − Acc
AccUB
</equation>
<bodyText confidence="0.9837966">
Looking at the results in Table 2, we see lit-
tle difference in the results for the two methods,
with MKWC performing better over two of the
datasets (BNC and SPORTS) and HDP-WSI per-
forming better over the third (FINANCE), but all
differences are small. Based on the McNemar’s
Test with Yates correction for continuity, MKWC
is significantly better over BNC and HDP-WSI is
significantly better over FINANCE (p &lt; 0.0001
in both cases), but the difference over SPORTS
is not statistically significance (p &gt; 0.1). Note
that there is still much room for improvement with
7The upper bound for a WSD approach which tags all to-
ken occurrences of a given word with the same sense, as a
first step towards context-sensitive unsupervised WSD.
</bodyText>
<equation confidence="0.9300872">
T
prevalence(si) =
j
T
j
</equation>
<page confidence="0.990945">
262
</page>
<table confidence="0.9992396">
Dataset FSCORPUS MKWC HDP-WSI
AccUB Acc ERR Acc ERR
BNC 0.524 0.407 (0.777) 0.376 (0.718)
FINANCE 0.801 0.499 (0.623) 0.555 (0.693)
SPORTS 0.774 0.437 (0.565) 0.422 (0.545)
</table>
<tableCaption confidence="0.969767">
Table 2: WSD accuracy for MKWC and HDP-WSI
</tableCaption>
<bodyText confidence="0.7828462">
on the WordNet-annotated datasets, as compared
to the upper-bound based on actual first sense in
the corpus (higher values indicate better perfor-
mance; the best system in each row [other than the
FSCORPUS upper bound] is indicated in boldface).
</bodyText>
<table confidence="0.7367595">
Dataset MKWC HDP-WSI
BNC 0.226 0.214
FINANCE 0.426 0.375
SPORTS 0.420 0.363
</table>
<tableCaption confidence="0.972524">
Table 3: Sense distribution evaluation of MKWC
</tableCaption>
<bodyText confidence="0.998643666666667">
and HDP-WSI on the WordNet-annotated datasets,
evaluated using JS divergence (lower values indi-
cate better performance; the best system in each
row is indicated in boldface).
both systems, as we see in the gap between the up-
per bound (based on perfect determination of the
first sense) and the respective system accuracies.
Given that both systems compute a continuous-
valued prevalence score for each sense of a tar-
get lemma, a distribution of senses can be ob-
tained by normalising the prevalence scores across
all senses. The predominant sense learning task
of McCarthy et al. (2007) evaluates the ability of
a method to identify only the head of this dis-
tribution, but it is also important to evaluate the
full sense distribution (Jin et al., 2009). To this
end, we introduce a second evaluation metric:
the Jensen–Shannon (JS) divergence between the
inferred sense distribution and the gold-standard
sense distribution, noting that smaller values are
better in this case, and that it is now theoretically
possible to obtain a JS divergence of 0 in the case
of a perfect estimate of the sense distribution. Re-
sults are presented in Table 3.
HDP-WSI consistently achieves lower JS diver-
gence, indicating that the distribution of senses
that it finds is closer to the gold standard distri-
bution. Testing for statistical significance over the
paired JS divergence values for each lemma using
the Wilcoxon signed-rank test, the result for FI-
NANCE is significant (p &lt; 0.05) but the results
for the other two datasets are not (p &gt; 0.1 in each
case).
</bodyText>
<table confidence="0.95322675">
Dataset FSCORPUS FSDICT HDP-WSI
AccUB Acc ERR Acc ERR
UKWAC 0.574 0.387 (0.674) 0.514 (0.895)
TWITTER 0.468 0.297 (0.635) 0.335 (0.716)
</table>
<tableCaption confidence="0.6305435">
Table 4: WSD accuracy for HDP-WSI on the
Macmillan-annotated datasets, as compared to the
upper-bound based on actual first sense in the cor-
pus (higher values indicate better performance; the
best system in each row [other than the FSCORPUS
upper bound] is indicated in boldface).
</tableCaption>
<table confidence="0.997748">
Dataset FSCORPUS FSDICT HDP-WSI
UKWAC 0.210 0.393 0.156
TWITTER 0.259 0.472 0.171
</table>
<tableCaption confidence="0.992227">
Table 5: Sense distribution evaluation of HDP-
</tableCaption>
<bodyText confidence="0.912366757575758">
WSI on the Macmillan-annotated datasets as com-
pared to corpus- and dictionary-based first sense
methods, evaluated using JS divergence (lower
values indicate better performance; the best sys-
tem in each row is indicated in boldface).
To summarise, the results for MKWC and HDP-
WSI are fairly even for predominant sense learn-
ing (each outperforms the other at a level of statis-
tical significance over one dataset), but HDP-WSI
is better at inducing the overall sense distribution.
It is important to bear in mind that MKWC in
these experiments makes use of full-text parsing in
calculating the distributional similarity thesaurus,
and the WordNet graph structure in calculating the
similarity between associated words and different
senses. Our method, on the other hand, uses no
parsing, and only the synset definitions (and not
the graph structure) of WordNet.8 The non-reliance
on parsing is significant in terms of portability to
text sources which are less amenable to parsing
(such as Twitter: (Baldwin et al., 2013)), and the
non-reliance on the graph structure of WordNet is
significant in terms of portability to conventional
“flat” sense inventories. While comparable results
on a different dataset have been achieved with a
proximity thesaurus (McCarthy et al., 2007) com-
pared to a dependency one,9 it is not stated how
8McCarthy et al. (2004b) obtained good results with def-
inition overlap, but their implementation uses the relation
structure alongside the definitions (Banerjee and Pedersen,
2002). Iida et al. (2008) demonstrate that further exten-
sions using distributional data are required when applying the
method to resources without hierarchical relations.
</bodyText>
<footnote confidence="0.993281">
9The thesauri used in the reimplementation of MKWC
in this paper were obtained from http://webdocs.cs.
ualberta.ca/˜lindek/downloads.htm.
</footnote>
<page confidence="0.997251">
263
</page>
<bodyText confidence="0.995040571428571">
wide a window is needed for the proximity the-
saurus. This could be a significant issue with Twit-
ter data, where context tends to be limited. In the
next section, we demonstrate the robustness of the
method in experimenting with two new datasets,
based on Twitter and a web corpus, and the Macmil-
lan English Dictionary.
</bodyText>
<sectionHeader confidence="0.989429" genericHeader="method">
5 Macmillan Experiments
</sectionHeader>
<bodyText confidence="0.999957641025641">
In our second set of experiments, we move to a
new dataset (Gella et al., to appear) based on text
from ukWaC (Ferraresi et al., 2008) and Twit-
ter, and annotated using the Macmillan English Dic-
tionary10 (henceforth “Macmillan”). For the pur-
poses of this research, the choice of Macmillan is
significant in that it is a conventional dictionary
with sense definitions and examples, but no link-
ing between senses.11 In terms of the original re-
search which gave rise to the sense-tagged dataset,
Macmillan was chosen over WordNet for reasons in-
cluding: (1) the well-documented difficulties of
sense tagging with fine-grained WordNet senses
(Palmer et al., 2004; Navigli et al., 2007); (2) the
regular update cycle of Macmillan (meaning it con-
tains many recently-emerged senses); and (3) the
finding in a preliminary sense-tagging task that it
better captured Twitter usages than WordNet (and
also OntoNotes: Hovy et al. (2006)).
The dataset is made up of 20 target nouns which
were selected to span the high- to mid-frequency
range in both Twitter and the ukWaC corpus, and
have at least 3 Macmillan senses. The average sense
ambiguity of the 20 target nouns in Macmillan is 5.6
(but 12.3 in WordNet). 100 usages of each target
noun were sampled from each of Twitter (from a
crawl over the time period Jan 3–Feb 28, 2013 us-
ing the Twitter Streaming API) and ukWaC, after
language identification using langid.py (Lui
and Baldwin, 2012) and POS tagging (based on
the CMU ARK Twitter POS tagger v2.0 (Owoputi
et al., 2012) for Twitter, and the POS tags provided
with the corpus for ukWaC). Amazon Mechani-
cal Turk (AMT) was then used to 5-way sense-tag
each usage relative to Macmillan, including allow-
ing the annotators the option to label a usage as
“Other” in instances where the usage was not cap-
tured by any of the Macmillan senses. After qual-
ity control over the annotators/annotations (see
</bodyText>
<footnote confidence="0.621409">
10http://www.macmillandictionary.com/
</footnote>
<bodyText confidence="0.9499312">
11Strictly speaking, there is limited linking in the form of
sets of synonyms in Macmillan, but we choose to not use this
information in our research.
Gella et al. (to appear) for details), and aggregation
of the annotations into a single sense per usage
(possibly “Other”), there were 2000 sense-tagged
ukWaC sentences and Twitter messages over the
20 target nouns. We refer to these two datasets as
UKWAC and TWITTER henceforth.
To apply our method to the two datasets, we use
HDP-WSI to train a model for each target noun,
based on the combined set of usages of that lemma
in each of the two background corpora, namely the
original Twitter crawl that gave rise to the TWIT-
TER dataset, and all of ukWaC.
</bodyText>
<subsectionHeader confidence="0.998565">
5.1 Learning Sense Distributions
</subsectionHeader>
<bodyText confidence="0.994695621621622">
As in Section 4, we evaluate in terms of WSD
accuracy (Table 4) and JS divergence over the
gold-standard sense distribution (Table 5). We
also present the results for: (a) a supervised base-
line (“FSCORPUS”), based on the most frequent
sense in the corpus; and (b) an unsupervised base-
line (“FSDICT”), based on the first-listed sense in
Macmillan. In each case, the sense distribution is
based on allocating all probability mass for a given
word to the single sense identified by the respec-
tive method.
We first notice that, despite the coarser-grained
senses of Macmillan as compared to WordNet, the
upper bound WSD accuracy using Macmillan is
comparable to that of the WordNet-based datasets
over the balanced BNC, and quite a bit lower than
that of the two domain corpora of Koeling et al.
(2005). This suggests that both datasets are di-
verse in domain and content.
In terms of WSD accuracy, the results over
UKWAC (ERR = 0.895) are substantially higher
than those for BNC, while those over TWITTER
(ERR = 0.716) are comparable. The accuracy is
significantly higher than the dictionary-based first
sense baseline (FSDICT) over both datasets (McNe-
mar’s test; p &lt; 0.0001), and the ERR is also con-
siderably higher than for the two domain datasets
in Section 4 (FINANCE and SPORTS). One
cause of difficulty in sense-modelling TWITTER
is large numbers of missing senses, with 12.3%
of usages in TWITTER and 6.6% in UKWAC hav-
ing no corresponding Macmillan sense.12 This chal-
lenges the assumption built into the sense preva-
lence calculation that all topics will align to a pre-
existing sense, a point we return to in Section 5.2.
12The relative occurrence of unlisted/unclear senses in the
datasets of Koeling et al. (2005) is comparable to UKWAC.
</bodyText>
<page confidence="0.981523">
264
</page>
<table confidence="0.999890333333333">
Dataset P R F
UKWAC 0.73 0.85 0.74
TWITTER 0.56 0.88 0.65
</table>
<tableCaption confidence="0.729841">
Table 6: Evaluation of our method for identify-
ing unattested senses, averaged over 10 runs of 10-
fold cross validation
</tableCaption>
<bodyText confidence="0.999634222222222">
The JS divergence results for both datasets are
well below (= better than) the results for all three
WordNet-based datasets, and also superior to both
the supervised and unsupervised first-sense base-
lines. Part of the reason for this improvement is
simply that the average polysemy in Macmillan (5.6
senses per target lemma) is slightly less than in
WordNet (6.7 senses per target lemma),13 making
the task slightly easier in the Macmillan case.
</bodyText>
<subsectionHeader confidence="0.998956">
5.2 Identification of Unattested Senses
</subsectionHeader>
<bodyText confidence="0.999779947368421">
We observed in Section 5.1 that there are rela-
tively frequent occurrences of usages (e.g. 12.3%
for TWITTER) which aren’t captured by Macmil-
lan. Conversely, there are also senses in Macmillan
which aren’t attested in the annotated sample of
usages. Specifically, of the 112 senses defined for
the 20 target lemmas, 25 (= 22.3%) of the senses
are not attested in the 2000 usages in either cor-
pora. Given that our methodology computes a
prevalence score for each sense, it can equally be
applied to the detection of these unattested senses,
and it is this task that we address in this section:
the identification of senses that are defined in the
sense inventory but not attested in a given corpus.
Intuitively, an unused sense should have low
similarity with the HDP induced topics. As such,
we introduce sense-to-topic affinity, a measure
that estimates how likely a sense is not attested in
the corpus:
</bodyText>
<equation confidence="0.9821855">
st-affinity(si) = S Tj sim(si,tj) (3)
Ek El sim(sk, tl)
</equation>
<bodyText confidence="0.999713583333333">
where sim(si, tj) is carried over from Equa-
tion (1), and T and S represent the number of top-
ics and senses, respectively.
We treat the task of identification of unused
senses as a binary classification problem, where
the goal is to find a sense-to-topic affinity thresh-
old below which a sense will be considered to
13Note that the set of lemmas differs between the respec-
tive datasets, so this isn’t an accurate reflection of the relative
granularity of the two dictionaries.
be unused. We pool together all the senses and
run 10-fold cross validation to learn the threshold
for identifying unused senses,14 evaluated using
sense-level precision (P), recall (R) and F-score
(F) at detecting unattested senses. We repeat the
experiment 10 times (partitioning the items ran-
domly into folds) and collect the mean precision,
recall and F-scores across the 10 runs. We found
encouraging results for the task, as detailed in Ta-
ble 6. For the threshold, the average value with
standard deviation is 0.092 ± 0.044 over UKWAC
and 0.125±0.052 over TWITTER, indicating rela-
tive stability in the value of the threshold both in-
ternally within a dataset, and also across datasets.
</bodyText>
<subsectionHeader confidence="0.998282">
5.3 Identification of Novel Senses
</subsectionHeader>
<bodyText confidence="0.999980454545454">
In both TWITTER and UKWAC, we observed fre-
quent occurrences of usages of our target nouns
which didn’t map onto a pre-existing Macmillan
sense. A natural question to ask is whether our
method can be used to predict word senses that are
missing from our sense inventory, and identify us-
ages associated with each such missing sense. We
will term these “novel senses”, and define “novel
sense identification” to be the task of identifying
new senses that are not recorded in the inventory
but are seen in the corpus.
An immediate complication in evaluating novel
sense identification is that we are attempting to
identify senses which explicitly aren’t in our sense
inventory. This contrasts with the identification of
unattested senses, e.g., where we were attempting
to identify which of the known senses wasn’t ob-
served in the corpus. Also, while we have annota-
tions of “Other” usages in TWITTER and UKWAC,
there is no real expectation that all such usages
will correspond to the same sense: in practice,
they are attributable to a myriad of effects such as
incorporation in a non-compositional multiword
expression, and errors in POS tagging (i.e. the us-
age not being nominal). As such, we can’t use the
“Other” annotations to evaluate novel sense iden-
tification. The evaluation of systems for this task
is a known challenge, which we address similarly
to Erk (2006) by artificially synthesising novel
senses through removal of senses from the sense
inventory. In this way, even if we remove multi-
ple senses for a given word, we still have access
to information about which usages correspond to
</bodyText>
<footnote confidence="0.9497385">
14We used a fixed step and increment at steps of 0.001, up
to the max value of st-affinity when optimising the threshold.
</footnote>
<page confidence="0.997067">
265
</page>
<figure confidence="0.886763">
No. Lemmas with Relative Freq Threshold P R F
a Removed Sense of Removed Sense Mean±stdev
20 0.0–0.2 0.052±0.009 0.35 0.42 0.36
9 0.2–0.4 0.089±0.024 0.24 0.59 0.29
6 0.4–0.6 0.061±0.004 0.63 0.64 0.63
</figure>
<tableCaption confidence="0.849066">
Table 7: Classification of usages with novel sense for all target lemmas.
</tableCaption>
<figure confidence="0.7684945">
No. Lemmas with Relative Freq Threshold P R F
a Removed Sense of Removed Sense Mean±stdev
9 0.2–0.4 0.093±0.023 0.50 0.66 0.52
6 0.4–0.6 0.099±0.018 0.73 0.90 0.80
</figure>
<tableCaption confidence="0.948456">
Table 8: Classification of usages with novel sense for target lemmas with a removed sense.
</tableCaption>
<bodyText confidence="0.996943128205128">
which novel sense. An additional advantage of
this procedure is that it allows us to control an im-
portant property of novel senses: their frequency
of occurrence.
In the experiments that follow, we randomly
select senses for removal from three frequency
bands: low, medium and high frequency senses.
Frequency is defined by relative occurrence in the
annotated usages: low = 0.0–0.2; medium = 0.2–
0.4; and high = 0.4–0.6. Note that we do not con-
sider high-frequency senses with frequency higher
than 0.6, as it is rare for a medium- to high-
frequency word to take on a novel sense which
is then the predominant sense in a given corpus.
Note also that not all target lemmas will have a
novel sense through synthesis, as they may have
no senses that fall within the indicated bounds of
relative occurrence (e.g. if &gt; 60% of usages are a
single sense). For example, only 6 of our 20 target
nouns have senses which are candidates for high-
frequency novel senses.
As before, we treat the novel sense identifica-
tion task as a classification problem, although with
a significantly different formulation: we are no
longer attempting to identify pre-existing senses,
as novel senses are by definition not included in
the sense inventory. Instead, we are seeking to
identify clusters of usages which are instances of
a novel sense, e.g. for presentation to a lexicogra-
pher as part of a dictionary update process (Run-
dell and Kilgarriff, 2011; Cook et al., 2013). That
is, for each usage, we want to classify whether it
is an instance of a given novel sense.
A usage that corresponds to a novel sense
should have a topic that does not align well with
any of the pre-existing senses in the sense inven-
tory. Based on this intuition, we introduce topic-
to-sense affinity to estimate the similarity of a
topic to the set of senses, as follows:
</bodyText>
<equation confidence="0.9841025">
ESZ sim(sZ, tj)
ts-affinity(tj) =
ET
l Ek sim(sk, tl) (4)
</equation>
<bodyText confidence="0.999316176470588">
where, once again, sim(sZ, tj) is defined as in
Equation (1), and T and S represent the number
of topics and senses, respectively.
Using topic-to-sense affinity as the sole fea-
ture, we pool together all instances and optimise
the affinity feature to classify instances that have
novel senses. Evaluation is done by computing the
mean precision, recall and F-score across 10 sepa-
rate runs; results are summarised in Table 7. Note
that we evaluate only over UKWAC in this section,
for ease of presentation.
The results show that instances with high-
frequency novel senses are more easily identifi-
able than instances with medium/low-frequency
novel senses. This is unsurprising given that high-
frequency senses have a higher probability of gen-
erating related topics (sense-related words are ob-
served more frequently in the corpus), and as such
are more easily identifiable.
We are interested in understanding whether
pooling all instances — instances from target lem-
mas that have a sense artificially removed and
those that do not — impacted the results (re-
call that not all target lemmas have a removed
sense). To that end, we chose to include only
instances from lemmas with a removed sense,
and repeated the experiment for the medium- and
high-frequency novel sense condition (for the low-
frequency condition, all target lemmas have a
novel sense). In other words, we are assuming
knowledge of which words have novel sense, and
the task is to identify specifically what the novel
sense is, as represented by novel usages. Results
are presented in Table 8.
</bodyText>
<page confidence="0.995574">
266
</page>
<figure confidence="0.9815398">
No. of Lemmas with No. of Lemmas without Relative Freq Wilcoxon Rank Sum
a Removed Sense a Removed Sense of Removed Sense p-value
10 0 0.0–0.2 0.4543
9 11 0.2–0.4 0.0391
6 14 0.4–0.6 0.0247
</figure>
<tableCaption confidence="0.924845">
Table 9: Wilcoxon Rank Sum p-value results for testing target lemmas with removed sense vs. target
lemmas without removed sense using novelty.
</tableCaption>
<bodyText confidence="0.917773882352941">
From the results, we see that the F-scores im-
proved notably. This reveals that an additional step
is necessary to determine whether a target lemma
has a potential novel sense before feeding its in-
stances to learn which of them contains the usage
of the novel sense.
In the last experiment, we propose a new mea-
sure to tackle this: the identification of target lem-
mas that have a novel sense. We introduce novelty,
a measure of the likelihood of a target lemma w
having a novel sense:
267 extensions to demonstrate the flexibility and ro-
where
is the frequency of topic tj in the
corpus. The intuition behind novelty is that a
target lemma with a novel sense should have a
(somewhat-)frequent topic that has low associa-
tion with any sense. That we use the frequency
rather than the probability of the topic here is de-
liberate, as topics with a higher raw number of oc-
currences (whether as
topic for
ahigh-frequency word, or ahigh-probability topic
f(tj)
alow-probability
a low-frequency word) are indicative of a novel
novel sense) are statistically
Results
are presented in Table 9. We see that the nov-
elty measure can readily identify target lemmas
with high- and medium-frequency novel senses
(p &lt; 0.05), but the results are less promising for
different.15
equency novel senses.
</bodyText>
<sectionHeader confidence="0.996989" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.7279939">
d novel senses are simple
that the number of words with low-frequency novel
senses here is restricted to 10 (cf. 20 in Table 7) to ensure we
have both positive and negative lemmas
15Note
in the dataset.
tj) for com-
A natural next step for this research would be to
of individual token
In summary, we have proposed a topic
</bodyText>
<equation confidence="0.463192">
sim(si,
WSD
.
</equation>
<sectionHeader confidence="0.861038" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<subsectionHeader confidence="0.452071">
f (tj )
</subsectionHeader>
<bodyText confidence="0.967272714285714">
for
word sense.
For each of our three datasets (with low-,
medium- and high-frequency novel senses, respec-
tively), we compute the novelty of the target lem-
mas and the p-value of aone-tailed Wilcoxon rank
sum test to test if the two groups of lemmas (i.e.
lemmas with a novel sense vs. lemmas without a
the low-fr
Our methodologies for the two proposed tasks of
identifying unused an
We wish to thank the anonymous reviewers for
their valuable comments. This research was sup-
ported in part by funding from the Australian
</bodyText>
<equation confidence="0.791583">
Re-
search Council.
C sim(si7 t�)1
novelty(w) = min max /I (5)
t si
</equation>
<bodyText confidence="0.999633125">
bustness of our methodology. Future work could
pursue a more sophisticated methodology, using
non-linear combinations of
puting the affinity measures or multiple features
in a supervised context. We contend, however,
that these extensions are ultimately a preliminary
demonstration to the flexibility and robustness of
our methodology.
couple sense distribution estimation and the detec-
tion of unattested senses with evidence from the
context, using topics or other information about
the local context (e.g. Agirre and Soroa (2009))
to carry out unsupervised
occurrences of a given word.
modelling-based method for estimating word
sense distributions, based on Hierarchical Dirich-
let Processes and the earlier work of Lau et al.
(2012) on word sense induction, in probabilisti-
cally mapping the automatically-learned topics to
senses in a sense inventory. We evaluated the abil-
ity of the method to learn predominant senses and
induce word sense distributions, based on a broad
range of datasets and two separate sense invento-
ries. In doing so, we established that our method
is comparable to the approach of McCarthy et al.
(2007) at predominant sense learning, and supe-
rior at inducing word sense distributions. We fur-
ther demonstrated the applicability of the method
to the novel tasks of detecting word senses which
are unattested in a corpus, and identifying novel
senses which are found in a corpus but not cap-
tured in a word sense inventory
</bodyText>
<sectionHeader confidence="0.970534" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994987981818182">
Eneko Agirre and Philip Edmonds, editors. 2006.
Word Sense Disambiguation: Algorithms and Appli-
cations. Springer, Dordrecht, Netherlands.
Eneko Agirre and David Martinez. 2004. Unsuper-
vised WSD based on automatically retrieved exam-
ples: The importance of bias. In Proceedings of
EMNLP 2004, pages 25–32, Barcelona, Spain.
Eneko Agirre and Aitor Soroa. 2007. SemEval-2007
task 02: Evaluating word sense induction and dis-
crimination systems. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 7–12, Prague, Czech Republic.
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the EACL (EACL
2009), pages 33–41, Athens, Greece.
Timothy Baldwin, Paul Cook, Marco Lui, Andrew
MacKinlay, and Li Wang. 2013. How noisy so-
cial media text, how diffrnt social media sources?
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing (IJCNLP
2013), pages 356–364, Nagoya, Japan.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the 3rd In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLing-2002),
pages 136–145, Mexico City, Mexico.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Jordan Boyd-Graber and David Blei. 2007. Putop:
Turning predominant senses into a topic model for
word sense disambiguation. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 277–281, Prague, Czech Re-
public.
Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambigua-
tion. In Proc. of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 1024–1033, Prague, Czech
Republic.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the EACL (EACL 2009), pages 103–
111, Athens, Greece.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. NUS-ML: Improving word sense disam-
biguation using topic features. In Proc. of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 249–252, Prague, Czech Re-
public.
Marine Carpuat, Hal Daum´e III, Katharine Henry,
Ann Irvine, Jagadeesh Jagarlamudi, and Rachel
Rudinger. 2013. SenseSpotting: Never let your par-
allel data tie you to an old domain. In Proc. of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1435–1445,
Sofia, Bulgaria.
Yee Seng Chan and Hwee Tou Ng. 2005. Word
sense disambiguation with distribution estimation.
In Proc. of the 19th International Joint Conference
on Artificial Intelligence (IJCAI 2005), pages 1010–
1015, Edinburgh, UK.
Yee Seng Chan and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for word sense dis-
ambiguation. In Proc. of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 89–96, Sydney, Australia.
Paul Cook and Suzanne Stevenson. 2010. Automati-
cally identifying changes in the semantic orientation
of words. In Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC 2010), pages 28–34, Valletta, Malta.
Paul Cook, Jey Han Lau, Michael Rundell, Diana Mc-
Carthy, and Timothy Baldwin. 2013. A lexico-
graphic appraisal of an automatic approach for de-
tecting new word senses. In Proceedings of eLex
2013, pages 49–65, Tallinn, Estonia.
Katrin Erk. 2006. Unknown word sense detection as
outlier detection. In Proc. of the Main Conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 128–135, New York
City, USA.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
USA.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
ukWaC, a very large web-derived corpus of English.
In Proc. of the 4th Web as Corpus Workshop: Can
we beat Google, pages 47–54, Marrakech, Morocco.
Spandana Gella, Paul Cook, and Timothy Baldwin. to
appear. One sense per tweeter ... and other lexical
semantic tales of Twitter. In Proceedings of the 14th
Conference of the EACL (EACL 2014), Gothenburg,
Sweden.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the Google Books Ngram corpus.
In Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67–71, Edinburgh, UK.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
</reference>
<page confidence="0.987219">
268
</page>
<reference confidence="0.991480463636364">
the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
57–60, New York City, USA.
Ryu Iida, Diana McCarthy, and Rob Koeling. 2008.
Gloss-based semantic similarity metrics for predom-
inant sense acquisition. In Proc. of the Third In-
ternational Joint Conference on Natural Language
Processing, pages 561–568.
Jay Jiang and David Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings on International Conference on Re-
search in Computational Linguistics, pages 19–33,
Taipei, Taiwan.
Peng Jin, Diana McCarthy, Rob Koeling, and John Car-
roll. 2009. Estimating and exploiting the entropy
of sense distributions. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics – Human Language Technologies
2009 (NAACL HLT 2009): Short Papers, pages 233–
236, Boulder, USA.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2013), pages 290–299, Atlanta, USA.
Adam Kilgarriff. 2004. How dominant is the common-
est sense of a word? Technical Report ITRI-04-10,
Information Technology Research Institute, Univer-
sity of Brighton.
Johannes Knopp, Johanna V¨olker, and Simone Paolo
Ponzetto. 2013. Topic modeling for word sense in-
duction. In Proc. of the International Conference of
the German Society for Computational Linguistics
and Language Technology, pages 97–103, Darm-
stadt, Germany.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of the
2005 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 419–
426, Vancouver, Canada.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Computa-
tional Linguistics, 30(1):45–75.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings
of the 13th Conference of the EACL (EACL 2012),
pages 591–601, Avignon, France.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a.
unimelb: Topic modelling-based word sense induc-
tion. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013), pages
307–311, Atlanta, USA.
Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b.
unimelb: Topic modelling-based word sense induc-
tion for web snippet clustering. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013), pages 217–221, Atlanta, USA.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the 1986 SIGDOC Conference, pages 24–26, On-
tario, Canada.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proc. of the
48th Annual Meeting of the Association for Com-
putational Linguistics, pages 1138–1147, Uppsala,
Sweden.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the ACL and 17th International Confer-
ence on Computational Linguistics (COLING/ACL-
98), pages 768–774, Montreal, Canada.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2012)
Demo Session, pages 25–30, Jeju, Republic of Ko-
rea.
Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,
and Sameer Pradhan. 2010. SemEval-2010 Task
14: Word sense induction &amp; disambiguation. In
Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 63–68, Uppsala, Swe-
den.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004a. Automatic identification of infre-
quent word senses. In Proc. of the 20th International
Conference of Computational Linguistics, COLING-
2004, pages 1220–1226, Geneva, Switzerland.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004b. Finding predominant senses in
untagged text. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2004), pages 280–287, Barcelona,
Spain.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
4(33):553–590.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of the ARPA Workshop on Human Language
Technology, pages 303–308.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–223.
</reference>
<page confidence="0.978188">
269
</page>
<reference confidence="0.996598507246377">
Saif Mohammad and Graeme Hirst. 2006. Determin-
ing word sense dominance using a thesaurus. In
Proc. of EACL-2006, pages 121–128, Trento, Italy.
Roberto Navigli and Daniele Vannella. 2013.
SemEval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Proceedings of the 7th International Workshop on
Semantic Evaluation (SemEval 2013), pages 193–
201, Atlanta, USA.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE Trans-
actions on Pattern Analysis and Machine Intelli-
gence, 27(7):1075–1088.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 task 07: Coarse-
grained English all-words task. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 30–35, Prague, Czech Republic.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. ACM Computing Surveys, 41(2).
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, and Nathan Schneider. 2012. Part-
of-speech tagging for Twitter: Word clusters and
other advances. Technical Report CMU-ML-12-
107, Machine Learning Department, Carnegie Mel-
lon University.
Martha Palmer, Olga Babko-Malaya, and Hoa Trang
Dang. 2004. Different sense granularities for differ-
ent applications. In Proceedings of the HLT-NAACL
2004 Workshop: 2nd Workshop on Scalable Natu-
ral Language Understanding, pages 49–56, Boston,
USA.
Yves Peirsman, Dirk Geeraerts, and Dirk Speelman.
2010. The automatic identification of lexical varia-
tion between language varieties. Natural Language
Engineering, 16(4):469–491.
Judita Preiss and Mark Stevenson. 2013. Unsuper-
vised domain tuning to improve word sense dis-
ambiguation. In Proc. of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 680–684, Atlanta, USA.
Michael Rundell and Adam Kilgarriff. 2011. Au-
tomating the creation of dictionaries: where will
it all end? In Fanny Meunier, Sylvie De
Cock, Ga¨etanelle Gilquin, and Magali Paquot, ed-
itors, A Taste for Corpora. In honour of Sylviane
Granger, pages 257–282. John Benjamins, Amster-
dam, Netherlands.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009.
Semantic density analysis: Comparing word mean-
ing across time and space. In Proceedings of
the EACL 2009 Workshop on GEMS: GEometrical
Models of Natural Language Semantics, pages 104–
111, Athens, Greece.
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
processes. Journal of the American Statistical Asso-
ciation, 101:1566–1581.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric Bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10–14,
Portland, USA.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proc. of the ACL 2010 System
Demonstrations, pages 78–83, Uppsala, Sweden.
</reference>
<page confidence="0.996596">
270
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934491">
<title confidence="0.997446">Learning Word Sense Distributions, Detecting Unattested Senses Identifying Novel Senses Using Topic Models</title>
<author confidence="0.998546">Han Paul Diana</author>
<affiliation confidence="0.980752">of Philosophy, King’s College London of Computing and Information Systems, The University of Melbourne ♦ University of</affiliation>
<abstract confidence="0.999573260869565">Unsupervised word sense disambiguation methods are an attractive approach all-words to their non-reliance on expensive annotated data. Unsupervised estimates of sense frequency have shown to be very useful for to the skewed nature of word sense distributions. This paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation, which is highly portable to different corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text. We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Word Sense Disambiguation: Algorithms and Applications.</title>
<date>2006</date>
<editor>Eneko Agirre and Philip Edmonds, editors.</editor>
<publisher>Springer,</publisher>
<location>Dordrecht, Netherlands.</location>
<contexts>
<context position="9931" citStr="(2006)" startWordPosition="1602" endWordPosition="1602">ntify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignments) in the form of a multinomial distribution over topics. Following Lau et al. (20</context>
<context position="13106" citStr="(2006)" startWordPosition="2139" endWordPosition="2139">efinition into a multinomial distribution over words, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: sim(si, tj) = 1 − JS(SIIT) (1) where S and T are the multinomial distributions 3For hyper-parameters α and γ, we used 0.1 for both. We did not tune the parameters, and opted to use the default parameters introduced in Teh et al. (2006). 4To avoid confusion, we will refer to the HDP-induced topics as topics, and reserve the term sense to denote senses in a sense inventory. 5The code used to learn predominant sense and run all experiments described in this paper is available at: https: //github.com/jhlau/predom_sense. 6Words are tokenised using OpenNLP and lemmatised with Morpha (Minnen et al., 2001). We additionally remove the target lemma, stopwords and words that are less than 3 characters in length. 261 Topic Num Top-10 Terms 1 network support @card@ information research service group development community member 2 servic</context>
<context position="23546" citStr="(2006)" startWordPosition="3823" endWordPosition="3823"> a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Tw</context>
<context position="31229" citStr="(2006)" startWordPosition="5115" endWordPosition="5115">ting to identify which of the known senses wasn’t observed in the corpus. Also, while we have annotations of “Other” usages in TWITTER and UKWAC, there is no real expectation that all such usages will correspond to the same sense: in practice, they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression, and errors in POS tagging (i.e. the usage not being nominal). As such, we can’t use the “Other” annotations to evaluate novel sense identification. The evaluation of systems for this task is a known challenge, which we address similarly to Erk (2006) by artificially synthesising novel senses through removal of senses from the sense inventory. In this way, even if we remove multiple senses for a given word, we still have access to information about which usages correspond to 14We used a fixed step and increment at steps of 0.001, up to the max value of st-affinity when optimising the threshold. 265 No. Lemmas with Relative Freq Threshold P R F a Removed Sense of Removed Sense Mean±stdev 20 0.0–0.2 0.052±0.009 0.35 0.42 0.36 9 0.2–0.4 0.089±0.024 0.24 0.59 0.29 6 0.4–0.6 0.061±0.004 0.63 0.64 0.63 Table 7: Classification of usages with nove</context>
</contexts>
<marker>2006</marker>
<rawString>Eneko Agirre and Philip Edmonds, editors. 2006. Word Sense Disambiguation: Algorithms and Applications. Springer, Dordrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
</authors>
<title>Unsupervised WSD based on automatically retrieved examples: The importance of bias.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>25--32</pages>
<location>Barcelona,</location>
<contexts>
<context position="5707" citStr="Agirre and Martinez, 2004" startWordPosition="905" endWordPosition="908">nguage, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic relationships (Navigli and Velardi, 2005) or annotated data (Zhong and Ng, 2010). One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2</context>
<context position="7330" citStr="Agirre and Martinez, 2004" startWordPosition="1165" endWordPosition="1168">1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the </context>
</contexts>
<marker>Agirre, Martinez, 2004</marker>
<rawString>Eneko Agirre and David Martinez. 2004. Unsupervised WSD based on automatically retrieved examples: The importance of bias. In Proceedings of EMNLP 2004, pages 25–32, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>SemEval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9732" citStr="Agirre and Soroa, 2007" startWordPosition="1570" endWordPosition="1573"> respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. SemEval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7–12, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the EACL (EACL</booktitle>
<pages>33--41</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="38660" citStr="Agirre and Soroa (2009)" startWordPosition="6372" endWordPosition="6375">nding from the Australian Research Council. C sim(si7 t�)1 novelty(w) = min max /I (5) t si bustness of our methodology. Future work could pursue a more sophisticated methodology, using non-linear combinations of puting the affinity measures or multiple features in a supervised context. We contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology. couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised occurrences of a given word. modelling-based method for estimating word sense distributions, based on Hierarchical Dirichlet Processes and the earlier work of Lau et al. (2012) on word sense induction, in probabilistically mapping the automatically-learned topics to senses in a sense inventory. We evaluated the ability of the method to learn predominant senses and induce word sense distributions, based on a broad range of datasets and two separate sense inventories. In doing so, we established that our method is comparable to the approach of McCarthy et al. (2007) a</context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for word sense disambiguation. In Proceedings of the 12th Conference of the EACL (EACL 2009), pages 33–41, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Paul Cook</author>
<author>Marco Lui</author>
<author>Andrew MacKinlay</author>
<author>Li Wang</author>
</authors>
<title>How noisy social media text, how diffrnt social media sources?</title>
<date>2013</date>
<booktitle>In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013),</booktitle>
<pages>356--364</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="21484" citStr="Baldwin et al., 2013" startWordPosition="3497" endWordPosition="3500"> one dataset), but HDP-WSI is better at inducing the overall sense distribution. It is important to bear in mind that MKWC in these experiments makes use of full-text parsing in calculating the distributional similarity thesaurus, and the WordNet graph structure in calculating the similarity between associated words and different senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how 8McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying t</context>
</contexts>
<marker>Baldwin, Cook, Lui, MacKinlay, Wang, 2013</marker>
<rawString>Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How noisy social media text, how diffrnt social media sources? In Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013), pages 356–364, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002),</booktitle>
<pages>136--145</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="6947" citStr="Banerjee and Pedersen (2002)" startWordPosition="1104" endWordPosition="1107">ad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been </context>
<context position="21973" citStr="Banerjee and Pedersen, 2002" startWordPosition="3571" endWordPosition="3574"> parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how 8McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Mac</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk algorithm for word sense disambiguation using WordNet. In Proceedings of the 3rd International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2002), pages 136–145, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="10023" citStr="Blei et al., 2003" startWordPosition="1613" endWordPosition="1616">ations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignments) in the form of a multinomial distribution over topics. Following Lau et al. (2012), we assign one topic to each usage by selecting the topic that has the highest cumulativ</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
</authors>
<title>Putop: Turning predominant senses into a topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>277--281</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="7888" citStr="Boyd-Graber and Blei (2007)" startWordPosition="1257" endWordPosition="1260">r WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document. They then predict the most likely sense for each word in the document based on the topic distribution and the words in context (“corroborators”), each of which, in turn, depends on the document’s topic distribution. Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improve</context>
</contexts>
<marker>Boyd-Graber, Blei, 2007</marker>
<rawString>Jordan Boyd-Graber and David Blei. 2007. Putop: Turning predominant senses into a topic model for word sense disambiguation. In Proc. of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 277–281, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
<author>Xiaojin Zhu</author>
</authors>
<title>A topic model for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1024--1033</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2402" citStr="Boyd-Graber et al., 2007" startWordPosition="364" endWordPosition="367">of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable se</context>
</contexts>
<marker>Boyd-Graber, Blei, Zhu, 2007</marker>
<rawString>Jordan Boyd-Graber, David Blei, and Xiaojin Zhu. 2007. A topic model for word sense disambiguation. In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1024–1033, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the EACL (EACL</booktitle>
<pages>103--111</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="4614" citStr="Brody and Lapata, 2009" startWordPosition="733" endWordPosition="736">let and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information </context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the EACL (EACL 2009), pages 103– 111, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Fu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>NUS-ML: Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proc. of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>249--252</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2483" citStr="Cai et al., 2007" startWordPosition="380" endWordPosition="383">ary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. NUS-ML: Improving word sense disambiguation using topic features. In Proc. of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 249–252, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Hal Daum´e Katharine Henry</author>
<author>Ann Irvine</author>
<author>Jagadeesh Jagarlamudi</author>
<author>Rachel Rudinger</author>
</authors>
<title>SenseSpotting: Never let your parallel data tie you to an old domain.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1435--1445</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9293" citStr="Carpuat et al. (2013)" startWordPosition="1502" endWordPosition="1505">ve of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dir</context>
</contexts>
<marker>Carpuat, Henry, Irvine, Jagarlamudi, Rudinger, 2013</marker>
<rawString>Marine Carpuat, Hal Daum´e III, Katharine Henry, Ann Irvine, Jagadeesh Jagarlamudi, and Rachel Rudinger. 2013. SenseSpotting: Never let your parallel data tie you to an old domain. In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1435–1445, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Word sense disambiguation with distribution estimation.</title>
<date>2005</date>
<booktitle>In Proc. of the 19th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>1010--1015</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="6134" citStr="Chan and Ng, 2005" startWordPosition="973" endWordPosition="976">stribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional simi</context>
<context position="7473" citStr="Chan and Ng, 2005" startWordPosition="1189" endWordPosition="1192">easure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to provide a generative model which assumes the words in a given document are generated according to the topi</context>
</contexts>
<marker>Chan, Ng, 2005</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2005. Word sense disambiguation with distribution estimation. In Proc. of the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005), pages 1010– 1015, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Estimating class priors in domain adaptation for word sense disambiguation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>89--96</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="6180" citStr="Chan and Ng, 2006" startWordPosition="981" endWordPosition="984">nse distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are as</context>
<context position="7492" citStr="Chan and Ng, 2006" startWordPosition="1193" endWordPosition="1196">se proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to provide a generative model which assumes the words in a given document are generated according to the topic distribution appr</context>
</contexts>
<marker>Chan, Ng, 2006</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2006. Estimating class priors in domain adaptation for word sense disambiguation. In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 89–96, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically identifying changes in the semantic orientation of words.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>28--34</pages>
<location>Valletta,</location>
<contexts>
<context position="8887" citStr="Cook and Stevenson, 2010" startWordPosition="1432" endWordPosition="1435">, each of which, in turn, depends on the document’s topic distribution. Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account. Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our me</context>
</contexts>
<marker>Cook, Stevenson, 2010</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2010. Automatically identifying changes in the semantic orientation of words. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC 2010), pages 28–34, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Jey Han Lau</author>
<author>Michael Rundell</author>
<author>Diana McCarthy</author>
<author>Timothy Baldwin</author>
</authors>
<title>A lexicographic appraisal of an automatic approach for detecting new word senses.</title>
<date>2013</date>
<booktitle>In Proceedings of eLex 2013,</booktitle>
<pages>49--65</pages>
<location>Tallinn, Estonia.</location>
<contexts>
<context position="4369" citStr="Cook et al., 2013" startWordPosition="689" endWordPosition="692">(e.g. the rela259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usage</context>
<context position="33565" citStr="Cook et al., 2013" startWordPosition="5510" endWordPosition="5513"> a single sense). For example, only 6 of our 20 target nouns have senses which are candidates for highfrequency novel senses. As before, we treat the novel sense identification task as a classification problem, although with a significantly different formulation: we are no longer attempting to identify pre-existing senses, as novel senses are by definition not included in the sense inventory. Instead, we are seeking to identify clusters of usages which are instances of a novel sense, e.g. for presentation to a lexicographer as part of a dictionary update process (Rundell and Kilgarriff, 2011; Cook et al., 2013). That is, for each usage, we want to classify whether it is an instance of a given novel sense. A usage that corresponds to a novel sense should have a topic that does not align well with any of the pre-existing senses in the sense inventory. Based on this intuition, we introduce topicto-sense affinity to estimate the similarity of a topic to the set of senses, as follows: ESZ sim(sZ, tj) ts-affinity(tj) = ET l Ek sim(sk, tl) (4) where, once again, sim(sZ, tj) is defined as in Equation (1), and T and S represent the number of topics and senses, respectively. Using topic-to-sense affinity as t</context>
</contexts>
<marker>Cook, Lau, Rundell, McCarthy, Baldwin, 2013</marker>
<rawString>Paul Cook, Jey Han Lau, Michael Rundell, Diana McCarthy, and Timothy Baldwin. 2013. A lexicographic appraisal of an automatic approach for detecting new word senses. In Proceedings of eLex 2013, pages 49–65, Tallinn, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Unknown word sense detection as outlier detection.</title>
<date>2006</date>
<booktitle>In Proc. of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>128--135</pages>
<location>New York City, USA.</location>
<contexts>
<context position="4737" citStr="Erk, 2006" startWordPosition="755" endWordPosition="756">A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic rel</context>
<context position="31229" citStr="Erk (2006)" startWordPosition="5114" endWordPosition="5115">tempting to identify which of the known senses wasn’t observed in the corpus. Also, while we have annotations of “Other” usages in TWITTER and UKWAC, there is no real expectation that all such usages will correspond to the same sense: in practice, they are attributable to a myriad of effects such as incorporation in a non-compositional multiword expression, and errors in POS tagging (i.e. the usage not being nominal). As such, we can’t use the “Other” annotations to evaluate novel sense identification. The evaluation of systems for this task is a known challenge, which we address similarly to Erk (2006) by artificially synthesising novel senses through removal of senses from the sense inventory. In this way, even if we remove multiple senses for a given word, we still have access to information about which usages correspond to 14We used a fixed step and increment at steps of 0.001, up to the max value of st-affinity when optimising the threshold. 265 No. Lemmas with Relative Freq Threshold P R F a Removed Sense of Removed Sense Mean±stdev 20 0.0–0.2 0.052±0.009 0.35 0.42 0.36 9 0.2–0.4 0.089±0.024 0.24 0.59 0.29 6 0.4–0.6 0.061±0.004 0.63 0.64 0.63 Table 7: Classification of usages with nove</context>
</contexts>
<marker>Erk, 2006</marker>
<rawString>Katrin Erk. 2006. Unknown word sense detection as outlier detection. In Proc. of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 128–135, New York City, USA.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proc. of the 4th Web as Corpus Workshop: Can we beat Google,</booktitle>
<pages>47--54</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="22758" citStr="Ferraresi et al., 2008" startWordPosition="3696" endWordPosition="3699">ns. 9The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dictionary. 5 Macmillan Experiments In our second set of experiments, we move to a new dataset (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proc. of the 4th Web as Corpus Workshop: Can we beat Google, pages 47–54, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spandana Gella</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>to appear. One sense per tweeter ... and other lexical semantic tales of Twitter.</title>
<date></date>
<booktitle>In Proceedings of the 14th Conference of the EACL (EACL 2014),</booktitle>
<location>Gothenburg,</location>
<marker>Gella, Cook, Baldwin, </marker>
<rawString>Spandana Gella, Paul Cook, and Timothy Baldwin. to appear. One sense per tweeter ... and other lexical semantic tales of Twitter. In Proceedings of the 14th Conference of the EACL (EACL 2014), Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Gulordava</author>
<author>Marco Baroni</author>
</authors>
<title>A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>67--71</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="8916" citStr="Gulordava and Baroni, 2011" startWordPosition="1436" endWordPosition="1440">depends on the document’s topic distribution. Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account. Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI</context>
</contexts>
<marker>Gulordava, Baroni, 2011</marker>
<rawString>Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 67–71, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>57--60</pages>
<location>New York City, USA.</location>
<contexts>
<context position="23546" citStr="Hovy et al. (2006)" startWordPosition="3820" endWordPosition="3823">n that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Tw</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: The 90% solution. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 57–60, New York City, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
</authors>
<title>Gloss-based semantic similarity metrics for predominant sense acquisition.</title>
<date>2008</date>
<booktitle>In Proc. of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>561--568</pages>
<contexts>
<context position="21993" citStr="Iida et al. (2008)" startWordPosition="3575" endWordPosition="3578">ms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how 8McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be limited. In the next section, we demonstrate the robustness of the method in experimenting with two new datasets, based on Twitter and a web corpus, and the Macmillan English Dicti</context>
</contexts>
<marker>Iida, McCarthy, Koeling, 2008</marker>
<rawString>Ryu Iida, Diana McCarthy, and Rob Koeling. 2008. Gloss-based semantic similarity metrics for predominant sense acquisition. In Proc. of the Third International Joint Conference on Natural Language Processing, pages 561–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings on International Conference on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="6914" citStr="Jiang and Conrath (1997)" startWordPosition="1099" endWordPosition="1102">cCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 200</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings on International Conference on Research in Computational Linguistics, pages 19–33, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Jin</author>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>John Carroll</author>
</authors>
<title>Estimating and exploiting the entropy of sense distributions.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies 2009 (NAACL HLT 2009): Short Papers,</booktitle>
<pages>233--236</pages>
<location>Boulder, USA.</location>
<contexts>
<context position="7369" citStr="Jin et al., 2009" startWordPosition="1171" endWordPosition="1174">he nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to pr</context>
<context position="19126" citStr="Jin et al., 2009" startWordPosition="3119" endWordPosition="3122">em in each row is indicated in boldface). both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained by normalising the prevalence scores across all senses. The predominant sense learning task of McCarthy et al. (2007) evaluates the ability of a method to identify only the head of this distribution, but it is also important to evaluate the full sense distribution (Jin et al., 2009). To this end, we introduce a second evaluation metric: the Jensen–Shannon (JS) divergence between the inferred sense distribution and the gold-standard sense distribution, noting that smaller values are better in this case, and that it is now theoretically possible to obtain a JS divergence of 0 in the case of a perfect estimate of the sense distribution. Results are presented in Table 3. HDP-WSI consistently achieves lower JS divergence, indicating that the distribution of senses that it finds is closer to the gold standard distribution. Testing for statistical significance over the paired J</context>
</contexts>
<marker>Jin, McCarthy, Koeling, Carroll, 2009</marker>
<rawString>Peng Jin, Diana McCarthy, Rob Koeling, and John Carroll. 2009. Estimating and exploiting the entropy of sense distributions. In Proceedings of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies 2009 (NAACL HLT 2009): Short Papers, pages 233– 236, Boulder, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Ioannis Klapaftis</author>
</authors>
<title>Semeval2013 task 13: Word sense induction for graded and non-graded senses.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>290--299</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="9846" citStr="Jurgens and Klapaftis, 2013" startWordPosition="1585" endWordPosition="1588"> model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignm</context>
</contexts>
<marker>Jurgens, Klapaftis, 2013</marker>
<rawString>David Jurgens and Ioannis Klapaftis. 2013. Semeval2013 task 13: Word sense induction for graded and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 290–299, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>How dominant is the commonest sense of a word?</title>
<date>2004</date>
<tech>Technical Report ITRI-04-10,</tech>
<institution>Information Technology Research Institute, University of Brighton.</institution>
<contexts>
<context position="5619" citStr="Kilgarriff, 2004" startWordPosition="890" endWordPosition="891">n order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic relationships (Navigli and Velardi, 2005) or annotated data (Zhong and Ng, 2010). One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on</context>
</contexts>
<marker>Kilgarriff, 2004</marker>
<rawString>Adam Kilgarriff. 2004. How dominant is the commonest sense of a word? Technical Report ITRI-04-10, Information Technology Research Institute, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Knopp</author>
<author>Johanna V¨olker</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Topic modeling for word sense induction.</title>
<date>2013</date>
<booktitle>In Proc. of the International Conference of the German Society for Computational Linguistics and Language Technology,</booktitle>
<pages>97--103</pages>
<location>Darmstadt, Germany.</location>
<marker>Knopp, V¨olker, Ponzetto, 2013</marker>
<rawString>Johannes Knopp, Johanna V¨olker, and Simone Paolo Ponzetto. 2013. Topic modeling for word sense induction. In Proc. of the International Conference of the German Society for Computational Linguistics and Language Technology, pages 97–103, Darmstadt, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>419--426</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2038" citStr="Koeling et al., 2005" startWordPosition="302" endWordPosition="305">ng-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predom</context>
<context position="15640" citStr="Koeling et al. (2005)" startWordPosition="2534" endWordPosition="2537">ion). Formally, the prevalence score of sense si is given as follows: (sim(si,tj) X P(tj)) (2) � sim(si,tj) X T(tj) Ek f(tk) where f(tj) is the frequency of topic tj (i.e. the number of usages assigned to topic tj), and T is the number of topics. The intuition behind the approach is that the predominant sense should be the sense that has relatively high similarity (in terms of lexical overlap) with high-probability topic(s). 4 WordNet Experiments We first test the proposed method over the tasks of predominant sense learning and sense distribution induction, using the WordNet-tagged dataset of Koeling et al. (2005), which is made up of 3 collections of documents: a domain-neutral corpus (BNC), and two domain-specific corpora (SPORTS and FINANCE). For each domain, annotators were asked to sense-annotate a random selection of sentences for each of 40 target nouns, based on WordNet v1.7. The predominant sense and distribution across senses for each target lemma was obtained by aggregating over the sense annotations. The authors evaluated their method in terms of WSD accuracy over a given corpus, based on assigning all instances of a target word with the predominant sense learned from that corpus. For the r</context>
<context position="26088" citStr="Koeling et al. (2005)" startWordPosition="4254" endWordPosition="4257">ervised baseline (“FSCORPUS”), based on the most frequent sense in the corpus; and (b) an unsupervised baseline (“FSDICT”), based on the first-listed sense in Macmillan. In each case, the sense distribution is based on allocating all probability mass for a given word to the single sense identified by the respective method. We first notice that, despite the coarser-grained senses of Macmillan as compared to WordNet, the upper bound WSD accuracy using Macmillan is comparable to that of the WordNet-based datasets over the balanced BNC, and quite a bit lower than that of the two domain corpora of Koeling et al. (2005). This suggests that both datasets are diverse in domain and content. In terms of WSD accuracy, the results over UKWAC (ERR = 0.895) are substantially higher than those for BNC, while those over TWITTER (ERR = 0.716) are comparable. The accuracy is significantly higher than the dictionary-based first sense baseline (FSDICT) over both datasets (McNemar’s test; p &lt; 0.0001), and the ERR is also considerably higher than for the two domain datasets in Section 4 (FINANCE and SPORTS). One cause of difficulty in sense-modelling TWITTER is large numbers of missing senses, with 12.3% of usages in TWITTE</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005), pages 419– 426, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="2062" citStr="Lapata and Brew, 2004" startWordPosition="306" endWordPosition="309"> NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because o</context>
<context position="7516" citStr="Lapata and Brew, 2004" startWordPosition="1197" endWordPosition="1200">g and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirre and Martinez, 2004), entropy estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that documen</context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>Mirella Lapata and Chris Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the EACL (EACL 2012),</booktitle>
<pages>591--601</pages>
<location>Avignon, France.</location>
<contexts>
<context position="2437" citStr="Lau et al., 2012" startWordPosition="372" endWordPosition="375">distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method f</context>
<context position="4654" citStr="Lau et al., 2012" startWordPosition="741" endWordPosition="744">s) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a giv</context>
<context position="8972" citStr="Lau et al., 2012" startWordPosition="1447" endWordPosition="1450"> they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account. Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been </context>
<context position="10534" citStr="Lau et al. (2012)" startWordPosition="1698" endWordPosition="1701"> et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignments) in the form of a multinomial distribution over topics. Following Lau et al. (2012), we assign one topic to each usage by selecting the topic that has the highest cumulative probability density, based on the topic allocations of all words in the context window for that usage.2 Note that in their original work, Lau et al. (2012) experimented with the use of features extracted from a dependency parser. Due to the computational overhead associated with these features, and the fact that the empirical impact of the features was found to be 1Based on the implementation available at: https:// github.com/jhlau/hdp-wsi 2This includes all words in the usage sentence except stopwords, </context>
<context position="38864" citStr="Lau et al. (2012)" startWordPosition="6403" endWordPosition="6406">of puting the affinity measures or multiple features in a supervised context. We contend, however, that these extensions are ultimately a preliminary demonstration to the flexibility and robustness of our methodology. couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised occurrences of a given word. modelling-based method for estimating word sense distributions, based on Hierarchical Dirichlet Processes and the earlier work of Lau et al. (2012) on word sense induction, in probabilistically mapping the automatically-learned topics to senses in a sense inventory. We evaluated the ability of the method to learn predominant senses and induce word sense distributions, based on a broad range of datasets and two separate sense inventories. In doing so, we established that our method is comparable to the approach of McCarthy et al. (2007) at predominant sense learning, and superior at inducing word sense distributions. We further demonstrated the applicability of the method to the novel tasks of detecting word senses which are unattested in</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the EACL (EACL 2012), pages 591–601, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>unimelb: Topic modelling-based word sense induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>307--311</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="9613" citStr="Lau et al., 2013" startWordPosition="1552" endWordPosition="1555">sman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per</context>
</contexts>
<marker>Lau, Cook, Baldwin, 2013</marker>
<rawString>Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013a. unimelb: Topic modelling-based word sense induction. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 307–311, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>unimelb: Topic modelling-based word sense induction for web snippet clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>217--221</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="9613" citStr="Lau et al., 2013" startWordPosition="1552" endWordPosition="1555">sman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per</context>
</contexts>
<marker>Lau, Cook, Baldwin, 2013</marker>
<rawString>Jey Han Lau, Paul Cook, and Timothy Baldwin. 2013b. unimelb: Topic modelling-based word sense induction for web snippet clustering. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 217–221, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 1986 SIGDOC Conference,</booktitle>
<pages>24--26</pages>
<location>Ontario, Canada.</location>
<contexts>
<context position="5323" citStr="Lesk, 1986" startWordPosition="846" endWordPosition="847">vidual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic relationships (Navigli and Velardi, 2005) or annotated data (Zhong and Ng, 2010). One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expens</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the 1986 SIGDOC Conference, pages 24–26, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linlin Li</author>
<author>Benjamin Roth</author>
<author>Caroline Sporleder</author>
</authors>
<title>Topic models for word sense disambiguation and token-based idiom detection.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1138--1147</pages>
<location>Uppsala,</location>
<contexts>
<context position="2419" citStr="Li et al., 2010" startWordPosition="368" endWordPosition="371">ever, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further </context>
</contexts>
<marker>Li, Roth, Sporleder, 2010</marker>
<rawString>Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010. Topic models for word sense disambiguation and token-based idiom detection. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1138–1147, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACL98),</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="6709" citStr="Lin, 1998" startWordPosition="1069" endWordPosition="1070">hy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was trained over. As well as sense ranking for predominant sense acquisition, automatic estimates of sense frequency distribution can be very useful for WSD for training data sampling purposes (Agirr</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics (COLING/ACL98), pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session,</booktitle>
<pages>25--30</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="24054" citStr="Lui and Baldwin, 2012" startWordPosition="3910" endWordPosition="3913">ary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl over the time period Jan 3–Feb 28, 2013 using the Twitter Streaming API) and ukWaC, after language identification using langid.py (Lui and Baldwin, 2012) and POS tagging (based on the CMU ARK Twitter POS tagger v2.0 (Owoputi et al., 2012) for Twitter, and the POS tags provided with the corpus for ukWaC). Amazon Mechanical Turk (AMT) was then used to 5-way sense-tag each usage relative to Macmillan, including allowing the annotators the option to label a usage as “Other” in instances where the usage was not captured by any of the Macmillan senses. After quality control over the annotators/annotations (see 10http://www.macmillandictionary.com/ 11Strictly speaking, there is limited linking in the form of sets of synonyms in Macmillan, but we choo</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012) Demo Session, pages 25–30, Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer Pradhan</author>
</authors>
<title>SemEval-2010 Task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<location>Uppsala,</location>
<contexts>
<context position="9771" citStr="Manandhar et al., 2010" startWordPosition="1575" endWordPosition="1578">an and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach, and Sameer Pradhan. 2010. SemEval-2010 Task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Automatic identification of infrequent word senses.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th International Conference of Computational Linguistics, COLING2004,</booktitle>
<pages>1220--1226</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="3272" citStr="McCarthy et al. (2004" startWordPosition="510" endWordPosition="513">. Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the rela259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270,</context>
<context position="4508" citStr="McCarthy et al. (2004" startWordPosition="713" endWordPosition="716">yland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to </context>
<context position="6114" citStr="McCarthy et al., 2004" startWordPosition="969" endWordPosition="972"> word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The</context>
<context position="7961" citStr="McCarthy et al. (2004" startWordPosition="1271" endWordPosition="1275">estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document. They then predict the most likely sense for each word in the document based on the topic distribution and the words in context (“corroborators”), each of which, in turn, depends on the document’s topic distribution. Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into ac</context>
<context position="21815" citStr="McCarthy et al. (2004" startWordPosition="3550" endWordPosition="3553"> senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how 8McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be li</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004a. Automatic identification of infrequent word senses. In Proc. of the 20th International Conference of Computational Linguistics, COLING2004, pages 1220–1226, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Finding predominant senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004),</booktitle>
<pages>280--287</pages>
<location>Barcelona,</location>
<contexts>
<context position="3272" citStr="McCarthy et al. (2004" startWordPosition="510" endWordPosition="513">. Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distribution acquisition system to the task of finding unattested senses — i.e., senses that are in the sense inventory but not attested in a given corpus. In contrast to the previous work of McCarthy et al. (2004a) on this topic which uses the sense ranking score from McCarthy et al. (2004b) to remove low-frequency senses from WordNet, we focus on finding senses that are unattested in the corpus on the premise that, given accurate disambiguation, rare senses in a corpus contribute to correct interpretation. Corpus instances of a word can also correspond to senses that are not present in a given sense inventory. This can be due to, for example, words taking on new meanings over time (e.g. the rela259 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 259–270,</context>
<context position="4508" citStr="McCarthy et al. (2004" startWordPosition="713" endWordPosition="716">yland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tively recent senses of tablet and swipe related to touchscreen computers) or domain-specific terms not being included in a more general-purpose sense inventory. A system for automatically identifying such novel senses — i.e. senses that are attested in the corpus but not in the sense inventory — would be a very valuable lexicographical tool for keeping sense inventories up-to-date (Cook et al., 2013). We further propose an application of our proposed method to the identification of such novel senses. In contrast to McCarthy et al. (2004b), the use of topic models makes this possible, using topics as a proxy for sense (Brody and Lapata, 2009; Yao and Durme, 2011; Lau et al., 2012). Earlier work on identifying novel senses focused on individual tokens (Erk, 2006), whereas our approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to </context>
<context position="6114" citStr="McCarthy et al., 2004" startWordPosition="969" endWordPosition="972"> word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The</context>
<context position="7961" citStr="McCarthy et al. (2004" startWordPosition="1271" endWordPosition="1275">estimation (Jin et al., 2009), and prior probability estimates, all of which can be integrated within a WSD system (Chan and Ng, 2005; Chan and Ng, 2006; Lapata and Brew, 2004). Various approaches have been adopted, such as normalizing sense ranking scores to obtain a probability distribution (Jin et al., 2009), using subcategorisation information as an indication of verb sense (Lapata and Brew, 2004) or alternatively using parallel text (Chan and Ng, 2005; Chan and Ng, 2006; Agirre and Martinez, 2004). The work of Boyd-Graber and Blei (2007) is highly related in that it extends the method of McCarthy et al. (2004b) to provide a generative model which assumes the words in a given document are generated according to the topic distribution appropriate for that document. They then predict the most likely sense for each word in the document based on the topic distribution and the words in context (“corroborators”), each of which, in turn, depends on the document’s topic distribution. Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into ac</context>
<context position="21815" citStr="McCarthy et al. (2004" startWordPosition="3550" endWordPosition="3553"> senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how 8McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This could be a significant issue with Twitter data, where context tends to be li</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004b. Finding predominant senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 280–287, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>4</volume>
<issue>33</issue>
<contexts>
<context position="1740" citStr="McCarthy et al., 2007" startWordPosition="257" endWordPosition="260">e learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. 1 Introduction The automatic determination of word sense information has been a long-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been</context>
<context position="6515" citStr="McCarthy et al., 2007" startWordPosition="1039" endWordPosition="1042">998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the nearest neighbours are associated with the respective target word senses using a WordNet similarity measure, such as those proposed by Jiang and Conrath (1997) and Banerjee and Pedersen (2002). The word senses are ranked based on these similarity scores, and the most frequent sense is selected for the corpus that the distributional similarity thesaurus was t</context>
<context position="18960" citStr="McCarthy et al. (2007)" startWordPosition="3089" endWordPosition="3092">Sense distribution evaluation of MKWC and HDP-WSI on the WordNet-annotated datasets, evaluated using JS divergence (lower values indicate better performance; the best system in each row is indicated in boldface). both systems, as we see in the gap between the upper bound (based on perfect determination of the first sense) and the respective system accuracies. Given that both systems compute a continuousvalued prevalence score for each sense of a target lemma, a distribution of senses can be obtained by normalising the prevalence scores across all senses. The predominant sense learning task of McCarthy et al. (2007) evaluates the ability of a method to identify only the head of this distribution, but it is also important to evaluate the full sense distribution (Jin et al., 2009). To this end, we introduce a second evaluation metric: the Jensen–Shannon (JS) divergence between the inferred sense distribution and the gold-standard sense distribution, noting that smaller values are better in this case, and that it is now theoretically possible to obtain a JS divergence of 0 in the case of a perfect estimate of the sense distribution. Results are presented in Table 3. HDP-WSI consistently achieves lower JS di</context>
<context position="21740" citStr="McCarthy et al., 2007" startWordPosition="3535" endWordPosition="3538">ructure in calculating the similarity between associated words and different senses. Our method, on the other hand, uses no parsing, and only the synset definitions (and not the graph structure) of WordNet.8 The non-reliance on parsing is significant in terms of portability to text sources which are less amenable to parsing (such as Twitter: (Baldwin et al., 2013)), and the non-reliance on the graph structure of WordNet is significant in terms of portability to conventional “flat” sense inventories. While comparable results on a different dataset have been achieved with a proximity thesaurus (McCarthy et al., 2007) compared to a dependency one,9 it is not stated how 8McCarthy et al. (2004b) obtained good results with definition overlap, but their implementation uses the relation structure alongside the definitions (Banerjee and Pedersen, 2002). Iida et al. (2008) demonstrate that further extensions using distributional data are required when applying the method to resources without hierarchical relations. 9The thesauri used in the reimplementation of MKWC in this paper were obtained from http://webdocs.cs. ualberta.ca/˜lindek/downloads.htm. 263 wide a window is needed for the proximity thesaurus. This c</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2007. Unsupervised acquisition of predominant word senses. Computational Linguistics, 4(33):553–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proc. of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<contexts>
<context position="5837" citStr="Miller et al., 1993" startWordPosition="925" endWordPosition="928">disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic relationships (Navigli and Velardi, 2005) or annotated data (Zhong and Ng, 2010). One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Proc. of the ARPA Workshop on Human Language Technology, pages 303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="13476" citStr="Minnen et al., 2001" startWordPosition="2193" endWordPosition="2196">d topic tj is: sim(si, tj) = 1 − JS(SIIT) (1) where S and T are the multinomial distributions 3For hyper-parameters α and γ, we used 0.1 for both. We did not tune the parameters, and opted to use the default parameters introduced in Teh et al. (2006). 4To avoid confusion, we will refer to the HDP-induced topics as topics, and reserve the term sense to denote senses in a sense inventory. 5The code used to learn predominant sense and run all experiments described in this paper is available at: https: //github.com/jhlau/predom_sense. 6Words are tokenised using OpenNLP and lemmatised with Morpha (Minnen et al., 2001). We additionally remove the target lemma, stopwords and words that are less than 3 characters in length. 261 Topic Num Top-10 Terms 1 network support @card@ information research service group development community member 2 service @card@ road company transport rail area government network public 3 network social model system family structure analysis form relationship neural 4 network @card@ computer system service user access internet datum server 5 system network management software support corp company service application product 6 @card@ radio news television show bbc programme call think</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Graeme Hirst</author>
</authors>
<title>Determining word sense dominance using a thesaurus.</title>
<date>2006</date>
<booktitle>In Proc. of EACL-2006,</booktitle>
<pages>121--128</pages>
<location>Trento, Italy. Roberto Navigli</location>
<contexts>
<context position="6160" citStr="Mohammad and Hirst, 2006" startWordPosition="977" endWordPosition="980"> important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has been some work on trying to estimate sense frequency information automatically (McCarthy et al., 2004b; Chan and Ng, 2005; Mohammad and Hirst, 2006; Chan and Ng, 2006). Much of this work has been focused on ranking word senses to find the predominant sense in a given corpus (McCarthy et al., 2004b; Mohammad and Hirst, 2006), which is a very powerful heuristic approach to WSD. Most WSD systems rely upon this heuristic for back-off in the absence of strong contextual evidence (McCarthy et al., 2007). McCarthy et al. (2004b) proposed a method which relies on distributionally similar words (nearest neighbours) associated with the target word in an automatically acquired thesaurus (Lin, 1998). The distributional similarity scores of the neare</context>
</contexts>
<marker>Mohammad, Hirst, 2006</marker>
<rawString>Saif Mohammad and Graeme Hirst. 2006. Determining word sense dominance using a thesaurus. In Proc. of EACL-2006, pages 121–128, Trento, Italy. Roberto Navigli and Daniele Vannella. 2013.</rawString>
</citation>
<citation valid="true">
<title>SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>193--201</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="9293" citStr="(2013)" startWordPosition="1505" endWordPosition="1505">we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dir</context>
</contexts>
<marker>2013</marker>
<rawString>SemEval-2013 task 11: Word sense induction and disambiguation within an end-user application. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 193– 201, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Structural semantic interconnections: a knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="5375" citStr="Navigli and Velardi, 2005" startWordPosition="851" endWordPosition="854">r approach goes further in identifying groups of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic relationships (Navigli and Velardi, 2005) or annotated data (Zhong and Ng, 2010). One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sen</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Roberto Navigli and Paola Velardi. 2005. Structural semantic interconnections: a knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7):1075–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval-2007 task 07: Coarsegrained English all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>30--35</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23303" citStr="Navigli et al., 2007" startWordPosition="3782" endWordPosition="3785"> (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of Twitter (from a crawl ov</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. SemEval-2007 task 07: Coarsegrained English all-words task. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 30–35, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1496" citStr="Navigli, 2009" startWordPosition="217" endWordPosition="218">rent corpora and sense inventories, in being applicable to any part of speech, and not requiring a hierarchical sense inventory, parsing or parallel text. We demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition, and also the novel tasks of detecting senses which aren’t attested in the corpus, and identifying novel senses in the corpus which aren’t captured in the sense inventory. 1 Introduction The automatic determination of word sense information has been a long-term pursuit of the NLP community (Agirre and Edmonds, 2006; Navigli, 2009). Word sense distributions tend to be Zipfian, and as such, a simple but surprisingly highaccuracy back-off heuristic for word sense disambiguation (WSD) is to tag each instance of a given word with its predominant sense (McCarthy et al., 2007). Such an approach requires knowledge of predominant senses; however, word sense distributions — and predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a meth</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Computing Surveys, 41(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
</authors>
<title>Partof-speech tagging for Twitter: Word clusters and other advances.</title>
<date>2012</date>
<tech>Technical Report CMU-ML-12-107,</tech>
<institution>Machine Learning Department, Carnegie Mellon University.</institution>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, 2012</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, and Nathan Schneider. 2012. Partof-speech tagging for Twitter: Word clusters and other advances. Technical Report CMU-ML-12-107, Machine Learning Department, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Olga Babko-Malaya</author>
<author>Hoa Trang Dang</author>
</authors>
<title>Different sense granularities for different applications.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding,</booktitle>
<pages>49--56</pages>
<location>Boston, USA.</location>
<contexts>
<context position="23280" citStr="Palmer et al., 2004" startWordPosition="3778" endWordPosition="3781">move to a new dataset (Gella et al., to appear) based on text from ukWaC (Ferraresi et al., 2008) and Twitter, and annotated using the Macmillan English Dictionary10 (henceforth “Macmillan”). For the purposes of this research, the choice of Macmillan is significant in that it is a conventional dictionary with sense definitions and examples, but no linking between senses.11 In terms of the original research which gave rise to the sense-tagged dataset, Macmillan was chosen over WordNet for reasons including: (1) the well-documented difficulties of sense tagging with fine-grained WordNet senses (Palmer et al., 2004; Navigli et al., 2007); (2) the regular update cycle of Macmillan (meaning it contains many recently-emerged senses); and (3) the finding in a preliminary sense-tagging task that it better captured Twitter usages than WordNet (and also OntoNotes: Hovy et al. (2006)). The dataset is made up of 20 target nouns which were selected to span the high- to mid-frequency range in both Twitter and the ukWaC corpus, and have at least 3 Macmillan senses. The average sense ambiguity of the 20 target nouns in Macmillan is 5.6 (but 12.3 in WordNet). 100 usages of each target noun were sampled from each of T</context>
</contexts>
<marker>Palmer, Babko-Malaya, Dang, 2004</marker>
<rawString>Martha Palmer, Olga Babko-Malaya, and Hoa Trang Dang. 2004. Different sense granularities for different applications. In Proceedings of the HLT-NAACL 2004 Workshop: 2nd Workshop on Scalable Natural Language Understanding, pages 49–56, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Dirk Geeraerts</author>
<author>Dirk Speelman</author>
</authors>
<title>The automatic identification of lexical variation between language varieties.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="9015" citStr="Peirsman et al. (2010)" startWordPosition="1455" endWordPosition="1458">hy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account. Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a;</context>
</contexts>
<marker>Peirsman, Geeraerts, Speelman, 2010</marker>
<rawString>Yves Peirsman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation between language varieties. Natural Language Engineering, 16(4):469–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
<author>Mark Stevenson</author>
</authors>
<title>Unsupervised domain tuning to improve word sense disambiguation.</title>
<date>2013</date>
<booktitle>In Proc. of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>680--684</pages>
<location>Atlanta, USA.</location>
<contexts>
<context position="2465" citStr="Preiss and Stevenson, 2013" startWordPosition="376" endWordPosition="379">d predominant senses too — vary from corpus to corpus. Therefore, methods for automatically learning predominant senses and sense distributions for specific corpora are required (Koeling et al., 2005; Lapata and Brew, 2004). In this paper, we propose a method which uses topic models to estimate word sense distributions. This method is in principle applicable to all parts of speech, and moreover does not require a parser, a hierarchical sense representation or parallel text. Topic models have been used for WSD in a number of studies (Boyd-Graber et al., 2007; Li et al., 2010; Lau et al., 2012; Preiss and Stevenson, 2013; Cai et al., 2007; Knopp et al., 2013), but our work extends significantly on this earlier work in focusing on the acquisition of prior word sense distributions (and predominant senses). Because of domain differences and the skewed nature of word sense distributions, it is often the case that some senses in a sense inventory will not be attested in a given corpus. A system capable of automatically finding such senses could reduce ambiguity, particularly in domain adaptation settings, while retaining rare but nevertheless viable senses. We further propose a method for applying our sense distri</context>
</contexts>
<marker>Preiss, Stevenson, 2013</marker>
<rawString>Judita Preiss and Mark Stevenson. 2013. Unsupervised domain tuning to improve word sense disambiguation. In Proc. of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 680–684, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Rundell</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Automating the creation of dictionaries: where will it all end?</title>
<date>2011</date>
<booktitle>In Fanny Meunier, Sylvie De Cock, Ga¨etanelle Gilquin, and Magali Paquot, editors, A Taste for Corpora. In honour of Sylviane Granger,</booktitle>
<pages>257--282</pages>
<publisher>John Benjamins,</publisher>
<location>Amsterdam, Netherlands.</location>
<contexts>
<context position="33545" citStr="Rundell and Kilgarriff, 2011" startWordPosition="5505" endWordPosition="5509">e (e.g. if &gt; 60% of usages are a single sense). For example, only 6 of our 20 target nouns have senses which are candidates for highfrequency novel senses. As before, we treat the novel sense identification task as a classification problem, although with a significantly different formulation: we are no longer attempting to identify pre-existing senses, as novel senses are by definition not included in the sense inventory. Instead, we are seeking to identify clusters of usages which are instances of a novel sense, e.g. for presentation to a lexicographer as part of a dictionary update process (Rundell and Kilgarriff, 2011; Cook et al., 2013). That is, for each usage, we want to classify whether it is an instance of a given novel sense. A usage that corresponds to a novel sense should have a topic that does not align well with any of the pre-existing senses in the sense inventory. Based on this intuition, we introduce topicto-sense affinity to estimate the similarity of a topic to the set of senses, as follows: ESZ sim(sZ, tj) ts-affinity(tj) = ET l Ek sim(sk, tl) (4) where, once again, sim(sZ, tj) is defined as in Equation (1), and T and S represent the number of topics and senses, respectively. Using topic-to</context>
</contexts>
<marker>Rundell, Kilgarriff, 2011</marker>
<rawString>Michael Rundell and Adam Kilgarriff. 2011. Automating the creation of dictionaries: where will it all end? In Fanny Meunier, Sylvie De Cock, Ga¨etanelle Gilquin, and Magali Paquot, editors, A Taste for Corpora. In honour of Sylviane Granger, pages 257–282. John Benjamins, Amsterdam, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Sagi</author>
<author>Stefan Kaufmann</author>
<author>Brady Clark</author>
</authors>
<title>Semantic density analysis: Comparing word meaning across time and space.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models of Natural Language Semantics,</booktitle>
<pages>104--111</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="8861" citStr="Sagi et al., 2009" startWordPosition="1428" endWordPosition="1431">t (“corroborators”), each of which, in turn, depends on the document’s topic distribution. Using this approach, they get comparable results to McCarthy et al. when context is ignored (i.e. using a model with one topic), and at most a 1% improvement on SemCor when they use more topics in order to take context into account. Since the results do not improve on McCarthy et al. as regards sense distribution acquisition irrespective of context, we will compare our model with that proposed by McCarthy et al. 260 Recent work on finding novel senses has tended to focus on comparing diachronic corpora (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011) and has also considered topic models (Lau et al., 2012). In a similar vein, Peirsman et al. (2010) considered the identification of words having a sense particular to one language variety with respect to another (specifically Belgian and Netherlandic Dutch). In contrast to these studies, we propose a model for comparing a corpus with a sense inventory. Carpuat et al. (2013) exploit parallel corpora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel </context>
</contexts>
<marker>Sagi, Kaufmann, Clark, 2009</marker>
<rawString>Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2009. Semantic density analysis: Comparing word meaning across time and space. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models of Natural Language Semantics, pages 104– 111, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="9931" citStr="Teh et al. (2006)" startWordPosition="1599" endWordPosition="1602">pora to identify words in domain-specific monolingual corpora with previously-unseen translations; the method we propose does not require parallel data. 3 Methodology Our methodology is based on the WSI system described in Lau et al. (2012),1 which has been shown (Lau et al., 2012; Lau et al., 2013a; Lau et al., 2013b) to achieve state-of-the-art results over the WSI tasks from SemEval-2007 (Agirre and Soroa, 2007), SemEval-2010 (Manandhar et al., 2010) and SemEval-2013 (Navigli and Vannella, 2013; Jurgens and Klapaftis, 2013). The system is built around a Hierarchical Dirichlet Process (HDP: Teh et al. (2006)), a non-parametric variant of a Latent Dirichlet Allocation topic model (Blei et al., 2003) where the model automatically optimises the number of topics in a fully-unsupervised fashion over the training data. To learn the senses of a target lemma, we train a single topic model per target lemma. The system reads in a collection of usages of that lemma, and automatically induces topics (= senses) in the form of a multinomial distribution over words, and per-usage topic assignments (= probabilistic sense assignments) in the form of a multinomial distribution over topics. Following Lau et al. (20</context>
<context position="13106" citStr="Teh et al. (2006)" startWordPosition="2136" endWordPosition="2139">the gloss/definition into a multinomial distribution over words, based on simple maximum likelihood estimation.6 We then calculate the Jensen– Shannon divergence between the multinomial distribution (over words) of the gloss and that of the topic, and convert the divergence value into a similarity score by subtracting it from 1. Formally, the similarity sense si and topic tj is: sim(si, tj) = 1 − JS(SIIT) (1) where S and T are the multinomial distributions 3For hyper-parameters α and γ, we used 0.1 for both. We did not tune the parameters, and opted to use the default parameters introduced in Teh et al. (2006). 4To avoid confusion, we will refer to the HDP-induced topics as topics, and reserve the term sense to denote senses in a sense inventory. 5The code used to learn predominant sense and run all experiments described in this paper is available at: https: //github.com/jhlau/predom_sense. 6Words are tokenised using OpenNLP and lemmatised with Morpha (Minnen et al., 2001). We additionally remove the target lemma, stopwords and words that are less than 3 characters in length. 261 Topic Num Top-10 Terms 1 network support @card@ information research service group development community member 2 servic</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, and David M. Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Nonparametric Bayesian word sense induction.</title>
<date>2011</date>
<booktitle>In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing,</booktitle>
<pages>10--14</pages>
<location>Portland, USA.</location>
<marker>Yao, Van Durme, 2011</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric Bayesian word sense induction. In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10–14, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proc. of the ACL 2010 System Demonstrations,</booktitle>
<pages>78--83</pages>
<location>Uppsala,</location>
<contexts>
<context position="5414" citStr="Zhong and Ng, 2010" startWordPosition="858" endWordPosition="861">of tokens exhibiting the same novel sense. 2 Background and Related Work There has been a considerable amount of research on representing word senses and disambiguating usages of words in context (WSD) as, in order to produce computational systems that understand and produce natural language, it is essential to have a means of representing and disambiguating word sense. WSD algorithms require word sense information to disambiguate token instances of a given ambiguous word, e.g. in the form of sense definitions (Lesk, 1986), semantic relationships (Navigli and Velardi, 2005) or annotated data (Zhong and Ng, 2010). One extremely useful piece of information is the word sense prior or expected word sense frequency distribution. This is important because word sense distributions are typically skewed (Kilgarriff, 2004), and systems do far better when they take bias into account (Agirre and Martinez, 2004). Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al., 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. Due to the expense of hand tagging, and sense distributions being sensitive to domain and genre, there has b</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proc. of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>