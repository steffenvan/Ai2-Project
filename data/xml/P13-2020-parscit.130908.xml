<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000199">
<title confidence="0.888984">
Easy-First POS Tagging and Dependency Parsing with Beam Search
</title>
<author confidence="0.676692">
Ji Ma† JingboZhu† Tong Xiao† Nan Yang‡†Natrual Language Processing Lab., Northeastern University, Shenyang, China
</author>
<affiliation confidence="0.3043155">
‡MOE-MS Key Lab of MCC, University of Science and Technology of China,
Hefei, China
</affiliation>
<email confidence="0.957554">
majineu@outlook.com
{zhujingbo, xiaotong}@mail.neu.edu.cn
nyang.ustc@gmail.com
</email>
<sectionHeader confidence="0.993782" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895857142857">
In this paper, we combine easy-first de-
pendency parsing and POS tagging algo-
rithms with beam search and structured
perceptron. We propose a simple variant
of “early-update” to ensure valid update
in the training process. The proposed so-
lution can also be applied to combine
beam search and structured perceptron
with other systems that exhibit spurious
ambiguity. On CTB, we achieve 94.01%
tagging accuracy and 86.33% unlabeled
attachment score with a relatively small
beam width. On PTB, we also achieve
state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948391304348">
The easy-first dependency parsing algorithm
(Goldberg and Elhadad, 2010) is attractive due to
its good accuracy, fast speed and simplicity. The
easy-first parser has been applied to many appli-
cations (Seeker et al., 2012; Søggard and Wulff,
2012). By processing the input tokens in an easy-
to-hard order, the algorithm could make use of
structured information on both sides of the hard
token thus making more indicative predictions.
However, rich structured information also causes
exhaustive inference intractable. As an alterna-
tive, greedy search which only explores a tiny
fraction of the search space is adopted (Goldberg
and Elhadad, 2010).
To enlarge the search space, a natural exten-
sion to greedy search is beam search. Recent
work also shows that beam search together with
perceptron-based global learning (Collins, 2002)
enable the use of non-local features that are help-
ful to improve parsing performance without
overfitting (Zhang and Nivre, 2012). Due to the-
se advantages, beam search and global learning
has been applied to many NLP tasks (Collins and
</bodyText>
<figureCaption confidence="0.9988475">
Figure 1: Example of cases without/with spurious
ambiguity. The 3 × 1 table denotes a beam. “C/P”
denotes correct/predicted action sequence. The
numbers following C/P are model scores.
</figureCaption>
<bodyText confidence="0.994048052631579">
Roark 2004; Zhang and Clark, 2007). However,
to the best of our knowledge, no work in the lit-
erature has ever applied the two techniques to
easy-first dependency parsing.
While applying beam-search is relatively
straightforward, the main difficulty comes from
combining easy-first dependency parsing with
perceptron-based global learning. In particular,
one needs to guarantee that each parameter up-
date is valid, i.e., the correct action sequence has
lower model score than the predicted one1. The
difficulty in ensuring validity of parameter up-
date for the easy-first algorithm is caused by its
spurious ambiguity, i.e., the same result might be
derived by more than one action sequences.
For algorithms which do not exhibit spurious
ambiguity, “early update” (Collins and Roark
2004) is always valid: at the k-th step when the
single correct action sequence falls off the beam,
</bodyText>
<footnote confidence="0.9716305">
1 As shown by (Huang et al., 2012), only valid update guar-
antees the convergence of any perceptron-based training.
Invalid update may lead to bad learning or even make the
learning not converge at all.
</footnote>
<page confidence="0.911123">
110
</page>
<note confidence="0.6314645">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110–114,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.995486333333333">
Figure 2: An example of parsing “I am valid”. Spu-
rious ambiguity: (d) can be derived by both
[RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)].
</figureCaption>
<bodyText confidence="0.990577846153846">
its model score must be lower than those still in
the beam (as illustrated in figure 1, also see the
proof in (Huang et al., 2012)). While for easy-
first dependency parsing, there could be multiple
action sequences that yield the gold result (C1 and
C2 in figure 1). When all correct sequences fall
off the beam, some may indeed have higher
model score than those still in the beam (C2 in
figure 1), causing invalid update.
For the purpose of valid update, we present a
simple solution which is based on early update.
The basic idea is to use one of the correct action
sequences that were pruned right at the k-th step
(C1 in figure 1) for parameter update.
The proposed solution is general and can also
be applied to other algorithms that exhibit spuri-
ous ambiguity, such as easy-first POS tagging
(Ma et al., 2012) and transition-based dependen-
cy parsing with dynamic oracle (Goldberg and
Nivre, 2012). In this paper, we report experi-
mental results on both easy-first dependency
parsing and POS tagging (Ma et al., 2012). We
show that both easy-first POS tagging and de-
pendency parsing can be improved significantly
from beam search and global learning. Specifi-
cally, on CTB we achieve 94.01% tagging accu-
racy which is the best result to date2 for a single
tagging model. With a relatively small beam, we
achieve 86.33% unlabeled score (assume gold
tags), better than state-of-the-art transition-based
parsers (Huang and Sagae, 2010; Zhang and
Nivre, 2011). On PTB, we also achieve good
results that are comparable to the state-of-the-art.
2 Easy-first dependency parsing
The easy-first dependency parsing algorithm
(Goldberg and Elhadad, 2010) builds a depend-
ency tree by performing two types of actions
LEFT(i) and RIGHT(i) to a list of sub-tree struc-
tures p1,..., pr. pi is initialized with the i-th word
</bodyText>
<listItem confidence="0.473304">
2 Joint tagging-parsing models achieve higher accuracy, but
those models are not directly comparable to ours.
Algorithm 1: Easy-first with beam search
</listItem>
<bodyText confidence="0.9527665">
Input: sentence x of n words, beam width s
Output: one best dependency tree
</bodyText>
<equation confidence="0.9530554">
BEST&apos; (x, f9, w) ° argtopY1EUyEf9EXTeN(y)w - 9(y,)// top s extensions from the beam
1 f90 F [] // initially, empty beam
2 for k E 1 ... n— 1 do
3 f9k F BESTS(x, f9k-1, w)
4 return f9n-1 [0] (x) // tree built by the best sequence
</equation>
<bodyText confidence="0.829653625">
of the input sentence. Action LEFT(i)/RIGHT(i)
attaches pi to its left/right neighbor and then re-
moves pi from the sub-tree list. The algorithm
proceeds until only one sub-tree left which is the
dependency tree of the input sentence (see the
example in figure 2). Each step, the algorithm
chooses the highest score action to perform ac-
cording to the linear model:
</bodyText>
<equation confidence="0.750481">
Score(x) = w - 9(x)
Here, w is the weight vector and 9 is the feature
</equation>
<bodyText confidence="0.99936425">
representation. In particular, 9( LEFT( i)/
RIGHT(i)) denotes features extracted from pi.
The parsing algorithm is greedy which ex-
plores a tiny fraction of the search space. Once
an incorrect action is selected, it can never yield
the correct dependency tree. To enlarge the
search space, we introduce the beam-search ex-
tension in the next section.
</bodyText>
<sectionHeader confidence="0.904405" genericHeader="method">
3 Easy-first with beam search
</sectionHeader>
<bodyText confidence="0.998599909090909">
In this section, we introduce easy-first with beam
search in our own notations that will be used
throughout the rest of this paper.
For a sentence x of n words, let y be the action
(sub-)sequence that can be applied, in sequence,
to x and the result sub-tree list is denoted by
y(x) . For example, suppose x is “I am valid” and
y is [RIGHT(1)], then y(x) yields figure 2(b). Let
Al to be LEFT(i)/RIGHT(i) actions where i E [1 , l].
Thus, the set of all possible one-action extension
of y is:
</bodyText>
<equation confidence="0.663169">
EXTEN(y) ° {y o aja E Ajy(x)j}
</equation>
<bodyText confidence="0.9998072">
Here, ‘ o’ means insert a to the end of y. Follow-
ing (Huang et al., 2012), in order to formalize
beam search, we also use the argtop&apos;,yw - 9(y)
operation which returns the top s action sequenc-
es in Y according to w - 9(y). Here, Ydenotes a
set of action sequences, 9(y) denotes the sum of
feature vectors of each action in y
Pseudo-code of easy-first with beam search is
shown in algorithm 1. Beam search grows s
(beam width) action sequences in parallel using a
</bodyText>
<page confidence="0.992907">
111
</page>
<table confidence="0.8368575">
Algorithm 2: Perceptron-based training over one
training sample (x, t)
Input: (x, t), s, parameter w
Output: new parameter w
(x, f9, w, C) - argmaxy ECn( uYEf9ErTEN(Y)) w • 9(Y,)
// top correct extension from the beam
1 f90F[]
2 for k E 1 ... n- 1 do
</table>
<figure confidence="0.928617875">
3 Y� = ToPc(x, f9k-1, w, C)
4 f9k F BESTs(x, f9k-1, w)
5 if f9k n C = 0 // all correct seq. falls off the beam
6 wFw+9(Y�) -9( f9k[0])
7 break
8 if f9n-1 [0] (x) # t // full update
9 w F w + 9(Y�) - 9(f9n-1[0])
10 return w
</figure>
<bodyText confidence="0.980996285714286">
beam f9, (sequences in f9 are sorted in terms of
model score, i.e., w • 9(f9[0]) &gt; w • 9(f9[1 ]) ...).
At each step, the sequences in f9 are expanded in
all possible ways and then f9 is filled up with the
top s newly expanded sequences (line 2 ~ line 3).
Finally, it returns the dependency tree built by
the top action sequence in f9n-1.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.999928631578948">
To learn the weight vector w, we use the percep-
tron-based global learning3 (Collins, 2002) which
updates w by rewarding the feature weights fired
in the correct action sequence and punish those
fired in the predicted incorrect action sequence.
Current work (Huang et al., 2012) rigorously
explained that only valid update ensures conver-
gence of any perceptron variants. They also justi-
fied that the popular “early update” (Collins and
Roark, 2004) is valid for the systems that do not
exhibit spurious ambiguity4.
However, for the easy-first algorithm or more
generally, systems that exhibit spurious ambigui-
ty, even “early update” could fail to ensure valid-
ity of update (see the example in figure 1). For
validity of update, we propose a simple solution
which is based on “early update” and which can
accommodate spurious ambiguity. The basic idea
is to use the correct action sequence which was
</bodyText>
<footnote confidence="0.856577777777778">
3 Following (Zhang and Nivre, 2012), we say the training
algorithm is global if it optimizes the score of an entire ac-
tion sequence. A local learner trains a classifier which dis-
tinguishes between single actions.
4 As shown in (Goldberg and Nivre 2012), most transition-
based dependency parsers (Nivre et al., 2003; Huang and
Sagae 2010;Zhang and Clark 2008) ignores spurious ambi-
guity by using a static oracle which maps a dependency tree
to a single action sequence.
</footnote>
<note confidence="0.619814">
Features of (Goldberg and Elhadad, 2010)
</note>
<equation confidence="0.967786285714286">
for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp,
tp-vrp, tlcp, trcp, wlcp, wlcp
for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp
for p, q, r in (pi-2, pi-1, pi), (pi- tp-tq-tr, tp-tq-wr
1, pi+1, pi), (pi+1, pi+2 ,pi)
for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,,
tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq
</equation>
<tableCaption confidence="0.536493">
Table 1: Feature templates for English dependency
parsing. wp denotes the head word of p, tp denotes the
POS tag of wp. vlp/vrp denotes the number p’s of
left/right child. lcp/rcp denotes p’s leftmost/rightmost
child. pi denotes partial tree being considered.
</tableCaption>
<bodyText confidence="0.999699571428571">
pruned right at the step when all correct sequence
falls off the beam (as C1 in figure 1).
Algorithm 2 shows the pseudo-code of the
training procedure over one training sample
(x, t), a sentence-tree pair. Here we assume C to
be the set of all correct action sequences/sub-
sequences. At step k, the algorithm constructs a
correct action sequence Y� of length k by extend-
ing those in f9k-1 (line 3). It also checks whether
f9k no longer contains any correct sequence. If so,
Y� together with f9k [0] are used for parameter up-
date (line 5 ~ line 6). It can be easily verified that
each update in line 6 is valid. Note that both
‘TOPC’ and the operation in line 5 use C to check
whether an action sequence y is correct or not.
This can be efficiently implemented (without
explicitly enumerating C ) by checking if each
LEFT(i)/RIGHT(i) in y are compatible with (x, t):
pi already collected all its dependents according
to t; pi is attached to the correct neighbor sug-
gested by t.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999889857142857">
For English, we use PTB as our data set. We use
the standard split for dependency parsing and the
split used by (Ratnaparkhi, 1996) for POS tag-
ging. Penn2Malt5 is used to convert the bracket-
ed structure into dependencies. For dependency
parsing, POS tags of the training set are generat-
ed using 10-fold jack-knifing.
For Chinese, we use CTB 5.1 and the split
suggested by (Duan et al., 2007) for both tagging
and dependency parsing. We also use Penn2Malt
and the head-finding rules of (Zhang and Clark
2008) to convert constituency trees into depend-
encies. For dependency parsing, we assume gold
segmentation and POS tags for the input.
</bodyText>
<footnote confidence="0.88638">
5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html
</footnote>
<page confidence="0.997576">
112
</page>
<bodyText confidence="0.99923175">
Features used in English dependency parsing
are listed in table 1. Besides the features in
(Goldberg and Elhadad, 2010), we also include
some trigram features and valency features
which are useful for transition-based dependency
parsing (Zhang and Nivre, 2011). For English
POS tagging, we use the same features as in
(Shen et al., 2007). For Chinese POS tagging and
dependency parsing, we use the same features as
(Ma et al., 2012). All of our experiments are
conducted on a Core i7 (2.93GHz) machine, both
the tagger and parser are implemented using C++.
</bodyText>
<subsectionHeader confidence="0.999771">
5.1 Effect of beam width
</subsectionHeader>
<bodyText confidence="0.999968333333333">
Tagging/parsing performances with different
beam widths on the development set are listed in
table 2 and table 3. We can see that Chinese POS
tagging, dependency parsing as well as English
dependency parsing greatly benefit from beam
search. While tagging accuracy on English only
slightly improved. This may because that the
accuracy of the greedy baseline tagger is already
very high and it is hard to get further improve-
ment. Table 2 and table 3 also show that the
speed of both tagging and dependency parsing
drops linearly with the growth of beam width.
</bodyText>
<subsectionHeader confidence="0.99936">
5.2 Final results
</subsectionHeader>
<bodyText confidence="0.99998365">
Tagging results on the test set together with some
previous results are listed in table 4. Dependency
parsing results on CTB and PTB are listed in ta-
ble 5 and table 6, respectively.
On CTB, tagging accuracy of our greedy base-
line is already comparable to the state-of-the-art.
As the beam size grows to 5, tagging accuracy
increases to 94.01% which is 2.3% error reduc-
tion. This is also the best tagging accuracy com-
paring with previous single tagging models (For
limited space, we do not list the performance of
joint tagging-parsing models).
Parsing performances on both PTB and CTB
are significantly improved with a relatively small
beam width (s = 8). In particular, we achieve
86.33% uas on CTB which is 1.54% uas im-
provement over the greedy baseline parser.
Moreover, the performance is better than the best
transition-based parser (Zhang and Nivre, 2011)
which adopts a much larger beam width (s = 64).
</bodyText>
<sectionHeader confidence="0.968465" genericHeader="conclusions">
6 Conclusion and related work
</sectionHeader>
<bodyText confidence="0.999775">
This work directly extends (Goldberg and El-
hadad, 2010) with beam search and global learn-
ing. We show that both the easy-first POS tagger
and dependency parser can be significantly impr-
</bodyText>
<table confidence="0.9987685">
s PTB CTB speed
1 97.17 93.91 1350
3 97.20 94.15 560
5 97.22 94.17 385
</table>
<tableCaption confidence="0.986110666666667">
Table 2: Tagging accuracy vs beam width vs. Speed is
evaluated using the number of sentences that can be
processed in one second
</tableCaption>
<table confidence="0.9996345">
s PTB CTB speed
uas compl uas compl
1 91.77 45.29 84.54 33.75 221
2 92.29 46.28 85.11 34.62 124
4 92.50 46.82 85.62 37.11 71
8 92.74 48.12 86.00 35.87 39
</table>
<tableCaption confidence="0.986310333333333">
Table 3: Parsing accuracy vs beam width. ‘uas’ and
‘compl’ denote unlabeled score and complete match
rate respectively (all excluding punctuations).
</tableCaption>
<table confidence="0.999743833333333">
PTB CTB
(Collins, 2002) 97.11 (Hatori et al., 2012) 93.82
(Shen et al., 2007) 97.33 (Li et al., 2012) 93.88
(Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84
this work 1 97.22 this work 1 93.87
this work 97.28 this work 94.011
</table>
<tableCaption confidence="0.913891">
Table 4: Tagging results on the test set. ‘†’ denotes
statistically significant over the greedy baseline by
</tableCaption>
<table confidence="0.997966357142857">
McNemar’s test ( )
Systems s uas compl
(Huang and Sagae, 2010) 8 85.20 33.72
(Zhang and Nivre, 2011) 64 86.00 36.90
(Li et al., 2012) - 86.55 -
this work 1 84.79 32.98
this work 8 86.331 36.13
Table 5: Parsing results on CTB test set.
Systems s uas compl
(Huang and Sagae, 2010) 8 92.10 -
(Zhang and Nivre, 2011) 64 92.90 48.50
(Koo and Collins, 2010) - 93.04 -
this work 1 91.72 44.04
this work 8 92.471 46.07
</table>
<tableCaption confidence="0.99913">
Table 6: Parsing results on PTB test set.
</tableCaption>
<bodyText confidence="0.991004555555556">
oved using beam search and global learning.
This work can also be considered as applying
(Huang et al., 2012) to the systems that exhibit
spurious ambiguity. One future direction might
be to apply the training method to transition-
based parsers with dynamic oracle (Goldberg and
Nivre, 2012) and potentially further advance per-
formances of state-of-the-art transition-based
parsers.
</bodyText>
<page confidence="0.997406">
113
</page>
<bodyText confidence="0.999639888888889">
Shen et al., (2007) and (Shen and Joshi, 2008)
also proposed bi-directional sequential classifica-
tion with beam search for POS tagging and
LTAG dependency parsing, respectively. The
main difference is that their training method aims
to learn a classifier which distinguishes between
each local action while our training method aims
to distinguish between action sequences. Our
method can also be applied to their framework.
</bodyText>
<sectionHeader confidence="0.997545" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999968181818182">
We would like to thank Yue Zhang, Yoav Gold-
berg and Zhenghua Li for discussions and sug-
gestions on earlier drift of this paper. We would
also like to thank the three anonymous reviewers
for their suggestions. This work was supported in
part by the National Science Foundation of Chi-
na (61073140; 61272376), Specialized Research
Fund for the Doctoral Program of Higher Educa-
tion (20100042110031) and the Fundamental
Research Funds for the Central Universities
(N100204002).
</bodyText>
<sectionHeader confidence="0.9991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852615384615">
Collins, M. 2002. Discriminative training methods for
hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of
EMNLP.
Duan, X., Zhao, J., , and Xu, B. 2007. Probabilistic
models for action-based Chinese dependency pars-
ing. In Proceedings of ECML/ECPPKDD.
Goldberg, Y. and Elhadad, M. 2010 An Efficient Al-
gorithm for Eash-First Non-Directional Dependen-
cy Parsing. In Proceedings of NAACL
Huang, L. and Sagae, K. 2010. Dynamic program-
ming for linear-time incremental parsing. In Pro-
ceedings of ACL.
Huang, L. Fayong, S. and Guo, Y. 2012. Structured
Perceptron with Inexact Search. In Proceedings of
NAACL.
Koo, T. and Collins, M. 2010. Efficient third-order
dependency parsers. In Proceedings of ACL.
Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W.
2012. A Separately Passive-Aggressive Training
Algorithm for Joint POS Tagging and Dependency
Parsing. In Proceedings of COLING
Ma, J., Xiao, T., Zhu, J. and Ren, F. 2012. Easy-First
Chinese POS Tagging and Dependency Parsing. In
Proceedings of COLING
Rataparkhi, A. (1996) A Maximum Entropy Part-Of-
Speech Tagger. In Proceedings of EMNLP
Shen, L., Satt, G. and Joshi, A. K. (2007) Guided
Learning for Bidirectional Sequence Classification.
In Proceedings of ACL.
Shen, L. and Josh, A. K. 2008. LTAG Dependency
Parsing with Bidirectional Incremental Construc-
tion. In Proceedings of EMNLP.
Seeker, W., Farkas, R. and Bohnet, B. 2012 Data-
driven Dependency Parsing With Empty Heads. In
Proceedings of COLING
Søggard, A. and Wulff, J. 2012. An Empirical Study
of Non-lexical Extensions to Delexicalized Trans-
fer. In Proceedings of COLING
Yue Zhang and Stephen Clark. 2007 Chinese Seg-
mentation Using a Word-based Perceptron Algo-
rithm. In Proceedings of ACL.
Zhang, Y. and Clark, S. 2008. Joint word segmenta-
tion and POS tagging using a single perceptron. In
Proceedings of ACL.
Zhang, Y. and Nivre, J. 2011. Transition-based de-
pendency parsing with rich non-local features. In
Proceedings of ACL.
Zhang, Y. and Nivre, J. 2012. Analyzing the Effect of
Global Learning and Beam-Search for Transition-
Based Dependency Parsing. In Proceedings of
COLING.
</reference>
<page confidence="0.998648">
114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.154287">
<title confidence="0.751426">Easy-First POS Tagging and Dependency Parsing with Beam Search</title>
<author confidence="0.548435">Tong Nan Language Processing Lab</author>
<author confidence="0.548435">Northeastern University</author>
<author confidence="0.548435">Shenyang</author>
<keyword confidence="0.411117">Key Lab of MCC, University of Science and Technology of</keyword>
<affiliation confidence="0.82238">Hefei,</affiliation>
<email confidence="0.932335333333333">majineu@outlook.com{zhujingbo,nyang.ustc@gmail.com</email>
<abstract confidence="0.9990238">In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1728" citStr="Collins, 2002" startWordPosition="252" endWordPosition="253"> 2012; Søggard and Wulff, 2012). By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Figure 1: Example of cases without/with spurious ambiguity. The 3 × 1 table denotes a beam. “C/P” denotes correct/predicted action sequence. The numbers following C/P are model scores. Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While ap</context>
<context position="8542" citStr="Collins, 2002" startWordPosition="1445" endWordPosition="1446">-1, w) 5 if f9k n C = 0 // all correct seq. falls off the beam 6 wFw+9(Y�) -9( f9k[0]) 7 break 8 if f9n-1 [0] (x) # t // full update 9 w F w + 9(Y�) - 9(f9n-1[0]) 10 return w beam f9, (sequences in f9 are sorted in terms of model score, i.e., w • 9(f9[0]) &gt; w • 9(f9[1 ]) ...). At each step, the sequences in f9 are expanded in all possible ways and then f9 is filled up with the top s newly expanded sequences (line 2 ~ line 3). Finally, it returns the dependency tree built by the top action sequence in f9n-1. 4 Training To learn the weight vector w, we use the perceptron-based global learning3 (Collins, 2002) which updates w by rewarding the feature weights fired in the correct action sequence and punish those fired in the predicted incorrect action sequence. Current work (Huang et al., 2012) rigorously explained that only valid update ensures convergence of any perceptron variants. They also justified that the popular “early update” (Collins and Roark, 2004) is valid for the systems that do not exhibit spurious ambiguity4. However, for the easy-first algorithm or more generally, systems that exhibit spurious ambiguity, even “early update” could fail to ensure validity of update (see the example i</context>
<context position="14944" citStr="Collins, 2002" startWordPosition="2529" endWordPosition="2530">show that both the easy-first POS tagger and dependency parser can be significantly imprs PTB CTB speed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Duan</author>
<author>J Zhao</author>
</authors>
<title>Probabilistic models for action-based Chinese dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of ECML/ECPPKDD.</booktitle>
<marker>Duan, Zhao, 2007</marker>
<rawString>Duan, X., Zhao, J., , and Xu, B. 2007. Probabilistic models for action-based Chinese dependency parsing. In Proceedings of ECML/ECPPKDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>An Efficient Algorithm for Eash-First Non-Directional Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="971" citStr="Goldberg and Elhadad, 2010" startWordPosition="131" endWordPosition="134">ract In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1 Introduction The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. The easy-first parser has been applied to many applications (Seeker et al., 2012; Søggard and Wulff, 2012). By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space</context>
<context position="5206" citStr="Goldberg and Elhadad, 2010" startWordPosition="812" endWordPosition="815">12). We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date2 for a single tagging model. With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011). On PTB, we also achieve good results that are comparable to the state-of-the-art. 2 Easy-first dependency parsing The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions LEFT(i) and RIGHT(i) to a list of sub-tree structures p1,..., pr. pi is initialized with the i-th word 2 Joint tagging-parsing models achieve higher accuracy, but those models are not directly comparable to ours. Algorithm 1: Easy-first with beam search Input: sentence x of n words, beam width s Output: one best dependency tree BEST&apos; (x, f9, w) ° argtopY1EUyEf9EXTeN(y)w - 9(y,)// top s extensions from the beam 1 f90 F [] // initially, empty beam 2 for k E 1 ... n— 1 do 3 f9k F BESTS(x, f9k-1, w) 4 return f9n-1 [0] (x) // tree built b</context>
<context position="9858" citStr="Goldberg and Elhadad, 2010" startWordPosition="1659" endWordPosition="1662">y update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp, tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp for p, q, r in (pi-2, pi-1, pi), (pi- tp-tq-tr, tp-tq-wr 1, pi+1, pi), (pi+1, pi+2 ,pi) for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,, tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq Table 1: Feature templates for English dependency parsing. wp denotes the head word of p, tp denotes the POS tag of wp. vlp/vrp denotes the number p’s of left/right child. lcp/rcp denotes p’s leftmost/rightmost child. pi denotes partial tree being considered. pruned ri</context>
<context position="12253" citStr="Goldberg and Elhadad, 2010" startWordPosition="2065" endWordPosition="2068">keted structure into dependencies. For dependency parsing, POS tags of the training set are generated using 10-fold jack-knifing. For Chinese, we use CTB 5.1 and the split suggested by (Duan et al., 2007) for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 112 Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Effect of beam width Tagging/parsing performances with different beam widths on the development set are listed in table 2 and table 3. We can see that Chinese</context>
<context position="14288" citStr="Goldberg and Elhadad, 2010" startWordPosition="2408" endWordPosition="2412">tion. This is also the best tagging accuracy comparing with previous single tagging models (For limited space, we do not list the performance of joint tagging-parsing models). Parsing performances on both PTB and CTB are significantly improved with a relatively small beam width (s = 8). In particular, we achieve 86.33% uas on CTB which is 1.54% uas improvement over the greedy baseline parser. Moreover, the performance is better than the best transition-based parser (Zhang and Nivre, 2011) which adopts a much larger beam width (s = 64). 6 Conclusion and related work This work directly extends (Goldberg and Elhadad, 2010) with beam search and global learning. We show that both the easy-first POS tagger and dependency parser can be significantly imprs PTB CTB speed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respective</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Goldberg, Y. and Elhadad, M. 2010 An Efficient Algorithm for Eash-First Non-Directional Dependency Parsing. In Proceedings of NAACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4994" citStr="Huang and Sagae, 2010" startWordPosition="782" endWordPosition="785">12) and transition-based dependency parsing with dynamic oracle (Goldberg and Nivre, 2012). In this paper, we report experimental results on both easy-first dependency parsing and POS tagging (Ma et al., 2012). We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date2 for a single tagging model. With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011). On PTB, we also achieve good results that are comparable to the state-of-the-art. 2 Easy-first dependency parsing The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions LEFT(i) and RIGHT(i) to a list of sub-tree structures p1,..., pr. pi is initialized with the i-th word 2 Joint tagging-parsing models achieve higher accuracy, but those models are not directly comparable to ours. Algorithm 1: Easy-first with beam search Input: sentence x of n words, beam width s Output: one best dependency tr</context>
<context position="9685" citStr="Huang and Sagae 2010" startWordPosition="1631" endWordPosition="1634">ven “early update” could fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp, tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp for p, q, r in (pi-2, pi-1, pi), (pi- tp-tq-tr, tp-tq-wr 1, pi+1, pi), (pi+1, pi+2 ,pi) for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,, tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq Table 1: Feature templates for English dependency parsing. wp denotes the head word of p, tp den</context>
<context position="15319" citStr="Huang and Sagae, 2010" startWordPosition="2596" endWordPosition="2599"> 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Collins, 2010) - 93.04 - this work 1 91.72 44.04 this work 8 92.471 46.07 Table 6: Parsing results on PTB test set. oved using beam search and global learning. This work can also be considered as applying (Huang et al., 2012) to the systems that exhibit spurious ambiguity. One future direction might be to apply the training</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Huang, L. and Sagae, K. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fayong Huang</author>
<author>S</author>
<author>Y Guo</author>
</authors>
<title>Structured Perceptron with Inexact Search.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="3064" citStr="Huang et al., 2012" startWordPosition="462" endWordPosition="465">ing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1. The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same result might be derived by more than one action sequences. For algorithms which do not exhibit spurious ambiguity, “early update” (Collins and Roark 2004) is always valid: at the k-th step when the single correct action sequence falls off the beam, 1 As shown by (Huang et al., 2012), only valid update guarantees the convergence of any perceptron-based training. Invalid update may lead to bad learning or even make the learning not converge at all. 110 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110–114, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 2: An example of parsing “I am valid”. Spurious ambiguity: (d) can be derived by both [RIGHT(1), LEFT(2)] and [LEFT(3), RIGHT(1)]. its model score must be lower than those still in the beam (as illustrated in figure 1, also see the pr</context>
<context position="7235" citStr="Huang et al., 2012" startWordPosition="1183" endWordPosition="1186">first with beam search In this section, we introduce easy-first with beam search in our own notations that will be used throughout the rest of this paper. For a sentence x of n words, let y be the action (sub-)sequence that can be applied, in sequence, to x and the result sub-tree list is denoted by y(x) . For example, suppose x is “I am valid” and y is [RIGHT(1)], then y(x) yields figure 2(b). Let Al to be LEFT(i)/RIGHT(i) actions where i E [1 , l]. Thus, the set of all possible one-action extension of y is: EXTEN(y) ° {y o aja E Ajy(x)j} Here, ‘ o’ means insert a to the end of y. Following (Huang et al., 2012), in order to formalize beam search, we also use the argtop&apos;,yw - 9(y) operation which returns the top s action sequences in Y according to w - 9(y). Here, Ydenotes a set of action sequences, 9(y) denotes the sum of feature vectors of each action in y Pseudo-code of easy-first with beam search is shown in algorithm 1. Beam search grows s (beam width) action sequences in parallel using a 111 Algorithm 2: Perceptron-based training over one training sample (x, t) Input: (x, t), s, parameter w Output: new parameter w (x, f9, w, C) - argmaxy ECn( uYEf9ErTEN(Y)) w • 9(Y,) // top correct extension fr</context>
<context position="8729" citStr="Huang et al., 2012" startWordPosition="1473" endWordPosition="1476">, (sequences in f9 are sorted in terms of model score, i.e., w • 9(f9[0]) &gt; w • 9(f9[1 ]) ...). At each step, the sequences in f9 are expanded in all possible ways and then f9 is filled up with the top s newly expanded sequences (line 2 ~ line 3). Finally, it returns the dependency tree built by the top action sequence in f9n-1. 4 Training To learn the weight vector w, we use the perceptron-based global learning3 (Collins, 2002) which updates w by rewarding the feature weights fired in the correct action sequence and punish those fired in the predicted incorrect action sequence. Current work (Huang et al., 2012) rigorously explained that only valid update ensures convergence of any perceptron variants. They also justified that the popular “early update” (Collins and Roark, 2004) is valid for the systems that do not exhibit spurious ambiguity4. However, for the easy-first algorithm or more generally, systems that exhibit spurious ambiguity, even “early update” could fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct actio</context>
<context position="15049" citStr="Huang et al., 2012" startWordPosition="2547" endWordPosition="2550">eed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Collins, 2010) - 93.04 - this work 1 91.72 44.04 this w</context>
</contexts>
<marker>Huang, S, Guo, 2012</marker>
<rawString>Huang, L. Fayong, S. and Guo, Y. 2012. Structured Perceptron with Inexact Search. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="15608" citStr="Koo and Collins, 2010" startWordPosition="2653" endWordPosition="2656">t al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Collins, 2010) - 93.04 - this work 1 91.72 44.04 this work 8 92.471 46.07 Table 6: Parsing results on PTB test set. oved using beam search and global learning. This work can also be considered as applying (Huang et al., 2012) to the systems that exhibit spurious ambiguity. One future direction might be to apply the training method to transitionbased parsers with dynamic oracle (Goldberg and Nivre, 2012) and potentially further advance performances of state-of-the-art transition-based parsers. 113 Shen et al., (2007) and (Shen and Joshi, 2008) also proposed bi-directional sequential classification with beam </context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Koo, T. and Collins, M. 2010. Efficient third-order dependency parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Li</author>
<author>M Zhang</author>
<author>W Che</author>
<author>T Liu</author>
<author>W Chen</author>
</authors>
<title>A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="15022" citStr="Li et al., 2012" startWordPosition="2542" endWordPosition="2545">icantly imprs PTB CTB speed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Collins, 2010) - 93.04 - thi</context>
</contexts>
<marker>Li, Zhang, Che, Liu, Chen, 2012</marker>
<rawString>Li, Z., Zhang, M., Che, W., Liu, T. and Chen, W. 2012. A Separately Passive-Aggressive Training Algorithm for Joint POS Tagging and Dependency Parsing. In Proceedings of COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ma</author>
<author>T Xiao</author>
<author>J Zhu</author>
<author>F Ren</author>
</authors>
<title>Easy-First Chinese POS Tagging and Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="4376" citStr="Ma et al., 2012" startWordPosition="685" endWordPosition="688">tion sequences that yield the gold result (C1 and C2 in figure 1). When all correct sequences fall off the beam, some may indeed have higher model score than those still in the beam (C2 in figure 1), causing invalid update. For the purpose of valid update, we present a simple solution which is based on early update. The basic idea is to use one of the correct action sequences that were pruned right at the k-th step (C1 in figure 1) for parameter update. The proposed solution is general and can also be applied to other algorithms that exhibit spurious ambiguity, such as easy-first POS tagging (Ma et al., 2012) and transition-based dependency parsing with dynamic oracle (Goldberg and Nivre, 2012). In this paper, we report experimental results on both easy-first dependency parsing and POS tagging (Ma et al., 2012). We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date2 for a single tagging model. With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers (Hua</context>
<context position="12566" citStr="Ma et al., 2012" startWordPosition="2117" endWordPosition="2120">convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 112 Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Effect of beam width Tagging/parsing performances with different beam widths on the development set are listed in table 2 and table 3. We can see that Chinese POS tagging, dependency parsing as well as English dependency parsing greatly benefit from beam search. While tagging accuracy on English only slightly improved. This may because that the accuracy of the greedy baseline tagger is already very high and it is hard to get further improvement. Table 2 and table 3 a</context>
<context position="15073" citStr="Ma et al., 2012" startWordPosition="2552" endWordPosition="2555">.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Collins, 2010) - 93.04 - this work 1 91.72 44.04 this work 8 92.471 46.07 Table</context>
</contexts>
<marker>Ma, Xiao, Zhu, Ren, 2012</marker>
<rawString>Ma, J., Xiao, T., Zhu, J. and Ren, F. 2012. Easy-First Chinese POS Tagging and Dependency Parsing. In Proceedings of COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rataparkhi</author>
</authors>
<title>A Maximum Entropy Part-OfSpeech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Rataparkhi, 1996</marker>
<rawString>Rataparkhi, A. (1996) A Maximum Entropy Part-OfSpeech Tagger. In Proceedings of EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>G Satt</author>
<author>A K Joshi</author>
</authors>
<title>Guided Learning for Bidirectional Sequence Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="12471" citStr="Shen et al., 2007" startWordPosition="2100" endWordPosition="2103">ependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 112 Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Effect of beam width Tagging/parsing performances with different beam widths on the development set are listed in table 2 and table 3. We can see that Chinese POS tagging, dependency parsing as well as English dependency parsing greatly benefit from beam search. While tagging accuracy on English only slightly improved. This may because that the accuracy of the greedy baseli</context>
<context position="14998" citStr="Shen et al., 2007" startWordPosition="2537" endWordPosition="2540">dency parser can be significantly imprs PTB CTB speed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.12 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB CTB (Collins, 2002) 97.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Coll</context>
</contexts>
<marker>Shen, Satt, Joshi, 2007</marker>
<rawString>Shen, L., Satt, G. and Joshi, A. K. (2007) Guided Learning for Bidirectional Sequence Classification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>A K Josh</author>
</authors>
<title>LTAG Dependency Parsing with Bidirectional Incremental Construction.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Shen, Josh, 2008</marker>
<rawString>Shen, L. and Josh, A. K. 2008. LTAG Dependency Parsing with Bidirectional Incremental Construction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Seeker</author>
<author>R Farkas</author>
<author>B Bohnet</author>
</authors>
<title>Datadriven Dependency Parsing With Empty Heads.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1119" citStr="Seeker et al., 2012" startWordPosition="156" endWordPosition="159">ant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1 Introduction The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. The easy-first parser has been applied to many applications (Seeker et al., 2012; Søggard and Wulff, 2012). By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Colli</context>
</contexts>
<marker>Seeker, Farkas, Bohnet, 2012</marker>
<rawString>Seeker, W., Farkas, R. and Bohnet, B. 2012 Datadriven Dependency Parsing With Empty Heads. In Proceedings of COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Søggard</author>
<author>J Wulff</author>
</authors>
<title>An Empirical Study of Non-lexical Extensions to Delexicalized Transfer.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="1145" citStr="Søggard and Wulff, 2012" startWordPosition="160" endWordPosition="163"> to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1 Introduction The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. The easy-first parser has been applied to many applications (Seeker et al., 2012; Søggard and Wulff, 2012). By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use o</context>
</contexts>
<marker>Søggard, Wulff, 2012</marker>
<rawString>Søggard, A. and Wulff, J. 2012. An Empirical Study of Non-lexical Extensions to Delexicalized Transfer. In Proceedings of COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese Segmentation Using a Word-based Perceptron Algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2183" citStr="Zhang and Clark, 2007" startWordPosition="323" endWordPosition="326">arch space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Figure 1: Example of cases without/with spurious ambiguity. The 3 × 1 table denotes a beam. “C/P” denotes correct/predicted action sequence. The numbers following C/P are model scores. Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1. The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same re</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007 Chinese Segmentation Using a Word-based Perceptron Algorithm. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>Joint word segmentation and POS tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9707" citStr="Zhang and Clark 2008" startWordPosition="1634" endWordPosition="1637">uld fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp, tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp for p, q, r in (pi-2, pi-1, pi), (pi- tp-tq-tr, tp-tq-wr 1, pi+1, pi), (pi+1, pi+2 ,pi) for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,, tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq Table 1: Feature templates for English dependency parsing. wp denotes the head word of p, tp denotes the POS tag of wp</context>
<context position="11946" citStr="Zhang and Clark 2008" startWordPosition="2023" endWordPosition="2026">collected all its dependents according to t; pi is attached to the correct neighbor suggested by t. 5 Experiments For English, we use PTB as our data set. We use the standard split for dependency parsing and the split used by (Ratnaparkhi, 1996) for POS tagging. Penn2Malt5 is used to convert the bracketed structure into dependencies. For dependency parsing, POS tags of the training set are generated using 10-fold jack-knifing. For Chinese, we use CTB 5.1 and the split suggested by (Duan et al., 2007) for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 112 Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features </context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Zhang, Y. and Clark, S. 2008. Joint word segmentation and POS tagging using a single perceptron. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5018" citStr="Zhang and Nivre, 2011" startWordPosition="786" endWordPosition="789">d dependency parsing with dynamic oracle (Goldberg and Nivre, 2012). In this paper, we report experimental results on both easy-first dependency parsing and POS tagging (Ma et al., 2012). We show that both easy-first POS tagging and dependency parsing can be improved significantly from beam search and global learning. Specifically, on CTB we achieve 94.01% tagging accuracy which is the best result to date2 for a single tagging model. With a relatively small beam, we achieve 86.33% unlabeled score (assume gold tags), better than state-of-the-art transition-based parsers (Huang and Sagae, 2010; Zhang and Nivre, 2011). On PTB, we also achieve good results that are comparable to the state-of-the-art. 2 Easy-first dependency parsing The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions LEFT(i) and RIGHT(i) to a list of sub-tree structures p1,..., pr. pi is initialized with the i-th word 2 Joint tagging-parsing models achieve higher accuracy, but those models are not directly comparable to ours. Algorithm 1: Easy-first with beam search Input: sentence x of n words, beam width s Output: one best dependency tree BEST&apos; (x, f9, w) ° ar</context>
<context position="12394" citStr="Zhang and Nivre, 2011" startWordPosition="2085" endWordPosition="2088">use CTB 5.1 and the split suggested by (Duan et al., 2007) for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 112 Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Effect of beam width Tagging/parsing performances with different beam widths on the development set are listed in table 2 and table 3. We can see that Chinese POS tagging, dependency parsing as well as English dependency parsing greatly benefit from beam search. While tagging accuracy on English on</context>
<context position="14154" citStr="Zhang and Nivre, 2011" startWordPosition="2385" endWordPosition="2388">y comparable to the state-of-the-art. As the beam size grows to 5, tagging accuracy increases to 94.01% which is 2.3% error reduction. This is also the best tagging accuracy comparing with previous single tagging models (For limited space, we do not list the performance of joint tagging-parsing models). Parsing performances on both PTB and CTB are significantly improved with a relatively small beam width (s = 8). In particular, we achieve 86.33% uas on CTB which is 1.54% uas improvement over the greedy baseline parser. Moreover, the performance is better than the best transition-based parser (Zhang and Nivre, 2011) which adopts a much larger beam width (s = 64). 6 Conclusion and related work This work directly extends (Goldberg and Elhadad, 2010) with beam search and global learning. We show that both the easy-first POS tagger and dependency parser can be significantly imprs PTB CTB speed 1 97.17 93.91 1350 3 97.20 94.15 560 5 97.22 94.17 385 Table 2: Tagging accuracy vs beam width vs. Speed is evaluated using the number of sentences that can be processed in one second s PTB CTB speed uas compl uas compl 1 91.77 45.29 84.54 33.75 221 2 92.29 46.28 85.11 34.62 124 4 92.50 46.82 85.62 37.11 71 8 92.74 48.</context>
<context position="15569" citStr="Zhang and Nivre, 2011" startWordPosition="2646" endWordPosition="2649">.11 (Hatori et al., 2012) 93.82 (Shen et al., 2007) 97.33 (Li et al., 2012) 93.88 (Huang et al., 2012) 97.35 (Ma et al., 2012) 93.84 this work 1 97.22 this work 1 93.87 this work 97.28 this work 94.011 Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems s uas compl (Huang and Sagae, 2010) 8 85.20 33.72 (Zhang and Nivre, 2011) 64 86.00 36.90 (Li et al., 2012) - 86.55 - this work 1 84.79 32.98 this work 8 86.331 36.13 Table 5: Parsing results on CTB test set. Systems s uas compl (Huang and Sagae, 2010) 8 92.10 - (Zhang and Nivre, 2011) 64 92.90 48.50 (Koo and Collins, 2010) - 93.04 - this work 1 91.72 44.04 this work 8 92.471 46.07 Table 6: Parsing results on PTB test set. oved using beam search and global learning. This work can also be considered as applying (Huang et al., 2012) to the systems that exhibit spurious ambiguity. One future direction might be to apply the training method to transitionbased parsers with dynamic oracle (Goldberg and Nivre, 2012) and potentially further advance performances of state-of-the-art transition-based parsers. 113 Shen et al., (2007) and (Shen and Joshi, 2008) also proposed bi-direction</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Zhang, Y. and Nivre, J. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Analyzing the Effect of Global Learning and Beam-Search for TransitionBased Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="1857" citStr="Zhang and Nivre, 2012" startWordPosition="270" endWordPosition="273">structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Figure 1: Example of cases without/with spurious ambiguity. The 3 × 1 table denotes a beam. “C/P” denotes correct/predicted action sequence. The numbers following C/P are model scores. Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with per</context>
<context position="9385" citStr="Zhang and Nivre, 2012" startWordPosition="1580" endWordPosition="1583">lid update ensures convergence of any perceptron variants. They also justified that the popular “early update” (Collins and Roark, 2004) is valid for the systems that do not exhibit spurious ambiguity4. However, for the easy-first algorithm or more generally, systems that exhibit spurious ambiguity, even “early update” could fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp, tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp</context>
</contexts>
<marker>Zhang, Nivre, 2012</marker>
<rawString>Zhang, Y. and Nivre, J. 2012. Analyzing the Effect of Global Learning and Beam-Search for TransitionBased Dependency Parsing. In Proceedings of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>