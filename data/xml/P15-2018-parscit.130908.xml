<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002380">
<title confidence="0.998908">
A Distributed Representation Based Query Expansion Approach for
Image Captioning
</title>
<author confidence="0.998961">
Semih Yagcioglu1 Erkut Erdem1 Aykut Erdem1 Ruket C¸ akıcı2
</author>
<affiliation confidence="0.9995505">
1 Hacettepe University Computer Vision Lab (HUCVL)
Dept. of Computer Engineering, Hacettepe University, Ankara, TURKEY
</affiliation>
<email confidence="0.950831">
semih.yagcioglu@hacettepe.edu.tr, {erkut,aykut}@cs.hacettepe.edu.tr
</email>
<affiliation confidence="0.825903">
2 Dept. of Computer Engineering, Middle East Technical University, Ankara, TURKEY
</affiliation>
<email confidence="0.992838">
ruken@ceng.metu.edu.tr
</email>
<sectionHeader confidence="0.993767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999862866666667">
In this paper, we propose a novel query ex-
pansion approach for improving transfer-
based automatic image captioning. The
core idea of our method is to translate the
given visual query into a distributional se-
mantics based form, which is generated
by the average of the sentence vectors ex-
tracted from the captions of images visu-
ally similar to the input image. Using three
image captioning benchmark datasets, we
show that our approach provides more ac-
curate results compared to the state-of-the-
art data-driven methods in terms of both
automatic metrics and subjective evalua-
tion.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928841269842">
Automatic image captioning is a fast growing area
of research which lies at the intersection of com-
puter vision and natural language processing and
refers to the problem of generating natural lan-
guage descriptions from images. In the literature,
there are a variety of image captioning models that
can be categorized into three main groups as sum-
marized below.
The first line of approaches attempts to gener-
ate novel captions directly from images (Farhadi
et al., 2010; Kulkarni et al., 2011; Mitchell et
al., 2012). Specifically, they borrow techniques
from computer vision such as object detectors and
scene/attribute classifiers, exploit their outputs to
extract the visual content of the input image and
then generate the caption through surface realiza-
tion. More recently, a particular set of generative
approaches have emerged over the last few years,
which depends on deep neural networks (Chen
and Zitnick., 2015; Karpathy and Fei-Fei, 2015;
Xu et al., 2015; Vinyals et al., 2015). In gen-
eral, these studies combine convolutional neural
networks (CNNs) with recurrent neural networks
(RNNs) to generate a description for a given im-
age.
The studies in the second group aim at learning
joint representations of images and captions (Ho-
dosh et al., 2013; Socher et al., 2014; Karpathy et
al., 2014). They employ certain machine learning
techniques to form a common embedding space
for the visual and textual data, and perform cross-
modal (image-sentence) retrieval in that interme-
diate space to accordingly score and rank the pool
of captions to find the most proper caption for a
given image.
The last group of works, on the other hand,
follows a data-driven approach and treats image
captioning as a caption transfer problem (Ordonez
et al., 2011; Kuznetsova et al., 2012; Patterson
et al., 2014; Mason and Charniak, 2014). For a
given image, these methods first search for visu-
ally similar images and then use the captions of the
retrieved images to provide a description, which
makes them much easier to implement compared
to the other two classes of approaches.
The success of these data-driven approaches de-
pends directly on the amount of data available and
the quality of the retrieval set. Clearly, the im-
age features and the corresponding similarity mea-
sures used in retrieval play a significant role here
but, as investigated in (Berg et al., 2012), what
makes this particularly difficult is that while de-
scribing an image humans do not explicitly men-
tion every detail. That is, some parts of an image
are more salient than the others. Hence, one also
needs to bridge the semantic gap between what is
there in the image and what people say when de-
scribing it.
As a step towards achieving this goal, in this pa-
per, we introduce a novel automatic query expan-
sion approach for image captioning to retrieve se-
mantically more relevant captions. As illustrated
in Fig. 1, we swap modalities at our query expan-
</bodyText>
<page confidence="0.872893">
106
</page>
<note confidence="0.272036">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 106–111,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.998870357142857">
transferred caption
c1: A man climbs up a snowy
mountain.
c2: A boy in orange jacket
appears unhappy.
É
c1: A man climbs up a snowy
mountain.
c2: A boy in orange jacket
appears unhappy.
: A person wearing a red
jacket climbs a snowy hill.
Query image IQ
Visually similar images
I1
I2
I5
É
c5
Query expansion using
distributed representations
c1
c2
É
c5
c5: A person wearing a red
jacket climbs a snowy hill.
Initial ranking Final ranking
</figure>
<figureCaption confidence="0.999987">
Figure 1: A system overview of the proposed query expansion approach for image captioning.
</figureCaption>
<bodyText confidence="0.999797555555555">
sion step and synthesize a new query, based on
distributional representations (Baroni and Lenci,
2010; Turney and Pantel, 2010; Mikolov et al.,
2013; Pennington et al., 2014) of the captions of
the images visually similar to the input image.
Through comprehensive experiments over three
benchmark datasets, we show that our model im-
proves upon existing methods and produces cap-
tions more appropriate to the query image.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999996073170732">
As mentioned earlier, a number of studies pose im-
age captioning as a caption transfer problem by
relying on the assumption that visually similar im-
ages generally contain very similar captions. The
pioneering work in this category is the im2text
model by Ordonez et al. (2011), which suggests
a two-step retrieval process to transfer a caption to
a given query image. The first step, which pro-
vides a baseline for the follow-up caption transfer
approaches, is to find visually similar images in
terms of some global image features. In the second
step, according to the retrieved captions, specific
detectors and classifiers are applied to images to
construct a semantic representation, which is then
used to re-rank the associated captions.
Kuznetsova et al. (2012) proposed performing
multiple retrievals for each detected visual ele-
ment in the query image and then combining the
relevant parts of the retrieved captions to gener-
ate the output caption. Patterson et al. (2014) ex-
tended the baseline model by replacing global fea-
tures with automatically extracted scene attributes,
and showed the importance of scene information
in caption transfer. Mason and Charniak (2014)
formulated caption transfer as an extractive sum-
marization problem and proposed to perform the
re-ranking step by means of a word frequency-
based representations of captions. More recently,
Mitchell et al. (2015) proposed to select the cap-
tion that best describes the remaining descrip-
tions of the retrieved similar images wrt an n-gram
overlap-based sentence similarity measure.
In this paper, we take a new perspective to
data-driven image captioning by proposing a novel
query expansion step based on compositional dis-
tributed semantics to improve the results. Our
approach uses the weighted average of the dis-
tributed representations of retrieved captions to ex-
pand the original query in order to obtain captions
that are semantically more related to the visual
content of the input image.
</bodyText>
<sectionHeader confidence="0.975472" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999466">
In this section, we describe the steps of the pro-
posed method in more detail.
</bodyText>
<subsectionHeader confidence="0.999755">
3.1 Retrieving Visually Similar Images
</subsectionHeader>
<bodyText confidence="0.999611954545454">
Representing Images. Data-driven approaches
such as ours rely heavily on the quality of the ini-
tial retrieval, which makes having a good visual
feature of utmost importance. In our study, we
use the recently proposed Caffe deep learning fea-
tures (Jia et al., 2014), trained on ImageNet, which
have been proven to be effective in many computer
vision problems. Specifically, we use the activa-
tions from the seventh hidden layer (fc7), resulting
in a 4096-dimensional feature vector.
Adaptive Neighborhood Selection. We create
our expanded query by using the distributed rep-
resentations of the captions associated with the
retrieved images, and thus, having no outliers is
also an important factor for the effectiveness of
the approach. For this, instead of using a fixed
neighborhood, we adopt an adaptive strategy to se-
lect the initial candidate set of image-caption pairs
{(Ii, Ci)}.
For a query image Iq, we utilize a ratio test and
only consider the candidates that fall within a ra-
dius defined by the distance score of the query im-
</bodyText>
<page confidence="0.993919">
107
</page>
<bodyText confidence="0.976178">
age to the nearest training image Iclosest, as
</bodyText>
<equation confidence="0.9989965">
N(Iq) = {(Ii, ci)  |dist(Iq, Ii) ≤ (1 + E)dist(Iq, Iclosest),
Iclosest = arg min dist(Iq, Ii), Ii E T } (1)
</equation>
<bodyText confidence="0.999985">
where dist denotes the Euclidean distance be-
tween two feature vectors, N represents the candi-
date set based on the adaptive neighborhood, T is
the training set, and c is a positive scalar value1.
</bodyText>
<subsectionHeader confidence="0.98999">
3.2 Query Expansion Based on Distributed
Representations
</subsectionHeader>
<bodyText confidence="0.999973275862069">
Representing Words and Captions. In this
study, we build our query expansion model on
the distributional models of semantics where the
meanings of words are represented with vectors
that characterize the set of contexts they occur in a
corpus. Existing approaches to distributional se-
mantics can be grouped into two, as count and
predict-based models (Baroni et al., 2014). In our
experiments, we tested our approach using two re-
cent models, namely word2vec (Mikolov et al.,
2013) and GloVe (Pennington et al., 2014), and
found out that the predict-based model of Mikolov
et al. (2013) performs better in our case.
To move from word level to caption level,
we take the simple addition based compositional
model described in (Blacoe and Lapata, 2012) and
form the vector representation of a caption as the
sum of the vectors of its constituent words. Note
that here we only use the non-stop words in the
caption.
Query Expansion. For a query image IQ, we
first retrieve visually similar images from a large
dataset of captioned images. In our query expan-
sion step, we swap modalities and construct a new
query based on the distributed representations of
captions. In particular, we expand the original vi-
sual query with a new textual query based on the
weighted average of the vectors of the retrieved
captions as follows:
</bodyText>
<equation confidence="0.411673">
sim(IQ, Ii) · cij (2)
</equation>
<bodyText confidence="0.9997726">
where N and M respectively denote the total num-
ber of image-caption pairs in the candidate set N
and the number of reference captions associated
with each training image, and sim(IQ, Ii) refers to
the visual similarity score of the image Ii to the
</bodyText>
<footnote confidence="0.546945">
1The adaptive neighborhood parameter E was emprically
set to 0.15.
</footnote>
<bodyText confidence="0.999837">
query image IQ2 which is used to give more im-
portance to the captions of images visually more
close to the query image.
Then, we re-rank the candidate captions by esti-
mating the cosine distance between the distributed
representation of the captions and the expanded
query vector q, and finally transfer the closest cap-
tion as the description of the input image.
</bodyText>
<sectionHeader confidence="0.990253" genericHeader="method">
4 Experimental Setup and Evaluation
</sectionHeader>
<bodyText confidence="0.989274897435897">
In the following, we give the details about our ex-
perimental setup.
Corpus. We estimated the distributed represen-
tation of words based on the captions of the MS
COCO (Lin et al., 2014) dataset, containing 620K
captions. As a preprocessing step, all captions
in the corpus were lowercased, and stripped from
punctuation.
In the training of word vectors, we used 500 di-
mensional vectors obtained with both GloVe (Pen-
nington et al., 2014) and word2vec (Mikolov et al.,
2013) models. The minimum word count was set
to 5, and the window size was set to 10. Although
these two methods seem to produce comparable
results, we found out that word2vec gives better
results in our case, and thus we only report our re-
sults with word2vec model.
Datasets. In our experiments, we used the popular
Flickr8K (Hodosh et al., 2013), Flickr30K (Young
et al., 2014), MS COCO (Lin et al., 2014) datasets,
containing 8K, 30K and 123K images, respec-
tively. Each image in these datasets comes with
5 captions annotated by different people. For each
dataset, we utilized the corresponding validation
split to optimize the parameters of our method, and
used the test split for evaluation where we consid-
ered all the image-caption pairs in the training and
the validation splits as our knowledge base.
Although Flickr8K, and Flickr30K datasets
have been in use for a while, MS COCO dataset
is under active development and might be subject
to change. Here, we report our results with version
1.0 of MS COCO dataset where we used the train,
validation and test splits provided by (Karpathy et
al., 2014).
We compared our proposed approach against
the adapted baseline model (VC) of im2text (Or-
donez et al., 2011) which corresponds to using
the caption of the nearest visually similar im-
</bodyText>
<footnote confidence="0.644656">
2We define sim(Iq, Ii) = 1 − dist(Iq, Ii)/Z where Z is
a normalization constant.
</footnote>
<equation confidence="0.824843571428571">
1
N
i=1
q=
M
j=1
NM
</equation>
<page confidence="0.895132">
108
</page>
<figure confidence="0.932544071428572">
MC-KL a black and white dog is playing
or fighting with a brown dog in
grass
VC a brown and white dog jump-
ing over a red yellow and white
pole
OURS a brown and white dog jumps
over a dog hurdle
HUMAN a brown and white sheltie leap-
ing over a rail
a man is sitting on a blue bench
with a blue blanket covering his
face
a father feeding his child on the
street
a man in a black shirt and his
little girl wearing orange are
sharing a treat
a man and a girl sit on the
ground and eat
a man in a white shirt and sun-
glasses is holding hands with a
woman wearing a red shirt out-
side
a girl is skipping across the road
in front of a white truck
a girl jumps rope in a parking
lot
a girl is in a parking lot jumping
rope
one brown and black pigmented
bird sitting on a tree branch
a tree with many leaves around
it
a black bear climbing a tree in
forest area
a bird standing on a tree branch
in a wooded area
a painted sign of a blue bird in
a tree in the woods
MC-SB a dog looks behind itself a girl looks at a woman s face a woman and her two dogs are
walking down the street
</figure>
<figureCaption confidence="0.999408">
Figure 2: Some example input images and the generated descriptions.
</figureCaption>
<table confidence="0.999628285714286">
Flickr8K Flickr30K MS COCO
BLEU METEOR CIDEr BLEU METEOR CIDEr BLEU METEOR CIDEr
OURS 3.78 11.57 0.31 3.22 10.06 0.20 5.36 13.17 0.58
MC-KL 2.71 10.95 0.15 2.02 9.92 0.07 4.04 12.56 0.37
MC-SB 3.08 9.06 0.27 2.76 8.06 0.20 5.02 11.78 0.56
VC 2.79 8.91 0.19 2.33 7.53 0.14 3.71 10.07 0.35
HUMAN 7.59 17.72 2.67 6.52 15.70 2.53 7.42 16.73 2.70
</table>
<tableCaption confidence="0.999963">
Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics.
</tableCaption>
<bodyText confidence="0.99988215">
age, and the word frequency-based approaches of
Mason and Charniak (2014) (MC-SB and MC-
KL). We also provide the human agreement results
(HUMAN) by comparing one groundtruth caption
against the rest.
For a fair comparison with the MC-SB and MC-
KL models (Mason and Charniak, 2014) and the
baseline approach VC, we used the same image
similarity metric and training splits in retrieving
visually similar images for all models. For hu-
man agreement, we had five groundtruth image
captions, thus we determine the human agreement
score by following a leave-one-out strategy. For
display purposes, we selected one description ran-
domly from the available five groundtruth captions
in the figures.
Automatic Evaluation. We evaluated our ap-
proach with a range of existing metrics, which
are thoroughly discussed in (Elliott and Keller,
2014; Vedantam et al., 2015). We used smoothed
BLEU (Papineni et al., 2002) for benchmarking
purposes. We also provided the scores of ME-
TEOR (Denkowski and Lavie, 2014) and the re-
cently proposed CIDEr metric (Vedantam et al.,
2015), which has been shown to correlate well
with the human judgments in (Elliott and Keller,
2014) and (Vedantam et al., 2015), respectively3.
Human Evaluation. We designed a subjective ex-
periment to measure how relevant the transferred
caption is to a given image using a setup similar
to those of (Kuznetsova et al., 2012; Mason and
Charniak, 2014)4. In this experiment, we provided
human annotators an image and a candidate de-
scription where it is rated according to a scale of
1 to 5 (5: perfect, 4: almost perfect, 3: 70-80%
good, 2: 50-70% good, 1: totally bad) for its rel-
evancy. We experimented on a randomly selected
set of 100 images from our test set and evaluated
our captions as well as those of the competing ap-
proaches.
</bodyText>
<footnote confidence="0.9959886">
3We collected METEOR and BLEU scores via MultE-
val (Clark et al., 2011) and for CIDEr scores we used the
authors’ publicly available code.
4We used CrowdFlower and at least 5 different human an-
notators for each question.
</footnote>
<page confidence="0.988541">
109
</page>
<table confidence="0.996155">
Rate Variance
OURS 2.73 0.65
MC-SB 2.38 0.58
VC 2.27 0.66
MC-KL 2.03 0.62
HUMAN 4.84 0.26
</table>
<tableCaption confidence="0.705584666666667">
Table 2: Human judgment scores on a scale of 1 to 5.
a man wearing a santa hat hold- a boy is holding a dog that is
ing a dog posing for a picture wearing a hat
</tableCaption>
<sectionHeader confidence="0.997836" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999981387096774">
In Figure 2, we present sample results obtained
with our framework, MC-SB, MC-KL and VC
models along with the groundtruth caption. We
provide the quantitative results based on automatic
evaluation measures and human judgment scores
in Table 1 and Table 2, respectively.
Our findings indicate that our query expansion
approach which is based on distributed representa-
tions of captions gives results better than those of
VC, MC-SB and MC-KL models. Although our
method makes a modest improvement compared
to the human scores we believe that there is still a
big gap between the human baseline, which align
well with the recently held MS COCO 2015 Cap-
tioning Challenge results.
One limitation in this work is the Out-of-
Vocabulary (OOV) words, which is around 1% on
average for the benchmark datasets. We omit them
in our calculations, since there is no practical way
to map word vectors for the OOV words, as they
are not included in the training of the word em-
beddings. Another limitation is that this approach
currently does not incorporate the syntactic struc-
tures in captions, therefore the position of a word
in a caption does not make any difference in the
representation, i.e. “a man with a hat is holding a
dog” and “a man is holding a dog with a hat” are
represented with the same vector. This limitation
is illustrated in Fig. 3, where the closest caption
from retrieval set contains similar scene elements
but does not depict the scene well.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999814">
In this paper, we present a novel query expansion
approach for image captioning, in which we uti-
lize a distributional model of meaning for sen-
tences. Extensive experimental results on three
well-established benchmark datasets have demon-
strated that our approach outperforms the state-of-
the art data-driven approaches. Our future plans
focus on incorporating other cues in images, and
</bodyText>
<figureCaption confidence="0.996946333333333">
Figure 3: Limitation. A query image on the left and its
actual caption, a proposed caption on the right along with its
actual image.
</figureCaption>
<bodyText confidence="0.9950635">
considering the syntactic structures in image de-
scriptions.
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.968173666666667">
This study was supported in part by The Scientific
and Technological Research Council of Turkey
(TUBITAK), with award no 113E116.
</bodyText>
<sectionHeader confidence="0.998117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999132928571428">
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A General Framework for
Corpus-Based Semantics. Computational Linguis-
tics, 36(4):673–721. 2
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proc. of
ACL. 3
Alexander C Berg, Tamara L Berg, Hal Daume, Jesse
Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch,
Margaret Mitchell, Aneesh Sood, Karl Stratos, et al.
2012. Understanding and Predicting Importance in
Images. In Proc. of CVPR. 1
William Blacoe and Mirella Lapata. 2012. A Compar-
ison of Vector-based Representations for Semantic
Composition. In Proc. of EMNLP-CoNLL. 3
Xinlei Chen and C. Lawrence Zitnick. 2015. Mind’s
Eye: A Recurrent Visual Representation for Image
Caption Generation. In Proc. of CVPR. 1
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: Controlling for optimizer
instability. In Proc. of ACL. 4
Michael Denkowski and Alon Lavie. 2014. Meteor
Universal: Language Specific Translation Evalua-
tion for Any Target Language. In Proc. of EACL
Workshop on Statistical Machine Translation. 4
</reference>
<page confidence="0.992772">
110
</page>
<reference confidence="0.998972076086957">
Desmond Elliott and Frank Keller. 2014. Comparing
Automatic Evaluation Measures for Image Descrip-
tion. In Proc. ofACL. 4
Ali Farhadi, M Hejrati, Mohammad Amin Sadeghi,
P Young, C Rashtchian, J Hockenmaier, and David
Forsyth. 2010. Every Picture Tells a Story: Gen-
erating Sentences from Images. In Proc. of ECCV.
1
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing Image Description as a Ranking
Task: Data, Models and Evaluation Metrics. Jour-
nal of Artificial Intelligence Research. 1, 3
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross Girshick, Sergio
Guadarrama, and Trevor Darrell. 2014. Caffe: Con-
volutional Architecture for Fast Feature Embedding.
In Proc. of ACM MM. 2
Andrej Karpathy and Li Fei-Fei. 2015. Deep Visual-
semantic Alignments for Generating Image Descrip-
tions. In Proc. of CVPR. 1
Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014.
Deep Fragment Embeddings for Bidirectional Image
Sentence Mapping. In Proc. of NIPS. 1, 3
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Baby Talk: Understanding and Gener-
ating Simple Image Descriptions. In Proc. of CVPR.
1
Polina Kuznetsova, Vicente Ordonez, Alexander C
Berg, Tamara L Berg, and Yejin Choi. 2012. Collec-
tive Generation of Natural Image Descriptions. In
Proc. ofACL. 1, 2, 4
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C Lawrence Zitnick. 2014. Microsoft COCO:
Common Objects in Context. In Proc. of ECCV. 3
Rebecca Mason and Eugene Charniak. 2014. Non-
parametric Method for Data-driven Image Caption-
ing. In Proc. ofACL. 1, 2, 4
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed Representa-
tions of Words and Phrases and their Composition-
ality. In Proc. of NIPS. 2, 3
Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa
Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi,
Tamara Berg, Karl Stratos, and Hal Daum´e III.
2012. Midge: Generating Image Descriptions from
Computer Vision Detections. In Proc. of EACL. 1
Margaret Mitchell, Hao Fang, Hao Cheng, Saurabh
Gupta, Jacob Devlin, and Geoffrey Zweig. 2015.
Language Models for Image Captioning: The
Quirks and What Works. In Proc. ofACL. 2
Vicente Ordonez, Girish Kulkarni, and Tamara L Berg.
2011. Im2text: Describing Images using 1 Million
Captioned Photographs. In Proc. of NIPS. 1, 2, 3
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. ofACL.
4
Genevieve Patterson, Chen Xu, Hang Su, and James
Hays. 2014. The SUN Attribute Database: Beyond
Categories for Deeper Scene Understanding. Inter-
national Journal of Computer Vision, 108(1-2):59–
81. 1, 2
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. GloVe: Global Vectors for Word
Representation. Proc. of EMNLP. 2, 3
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded Compositional Semantics for Finding and
Describing Images with Sentences. Transactions of
the Association for Computational Linguistics. 1
Peter Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Seman-
tics. Journal of Artificial Intelligence Research. 2
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. 2015. CIDEr: Consensus-based Image De-
scription Evaluation. In Proc. of CVPR. 4
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2015. Show and Tell: A Neural
Image Caption Generator. In Proc. of CVPR. 1
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,
Aaron Courville, Ruslan Salakhutdinov, Richard
Zemel, and Yoshua Bengio. 2015. Show, Attend
and Tell: Neural Image Caption Generation with Vi-
sual attention. In Proc. of ICML. 1
Peter Young, Alice Lai, Micah Hodosh, and Julia
Hockenmaier. 2014. From Image Descriptions to
Visual Denotations: New similarity Metrics for Se-
mantic Inference over Event Descriptions. Transac-
tions of the Association for Computational Linguis-
tics. 3
</reference>
<page confidence="0.998714">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787387">
<title confidence="0.9987295">A Distributed Representation Based Query Expansion Approach Image Captioning</title>
<author confidence="0.976612">Erkut Aykut Ruket</author>
<affiliation confidence="0.999268">1Hacettepe University Computer Vision Lab Dept. of Computer Engineering, Hacettepe University, Ankara,</affiliation>
<address confidence="0.986314">2Dept. of Computer Engineering, Middle East Technical University, Ankara, TURKEY</address>
<email confidence="0.991165">ruken@ceng.metu.edu.tr</email>
<abstract confidence="0.989156625">In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory: A General Framework for Corpus-Based Semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>2</pages>
<contexts>
<context position="4861" citStr="Baroni and Lenci, 2010" startWordPosition="773" endWordPosition="776">red caption c1: A man climbs up a snowy mountain. c2: A boy in orange jacket appears unhappy. É c1: A man climbs up a snowy mountain. c2: A boy in orange jacket appears unhappy. : A person wearing a red jacket climbs a snowy hill. Query image IQ Visually similar images I1 I2 I5 É c5 Query expansion using distributed representations c1 c2 É c5 c5: A person wearing a red jacket climbs a snowy hill. Initial ranking Final ranking Figure 1: A system overview of the proposed query expansion approach for image captioning. sion step and synthesize a new query, based on distributional representations (Baroni and Lenci, 2010; Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014) of the captions of the images visually similar to the input image. Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image. 2 Related Work As mentioned earlier, a number of studies pose image captioning as a caption transfer problem by relying on the assumption that visually similar images generally contain very similar captions. The pioneering work in this category is the im2text model by Ordonez et a</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A General Framework for Corpus-Based Semantics. Computational Linguistics, 36(4):673–721. 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<volume>3</volume>
<contexts>
<context position="9113" citStr="Baroni et al., 2014" startWordPosition="1461" endWordPosition="1464"> dist denotes the Euclidean distance between two feature vectors, N represents the candidate set based on the adaptive neighborhood, T is the training set, and c is a positive scalar value1. 3.2 Query Expansion Based on Distributed Representations Representing Words and Captions. In this study, we build our query expansion model on the distributional models of semantics where the meanings of words are represented with vectors that characterize the set of contexts they occur in a corpus. Existing approaches to distributional semantics can be grouped into two, as count and predict-based models (Baroni et al., 2014). In our experiments, we tested our approach using two recent models, namely word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), and found out that the predict-based model of Mikolov et al. (2013) performs better in our case. To move from word level to caption level, we take the simple addition based compositional model described in (Blacoe and Lapata, 2012) and form the vector representation of a caption as the sum of the vectors of its constituent words. Note that here we only use the non-stop words in the caption. Query Expansion. For a query image IQ, we first retrieve vis</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proc. of ACL. 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daume</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Aneesh Sood</author>
<author>Karl Stratos</author>
</authors>
<title>Understanding and Predicting Importance in Images.</title>
<date>2012</date>
<booktitle>In Proc. of CVPR.</booktitle>
<pages>1</pages>
<contexts>
<context position="3396" citStr="Berg et al., 2012" startWordPosition="527" endWordPosition="530">l., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches. The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in (Berg et al., 2012), what makes this particularly difficult is that while describing an image humans do not explicitly mention every detail. That is, some parts of an image are more salient than the others. Hence, one also needs to bridge the semantic gap between what is there in the image and what people say when describing it. As a step towards achieving this goal, in this paper, we introduce a novel automatic query expansion approach for image captioning to retrieve semantically more relevant captions. As illustrated in Fig. 1, we swap modalities at our query expan106 Proceedings of the 53rd Annual Meeting of</context>
</contexts>
<marker>Berg, Berg, Daume, Dodge, Goyal, Han, Mensch, Mitchell, Sood, Stratos, 2012</marker>
<rawString>Alexander C Berg, Tamara L Berg, Hal Daume, Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Aneesh Sood, Karl Stratos, et al. 2012. Understanding and Predicting Importance in Images. In Proc. of CVPR. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A Comparison of Vector-based Representations for Semantic Composition.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<pages>3</pages>
<contexts>
<context position="9490" citStr="Blacoe and Lapata, 2012" startWordPosition="1524" endWordPosition="1527">where the meanings of words are represented with vectors that characterize the set of contexts they occur in a corpus. Existing approaches to distributional semantics can be grouped into two, as count and predict-based models (Baroni et al., 2014). In our experiments, we tested our approach using two recent models, namely word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), and found out that the predict-based model of Mikolov et al. (2013) performs better in our case. To move from word level to caption level, we take the simple addition based compositional model described in (Blacoe and Lapata, 2012) and form the vector representation of a caption as the sum of the vectors of its constituent words. Note that here we only use the non-stop words in the caption. Query Expansion. For a query image IQ, we first retrieve visually similar images from a large dataset of captioned images. In our query expansion step, we swap modalities and construct a new query based on the distributed representations of captions. In particular, we expand the original visual query with a new textual query based on the weighted average of the vectors of the retrieved captions as follows: sim(IQ, Ii) · cij (2) where</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A Comparison of Vector-based Representations for Semantic Composition. In Proc. of EMNLP-CoNLL. 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinlei Chen</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation.</title>
<date>2015</date>
<booktitle>In Proc. of CVPR.</booktitle>
<pages>1</pages>
<marker>Chen, Zitnick, 2015</marker>
<rawString>Xinlei Chen and C. Lawrence Zitnick. 2015. Mind’s Eye: A Recurrent Visual Representation for Image Caption Generation. In Proc. of CVPR. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. of ACL.</booktitle>
<volume>4</volume>
<contexts>
<context position="16096" citStr="Clark et al., 2011" startWordPosition="2697" endWordPosition="2700">experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we provided human annotators an image and a candidate description where it is rated according to a scale of 1 to 5 (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad) for its relevancy. We experimented on a randomly selected set of 100 images from our test set and evaluated our captions as well as those of the competing approaches. 3We collected METEOR and BLEU scores via MultEval (Clark et al., 2011) and for CIDEr scores we used the authors’ publicly available code. 4We used CrowdFlower and at least 5 different human annotators for each question. 109 Rate Variance OURS 2.73 0.65 MC-SB 2.38 0.58 VC 2.27 0.66 MC-KL 2.03 0.62 HUMAN 4.84 0.26 Table 2: Human judgment scores on a scale of 1 to 5. a man wearing a santa hat hold- a boy is holding a dog that is ing a dog posing for a picture wearing a hat 5 Results and Discussion In Figure 2, we present sample results obtained with our framework, MC-SB, MC-KL and VC models along with the groundtruth caption. We provide the quantitative results bas</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. of ACL. 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor Universal: Language Specific Translation Evaluation for Any Target Language.</title>
<date>2014</date>
<booktitle>In Proc. of EACL Workshop on Statistical Machine Translation.</booktitle>
<volume>4</volume>
<contexts>
<context position="15231" citStr="Denkowski and Lavie, 2014" startWordPosition="2545" endWordPosition="2548">ts in retrieving visually similar images for all models. For human agreement, we had five groundtruth image captions, thus we determine the human agreement score by following a leave-one-out strategy. For display purposes, we selected one description randomly from the available five groundtruth captions in the figures. Automatic Evaluation. We evaluated our approach with a range of existing metrics, which are thoroughly discussed in (Elliott and Keller, 2014; Vedantam et al., 2015). We used smoothed BLEU (Papineni et al., 2002) for benchmarking purposes. We also provided the scores of METEOR (Denkowski and Lavie, 2014) and the recently proposed CIDEr metric (Vedantam et al., 2015), which has been shown to correlate well with the human judgments in (Elliott and Keller, 2014) and (Vedantam et al., 2015), respectively3. Human Evaluation. We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we provided human annotators an image and a candidate description where it is rated according to a scale of 1 to 5 (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 5</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor Universal: Language Specific Translation Evaluation for Any Target Language. In Proc. of EACL Workshop on Statistical Machine Translation. 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desmond Elliott</author>
<author>Frank Keller</author>
</authors>
<title>Comparing Automatic Evaluation Measures for Image Description.</title>
<date>2014</date>
<booktitle>In Proc. ofACL. 4</booktitle>
<contexts>
<context position="15067" citStr="Elliott and Keller, 2014" startWordPosition="2518" endWordPosition="2521">fair comparison with the MC-SB and MCKL models (Mason and Charniak, 2014) and the baseline approach VC, we used the same image similarity metric and training splits in retrieving visually similar images for all models. For human agreement, we had five groundtruth image captions, thus we determine the human agreement score by following a leave-one-out strategy. For display purposes, we selected one description randomly from the available five groundtruth captions in the figures. Automatic Evaluation. We evaluated our approach with a range of existing metrics, which are thoroughly discussed in (Elliott and Keller, 2014; Vedantam et al., 2015). We used smoothed BLEU (Papineni et al., 2002) for benchmarking purposes. We also provided the scores of METEOR (Denkowski and Lavie, 2014) and the recently proposed CIDEr metric (Vedantam et al., 2015), which has been shown to correlate well with the human judgments in (Elliott and Keller, 2014) and (Vedantam et al., 2015), respectively3. Human Evaluation. We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we </context>
</contexts>
<marker>Elliott, Keller, 2014</marker>
<rawString>Desmond Elliott and Frank Keller. 2014. Comparing Automatic Evaluation Measures for Image Description. In Proc. ofACL. 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>M Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>P Young</author>
<author>C Rashtchian</author>
<author>J Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every Picture Tells a Story: Generating Sentences from Images.</title>
<date>2010</date>
<booktitle>In Proc. of ECCV.</booktitle>
<pages>1</pages>
<contexts>
<context position="1503" citStr="Farhadi et al., 2010" startWordPosition="219" endWordPosition="222"> more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation. 1 Introduction Automatic image captioning is a fast growing area of research which lies at the intersection of computer vision and natural language processing and refers to the problem of generating natural language descriptions from images. In the literature, there are a variety of image captioning models that can be categorized into three main groups as summarized below. The first line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with re</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, M Hejrati, Mohammad Amin Sadeghi, P Young, C Rashtchian, J Hockenmaier, and David Forsyth. 2010. Every Picture Tells a Story: Generating Sentences from Images. In Proc. of ECCV. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<volume>1</volume>
<contexts>
<context position="2292" citStr="Hodosh et al., 2013" startWordPosition="342" endWordPosition="346">it their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these met</context>
<context position="11611" citStr="Hodosh et al., 2013" startWordPosition="1889" endWordPosition="1892"> containing 620K captions. As a preprocessing step, all captions in the corpus were lowercased, and stripped from punctuation. In the training of word vectors, we used 500 dimensional vectors obtained with both GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) models. The minimum word count was set to 5, and the window size was set to 10. Although these two methods seem to produce comparable results, we found out that word2vec gives better results in our case, and thus we only report our results with word2vec model. Datasets. In our experiments, we used the popular Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), MS COCO (Lin et al., 2014) datasets, containing 8K, 30K and 123K images, respectively. Each image in these datasets comes with 5 captions annotated by different people. For each dataset, we utilized the corresponding validation split to optimize the parameters of our method, and used the test split for evaluation where we considered all the image-caption pairs in the training and the validation splits as our knowledge base. Although Flickr8K, and Flickr30K datasets have been in use for a while, MS COCO dataset is under active development and might be subject t</context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research. 1, 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqing Jia</author>
<author>Evan Shelhamer</author>
<author>Jeff Donahue</author>
<author>Sergey Karayev</author>
<author>Jonathan Long</author>
<author>Ross Girshick</author>
<author>Sergio Guadarrama</author>
<author>Trevor Darrell</author>
</authors>
<title>Caffe: Convolutional Architecture for Fast Feature Embedding.</title>
<date>2014</date>
<booktitle>In Proc. of ACM MM.</booktitle>
<volume>2</volume>
<contexts>
<context position="7559" citStr="Jia et al., 2014" startWordPosition="1204" endWordPosition="1207">ach uses the weighted average of the distributed representations of retrieved captions to expand the original query in order to obtain captions that are semantically more related to the visual content of the input image. 3 Our Approach In this section, we describe the steps of the proposed method in more detail. 3.1 Retrieving Visually Similar Images Representing Images. Data-driven approaches such as ours rely heavily on the quality of the initial retrieval, which makes having a good visual feature of utmost importance. In our study, we use the recently proposed Caffe deep learning features (Jia et al., 2014), trained on ImageNet, which have been proven to be effective in many computer vision problems. Specifically, we use the activations from the seventh hidden layer (fc7), resulting in a 4096-dimensional feature vector. Adaptive Neighborhood Selection. We create our expanded query by using the distributed representations of the captions associated with the retrieved images, and thus, having no outliers is also an important factor for the effectiveness of the approach. For this, instead of using a fixed neighborhood, we adopt an adaptive strategy to select the initial candidate set of image-capti</context>
</contexts>
<marker>Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, Darrell, 2014</marker>
<rawString>Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. In Proc. of ACM MM. 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep Visualsemantic Alignments for Generating Image Descriptions.</title>
<date>2015</date>
<booktitle>In Proc. of CVPR.</booktitle>
<pages>1</pages>
<contexts>
<context position="1983" citStr="Karpathy and Fei-Fei, 2015" startWordPosition="291" endWordPosition="294">three main groups as summarized below. The first line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to</context>
</contexts>
<marker>Karpathy, Fei-Fei, 2015</marker>
<rawString>Andrej Karpathy and Li Fei-Fei. 2015. Deep Visualsemantic Alignments for Generating Image Descriptions. In Proc. of CVPR. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Armand Joulin</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep Fragment Embeddings for Bidirectional Image Sentence Mapping.</title>
<date>2014</date>
<booktitle>In Proc. of NIPS.</booktitle>
<volume>1</volume>
<contexts>
<context position="2337" citStr="Karpathy et al., 2014" startWordPosition="351" endWordPosition="354">tent of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images</context>
<context position="12372" citStr="Karpathy et al., 2014" startWordPosition="2016" endWordPosition="2019">e datasets comes with 5 captions annotated by different people. For each dataset, we utilized the corresponding validation split to optimize the parameters of our method, and used the test split for evaluation where we considered all the image-caption pairs in the training and the validation splits as our knowledge base. Although Flickr8K, and Flickr30K datasets have been in use for a while, MS COCO dataset is under active development and might be subject to change. Here, we report our results with version 1.0 of MS COCO dataset where we used the train, validation and test splits provided by (Karpathy et al., 2014). We compared our proposed approach against the adapted baseline model (VC) of im2text (Ordonez et al., 2011) which corresponds to using the caption of the nearest visually similar im2We define sim(Iq, Ii) = 1 − dist(Iq, Ii)/Z where Z is a normalization constant. 1 N i=1 q= M j=1 NM 108 MC-KL a black and white dog is playing or fighting with a brown dog in grass VC a brown and white dog jumping over a red yellow and white pole OURS a brown and white dog jumps over a dog hurdle HUMAN a brown and white sheltie leaping over a rail a man is sitting on a blue bench with a blue blanket covering his </context>
</contexts>
<marker>Karpathy, Joulin, Fei-Fei, 2014</marker>
<rawString>Andrej Karpathy, Armand Joulin, and Li Fei-Fei. 2014. Deep Fragment Embeddings for Bidirectional Image Sentence Mapping. In Proc. of NIPS. 1, 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby Talk: Understanding and Generating Simple Image Descriptions.</title>
<date>2011</date>
<booktitle>In Proc. of CVPR.</booktitle>
<pages>1</pages>
<contexts>
<context position="1526" citStr="Kulkarni et al., 2011" startWordPosition="223" endWordPosition="226"> compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation. 1 Introduction Automatic image captioning is a fast growing area of research which lies at the intersection of computer vision and natural language processing and refers to the problem of generating natural language descriptions from images. In the literature, there are a variety of image captioning models that can be categorized into three main groups as summarized below. The first line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2011. Baby Talk: Understanding and Generating Simple Image Descriptions. In Proc. of CVPR. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective Generation of Natural Image Descriptions.</title>
<date>2012</date>
<booktitle>In Proc. ofACL.</booktitle>
<volume>1</volume>
<contexts>
<context position="2811" citStr="Kuznetsova et al., 2012" startWordPosition="429" endWordPosition="432">es in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches. The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in (Berg et al., 2012), what makes th</context>
<context position="5959" citStr="Kuznetsova et al. (2012)" startWordPosition="950" endWordPosition="953">ilar images generally contain very similar captions. The pioneering work in this category is the im2text model by Ordonez et al. (2011), which suggests a two-step retrieval process to transfer a caption to a given query image. The first step, which provides a baseline for the follow-up caption transfer approaches, is to find visually similar images in terms of some global image features. In the second step, according to the retrieved captions, specific detectors and classifiers are applied to images to construct a semantic representation, which is then used to re-rank the associated captions. Kuznetsova et al. (2012) proposed performing multiple retrievals for each detected visual element in the query image and then combining the relevant parts of the retrieved captions to generate the output caption. Patterson et al. (2014) extended the baseline model by replacing global features with automatically extracted scene attributes, and showed the importance of scene information in caption transfer. Mason and Charniak (2014) formulated caption transfer as an extractive summarization problem and proposed to perform the re-ranking step by means of a word frequencybased representations of captions. More recently, </context>
<context position="15614" citStr="Kuznetsova et al., 2012" startWordPosition="2609" endWordPosition="2612">xisting metrics, which are thoroughly discussed in (Elliott and Keller, 2014; Vedantam et al., 2015). We used smoothed BLEU (Papineni et al., 2002) for benchmarking purposes. We also provided the scores of METEOR (Denkowski and Lavie, 2014) and the recently proposed CIDEr metric (Vedantam et al., 2015), which has been shown to correlate well with the human judgments in (Elliott and Keller, 2014) and (Vedantam et al., 2015), respectively3. Human Evaluation. We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we provided human annotators an image and a candidate description where it is rated according to a scale of 1 to 5 (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad) for its relevancy. We experimented on a randomly selected set of 100 images from our test set and evaluated our captions as well as those of the competing approaches. 3We collected METEOR and BLEU scores via MultEval (Clark et al., 2011) and for CIDEr scores we used the authors’ publicly available code. 4We used CrowdFlower and at least 5 different huma</context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi. 2012. Collective Generation of Natural Image Descriptions. In Proc. ofACL. 1, 2, 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung-Yi Lin</author>
<author>Michael Maire</author>
<author>Serge Belongie</author>
<author>James Hays</author>
<author>Pietro Perona</author>
<author>Deva Ramanan</author>
<author>Piotr Doll´ar</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Microsoft COCO: Common Objects in Context.</title>
<date>2014</date>
<booktitle>In Proc. of ECCV.</booktitle>
<volume>3</volume>
<marker>Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll´ar, Zitnick, 2014</marker>
<rawString>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In Proc. of ECCV. 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>Nonparametric Method for Data-driven Image Captioning.</title>
<date>2014</date>
<booktitle>In Proc. ofACL.</booktitle>
<volume>1</volume>
<contexts>
<context position="2862" citStr="Mason and Charniak, 2014" startWordPosition="437" endWordPosition="440">esentations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches. The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in (Berg et al., 2012), what makes this particularly difficult is that while describing </context>
<context position="6369" citStr="Mason and Charniak (2014)" startWordPosition="1013" endWordPosition="1016">, according to the retrieved captions, specific detectors and classifiers are applied to images to construct a semantic representation, which is then used to re-rank the associated captions. Kuznetsova et al. (2012) proposed performing multiple retrievals for each detected visual element in the query image and then combining the relevant parts of the retrieved captions to generate the output caption. Patterson et al. (2014) extended the baseline model by replacing global features with automatically extracted scene attributes, and showed the importance of scene information in caption transfer. Mason and Charniak (2014) formulated caption transfer as an extractive summarization problem and proposed to perform the re-ranking step by means of a word frequencybased representations of captions. More recently, Mitchell et al. (2015) proposed to select the caption that best describes the remaining descriptions of the retrieved similar images wrt an n-gram overlap-based sentence similarity measure. In this paper, we take a new perspective to data-driven image captioning by proposing a novel query expansion step based on compositional distributed semantics to improve the results. Our approach uses the weighted avera</context>
<context position="14311" citStr="Mason and Charniak (2014)" startWordPosition="2398" endWordPosition="2401"> her two dogs are walking down the street Figure 2: Some example input images and the generated descriptions. Flickr8K Flickr30K MS COCO BLEU METEOR CIDEr BLEU METEOR CIDEr BLEU METEOR CIDEr OURS 3.78 11.57 0.31 3.22 10.06 0.20 5.36 13.17 0.58 MC-KL 2.71 10.95 0.15 2.02 9.92 0.07 4.04 12.56 0.37 MC-SB 3.08 9.06 0.27 2.76 8.06 0.20 5.02 11.78 0.56 VC 2.79 8.91 0.19 2.33 7.53 0.14 3.71 10.07 0.35 HUMAN 7.59 17.72 2.67 6.52 15.70 2.53 7.42 16.73 2.70 Table 1: Comparison of the methods on the benchmark datasets based on automatic evaluation metrics. age, and the word frequency-based approaches of Mason and Charniak (2014) (MC-SB and MCKL). We also provide the human agreement results (HUMAN) by comparing one groundtruth caption against the rest. For a fair comparison with the MC-SB and MCKL models (Mason and Charniak, 2014) and the baseline approach VC, we used the same image similarity metric and training splits in retrieving visually similar images for all models. For human agreement, we had five groundtruth image captions, thus we determine the human agreement score by following a leave-one-out strategy. For display purposes, we selected one description randomly from the available five groundtruth captions i</context>
<context position="15641" citStr="Mason and Charniak, 2014" startWordPosition="2613" endWordPosition="2616">e thoroughly discussed in (Elliott and Keller, 2014; Vedantam et al., 2015). We used smoothed BLEU (Papineni et al., 2002) for benchmarking purposes. We also provided the scores of METEOR (Denkowski and Lavie, 2014) and the recently proposed CIDEr metric (Vedantam et al., 2015), which has been shown to correlate well with the human judgments in (Elliott and Keller, 2014) and (Vedantam et al., 2015), respectively3. Human Evaluation. We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we provided human annotators an image and a candidate description where it is rated according to a scale of 1 to 5 (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad) for its relevancy. We experimented on a randomly selected set of 100 images from our test set and evaluated our captions as well as those of the competing approaches. 3We collected METEOR and BLEU scores via MultEval (Clark et al., 2011) and for CIDEr scores we used the authors’ publicly available code. 4We used CrowdFlower and at least 5 different human annotators for each quest</context>
</contexts>
<marker>Mason, Charniak, 2014</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2014. Nonparametric Method for Data-driven Image Captioning. In Proc. ofACL. 1, 2, 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<date>2013</date>
<booktitle>Distributed Representations of Words and Phrases and their Compositionality. In Proc. of NIPS.</booktitle>
<volume>2</volume>
<contexts>
<context position="4908" citStr="Mikolov et al., 2013" startWordPosition="781" endWordPosition="784"> c2: A boy in orange jacket appears unhappy. É c1: A man climbs up a snowy mountain. c2: A boy in orange jacket appears unhappy. : A person wearing a red jacket climbs a snowy hill. Query image IQ Visually similar images I1 I2 I5 É c5 Query expansion using distributed representations c1 c2 É c5 c5: A person wearing a red jacket climbs a snowy hill. Initial ranking Final ranking Figure 1: A system overview of the proposed query expansion approach for image captioning. sion step and synthesize a new query, based on distributional representations (Baroni and Lenci, 2010; Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014) of the captions of the images visually similar to the input image. Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image. 2 Related Work As mentioned earlier, a number of studies pose image captioning as a caption transfer problem by relying on the assumption that visually similar images generally contain very similar captions. The pioneering work in this category is the im2text model by Ordonez et al. (2011), which suggests a two-step retrieval </context>
<context position="9221" citStr="Mikolov et al., 2013" startWordPosition="1479" endWordPosition="1482">he adaptive neighborhood, T is the training set, and c is a positive scalar value1. 3.2 Query Expansion Based on Distributed Representations Representing Words and Captions. In this study, we build our query expansion model on the distributional models of semantics where the meanings of words are represented with vectors that characterize the set of contexts they occur in a corpus. Existing approaches to distributional semantics can be grouped into two, as count and predict-based models (Baroni et al., 2014). In our experiments, we tested our approach using two recent models, namely word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), and found out that the predict-based model of Mikolov et al. (2013) performs better in our case. To move from word level to caption level, we take the simple addition based compositional model described in (Blacoe and Lapata, 2012) and form the vector representation of a caption as the sum of the vectors of its constituent words. Note that here we only use the non-stop words in the caption. Query Expansion. For a query image IQ, we first retrieve visually similar images from a large dataset of captioned images. In our query expansion step, we swap modaliti</context>
<context position="11269" citStr="Mikolov et al., 2013" startWordPosition="1828" endWordPosition="1831"> and the expanded query vector q, and finally transfer the closest caption as the description of the input image. 4 Experimental Setup and Evaluation In the following, we give the details about our experimental setup. Corpus. We estimated the distributed representation of words based on the captions of the MS COCO (Lin et al., 2014) dataset, containing 620K captions. As a preprocessing step, all captions in the corpus were lowercased, and stripped from punctuation. In the training of word vectors, we used 500 dimensional vectors obtained with both GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) models. The minimum word count was set to 5, and the window size was set to 10. Although these two methods seem to produce comparable results, we found out that word2vec gives better results in our case, and thus we only report our results with word2vec model. Datasets. In our experiments, we used the popular Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), MS COCO (Lin et al., 2014) datasets, containing 8K, 30K and 123K images, respectively. Each image in these datasets comes with 5 captions annotated by different people. For each dataset, we utilized the corresponding validat</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proc. of NIPS. 2, 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Xufeng Han</author>
<author>Jesse Dodge</author>
<author>Alyssa Mensch</author>
<author>Amit Goyal</author>
</authors>
<title>Midge: Generating Image Descriptions from Computer Vision Detections.</title>
<date>2012</date>
<booktitle>In Proc. of EACL.</booktitle>
<pages>1</pages>
<location>Alex Berg, Kota Yamaguchi, Tamara Berg, Karl</location>
<contexts>
<context position="1550" citStr="Mitchell et al., 2012" startWordPosition="227" endWordPosition="230">of-theart data-driven methods in terms of both automatic metrics and subjective evaluation. 1 Introduction Automatic image captioning is a fast growing area of research which lies at the intersection of computer vision and natural language processing and refers to the problem of generating natural language descriptions from images. In the literature, there are a variety of image captioning models that can be categorized into three main groups as summarized below. The first line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a de</context>
</contexts>
<marker>Mitchell, Han, Dodge, Mensch, Goyal, 2012</marker>
<rawString>Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota Yamaguchi, Tamara Berg, Karl Stratos, and Hal Daum´e III. 2012. Midge: Generating Image Descriptions from Computer Vision Detections. In Proc. of EACL. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Hao Fang</author>
<author>Hao Cheng</author>
<author>Saurabh Gupta</author>
<author>Jacob Devlin</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Language Models for Image Captioning: The Quirks and What Works.</title>
<date>2015</date>
<booktitle>In Proc. ofACL.</booktitle>
<volume>2</volume>
<contexts>
<context position="6581" citStr="Mitchell et al. (2015)" startWordPosition="1045" endWordPosition="1048"> proposed performing multiple retrievals for each detected visual element in the query image and then combining the relevant parts of the retrieved captions to generate the output caption. Patterson et al. (2014) extended the baseline model by replacing global features with automatically extracted scene attributes, and showed the importance of scene information in caption transfer. Mason and Charniak (2014) formulated caption transfer as an extractive summarization problem and proposed to perform the re-ranking step by means of a word frequencybased representations of captions. More recently, Mitchell et al. (2015) proposed to select the caption that best describes the remaining descriptions of the retrieved similar images wrt an n-gram overlap-based sentence similarity measure. In this paper, we take a new perspective to data-driven image captioning by proposing a novel query expansion step based on compositional distributed semantics to improve the results. Our approach uses the weighted average of the distributed representations of retrieved captions to expand the original query in order to obtain captions that are semantically more related to the visual content of the input image. 3 Our Approach In </context>
</contexts>
<marker>Mitchell, Fang, Cheng, Gupta, Devlin, Zweig, 2015</marker>
<rawString>Margaret Mitchell, Hao Fang, Hao Cheng, Saurabh Gupta, Jacob Devlin, and Geoffrey Zweig. 2015. Language Models for Image Captioning: The Quirks and What Works. In Proc. ofACL. 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing Images using 1 Million Captioned Photographs.</title>
<date>2011</date>
<booktitle>In Proc. of NIPS.</booktitle>
<volume>1</volume>
<contexts>
<context position="2786" citStr="Ordonez et al., 2011" startWordPosition="425" endWordPosition="428">given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches. The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in (Berg et </context>
<context position="5470" citStr="Ordonez et al. (2011)" startWordPosition="873" endWordPosition="876"> Lenci, 2010; Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014) of the captions of the images visually similar to the input image. Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image. 2 Related Work As mentioned earlier, a number of studies pose image captioning as a caption transfer problem by relying on the assumption that visually similar images generally contain very similar captions. The pioneering work in this category is the im2text model by Ordonez et al. (2011), which suggests a two-step retrieval process to transfer a caption to a given query image. The first step, which provides a baseline for the follow-up caption transfer approaches, is to find visually similar images in terms of some global image features. In the second step, according to the retrieved captions, specific detectors and classifiers are applied to images to construct a semantic representation, which is then used to re-rank the associated captions. Kuznetsova et al. (2012) proposed performing multiple retrievals for each detected visual element in the query image and then combining</context>
<context position="12481" citStr="Ordonez et al., 2011" startWordPosition="2033" endWordPosition="2037">g validation split to optimize the parameters of our method, and used the test split for evaluation where we considered all the image-caption pairs in the training and the validation splits as our knowledge base. Although Flickr8K, and Flickr30K datasets have been in use for a while, MS COCO dataset is under active development and might be subject to change. Here, we report our results with version 1.0 of MS COCO dataset where we used the train, validation and test splits provided by (Karpathy et al., 2014). We compared our proposed approach against the adapted baseline model (VC) of im2text (Ordonez et al., 2011) which corresponds to using the caption of the nearest visually similar im2We define sim(Iq, Ii) = 1 − dist(Iq, Ii)/Z where Z is a normalization constant. 1 N i=1 q= M j=1 NM 108 MC-KL a black and white dog is playing or fighting with a brown dog in grass VC a brown and white dog jumping over a red yellow and white pole OURS a brown and white dog jumps over a dog hurdle HUMAN a brown and white sheltie leaping over a rail a man is sitting on a blue bench with a blue blanket covering his face a father feeding his child on the street a man in a black shirt and his little girl wearing orange are s</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. 2011. Im2text: Describing Images using 1 Million Captioned Photographs. In Proc. of NIPS. 1, 2, 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL. 4</booktitle>
<contexts>
<context position="15138" citStr="Papineni et al., 2002" startWordPosition="2530" endWordPosition="2533"> and the baseline approach VC, we used the same image similarity metric and training splits in retrieving visually similar images for all models. For human agreement, we had five groundtruth image captions, thus we determine the human agreement score by following a leave-one-out strategy. For display purposes, we selected one description randomly from the available five groundtruth captions in the figures. Automatic Evaluation. We evaluated our approach with a range of existing metrics, which are thoroughly discussed in (Elliott and Keller, 2014; Vedantam et al., 2015). We used smoothed BLEU (Papineni et al., 2002) for benchmarking purposes. We also provided the scores of METEOR (Denkowski and Lavie, 2014) and the recently proposed CIDEr metric (Vedantam et al., 2015), which has been shown to correlate well with the human judgments in (Elliott and Keller, 2014) and (Vedantam et al., 2015), respectively3. Human Evaluation. We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we provided human annotators an image and a candidate description where it</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. ofACL. 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genevieve Patterson</author>
<author>Chen Xu</author>
<author>Hang Su</author>
<author>James Hays</author>
</authors>
<title>The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding.</title>
<date>2014</date>
<journal>International Journal of Computer Vision,</journal>
<volume>108</volume>
<issue>1</issue>
<pages>2</pages>
<contexts>
<context position="2835" citStr="Patterson et al., 2014" startWordPosition="433" endWordPosition="436">m at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches. The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in (Berg et al., 2012), what makes this particularly difficul</context>
<context position="6171" citStr="Patterson et al. (2014)" startWordPosition="984" endWordPosition="987"> query image. The first step, which provides a baseline for the follow-up caption transfer approaches, is to find visually similar images in terms of some global image features. In the second step, according to the retrieved captions, specific detectors and classifiers are applied to images to construct a semantic representation, which is then used to re-rank the associated captions. Kuznetsova et al. (2012) proposed performing multiple retrievals for each detected visual element in the query image and then combining the relevant parts of the retrieved captions to generate the output caption. Patterson et al. (2014) extended the baseline model by replacing global features with automatically extracted scene attributes, and showed the importance of scene information in caption transfer. Mason and Charniak (2014) formulated caption transfer as an extractive summarization problem and proposed to perform the re-ranking step by means of a word frequencybased representations of captions. More recently, Mitchell et al. (2015) proposed to select the caption that best describes the remaining descriptions of the retrieved similar images wrt an n-gram overlap-based sentence similarity measure. In this paper, we take</context>
</contexts>
<marker>Patterson, Xu, Su, Hays, 2014</marker>
<rawString>Genevieve Patterson, Chen Xu, Hang Su, and James Hays. 2014. The SUN Attribute Database: Beyond Categories for Deeper Scene Understanding. International Journal of Computer Vision, 108(1-2):59– 81. 1, 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>GloVe: Global Vectors for Word Representation.</title>
<date>2014</date>
<booktitle>Proc. of EMNLP.</booktitle>
<volume>2</volume>
<contexts>
<context position="4934" citStr="Pennington et al., 2014" startWordPosition="785" endWordPosition="788">acket appears unhappy. É c1: A man climbs up a snowy mountain. c2: A boy in orange jacket appears unhappy. : A person wearing a red jacket climbs a snowy hill. Query image IQ Visually similar images I1 I2 I5 É c5 Query expansion using distributed representations c1 c2 É c5 c5: A person wearing a red jacket climbs a snowy hill. Initial ranking Final ranking Figure 1: A system overview of the proposed query expansion approach for image captioning. sion step and synthesize a new query, based on distributional representations (Baroni and Lenci, 2010; Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014) of the captions of the images visually similar to the input image. Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image. 2 Related Work As mentioned earlier, a number of studies pose image captioning as a caption transfer problem by relying on the assumption that visually similar images generally contain very similar captions. The pioneering work in this category is the im2text model by Ordonez et al. (2011), which suggests a two-step retrieval process to transfer a capt</context>
<context position="9257" citStr="Pennington et al., 2014" startWordPosition="1485" endWordPosition="1488">e training set, and c is a positive scalar value1. 3.2 Query Expansion Based on Distributed Representations Representing Words and Captions. In this study, we build our query expansion model on the distributional models of semantics where the meanings of words are represented with vectors that characterize the set of contexts they occur in a corpus. Existing approaches to distributional semantics can be grouped into two, as count and predict-based models (Baroni et al., 2014). In our experiments, we tested our approach using two recent models, namely word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), and found out that the predict-based model of Mikolov et al. (2013) performs better in our case. To move from word level to caption level, we take the simple addition based compositional model described in (Blacoe and Lapata, 2012) and form the vector representation of a caption as the sum of the vectors of its constituent words. Note that here we only use the non-stop words in the caption. Query Expansion. For a query image IQ, we first retrieve visually similar images from a large dataset of captioned images. In our query expansion step, we swap modalities and construct a new query based o</context>
<context position="11233" citStr="Pennington et al., 2014" startWordPosition="1821" endWordPosition="1825">tributed representation of the captions and the expanded query vector q, and finally transfer the closest caption as the description of the input image. 4 Experimental Setup and Evaluation In the following, we give the details about our experimental setup. Corpus. We estimated the distributed representation of words based on the captions of the MS COCO (Lin et al., 2014) dataset, containing 620K captions. As a preprocessing step, all captions in the corpus were lowercased, and stripped from punctuation. In the training of word vectors, we used 500 dimensional vectors obtained with both GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) models. The minimum word count was set to 5, and the window size was set to 10. Although these two methods seem to produce comparable results, we found out that word2vec gives better results in our case, and thus we only report our results with word2vec model. Datasets. In our experiments, we used the popular Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), MS COCO (Lin et al., 2014) datasets, containing 8K, 30K and 123K images, respectively. Each image in these datasets comes with 5 captions annotated by different people. For each dataset, w</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global Vectors for Word Representation. Proc. of EMNLP. 2, 3</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded Compositional Semantics for Finding and Describing Images with Sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics.</journal>
<volume>1</volume>
<contexts>
<context position="2313" citStr="Socher et al., 2014" startWordPosition="347" endWordPosition="350">xtract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded Compositional Semantics for Finding and Describing Images with Sentences. Transactions of the Association for Computational Linguistics. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research.</journal>
<volume>2</volume>
<contexts>
<context position="4886" citStr="Turney and Pantel, 2010" startWordPosition="777" endWordPosition="780">imbs up a snowy mountain. c2: A boy in orange jacket appears unhappy. É c1: A man climbs up a snowy mountain. c2: A boy in orange jacket appears unhappy. : A person wearing a red jacket climbs a snowy hill. Query image IQ Visually similar images I1 I2 I5 É c5 Query expansion using distributed representations c1 c2 É c5 c5: A person wearing a red jacket climbs a snowy hill. Initial ranking Final ranking Figure 1: A system overview of the proposed query expansion approach for image captioning. sion step and synthesize a new query, based on distributional representations (Baroni and Lenci, 2010; Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014) of the captions of the images visually similar to the input image. Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image. 2 Related Work As mentioned earlier, a number of studies pose image captioning as a caption transfer problem by relying on the assumption that visually similar images generally contain very similar captions. The pioneering work in this category is the im2text model by Ordonez et al. (2011), which suggests</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research. 2</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramakrishna Vedantam</author>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
</authors>
<title>CIDEr: Consensus-based Image Description Evaluation.</title>
<date>2015</date>
<booktitle>In Proc. of CVPR.</booktitle>
<volume>4</volume>
<contexts>
<context position="15091" citStr="Vedantam et al., 2015" startWordPosition="2522" endWordPosition="2525">C-SB and MCKL models (Mason and Charniak, 2014) and the baseline approach VC, we used the same image similarity metric and training splits in retrieving visually similar images for all models. For human agreement, we had five groundtruth image captions, thus we determine the human agreement score by following a leave-one-out strategy. For display purposes, we selected one description randomly from the available five groundtruth captions in the figures. Automatic Evaluation. We evaluated our approach with a range of existing metrics, which are thoroughly discussed in (Elliott and Keller, 2014; Vedantam et al., 2015). We used smoothed BLEU (Papineni et al., 2002) for benchmarking purposes. We also provided the scores of METEOR (Denkowski and Lavie, 2014) and the recently proposed CIDEr metric (Vedantam et al., 2015), which has been shown to correlate well with the human judgments in (Elliott and Keller, 2014) and (Vedantam et al., 2015), respectively3. Human Evaluation. We designed a subjective experiment to measure how relevant the transferred caption is to a given image using a setup similar to those of (Kuznetsova et al., 2012; Mason and Charniak, 2014)4. In this experiment, we provided human annotator</context>
</contexts>
<marker>Vedantam, Zitnick, Parikh, 2015</marker>
<rawString>Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based Image Description Evaluation. In Proc. of CVPR. 4</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Alexander Toshev</author>
<author>Samy Bengio</author>
<author>Dumitru Erhan</author>
</authors>
<title>Show and Tell: A Neural Image Caption Generator.</title>
<date>2015</date>
<booktitle>In Proc. of CVPR.</booktitle>
<pages>1</pages>
<contexts>
<context position="2023" citStr="Vinyals et al., 2015" startWordPosition="299" endWordPosition="302">rst line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a give</context>
</contexts>
<marker>Vinyals, Toshev, Bengio, Erhan, 2015</marker>
<rawString>Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and Tell: A Neural Image Caption Generator. In Proc. of CVPR. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelvin Xu</author>
<author>Jimmy Ba</author>
<author>Ryan Kiros</author>
<author>Kyunghyun Cho</author>
<author>Aaron Courville</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard Zemel</author>
<author>Yoshua Bengio</author>
</authors>
<title>Show, Attend and Tell: Neural Image Caption Generation with Visual attention.</title>
<date>2015</date>
<booktitle>In Proc. of ICML.</booktitle>
<pages>1</pages>
<contexts>
<context position="2000" citStr="Xu et al., 2015" startWordPosition="295" endWordPosition="298">zed below. The first line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most pr</context>
</contexts>
<marker>Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel, Bengio, 2015</marker>
<rawString>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual attention. In Proc. of ICML. 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Young</author>
<author>Alice Lai</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>From Image Descriptions to Visual Denotations: New similarity Metrics for Semantic Inference over Event Descriptions. Transactions of the Association for Computational Linguistics.</title>
<date>2014</date>
<volume>3</volume>
<contexts>
<context position="11643" citStr="Young et al., 2014" startWordPosition="1894" endWordPosition="1897">reprocessing step, all captions in the corpus were lowercased, and stripped from punctuation. In the training of word vectors, we used 500 dimensional vectors obtained with both GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) models. The minimum word count was set to 5, and the window size was set to 10. Although these two methods seem to produce comparable results, we found out that word2vec gives better results in our case, and thus we only report our results with word2vec model. Datasets. In our experiments, we used the popular Flickr8K (Hodosh et al., 2013), Flickr30K (Young et al., 2014), MS COCO (Lin et al., 2014) datasets, containing 8K, 30K and 123K images, respectively. Each image in these datasets comes with 5 captions annotated by different people. For each dataset, we utilized the corresponding validation split to optimize the parameters of our method, and used the test split for evaluation where we considered all the image-caption pairs in the training and the validation splits as our knowledge base. Although Flickr8K, and Flickr30K datasets have been in use for a while, MS COCO dataset is under active development and might be subject to change. Here, we report our re</context>
</contexts>
<marker>Young, Lai, Hodosh, Hockenmaier, 2014</marker>
<rawString>Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From Image Descriptions to Visual Denotations: New similarity Metrics for Semantic Inference over Event Descriptions. Transactions of the Association for Computational Linguistics. 3</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>