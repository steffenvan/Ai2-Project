<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.9990795">
Towards Translingual Information Access
using Portable Information Extraction
</title>
<author confidence="0.996714">
Michael White, Claire Cardie, Chung-hye Han, Nan i Kim,*
Benoit Lavoie, Martha Palmer, Owen Rambow,* Juntae Yoon
</author>
<affiliation confidence="0.95979075">
CoGenTex, Inc. Institute for Research in Dept. of Computer Science
Ithaca, NY, USA Cognitive Science Cornell University
Imike,benoit.owen) University of Pennsylvania Ithaca, NY, USA
@cogentex.com Philadelphia, PA, USA cardie@cs.cornell.edu
</affiliation>
<email confidence="0.619402666666667">
chunghye@babel.ling.upenn edu
[nari,mpalmer,jtyoon)
@linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.982315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920181818182">
We report on a small study undertaken to
demonstrate the feasibility of combining
portable information extraction with MT in
order to support translingual information
access. After describing the proposed
system&apos;s usage scenario and system design,
we describe our investigation of transferring
information extraction techniques developed
for English to Korean. We conclude with a
brief discussion of related MT issues we plan
to investigate in future work.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910256410257">
In this paper, we report on a small study
undertaken to demonstrate the feasibility of
combining portable information extraction with
MT in order to support translingual information
access. The goal of our proposed system is to
better enable analysts to perform information
filtering tasks on foreign language documents.
This effort was funded by a SBIR Phase I award
from the U.S. Army Research Lab, and will be
pursued further under the DARPA TIDES
initiative.
Information extraction (IE) systems are
designed to extract specific types of information
from natural language texts. In order to achieve
acceptable accuracy, IE systems need to be
tuned for a given topic domain. Since this
domain tuning can be labor intensive, recent IE
research has focused on developing learning
algorithms for training IF system components
(cf. Cardie, 1997, for a survey). To date,
however, little work has been done on 1E
systems for languages other than English
(though cf. MUC-5, 1994, and MUC-7, 1998,
for Japanese IE systems); and, to our knowledge,
none of the available techniques for the core task
of learning information extraction patterns have
been extended or evaluated for multilingual
information extraction (though again cf. MUC-7,
1998, where the use of learning techniques for
the IE subtasks of named entity recognition and
coreference resolution are described).
Given this situation, the primary objective of
our study was to demonstrate the feasibility of
using portable—i.e., easily trainable—IE
technology on Korean documents, focusing on
techniques for learning information extraction
patterns. Secondary objectives of the study were
to elaborate the analyst scenario and system
design.
</bodyText>
<sectionHeader confidence="0.951428" genericHeader="method">
2 Analyst Scenario
</sectionHeader>
<bodyText confidence="0.6628214">
Figure 1 illustrates how an intelligence analyst
might use the proposed system:
• The analyst selects one or more Korean
documents in which to search for
information (this step not shown).
</bodyText>
<note confidence="0.67315">
* Current affiliation: Konan Technology, Inc., Korea, nari@konantech.co.lcr
* Current affiliation: ATT Labs-Research, Florham Park, NJ, USA, rambow@research.att.com
</note>
<page confidence="0.999858">
31
</page>
<figure confidence="0.979184642857143">
Report
Find
Event: rifaTrna
Query
Source:
Date: r----
ion:&apos;SraTii Korea
Part I c !pant :
issuelliMli_Korea AND missiles
Response to Query
The reports Indicate 2 meetings held In South Korea on the
Issues of North Korea and missiles:
Sources
Joongang
Daily
05/106/99
The Korea
News
05/01/99
Translated Extracts
A nas held
A Besting mas held orrAWall JO tOout the hot
lines batman Seoul and Tokyo for the
954! pansy situations such as Abrth Korea&apos;s
ftuncning F1661186.
Translation of Korean Source Report
IJoongang Dal I yI
Korean. Japanese Ministers Discuss NK Policy
</figure>
<bodyText confidence="0.977540375">
hong Soon-young, the minister of Foreign Affairs and Trade
with his Japanese counterpart Masahiko Komura
The two ministers agreed that any further launching of a
missile by North Korean would undermine the security of
Northeast Asia and the Korea, the United States and Japan
should take joint steps against the North Korean missile
threat.
Hong requested that Komura work to normalize Japan&apos;s
relations with North Korea, rather than cutting channels
of dialogue between the two countries.
Komura said that If North Korea continues Its missile
testing, the Japanese government will definitely stop
making contributions to KE.
The two ministers also tentatively agreed that Japanese
Prime minister Keizo Obuchi should make a state visit to
Korea on or around March 20.
</bodyText>
<figure confidence="0.968866">
Korean Source Report
l$VVI1
• &amp;quot;Xq-EllAr4 Cig&amp;quot;
Ne* 2-12114P4 WIZ 2-7-2} MAIMS IJ 2ult.2-lli
;n
..
t1131 DIAlW* #21 WAitt *V-. ?JD .1711
W0II atT*11 DIV 1101aPeCil 51&apos; alit VOlti.a. 3-01
*MOM: -WAN I-17171M 9/0•2 lill1tE4 .M*31 212V-
*0l ?Lista.
4 gEff- Wg0I CHM* EPaitt71 I9 HUti
3,10l V_+711JoII 11)1VCIC11 011 Wit 31
?!..1811.1tCJ.
01011 CliNi 2-72}9lf VOI 01A1V1 47FVtAitial .IRJ1S01114
x1211VVI?(KED0)/a `4**93J01 J
C/1201.7UZ Xlqatm,
gt101 NOE! ENS* .1310kIll. M-c4.711 61RXIEI *917I
92Cre. IMS1C-fM .Rigt- nto.
c.1R WIT! 24-tX1 711E0.421 VER 331 20VIA2M .1tg Et2181
• EtlEot tiq* *NH 141&apos;121* &apos;EVZIVNEND211111
2IZ ,C-F. MOIR StED1. D*I*1
01I02111 11MaltLH Al01IMZ)IZ 110.
</figure>
<figureCaption confidence="0.964781">
Figure 1
</figureCaption>
<listItem confidence="0.918257222222222">
• The analyst selects one or more scenario
template to activate in the query. Each
scenario template corresponds to a specific
type of event. Available scenario templates
might include troop movements, acts of
violence, meetings and negotiations,
protests, etc. In Figure 1, the selected event
is of type meeting (understood broadly).
• The analyst fills in the available slots of the
</listItem>
<bodyText confidence="0.716159285714286">
selected scenario template in order to restrict
the search to the information considered to
be relevant. In Figure 1, the values specified
in the scenario template indicate that the
information to find is about meetings having
as location South Korea and as issue North
Korea and missiles. The analyst also
</bodyText>
<page confidence="0.996002">
32
</page>
<bodyText confidence="0.929386285714286">
specifies what information s/he wants to be
reported when information matching the
query is found. In Figure 1, the selected
boxes under the Report column indicate that
all information found satisfying the query
should be reported except for the meeting
participants.&apos;
• Once the analyst submits the query for
evaluation, the system searches the input
documents for information matching the
query. As a result, a hypertext document is
generated describing the information
matching the query as well as the source of
this information. Note that the query
contains English keywords that are
automatically translated into Korean prior to
matching. The extracted information is
presented in English after being translated
from Korean. In Figure 1, the generated
hypertext response indicates two documents
in the input set that matched the query
totally or in part. Each summary in the
response includes just the translations of the
extracted information that the analyst
requested to be reported.
• For each document extract matching the
analyst query, the analyst can obtain a
complete machine translation of the Korean
document where the match was found, and
where the matched information is
highlighted. Working with a human
translator, the analyst can also verify the
accuracy of the reported information by
accessing the documents in their original
language.
</bodyText>
<sectionHeader confidence="0.899694" genericHeader="method">
3 System Design
</sectionHeader>
<bodyText confidence="0.787611">
Figure 2 shows the high-level design of the
system. It consists of the following components:
</bodyText>
<listItem confidence="0.966264666666667">
• The User Interface. The browser-based
interface is for entering queries and
displaying the resulting presentations.
• The Portable Information Extractor (PIE)
component. The PIE component uses the
1 While in this example the exclusion of participant
</listItem>
<bodyText confidence="0.95437865">
information in the resulting report is rather artificial,
in general a scenario template may contain many
different types of information, not all of which are
likely to interest an analyst at once.
Extraction Pattern Library — which
contains the set of extraction patterns
learned in the lab, one set per scenario
template — to extract specific types of
information from the input Korean
documents, once parsed.
• The Ranker component. This component
ranks the extracted information returned by
the PIE component according to how well it
matches the keyword restrictions in the
query. The MT component&apos;s English-to-
Korean Transfer Lexicon is used to map the
English keywords to corresponding Korean
ones. When the match falls below a user-
configurable threshold, the extracted
information is filtered out.
</bodyText>
<listItem confidence="0.962137230769231">
• The MT component. The MT component
(cf. Lavoie et al., 2000) translates the
extracted Korean phrases or sentences into
corresponding English ones.
• The Presentation Generator component.
This component generates well-organized,
easy-to-read hypertext presentations by
organizing and formatting the ranked
extracted information. It uses existing NLG
components, including the Exemplars text
planning framework (White and Caldwell,
1998) and the RealPro syntactic realizer
(Lavoie and Rambow, 1997).
</listItem>
<bodyText confidence="0.9999579">
In our feasibility study, the majority of the effort
went towards developing the PIE component,
described in the next section. This component
was implemented in a general way, i.e. in a way
that we would expect to work beyond the
specific training/test corpus described below. In
contrast, we only implemented initial versions of
the User Interface, Ranker and Presentation
Generator components, in order to demonstrate
the system concept; that is, these initial versions
were only intended to work with our training/test
corpus, and will require considerable further
development prior to reaching operational status.
For the MT component, we used an early
version of the lexical transfer–based system
currently under development in an ongoing
SBIR Phase II project (cf. Nasr et al., 1997;
Palmer et al., 1998; Lavoie et al., 2000), though
with a limited lexicon specifically for translating
the slot fillers in our training/test corpus.
</bodyText>
<page confidence="0.997211">
33
</page>
<figureCaption confidence="0.941137">
Figure 2
</figureCaption>
<figure confidence="0.999335666666667">
component
End user Document Processing Knowledge base
component
POS Tagger
Korean
Grammar
Parser
Extracted Information
(Korean)
Tagged
Korean Documents
Ordered Extracted
Information (Korean)
Korean
Lexicon
yUser Input Data
,TransIA,
**face
Presentation (English)
Parsed Document
Machine Translation
Component (MT)
Korean-English
Transfer Lexicon
English Grammar
English Lexicon
RealPro
Syntactic Realizer
Ordered Extracted
Information (English)
Syntactic Structure (English)
Sentence (English)
Presentation (English
Information Extraction
Query (English)
==
Korean Documents
Parsed Document
Portable Infotthatio*,....._ &apos;Extraction
Extractor (Pm)
English-Korean
Transfer Lexicon
LII (C)OTS component
Component created in Phase I
Component created or improved in Phase II
</figure>
<sectionHeader confidence="0.865956" genericHeader="method">
4 Portable Information Extraction
</sectionHeader>
<subsectionHeader confidence="0.996664">
4.1 Scenario Template and Training/Test
Corpus
</subsectionHeader>
<bodyText confidence="0.999667">
For our Phase I feasibility demonstration, we
chose a minimal scenario template for meeting
and negotiation events consisting of one or more
participant slots plus optional date and location
slots.2 We then gathered a small corpus of thirty
articles by searching for articles containing
&amp;quot;North Korea&amp;quot; and one or more of about 15
keywords. The first two sentences (with a few
exceptions) were then annotated with the slots to
be extracted, leading to a total of 51 sentences
containing 47 scenario templates and 89 total
</bodyText>
<figureCaption confidence="0.804348">
2 In the end, we did not use the &apos;issue&apos; slot shown in
Figure 1, as it contained more complex fillers than
those that typically have been handled in IE systems.
</figureCaption>
<bodyText confidence="0.973421">
correct slots. Note that in a couple of cases
more than one template was given for a single
long sentence.
When compared to the MUC scenario
template task, our extraction task was
considerably simpler, for the following reasons:
</bodyText>
<listItem confidence="0.954426333333333">
• The answer keys only contained information
that could be found within a single sentence,
i.e. the answer keys did not require merging
information across sentences.
• The answer &apos;keys did not require anaphoric
references to be resolved, and we did not
deal with conjuncts separately.
• We did not attempt to normalize dates or
remove appositives from NPs.
</listItem>
<subsectionHeader confidence="0.995383">
4.2 Extraction Pattern Learning
</subsectionHeader>
<bodyText confidence="0.999938">
For our feasibility study, we chose to follow the
AutoSlog (Lehnert et al., 1992; Riloff, 1993)
approach to extraction pattern acquisition. In
this approach, extraction patterns are acquired
</bodyText>
<page confidence="0.9938">
34
</page>
<figure confidence="0.978232636363636">
1. E: &lt;target-np&gt;=&lt;subject&gt; &lt;active voice verb&gt;
&lt;participant&gt; MET
K: &lt;target-np&gt;=&lt;subject&gt; &lt;active voice verb&gt;
&lt;John-i&gt; MANNASSTA
&lt;John-nom&gt;&amp;quot;MET
2. E: &lt;target-np&gt;=&lt;subject&gt; &lt;verb&gt; &lt;infinitive&gt;
&lt;participant&gt; agreed to MEET
K: &lt;target-np&gt;=&lt;subject&gt; &lt;verbl-ki-lo&gt; &lt;verb2&gt;
&lt;John-un&gt; MANNA-ki-lo hapuyhayssta
&lt;John-nom&gt; MEET-ki-lo agreed
(-ki: nominalization ending, -lo: an adverbial postposition)
</figure>
<figureCaption confidence="0.969588">
Figure 3
</figureCaption>
<bodyText confidence="0.99969804">
via a one-shot general-to-specific learning
algorithm designed specifically for the
information extraction task.&apos; The learning
algorithm is straightforward and depends only
on the existence of a (partial) parser and a small
set of general linguistic patterns that direct the
creation of specific patterns. As a training
corpus, it requires a set of texts with noun
phrases annotated with the slot type to be
extracted.
To adapt the AutoSlog approach to Korean,
we first devised Korean equivalents of the
English patterns, two of which are shown in
Figure 3. It turned out that for our corpus, we
could collapse some of these patterns, though
some new ones were also needed. In the end we
used just nine generic patterns.
Important issues that arose in adapting the
approach were (1) greater flexibility in word
order and heavier reliance on morphological
cues in Korean, and (2) the predominance of
light verbs (verbs with little semantic content of
their own) and aspectual verbs in the chosen
domain. We discuss these issues in the next two
sections.
</bodyText>
<subsectionHeader confidence="0.999844">
4.3 Korean Parser
</subsectionHeader>
<bodyText confidence="0.9994505">
We used Yoon&apos;s hybrid statistical Korean parser
(Yoon et al., 1997, 1999; Yoon, 1999) to process
the input sentences prior to extraction. The
parser incorporates a POS tagger and
</bodyText>
<footnote confidence="0.90513175">
3 For TIDES, we plan to use more sophisticated
learning algorithms, as well as active learning
techniques, such as those described in Thompson et
al. (1999).
</footnote>
<bodyText confidence="0.9996593">
morphological analyzer and yields a dependency
representation as its output.4 The use of a
dependency representation enabled us to handle
the greater flexibility in word order in Korean.
To facilitate pattern matching, we wrote a
simple program to convert the parser&apos;s output to
XML form. During the XML conversion, two
simple heuristics were applied, one to recover
implicit subjects, and another to correct a
recurring misanalysis of noun compounds.
</bodyText>
<subsectionHeader confidence="0.98976">
4.4 Trigger Word Filtering and
Generalization
</subsectionHeader>
<bodyText confidence="0.999956894736842">
In the newswire corpus we looked at, meeting
events were rarely described with the verb
`mannata&apos; (&apos;to meet&apos;). Instead, they were
usually described with a noun that stands for
&apos;meeting&apos; and a light or aspectual verb, for
example, `hoyuy-lul kacta&apos; (&apos;to have a meeting&apos;)
or `hoyuy-lul machita&apos; (&apos;to finish a meeting&apos;).
In order to acquire extraction patterns that made
appropriate use of such collocations, we decided
to go beyond the AutoSlog approach and
explicitly group trigger words (such as `hoyuy&apos;)
into classes, and to likewise group any
collocations, such as those involving light verbs
or aspectual verbs. To find collocations for the
trigger words, we reviewed a Korean lexical co-
occurrence base which was constructed from a
corpus of 40 million words (Yoon et al., 1997).
We then used the resulting specification to filter
the learned patterns to just those containing the
</bodyText>
<footnote confidence="0.990699">
4 Overall dependency precision is reported to be
89.4% (Yoon, 1999).
</footnote>
<page confidence="0.999205">
35
</page>
<bodyText confidence="0.999975333333333">
trigger words or trigger word collocations, as
well as to generalize the patterns to the word
class level. Because the number of trigger
words is small, this specification can be done
quickly, and soon pays off in terms of time
saved in manually filtering the learned patterns.
</bodyText>
<sectionHeader confidence="0.495078" genericHeader="evaluation">
4.5 Results
</sectionHeader>
<bodyText confidence="0.999961789473684">
In testing our approach, we obtained overall
results of 79% recall and 67% precision in a
hold-one-out cross validation test. In a cross
validation test, one repeatedly divides a corpus
into different training and test sets, averaging the
results; in the hold-one-out version, the system
is tested on a held-out example after being
trained on the rest. In the IE setting, the recall
measure is the number of correct slots found
divided by the total number of correct slots,
while the precision measure is the number of
correct slots found divided by the total number
of slots found.
While direct comparisons with the MUC
conference results cannot be made for the
reasons we gave above, we nevertheless
consider these results quite promising, as these
scores exceed the best scores reported at MUC-6
on the scenario template task.5
</bodyText>
<tableCaption confidence="0.999008">
Table 1: Hold-One-Out Cross Validation
</tableCaption>
<table confidence="0.98850475">
Slots Recall Precision
All 79% 67%
Participant 75% 84%
Date/Location 86% 54%
</table>
<tableCaption confidence="0.912124">
Table 2: Hold-One-Out Cross Validation
without Generalization
</tableCaption>
<table confidence="0.97477">
Slots Recall Precision
All 61% 64%
Participant 57% 81%
Date/Location 67% 52%
</table>
<bodyText confidence="0.999811333333333">
A breakdown by slot is shown in Table 1. We
may note that precision is low for date and
location slots because we used a simplistic
sentence-level merge, rather than dependencies.
To measure the impact of our approach to
generalization, we may compare the results in
</bodyText>
<page confidence="0.857231">
5
</page>
<bodyText confidence="0.953086863636364">
http://wvvw.nistgov/itl/div894/894.02/related_project
thipsterimuc.htm
Table 1 with those shown in Table 2, where
generalization is not used. As can be seen, the
generalization step adds substantially to overall
recall.
To illustrate the effect of generalization,
consider the pattern to extract the subject NP of
the light verb `kac (hold)&apos; when paired with an
object NP headed by the noun tyepsang
(negotiation)&apos;. Since this pattern only occurs
once in our corpus, the slot is not successfully
extracted in the cross-validation test without
generalization. However, since this example
does fall under the more generalized pattern of
extracting the subject NP of a verb in the light
verb class when paired with an object NP
headed by a noun the `hoytam-hyepsang&apos; class,
the slot is successfully extracted in the cross-
validation test using the generalized patterns.
Cases like these are the source of the 18% boost
in recall of participant slots, from 57% to 75%.
</bodyText>
<sectionHeader confidence="0.997694" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999985210526316">
Our feasibility study has focused our attention
on several questions concerning the interaction
of IE and MT, which we hope to pursue under
the DARPA TIDES initiative. One question is
the extent to which slot filler translation is more
practicable than general-purpose MT; one would
expect to achieve much higher quality on slot
fillers, as they are typically relatively brief noun
phrases, and instantiation of a slot implies a
degree of semantic classification. On the other
hand, one might find that higher quality is
required in order to take translated phrases out
of their original context. Another question is
how to automate the construction of bilingual
lexicons. An important issue here will be how
to combine information from different sources,
given that automatically acquired lexical
information is apt to be less reliable, though
domain-specific.
</bodyText>
<sectionHeader confidence="0.997513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99981525">
Our thanks go to Richard Kittredge and Tanya
Korelsky for helpful comments and advice. This
work was supported by ARL contract DAAD17-
99-C-0005.
</bodyText>
<page confidence="0.997307">
36
</page>
<sectionHeader confidence="0.846763" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999733234375">
Cardie, C. (1997). Empirical Methods in Information
Extraction. Al Magazine 18(4):65-79.
Lavoie, B. and Rambow, 0. (1997). RealPro — A
fast, portable sentence realizer. In Proceedings of
the Conference on Applied Natural Language
Processing (ANLP&apos;97), Washington, DC.
Lavoie, B., Korelsky, T., and Rambow, 0. (2000). A
Framework for MT and Multilingual NLG Systems
Based on Uniform Lexico-Structural Processing.
To appear in Proceedings of the Sixth Conference
on Applied Natural Language Processing (ANLP-
2000), Seattle, WA.
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J.,
Riloff, E., and Soderland, S. (1992). University of
Massachusetts: Description of the CIRCUS system
as used in MUC-4. In Proceedings of the Fourth
Message Understanding Conference (MUC-4),
pages 282-288, San Mateo, CA. Morgan
Kaufmann.
MUC-5 (1994). Proceedings of the Fifth Message
Understanding Conference (MUC-5). Morgan
Kaufmann, San Mateo, CA.
MUC-7 (1998). Proceedings of the Seventh Message
Understanding Conference (MUC-7). Morgan
Kaufmann, San Francisco, CA.
Nasr, A., Rambow, 0., Palmer, M., and Rosenzweig,
J. (1997). Enriching lexical transfer with cross-
linguistic semantic features. In Proceedings of the
Interlingua Workshop at the MT Summit, San
Diego, CA.
Palmer, M., Rambow, 0., and Nasr, A. (1998).
Rapid prototyping of domain-specific machine
translation systems. In Machine Translation and
the Information Soup - Proceedings of the Third
Conference of the Association for Machine
Translation in the Americas AMTA&apos;98, Springer
Verlag (Lecture Notes in Artificial Intelligence No.
1529), Berlin.
Riloff, E. (1993). Automatically constructing a
dictionary for information extraction tasks. In
Proceedings of the Eleventh National Conference
on Artificial Intelligence, pages 811-816,
Washington, DC. AAAI Press / MIT Press.
Thompson, C. A., Califf, M. E., and Mooney, R. J.
(1999). Active learning for natural language
parsing and information extraction. In Proceedings
of the Sixteenth International Machine Learning
Conference (ICML-99), Bled, Slovenia.
White, M. and Caldwell, T. (1998). EXEMPLARS: A
practical, extensible framework for dynamic text
generation. In Proceedings of the 8th International
Workshop on Natural Language Generation,
Niagara-on-the-Lake, Ontario.
Yoon, J. (1999). Efficient dependency parsing based
on three types of chunking and lexical association.
Submitted.
Yoon, J., Choi, K.-S., and Song, M. (1999). Three
types of chunking in Korean and dependency
analysis based on lexical association. In
Proceedings of ICCPOL.
Yoon, J., Kim, S., and Song, M. (1997). New parsing
method using global association table. In
Proceedings of the 5th International Workshop on
Parsing Technology.
</reference>
<page confidence="0.999602">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.278180">
<title confidence="0.9996295">Towards Translingual Information using Portable Information Extraction</title>
<author confidence="0.997414">Michael White</author>
<author confidence="0.997414">Claire Cardie</author>
<author confidence="0.997414">Chung-hye Han</author>
<author confidence="0.997414">Nan i Benoit Lavoie</author>
<author confidence="0.997414">Martha Palmer</author>
<author confidence="0.997414">Owen Rambow</author>
<author confidence="0.997414">Juntae Yoon</author>
<affiliation confidence="0.99905">CoGenTex, Inc. Institute for Research Dept. of Computer</affiliation>
<address confidence="0.810603">Ithaca, NY, Cognitive</address>
<affiliation confidence="0.964188">cogentex.com University of Pennsylvania Ithaca, NY,</affiliation>
<address confidence="0.869336">Philadelphia, PA, cardie@cs.cornell.edu</address>
<email confidence="0.69181">chunghye@babel.ling.upenn@linc.cis.upenn.edu</email>
<abstract confidence="0.998639166666667">We report on a small study undertaken to demonstrate the feasibility of combining portable information extraction with MT in order to support translingual information access. After describing the system&apos;s usage scenario and system design, we describe our investigation of transferring information extraction techniques developed for English to Korean. We conclude with a brief discussion of related MT issues we plan to investigate in future work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<date>1997</date>
<booktitle>Empirical Methods in Information Extraction. Al Magazine</booktitle>
<pages>18--4</pages>
<contexts>
<context position="1826" citStr="Cardie, 1997" startWordPosition="257" endWordPosition="258">tter enable analysts to perform information filtering tasks on foreign language documents. This effort was funded by a SBIR Phase I award from the U.S. Army Research Lab, and will be pursued further under the DARPA TIDES initiative. Information extraction (IE) systems are designed to extract specific types of information from natural language texts. In order to achieve acceptable accuracy, IE systems need to be tuned for a given topic domain. Since this domain tuning can be labor intensive, recent IE research has focused on developing learning algorithms for training IF system components (cf. Cardie, 1997, for a survey). To date, however, little work has been done on 1E systems for languages other than English (though cf. MUC-5, 1994, and MUC-7, 1998, for Japanese IE systems); and, to our knowledge, none of the available techniques for the core task of learning information extraction patterns have been extended or evaluated for multilingual information extraction (though again cf. MUC-7, 1998, where the use of learning techniques for the IE subtasks of named entity recognition and coreference resolution are described). Given this situation, the primary objective of our study was to demonstrate</context>
</contexts>
<marker>Cardie, 1997</marker>
<rawString>Cardie, C. (1997). Empirical Methods in Information Extraction. Al Magazine 18(4):65-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
<author>Rambow</author>
</authors>
<title>RealPro — A fast, portable sentence realizer.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Applied Natural Language Processing (ANLP&apos;97),</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="8793" citStr="Lavoie and Rambow, 1997" startWordPosition="1318" endWordPosition="1321">orresponding Korean ones. When the match falls below a userconfigurable threshold, the extracted information is filtered out. • The MT component. The MT component (cf. Lavoie et al., 2000) translates the extracted Korean phrases or sentences into corresponding English ones. • The Presentation Generator component. This component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information. It uses existing NLG components, including the Exemplars text planning framework (White and Caldwell, 1998) and the RealPro syntactic realizer (Lavoie and Rambow, 1997). In our feasibility study, the majority of the effort went towards developing the PIE component, described in the next section. This component was implemented in a general way, i.e. in a way that we would expect to work beyond the specific training/test corpus described below. In contrast, we only implemented initial versions of the User Interface, Ranker and Presentation Generator components, in order to demonstrate the system concept; that is, these initial versions were only intended to work with our training/test corpus, and will require considerable further development prior to reaching </context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>Lavoie, B. and Rambow, 0. (1997). RealPro — A fast, portable sentence realizer. In Proceedings of the Conference on Applied Natural Language Processing (ANLP&apos;97), Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lavoie</author>
<author>T Korelsky</author>
<author>Rambow</author>
</authors>
<title>A Framework for</title>
<date>2000</date>
<booktitle>MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing. To appear in Proceedings of the Sixth Conference on Applied Natural Language Processing (ANLP2000),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="8357" citStr="Lavoie et al., 2000" startWordPosition="1263" endWordPosition="1266">set of extraction patterns learned in the lab, one set per scenario template — to extract specific types of information from the input Korean documents, once parsed. • The Ranker component. This component ranks the extracted information returned by the PIE component according to how well it matches the keyword restrictions in the query. The MT component&apos;s English-toKorean Transfer Lexicon is used to map the English keywords to corresponding Korean ones. When the match falls below a userconfigurable threshold, the extracted information is filtered out. • The MT component. The MT component (cf. Lavoie et al., 2000) translates the extracted Korean phrases or sentences into corresponding English ones. • The Presentation Generator component. This component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information. It uses existing NLG components, including the Exemplars text planning framework (White and Caldwell, 1998) and the RealPro syntactic realizer (Lavoie and Rambow, 1997). In our feasibility study, the majority of the effort went towards developing the PIE component, described in the next section. This component was implemented in a</context>
<context position="9626" citStr="Lavoie et al., 2000" startWordPosition="1448" endWordPosition="1451">to work beyond the specific training/test corpus described below. In contrast, we only implemented initial versions of the User Interface, Ranker and Presentation Generator components, in order to demonstrate the system concept; that is, these initial versions were only intended to work with our training/test corpus, and will require considerable further development prior to reaching operational status. For the MT component, we used an early version of the lexical transfer–based system currently under development in an ongoing SBIR Phase II project (cf. Nasr et al., 1997; Palmer et al., 1998; Lavoie et al., 2000), though with a limited lexicon specifically for translating the slot fillers in our training/test corpus. 33 Figure 2 component End user Document Processing Knowledge base component POS Tagger Korean Grammar Parser Extracted Information (Korean) Tagged Korean Documents Ordered Extracted Information (Korean) Korean Lexicon yUser Input Data ,TransIA, **face Presentation (English) Parsed Document Machine Translation Component (MT) Korean-English Transfer Lexicon English Grammar English Lexicon RealPro Syntactic Realizer Ordered Extracted Information (English) Syntactic Structure (English) Senten</context>
</contexts>
<marker>Lavoie, Korelsky, Rambow, 2000</marker>
<rawString>Lavoie, B., Korelsky, T., and Rambow, 0. (2000). A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing. To appear in Proceedings of the Sixth Conference on Applied Natural Language Processing (ANLP2000), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>D Fisher</author>
<author>J McCarthy</author>
<author>E Riloff</author>
<author>S Soderland</author>
</authors>
<title>University of Massachusetts: Description of the CIRCUS system as used in MUC-4.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth Message Understanding Conference (MUC-4),</booktitle>
<pages>282--288</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="11976" citStr="Lehnert et al., 1992" startWordPosition="1798" endWordPosition="1801"> for a single long sentence. When compared to the MUC scenario template task, our extraction task was considerably simpler, for the following reasons: • The answer keys only contained information that could be found within a single sentence, i.e. the answer keys did not require merging information across sentences. • The answer &apos;keys did not require anaphoric references to be resolved, and we did not deal with conjuncts separately. • We did not attempt to normalize dates or remove appositives from NPs. 4.2 Extraction Pattern Learning For our feasibility study, we chose to follow the AutoSlog (Lehnert et al., 1992; Riloff, 1993) approach to extraction pattern acquisition. In this approach, extraction patterns are acquired 34 1. E: &lt;target-np&gt;=&lt;subject&gt; &lt;active voice verb&gt; &lt;participant&gt; MET K: &lt;target-np&gt;=&lt;subject&gt; &lt;active voice verb&gt; &lt;John-i&gt; MANNASSTA &lt;John-nom&gt;&amp;quot;MET 2. E: &lt;target-np&gt;=&lt;subject&gt; &lt;verb&gt; &lt;infinitive&gt; &lt;participant&gt; agreed to MEET K: &lt;target-np&gt;=&lt;subject&gt; &lt;verbl-ki-lo&gt; &lt;verb2&gt; &lt;John-un&gt; MANNA-ki-lo hapuyhayssta &lt;John-nom&gt; MEET-ki-lo agreed (-ki: nominalization ending, -lo: an adverbial postposition) Figure 3 via a one-shot general-to-specific learning algorithm designed specifically for the</context>
</contexts>
<marker>Lehnert, Cardie, Fisher, McCarthy, Riloff, Soderland, 1992</marker>
<rawString>Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., and Soderland, S. (1992). University of Massachusetts: Description of the CIRCUS system as used in MUC-4. In Proceedings of the Fourth Message Understanding Conference (MUC-4), pages 282-288, San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<date>1994</date>
<booktitle>Proceedings of the Fifth Message Understanding Conference (MUC-5).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<marker>1994</marker>
<rawString>MUC-5 (1994). Proceedings of the Fifth Message Understanding Conference (MUC-5). Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<marker>1998</marker>
<rawString>MUC-7 (1998). Proceedings of the Seventh Message Understanding Conference (MUC-7). Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>J Rosenzweig</author>
</authors>
<title>Enriching lexical transfer with crosslinguistic semantic features.</title>
<date>1997</date>
<booktitle>In Proceedings of the Interlingua Workshop at the MT Summit,</booktitle>
<location>San Diego, CA.</location>
<marker>Palmer, Rosenzweig, 1997</marker>
<rawString>Nasr, A., Rambow, 0., Palmer, M., and Rosenzweig, J. (1997). Enriching lexical transfer with crosslinguistic semantic features. In Proceedings of the Interlingua Workshop at the MT Summit, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>Rambow</author>
</authors>
<title>Rapid prototyping of domain-specific machine translation systems.</title>
<date>1998</date>
<booktitle>In Machine Translation and the Information Soup - Proceedings of the Third Conference of the Association for Machine Translation in the Americas AMTA&apos;98, Springer Verlag (Lecture Notes in Artificial Intelligence No. 1529),</booktitle>
<location>Berlin.</location>
<marker>Palmer, Rambow, 1998</marker>
<rawString>Palmer, M., Rambow, 0., and Nasr, A. (1998). Rapid prototyping of domain-specific machine translation systems. In Machine Translation and the Information Soup - Proceedings of the Third Conference of the Association for Machine Translation in the Americas AMTA&apos;98, Springer Verlag (Lecture Notes in Artificial Intelligence No. 1529), Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence,</booktitle>
<pages>811--816</pages>
<publisher>AAAI Press / MIT Press.</publisher>
<location>Washington, DC.</location>
<contexts>
<context position="11991" citStr="Riloff, 1993" startWordPosition="1802" endWordPosition="1803">tence. When compared to the MUC scenario template task, our extraction task was considerably simpler, for the following reasons: • The answer keys only contained information that could be found within a single sentence, i.e. the answer keys did not require merging information across sentences. • The answer &apos;keys did not require anaphoric references to be resolved, and we did not deal with conjuncts separately. • We did not attempt to normalize dates or remove appositives from NPs. 4.2 Extraction Pattern Learning For our feasibility study, we chose to follow the AutoSlog (Lehnert et al., 1992; Riloff, 1993) approach to extraction pattern acquisition. In this approach, extraction patterns are acquired 34 1. E: &lt;target-np&gt;=&lt;subject&gt; &lt;active voice verb&gt; &lt;participant&gt; MET K: &lt;target-np&gt;=&lt;subject&gt; &lt;active voice verb&gt; &lt;John-i&gt; MANNASSTA &lt;John-nom&gt;&amp;quot;MET 2. E: &lt;target-np&gt;=&lt;subject&gt; &lt;verb&gt; &lt;infinitive&gt; &lt;participant&gt; agreed to MEET K: &lt;target-np&gt;=&lt;subject&gt; &lt;verbl-ki-lo&gt; &lt;verb2&gt; &lt;John-un&gt; MANNA-ki-lo hapuyhayssta &lt;John-nom&gt; MEET-ki-lo agreed (-ki: nominalization ending, -lo: an adverbial postposition) Figure 3 via a one-shot general-to-specific learning algorithm designed specifically for the information ex</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Riloff, E. (1993). Automatically constructing a dictionary for information extraction tasks. In Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 811-816, Washington, DC. AAAI Press / MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Thompson</author>
<author>M E Califf</author>
<author>R J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Machine Learning Conference (ICML-99),</booktitle>
<location>Bled, Slovenia.</location>
<contexts>
<context position="13899" citStr="Thompson et al. (1999)" startWordPosition="2087" endWordPosition="2090">reater flexibility in word order and heavier reliance on morphological cues in Korean, and (2) the predominance of light verbs (verbs with little semantic content of their own) and aspectual verbs in the chosen domain. We discuss these issues in the next two sections. 4.3 Korean Parser We used Yoon&apos;s hybrid statistical Korean parser (Yoon et al., 1997, 1999; Yoon, 1999) to process the input sentences prior to extraction. The parser incorporates a POS tagger and 3 For TIDES, we plan to use more sophisticated learning algorithms, as well as active learning techniques, such as those described in Thompson et al. (1999). morphological analyzer and yields a dependency representation as its output.4 The use of a dependency representation enabled us to handle the greater flexibility in word order in Korean. To facilitate pattern matching, we wrote a simple program to convert the parser&apos;s output to XML form. During the XML conversion, two simple heuristics were applied, one to recover implicit subjects, and another to correct a recurring misanalysis of noun compounds. 4.4 Trigger Word Filtering and Generalization In the newswire corpus we looked at, meeting events were rarely described with the verb `mannata&apos; (&apos;</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Thompson, C. A., Califf, M. E., and Mooney, R. J. (1999). Active learning for natural language parsing and information extraction. In Proceedings of the Sixteenth International Machine Learning Conference (ICML-99), Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>T Caldwell</author>
</authors>
<title>EXEMPLARS: A practical, extensible framework for dynamic text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 8th International Workshop on Natural Language Generation,</booktitle>
<location>Niagara-on-the-Lake, Ontario.</location>
<contexts>
<context position="8732" citStr="White and Caldwell, 1998" startWordPosition="1309" endWordPosition="1312">rean Transfer Lexicon is used to map the English keywords to corresponding Korean ones. When the match falls below a userconfigurable threshold, the extracted information is filtered out. • The MT component. The MT component (cf. Lavoie et al., 2000) translates the extracted Korean phrases or sentences into corresponding English ones. • The Presentation Generator component. This component generates well-organized, easy-to-read hypertext presentations by organizing and formatting the ranked extracted information. It uses existing NLG components, including the Exemplars text planning framework (White and Caldwell, 1998) and the RealPro syntactic realizer (Lavoie and Rambow, 1997). In our feasibility study, the majority of the effort went towards developing the PIE component, described in the next section. This component was implemented in a general way, i.e. in a way that we would expect to work beyond the specific training/test corpus described below. In contrast, we only implemented initial versions of the User Interface, Ranker and Presentation Generator components, in order to demonstrate the system concept; that is, these initial versions were only intended to work with our training/test corpus, and wil</context>
</contexts>
<marker>White, Caldwell, 1998</marker>
<rawString>White, M. and Caldwell, T. (1998). EXEMPLARS: A practical, extensible framework for dynamic text generation. In Proceedings of the 8th International Workshop on Natural Language Generation, Niagara-on-the-Lake, Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yoon</author>
</authors>
<title>Efficient dependency parsing based on three types of chunking and lexical association.</title>
<date>1999</date>
<note>Submitted.</note>
<contexts>
<context position="13649" citStr="Yoon, 1999" startWordPosition="2048" endWordPosition="2049">in Figure 3. It turned out that for our corpus, we could collapse some of these patterns, though some new ones were also needed. In the end we used just nine generic patterns. Important issues that arose in adapting the approach were (1) greater flexibility in word order and heavier reliance on morphological cues in Korean, and (2) the predominance of light verbs (verbs with little semantic content of their own) and aspectual verbs in the chosen domain. We discuss these issues in the next two sections. 4.3 Korean Parser We used Yoon&apos;s hybrid statistical Korean parser (Yoon et al., 1997, 1999; Yoon, 1999) to process the input sentences prior to extraction. The parser incorporates a POS tagger and 3 For TIDES, we plan to use more sophisticated learning algorithms, as well as active learning techniques, such as those described in Thompson et al. (1999). morphological analyzer and yields a dependency representation as its output.4 The use of a dependency representation enabled us to handle the greater flexibility in word order in Korean. To facilitate pattern matching, we wrote a simple program to convert the parser&apos;s output to XML form. During the XML conversion, two simple heuristics were appli</context>
<context position="15350" citStr="Yoon, 1999" startWordPosition="2317" endWordPosition="2318">traction patterns that made appropriate use of such collocations, we decided to go beyond the AutoSlog approach and explicitly group trigger words (such as `hoyuy&apos;) into classes, and to likewise group any collocations, such as those involving light verbs or aspectual verbs. To find collocations for the trigger words, we reviewed a Korean lexical cooccurrence base which was constructed from a corpus of 40 million words (Yoon et al., 1997). We then used the resulting specification to filter the learned patterns to just those containing the 4 Overall dependency precision is reported to be 89.4% (Yoon, 1999). 35 trigger words or trigger word collocations, as well as to generalize the patterns to the word class level. Because the number of trigger words is small, this specification can be done quickly, and soon pays off in terms of time saved in manually filtering the learned patterns. 4.5 Results In testing our approach, we obtained overall results of 79% recall and 67% precision in a hold-one-out cross validation test. In a cross validation test, one repeatedly divides a corpus into different training and test sets, averaging the results; in the hold-one-out version, the system is tested on a he</context>
</contexts>
<marker>Yoon, 1999</marker>
<rawString>Yoon, J. (1999). Efficient dependency parsing based on three types of chunking and lexical association. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yoon</author>
<author>K-S Choi</author>
<author>M Song</author>
</authors>
<title>Three types of chunking in Korean and dependency analysis based on lexical association.</title>
<date>1999</date>
<booktitle>In Proceedings of ICCPOL.</booktitle>
<marker>Yoon, Choi, Song, 1999</marker>
<rawString>Yoon, J., Choi, K.-S., and Song, M. (1999). Three types of chunking in Korean and dependency analysis based on lexical association. In Proceedings of ICCPOL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yoon</author>
<author>S Kim</author>
<author>M Song</author>
</authors>
<title>New parsing method using global association table.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th International Workshop on Parsing Technology.</booktitle>
<contexts>
<context position="13630" citStr="Yoon et al., 1997" startWordPosition="2043" endWordPosition="2046">, two of which are shown in Figure 3. It turned out that for our corpus, we could collapse some of these patterns, though some new ones were also needed. In the end we used just nine generic patterns. Important issues that arose in adapting the approach were (1) greater flexibility in word order and heavier reliance on morphological cues in Korean, and (2) the predominance of light verbs (verbs with little semantic content of their own) and aspectual verbs in the chosen domain. We discuss these issues in the next two sections. 4.3 Korean Parser We used Yoon&apos;s hybrid statistical Korean parser (Yoon et al., 1997, 1999; Yoon, 1999) to process the input sentences prior to extraction. The parser incorporates a POS tagger and 3 For TIDES, we plan to use more sophisticated learning algorithms, as well as active learning techniques, such as those described in Thompson et al. (1999). morphological analyzer and yields a dependency representation as its output.4 The use of a dependency representation enabled us to handle the greater flexibility in word order in Korean. To facilitate pattern matching, we wrote a simple program to convert the parser&apos;s output to XML form. During the XML conversion, two simple he</context>
<context position="15180" citStr="Yoon et al., 1997" startWordPosition="2288" endWordPosition="2291">t stands for &apos;meeting&apos; and a light or aspectual verb, for example, `hoyuy-lul kacta&apos; (&apos;to have a meeting&apos;) or `hoyuy-lul machita&apos; (&apos;to finish a meeting&apos;). In order to acquire extraction patterns that made appropriate use of such collocations, we decided to go beyond the AutoSlog approach and explicitly group trigger words (such as `hoyuy&apos;) into classes, and to likewise group any collocations, such as those involving light verbs or aspectual verbs. To find collocations for the trigger words, we reviewed a Korean lexical cooccurrence base which was constructed from a corpus of 40 million words (Yoon et al., 1997). We then used the resulting specification to filter the learned patterns to just those containing the 4 Overall dependency precision is reported to be 89.4% (Yoon, 1999). 35 trigger words or trigger word collocations, as well as to generalize the patterns to the word class level. Because the number of trigger words is small, this specification can be done quickly, and soon pays off in terms of time saved in manually filtering the learned patterns. 4.5 Results In testing our approach, we obtained overall results of 79% recall and 67% precision in a hold-one-out cross validation test. In a cros</context>
</contexts>
<marker>Yoon, Kim, Song, 1997</marker>
<rawString>Yoon, J., Kim, S., and Song, M. (1997). New parsing method using global association table. In Proceedings of the 5th International Workshop on Parsing Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>