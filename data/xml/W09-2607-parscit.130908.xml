<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000412">
<title confidence="0.6792525">
Using Artificially Generated Data
to Evaluate Statistical Machine Translation
</title>
<author confidence="0.764943">
Manny Rayner, Paula Estrella, Pierrette Bouillon
</author>
<affiliation confidence="0.988832">
University of Geneva, TIM/ISSCO
</affiliation>
<address confidence="0.942128">
40 bvd du Pont-d&apos;Arve, CH-1211 Geneva 4, Switzerland
</address>
<email confidence="0.96187">
{Emmanuel.Rayner,Paula.Estrella,Pierrette.Bouillon}@unige.ch
</email>
<author confidence="0.7711835">
Beth Ann Hockey
Mail Stop 19-26, UCSC UARC
</author>
<affiliation confidence="0.93431">
NASA Ames Research Center, Moffett Field, CA 94035–1000
</affiliation>
<email confidence="0.995682">
bahockey@ucsc.edu
</email>
<author confidence="0.953083">
Yukie Nakao
</author>
<affiliation confidence="0.874497">
LINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex 03
</affiliation>
<email confidence="0.479252">
yukie.nakao@univ-nantes.fr
</email>
<sectionHeader confidence="0.911811" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993820303030303">
AlthoughStatisticalMachineTranslation
(SMT) is now the dominant paradigm
within Machine Translation, we argue that
it is far from clear that it can outperform
Rule-Based Machine Translation (RBMT)
on small- to medium-vocabulary applica-
tions where high precision is more impor-
tant than recall. A particularly important
practical example is medical speech trans-
lation. We report the results of exper-
iments where we configured the various
grammars and rule-sets in an Open Source
medium-vocabulary multi-lingual medical
speech translation system to generate large
aligned bilingual corpora for English —�
French and English —� Japanese, which
were then used to train SMT models based
on the common combination of Giza++,
Moses and SRILM. The resulting SMTs
were unable fully to reproduce the per-
formance of the RBMT, with performance
topping out, even for English —� French,
with less than 70% of the SMT translations
of previously unseen sentences agreeing
with RBMT translations. When the out-
puts of the two systems differed, human
judges reported the SMT result as fre-
quently being worse than the RBMT re-
sult, and hardly ever better; moreover, the
added robustness of the SMT only yielded
a small improvement in recall, with a large
penalty in precision.
IIntroduction
WhenStatisticalMachineTranslation(SMT)was
first introduced in the early 90s, it encountered a
hostile reception, and many people in the research
community were unwilling to believe it could ever
be a serious competitor to symbolic approaches
(cf. for example (Arnold et al., 1994)). The pendu-
lum has now swung all the way to the other end of
the scale; right now, the prevailing wisdom within
the research community is that SMT is the only
truly viable architecture, and that rule-based ma-
chine translation (RBMT) is ultimately doomed to
failure. In this paper, one of our initial concerns
will be to argue for a compromise position. In our
opinion, the initial scepticism about SMT was not
groundless; the arguments presented against it of-
ten took the form of examples involving deep lin-
guistic reasoning, which, it was claimed, would be
hard to address using surface methods. Proponents
of RBMT had, however, greatly underestimated
the extent to which SMT would be able to tackle
the problem of robustness, where it appears to be
far more powerful than RBMT. For most machine
translation applications, robustness is the central
issue, so SMT&apos;s current preeminence is hardly sur-
prising.
Even for the large-vocabulary tasks where SMT
does best, the situation is by no means as clear as
one might imagine: according to (Wilks, 2007),
purely statistical systems are still unable to out-
perform SYSTRAN. In this paper, we will how-
ever be more concerned with limited-domain MT
tasks, where robustness is not the key requirement,
andaccuracyisparamount.Animediateexam-
</bodyText>
<page confidence="0.992854">
54
</page>
<note confidence="0.9989895">
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54–62,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.995890891089109">
ple is medical speech translation, which is estab-
lishing itself as an an application area of some sig-
nificance (Bouillon et al., 2006; Bouillon et al.,
2008a). Translation in medical applications needs
to be extremely accurate, since mistranslations
can have serious or even fatal consequences. At
the panel discussion at the 2008 COLING work-
shop on safety-critical speech translation (Rayner
et al., 2008), the consensus opinion, based on in-
put from practising physicians, was that an appro-
priate evaluation metric for medical applications
would be heavily slanted towards accuracy, as op-
posed to robustness. If the metric is normalised so
as to award 0 points for no translation, and 1 point
for a correct translation, the estimate was that a
suitable score for an incorrect translation would
be something between –25 and –100 points. With
these requirements, it seems unlikely that a robust,
broad-coverage architecture has much chance of
success. The obvious strategy is to build a limited-
domain controlled-language system, and tune it to
the point where accuracy reaches the desired level.
For systems of this kind, it is at least conceiv-
able that RBMT may be able to outperform SMT.
The next question is how to investigate the issues
in a methodologically even-handed way. A few
studies, notably (Seneff et al., 2006), suggest that
rule-based translation may in fact be preferable in
these cases. (Another related experiment is de-
scribed in (Dugast et al., 2008), though this was
carried out in a large-vocabulary system). These
studies, however, have not been widely cited. One
possible explanation is suspicion about method-
ological issues. Seneff and her colleagues trained
their SMT system on 20 000 sentence pairs, a
small number by the standards of SMT. It is a pri-
ori not implausible that more training data would
have enabled them to create an SMT system that
was as good as, or better than, the rule-based sys-
tem.
In this paper, our primary goal is to take this
kind of objection seriously, and develop a method-
ology designed to enable a tight comparison be-
tween rule-based and statistical architectures. In
particular, we wish to examine the widely be-
lieved claim that SMT is now inherently better
than RBMT. In order to do this, we start with a
limited-domain RBMT system; we use it to auto-
matically generate a large corpus of aligned pairs,
which is used to train a corresponding SMT sys-
tem. We then compare the performance of the two
systems.
Our argument will be that this situation essen-
tially represents an upper bound for what is possi-
ble using the SMT approach in a limited domain.
It has been widely remarked that quality, as well
as quantity, of training data is important for good
SMT; in many projects, significant effort is ex-
pended to clean the original training data. Here,
since the data is automatically generated by a rule-
based system, we can be sure that it is already
completely clean (in the sense of being internally
consistent), and we can generate as large a quan-
tity of it as we require. The application, more-
over, uses only a smallish vocabulary and a fairly
constrained syntax. If the derived SMT system is
unable to match the original RBMT system&apos;s per-
formance, it seems reasonable to claim that this
shows that there are types of applications where
RBMT architectures are superior.
The experiments described have been carried
out using MedSLT, an Open Source interlingua-
based limited-domain medical speech translation
system. The rest of the paper is organised as fol-
lows. Section 2 provides background on the Med-
SLT system. Section 3 describes the experimen-
tal framework, and Section 4 the results obtained.
Section5concludes.
2TheMedSLTSystem
MedSLT(Bouillonetal.,2005;Bouillonetal.,
2008b) is a medium-vocabulary interlingua-based
Open Source speech translation system for doctor-
patient medical examination questions, which
provides any-language-to-any-language transla-
tion capabilities for all languages in the set En-
glish, French, Japanese, Arabic, Catalan. Both
speech recognition and translation are rule-based.
Speech recognition runs on the Nuance 8.5 recog-
nition platform, with grammar-based language
models built using the Open Source Regulus com-
piler. As described in (Rayner et al., 2006),
each domain-specific language model is extracted
from a general resource grammar using corpus-
based methods driven by a seed corpus of domain-
specific examples. The seed corpus, which typi-
cally contains between 500 and 1500 utterances,
is then used a second time to add probabilistic
weights to the grammar rules; this substantially
improves recognition performance (Rayner et al.,
2006, § 1 1 .5). Vocabulary sizes and performance
measures for speech recognition in the three lan-
</bodyText>
<page confidence="0.997507">
55
</page>
<bodyText confidence="0.9924145">
guages where serious evaluations have been car-
ried out are shown in Figure 1.
</bodyText>
<table confidence="0.99915375">
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
</table>
<tableCaption confidence="0.999903">
Table 1: Recognition performance for English,
</tableCaption>
<bodyText confidence="0.994221173913044">
French and Japanese MedSLT recognisers. “Vo-
cab” = number of surface words in source lan-
guage recogniser vocabulary; “WER” = Word Er-
ror Rate for source language recogniser, on in-
coverage material; “SemER” = semantic error rate
(proportion of utterances failing to produce correct
interlingua) for source language recogniser, on in-
coverage material.
At run-time, the recogniser produces a source-
langage semantic representation. This is first
translated by one set of rules into an interlingual
form, and then by a second set into a target lan-
guage representation. A target-language Regu-
lus grammar, compiled into generation form, turns
this into one or more possible surface strings, af-
ter which a set of generation preferences picks
one out. Finally, the selected string is realised in
spoken form. Robustness issues are addressed by
means of a back-up statistical recogniser, which
drives a robust embedded help system. The pur-
pose of the help system (Chatzichrisafis et al.,
2006) is to guide the user towards supported cov-
erage; it performs approximate matching of out-
put from the statistical recogniser again a library
of sentences which have been marked as correctly
processed during system development, and then
presents the closest matches to the user.
Examples of typical English domain sentences
and their translations into French and Japanese are
shown in Figure 2.
3Experimentalframework
In the literature on language modelling, there is
a known technique for bootstrapping a statisti-
cal language model (SLM) from a grammar-based
language model (GLM). The grammar which
forms the basis of the GLM is sampled randomly
in order to create an arbitrarily large corpus of ex-
amples; these examples are then used as a train-
ing corpus to build the SLM (Jurafsky et al., 1995;
Jonson, 2005). We adapt this process in a straight-
forward way to construct an SMT for a given
language pair, using the source language gram-
mar, the source-to-interlingua translation rules, the
interlingua-to-target-language rules, and the tar-
get language generation grammar. We start in the
same way, using the source language grammar to
build a randomly generated source language cor-
pus; as shown in (Hockey et al., 2008), it is im-
portant to have a probabilistic grammar. We then
use the composition of the other components to
attempt to translate each source language sentence
into a target language equivalent, discarding the
examples for which no translation is produced.
The result is an aligned bilingual corpus of ar-
bitrary size, which can be used to train an SMT
model.
We used this method to generate aligned cor-
pora for the two MedSLT language pairs English
—� French and English —� Japanese. For each lan-
guage pair, we first generated one million source-
language utterances; we next filtered them to keep
only examples which were full sentences, as op-
posed to elliptical phrases, and finally used the
translation rules and target-language generators to
attempt to translate each sentence. This created
approximately 305K aligned sentence-pairs for
English —� French (1901K words English, 1993K
words French), and 31 1 K aligned sentence-pairs
for English —� Japanese (1941K words English,
2214K words Japanese). We held out 2.5% of
each set as development data, and 2.5% as test
data. Using Giza++, Moses and SRILM (Och and
Ney, 2000; Koehn et al., 2007; Stolcke, 2002), we
trained SMT models from increasingly large sub-
sets of the training portion, using the development
portion in the usual way to optimize parameter val-
ues. Finally, we used the resulting models to trans-
late the test portion.
Our primary goal was to measure the extent to
which the derived versions of the SMT were able
to approximate the original RBMT on data which
was within the RBMT&apos;s coverage. There is a sim-
ple and natural way to perform this measurement:
we apply the BLEU metric (Papineni et al., 2001),
with the RBMT&apos;s translation taken as the refer-
ence. This means that perfect correspondence be-
tween the two translations would yield a BLEU
scoreof1.0.
This raises an important point. The BLEU
scores we are using here are non-standard; they
measure the extent to which the SMT approxi-
mates the RBMT, rather than, as usual, measuring
</bodyText>
<page confidence="0.980621">
56
</page>
<bodyText confidence="0.967912711864406">
English Is the pain above your eye?
French Avez-vous mal au dessus des yeux?
Japanese Itami wa me no ue no atari desu ka?
English Have you had the pain for more than a month?
French Avez-vous mal depuis plus d&apos;un mois?
Japanese Ikkagetsu ijou itami wa tsuzuki mashita ka?
English Is the pain associated with nausea?
French Avez-vous des nausees quand vous avez la douleur?
Japanese Itamu to hakike wa okori masu ka?
English Does bright light make the pain worse?
French La douleur est-elle aggravee par une lumiere forte?
Japanese Akarui hikari wo miru to zutsu wa hidoku nari masu ka?
Table2:ExamplesofEnglishdomainsentences,andthesystem&apos;stranslationsintoFrenchandJapanese.
the extent to which it approximates human trans-
lations. It is important to bring in human judge-
ment, to evaluate the cases where the SMT and
RBMT differ. If, in these cases, it transpired that
human judges typically thought that the SMT was
as good as the RBMT, then the difference would
be purely academic. We need to satisfy ourselves
that human judges typically ascribe differences be-
tween SMT and RBMT to shortcomings in the
SMT rather than in the RBMT.
Concretely, we collected all the different
(Source, SMT-translation, RBMT-translation)
triples produced during the course of the ex-
periments, and extracted those where the two
translations were different. We randomly selected
a set of examples for each language pair, and
asked human judges to classify them into one of
the following categories:
•RBMTbetter:TheRBMTtranslationwas
better,intermsofpreservingmeaningand/or
beinggramaticallycorect;
•SMTbetter:TheSMTtranslationwasbet-
ter,intermsofpreservingmeaningand/orbe-
inggramaticallycorect;
•Similar:Bothtranslationswereabout
equally good OR the source sentence was
meaningless in the domain.
In order to show that our metrics are intuitively
meaningful, it is sufficient to demonstrate that the
frequency of occurrence of RBMT better is both
large in comparison to that of SMT better, and
accounts for a substantial proportion of the total
population.
Finally, we consider the question of whether
the SMT, which is capable of translating out-of-
grammar sentences, can add useful robustness to
the base system. We collected, from the set used in
the experiments described in (Rayner et al., 2005),
all the English sentences which failed to be trans-
lated into French. We used the best version of
the English —� French SMT to translate each of
these sentences, and asked human judges to eval-
uate the translations as being clearly acceptable,
clearlyunacceptable,orborderline.
In the next section, we present the results of the
various experiments we have just described.
</bodyText>
<subsectionHeader confidence="0.641374">
4Results
</subsectionHeader>
<bodyText confidence="0.999958380952381">
We begin with Figure 1, which shows non-
standard BLEU scores for versions of the English
—� French SMT system trained on quantities of
data increasing from 14 287 to 285 740 pairs. As
can be seen, translation performance improves up
to about 175 000 pairs. After this, it levels out
at around BLEU = 0.90, well below that of the
RBMT system with which it is being compared.
A more direct way to report the result is simply to
count the proportion of test sentences that are not
in the training data, which are translated similarly
by the SMT and the RBMT. This figure tops out at
around 68%.
The results strongly suggest that the SMT is
unable to replicate the RBMT&apos;s performance at
all closely even in an easy language-pair, irre-
spective of the amount of training data available.
Out of curiosity, and to reassure ourselves that the
automatic generation procedure was doing some-
thing useful, we also tried training the English —�
French SMT on pairs derived from the 669 ut-
</bodyText>
<page confidence="0.995934">
57
</page>
<figureCaption confidence="0.812564">
Figure1:Non-standardBLEUscoresagainst
numberofpairsoftrainingsentencesforEnglish
! French; training and test data both indepen-
dently generated, hence overlapping.
</figureCaption>
<bodyText confidence="0.999485819672131">
terance“seedcorpus”usedtogeneratethegram-
mar(cf.Section2).Thisproducedutterlydis-
malperformance,withBLEU=0.52.Theresultis
moreinterestingthanitmayfirstappear,since,in
speech recognition, the difference in performance
between the SLMs trained from seed corpora and
large generated corpora is fairly small (Hockey et
al., 2008).
It seemed possible that the improvement in per-
formance with increased quantities of training data
might, in effect, only be due to the SMT func-
tioning as a translation memory; since training
and test data are independently generated by the
same random process, they overlap, with the de-
gree of overlap increasing as the training set gets
larger. In order to investigate this hypothesis,
we repeated the experiments with data which had
been uniqued, so that the training and test sets
were completely disjoint, and neither contained
any duplicate sentences&apos;. In fact, Figure 2 show
that the graph for uniqued English ! French data
are fairly similar to the one for the original non-
uniqued data shown in Figures 1. The main differ-
ence is that the non-standard BLEU score for the
&apos;Our opinion is that this is not a realistic way to evaluate
the performance of a small-vocabulary system; for example,
in MedSLT, one expects that at least some training sentences,
e.g. “Where is the pain?”, will also occur frequently in test
data.
Figure2:Non-standardBLEUscoresagainst
numberofpairsoftrainingsentencesforEnglish
! French; training and test data both indepen-
dently generated, then uniqued to remove dupli-
cates and overlapping items.
uniqueddata,unsurprisingly,topsoutatalower
level, reflecting the fact that a “translation mem-
ory” effect does indeed occur to some extent.
Results for English!Japanese showed the
same trends as English!French, but were more
pronounced. Table 3 compares the performance
of the best versions of the SMTs for the two
language-pairs, using both plain and artificially
uniqued data. We see that, with plain data, the
English ! Japanese SMT falls even further short
of replicating the performance of the RBMT than
was the case for English ! French; BLEU is
only 0.76. The difference between the plain and
uniqued versions is also more extreme. BLEU
(0.64) is considerably lower for the version trained
on uniqued data, suggesting that the SMT for this
language pair is finding it harder to generalise,
and is in effect closer to functioning as a trans-
lation memory. This is confirmed by counting
the sentences in test data and not in training data
which were translated similarly by the SMT and
the RBMT; we find that the figure tops out at the
verylowvalueof26%.
As noted in our discussion of the experimental
framework, the non-standard BLEU scores only
address the question of whether the performance
of the SMT and RBMT systems is the same. It is
</bodyText>
<page confidence="0.998408">
58
</page>
<table confidence="0.999866571428571">
Training data Test data BLEU
English —� French
Generated Generated 0.90
Gen/uniqued Gen/uniqued 0.85
English —� Japanese
Generated Generated 0.76
Gen/uniqued Gen/uniqued 0.64
</table>
<tableCaption confidence="0.999283">
Table 3: Translation performance, in terms of non-
</tableCaption>
<bodyText confidence="0.997961957446808">
standard BLEU metric, for different configura-
tions, training on all available data of the spec-
ified type. “Generated” = data randomly gener-
ated; “Gen/uniqued” = data randomly generated,
then uniqued so that duplicates are removed and
test and training pairs do not overlap.
necessarytoestablishwhatthediferencesmean
intermsofhumanjudgements.Weconsequently
turntoevaluationofthepairsforwhichtheSMT
andtheRBMTsystemsproduceddiferenttrans-
lationresults.
Table 4 shows the categorisation, according to
the criteria outlined at the end of Section 3, for 500
English —� French pairs randomly selected from
the set of examples where RBMT and SMT gave
different results; we asked three judges to evalu-
ate them independently, and combined their judg-
ments by majority decision where appropriate. We
observed a very heavy bias towards the RBMT,
with unanimous agreement among the judges that
the RBMT translation was better in 201/500 cases,
and 2-1 agreement in a further 127. In contrast,
there were only 4/500 cases where the judges
unanimously thought that the SMT translation was
preferable, with a further 12 supported by a ma-
jority decision. The rest of the table gives the
cases where the RBMT and SMT translations were
judged the same or cases in which the judges dis-
agreed; there were only 41/500 cases where no
majority decision was reached. Our overall con-
clusion is that we are justified in evaluating the
SMT by using the BLEU scores with the RBMT as
the reference. Of the cases where the two systems
differ, only a tiny fraction, at most 16/500, indi-
cate a better translation from the SMT, and well
over half are translated better by the RBMT. Ta-
ble 5 presents typical examples of bad SMT trans-
lations in the English —� French pair, contrasted
withthetranslationsproducedbytheRBMT.The
firsttwoaregrammaticalerors(asuperfluousex-
traverbinthefirst,andagreementerorsinthe
second). The third is an bad choice of tense and
preposition; although grammatical, the target lan-
guage sentence fails to preserve the meaning, and,
rather than referring to a 20 day period ending
now, instead refers to a 20 day period some time
in the past.
</bodyText>
<table confidence="0.996159111111111">
Result Agreement Count
RBMT better all judges 201
RBMT better majority 127
SMT better all judges 4
SMT better majority 12
Similar all judges 34
Similar majority 81
Unclear disagree 41
Total 500
</table>
<tableCaption confidence="0.634948">
Table 4: Comparison of RBMT and SMT perfor-
mance on 500 randomly chosen English —� French
translation examples, evaluated independently by
three judges.
</tableCaption>
<bodyText confidence="0.990688">
Table 6 shows a similar evaluation for the En-
glish —� Japanese. Here, the difference between
the SMT and RBMT versions was so pronounced
that we felt justified in taking a smaller sample, of
only 150 sentences. This time, 92/150 cases were
unanimously judged as having a better RBMT
translation, and there was not a single case where
even a majority found that the SMT was better.
Agreement was good here too, with only 8/150
cases not yielding at least a majority decision.
</bodyText>
<table confidence="0.996674333333333">
Result Agreement Count
RBMT better all judges 92
RBMT better majority 32
SMT better all judges 0
SMT better majority 0
Similar all judges 2
Similar majority 16
Unclear disagree 8
Total 150
</table>
<tableCaption confidence="0.991195">
Table 6: Comparison of RBMT and SMT per-
</tableCaption>
<bodyText confidence="0.904800166666667">
formance on 150 randomly chosen English —�
Japanese translation examples, evaluated indepen-
dently by three judges.
Finally, we look at the performance of the SMT
on material which the RBMT is not able to trans-
late. This would seem to be a situation where
</bodyText>
<page confidence="0.998022">
59
</page>
<table confidence="0.999545133333333">
English doesatemperaturechangecausetheheadache
RBMT French vos maux de tete sont-ils causes par des changements de temperature
SMT French (your headaches are-they caused by changes of temperature)
avez-vous vos maux de tete sont-ils causes par des changements de temperature
(have-you your headaches are-they caused by changes of temperature)
English are headaches relieved in the afternoon
RBMT French vos maux de tete diminuent-ils Fapres-midi
SMT French (your headaches (MASC-PLUR) decrease-MASC-PLUR the afternoon)
vos maux de tete diminue-t-elle Fapres-midi
(your headaches (MASC-PLUR) decrease-FEM-SING the afternoon)
English have you had them for twenty days
RBMT French avez-vous vos maux de tete depuis vingt jours
SMT French (have-you your headaches since twenty days)
avez-vous eu vos maux de tete pendant vingt jours
(have-you had your headaches during twenty days)
</table>
<tableCaption confidence="0.9248315">
Table 5: Examples of incorrect SMT translations from English into French. Errors are highlighted in
bold.
</tableCaption>
<bodyText confidence="0.953921161290322">
theSMTcouldhaveanadvantage;robustnessis
generally a strength of statistical approaches. We
return to English —� French in Table 7, which
presents the result of running the best SMT model
on the 357 examples from the test set in (Rayner
et al., 2005) which failed to be translated by the
RBMT. We divide the set into categories based on
the reason for failure of the RBMT.
In the most populous group, translations that
failed due to out of vocabulary items, the SMT
was, more or less by construction, also unable
to produce a translation. For the 110 items that
were out of grammar coverage for the RBMT, the
SMT produced 38 good translations, and another 4
borderline translations. There were 50 items that
were within the source grammar coverage of the
RBMT, but failed somewhere in transfer and gen-
eration processing. Of those, the majority (32)
represented “bad” source sentences, considered as
ill-formed for the purposes of this experiment. Out
of the remaining items that were within RBMT
grammar coverage, the SMT managed to produce
5 good translations and 1 borderline translation. In
total, on the most lenient interpretation, the SMT
produced 48 additional translations out of 357.
While this improvement in recall is arguably worth
having, it would come at the price of a substantial
declineinprecision.
5DiscussionandConclusions
Wehavepresentedanovelmethodologyforcom-
paringRBMTandSMT,andtesteditonaspe-
</bodyText>
<table confidence="0.998019642857143">
Result Count
Out of vocabulary
Bad translation 187
Out of source grammar coverage
Good translation 38
Bad translation 44
Borderline translation 4
Bad source sentence 34
In source grammar coverage
Good translation 5
Bad translation 12
Borderline translation 1
Bad source sentence 32
Total 357
</table>
<bodyText confidence="0.9905418">
Table7:English—�FrenchSMTperformanceon
examplesfromthetestsetwhichfailedtobetrans-
latedbytheRBMT,evaluatedbyonejudge.
cific pair of RBMT and SMT architectures. Our
claim is that these results show that the version
of SMT used here is not in fact capable of repro-
ducing the output of the RBMT system. Although
there has been some interest in attempting to train
SMT systems from RBMT output, the evaluation
issues that arise when comparing SMT and RBMT
versions of a high-precision limited-domain sys-
tem are different from those arising in most MT
tasks, and necessitate a correspondingly different
methodology. It is easy to gain the impression that
it is unsound, and that the experiment has been set
</bodyText>
<page confidence="0.994647">
60
</page>
<bodyText confidence="0.999969075268818">
upinsuchawaythatonlyoneresultispossible.
This is not, in fact, true.
When we have discussed the methodology with
people who work primarily with SMT, we have
heard two main objections. The first is that the
SMT is being trained on RBMT output, and hence
can only be worse; a common suggestion is that
a system trained on human-produced translations
could yield better results. It is not at all implau-
sible that an SMT trained on this kind of data
might perform better on material which is outside
the coverage of the RBMT system. In this do-
main, however, the important issue is precision,
not recall; what is critical is the ability to trans-
late accurately on material that is within the con-
strained language defined by the RBMT coverage.
The RBMT engine gives very good performance
on in-coverage data, as has been shown in other
evaluations of the MedSLT system, e.g. (Rayner et
al., 2005); over 97% of all in-coverage sentences
are correctly translated. Human-generated transla-
tions would often, no doubt, be more natural than
those produced by the RBMT, and there would be
slightly fewer outright mistranslations. But the
primary reason why the SMT is doing badly is
not that the training material contains bad trans-
lations, but rather that the SMT is incapable of
correctly reproducing the translations it sees in the
training data. Even in the easy English —� French
language-pair, the SMT often produces a different
translation from the RBMT. It could a priori have
been conceivable that the differences were unin-
teresting, in the sense that SMT outputs different
from RBMT outputs were as good, or even better.
In fact, Table 4 show that this is not true; when the
two translations differ, although the SMT transla-
tion can occasionally be better, it is usually worse.
Table 6 shows that this problem is considerably
more acute in English —� Japanese. Thus the
SMT system&apos;s inability to model the RBMT sys-
tem points to a real limitation.
If the SMT had instead been trained on human-
generated data, its performance on in-coverage
material could only have improved substantially if
the SMT for some reason found it easier to learn to
reproduce patterns in human-generated data than
in RBMT-generated data. This seems unlikely.
The SMT is being trained from a set of translation
pairs which are guaranteed to be completely con-
sistent, since they have been automatically gener-
ated by the RBMT; the fact that the RBMT system
only has a small vocabulary should also work in
its favour. If the SMT is unable to reproduce the
RBMT&apos;s output, it is reasonable to assume it will
have even greater difficulty reproducing transla-
tions present in normal human-generated training
data, which is always far from consistent, and will
have a larger vocabulary.
The second objection we have heard is that the
non-standard BLEU scores which we have used to
measure performance use the RBMT translations
as a reference. People are quick to point out that,
if real human translations were scored in this way,
they would do less well on the non-standard met-
rics than the RBMT translations. This is, indeed,
absolutely true, and explains why it was essential
to carry out the comparison judging shown in Ta-
bles 4 and 6. If we had compared human transla-
tions with RBMT translations in the same way, we
would have found that human translations which
differed from RBMT translations were sometimes
better, and hardly ever worse. This would have
shown that the non-standard metrics were inap-
propriate for the task of evaluating human trans-
lations. In the actual case considered in this paper,
we find a completely different pattern: the differ-
ences are one-sided in the opposite direction, in-
dicating that the non-standard metrics do in fact
agree with human judgements here.
A general objection to all these experiments is
that there may be more powerful SMT architec-
tures. We used the Giza++/Moses/SRILM com-
binination because it is the de facto standard. We
have posted the data we used at http : / /www.
bahrc.net/geaf2009; this will allow other
groups to experiment with alternate architectures,
and determine whether they do in fact yield sig-
nificant improvements. For the moment, however,
we think it is reasonable to claim that, in domains
where high accuracy is required, it remains to be
shown that SMT approaches are capable of achiev-
ing the levels of performance that rule-based sys-
tems can deliver.
</bodyText>
<page confidence="0.99877">
61
</page>
<sectionHeader confidence="0.938575" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999055255102041">
D.Arnold,L.Balkan,S.Meijer,R.L.Humphreys,and
L.Sadler.1994.MachineTranslation:AnIntroduc-
toryGuide.Blackwell,Oxford.
P.Bouillon,M.Rayner,N.Chatzichrisa~s,B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50–58, Bu-
dapest, Hungary.
P.Bouillon,F.Ehsani,R.Frederking,andM.Rayner,
editors.2006.ProceedingsoftheHLT-NACLIn-
ternationalWorkshoponMedicalSpeechTransla-
tion,NewYork.
P.Bouillon,F.Ehsani,R.Frederking,M.McTear,
and M. Rayner, editors. 2008a. Proceedings of
the COLING Workshop on Speech Processing for
Safety Critical Translation and Pervasive Applica-
tions, Manchester.
P.Bouillon,G.Flores,M.Georgescul,S.Halimi,
B.A. Hockey, H. Isahara, K. Kanzaki, Y. Nakao,
M. Rayner, M. Santaholma, M. Starlander, and
N. Tsourakis. 2008b. Many-to-many multilingual
medical speech translation on a PDA. In Proceed-
ings of The Eighth Conference of the Association
for Machine Translation in the Americas, Waikiki,
Hawaii.
N.Chatzichrisa~s,P.Bouillon,M.Rayner,M.San-
taholma,M.Starlander,andB.A.Hockey.2006.
Evaluatingtaskperformanceforaunidirectional
pages9–16,NewYork.
tem.InProceedingsoftheHLT-NACLInterna-
tionalWorkshoponMedicalSpeechTranslation,
controlledlanguagemedicalspeechtranslationsys-
L.Dugast,J.Senellart,andP.Koehn.2008.Canwe
relearnanRBMTsystem?InProceedingsofthe
Third Workshop on Statistical Machine Translation,
pages 175–178, Columbus, Ohio.
B.A.Hockey,M.Rayner,andG.Christian.2008.
Trainingstatisticallanguagemodelsfromgramar-
generateddata:Acomparativecase-study.InPro-
ceedingsofthe6thInternationalConferenceonNat-
uralLanguageProcessing,Gothenburg,Sweden.
R. Jonson. 2005. Generating statistical language mod-
els from interpretation grammars in dialogue sys-
tems. In Proceedings of the 11th EACL, Trento,
Italy.
A. Jurafsky, C. Wooters, J. Segal, A. Stolcke, E. Fos-
ler, G. Tajchman, and N. Morgan. 1995. Us-
ing a stochastic context-free grammar as a language
model for speech recognition. In Proceedings of
the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 189–192.
P.Koehn,H.Hoang,A.Birch,C.Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 2.
F.J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics, Hong Kong.
K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu.2001.
BLEU:amethodforautomaticevaluationofma-
chinetranslation.ResearchReport,ComputerSci-
enceRC22176(W0109-022),IBMResearchDivi-
sion,T.J.WatsonResearchCenter.
M.Rayner,P.Bouillon,N.Chatzichrisa~s,B.A.
Hockey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for comparing grammar-based and robust ap-
proaches to speech understanding. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (ICSLP), pages 1103–1107, Lis-
boa, Portugal.
M.Rayner,B.A.Hockey,andP.Bouillon.2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
M.Rayner,P.Bouillon,G.Flores,F.Ehsani,M.Star-
lander,B.A.Hockey,J.Brotanek,andL.Biewald.
2008.Asmall-vocabularysharedtaskformedical
speech translation. In Proceedings of the COLING
Workshop on Speech Processing for Safety Criti-
cal Translation and Pervasive Applications, Manch-
ester.
S. Seneff, C. Wang, and J. Lee. 2006. Combining lin-
guistic and statistical methods for bi-directional En-
glish Chinese translation in the ~ight domain. In
Proceedings ofAMTA 2006.
A.Stolcke.2002.SRILM-anextensiblelanguage
modelingtoolkit.InSeventhInternationalConfer-
enceonSpokenLanguageProcessing.ISCA.
Y.Wilks.2007.StonesoupandtheFrenchroom.In
K.Ahmad,C.Brewster,andM.Stevenson,editors,
WordsandIntelligenceI:SelectedPapersbyYorick
Wilks,pages255–265.
</reference>
<page confidence="0.999058">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.125247">
<title confidence="0.9993395">Using Artificially Generated to Evaluate Statistical Machine Translation</title>
<author confidence="0.999766">Manny Rayner</author>
<author confidence="0.999766">Paula Estrella</author>
<author confidence="0.999766">Pierrette</author>
<affiliation confidence="0.999648">University of Geneva,</affiliation>
<address confidence="0.91882">40 bvd du Pont-d&apos;Arve, CH-1211 Geneva 4,</address>
<author confidence="0.937431">Beth Ann</author>
<address confidence="0.4008885">Mail Stop 19-26, UCSC Ames Research Center, CA</address>
<email confidence="0.987075">bahockey@ucsc.edu</email>
<author confidence="0.884536">Yukie Nakao</author>
<address confidence="0.577304">LINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex</address>
<email confidence="0.967054">yukie.nakao@univ-nantes.fr</email>
<abstract confidence="0.99956396969697">AlthoughStatisticalMachineTranslation (SMT) is now the dominant paradigm within Machine Translation, we argue that it is far from clear that it can outperform Rule-Based Machine Translation (RBMT) on smallto medium-vocabulary applications where high precision is more important than recall. A particularly important practical example is medical speech translation. We report the results of experiments where we configured the various grammars and rule-sets in an Open Source medium-vocabulary multi-lingual medical speech translation system to generate large bilingual corpora for English and English which were then used to train SMT models based the combination of Giza++, Moses and SRILM. The resulting SMTs were unable fully to reproduce the performance of the RBMT, with performance out, even for English with less than 70% of the SMT translations of previously unseen sentences agreeing with RBMT translations. When the outof the two systems human judges reported the SMT result as frequently being worse than the RBMT result, and hardly ever better; moreover, the added robustness of the SMT only yielded a small improvement in recall, with a large penalty in precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<pages>1994</pages>
<marker></marker>
<rawString>L.Sadler.1994.MachineTranslation:AnIntroductoryGuide.Blackwell,Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rayner P Bouillon</author>
<author>B A Hockey N Chatzichrisas</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>Y Nakao</author>
<author>K Kanzaki</author>
<author>H Isahara</author>
</authors>
<title>A generic multilingual open source platform for limited-domain medical speech translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>50--58</pages>
<location>Budapest, Hungary.</location>
<marker>Bouillon, Chatzichrisas, Santaholma, Starlander, Nakao, Kanzaki, Isahara, 2005</marker>
<rawString>P.Bouillon,M.Rayner,N.Chatzichrisa~s,B.A. Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara. 2005. A generic multilingual open source platform for limited-domain medical speech translation. In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT), pages 50–58, Budapest, Hungary.</rawString>
</citation>
<citation valid="false">
<editor>P.Bouillon,F.Ehsani,R.Frederking,andM.Rayner, editors.2006.ProceedingsoftheHLT-NACLInternationalWorkshoponMedicalSpeechTranslation,NewYork.</editor>
<marker></marker>
<rawString>P.Bouillon,F.Ehsani,R.Frederking,andM.Rayner, editors.2006.ProceedingsoftheHLT-NACLInternationalWorkshoponMedicalSpeechTranslation,NewYork.</rawString>
</citation>
<citation valid="false">
<booktitle>2008a. Proceedings of the COLING Workshop on Speech Processing for Safety Critical Translation and Pervasive Applications,</booktitle>
<editor>P.Bouillon,F.Ehsani,R.Frederking,M.McTear, and M. Rayner, editors.</editor>
<location>Manchester.</location>
<marker></marker>
<rawString>P.Bouillon,F.Ehsani,R.Frederking,M.McTear, and M. Rayner, editors. 2008a. Proceedings of the COLING Workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Flores P Bouillon</author>
<author>S Halimi M Georgescul</author>
<author>B A Hockey</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>Y Nakao</author>
<author>M Rayner</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>N Tsourakis</author>
</authors>
<title>Many-to-many multilingual medical speech translation on a PDA.</title>
<date>2008</date>
<booktitle>In Proceedings of The Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Waikiki, Hawaii.</location>
<contexts>
<context position="3669" citStr="Bouillon et al., 2008" startWordPosition="555" endWordPosition="558">is by no means as clear as one might imagine: according to (Wilks, 2007), purely statistical systems are still unable to outperform SYSTRAN. In this paper, we will however be more concerned with limited-domain MT tasks, where robustness is not the key requirement, andaccuracyisparamount.Animediateexam54 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54–62, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP ple is medical speech translation, which is establishing itself as an an application area of some significance (Bouillon et al., 2006; Bouillon et al., 2008a). Translation in medical applications needs to be extremely accurate, since mistranslations can have serious or even fatal consequences. At the panel discussion at the 2008 COLING workshop on safety-critical speech translation (Rayner et al., 2008), the consensus opinion, based on input from practising physicians, was that an appropriate evaluation metric for medical applications would be heavily slanted towards accuracy, as opposed to robustness. If the metric is normalised so as to award 0 points for no translation, and 1 point for a correct translation, the estimate was that a suitable sc</context>
</contexts>
<marker>Bouillon, Georgescul, Hockey, Isahara, Kanzaki, Nakao, Rayner, Santaholma, Starlander, Tsourakis, 2008</marker>
<rawString>P.Bouillon,G.Flores,M.Georgescul,S.Halimi, B.A. Hockey, H. Isahara, K. Kanzaki, Y. Nakao, M. Rayner, M. Santaholma, M. Starlander, and N. Tsourakis. 2008b. Many-to-many multilingual medical speech translation on a PDA. In Proceedings of The Eighth Conference of the Association for Machine Translation in the Americas, Waikiki, Hawaii.</rawString>
</citation>
<citation valid="false">
<pages>2006--9</pages>
<marker></marker>
<rawString>N.Chatzichrisa~s,P.Bouillon,M.Rayner,M.Santaholma,M.Starlander,andB.A.Hockey.2006. Evaluatingtaskperformanceforaunidirectional pages9–16,NewYork.</rawString>
</citation>
<citation valid="false">
<note>tem.InProceedingsoftheHLT-NACLInternationalWorkshoponMedicalSpeechTranslation, controlledlanguagemedicalspeechtranslationsys-</note>
<marker></marker>
<rawString>tem.InProceedingsoftheHLT-NACLInternationalWorkshoponMedicalSpeechTranslation, controlledlanguagemedicalspeechtranslationsys-</rawString>
</citation>
<citation valid="false">
<booktitle>L.Dugast,J.Senellart,andP.Koehn.2008.Canwe relearnanRBMTsystem?InProceedingsofthe Third Workshop on Statistical Machine Translation,</booktitle>
<pages>175--178</pages>
<location>Columbus, Ohio.</location>
<marker></marker>
<rawString>L.Dugast,J.Senellart,andP.Koehn.2008.Canwe relearnanRBMTsystem?InProceedingsofthe Third Workshop on Statistical Machine Translation, pages 175–178, Columbus, Ohio.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Rayner B A Hockey</author>
<author>andG Christian 2008 Trainingstatisticallanguagemodelsfromgramargenerateddata Acomparativecase-study InProceedingsofthe6thInternationalConferenceonNaturalLanguageProcessing</author>
<author>Sweden Gothenburg</author>
</authors>
<marker>Hockey, InProceedingsofthe6thInternationalConferenceonNaturalLanguageProcessing, Gothenburg, </marker>
<rawString>B.A.Hockey,M.Rayner,andG.Christian.2008. Trainingstatisticallanguagemodelsfromgramargenerateddata:Acomparativecase-study.InProceedingsofthe6thInternationalConferenceonNaturalLanguageProcessing,Gothenburg,Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jonson</author>
</authors>
<title>Generating statistical language models from interpretation grammars in dialogue systems.</title>
<date>2005</date>
<booktitle>In Proceedings of the 11th EACL,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="10238" citStr="Jonson, 2005" startWordPosition="1612" endWordPosition="1613">ed during system development, and then presents the closest matches to the user. Examples of typical English domain sentences and their translations into French and Japanese are shown in Figure 2. 3Experimentalframework In the literature on language modelling, there is a known technique for bootstrapping a statistical language model (SLM) from a grammar-based language model (GLM). The grammar which forms the basis of the GLM is sampled randomly in order to create an arbitrarily large corpus of examples; these examples are then used as a training corpus to build the SLM (Jurafsky et al., 1995; Jonson, 2005). We adapt this process in a straightforward way to construct an SMT for a given language pair, using the source language grammar, the source-to-interlingua translation rules, the interlingua-to-target-language rules, and the target language generation grammar. We start in the same way, using the source language grammar to build a randomly generated source language corpus; as shown in (Hockey et al., 2008), it is important to have a probabilistic grammar. We then use the composition of the other components to attempt to translate each source language sentence into a target language equivalent,</context>
</contexts>
<marker>Jonson, 2005</marker>
<rawString>R. Jonson. 2005. Generating statistical language models from interpretation grammars in dialogue systems. In Proceedings of the 11th EACL, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Jurafsky</author>
<author>C Wooters</author>
<author>J Segal</author>
<author>A Stolcke</author>
<author>E Fosler</author>
<author>G Tajchman</author>
<author>N Morgan</author>
</authors>
<title>Using a stochastic context-free grammar as a language model for speech recognition.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="10223" citStr="Jurafsky et al., 1995" startWordPosition="1608" endWordPosition="1611">ed as correctly processed during system development, and then presents the closest matches to the user. Examples of typical English domain sentences and their translations into French and Japanese are shown in Figure 2. 3Experimentalframework In the literature on language modelling, there is a known technique for bootstrapping a statistical language model (SLM) from a grammar-based language model (GLM). The grammar which forms the basis of the GLM is sampled randomly in order to create an arbitrarily large corpus of examples; these examples are then used as a training corpus to build the SLM (Jurafsky et al., 1995; Jonson, 2005). We adapt this process in a straightforward way to construct an SMT for a given language pair, using the source language grammar, the source-to-interlingua translation rules, the interlingua-to-target-language rules, and the target language generation grammar. We start in the same way, using the source language grammar to build a randomly generated source language corpus; as shown in (Hockey et al., 2008), it is important to have a probabilistic grammar. We then use the composition of the other components to attempt to translate each source language sentence into a target langu</context>
</contexts>
<marker>Jurafsky, Wooters, Segal, Stolcke, Fosler, Tajchman, Morgan, 1995</marker>
<rawString>A. Jurafsky, C. Wooters, J. Segal, A. Stolcke, E. Fosler, G. Tajchman, and N. Morgan. 1995. Using a stochastic context-free grammar as a language model for speech recognition. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoang P Koehn</author>
<author>C Callison-Burch A Birch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>2</pages>
<contexts>
<context position="11788" citStr="Koehn et al., 2007" startWordPosition="1863" endWordPosition="1866"> one million sourcelanguage utterances; we next filtered them to keep only examples which were full sentences, as opposed to elliptical phrases, and finally used the translation rules and target-language generators to attempt to translate each sentence. This created approximately 305K aligned sentence-pairs for English —� French (1901K words English, 1993K words French), and 31 1 K aligned sentence-pairs for English —� Japanese (1941K words English, 2214K words Japanese). We held out 2.5% of each set as development data, and 2.5% as test data. Using Giza++, Moses and SRILM (Och and Ney, 2000; Koehn et al., 2007; Stolcke, 2002), we trained SMT models from increasingly large subsets of the training portion, using the development portion in the usual way to optimize parameter values. Finally, we used the resulting models to translate the test portion. Our primary goal was to measure the extent to which the derived versions of the SMT were able to approximate the original RBMT on data which was within the RBMT&apos;s coverage. There is a simple and natural way to perform this measurement: we apply the BLEU metric (Papineni et al., 2001), with the RBMT&apos;s translation taken as the reference. This means that per</context>
</contexts>
<marker>Koehn, Birch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P.Koehn,H.Hoang,A.Birch,C.Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="11768" citStr="Och and Ney, 2000" startWordPosition="1859" endWordPosition="1862"> we first generated one million sourcelanguage utterances; we next filtered them to keep only examples which were full sentences, as opposed to elliptical phrases, and finally used the translation rules and target-language generators to attempt to translate each sentence. This created approximately 305K aligned sentence-pairs for English —� French (1901K words English, 1993K words French), and 31 1 K aligned sentence-pairs for English —� Japanese (1941K words English, 2214K words Japanese). We held out 2.5% of each set as development data, and 2.5% as test data. Using Giza++, Moses and SRILM (Och and Ney, 2000; Koehn et al., 2007; Stolcke, 2002), we trained SMT models from increasingly large subsets of the training portion, using the development portion in the usual way to optimize parameter values. Finally, we used the resulting models to translate the test portion. Our primary goal was to measure the extent to which the derived versions of the SMT were able to approximate the original RBMT on data which was within the RBMT&apos;s coverage. There is a simple and natural way to perform this measurement: we apply the BLEU metric (Papineni et al., 2001), with the RBMT&apos;s translation taken as the reference.</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F.J. Och and H. Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong.</rawString>
</citation>
<citation valid="false">
<volume>2001</volume>
<pages>22176--0109</pages>
<marker></marker>
<rawString>K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu.2001. BLEU:amethodforautomaticevaluationofmachinetranslation.ResearchReport,ComputerScienceRC22176(W0109-022),IBMResearchDivision,T.J.WatsonResearchCenter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bouillon M Rayner</author>
<author>B A Hockey N Chatzichrisas</author>
<author>M Santaholma</author>
<author>M Starlander</author>
<author>H Isahara</author>
<author>K Kanzaki</author>
<author>Y Nakao</author>
</authors>
<title>A methodology for comparing grammar-based and robust approaches to speech understanding.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>1103--1107</pages>
<location>Lisboa, Portugal.</location>
<contexts>
<context position="14929" citStr="Rayner et al., 2005" startWordPosition="2348" endWordPosition="2351">aticallycorect; •Similar:Bothtranslationswereabout equally good OR the source sentence was meaningless in the domain. In order to show that our metrics are intuitively meaningful, it is sufficient to demonstrate that the frequency of occurrence of RBMT better is both large in comparison to that of SMT better, and accounts for a substantial proportion of the total population. Finally, we consider the question of whether the SMT, which is capable of translating out-ofgrammar sentences, can add useful robustness to the base system. We collected, from the set used in the experiments described in (Rayner et al., 2005), all the English sentences which failed to be translated into French. We used the best version of the English —� French SMT to translate each of these sentences, and asked human judges to evaluate the translations as being clearly acceptable, clearlyunacceptable,orborderline. In the next section, we present the results of the various experiments we have just described. 4Results We begin with Figure 1, which shows nonstandard BLEU scores for versions of the English —� French SMT system trained on quantities of data increasing from 14 287 to 285 740 pairs. As can be seen, translation performanc</context>
<context position="24145" citStr="Rayner et al., 2005" startWordPosition="3782" endWordPosition="3785">afternoon) English have you had them for twenty days RBMT French avez-vous vos maux de tete depuis vingt jours SMT French (have-you your headaches since twenty days) avez-vous eu vos maux de tete pendant vingt jours (have-you had your headaches during twenty days) Table 5: Examples of incorrect SMT translations from English into French. Errors are highlighted in bold. theSMTcouldhaveanadvantage;robustnessis generally a strength of statistical approaches. We return to English —� French in Table 7, which presents the result of running the best SMT model on the 357 examples from the test set in (Rayner et al., 2005) which failed to be translated by the RBMT. We divide the set into categories based on the reason for failure of the RBMT. In the most populous group, translations that failed due to out of vocabulary items, the SMT was, more or less by construction, also unable to produce a translation. For the 110 items that were out of grammar coverage for the RBMT, the SMT produced 38 good translations, and another 4 borderline translations. There were 50 items that were within the source grammar coverage of the RBMT, but failed somewhere in transfer and generation processing. Of those, the majority (32) r</context>
<context position="27195" citStr="Rayner et al., 2005" startWordPosition="4259" endWordPosition="4262">; a common suggestion is that a system trained on human-produced translations could yield better results. It is not at all implausible that an SMT trained on this kind of data might perform better on material which is outside the coverage of the RBMT system. In this domain, however, the important issue is precision, not recall; what is critical is the ability to translate accurately on material that is within the constrained language defined by the RBMT coverage. The RBMT engine gives very good performance on in-coverage data, as has been shown in other evaluations of the MedSLT system, e.g. (Rayner et al., 2005); over 97% of all in-coverage sentences are correctly translated. Human-generated translations would often, no doubt, be more natural than those produced by the RBMT, and there would be slightly fewer outright mistranslations. But the primary reason why the SMT is doing badly is not that the training material contains bad translations, but rather that the SMT is incapable of correctly reproducing the translations it sees in the training data. Even in the easy English —� French language-pair, the SMT often produces a different translation from the RBMT. It could a priori have been conceivable t</context>
</contexts>
<marker>Rayner, Chatzichrisas, Santaholma, Starlander, Isahara, Kanzaki, Nakao, 2005</marker>
<rawString>M.Rayner,P.Bouillon,N.Chatzichrisa~s,B.A. Hockey, M. Santaholma, M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao. 2005. A methodology for comparing grammar-based and robust approaches to speech understanding. In Proceedings of the 9th International Conference on Spoken Language Processing (ICSLP), pages 1103–1107, Lisboa, Portugal.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B A Hockey M Rayner</author>
<author>andP Bouillon 2006</author>
</authors>
<title>Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler.</title>
<publisher>CSLI Press,</publisher>
<location>Chicago.</location>
<marker>Rayner, 2006, </marker>
<rawString>M.Rayner,B.A.Hockey,andP.Bouillon.2006. Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler. CSLI Press, Chicago.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Bouillon M Rayner</author>
<author>F Ehsani G Flores</author>
<author>B A Hockey M Starlander</author>
<author>andL Biewald J Brotanek</author>
</authors>
<title>2008.Asmall-vocabularysharedtaskformedical speech translation.</title>
<booktitle>In Proceedings of the COLING Workshop on Speech Processing for Safety Critical Translation and Pervasive Applications,</booktitle>
<location>Manchester.</location>
<marker>Rayner, Flores, Starlander, Brotanek, </marker>
<rawString>M.Rayner,P.Bouillon,G.Flores,F.Ehsani,M.Starlander,B.A.Hockey,J.Brotanek,andL.Biewald. 2008.Asmall-vocabularysharedtaskformedical speech translation. In Proceedings of the COLING Workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>C Wang</author>
<author>J Lee</author>
</authors>
<title>Combining linguistic and statistical methods for bi-directional English Chinese translation in the ~ight domain.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA</booktitle>
<contexts>
<context position="4836" citStr="Seneff et al., 2006" startWordPosition="742" endWordPosition="745">rect translation, the estimate was that a suitable score for an incorrect translation would be something between –25 and –100 points. With these requirements, it seems unlikely that a robust, broad-coverage architecture has much chance of success. The obvious strategy is to build a limiteddomain controlled-language system, and tune it to the point where accuracy reaches the desired level. For systems of this kind, it is at least conceivable that RBMT may be able to outperform SMT. The next question is how to investigate the issues in a methodologically even-handed way. A few studies, notably (Seneff et al., 2006), suggest that rule-based translation may in fact be preferable in these cases. (Another related experiment is described in (Dugast et al., 2008), though this was carried out in a large-vocabulary system). These studies, however, have not been widely cited. One possible explanation is suspicion about methodological issues. Seneff and her colleagues trained their SMT system on 20 000 sentence pairs, a small number by the standards of SMT. It is a priori not implausible that more training data would have enabled them to create an SMT system that was as good as, or better than, the rule-based sys</context>
</contexts>
<marker>Seneff, Wang, Lee, 2006</marker>
<rawString>S. Seneff, C. Wang, and J. Lee. 2006. Combining linguistic and statistical methods for bi-directional English Chinese translation in the ~ight domain. In Proceedings ofAMTA 2006.</rawString>
</citation>
<citation valid="false">
<note>A.Stolcke.2002.SRILM-anextensiblelanguage modelingtoolkit.InSeventhInternationalConferenceonSpokenLanguageProcessing.ISCA.</note>
<marker></marker>
<rawString>A.Stolcke.2002.SRILM-anextensiblelanguage modelingtoolkit.InSeventhInternationalConferenceonSpokenLanguageProcessing.ISCA.</rawString>
</citation>
<citation valid="false">
<booktitle>Y.Wilks.2007.StonesoupandtheFrenchroom.In K.Ahmad,C.Brewster,andM.Stevenson,editors, WordsandIntelligenceI:SelectedPapersbyYorick Wilks,pages255–265.</booktitle>
<marker></marker>
<rawString>Y.Wilks.2007.StonesoupandtheFrenchroom.In K.Ahmad,C.Brewster,andM.Stevenson,editors, WordsandIntelligenceI:SelectedPapersbyYorick Wilks,pages255–265.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>