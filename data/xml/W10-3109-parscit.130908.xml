<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.510701">
<title confidence="0.979402">
Negation and Modality in Distributional Semantics
</title>
<author confidence="0.990454">
Ed Hovy
</author>
<affiliation confidence="0.994043">
Information Sciences Institute
University of Southern California
</affiliation>
<email confidence="0.993479">
hovy@isi.edu
</email>
<sectionHeader confidence="0.987002" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923461538462">
In Natural Language Processing, negation and modality have mostly been handled
using the older, pre-statistical methodologies of formal representations subject to
rule-based processing. This fits the traditional treatment of negation and modality
in logic-based knowledge representation and linguistics. However, in modern-day
statistics-based NLP, how exactly negation and modality should be taken into ac-
count, and what role these phenomena play overall, is much less clear. The closest
statistics-based NLP gets to semantics at this time is lexical-based word distribu-
tions (such as used in word sense disambiguation) and topic models (such as pro-
duced by Latent Dirichlet Allocation). What exactly in such representations should
a negation or a modality actually apply to? What would, or should, the resulting
effects be? The traditional approaches are of little or no help.
In this talk I argue that neither model is adequate, and that one needs a different
model of semantics to be able to accommodate negation and modality. The tradi-
tional formalisms are impoverished in their absence of an explicit representation
of the denotations of each symbol, and the statistics-based word distributions do
not support the compositionality required of semantics since it is unclear how to
link together two separate word distributions in a semantically meaningful way. A
kind of hybrid, which one could call Distributional Semantics, should be formu-
lated to include the necessary aspects of both: the ability to carry explicit word
associations that are still partitioned so as to allow negation and modality to affect
the representations in intuitively plausible ways is what is required.
I present a specific model of Distributional Semantics that, although still rudi-
mentary, exhibits some of the desired features. I explore the possibilities for ac-
commodating the phenomena of negation and modality. The talk poses many more
questions than it answers and is an invitation to consider Distributional Semantics
as a model for richer and more semantics-oriented statistics-based NLP.
</bodyText>
<page confidence="0.976279">
50
</page>
<note confidence="0.5530855">
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, page 50,
Uppsala, July 2010.
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.540196">
<title confidence="0.995964">Negation and Modality in Distributional Semantics</title>
<author confidence="0.964856">Ed</author>
<affiliation confidence="0.9979795">Information Sciences University of Southern</affiliation>
<email confidence="0.999602">hovy@isi.edu</email>
<abstract confidence="0.998490962962963">In Natural Language Processing, negation and modality have mostly been handled using the older, pre-statistical methodologies of formal representations subject to rule-based processing. This fits the traditional treatment of negation and modality in logic-based knowledge representation and linguistics. However, in modern-day statistics-based NLP, how exactly negation and modality should be taken into account, and what role these phenomena play overall, is much less clear. The closest statistics-based NLP gets to semantics at this time is lexical-based word distributions (such as used in word sense disambiguation) and topic models (such as produced by Latent Dirichlet Allocation). What exactly in such representations should a negation or a modality actually apply to? What would, or should, the resulting effects be? The traditional approaches are of little or no help. In this talk I argue that neither model is adequate, and that one needs a different model of semantics to be able to accommodate negation and modality. The traditional formalisms are impoverished in their absence of an explicit representation of the denotations of each symbol, and the statistics-based word distributions do not support the compositionality required of semantics since it is unclear how to link together two separate word distributions in a semantically meaningful way. A kind of hybrid, which one could call Distributional Semantics, should be formulated to include the necessary aspects of both: the ability to carry explicit word associations that are still partitioned so as to allow negation and modality to affect the representations in intuitively plausible ways is what is required. I present a specific model of Distributional Semantics that, although still rudimentary, exhibits some of the desired features. I explore the possibilities for accommodating the phenomena of negation and modality. The talk poses many more questions than it answers and is an invitation to consider Distributional Semantics as a model for richer and more semantics-oriented statistics-based NLP.</abstract>
<note confidence="0.773313333333333">50 of the Workshop on Negation and Speculation in Natural Language page 50, Uppsala, July 2010.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>