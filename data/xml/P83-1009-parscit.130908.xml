<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9580155">
AN IMPROPER TREATMENT OF QUANTIFICATION IN ORDINARY ENGLISH
Jerry R. Hobbs
SRI International
Menlo Park, California
</note>
<sectionHeader confidence="0.93714" genericHeader="abstract">
1. The Problem
</sectionHeader>
<bodyText confidence="0.997364733333334">
Consider the sentence
In most democratic countries most politicians
can fool most of the people on almost every
issue most of the time.
In the currently standard ways of representing
quantification in logical form, this sentence has
120 different readings, or quantifier scopings.
Moreover, they are truly distinct, in the sense
that for any two readings, there is a model that
satisfies one and not the other. With the
standard logical forms produced by the syntactic
and semantic translation components of current
theoretical frameworks and implemented systems, it
would seem that an inferencing component must
process each of these 120 readings in turn in
order to produce a best reading. Yet it is
obvious that people do not entertain all 120
possibilities, and people really do understand the
sentence. The problem is not just that
inferencing is required for disambiguation. It is
that people never do disambiguate completely. A
single quantifier scoping is never chosen. (Van
Lehn [1978] and Bobrow and Webber [1980] have also
made this point.) In the currently standard
logical notations, it is not clear how this
vagueness can be represented.1
What is needed is a logical form for such
sentences that is neutral with respect to the
various scoping possibilities. It should be a
notation that can be used easily by an inferencing
component. That is, it should be easy to define
deductive operations on it, and the logical forms
of typical sentences should not be unwieldy.
Moreover, when the inferencing component discovers
further information about dependencies among sets
of entities, it should entail only a minor
modification in the logical form, such as
conjoining a new proposition, rather than a major
restructuring. Finally, since the notion of
&amp;quot;scope&amp;quot; is a powerful tool in semantic analysis,
there should be a fairly transparent relationship
between dependency information in the notation and
standard representations of scope.
Three possible approaches are ruled out by
these criteria.
</bodyText>
<listItem confidence="0.886976">
1. Representing the sentence as a
</listItem>
<bodyText confidence="0.5307485">
disjunction of the various readings. This is
impossibly unwieldy.
</bodyText>
<page confidence="0.804076">
1
</page>
<bodyText confidence="0.8570090625">
Many people feel that most sentences exhibit too
few quantifier scope ambiguities for much effort
to be devoted to this problem, but a casual
inspection of several sentences from any text
should convince almost everyone otherwise.
2. Using as the logical notation a triple
consisting of an expression of the propositional
content of the sentence, a store of quantifier
structures (e.g., as in Cooper [1975], Woods
[1978]), and a set of constraints on how the
quantifier structures could be unstored. This
would adequately capture the vagueness, but it is
difficult to imagine defining inference procedures
that would work on such an object. Indeed, Cooper
did no inferencing; Woods did little and chose a
default reading heuristically before doing so.
</bodyText>
<listItem confidence="0.516493125">
3. Using a set—theoretic notation like that
of (1) below, pushing all the universal
quantifiers to the outside and the existential
quantifiers to the inside, and replacing the
existentially quantified variables by Skolem
functions of all the universally quantified
variables. Then when inferencing discovers a
nondependency, one of the arguments is dropped
from one of the Skolem functions. One difficulty
with this is that it yields representations that
are too general, being satisfied by models that
correspond to none of the possible intended
interpretations. Moreover, in sentences in which
one quantified noun phrase syntactically embeds
another (what Woods [1978] calls &amp;quot;functional
nesting&amp;quot;), as in
</listItem>
<bodyText confidence="0.998793964285714">
Every representative of a company arrived.
no representation that is neutral between the two
is immediately apparent. With wide scope, &amp;quot;a
company&amp;quot; is existential, with narrow scope it is
universal, and a shift in commitment from one to
the other would involve significant restructuring
of the logical form.
The approach taken here uses the notion of
the -typical element- of a set, to produce a flat
logical form of conjoined atomic predications. A
treatment has been worked out only for monotone
increasing determiners; this is described in
Section 2. In Section 3 some ideas about other
determiners are discussed. An inferencing
component, such as that explored in Hobbs [1976,
1980], capable of resolving coreference, doing
coercions, and refining predicates, will be
assumed (but not discussed). Thus, translating
the quantifier scoping problem into one of those
three processes will count as a solution for the
purposes of this paper.
This problem has received little attention in
linguistics and computational linguistics. Those
who have investigated the processes by which a
rich knowledge base is used in interpreting texts
have largely ignored quantifier ambiguities.
Those who have studied quantifiers have generally
noted that inferencing is required for
</bodyText>
<page confidence="0.997318">
57
</page>
<bodyText confidence="0.999492">
disambiguation, without attempting to provide a
notation that would accommodate this inferencing.
There are some exceptions. Bobrow and Webber
[1980] discuss many of the issues involved, but it
is not entirely clear what their proposals are.
The work of Webber (1978] and Mellish (1980] are
discussed below.
</bodyText>
<listItem confidence="0.5331705">
2. Monotone Increasing Determiners
2.1. A Set-Theoretic Notation
</listItem>
<bodyText confidence="0.99602212">
Let us represent the pattern of a simple
intransitive sentence with a quantifier as &amp;quot;Q Ps
R&amp;quot;. In &amp;quot;Most men work,&amp;quot; Q = &amp;quot;most&amp;quot;, P = &amp;quot;man&amp;quot;,
and R = &amp;quot;work&amp;quot;. Q will be referred to as a
determiner. A determiner Q is monotone increasing
if and only if for any R1 and R2 such that the
denotation of RI is a subset of the denotation of
R2, &amp;quot;Q Ps RI&amp;quot; implies &amp;quot;Q Ps R2&amp;quot; (Barwise and
Cooper (1981]). For example, letting RI = &amp;quot;work
hard&amp;quot; and R2 = &amp;quot;work&amp;quot;, since &amp;quot;most men work hard&amp;quot;
implies &amp;quot;most men work,&amp;quot; the determiner &amp;quot;most&amp;quot; is
monotone increasing. Intuitively, making the verb
phrase more general doesn&apos;t change the truth
value. Other monotone increasing determiners are
&amp;quot;every&amp;quot;, &amp;quot;some&amp;quot;, &amp;quot;many&amp;quot;, &amp;quot;several&amp;quot;, &amp;quot;any&amp;quot; and &amp;quot;a
few&amp;quot;. &amp;quot;No&amp;quot; and &amp;quot;few&amp;quot; are not.
Any noun phrase Q Ps with a monotone
increasing determiner Q involves two sets, an
intensionally defined set denoted by the noun
phrase minus the determiner, the set of all Ps,
and a nonconstructively specified set denoted by
the entire noun phrase. The determiner Q can be
viewed as expressing a relation between these two
sets. Thus the sentence pattern Q Ps R can be
represented as follows:
</bodyText>
<equation confidence="0.932529">
(1) (3s)(Q(s,(x I P(x)}) &amp; (Vy)(yEs -&gt; R(y)))
</equation>
<bodyText confidence="0.98164475">
That is, there is a set s which bears the relation
Q to the set of all Ps, and R is true of every
element of s. (Barwise and Cooper call s a
&amp;quot;witness set&amp;quot;.) &amp;quot;Most men work&amp;quot; would be
</bodyText>
<equation confidence="0.898183666666667">
represented
(3 s)(most(s,tx I man(x)})
&amp; Y)(Yes —&gt; work(y)))
</equation>
<bodyText confidence="0.994766333333333">
For collective predicates such as &amp;quot;meet&amp;quot; and
&amp;quot;agree&amp;quot;, R would apply to the set rather than to
each of its elements.
</bodyText>
<equation confidence="0.801765">
(3 s) o(s,[x I p(x)l) &amp; R(s)
</equation>
<bodyText confidence="0.99710225">
Sometimes with singular noun phrases and
determiners like &amp;quot;a&amp;quot;, &amp;quot;some&amp;quot; and -any&amp;quot; it will be
more convenient to treat the determiner as a
relation between a set and one of its elements.
</bodyText>
<equation confidence="0.844652">
(3 Y) Q(Y,{x I P(x)}) &amp; R(y).
</equation>
<bodyText confidence="0.946288090909091">
According to notation (1) there are two
aspects to quantification. The first, which
concerns a relation between two sets, is discussed
in Section 2.2. The second aspect involves a
predication made about the elements of one of &apos;the
sets. The approach taken here to this aspect of
quantification is somewhat more radical, and
depends on a view of semantics that might be
called &amp;quot;ontological promiscuity&amp;quot;. This is
described briefly in Section 2.3. Then in Section
2.4 the scope-neutral representation is presented.
</bodyText>
<subsectionHeader confidence="0.673872">
2.2. Determiners as Relations between Sets
</subsectionHeader>
<bodyText confidence="0.999852428571429">
Expressing determiners as relations between
sets allows us to express as axioms in a knowledge
base more refined properties of the determiners
than can be captured by representing them in terms
of the standard quantifiers.
First let us note that, with the proper
definitions of &amp;quot;every&amp;quot; and &amp;quot;some&amp;quot;,
</bodyText>
<equation confidence="0.992831">
sl,s2) every(sl,s2) &lt;-&gt; sl = s2
(y x,s2) some(x,s2) &lt;-&gt; xes2
</equation>
<bodyText confidence="0.992427428571429">
formula (1) reduces to the standard notation.
(This can be seen as explaining why the
restriction is implicative in universal
quantification and conjunctive in existential
quantification.)
A meaning postulate for &amp;quot;most&amp;quot; that is
perhaps too mathematical is
</bodyText>
<equation confidence="0.967667">
(Vsl,s2) most(sl,s2) -&gt; Isll &gt; 1/2 1s21
</equation>
<bodyText confidence="0.974038">
Next, consider &amp;quot;any&amp;quot;. Instead of trying to
force an interpretation of &amp;quot;any&amp;quot; as a standard
quantifier, let us take it to mean -a random
element of&amp;quot;.
</bodyText>
<listItem confidence="0.756191">
(2) (gex,$) any(x,$) -&gt; x = random(s),
</listItem>
<bodyText confidence="0.983666083333333">
where &amp;quot;random&amp;quot; is a function that returns a random
element of a set. This means that the
prototypical use of &amp;quot;any&amp;quot; is in sentences like
Pick any card.
Let me surround this with caveats. This can&apos;t be
right, if for no other reason than that &amp;quot;any&amp;quot; is
surely a more &amp;quot;primitive&amp;quot; notion in language than
&amp;quot;random&amp;quot;. Nevertheless, mathematics gives us firm
intuitions about &amp;quot;random&amp;quot; and (2) may thus shed
light on some linguistic facts.
Many of the linguistic facts about &amp;quot;any&amp;quot; can
be subsumed under two broad characterizations:
</bodyText>
<listItem confidence="0.948948">
1. It requires a &amp;quot;modal&amp;quot; or &amp;quot;nondefinite&amp;quot;
</listItem>
<bodyText confidence="0.901408166666667">
context. For example, &amp;quot;John talks to any woman&amp;quot;
must be interpreted dispositionally. If we adopt
(2), we can see this as deriving from the nature
of randomness. It simply does not make sense to
say of an actual entity that it is random.
2. It normally acts as a universal
quantifier outside the scope of the most immediate
modal embedder. This is usually the most natural
interpretation of &amp;quot;random&amp;quot;.
Moreover, since &amp;quot;any&amp;quot; extracts a single
element, we can make sense out of cases in which
&amp;quot;any&amp;quot; fails to act like &amp;quot;every&amp;quot;.
</bodyText>
<page confidence="0.989157">
58
</page>
<bodyText confidence="0.984153666666667">
I&apos;ll talk to anyone but only to one person.
* I&apos;ll talk to everyone but only to one person.
John wants to marry any Swedish woman.
* John wants to marry every Swedish woman.
(The second pair is due to Moore [19731.)
This approach does not, however, seem to
offer an especially convincing explanation as to
why &amp;quot;any&amp;quot; functions in questions as an existential
quantifier.
</bodyText>
<subsectionHeader confidence="0.963235">
2.3. Ontological Promiscuity
</subsectionHeader>
<bodyText confidence="0.995099">
Davidson [1967] proposed a treatment of
action sentences in which events are treated as
individuals. This facilitated the representation
of sentences with adverbials. But virtually every
predication that can be made in natural language
can be modified adverbially, be specified as to
time, function as a cause or effect of something
else, constitute a belief, be nominalized, and be
referred to pronominally. It is therefore
convenient to extend Davidson&apos;s approach Co all
predications, an approach that might be called
&amp;quot;ontological promiscuity&amp;quot;. One abandons all
</bodyText>
<listItem confidence="0.5341575">
ontological scruples. A similar approach is used
in many Al systems.
</listItem>
<bodyText confidence="0.999986454545455">
We will use what might be called a
&amp;quot;nominalization&amp;quot; operator &amp;quot;&apos;&amp;quot; for predicates.
Corresponding to every n-ary predicate p there
will be an n+1-ary predicate p&apos; whose first
argument can be thought of as a condition of p&apos;s
being true of the subsequent arguments. Thus, if
&amp;quot;see(J,B)&amp;quot; means that John sees Bill,
&amp;quot;see&apos;(E,J,B)&amp;quot; will mean that E is John&apos;s seeing of
Bill. For the purposes of this paper, we can
consider that the primed and unprimed predicates
are related by the following axiom schema:
</bodyText>
<equation confidence="0.6689695">
(3) (V x,e) p&amp;quot;(e,x) —&gt; p(x)
(yx)(3e) p(x) —&gt; /3&amp;quot; (e,x)
</equation>
<bodyText confidence="0.999796375">
It is beyond the scope of this paper to
elaborate on the approach further, but it will be
assumed, and taken to extremes, in the remainder
of the paper. Let me illustrate the extremes to
which it will be taken. Frequently we want to
refer to the condition of two predicates p and q
holding simultaneously of x. For this we will
refer to the entity e such that
</bodyText>
<equation confidence="0.873313">
and&amp;quot;(e,e1,e2) &amp; p&apos;(el,x) &amp; q&amp;quot;(e2,x)
</equation>
<bodyText confidence="0.998727">
Here el is the condition of p being true of x, e2
is the condition of q being true of x, and e the
condition of the conjunction being true.
</bodyText>
<subsectionHeader confidence="0.983612">
2.4. The Scope-Neutral Representation
</subsectionHeader>
<bodyText confidence="0.999687736842105">
We will assume that a set has a typical
element and that the logical form for a plural
noun phrase will include reference to a set and
its_tnical element.2 The linguistic intuition
2 Woods [1978] mentions something like this
approach, but rejects it because difficulties that
are worked out here would have to be worked out.
behind this idea is that one can use singular
pronouns and definite noun phrases as anaphors for
plurals. Definite and indefinite generics can
also be understood as referring to the typical
element of a set.
In the spirit of ontological promiscuity, we
simply assume that typical elements of set-, ,re
things that exist, and encode in meaning
postulates the necessary relations between a set&apos;s
typical element and its real elements. This move
amounts to reifying the universally quantified
variable. The typical element of s will be
referred to as
There are two very nearly contradictory
properties that typical elements must have. The
first is the equivalent of universal
instantiation; real elements should inherit the
properties of the typical element. The second is
that the typical element cannot itself be an
element of the set, for that would lead to
cardinality problems. The two together would
imply the set has no elements.3
We could get around this problem by positing
a special set of predicates that apply to typical
elements and are systematically related to the
predicates that apply to real elements. This idea
should be rejected as being ad hoc, if aid did not
come to us from an unexpected quarter -- the
notion of &amp;quot;grain size&amp;quot;.
When utterances predicate, it is normally at
some degree of resolution, or &amp;quot;grain&amp;quot;. At a
fairly coarse grain, we might say that John is at
the post office -- &amp;quot;at(J,P0)&amp;quot;. At a more refined
grain, we have to say that he is at the stamp
window -- &amp;quot;at(J,SW)&amp;quot;. We normally think of grain
in terms of distance, but more generally we can -
move from entities at one grain to entities at a
coarser grain by means of an arbitrary partition.
Fine-grained entities in the same equivalence
class are indistinguishable at the coarser grain.
Given a set S, consider the partition that
collapses all elements of S into one element and
leaves everything else unchanged. We can view the
typical element of S as the set of real elements
seen at this coarser grain -- a grain at which,
precisely, the elements of the set are
indistinguishable. Formally, we can define an
operator cr which takes a set and a predicate as
its arguments and produces what will be referred
to as an &amp;quot;indexed predicate&amp;quot;:
</bodyText>
<equation confidence="0.913836666666667">
1 T, if x—p(s) &amp; (V yes) p(y),
Cr(s,p)(x) - F, if x=r(s) &amp;-i(y yEs) P(Y),
p(x) otherwise.
</equation>
<bodyText confidence="0.992249090909091">
We will frequently abbreviate this &amp;quot;p,&amp;quot;. Note
that predicate indexing gets us out of the above
--------
3 An alternative approach would be to say that the
typical element is in fact one of the real
elements of the set, but that we will never know
which one, and that furthermore, we will never
know about the typical element any property that
is not true of all the elements. This approach
runs into technical difficulties involving the
empty set.
</bodyText>
<page confidence="0.994729">
59
</page>
<bodyText confidence="0.9995505">
contradiction, for now &amp;quot;2-(s) E, s&amp;quot; is not only
true but tautologous.
We are now in a position to state the
properties typical elements should have. The
first implements universal instantiation:
With what we have, we can represent the
distinction between the distributive and
collective readings of a sentence like
</bodyText>
<listItem confidence="0.869890666666667">
(10) The men lifted the piano.
(4) (t(s,y) p5(2(s)) &amp; yes -&gt; p(y)
(5) (is)([(ixes) p(x)] —&gt; pi (2s)))
</listItem>
<bodyText confidence="0.999858285714286">
That is, the properties of the typical element at
the coarser grain are also the properties of the
real elements at the finer grain, and the typical
element has those properties that all the real
elements have.
Note that while we can infer a property from
set membership, we cannot infer set membership
from a property. That is, the fact that p is
true of a typical element of a set s and p is true
of an entity y, does not imply that y is an
element of s. After all, we will want &amp;quot;three men-
to refer to a set, and to be able to infer from
y&apos;s being in the set the fact that y is a man.
But we do not want to infer from y&apos;s being a man
that y is in the set. Nevertheless, we will need
a notation for expressing this stronger relation
among a set, a typical element, and a defining
condition. In particular, we need it for
representing &amp;quot;every man&amp;quot;. Let us develop the
notation from the standard notation for
intensionally defined sets,
</bodyText>
<listItem confidence="0.947013">
(6) s • (x
</listItem>
<bodyText confidence="0.99924275">
by performing a fairly straightforward, though
ontologically promiscuous, syntactic translation
on it. First, instead of viewing x as a
universally quantified variable, let us treat it
as the typical element of s. Next, as a way of
getting a handle on &amp;quot;p(x)&amp;quot;, we will use the
nominalization operator &amp;quot;&apos;&amp;quot; to reify it, and refer
to the condition e of p (or p,) being true of the
</bodyText>
<listItem confidence="0.82385075">
typical element x of s &amp;quot;p&apos;s (e,x)&amp;quot;. Expression
(6) can then be translated into the following flat
predicate-argument form:
(7) set(s,x,e) &amp; p&apos;s(e,x)
</listItem>
<bodyText confidence="0.995184571428571">
This should be read as saying that s is a set
whose typical element is x and which is defined by
condition e, which is the condition of p
(interpreted at the level of the typical element)
being true of x. The two critical properties of
the predicate &amp;quot;set&amp;quot; which make (7) equivalent to
(6) are the following:
</bodyText>
<listItem confidence="0.9959015">
(8) Ois,x,e,y) set(s,x,e) &amp; p&apos;s (e,x) &amp; p(y) -&gt; yEs
(9) (/s,x,e) set(s,x,e) -&gt; x
</listItem>
<bodyText confidence="0.997142729729729">
Axiom schema (8) tells us that if an entity y has
the defining property p of the set s, then y is an
element of s. Axiom (9), along with axiom schemas
(4) and (3), tells us that an element of a set has
the set&apos;s defining property.
For the collective reading the representation
would include &amp;quot;lift(m)&amp;quot; where m is the set of men.
For the distributive reading, the representation
would have &amp;quot;lift(2,&amp;quot;(m))&amp;quot;, where Z(m) is the
typical element of the set m. To represent the
ambiguity of (10), we could use the device
suggested in Hobbs (1982] for prepositional phrase
and other ambiguities, and write &amp;quot;lift(x) &amp; (x=m v
x=
This approach involves a more thorough use of
typical elements than two previous approaches.
Webber (19781 admitted both set and prototype (my
typical element) interpretations of phrases like
&amp;quot;each man&amp;quot; in order to have antecedents for both
&amp;quot;they&amp;quot; and &amp;quot;he&amp;quot;, but she maintained a distinction
between the two. Essentially, she treated &amp;quot;each
man&amp;quot; as ambiguous, whereas the present approach
makes both the typical element and the set
available for subsequent reference. Mellish
(1980j uses typical elements strictly as an
intermediate representation that must be resolved
into more standard notation by the end of
processing. He can do this because he is working
in a task domain -- physics problems -- in which
sets are not just finite but small, and vagueness
as to their composition must be resolved. Webber
did not attempt to use typical elements to derive
a scope-neutral representation; Mellish did so
only in a limited way.
Scope dependencies can now be represented as
relations among typical elements. Consider the
sentence
</bodyText>
<listItem confidence="0.843677">
(11) Most men love several women,
</listItem>
<bodyText confidence="0.9998455">
under the reading in which there is a different
set of women for each man. We can define a
dependency function f which for each man returns
the set of women whom that man loves.
</bodyText>
<equation confidence="0.98892">
f(m) = I woman(w) &amp; love(m,w)}
</equation>
<bodyText confidence="0.999641">
The relevant parts of the initial logical form,
produced by a syntactic and semantic translation
component, for sentence (11) will be
</bodyText>
<listItem confidence="0.6221735">
(12) love(r(m),r(w)) &amp; most(m,m1) &amp; man1(&apos;r(m1))
&amp; several(w) &amp; womanl(r(w))
</listItem>
<bodyText confidence="0.999548222222222">
where ml is the set of all men, m the set of most
of them referred to by the noun phrase most men&amp;quot;,
and w the set referred to by the noun phrase
&amp;quot;several women&amp;quot;, and where &amp;quot;manl = of(ml,man)&amp;quot; and
&amp;quot;womanl = (w,woman)&amp;quot;. When the inferencing
component discovers there is a different set w for
each element of the set m, w can be viewed as
refering to the typical element of this set of
sets:
</bodyText>
<equation confidence="0.80736">
w =D({f(x) I xEm})
</equation>
<page confidence="0.966277">
60
</page>
<bodyText confidence="0.999840666666667">
To eliminate the set notation, we can extend the
definition of the dependency function to the
typical element of m as follows:
</bodyText>
<equation confidence="0.453347">
“VM)) &apos; Z({f(X) I )(EM})
</equation>
<bodyText confidence="0.993099166666667">
That is, f maps the typical element of a set into
the typical element of the set of images under f
of the elements of the set. From here on, we will
consider all dependency functions so extended to
the typical elements of their domains.
The identity &amp;quot;w = f(p(m))&amp;quot; now
simultaneously encodes the scoping information and
involves only existentially quantified variables
denoting individuals in an (admittedly
ontologically promiscuous) domain. Expressions
like (12) are thus the scope-neutral
representation, and scoping information is added
by conjoining such identities.
Let us now consider several examples in which
processes of interpretation result in the
acquisition of scoping information. The first
will involve interpretation against a small model.
The second will make use of world knowledge, while
the third illustrates the treatment of embedded
quantifiers.
First the simple, and classic, example.
(13) Every man loves some woman.
The initial logical form for this sentence
includes the following:
</bodyText>
<equation confidence="0.981514">
lovel(r(ms),w) &amp; manl(r(ms)) &amp; woman(w)
</equation>
<bodyText confidence="0.972091888888889">
where &amp;quot;lovel =cr(ms,xx[love(x,w)])&amp;quot; and &amp;quot;manl =
T(ms,man)&amp;quot;. Figure 1 illustrates two small models
of this sentence. M is the set of men [AM, W is
the set of women {X,Y}, and the arrows signify
love. Let us assume that the process of
interpreting this sentence is just the process of
identifying the existentially quantified variables
ms and w and possibly coercing the predicates, in
a way that makes the sentence true.4
</bodyText>
<figure confidence="0.9983835">
B Y
(b)
</figure>
<figureCaption confidence="0.999886">
Figure 1. Two models of sentence (13).
</figureCaption>
<bodyText confidence="0.998821857142857">
In Figure 1(a), &amp;quot;love(A,X)&amp;quot; and &amp;quot;love(B,X)&amp;quot;
are both true, so we can use axiom schema (5) to
derive &amp;quot;lovel(p(M),X)&amp;quot;. Thus, the
identifications &amp;quot;ms = M&amp;quot; and &amp;quot;w = X&amp;quot; result in the
sentence being true.
In Figure 1(b), &amp;quot;love(A,X)&amp;quot; and &amp;quot;love(B,Y)&amp;quot;
are both true, but since these predications differ
</bodyText>
<page confidence="0.950642">
4
</page>
<bodyText confidence="0.981933041666667">
Bobrow and Webber [1980] similarly show scoping
information acquired by interpretation against a
small model.
in more than one argument, we cannot apply axiom
schema (5). First we define a dependency function
f, mapping each man into a woman he loves,
yielding &amp;quot;love(A,f(A))&amp;quot; and &amp;quot;love(B,f(B))&amp;quot;. We
can now apply axiom schema (5) to derive
&amp;quot;love2(Z(M),f(Z(M)))&amp;quot;, where &amp;quot;love2
cr(m,xflove(x,f(x))))&amp;quot;. Thus, we can make the
sentence true by identifying as with M and w with
f(p(M)), and by coercing &amp;quot;love&amp;quot; to &amp;quot;love2&amp;quot; and
&amp;quot;woman&amp;quot; to &amp;quot;cr(W,woman)&amp;quot;. ,
In each case we see that the identification
of w is equivalent to solving the scope ambiguity
problem.
In our subsequent examples we will ignore the
indexing on the predicates, until it must be
mentioned in the case of embedded quantifiers.
Next consider an example in which world
knowledge leads to disambiguation:
Three women had a baby.
Before inferencing, the scope-neutral
representation is
</bodyText>
<equation confidence="0.958094">
had(rws),b) &amp; Iws1=3 &amp; woman(r(ws)) &amp; baby(b)
</equation>
<bodyText confidence="0.999862">
Let us suppose the inferencing component has
axioms about the functionality of having a baby --
something like
</bodyText>
<equation confidence="0.943769">
(lex,y) had(x,y) -&gt; x = mother-of(y)
</equation>
<bodyText confidence="0.93689975">
and that we know about cardinality the fact that
for any function g and set s,
Ig(s)1 Is]
Then we know the following:
</bodyText>
<equation confidence="0.676612">
3 = Iwsl = Imother-of(b)I Ibl
</equation>
<bodyText confidence="0.842767631578947">
This tells us that b cannot be an individual but
must be the typical element of some set. Let f be
a dependency function such that
wfws &amp; f(w) = x -&gt; had(w,x)
that is, a function that maps each woman into some
baby she had. Then we can identify b with
Or equivalently, with
r(ff(w) I wEws}), giving us the correct scope.
Finally, let us return to interpretation with
respect to small models to see how embedded
quantifiers are represented. Consider
(14) Every representative of a company arrived.
The initial logical form .includes
arrive(r) &amp; set(rs,r,ea) &amp; and&apos;(ea,er,eo)
&amp; rep&apos;(er,r) &amp; of&apos;(eo,r,c) &amp; co(c)
That is, r arrives, where r is the typical element
of a set rs defined by the conjunction ea of r&apos;s
being a representative and r&apos;s being of c, where c
is a company. We will consider the two models in
</bodyText>
<page confidence="0.998689">
61
</page>
<figureCaption confidence="0.809345">
Figure 2. R is the set of representatives
</figureCaption>
<bodyText confidence="0.980276777777778">
(A,B,(C), K is the set of companies {X,Y,(Z,W)},
there is an arrow from the representatives to the
companies they represent, and the representatives
who arrived are circled.
in the algorithm below it is refered to as
&amp;quot;Quant(v)&amp;quot;. The translation of the remainder of
the inferential logical form into bracketed
notation is best shown by example. For the
sentence
</bodyText>
<figure confidence="0.924476875">
B Y A representative of every company saw a sample
(a) the relevant parts of the inferential logical form
are
see(r,$) &amp; r;p(r) &amp; of(r,c) &amp; co(c) &amp; sample(s)
where &amp;quot;see(r,$)&amp;quot; is asserted. This is translated &amp;quot;
in a straightforward way into
(18) seegr I rep(r) &amp; of(r,[c I co(c)I)I,
[s I sample(s)])
</figure>
<figureCaption confidence="0.999825">
Figure 2. Two models of sentence (14).
</figureCaption>
<bodyText confidence="0.978903950819673">
In Figure 2(a), &amp;quot;of(A,X)&amp;quot;, &amp;quot;of(B,Y)&amp;quot; and &amp;quot;of(B,Z)&amp;quot;
are true. Define a dependency function f to map A
into X and B into Y. Then &amp;quot;of(A,f(A))&amp;quot; and
&amp;quot;of(B,f(B))&amp;quot; are both true, so that
&amp;quot;of(r(R),f(r(R)))&amp;quot; is also true. Thus we have
the following identifications:
c f(r(R)) .1t({X,Y}), rs R, r .r(R)
In Figure 2(b) &amp;quot;of(B,y)&amp;quot; and &amp;quot;of(C,Y)&amp;quot; are
both true, so &amp;quot;of(2-(R1),Yris also. Thus we may
let c be Y and rs be R1, giving us the wide
reading for &amp;quot;a company&amp;quot;.
In the case where no one represents any
company and no one arrived, we can let c be
anything and rs be the empty set. Since, by the
definition of dr , any predicate indexed by the
empty set will be true of the typical element of
the empty set, &amp;quot;arrive 9, (2-(#))- will be true,
and the sentence will be satisfied.
It is worth pointing out that this approach
solves the problem of the classic &amp;quot;donkey
sentences&amp;quot;. If in sentence (14) we had had the
verb phrase &amp;quot;hates it&amp;quot;, then &amp;quot;it&amp;quot; would be
resolved to c, and thus to whatever c was resolved
to.
So Ear the notation of typical elements and
dependency functions has been introduced; it has
been shown how scope information can be
represented by these means; and an example of
inferential processing acquiring that scope
information has been given. Now the precise
relation of this notation to standard notation
must be specified. This can be done by means of
an algorithm that takes the inferential notation,
together with an indication of which proposition
is asserted by the sentence, and produces in the
conventional form all of the readings consistent
with the known dependency information.
First we must put the sentence into what will
be called a &amp;quot;bracketed notation&amp;quot;. We associate
with each variable v an indication of the
corresponding quantifier; this is determined from
such pieces of the inferential logical form as
those involving the predicates &amp;quot;set&amp;quot; and &amp;quot;most&amp;quot;;
This may be read &amp;quot;An r such that r is a
representative and r is of a c such that c is a
company sees an s such that s is a sample.
The nondeterministic algorithm below
generates all the scopings from the bracketed
notation. The function TOPBVS returns a list of
all the top—level bracketed variables in Form,
that is, all the bracketed variables except those
within the brackets of some other variable -- in
(18) r and s but not c. BRANCH
nondeterministically generates a separate process
for each element in a list it is given as
argument. A four—part notation is used for
quantifiers (similar to that of Woods [19781)
&amp;quot;(quantifier variable restriction body)&amp;quot;.
G(Form):
if [vIR] 4- BRANCH(TOPBVS(Form))
then Form. (Quant(v) v 3RANCH(CR,G(R)1) Form&apos;)
</bodyText>
<subsectionHeader confidence="0.185993">
C+0,7
</subsectionHeader>
<bodyText confidence="0.89667162962963">
if Form is whole sentence
then Return G(Form)
else Return BRANCH0Form,G(Form)1)
else Return Form
In this algorithm the first BRANCH corresponds to
the choice in ordering the top—level quantifiers.
The variable chosen will get the narrowest scope.
The second BRANCH corresponds to the decision of
whether or not to give an embedded quantifier a
wide reading. The choice R corresponds to a wide
reading, G(R) to a narrow reading. The third
BRANCH corresponds to the decision of how wide a
reading to give to an embedded quantifier.
Dependency constraints can be built into this
algorithm by restricting the elements of its
argument that BRANCH can choose. If the variables
x and y are at the same level and y is dependent
on x, then the first BRANCH cannot choose x. If y
is embedded under x and y is dependent on x, then
the second BRANCH must choose G(R). In the third
BRANCH, if any top—level bracketed variable in
Form is dependent on any variable one level of
recursion up, then G(Form) must be chosen.
A fuller explanation of this algorithm and
several further examples of the use of this
notation are given in a longer version of this
paper.
</bodyText>
<page confidence="0.998709">
62
</page>
<sectionHeader confidence="0.932979" genericHeader="categories and subject descriptors">
3. Other Determiners
</sectionHeader>
<bodyText confidence="0.995331636363637">
The approach of Section 2 will not work for
monotone decreasing determiners, such as &amp;quot;few&amp;quot; and
Intuitively, the reason is that the
sentences they occur in make statements about
entities other than just those in the sets
referred to by the noun phrase. Thus,
Few men work.
is more a negative statement about all but a few
of the men than a positive statement about few of
them. One possible representation would be
similar to (1), but with the implication reversed.
</bodyText>
<equation confidence="0.9706695">
(3 s)(Q(s,fx P(x)l)
&amp; (gly)(P(y) &amp; R(y) -&gt; yes))
</equation>
<bodyText confidence="0.999609857142857">
This is unappealing, however, among other things,
because the predicate P occurs twice, making the
relation between sentences and logical forms less
direct.
Another approach would take advantage of the
above intuition about what monotone decreasing
determiners convey.
</bodyText>
<equation confidence="0.570942">
(3 s)(&amp;(s,[x I P(x)l) &amp; (V y)(yss -&gt;-/R(y)))
</equation>
<bodyText confidence="0.973760190476191">
That is, we convert the sentence into a negative
assertion about the complement of the noun phrase,
reducing this case to the monotone increasing
case. For example, &amp;quot;few men work&amp;quot; would be
represented as follows:
(3 s)(few(s,fx man(x)l)
&amp; y)(yEs -&gt;nwork(Y)))5
(This formulation is equivalent to, but not
identical with, Barwise and Cooper&apos;s [1981]
witness set condition for monotone decreasing
determiners.)
Some determiners are neither monotone
increasing nor monotone decreasing, but Barwise
and Cooper conjecture that it is a linguistic
universal that all such determiners can be
expressed as conjunctions of monotone determiners.
For example, &amp;quot;exactly three&amp;quot; means &amp;quot;at least three
and at most three&amp;quot;. If this is true, then they
all yield to the approach presented here.
Moreover, because of redundancy, only two new
conjuncts would be introduced by this method.
</bodyText>
<sectionHeader confidence="0.995449" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.927101">
I have profited considerably in this research
from discussions with Lauri Kartunnen, Bob Moore,
Fernando Pereira, Stan Rosenscheln, and Stu
Shieber, none of whom would necessarily agree with
what I have written, nor even view it with
sympathy. This research was supported by the
Defense Advanced Research Projects Agency under
Contract No. N00039-82-C-0571, by the National
Library of Medicine under Grant No. IRO1 LM03611-
5
few is pronounced &amp;quot;few bar&amp;quot;.
01, and by the National Science Foundation under
Grant No. IST-8209346.
</bodyText>
<sectionHeader confidence="0.98514" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.996814375">
Barwise, J. and R. Cooper 1981. Generalized
quantifiers and natural language. Linguistics
and philosophy, Vol. 4, No. 2, 159-219.
Bobrow, R. and B. Webber 1980. PSI-KLONE: Parsing
and semantic interpretation in the BBN
natural language understanding system.
Proceedings, Third National Conference of
Canadian Society for Computational Studies of
Intelligence. 131-142. Victoria, British
Columbia. May 1980.
Cooper, R. 1975. Montague&apos;s semantic theory and
transformational syntax. Ph.D. thesis.
University of Massachusetts.
Davidson, D. 1967. The logical form of action
sentences. In N. Rescher (Ed.), The Logic of
Decision and Action. 81-95. University of
Pittsburgh Press, Pittsburgh, Pennsylvania.
Hobbs, J. 1976. A computational approach to
discourse analysis. Research Report 76-2,
Department of Computer Sciences, City
College, City University of New York.
Hobbs, J. 1980. Selective inferencing.
Proceedings, Third National Conference of
Canadian Society for Computational Studies of
Intelligence. 101-114. Victoria, British
Columbia. May 1980.
Hobbs, J. 1982. Representing ambiguity.
Proceedings of the First West Coast
Conference on Formal Linguistics. 15-28.
Stanford, California.
Mellish, C. 1980. Coping with uncertainty: Noun
phrase interpretation and early semantic
analysis. Ph.D. thesis. University of
Edinburgh.
Moore, R. 1973. Is there any reason to want
lexical decomposition? Unpublished
manuscript.
Van Lehn, K. 1978. Determining the scope of
English quantifiers. Massachusetts Institute
of Technology Artificial Intelligence
Laboratory Technical Report AI-TR-483.
Webber, B. 1978. A formal approach to discourse
anaphora. Technical Report 3761, Bolt Beranek
and Newman, Inc. Cambridge, Massachusetts.
Woods, W. 1977. Semantics and quantification in
natural language question answering. Advances
in Computers, Vol. 17. 1-87. Academic Press,
New York.
</reference>
<page confidence="0.999459">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000674">
<title confidence="0.99193">AN IMPROPER TREATMENT OF QUANTIFICATION IN ORDINARY ENGLISH</title>
<author confidence="0.999983">Jerry R Hobbs</author>
<affiliation confidence="0.999968">SRI International</affiliation>
<address confidence="0.998873">Menlo Park, California</address>
<abstract confidence="0.998053873772794">1. The Problem Consider the sentence In most democratic countries most politicians can fool most of the people on almost every issue most of the time. In the currently standard ways of representing quantification in logical form, this sentence has 120 different readings, or quantifier scopings. Moreover, they are truly distinct, in the sense that for any two readings, there is a model that satisfies one and not the other. With the standard logical forms produced by the syntactic and semantic translation components of current theoretical frameworks and implemented systems, it would seem that an inferencing component must process each of these 120 readings in turn in order to produce a best reading. Yet it is obvious that people do not entertain all 120 possibilities, and people really do understand the sentence. The problem is not just that inferencing is required for disambiguation. It is that people never do disambiguate completely. A singlequantifier scoping is never chosen. (Van [1978] and and Webber [1980] have made this point.) In the currently standard logical notations, it is not clear how this can be What is needed is a logical form for such sentences that is neutral with respect to the various scoping possibilities. It should be a notation that can be used easily by an inferencing component. That is, it should be easy to define deductive operations on it, and the logical forms of typical sentences should not be unwieldy. Moreover, when the inferencing component discovers further information about dependencies among sets of entities, it should entail only a minor modification in the logical form, such as conjoining a new proposition, rather than a major restructuring. Finally, since the notion of &amp;quot;scope&amp;quot; is a powerful tool in semantic analysis, there should be a fairly transparent relationship between dependency information in the notation and standard representations of scope. Three possible approaches are ruled out by these criteria. 1. Representing the sentence as a disjunction of the various readings. This is impossibly unwieldy. 1 Many people feel that most sentences exhibit too few quantifier scope ambiguities for much effort to be devoted to this problem, but a casual of sentences from any text convince almost otherwise. 2. Using as the logical notation a triple consisting of an expression of the propositional content of the sentence, a store of quantifier structures (e.g., as in Cooper [1975], Woods [1978]), and a set of constraints on how the quantifier structures could be unstored. This would adequately capture the vagueness, but it is difficult to imagine defining inference procedures that would work on such an object. Indeed, Cooper did no inferencing; Woods did little and chose a default reading heuristically before doing so. 3. Using a set—theoretic notation like that of (1) below, pushing all the universal quantifiers to the outside and the existential quantifiers to the inside, and replacing the existentially quantified variables by Skolem of all the universally variables. Then when inferencing discovers a nondependency, one of the arguments is dropped from one of the Skolem functions. One difficulty with this is that it yields representations that are too general, being satisfied by models that correspond to none of the possible intended interpretations. Moreover, in sentences in which one quantified noun phrase syntactically embeds another (what Woods [1978] calls &amp;quot;functional nesting&amp;quot;), as in Every representative of a company arrived. no representation that is neutral between the two is immediately apparent. With wide scope, &amp;quot;a company&amp;quot; is existential, with narrow scope it is universal, and a shift in commitment from one to the other would involve significant restructuring of the logical form. The approach taken here uses the notion of of a set, to produce a flat form of conjoined atomic predications. treatment has been worked out only for monotone increasing determiners; this is described in Section 2. In Section 3 some ideas about other determiners are discussed. An inferencing component, such as that explored in Hobbs [1976, 1980], capable of resolving coreference, doing coercions, and refining predicates, will be assumed (but not discussed). Thus, translating the quantifier scoping problem into one of those three processes will count as a solution for the purposes of this paper. This problem has received little attention in linguistics and computational linguistics. Those who have investigated the processes by which a rich knowledge base is used in interpreting texts have largely ignored quantifier ambiguities. Those who have studied quantifiers have generally noted that inferencing is required for 57 disambiguation, without attempting to provide a notation that would accommodate this inferencing. There are some exceptions. Bobrow and Webber [1980] discuss many of the issues involved, but it is not entirely clear what their proposals are. The work of Webber (1978] and Mellish (1980] are discussed below. 2. Monotone Increasing Determiners 2.1. A Set-Theoretic Notation Let us represent the pattern of a simple intransitive sentence with a quantifier as &amp;quot;Q Ps R&amp;quot;. In &amp;quot;Most men work,&amp;quot; Q = &amp;quot;most&amp;quot;, P = &amp;quot;man&amp;quot;, and R = &amp;quot;work&amp;quot;. Q will be referred to as a A determiner Q is increasing if and only if for any R1 and R2 such that the denotation of RI is a subset of the denotation of R2, &amp;quot;Q Ps RI&amp;quot; implies &amp;quot;Q Ps R2&amp;quot; (Barwise and Cooper (1981]). For example, letting RI = &amp;quot;work hard&amp;quot; and R2 = &amp;quot;work&amp;quot;, since &amp;quot;most men work hard&amp;quot; implies &amp;quot;most men work,&amp;quot; the determiner &amp;quot;most&amp;quot; is monotone increasing. Intuitively, making the verb phrase more general doesn&apos;t change the truth value. Other monotone increasing determiners are &amp;quot;every&amp;quot;, &amp;quot;some&amp;quot;, &amp;quot;many&amp;quot;, &amp;quot;several&amp;quot;, &amp;quot;any&amp;quot; and &amp;quot;a few&amp;quot;. &amp;quot;No&amp;quot; and &amp;quot;few&amp;quot; are not. Any noun phrase Q Ps with a monotone increasing determiner Q involves two sets, an intensionally defined set denoted by the noun phrase minus the determiner, the set of all Ps, and a nonconstructively specified set denoted by the entire noun phrase. The determiner Q can be viewed as expressing a relation between these two sets. Thus the sentence pattern Q Ps R can be represented as follows: (3s)(Q(s,(xI P(x)}) &amp; (Vy)(yEs -&gt; R(y))) That is, there is a set s which bears the relation Q to the set of all Ps, and R is true of every element of s. (Barwise and Cooper call s a &amp;quot;witness set&amp;quot;.) &amp;quot;Most men work&amp;quot; would be represented I man(x)}) Y)(Yes —&gt; For collective predicates such as &amp;quot;meet&amp;quot; and &amp;quot;agree&amp;quot;, R would apply to the set rather than to each of its elements. s) I Sometimes with singular noun phrases and like &amp;quot;a&amp;quot;, &amp;quot;some&amp;quot; and it will be more convenient to treat the determiner as a relation between a set and one of its elements. (3 Y) Q(Y,{x I P(x)}) &amp; R(y). According to notation (1) there are two aspects to quantification. The first, which concerns a relation between two sets, is discussed in Section 2.2. The second aspect involves a predication made about the elements of one of &apos;the sets. The approach taken here to this aspect of quantification is somewhat more radical, and depends on a view of semantics that might be called &amp;quot;ontological promiscuity&amp;quot;. This is described briefly in Section 2.3. Then in Section 2.4 the scope-neutral representation is presented. 2.2. Determiners as Relations between Sets Expressing determiners as relations between sets allows us to express as axioms in a knowledge base more refined properties of the determiners than can be captured by representing them in terms of the standard quantifiers. First let us note that, with the proper definitions of &amp;quot;every&amp;quot; and &amp;quot;some&amp;quot;, sl,s2) every(sl,s2) &lt;-&gt; sl = s2 some(x,s2) &lt;-&gt; xes2 formula (1) reduces to the standard notation. (This can be seen as explaining why the restriction is implicative in universal quantification and conjunctive in existential quantification.) A meaning postulate for &amp;quot;most&amp;quot; that is perhaps too mathematical is (Vsl,s2) most(sl,s2) -&gt; Isll &gt; 1/2 1s21 Next, consider &amp;quot;any&amp;quot;. Instead of trying to force an interpretation of &amp;quot;any&amp;quot; as a standard let us take it to mean random element of&amp;quot;. (2) (gex,$) any(x,$) -&gt; x = random(s), where &amp;quot;random&amp;quot; is a function that returns a random element of a set. This means that the prototypical use of &amp;quot;any&amp;quot; is in sentences like Pick any card. Let me surround this with caveats. This can&apos;t be right, if for no other reason than that &amp;quot;any&amp;quot; is surely a more &amp;quot;primitive&amp;quot; notion in language than &amp;quot;random&amp;quot;. Nevertheless, mathematics gives us firm intuitions about &amp;quot;random&amp;quot; and (2) may thus shed light on some linguistic facts. Many of the linguistic facts about &amp;quot;any&amp;quot; can be subsumed under two broad characterizations: 1. It requires a &amp;quot;modal&amp;quot; or &amp;quot;nondefinite&amp;quot; context. For example, &amp;quot;John talks to any woman&amp;quot; must be interpreted dispositionally. If we adopt (2), we can see this as deriving from the nature of randomness. It simply does not make sense to of an actualentity that it is random. 2. It normally acts as a universal quantifier outside the scope of the most immediate modal embedder. This is usually the most natural interpretation of &amp;quot;random&amp;quot;. Moreover, since &amp;quot;any&amp;quot; extracts a single element, we can make sense out of cases in which &amp;quot;any&amp;quot; fails to act like &amp;quot;every&amp;quot;. 58 I&apos;ll talk to anyone but only to one person. * I&apos;ll talk to everyone but only to one person. John wants to marry any Swedish woman. * John wants to marry every Swedish woman. (The second pair is due to Moore [19731.) This approach does not, however, seem to offer an especially convincing explanation as to why &amp;quot;any&amp;quot; functions in questions as an existential quantifier. 2.3. Ontological Promiscuity Davidson [1967] proposed a treatment of action sentences in which events are treated as individuals. This facilitated the representation of sentences with adverbials. But virtually every predication that can be made in natural language can be modified adverbially, be specified as to time, function as a cause or effect of something else, constitute a belief, be nominalized, and be referred to pronominally. It is therefore convenient to extend Davidson&apos;s approach Co all predications, an approach that might be called &amp;quot;ontological promiscuity&amp;quot;. One abandons all ontological scruples. A similar approach is used in many Al systems. We will use what might be called a &amp;quot;nominalization&amp;quot; operator &amp;quot;&apos;&amp;quot; for predicates. Corresponding to every n-ary predicate p there will be an n+1-ary predicate p&apos; whose first argument can be thought of as a condition of p&apos;s being true of the subsequent arguments. Thus, if &amp;quot;see(J,B)&amp;quot; means that John sees Bill, &amp;quot;see&apos;(E,J,B)&amp;quot; will mean that E is John&apos;s seeing of Bill. For the purposes of this paper, we can consider that the primed and unprimed predicates are related by the following axiom schema: p&amp;quot;(e,x) —&gt; p(x) —&gt; /3&amp;quot; (e,x) It is beyond the scope of this paper to elaborate on the approach further, but it will be assumed, and taken to extremes, in the remainder of the paper. Let me illustrate the extremes to which it will be taken. Frequently we want to refer to the condition of two predicates p and q holding simultaneously of x. For this we will refer to the entity e such that and&amp;quot;(e,e1,e2) &amp; p&apos;(el,x) &amp; q&amp;quot;(e2,x) Here el is the condition of p being true of x, e2 is the condition of q being true of x, and e the condition of the conjunction being true. 2.4. The Scope-Neutral Representation We will assume that a set has a typical element and that the logical form for a plural noun phrase will include reference to a set and The linguistic intuition 2 Woods [1978] mentions something like this approach, but rejects it because difficulties that worked out here would have worked out. behind this idea is that one can use singular pronouns and definite noun phrases as anaphors for plurals. Definite and indefinite generics can also be understood as referring to the typical element of a set. In the spirit of ontological promiscuity, we assume that typical elements of things that exist, and encode in meaning postulates the necessary relations between a set&apos;s typical element and its real elements. This move amounts to reifying the universally quantified variable. The typical element of s will be referred to as There are two very nearly contradictory properties that typical elements must have. The first is the equivalent of universal instantiation; real elements should inherit the properties of the typical element. The second is that the typical element cannot itself be an element of the set, for that would lead to cardinality problems. The two together would the set has no We could get around this problem by positing a special set of predicates that apply to typical elements and are systematically related to the predicates that apply to real elements. This idea should be rejected as being ad hoc, if aid did not come to us from an unexpected quarter -the notion of &amp;quot;grain size&amp;quot;. When utterances predicate, it is normally at some degree of resolution, or &amp;quot;grain&amp;quot;. At a fairly coarse grain, we might say that John is at the post office -- &amp;quot;at(J,P0)&amp;quot;. At a more refined we have to say that he at the window -- &amp;quot;at(J,SW)&amp;quot;. We normally think of grain in terms of distance, but more generally we can move from entities at one grain to entities at a coarser grain by means of an arbitrary partition. Fine-grained entities in the same equivalence class are indistinguishable at the coarser grain. Given a set S, consider the partition that all elements of one element and leaves everything else unchanged. We can view the typical element of S as the set of real elements seen at this coarser grain -a grain at which, the elements the set Formally, we define an takes a set and a predicate as arguments and produces what referred to as an &amp;quot;indexed predicate&amp;quot;: T, if &amp; (V yes) p(y), if x=r(s) P(Y), p(x) otherwise. We will frequently abbreviate this &amp;quot;p,&amp;quot;. Note that predicate indexing gets us out of the above -------- 3 An alternative approach would be to say that the typical element is in fact one of the real elements of the set, but that we will never know which one, and that furthermore, we will never know about the typical element any property that is not true of all the elements. This approach runs into technical difficulties involving the empty set. 59 for now E, s&amp;quot; is not only true but tautologous. We are now in a position to state the properties typical elements should have. The first implements universal instantiation: With what we have, we can represent the distinction between the distributive and collective readings of a sentence like (10) The men lifted the piano. (t(s,y) &amp; yes -&gt; p(y) (is)([(ixes) —&gt; (2s))) That is, the properties of the typical element at the coarser grain are also the properties of the real elements at the finer grain, and the typical element has those properties that all the real elements have. Note that while we can infer a property from set membership, we cannot infer set membership from a property. That is, the fact that p is true of a typical element of a set s and p is true of an entity y, does not imply that y is an of s. After all, we will want &amp;quot;three to refer to a set, and to be able to infer from y&apos;s being in the set the fact that y is a man. But we do not want to infer from y&apos;s being a man that y is in the set. Nevertheless, we will need a notation for expressing this stronger relation among a set, a typical element, and a defining condition. In particular, we need it for representing &amp;quot;every man&amp;quot;. Let us develop the notation from the standard notation for intensionally defined sets, (6) s • (x by performing a fairly straightforward, though ontologically promiscuous, syntactic translation on it. First, instead of viewing x as a universally quantified variable, let us treat it as the typical element of s. Next, as a way of getting a handle on &amp;quot;p(x)&amp;quot;, we will use the nominalization operator &amp;quot;&apos;&amp;quot; to reify it, and refer to the condition e of p (or p,) being true of the element x of s (e,x)&amp;quot;. Expression (6) can then be translated into the following flat predicate-argument form: set(s,x,e) &amp; This should be read as saying that s is a set whose typical element is x and which is defined by condition e, which is the condition of p (interpreted at the level of the typical element) being true of x. The two critical properties of the predicate &amp;quot;set&amp;quot; which make (7) equivalent to (6) are the following: Ois,x,e,y) set(s,x,e) &amp; (e,x) &amp; p(y) -&gt; yEs (9) (/s,x,e) set(s,x,e) -&gt; x Axiom schema (8) tells us that if an entity y has the defining property p of the set s, then y is an element of s. Axiom (9), along with axiom schemas (4) and (3), tells us that an element of a set has the set&apos;s defining property. For the collective reading the representation would include &amp;quot;lift(m)&amp;quot; where m is the set of men. For the distributive reading, the representation would have &amp;quot;lift(2,&amp;quot;(m))&amp;quot;, where Z(m) is the typical element of the set m. To represent the ambiguity of (10), we could use the device suggested in Hobbs (1982] for prepositional phrase and other ambiguities, and write &amp;quot;lift(x) &amp; (x=m v x= This approach involves a more thorough use of typical elements than two previous approaches. Webber (19781 admitted both set and prototype (my typical element) interpretations of phrases like &amp;quot;each man&amp;quot; in order to have antecedents for both &amp;quot;they&amp;quot; and &amp;quot;he&amp;quot;, but she maintained a distinction between the two. Essentially, she treated &amp;quot;each man&amp;quot; as ambiguous, whereas the present approach makes both the typical element and the set available for subsequent reference. Mellish (1980j uses typical elements strictly as an intermediate representation that must be resolved into more standard notation by the end of processing. He can do this because he is working in a task domain -physics problems -in which sets are not just finite but small, and vagueness as to their composition must be resolved. Webber did not attempt to use typical elements to derive a scope-neutral representation; Mellish did so only in a limited way. Scope dependencies can now be represented as relations among typical elements. Consider the sentence (11) Most men love several women, under the reading in which there is a different set of women for each man. We can define a functionf which for each man returns the set of women whom that man loves. f(m) = I woman(w) &amp; love(m,w)} The relevant parts of the initial logical form, produced by a syntactic and semantic translation component, for sentence (11) will be (12) love(r(m),r(w)) &amp; most(m,m1) &amp; man1(&apos;r(m1)) &amp; several(w) &amp; womanl(r(w)) where ml is the set of all men, m the set of most of them referred to by the noun phrase most men&amp;quot;, and w the set referred to by the noun phrase &amp;quot;several women&amp;quot;, and where &amp;quot;manl = of(ml,man)&amp;quot; and &amp;quot;womanl = (w,woman)&amp;quot;. When the inferencing component discovers there is a different set w for each element of the set m, w can be viewed as refering to the typical element of this set of sets: 60 To eliminate the set notation, we can extend the definition of the dependency function to the typical element of m as follows: Z({f(X) I That is, f maps the typical element of a set into the typical element of the set of images under f of the elements of the set. From here on, we will consider all dependency functions so extended to the typical elements of their domains. The identity &amp;quot;w = f(p(m))&amp;quot; now simultaneously encodes the scoping information and involves only existentially quantified variables denoting individuals in an (admittedly ontologically promiscuous) domain. Expressions like (12) are thus the scope-neutral representation, and scoping information is added by conjoining such identities. Let us now consider several examples in which processes of interpretation result in the acquisition of scoping information. The first will involve interpretation against a small model. The second will make use of world knowledge, while the third illustrates the treatment of embedded quantifiers. First the simple, and classic, example. (13) Every man loves some woman. The initial logical form for this sentence includes the following: lovel(r(ms),w) &amp; manl(r(ms)) &amp; woman(w) where &amp;quot;lovel =cr(ms,xx[love(x,w)])&amp;quot; and &amp;quot;manl = T(ms,man)&amp;quot;. Figure 1 illustrates two small models of this sentence. M is the set of men [AM, W is the set of women {X,Y}, and the arrows signify love. Let us assume that the process of interpreting this sentence is just the process of identifying the existentially quantified variables ms and w and possibly coercing the predicates, in way that makes the sentence B Y (b) Figure 1. Two models of sentence (13). In Figure 1(a), &amp;quot;love(A,X)&amp;quot; and &amp;quot;love(B,X)&amp;quot; are both true, so we can use axiom schema (5) to derive &amp;quot;lovel(p(M),X)&amp;quot;. Thus, the identifications &amp;quot;ms = M&amp;quot; and &amp;quot;w = X&amp;quot; result in the sentence being true. In Figure 1(b), &amp;quot;love(A,X)&amp;quot; and &amp;quot;love(B,Y)&amp;quot; are both true, but since these predications differ 4 Bobrow and Webber [1980] similarly show scoping information acquired by interpretation against a small model. in more than one argument, we cannot apply axiom schema (5). First we define a dependency function f, mapping each man into a woman he loves, yielding &amp;quot;love(A,f(A))&amp;quot; and &amp;quot;love(B,f(B))&amp;quot;. We can now apply axiom schema (5) to derive &amp;quot;love2(Z(M),f(Z(M)))&amp;quot;, where &amp;quot;love2 we can make the sentence true by identifying as with M and w with f(p(M)), and by coercing &amp;quot;love&amp;quot; to &amp;quot;love2&amp;quot; and &amp;quot;woman&amp;quot; to &amp;quot;cr(W,woman)&amp;quot;. , In each case we see that the identification of w is equivalent to solving the scope ambiguity problem. In our subsequent examples we will ignore the indexing on the predicates, until it must be mentioned in the case of embedded quantifiers. Next consider an example in which world knowledge leads to disambiguation: Three women had a baby. Before inferencing, the scope-neutral representation is had(rws),b) &amp; Iws1=3 &amp; woman(r(ws)) &amp; baby(b) Let us suppose the inferencing component has axioms about the functionality of having a baby -something like (lex,y) had(x,y) -&gt; x = mother-of(y) and that we know about cardinality the fact that for any function g and set s, Ig(s)1 Is] Then we know the following: 3 = Iwsl = Imother-of(b)I Ibl This tells us that b cannot be an individual but must be the typical element of some set. Let f be a dependency function such that wfws &amp; f(w) = x -&gt; had(w,x) that is, a function that maps each woman into some baby she had. Then we can identify b with with r(ff(w) I wEws}), giving us the correct scope. Finally, let us return to interpretation with respect to small models to see how embedded quantifiers are represented. Consider (14) Every representative of a company arrived. The initial logical form .includes arrive(r) &amp; set(rs,r,ea) &amp; and&apos;(ea,er,eo) &amp; rep&apos;(er,r) &amp; of&apos;(eo,r,c) &amp; co(c) That is, r arrives, where r is the typical element of a set rs defined by the conjunction ea of r&apos;s being a representative and r&apos;s being of c, where c is a company. We will consider the two models in 61 Figure 2. R is the set of representatives (A,B,(C), K is the set of companies {X,Y,(Z,W)}, there is an arrow from the representatives to the companies they represent, and the representatives who arrived are circled. in the algorithm below it is refered to as &amp;quot;Quant(v)&amp;quot;. The translation of the remainder of the inferential logical form into bracketed notation is best shown by example. For the sentence B Y A representative of every company saw a sample (a) the relevant parts of the inferential logical form are see(r,$) &amp; r;p(r) &amp; of(r,c) &amp; co(c) &amp; sample(s) where &amp;quot;see(r,$)&amp;quot; is asserted. This is translated &amp;quot; in a straightforward way into (18) seegr I rep(r) &amp; of(r,[c I co(c)I)I, [s I sample(s)]) Figure 2. Two models of sentence (14). In Figure 2(a), &amp;quot;of(A,X)&amp;quot;, &amp;quot;of(B,Y)&amp;quot; and &amp;quot;of(B,Z)&amp;quot; are true. Define a dependency function f to map A into X and B into Y. Then &amp;quot;of(A,f(A))&amp;quot; and &amp;quot;of(B,f(B))&amp;quot; are both true, so that &amp;quot;of(r(R),f(r(R)))&amp;quot; is also true. Thus we have the following identifications: c f(r(R)) .1t({X,Y}), rs R, r .r(R) In Figure 2(b) &amp;quot;of(B,y)&amp;quot; and &amp;quot;of(C,Y)&amp;quot; are both true, so &amp;quot;of(2-(R1),Yris also. Thus we may let c be Y and rs be R1, giving us the wide reading for &amp;quot;a company&amp;quot;. In the case where no one represents any company and no one arrived, we can let c be anything and rs be the empty set. Since, by the definition of dr , any predicate indexed by the empty set will be true of the typical element of empty set, &amp;quot;arrive (2-(#))be true, and the sentence will be satisfied. It is worth pointing out that this approach solves the problem of the classic &amp;quot;donkey sentences&amp;quot;. If in sentence (14) we had had the verb phrase &amp;quot;hates it&amp;quot;, then &amp;quot;it&amp;quot; would be resolved to c, and thus to whatever c was resolved to. So Ear the notation of typical elements and dependency functions has been introduced; it has been shown how scope information can be represented by these means; and an example of inferential processing acquiring that scope information has been given. Now the precise relation of this notation to standard notation must be specified. This can be done by means of an algorithm that takes the inferential notation, together with an indication of which proposition is asserted by the sentence, and produces in the conventional form all of the readings consistent with the known dependency information. First we must put the sentence into what will be called a &amp;quot;bracketed notation&amp;quot;. We associate with each variable v an indication of the corresponding quantifier; this is determined from such pieces of the inferential logical form as those involving the predicates &amp;quot;set&amp;quot; and &amp;quot;most&amp;quot;; This may be read &amp;quot;An r such that r is a representative and r is of a c such that c is a company sees an s such that s is a sample. The nondeterministic algorithm below generates all the scopings from the bracketed notation. The function TOPBVS returns a list of all the top—level bracketed variables in Form, that is, all the bracketed variables except those within the brackets of some other variable -in (18) r and s but not c. BRANCH nondeterministically generates a separate process for each element in a list it is given as argument. A four—part notation is used for quantifiers (similar to that of Woods [19781) &amp;quot;(quantifier variable restriction body)&amp;quot;. G(Form): [vIR] v 3RANCH(CR,G(R)1) Form&apos;) if Form is whole sentence then Return G(Form) else Return BRANCH0Form,G(Form)1) else Return Form In this algorithm the first BRANCH corresponds to the choice in ordering the top—level quantifiers. The variable chosen will get the narrowest scope. The second BRANCH corresponds to the decision of whether or not to give an embedded quantifier a wide reading. The choice R corresponds to a wide reading, G(R) to a narrow reading. The third BRANCH corresponds to the decision of how wide a reading to give to an embedded quantifier. Dependency constraints can be built into this algorithm by restricting the elements of its argument that BRANCH can choose. If the variables x and y are at the same level and y is dependent on x, then the first BRANCH cannot choose x. If y is embedded under x and y is dependent on x, then the second BRANCH must choose G(R). In the third BRANCH, if any top—level bracketed variable in Form is dependent on any variable one level of recursion up, then G(Form) must be chosen. A fuller explanation of this algorithm and several further examples of the use of this notation are given in a longer version of this paper. 62 3. Other Determiners The approach of Section 2 will not work for monotone decreasing determiners, such as &amp;quot;few&amp;quot; and Intuitively, the reason is that the sentences they occur in make statements about entities other than just those in the sets referred to by the noun phrase. Thus, Few men work. is more a negative statement about all but a few of the men than a positive statement about few of them. One possible representation would be similar to (1), but with the implication reversed. P(x)l) &amp; (gly)(P(y) &amp; R(y) -&gt; yes)) This is unappealing, however, among other things, because the predicate P occurs twice, making the relation between sentences and logical forms less direct. Another approach would take advantage of the above intuition about what monotone decreasing determiners convey. P(x)l) &amp; (V y)(yss That is, we convert the sentence into a negative assertion about the complement of the noun phrase, reducing this case to the monotone increasing case. For example, &amp;quot;few men work&amp;quot; would be represented as follows: man(x)l) (This formulation is equivalent to, but not identical with, Barwise and Cooper&apos;s [1981] witness set condition for monotone decreasing determiners.) Some determiners are neither monotone increasing nor monotone decreasing, but Barwise and Cooper conjecture that it is a linguistic universal that all such determiners can be expressed as conjunctions of monotone determiners. For example, &amp;quot;exactly three&amp;quot; means &amp;quot;at least three and at most three&amp;quot;. If this is true, then they all yield to the approach presented here. Moreover, because of redundancy, only two new conjuncts would be introduced by this method. Acknowledgments I have profited considerably in this research from discussions with Lauri Kartunnen, Bob Moore, Fernando Pereira, Stan Rosenscheln, and Stu Shieber, none of whom would necessarily agree with what I have written, nor even view it with sympathy. This research was supported by the</abstract>
<note confidence="0.987986142857143">Defense Advanced Research Projects Agency under Contract No. N00039-82-C-0571, by the National Library of Medicine under Grant No. IRO1 LM03611- 5 few is pronounced &amp;quot;few bar&amp;quot;. 01, and by the National Science Foundation under Grant No. IST-8209346.</note>
<title confidence="0.293335">REFERENCES</title>
<author confidence="0.426619">J Barwise</author>
<author confidence="0.426619">R Cooper</author>
<abstract confidence="0.8815188">and natural language. philosophy,Vol. 4, No. 2, 159-219. Bobrow, R. and B. Webber 1980. PSI-KLONE: Parsing and semantic interpretation in the BBN natural language understanding system.</abstract>
<note confidence="0.746115384615385">Third National Conferenceof Societyfor Studiesof Intelligence.131-142. Victoria, British Columbia. May 1980. Cooper, R. 1975. Montague&apos;s semantic theory and transformational syntax. Ph.D. thesis. University of Massachusetts. Davidson, D. 1967. The logical form of action In N. Rescher (Ed.), The Logicof Decisionand Action.81-95. University of Pittsburgh Press, Pittsburgh, Pennsylvania. Hobbs, J. 1976. A computational approach to discourse analysis. Research Report 76-2,</note>
<affiliation confidence="0.996983">Department of Computer Sciences, City</affiliation>
<address confidence="0.850432">College, City University of New York. Hobbs, J. 1980. Selective inferencing.</address>
<note confidence="0.655571296296296">Third National Conferenceof Societyfor Studiesof Intelligence.101-114. Victoria, British Columbia. May 1980. Hobbs, J. 1982. Representing ambiguity. Proceedingsof the FirstWest Conferenceon Linguistics.15-28. Stanford, California. Mellish, C. 1980. Coping with uncertainty: Noun phrase interpretation and early semantic analysis. Ph.D. thesis. University of Edinburgh. R. 1973. Is there anyreason to want lexical decomposition? Unpublished manuscript. Van Lehn, K. 1978. Determining the scope of English quantifiers. Massachusetts Institute of Technology Artificial Intelligence Laboratory Technical Report AI-TR-483. Webber, B. 1978. A formal approach to discourse anaphora. Technical Report 3761, Bolt Beranek and Newman, Inc. Cambridge, Massachusetts. Woods, W. 1977. Semantics and quantification in language question answering. Computers,Vol. 17. 1-87. Academic Press, New York. 63</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Barwise</author>
<author>R Cooper</author>
</authors>
<title>Generalized quantifiers and natural language.</title>
<date>1981</date>
<journal>Linguistics and philosophy,</journal>
<volume>4</volume>
<pages>159--219</pages>
<contexts>
<context position="5722" citStr="Barwise and Cooper (1981" startWordPosition="900" endWordPosition="903">nd Webber [1980] discuss many of the issues involved, but it is not entirely clear what their proposals are. The work of Webber (1978] and Mellish (1980] are discussed below. 2. Monotone Increasing Determiners 2.1. A Set-Theoretic Notation Let us represent the pattern of a simple intransitive sentence with a quantifier as &amp;quot;Q Ps R&amp;quot;. In &amp;quot;Most men work,&amp;quot; Q = &amp;quot;most&amp;quot;, P = &amp;quot;man&amp;quot;, and R = &amp;quot;work&amp;quot;. Q will be referred to as a determiner. A determiner Q is monotone increasing if and only if for any R1 and R2 such that the denotation of RI is a subset of the denotation of R2, &amp;quot;Q Ps RI&amp;quot; implies &amp;quot;Q Ps R2&amp;quot; (Barwise and Cooper (1981]). For example, letting RI = &amp;quot;work hard&amp;quot; and R2 = &amp;quot;work&amp;quot;, since &amp;quot;most men work hard&amp;quot; implies &amp;quot;most men work,&amp;quot; the determiner &amp;quot;most&amp;quot; is monotone increasing. Intuitively, making the verb phrase more general doesn&apos;t change the truth value. Other monotone increasing determiners are &amp;quot;every&amp;quot;, &amp;quot;some&amp;quot;, &amp;quot;many&amp;quot;, &amp;quot;several&amp;quot;, &amp;quot;any&amp;quot; and &amp;quot;a few&amp;quot;. &amp;quot;No&amp;quot; and &amp;quot;few&amp;quot; are not. Any noun phrase Q Ps with a monotone increasing determiner Q involves two sets, an intensionally defined set denoted by the noun phrase minus the determiner, the set of all Ps, and a nonconstructively specified set denoted by the entire noun</context>
</contexts>
<marker>Barwise, Cooper, 1981</marker>
<rawString>Barwise, J. and R. Cooper 1981. Generalized quantifiers and natural language. Linguistics and philosophy, Vol. 4, No. 2, 159-219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bobrow</author>
<author>B Webber</author>
</authors>
<title>PSI-KLONE: Parsing and semantic interpretation in the BBN natural language understanding system.</title>
<date>1980</date>
<booktitle>Proceedings, Third National Conference of Canadian Society for Computational Studies of Intelligence.</booktitle>
<pages>131--142</pages>
<location>Victoria, British Columbia.</location>
<marker>Bobrow, Webber, 1980</marker>
<rawString>Bobrow, R. and B. Webber 1980. PSI-KLONE: Parsing and semantic interpretation in the BBN natural language understanding system. Proceedings, Third National Conference of Canadian Society for Computational Studies of Intelligence. 131-142. Victoria, British Columbia. May 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cooper</author>
</authors>
<title>Montague&apos;s semantic theory and transformational syntax.</title>
<date>1975</date>
<tech>Ph.D. thesis.</tech>
<institution>University of Massachusetts.</institution>
<marker>Cooper, 1975</marker>
<rawString>Cooper, R. 1975. Montague&apos;s semantic theory and transformational syntax. Ph.D. thesis. University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>In N. Rescher (Ed.), The Logic of Decision and Action.</booktitle>
<pages>81--95</pages>
<institution>University of Pittsburgh Press,</institution>
<location>Pittsburgh, Pennsylvania.</location>
<marker>Davidson, 1967</marker>
<rawString>Davidson, D. 1967. The logical form of action sentences. In N. Rescher (Ed.), The Logic of Decision and Action. 81-95. University of Pittsburgh Press, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>A computational approach to discourse analysis.</title>
<date>1976</date>
<tech>Research Report 76-2,</tech>
<institution>Department of Computer Sciences, City College, City University of New York.</institution>
<marker>Hobbs, 1976</marker>
<rawString>Hobbs, J. 1976. A computational approach to discourse analysis. Research Report 76-2, Department of Computer Sciences, City College, City University of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Selective inferencing.</title>
<date>1980</date>
<booktitle>Proceedings, Third National Conference of Canadian Society for Computational Studies of Intelligence.</booktitle>
<pages>101--114</pages>
<location>Victoria, British Columbia.</location>
<marker>Hobbs, 1980</marker>
<rawString>Hobbs, J. 1980. Selective inferencing. Proceedings, Third National Conference of Canadian Society for Computational Studies of Intelligence. 101-114. Victoria, British Columbia. May 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hobbs</author>
</authors>
<title>Representing ambiguity.</title>
<date>1982</date>
<booktitle>Proceedings of the First West Coast Conference on Formal Linguistics.</booktitle>
<pages>15--28</pages>
<location>Stanford, California.</location>
<contexts>
<context position="17670" citStr="Hobbs (1982" startWordPosition="2977" endWordPosition="2978">s,x,e,y) set(s,x,e) &amp; p&apos;s (e,x) &amp; p(y) -&gt; yEs (9) (/s,x,e) set(s,x,e) -&gt; x Axiom schema (8) tells us that if an entity y has the defining property p of the set s, then y is an element of s. Axiom (9), along with axiom schemas (4) and (3), tells us that an element of a set has the set&apos;s defining property. For the collective reading the representation would include &amp;quot;lift(m)&amp;quot; where m is the set of men. For the distributive reading, the representation would have &amp;quot;lift(2,&amp;quot;(m))&amp;quot;, where Z(m) is the typical element of the set m. To represent the ambiguity of (10), we could use the device suggested in Hobbs (1982] for prepositional phrase and other ambiguities, and write &amp;quot;lift(x) &amp; (x=m v x= This approach involves a more thorough use of typical elements than two previous approaches. Webber (19781 admitted both set and prototype (my typical element) interpretations of phrases like &amp;quot;each man&amp;quot; in order to have antecedents for both &amp;quot;they&amp;quot; and &amp;quot;he&amp;quot;, but she maintained a distinction between the two. Essentially, she treated &amp;quot;each man&amp;quot; as ambiguous, whereas the present approach makes both the typical element and the set available for subsequent reference. Mellish (1980j uses typical elements strictly as an i</context>
</contexts>
<marker>Hobbs, 1982</marker>
<rawString>Hobbs, J. 1982. Representing ambiguity. Proceedings of the First West Coast Conference on Formal Linguistics. 15-28. Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
</authors>
<title>Coping with uncertainty: Noun phrase interpretation and early semantic analysis.</title>
<date>1980</date>
<tech>Ph.D. thesis.</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="5250" citStr="Mellish (1980" startWordPosition="812" endWordPosition="813">his problem has received little attention in linguistics and computational linguistics. Those who have investigated the processes by which a rich knowledge base is used in interpreting texts have largely ignored quantifier ambiguities. Those who have studied quantifiers have generally noted that inferencing is required for 57 disambiguation, without attempting to provide a notation that would accommodate this inferencing. There are some exceptions. Bobrow and Webber [1980] discuss many of the issues involved, but it is not entirely clear what their proposals are. The work of Webber (1978] and Mellish (1980] are discussed below. 2. Monotone Increasing Determiners 2.1. A Set-Theoretic Notation Let us represent the pattern of a simple intransitive sentence with a quantifier as &amp;quot;Q Ps R&amp;quot;. In &amp;quot;Most men work,&amp;quot; Q = &amp;quot;most&amp;quot;, P = &amp;quot;man&amp;quot;, and R = &amp;quot;work&amp;quot;. Q will be referred to as a determiner. A determiner Q is monotone increasing if and only if for any R1 and R2 such that the denotation of RI is a subset of the denotation of R2, &amp;quot;Q Ps RI&amp;quot; implies &amp;quot;Q Ps R2&amp;quot; (Barwise and Cooper (1981]). For example, letting RI = &amp;quot;work hard&amp;quot; and R2 = &amp;quot;work&amp;quot;, since &amp;quot;most men work hard&amp;quot; implies &amp;quot;most men work,&amp;quot; the determiner &amp;quot;m</context>
<context position="18230" citStr="Mellish (1980" startWordPosition="3063" endWordPosition="3064">10), we could use the device suggested in Hobbs (1982] for prepositional phrase and other ambiguities, and write &amp;quot;lift(x) &amp; (x=m v x= This approach involves a more thorough use of typical elements than two previous approaches. Webber (19781 admitted both set and prototype (my typical element) interpretations of phrases like &amp;quot;each man&amp;quot; in order to have antecedents for both &amp;quot;they&amp;quot; and &amp;quot;he&amp;quot;, but she maintained a distinction between the two. Essentially, she treated &amp;quot;each man&amp;quot; as ambiguous, whereas the present approach makes both the typical element and the set available for subsequent reference. Mellish (1980j uses typical elements strictly as an intermediate representation that must be resolved into more standard notation by the end of processing. He can do this because he is working in a task domain -- physics problems -- in which sets are not just finite but small, and vagueness as to their composition must be resolved. Webber did not attempt to use typical elements to derive a scope-neutral representation; Mellish did so only in a limited way. Scope dependencies can now be represented as relations among typical elements. Consider the sentence (11) Most men love several women, under the reading</context>
</contexts>
<marker>Mellish, 1980</marker>
<rawString>Mellish, C. 1980. Coping with uncertainty: Noun phrase interpretation and early semantic analysis. Ph.D. thesis. University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Is there any reason to want lexical decomposition?</title>
<date>1973</date>
<note>Unpublished manuscript.</note>
<marker>Moore, 1973</marker>
<rawString>Moore, R. 1973. Is there any reason to want lexical decomposition? Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Van Lehn</author>
</authors>
<title>Determining the scope of English quantifiers.</title>
<date>1978</date>
<tech>Technical Report AI-TR-483.</tech>
<institution>Massachusetts Institute of Technology Artificial Intelligence Laboratory</institution>
<marker>Van Lehn, 1978</marker>
<rawString>Van Lehn, K. 1978. Determining the scope of English quantifiers. Massachusetts Institute of Technology Artificial Intelligence Laboratory Technical Report AI-TR-483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Webber</author>
</authors>
<title>A formal approach to discourse anaphora.</title>
<date>1978</date>
<tech>Technical Report 3761,</tech>
<institution>Bolt Beranek and Newman, Inc.</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="5231" citStr="Webber (1978" startWordPosition="809" endWordPosition="810">s of this paper. This problem has received little attention in linguistics and computational linguistics. Those who have investigated the processes by which a rich knowledge base is used in interpreting texts have largely ignored quantifier ambiguities. Those who have studied quantifiers have generally noted that inferencing is required for 57 disambiguation, without attempting to provide a notation that would accommodate this inferencing. There are some exceptions. Bobrow and Webber [1980] discuss many of the issues involved, but it is not entirely clear what their proposals are. The work of Webber (1978] and Mellish (1980] are discussed below. 2. Monotone Increasing Determiners 2.1. A Set-Theoretic Notation Let us represent the pattern of a simple intransitive sentence with a quantifier as &amp;quot;Q Ps R&amp;quot;. In &amp;quot;Most men work,&amp;quot; Q = &amp;quot;most&amp;quot;, P = &amp;quot;man&amp;quot;, and R = &amp;quot;work&amp;quot;. Q will be referred to as a determiner. A determiner Q is monotone increasing if and only if for any R1 and R2 such that the denotation of RI is a subset of the denotation of R2, &amp;quot;Q Ps RI&amp;quot; implies &amp;quot;Q Ps R2&amp;quot; (Barwise and Cooper (1981]). For example, letting RI = &amp;quot;work hard&amp;quot; and R2 = &amp;quot;work&amp;quot;, since &amp;quot;most men work hard&amp;quot; implies &amp;quot;most men work,</context>
<context position="17856" citStr="Webber (1978" startWordPosition="3006" endWordPosition="3007">f s. Axiom (9), along with axiom schemas (4) and (3), tells us that an element of a set has the set&apos;s defining property. For the collective reading the representation would include &amp;quot;lift(m)&amp;quot; where m is the set of men. For the distributive reading, the representation would have &amp;quot;lift(2,&amp;quot;(m))&amp;quot;, where Z(m) is the typical element of the set m. To represent the ambiguity of (10), we could use the device suggested in Hobbs (1982] for prepositional phrase and other ambiguities, and write &amp;quot;lift(x) &amp; (x=m v x= This approach involves a more thorough use of typical elements than two previous approaches. Webber (19781 admitted both set and prototype (my typical element) interpretations of phrases like &amp;quot;each man&amp;quot; in order to have antecedents for both &amp;quot;they&amp;quot; and &amp;quot;he&amp;quot;, but she maintained a distinction between the two. Essentially, she treated &amp;quot;each man&amp;quot; as ambiguous, whereas the present approach makes both the typical element and the set available for subsequent reference. Mellish (1980j uses typical elements strictly as an intermediate representation that must be resolved into more standard notation by the end of processing. He can do this because he is working in a task domain -- physics problems -- in whi</context>
</contexts>
<marker>Webber, 1978</marker>
<rawString>Webber, B. 1978. A formal approach to discourse anaphora. Technical Report 3761, Bolt Beranek and Newman, Inc. Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Semantics and quantification in natural language question answering.</title>
<date>1977</date>
<journal>Advances in Computers,</journal>
<volume>17</volume>
<pages>1--87</pages>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<marker>Woods, 1977</marker>
<rawString>Woods, W. 1977. Semantics and quantification in natural language question answering. Advances in Computers, Vol. 17. 1-87. Academic Press, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>