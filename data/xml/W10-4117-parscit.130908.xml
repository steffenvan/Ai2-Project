<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000550">
<title confidence="0.986813">
Exploiting Social Q&amp;A Collection in Answering Complex Questions
</title>
<author confidence="0.997225">
Youzheng Wu Hisashi Kawai
</author>
<affiliation confidence="0.862474">
Spoken Language Communication Group, MASTAR Project
National Institute of Information and Communications Technology
2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, Japan
</affiliation>
<email confidence="0.997554">
{youzheng.wu, hisashi.kawai}@nict.go.jp
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920058823529">
This paper investigates techniques to au-
tomatically construct training data from
social Q&amp;A collections such as Yahoo!
Answer to support a machine learning-
based complex QA system1. We extract
cue expressions for each type of question
from collected training data and build
question-type-specific classifiers to im-
prove complex QA system. Experiments
on 10 types of complex Chinese ques-
tions verify that it is effective to mine
knowledge from social Q&amp;A collections
for answering complex questions, for in-
stance, the F3 improvement of our sys-
tem over the baseline and translation-
based model reaches 7.9% and 5.1%, re-
spectively.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985052692307692">
Research on the topic of QA systems has mainly
concentrated on answering factoid, definitional,
reason and opinion questions. Among the ap-
proaches proposed to answer these questions,
machine learning techniques have been found
more effective in constructing QA components
from scratch. Yet these supervised techniques re-
quire a certain scale of (question, answer), short
for Q&amp;A, pairs as training data. For example,
(Echihabi et al., 2003) and (Sasaki, 2005) con-
structed 90,000 English Q&amp;A pairs and 2,000
Japanese Q&amp;A pairs, respectively for their fac-
toid QA systems. (Cui et al., 2004) constructed
</bodyText>
<footnote confidence="0.748854">
1Complex questions cannot be answered by simply ex-
tracting named entities. In this paper complex questions do
not include definitional questions.
</footnote>
<bodyText confidence="0.99991862264151">
76 term-definition pairs for their definitional QA
systems. (Stoyanov et al., 2005) required a
known subjective vocabulary for their opinion
QA. (Higashinaka and Isozaki, 2008) used 4,849
positive and 521,177 negative examples in their
reason QA system. Among complex QA sys-
tems, many other types of questions have not
been well studied, apart from reason and defi-
nitional questions. Appendix A lists 10 types of
complex Chinese questions and their examples
we discussed in this paper.
According to the related studies on QA, su-
pervised machine-learning technique may be ef-
fective for answering these questions. To em-
ploy the supervised approach, we need to re-
construct training Q&amp;A pairs for each type of
question, though this is an extremely expensive
and labor-intensive task. To deal with the ac-
quisition problem of training Q&amp;A pairs, we in-
vestigate techniques to automatically construct
training data by utilizing social Q&amp;A collections
crawled from the Web, which contains millions
of user-generated Q&amp;A pairs. Many studies
(Surdeanu et al., 2008) (Duan et al., 2008) have
been done on retrieving similar Q&amp;A pairs from
social QA websites as answers to test questions.
Our study, however, regards social Q&amp;A web-
sites as a knowledge repository and aims to mine
knowledge from them for synthesizing answers
to questions from multiple documents. There is
very little literature on this aspect. Our work can
be seen as a kind of query-based summarization
(Dang, 2006) (Harabagiu et al., 2006) (Erkan
and Radev, 2004), and can also be employed to
answer questions that have not been answered in
social Q&amp;A websites.
This paper mainly focuses on the following three
steps: (1) automatically constructing question -
type-specific training Q&amp;A pairs from the so-
cial Q&amp;A collection; (2) extracting cue expres-
sions for each type of question from the col-
lected training data, and (3) building question-
type-specific classifiers to filer out noise sen-
tences before using a state-of-the-art IR formula
to select answers.
We evaluate our system on 10 types of Chi-
nese questions by using the Pourpre evalua-
tion tool (Lin and Demner-Fushman, 2006).
The experimental results show the effectiveness
of our system, for instance, the F3/NR im-
provement of our system over the baseline and
translation-based model reaches 7.9%/11.1%,
and 5.1%/5.6%, respectively.
</bodyText>
<sectionHeader confidence="0.982945" genericHeader="method">
2 Social Q&amp;A Collection
</sectionHeader>
<bodyText confidence="0.99978256">
Recently launched social QA websites such as
Yahoo! Answer2 and Baidu Zhidao3 provide
an interactive platform for users to post ques-
tions and answers. After questions are answered
by users, the best answer can be chosen by the
asker or nominated by the community. The num-
ber of Q&amp;A pairs on such sites has risen dra-
matically. These pairs could collectively form a
source of training data that is required in super-
vised machine-learning-based QA systems.
In this paper we aim to explore such user-
generated Q&amp;A collections to automatically col-
lect Q&amp;A training data. However, social col-
lections have two salient characteristics: tex-
tual mismatch between questions and answers
(i.e., question words are not necessarily used
in answers); and user-generated spam or flip-
pant answers, which are unfavorable factors in
our study. Thus, we only crawl questions and
their best answers to form Q&amp;A pairs, wherein
the best answers are longer than the empiri-
cal threshold. Finally, 60.0 million Q&amp;A pairs
were crawled from Chinese social QA websites.
These pairs will be used as the source of training
data required in our study.
</bodyText>
<footnote confidence="0.999745">
2http://answers.yahoo.com/
3http://zhidao.baidu.com/
</footnote>
<sectionHeader confidence="0.971876" genericHeader="method">
3 Our Complex QA System
</sectionHeader>
<bodyText confidence="0.999956541666667">
The typical complex QA system architecture is
a cascade of three modules. The Question Ana-
lyzer analyzes test questions and identifies an-
swer types of questions. The Document Re-
triever &amp; Answer Candidate Extractor retrieves
documents related to questions from the given
collection (Xinhua and Lianhe Zaobao newspa-
pers from 1998-2001 were used in this study) for
consideration, and segments the documents into
sentences as answer candidates. The Answer Ex-
traction module applies state-of-the-art IR for-
mulas (e.g., KL-divergence language model) to
directly estimate similarities between sentences
(1,024 sentences were used in our case) and
questions, and selects the most similar sentences
as the final answers. Given three answer candi-
dates, s1 = “Solutions to global warming range
from changing a light bulb to engineering giant
reflectors in space ...”, s2 = “Global warming
will bring bigger storms and hurricanes that will
hold more water ...”, and s3 = “nuclear power
is the relatively low emission of carbon diox-
ide (CO2), one of the major causes of global
warming,” to the question of “What are the haz-
ards of global warming?”, however, it is hard for
this architecture to select the correct answer, s2,
because the three candidates contain the same
question words “global warming”.
According to our observation, answers to a
type of question usually contain some type-
of-question dependent cue expressions (“will
bring” in this case). This paper argues that
the above QA system can be improved by us-
ing such question-type-specific cue expressions.
For each test question, we perform the follow-
ing three steps. (1) Collecting question-type-
specific Q&amp;A pairs from the social Q&amp;A collec-
tion which question types are same as the test
question to form positive training data. Sim-
ilarly, negative Q&amp;A pairs are also collected
which question types are different from the
test question. (2) Extracting and weighting
question-type-specific cue expressions from the
collected Q&amp;A pairs. (3) Building a question-
type-specific classifier by employing the cue ex-
pressions and the collected Q&amp;A pairs, which re-
moves noise sentences from answer candidates
before using the Answer Extraction module.
</bodyText>
<subsectionHeader confidence="0.999906">
3.1 Collecting Q&amp;A Pairs
</subsectionHeader>
<bodyText confidence="0.999089285714286">
We first introduce the notion of the answer type
informer of the question as follows. In a ques-
tion, a short subsequence of tokens (typically 1-
3 words) that are adequate for question classi-
fication is considered an answer-type informer,
e.g., “hazard” in the question of “What are the
hazards of global warming?” This paper makes
the following assumption: type of complex ques-
tion is determined by its answer type informer.
For example, the question of “What are the haz-
ards of global warming?” belongs to hazard-type
question, because its answer type informer is
“hazard”. Therefore, the task of recognizing
question-types is shifted to identifying answer
type informer of question.
In this paper, we regard answer-type informer
recognition as a sequence tagging problem and
adopt conditional random fields (CRFs) because
many work has shown that CRFs have a con-
sistent advantage in sequence tagging. We
manually label 3,262 questions with answer-
type informers to train a CRF, which classi-
fies each question word into a set of tags O =
{IB, II, IO}: IB for a word that begins an in-
former, II for a word that occurs in the mid-
dle of an informer, and IO for a word that
is outside of an informer. In the following
feature templates used in the CRF model, wn
and tn, refer to word and PoS, respectively;
n refers to the relative position from the cur-
rent word n=0. The feature templates in-
clude the following four types: unigrams of
wn and tn, where n=−2,−1,0,1,2; bigrams
of wnwn+1 and tntn+1, where n=−1,0; tri-
grams of wnwn+1wn+2 and tntn+1tn+2, where
n=−2, −1, 0; and bigrams of OnOn+1, where
n=−1, 0.
The trained CRF model is then employed to
recognize answer-type informers from questions
of social Q&amp;A pairs. Finally, we recognized 103
answer-type informers in which frequencies are
larger than 10,000. Moreover, the numbers of
answer type informers for which frequencies are
larger than 100, 1,000, and 5,000 are 2,714, 807,
and 194, respectively.
Based on answer-type informers of questions
recognized, we can collect training data for each
type of question as follows: (1) Q&amp;A pairs are
grouped together in cases in which the answer-
type informers X of their questions are the same,
and (2) Q&amp;A pairs clustered by informers X
are regarded as the positive training data of
X-type questions. For instance, 10,362 Q&amp;A
pairs grouped via informer X (=“hazard”) are
regarded as positive training data of answering
hazard-type questions. Table 1 lists some ques-
tions, which, together with their best answers,
are employed as the training data of the corre-
sponding type of questions. For each type of
question, we also randomly select some Q&amp;A
pairs that do not contain informers in questions
as negative training data. Preprocessing of the
training data, including word segmentation, PoS
tagging, and named entity (NE) tagging (Wu et
al., 2005), is conducted. We also replace each
NE with its tag type.
Qtype Questions of Q&amp;A pairs
Hazard- What are the hazards of the tro-
type jan.psw.misc.kah virus?
What are the hazards of RMB appreciation
on China’s economy?
Hazards of smoke
What are the hazards of contact lenses?
What are the hazards of waste accumula-
tion?
Casualty- What were the casualties on either side from
type the U.S.-Iraq war?
What were the casualties of the Sino-French
War?
What were the casualties of the Sichuan
earthquake in 2008?
What were the casualties of highway acci-
dents over the years?
What were the casualties of the Ryukyu Is-
lands tsunami?
Reason- What are the main reasons of China’s water
type shortage?
What are the reasons of asthma?
What are the reasons of blurred photos?
What are the reasons of air pollution?
The reasons for the soaring prices!
</bodyText>
<tableCaption confidence="0.840681">
Table 1: Questions (translated from Chinese) of
</tableCaption>
<bodyText confidence="0.8341135">
social Q&amp;A pairs (words in bold denote answer-
type informers of questions). These questions
and their best answers are regarded as positive
training data for hazard-type question.
</bodyText>
<subsectionHeader confidence="0.9997">
3.2 Cue Expressions
</subsectionHeader>
<bodyText confidence="0.999939277777778">
We extract lexical and PoS-based n-grams as cue
expressions from the collected training data. To
reduce the dimensionality of the cue expression
space, we first select the top 3,000 lexical un-
igrams using the formula: score,,, = tf,,, x
log(idf,,,), where tf(w) denotes the frequency of
word w, and idf(w) represents the inverted doc-
ument frequency of w that indicates its global
importance. Table 2 shows some of the learned
unigrams. The top 300 unigrams are then used as
seeds to learn lexical bigrams and trigrams iter-
atively. Only lexical bigrams and trigrams that
contain seed unigrams with frequencies larger
than the thresholds are retained as lexical fea-
tures. Moreover, we extract PoS-based unigrams
and bigrams as cue expressions.
Further, we assign each extracted feature si a
weight calculated using the equation weightsi =
</bodyText>
<equation confidence="0.973279">
csi
1 /(csi
1 + csi
2 ), where, csi 1and csi
</equation>
<bodyText confidence="0.904784333333333">
2 denote its fre-
quencies in positive and negative training Q&amp;A
pairs, respectively.
</bodyText>
<table confidence="0.959283285714286">
Qtype Top Unigrams
Hazard-type ˆ3/hazard -. —/lead to 1Ä/cause
Zå/give rise to —_t/bring about W
Ana/influence »3/damage
Casualty-type f� }/casualty �E }/death 7-- fVhurt
�kr/missing ±A/wrecked j}/die
in battle ‹Ú/wounded
</table>
<tableCaption confidence="0.949893">
Table 2: Top unigrams learned from hazard-type
and casualty-type Q&amp;A pairs
</tableCaption>
<subsectionHeader confidence="0.991729">
3.3 Classifiers
</subsectionHeader>
<bodyText confidence="0.997450789473684">
As mentioned above, we use the extracted cue
expressions and the collected Q&amp;A pairs to build
question-type-specific classifiers, which is used
to remove noise sentences from answer candi-
dates. For classifiers, we employ multivariate
classification SVMs (Thorsten Joachims, 2005)
that can directly optimize a large class of perfor-
mance measures like F1-Score, prec@k (preci-
sion of a classifier that predicts exactly k = 100
examples to be positive) and error-rate (percent-
age of errors in predictions). Instead of learn-
ing a univariate rule that predicts the label of a
single example in conventional SVMs (Vapnik,
1998), multivariate SVMs formulate the learn-
ing problem as a multivariate prediction of all
examples in the data set. Considering hypothe-
ses h that map a tuple x of n feature vectors
x = (x1, ..., xn) to a tuple y of n labels y =
(y1, ..., yn), multivariate SVMs learn a classifier
</bodyText>
<equation confidence="0.996471">
hw(x) = argmaxy′EY {wTp(x, y′)1 (1)
</equation>
<bodyText confidence="0.578921">
by solving the following optimization problem.
</bodyText>
<equation confidence="0.922505">
minw,ξ&gt;0 211w112 + Cξ 1 (2)
s.t.: by′ E Y \y : wT [p(x, y) − p(x, y′)]
&gt; A(y′, y) − ξ
(3)
</equation>
<bodyText confidence="0.7582208">
where, w is a parameter vector, p is a function
that returns a feature vector describing the match
between (x1, ..., xn) and (y′1,..., y′n), A denotes
types of multivariate loss functions, and ξ is a
slack variable.
</bodyText>
<sectionHeader confidence="0.999631" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999634">
The NTCIR 2008 test data set (Mitamura et al.,
2008) contains 30 complex questions4 we dis-
cussed here. However, a small number of test
questions are included for some question types,
e.g.; it contains only 1 hazard-type, 1 scale-type,
and 3 significance-type questions. To form a
more complete test set, we create another 65 test
questions5. Therefore, the test data used in this
paper includes 95 complex questions.
For each test question we also provide a list
of weighted nuggets, which are used as the gold
standard answers for evaluation. The evaluation
is conducted by employing Pourpre v1.0c (Lin
and Demner-Fushman, 2006), which uses the
standard scoring methodology for TREC other
questions (Voorhees, 2003), i.e., answer nugget
recall NR, nugget precision NP, and a combi-
nation score F3 of NR and NP. For better un-
derstanding, we evaluate the systems when out-
putting the top N sentences as answers.
</bodyText>
<footnote confidence="0.99779675">
4Because definitional, biography, and relationship ques-
tions in the NTCIR 2008 test set are not discussed here.
5The approach of creating test data is same as that in the
NTCIR 2008.
</footnote>
<table confidence="0.999539285714286">
F3 (%) NR (%) NP (%)
N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 N = 1 N = 5 N = 10
Baseline 9.82 18.18 21.95 9.44 19.85 27.64 34.35 25.32 18.96
TransM 9.76 20.47 24.76 9.44 19.85 33.10 31.96 21.73 13.57
Ourslin 10.92 22.61 25.74 10.49 25.95 34.70 34.98 23.40 15.11
Ourserrorrate 12.37 23.10 27.74 12.05 26.98 37.03 33.22 26.48 18.67
Ourspre@k 8.96 22.85 29.85 8.72 25.67 38.78 26.28 28.82 20.45
</table>
<tableCaption confidence="0.999798">
Table 3: Overall performance for the test data
</tableCaption>
<subsectionHeader confidence="0.994386">
4.1 Overall Results
</subsectionHeader>
<bodyText confidence="0.9999">
Table 3 summarizes the evaluation results for
several N values. The baseline refers to the con-
ventional method introduced in Section 3, which
does not employ question-type-specific classi-
fiers before the Answer Extraction. The baseline
can be expressed by the formula:
</bodyText>
<equation confidence="0.990041">
sim(q, s) = hVq · Vsi (4)
kVqk × kVsk
</equation>
<bodyText confidence="0.9982455">
where, Vq and Vs are the vectors of the ques-
tion and candidate answer. The TransM de-
notes a translation model for QA (Xue, et al.,
2008) (Bernhard et al., 2009), which uses Q&amp;A
pairs as the parallel corpus, with questions to the
“source” language and answers corresponding to
the “target” language. This model can be ex-
pressed by:
</bodyText>
<equation confidence="0.9081494">
P(q|S) = � ((1 − γ)Pmx(w|S) + γPml(w|C))
w∈q
Pmx(w|S) = (1 − ζ)Pml(w|S)+
P(w|t)Pml(t|S)
(5)
</equation>
<bodyText confidence="0.987293538461538">
where, q is the question, S the sentence, P(w|t)
the probability of translating a sentence term t to
the question term w, which is obtained by using
the GIZA++ toolkit (Och and Ney, 2003). We
use six million Q&amp;A pairs to train IBM model 1
for obtaining word-to-word probability P(w|t).
Ourserrorrate and Ourspre@k denote our models
that are based on classifiers optimizing perfor-
mance measure error-rate and prec@k, respec-
tively. Ourslin, a linear interpolation model, that
combines scores of classifiers and the baseline,
which is similar to (Mori et al., 2008) and can be
expressed by the equation:
</bodyText>
<equation confidence="0.822428">
sim(q, s)′ = sim(q, s) + α × φ(s) (6)
</equation>
<bodyText confidence="0.9675555">
where, φ(s) is the score calculated by classi-
fiers (Thorsten Joachims, 2005) and α denotes
the weight of the score.
This experiment shows that: (1) Question-
type-specific classifiers can greatly outperform
the baseline; for example, the F3 improvements
of Ourserrorrate and Ourspre@k over the base-
line in terms of N=10 are 5.8% and 7.9%,
respectively. (2) Ourserrorrate is better than
Ourspre@k when N &lt; 10. The average num-
bers of sentences retained in Ourserrorrate and
Ourspre@k are 130, and 217, respectively. That
means the precision of the classifier optimiz-
ing errorrate is superior to the classifier optimiz-
ing prec@k, while the recall is relatively infe-
rior. (3) Ourslin is worse than Ourserrorrate and
Ourspre@k, which indicates that using question-
type-specific classifiers by classification is better
than using it by interpolation like (Mori et al.,
2008). (4) Our models also outperform TransM,
e.g.; the F3 improvement is 5.1% when N is
set to 10. TransM exploits the social Q&amp;A col-
lection without consideration of question types,
while our models select and exploit the social
Q&amp;A pairs of the same question types. Thereby,
this experiment also indicates that it is better to
exploit social Q&amp;A pairs by type of question.
The performance ranking of these models when
N=10 is: Oursprec@k &gt; Ourserrorrate &gt; Ourslin
&gt; TransM &gt; Baseline.
</bodyText>
<subsectionHeader confidence="0.996111">
4.2 Impact of Features
</subsectionHeader>
<bodyText confidence="0.9999585">
In order to evaluate the contributions of indi-
vidual features to our models, this experiment
</bodyText>
<equation confidence="0.922307666666667">
1:
ζ
t∈S
</equation>
<bodyText confidence="0.999185875">
is conducted by gradually adding them. Table
4 summarizes the performance of Ourprec@k on
different set of features, L and P represent lex-
ical and PoS-based features, respectively. This
table demonstrates that all the lexical and PoS
features can positively impact Ourprec@k, espe-
cially, the contribution of the PoS-based features
is largest.
</bodyText>
<table confidence="0.999617">
Features F3 NR NP
Lunigram 23.44 31.23 17.32
+Lbigram +Ltrigram 25.34 33.15 18.87
+Punigram 28.24 36.27 20.18
+Pbigram 29.85 38.78 20.45
</table>
<tableCaption confidence="0.999636">
Table 4: Impact of features on Ourprec@k.
</tableCaption>
<subsectionHeader confidence="0.984059">
4.3 Improvement
</subsectionHeader>
<bodyText confidence="0.997979307692308">
As discussed in Section 2, the writing style of
social Q&amp;A collections slightly differs from that
of our complex QA system, which is an unfavor-
able circumstance in utilizing social Q&amp;A col-
lections. For better understanding we randomly
select 100 Q&amp;A training pairs of each type of
question acquired in Section 3, and manually
classify each Q&amp;A pair into NON-NOISE and
NOISE6 categories. Figure 1 reports the percent-
age of NON-NOISE. This figure indicates that
71% of the training pairs of the scale-type ques-
tions are noises, which may lead to a small im-
provement.
</bodyText>
<figureCaption confidence="0.9801815">
Figure 1: Percentage of NON-NOISE pairs by
type of questions.
</figureCaption>
<bodyText confidence="0.914491">
To further improve the performance, we em-
</bodyText>
<footnote confidence="0.978715">
6NOISE means that the Q&amp;A pair is not useful in our
study.
</footnote>
<bodyText confidence="0.999792291666667">
ploy k-fold cross validation to remove noises
from the collected training data in Section 3.1.
Specifically, the collected training data are first
divided into k (= 5) sets. Secondly, k-1 sets are
used to train classifiers that are applied to clas-
sify the Q&amp;A pairs in the remaining set. Finally,
part of the Q&amp;A pairs classified as negative pairs
are removed7. According to Figure 1, we re-
move 20% of the training data from the nega-
tive pairs for the hazard-type, impact-type, and
function-type questions, and 40% of the train-
ing data for significance-type, event-type, and
reason-type questions. Because the sizes of the
training pairs of the other four types of ques-
tions are small, we do not use this approach on
them. Table 5 shows the results of Ourspre@k on
the above six types of questions. The numbers
in brackets indicate absolute improvements over
the system based on the data without removing
noises. N is the number of answer sentences to a
question. The experiment shows that the perfor-
mance is generally improved by removing noise
in the training Q&amp;A pairs using k-fold cross-
validation.
</bodyText>
<table confidence="0.539448">
F3 (%) NR (%) NP (%)
N = 1 9.6+2.1 9.3+2.0 30.8+7.4
N = 5 21.6+0.7 24.9+1.2 26.0−1.3
N = 10 28.6+0.9 37.9+1.7 19.2−0.2
</table>
<tableCaption confidence="0.9243925">
Table 5: Performance of Ourspre@k after remov-
ing noises in the training Q&amp;A pairs.
</tableCaption>
<subsectionHeader confidence="0.999031">
4.4 Subjective evaluation
</subsectionHeader>
<bodyText confidence="0.9996442">
Pourpre v1.0c evaluation is based on n-gram
overlap between the automatically produced an-
swers and the human generated reference an-
swers. Thus, it is not able to measure concep-
tual equivalent. In subjective evaluation, the an-
swer sentences returned by systems are labeled
by a native Chinese assessor. Figure 2 shows the
distribution of the ranks of the first correct an-
swers for all questions. This figure demonstrates
that the Ourspre@k answers 57 questions which
</bodyText>
<footnote confidence="0.986177333333333">
7We do not remove all negative Q&amp;A pairs to ensure
the coverage of training data because the classifiers have
relatively lower recall, as mentioned in Section 3.3.
</footnote>
<figure confidence="0.987548421052632">
0.79
0.79
0.5 0.51
0.54 0.58
0.29
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0.87
0.86
0.85
</figure>
<bodyText confidence="0.998892">
first answers are ranked in top 3, which is larger
than that of the baseline, i.e., 49. Moreover,
the Ourspre@k contains only 11.5% of questions
which answers are ranked after top 10, while this
number of the baseline is 20.7%.
</bodyText>
<figureCaption confidence="0.961151">
Figure 2: Distribution of the ranks of first an-
swers.
</figureCaption>
<sectionHeader confidence="0.999774" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999898973684211">
Recently, some pioneering studies on the social
Q&amp;A collection have been conducted. Among
them, much of the research aims to retrieve an-
swers to queried questions from the social Q&amp;A
collection. For example, (Surdeanu et al., 2008)
proposed an answer ranking engine for non-
factoid questions by incorporating textual fea-
tures into a machine learning approach. (Duan
et al., 2008) proposed searching questions se-
mantically equivalent or close to the queried
question for a question recommendation sys-
tem. (Agichtein et al., 2008) investigated tech-
niques of finding high-quality content in the so-
cial Q&amp;A collection, and indicated that 94% of
answers to questions with high quality have high
quality. (Xue, et al., 2008) proposed a retrieval
model that combines a translation-based lan-
guage model for the question part with a query
likelihood approach for the answer part.
Another category of study regards the social
Q&amp;A collection as a kind of knowledge reposi-
tory and aims to mine knowledge from it for gen-
erating answers to questions. To the best of our
knowledge, there is very limited work reported
on this aspect. This paper is similar to (Mori et
al., 2008), but different from it as follows. (1)
(Mori et al., 2008) collects training data for each
test question using 7-grams for which centers are
interrogatives, while this paper collects training
data for each type of question using answer type
informers. (2) About the knowledge learned,
we extract lexical/class-based, PoS-based uni-
grams, bigrams, and trigrams. (Mori et al., 2008)
only extracts lexical bigrams. (3) They incor-
porated knowledge learned by interpolating with
the baseline. However, we utilize the learned
knowledge to train a binary classifier, which can
remove noise sentences before answer selection.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998505285714286">
This paper investigated a technique for mining
knowledge from social Q&amp;A websites for im-
proving a sentence-based complex QA system.
More specifically, it explored a social Q&amp;A col-
lection to automatically construct training data,
and created question-type-specific classifier for
each type of question to filter out noise sentences
before answer selection.
The experiments on 10 types of complex Chi-
nese questions show that the proposed approach
is effective; e.g., the improvement in F3 reaches
7.9%. In the future, we will endeavor to reduce
NOISE pairs in the training data, and to extract
type-of-question dependent features. Future re-
search tasks also include adapting the QA system
to a topic-based summarization system, which,
for example, summarizes accidents according to
“casualty”, “reason”, and summarizes events ac-
cording to “reason”, “measure,” “impact”, etc.
Appendix A. Examples of 10 Types of Ques-
tions.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995347454545454">
Abdessamad Echihabi and Daniel Marcu. 2003. A
Noisy-Channel Approach to Question Answering.
In Proc. ofACL 2003, Japan.
Delphine Bernhard and Iryna Gurevych. 2009. Com-
bining Lexical Semantic Resources with Question
&amp; Answer Archives for Translation-based Answer
Finding. In Proc. of ACL-IJCNLP 2009, Singa-
pore, pp728-736.
Ellen M. Voorhees. 2003 Overview of the TREC
2003 Question Answering Track. In Proc. of
TREC 2003, pp54-68, USA.
</reference>
<figure confidence="0.983844653846154">
30
26
35
Ourprec@k
Baseline
16 15
9 10
3 2 4
1 2 0
1 2 3 4 5 6 7 8 9 10
8
6
6
3
2
1
0
2
30
25
20
15
10
5
0
Qtype Examples
</figure>
<figureCaption confidence="0.951651166666667">
t 3/Hazard- ÚWhat
type are the hazards of global warming?
*~/Function- 0\){�~4��ÚWhat are the
type functions of the United Nations?
k Ana/Impact- 34J#911/�é)�){kidol-List
type the impact of the 911 attacks on the
United States.
?B/ 34J#+)hQAWTO{?B~
Significance- List the significance of China’s acces-
type sion to the WTO.
Õ Ý/Attitude- 34J#�-)éE1%��{ÕÝ-List
type the attitudes of other countries toward
the Israeli-Palestinian conflict.
D A/Measure- Q * ó I N 9 0 -t &amp;quot; ya T
type 0 s D fig ÚWhat measures have
4
been taken for energy-saving and
emissions-reduction in Japan?
Æ O/Reason- ÚWhat
type are the reasons for global warming?
f9! t/Casualty- 34J#�5d1t��9{f9!t-List the
type casualties of the Lockerbie Air Disas-
ter.
/ #/Event- 34J # iT-, � Z 2 Z 2 á � /
type List the events in the Northern
Ireland peace process.
A &amp;/Scale- ������-�É2��Ì{�
type &amp;-Give information about the scale
of the Kunming World Horticulture
Exposition.
</figureCaption>
<reference confidence="0.999774633802817">
Eugene Agichtein, Carlos Castillo, Debora Donato.
2008 Finding High-Quality Content in Social Me-
dia. In Proc. of WSDM 2008, California, USA.
Franz J. Och and Hermann Ney. 2003. A system-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics, 29(1):19-
51.
Gunes Erkan and Dragomir Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in
Text. In Journal of Artificial Intelligence
Research,22:457-479.
Hang Cui, Min Yen Kan, and Tat Seng Chua. 2004.
Unsupervised Learning of Soft Patterns for Defini-
tion Question Answering. In Proc. of WWW 2004.
Hoa Trang Dang. 2006. Overview of DUC 2006. In
Proc. of TREC 2006.
Huizhong Duan, Yunbo Cao, Chin Yew Lin, and
Yong Yu. 2008. Searching Questions by Identify-
ing Question Topic and Question Focus. In Proc.
ofACL 2008, Canada, pp 156-164.
Jimmy Lin and Dina Demner-Fushman. 2006. Will
Pyramids Built of Nuggets Topple Over. In Proc.
ofHLT/NAACL2006, pp 383-390.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to Rank Answers on
Large Online QA Collections. In Proc. of ACL
2008, Ohio, USA, pp 719-727.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based Question Answering for why-
Questions. In Proc. ofIJCNLP 2008, pp 418-425.
Tatsunori Mori, Takuya Okubo, and Madoka Ish-
ioroshi. 2008. A QA system that can answer any
class of Japanese non-factoid questions and its ap-
plication to CCLQA EN-JA task. In Proc. ofNT-
CIR2008, Tokyo, pp 41-48.
Sanda Harabagiu, Finley Lacatusu, Andrew Hickl.
2006. Answering Complex Questions with Ran-
dom Walk Models. In Proc. of the 29th SIGIR, pp
220-227, ACM.
Ves Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-Perspective Question Answering Us-
ing the OpQA Corpus. In Proc. of HLT/EMNLP
2005, Canada, pp 923-930.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin,
Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai,
Donghong Ji and Noriko Kando. 2008. Overview
of the NTCIR-7 ACLIA Tasks: Advanced Cross-
Lingual Information Access. In Proc. of NTCIR
2008.
Thorsten Joachims. 2005. A Support Vector Method
for Multivariate Performance Measures. In Proc.
ofICML2005, pp 383-390.
Vladimir Vapnik 1998. Statistical learning theory.
John Wiley.
Xiaobing Xue, Jiwoon Jeon, W.Bruce Croft. 2008.
Retrieval Models for Question and Answer
Archives. In Proc. of SIGIR 2008, pp 475-482.
Yutaka Sasaki. 2005. Question Answering as
Question-biased Term Extraction: A New Ap-
proach toward Multilingual QA. In Proc. of ACL
2005, pp 215-222.
Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu. 2005.
Chinese Named Entity Recognition Model based
on Multiple Features. In Proc. of HLT/EMNLP
2005, Canada, pp 427-434.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, Yong Yu. 2008. Understanding
and Summarizing Answers in Community-Based
Question Answering Services. In Proc. of COL-
ING 2008, Manchester, pp 497-504.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.725778">
<title confidence="0.999928">Exploiting Social Q&amp;A Collection in Answering Complex Questions</title>
<author confidence="0.992812">Youzheng Wu Hisashi Kawai</author>
<affiliation confidence="0.974666">Spoken Language Communication Group, MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.87928">2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288,</address>
<abstract confidence="0.991951666666667">This paper investigates techniques to automatically construct training data from social Q&amp;A collections such as Yahoo! Answer to support a machine learningcomplex QA We extract cue expressions for each type of question from collected training data and build question-type-specific classifiers to improve complex QA system. Experiments on 10 types of complex Chinese questions verify that it is effective to mine knowledge from social Q&amp;A collections for answering complex questions, for inthe of our system over the baseline and translationbased model reaches 7.9% and 5.1%, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdessamad Echihabi</author>
<author>Daniel Marcu</author>
</authors>
<title>A Noisy-Channel Approach to Question Answering.</title>
<date>2003</date>
<booktitle>In Proc. ofACL</booktitle>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>Abdessamad Echihabi and Daniel Marcu. 2003. A Noisy-Channel Approach to Question Answering. In Proc. ofACL 2003, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>Combining Lexical Semantic Resources with Question &amp; Answer Archives for Translation-based Answer Finding.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP 2009, Singapore,</booktitle>
<pages>728--736</pages>
<marker>Bernhard, Gurevych, 2009</marker>
<rawString>Delphine Bernhard and Iryna Gurevych. 2009. Combining Lexical Semantic Resources with Question &amp; Answer Archives for Translation-based Answer Finding. In Proc. of ACL-IJCNLP 2009, Singapore, pp728-736.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2003</date>
<booktitle>In Proc. of TREC</booktitle>
<pages>54--68</pages>
<location>USA.</location>
<contexts>
<context position="14712" citStr="Voorhees, 2003" startWordPosition="2357" endWordPosition="2358">. However, a small number of test questions are included for some question types, e.g.; it contains only 1 hazard-type, 1 scale-type, and 3 significance-type questions. To form a more complete test set, we create another 65 test questions5. Therefore, the test data used in this paper includes 95 complex questions. For each test question we also provide a list of weighted nuggets, which are used as the gold standard answers for evaluation. The evaluation is conducted by employing Pourpre v1.0c (Lin and Demner-Fushman, 2006), which uses the standard scoring methodology for TREC other questions (Voorhees, 2003), i.e., answer nugget recall NR, nugget precision NP, and a combination score F3 of NR and NP. For better understanding, we evaluate the systems when outputting the top N sentences as answers. 4Because definitional, biography, and relationship questions in the NTCIR 2008 test set are not discussed here. 5The approach of creating test data is same as that in the NTCIR 2008. F3 (%) NR (%) NP (%) N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 Baseline 9.82 18.18 21.95 9.44 19.85 27.64 34.35 25.32 18.96 TransM 9.76 20.47 24.76 9.44 19.85 33.10 31.96 21.73 13.57 Ourslin 10.92 22.61 25.74 </context>
</contexts>
<marker>Voorhees, 2003</marker>
<rawString>Ellen M. Voorhees. 2003 Overview of the TREC 2003 Question Answering Track. In Proc. of TREC 2003, pp54-68, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
</authors>
<title>Finding High-Quality Content in Social Media.</title>
<date>2008</date>
<booktitle>In Proc. of WSDM</booktitle>
<location>California, USA.</location>
<contexts>
<context position="22524" citStr="Agichtein et al., 2008" startWordPosition="3678" endWordPosition="3681"> of the baseline is 20.7%. Figure 2: Distribution of the ranks of first answers. 5 Related Work Recently, some pioneering studies on the social Q&amp;A collection have been conducted. Among them, much of the research aims to retrieve answers to queried questions from the social Q&amp;A collection. For example, (Surdeanu et al., 2008) proposed an answer ranking engine for nonfactoid questions by incorporating textual features into a machine learning approach. (Duan et al., 2008) proposed searching questions semantically equivalent or close to the queried question for a question recommendation system. (Agichtein et al., 2008) investigated techniques of finding high-quality content in the social Q&amp;A collection, and indicated that 94% of answers to questions with high quality have high quality. (Xue, et al., 2008) proposed a retrieval model that combines a translation-based language model for the question part with a query likelihood approach for the answer part. Another category of study regards the social Q&amp;A collection as a kind of knowledge repository and aims to mine knowledge from it for generating answers to questions. To the best of our knowledge, there is very limited work reported on this aspect. This pape</context>
</contexts>
<marker>Agichtein, Castillo, Donato, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato. 2008 Finding High-Quality Content in Social Media. In Proc. of WSDM 2008, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>29--1</pages>
<contexts>
<context position="16462" citStr="Och and Ney, 2003" startWordPosition="2675" endWordPosition="2678"> × kVsk where, Vq and Vs are the vectors of the question and candidate answer. The TransM denotes a translation model for QA (Xue, et al., 2008) (Bernhard et al., 2009), which uses Q&amp;A pairs as the parallel corpus, with questions to the “source” language and answers corresponding to the “target” language. This model can be expressed by: P(q|S) = � ((1 − γ)Pmx(w|S) + γPml(w|C)) w∈q Pmx(w|S) = (1 − ζ)Pml(w|S)+ P(w|t)Pml(t|S) (5) where, q is the question, S the sentence, P(w|t) the probability of translating a sentence term t to the question term w, which is obtained by using the GIZA++ toolkit (Och and Ney, 2003). We use six million Q&amp;A pairs to train IBM model 1 for obtaining word-to-word probability P(w|t). Ourserrorrate and Ourspre@k denote our models that are based on classifiers optimizing performance measure error-rate and prec@k, respectively. Ourslin, a linear interpolation model, that combines scores of classifiers and the baseline, which is similar to (Mori et al., 2008) and can be expressed by the equation: sim(q, s)′ = sim(q, s) + α × φ(s) (6) where, φ(s) is the score calculated by classifiers (Thorsten Joachims, 2005) and α denotes the weight of the score. This experiment shows that: (1) </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic Comparison of Various Statistical Alignment Models. In Computational Linguistics, 29(1):19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gunes Erkan</author>
<author>Dragomir Radev</author>
</authors>
<title>LexRank: Graph-based Lexical Centrality as Salience in Text.</title>
<date>2004</date>
<journal>In Journal of Artificial Intelligence Research,22:457-479.</journal>
<contexts>
<context position="3225" citStr="Erkan and Radev, 2004" startWordPosition="487" endWordPosition="490">ning data by utilizing social Q&amp;A collections crawled from the Web, which contains millions of user-generated Q&amp;A pairs. Many studies (Surdeanu et al., 2008) (Duan et al., 2008) have been done on retrieving similar Q&amp;A pairs from social QA websites as answers to test questions. Our study, however, regards social Q&amp;A websites as a knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents. There is very little literature on this aspect. Our work can be seen as a kind of query-based summarization (Dang, 2006) (Harabagiu et al., 2006) (Erkan and Radev, 2004), and can also be employed to answer questions that have not been answered in social Q&amp;A websites. This paper mainly focuses on the following three steps: (1) automatically constructing question - type-specific training Q&amp;A pairs from the social Q&amp;A collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using a state-of-the-art IR formula to select answers. We evaluate our system on 10 types of Chinese questions by using the Pourpre evaluation tool (Lin and Dem</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>Gunes Erkan and Dragomir Radev. 2004. LexRank: Graph-based Lexical Centrality as Salience in Text. In Journal of Artificial Intelligence Research,22:457-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Min Yen Kan</author>
<author>Tat Seng Chua</author>
</authors>
<title>Unsupervised Learning of Soft Patterns for Definition Question Answering.</title>
<date>2004</date>
<booktitle>In Proc. of WWW</booktitle>
<contexts>
<context position="1547" citStr="Cui et al., 2004" startWordPosition="223" endWordPosition="226">respectively. 1 Introduction Research on the topic of QA systems has mainly concentrated on answering factoid, definitional, reason and opinion questions. Among the approaches proposed to answer these questions, machine learning techniques have been found more effective in constructing QA components from scratch. Yet these supervised techniques require a certain scale of (question, answer), short for Q&amp;A, pairs as training data. For example, (Echihabi et al., 2003) and (Sasaki, 2005) constructed 90,000 English Q&amp;A pairs and 2,000 Japanese Q&amp;A pairs, respectively for their factoid QA systems. (Cui et al., 2004) constructed 1Complex questions cannot be answered by simply extracting named entities. In this paper complex questions do not include definitional questions. 76 term-definition pairs for their definitional QA systems. (Stoyanov et al., 2005) required a known subjective vocabulary for their opinion QA. (Higashinaka and Isozaki, 2008) used 4,849 positive and 521,177 negative examples in their reason QA system. Among complex QA systems, many other types of questions have not been well studied, apart from reason and definitional questions. Appendix A lists 10 types of complex Chinese questions an</context>
</contexts>
<marker>Cui, Kan, Chua, 2004</marker>
<rawString>Hang Cui, Min Yen Kan, and Tat Seng Chua. 2004. Unsupervised Learning of Soft Patterns for Definition Question Answering. In Proc. of WWW 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of DUC</title>
<date>2006</date>
<booktitle>In Proc. of TREC</booktitle>
<contexts>
<context position="3176" citStr="Dang, 2006" startWordPosition="481" endWordPosition="482">niques to automatically construct training data by utilizing social Q&amp;A collections crawled from the Web, which contains millions of user-generated Q&amp;A pairs. Many studies (Surdeanu et al., 2008) (Duan et al., 2008) have been done on retrieving similar Q&amp;A pairs from social QA websites as answers to test questions. Our study, however, regards social Q&amp;A websites as a knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents. There is very little literature on this aspect. Our work can be seen as a kind of query-based summarization (Dang, 2006) (Harabagiu et al., 2006) (Erkan and Radev, 2004), and can also be employed to answer questions that have not been answered in social Q&amp;A websites. This paper mainly focuses on the following three steps: (1) automatically constructing question - type-specific training Q&amp;A pairs from the social Q&amp;A collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using a state-of-the-art IR formula to select answers. We evaluate our system on 10 types of Chinese questions </context>
</contexts>
<marker>Dang, 2006</marker>
<rawString>Hoa Trang Dang. 2006. Overview of DUC 2006. In Proc. of TREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Yunbo Cao</author>
<author>Chin Yew Lin</author>
<author>Yong Yu</author>
</authors>
<title>Searching Questions by Identifying Question Topic and Question Focus.</title>
<date>2008</date>
<booktitle>In Proc. ofACL 2008, Canada,</booktitle>
<pages>156--164</pages>
<contexts>
<context position="2780" citStr="Duan et al., 2008" startWordPosition="414" endWordPosition="417">s we discussed in this paper. According to the related studies on QA, supervised machine-learning technique may be effective for answering these questions. To employ the supervised approach, we need to reconstruct training Q&amp;A pairs for each type of question, though this is an extremely expensive and labor-intensive task. To deal with the acquisition problem of training Q&amp;A pairs, we investigate techniques to automatically construct training data by utilizing social Q&amp;A collections crawled from the Web, which contains millions of user-generated Q&amp;A pairs. Many studies (Surdeanu et al., 2008) (Duan et al., 2008) have been done on retrieving similar Q&amp;A pairs from social QA websites as answers to test questions. Our study, however, regards social Q&amp;A websites as a knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents. There is very little literature on this aspect. Our work can be seen as a kind of query-based summarization (Dang, 2006) (Harabagiu et al., 2006) (Erkan and Radev, 2004), and can also be employed to answer questions that have not been answered in social Q&amp;A websites. This paper mainly focuses on the following three steps: </context>
<context position="22375" citStr="Duan et al., 2008" startWordPosition="3656" endWordPosition="3659">hat of the baseline, i.e., 49. Moreover, the Ourspre@k contains only 11.5% of questions which answers are ranked after top 10, while this number of the baseline is 20.7%. Figure 2: Distribution of the ranks of first answers. 5 Related Work Recently, some pioneering studies on the social Q&amp;A collection have been conducted. Among them, much of the research aims to retrieve answers to queried questions from the social Q&amp;A collection. For example, (Surdeanu et al., 2008) proposed an answer ranking engine for nonfactoid questions by incorporating textual features into a machine learning approach. (Duan et al., 2008) proposed searching questions semantically equivalent or close to the queried question for a question recommendation system. (Agichtein et al., 2008) investigated techniques of finding high-quality content in the social Q&amp;A collection, and indicated that 94% of answers to questions with high quality have high quality. (Xue, et al., 2008) proposed a retrieval model that combines a translation-based language model for the question part with a query likelihood approach for the answer part. Another category of study regards the social Q&amp;A collection as a kind of knowledge repository and aims to mi</context>
</contexts>
<marker>Duan, Cao, Lin, Yu, 2008</marker>
<rawString>Huizhong Duan, Yunbo Cao, Chin Yew Lin, and Yong Yu. 2008. Searching Questions by Identifying Question Topic and Question Focus. In Proc. ofACL 2008, Canada, pp 156-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimmy Lin</author>
<author>Dina Demner-Fushman</author>
</authors>
<title>Will Pyramids Built of Nuggets Topple Over.</title>
<date>2006</date>
<booktitle>In Proc. ofHLT/NAACL2006,</booktitle>
<pages>383--390</pages>
<contexts>
<context position="3843" citStr="Lin and Demner-Fushman, 2006" startWordPosition="587" endWordPosition="590">adev, 2004), and can also be employed to answer questions that have not been answered in social Q&amp;A websites. This paper mainly focuses on the following three steps: (1) automatically constructing question - type-specific training Q&amp;A pairs from the social Q&amp;A collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using a state-of-the-art IR formula to select answers. We evaluate our system on 10 types of Chinese questions by using the Pourpre evaluation tool (Lin and Demner-Fushman, 2006). The experimental results show the effectiveness of our system, for instance, the F3/NR improvement of our system over the baseline and translation-based model reaches 7.9%/11.1%, and 5.1%/5.6%, respectively. 2 Social Q&amp;A Collection Recently launched social QA websites such as Yahoo! Answer2 and Baidu Zhidao3 provide an interactive platform for users to post questions and answers. After questions are answered by users, the best answer can be chosen by the asker or nominated by the community. The number of Q&amp;A pairs on such sites has risen dramatically. These pairs could collectively form a so</context>
<context position="14625" citStr="Lin and Demner-Fushman, 2006" startWordPosition="2343" endWordPosition="2346">The NTCIR 2008 test data set (Mitamura et al., 2008) contains 30 complex questions4 we discussed here. However, a small number of test questions are included for some question types, e.g.; it contains only 1 hazard-type, 1 scale-type, and 3 significance-type questions. To form a more complete test set, we create another 65 test questions5. Therefore, the test data used in this paper includes 95 complex questions. For each test question we also provide a list of weighted nuggets, which are used as the gold standard answers for evaluation. The evaluation is conducted by employing Pourpre v1.0c (Lin and Demner-Fushman, 2006), which uses the standard scoring methodology for TREC other questions (Voorhees, 2003), i.e., answer nugget recall NR, nugget precision NP, and a combination score F3 of NR and NP. For better understanding, we evaluate the systems when outputting the top N sentences as answers. 4Because definitional, biography, and relationship questions in the NTCIR 2008 test set are not discussed here. 5The approach of creating test data is same as that in the NTCIR 2008. F3 (%) NR (%) NP (%) N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 N = 1 N = 5 N = 10 Baseline 9.82 18.18 21.95 9.44 19.85 27.64 34.35 25.32 18.9</context>
</contexts>
<marker>Lin, Demner-Fushman, 2006</marker>
<rawString>Jimmy Lin and Dina Demner-Fushman. 2006. Will Pyramids Built of Nuggets Topple Over. In Proc. ofHLT/NAACL2006, pp 383-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to Rank Answers on Large Online QA Collections.</title>
<date>2008</date>
<booktitle>In Proc. of ACL 2008,</booktitle>
<pages>719--727</pages>
<location>Ohio, USA,</location>
<contexts>
<context position="2760" citStr="Surdeanu et al., 2008" startWordPosition="410" endWordPosition="413">stions and their examples we discussed in this paper. According to the related studies on QA, supervised machine-learning technique may be effective for answering these questions. To employ the supervised approach, we need to reconstruct training Q&amp;A pairs for each type of question, though this is an extremely expensive and labor-intensive task. To deal with the acquisition problem of training Q&amp;A pairs, we investigate techniques to automatically construct training data by utilizing social Q&amp;A collections crawled from the Web, which contains millions of user-generated Q&amp;A pairs. Many studies (Surdeanu et al., 2008) (Duan et al., 2008) have been done on retrieving similar Q&amp;A pairs from social QA websites as answers to test questions. Our study, however, regards social Q&amp;A websites as a knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents. There is very little literature on this aspect. Our work can be seen as a kind of query-based summarization (Dang, 2006) (Harabagiu et al., 2006) (Erkan and Radev, 2004), and can also be employed to answer questions that have not been answered in social Q&amp;A websites. This paper mainly focuses on the fol</context>
<context position="22228" citStr="Surdeanu et al., 2008" startWordPosition="3633" endWordPosition="3636">3.3. 0.79 0.79 0.5 0.51 0.54 0.58 0.29 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0.87 0.86 0.85 first answers are ranked in top 3, which is larger than that of the baseline, i.e., 49. Moreover, the Ourspre@k contains only 11.5% of questions which answers are ranked after top 10, while this number of the baseline is 20.7%. Figure 2: Distribution of the ranks of first answers. 5 Related Work Recently, some pioneering studies on the social Q&amp;A collection have been conducted. Among them, much of the research aims to retrieve answers to queried questions from the social Q&amp;A collection. For example, (Surdeanu et al., 2008) proposed an answer ranking engine for nonfactoid questions by incorporating textual features into a machine learning approach. (Duan et al., 2008) proposed searching questions semantically equivalent or close to the queried question for a question recommendation system. (Agichtein et al., 2008) investigated techniques of finding high-quality content in the social Q&amp;A collection, and indicated that 94% of answers to questions with high quality have high quality. (Xue, et al., 2008) proposed a retrieval model that combines a translation-based language model for the question part with a query li</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2008. Learning to Rank Answers on Large Online QA Collections. In Proc. of ACL 2008, Ohio, USA, pp 719-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Hideki Isozaki</author>
</authors>
<title>Corpus-based Question Answering for whyQuestions.</title>
<date>2008</date>
<booktitle>In Proc. ofIJCNLP</booktitle>
<pages>418--425</pages>
<contexts>
<context position="1882" citStr="Higashinaka and Isozaki, 2008" startWordPosition="270" endWordPosition="273">se supervised techniques require a certain scale of (question, answer), short for Q&amp;A, pairs as training data. For example, (Echihabi et al., 2003) and (Sasaki, 2005) constructed 90,000 English Q&amp;A pairs and 2,000 Japanese Q&amp;A pairs, respectively for their factoid QA systems. (Cui et al., 2004) constructed 1Complex questions cannot be answered by simply extracting named entities. In this paper complex questions do not include definitional questions. 76 term-definition pairs for their definitional QA systems. (Stoyanov et al., 2005) required a known subjective vocabulary for their opinion QA. (Higashinaka and Isozaki, 2008) used 4,849 positive and 521,177 negative examples in their reason QA system. Among complex QA systems, many other types of questions have not been well studied, apart from reason and definitional questions. Appendix A lists 10 types of complex Chinese questions and their examples we discussed in this paper. According to the related studies on QA, supervised machine-learning technique may be effective for answering these questions. To employ the supervised approach, we need to reconstruct training Q&amp;A pairs for each type of question, though this is an extremely expensive and labor-intensive ta</context>
</contexts>
<marker>Higashinaka, Isozaki, 2008</marker>
<rawString>Ryuichiro Higashinaka and Hideki Isozaki. 2008. Corpus-based Question Answering for whyQuestions. In Proc. ofIJCNLP 2008, pp 418-425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsunori Mori</author>
<author>Takuya Okubo</author>
<author>Madoka Ishioroshi</author>
</authors>
<title>A QA system that can answer any class of Japanese non-factoid questions and its application to CCLQA EN-JA task.</title>
<date>2008</date>
<booktitle>In Proc. ofNTCIR2008,</booktitle>
<pages>41--48</pages>
<location>Tokyo,</location>
<contexts>
<context position="16837" citStr="Mori et al., 2008" startWordPosition="2732" endWordPosition="2735">|C)) w∈q Pmx(w|S) = (1 − ζ)Pml(w|S)+ P(w|t)Pml(t|S) (5) where, q is the question, S the sentence, P(w|t) the probability of translating a sentence term t to the question term w, which is obtained by using the GIZA++ toolkit (Och and Ney, 2003). We use six million Q&amp;A pairs to train IBM model 1 for obtaining word-to-word probability P(w|t). Ourserrorrate and Ourspre@k denote our models that are based on classifiers optimizing performance measure error-rate and prec@k, respectively. Ourslin, a linear interpolation model, that combines scores of classifiers and the baseline, which is similar to (Mori et al., 2008) and can be expressed by the equation: sim(q, s)′ = sim(q, s) + α × φ(s) (6) where, φ(s) is the score calculated by classifiers (Thorsten Joachims, 2005) and α denotes the weight of the score. This experiment shows that: (1) Questiontype-specific classifiers can greatly outperform the baseline; for example, the F3 improvements of Ourserrorrate and Ourspre@k over the baseline in terms of N=10 are 5.8% and 7.9%, respectively. (2) Ourserrorrate is better than Ourspre@k when N &lt; 10. The average numbers of sentences retained in Ourserrorrate and Ourspre@k are 130, and 217, respectively. That means </context>
<context position="23159" citStr="Mori et al., 2008" startWordPosition="3786" endWordPosition="3789">hniques of finding high-quality content in the social Q&amp;A collection, and indicated that 94% of answers to questions with high quality have high quality. (Xue, et al., 2008) proposed a retrieval model that combines a translation-based language model for the question part with a query likelihood approach for the answer part. Another category of study regards the social Q&amp;A collection as a kind of knowledge repository and aims to mine knowledge from it for generating answers to questions. To the best of our knowledge, there is very limited work reported on this aspect. This paper is similar to (Mori et al., 2008), but different from it as follows. (1) (Mori et al., 2008) collects training data for each test question using 7-grams for which centers are interrogatives, while this paper collects training data for each type of question using answer type informers. (2) About the knowledge learned, we extract lexical/class-based, PoS-based unigrams, bigrams, and trigrams. (Mori et al., 2008) only extracts lexical bigrams. (3) They incorporated knowledge learned by interpolating with the baseline. However, we utilize the learned knowledge to train a binary classifier, which can remove noise sentences before </context>
</contexts>
<marker>Mori, Okubo, Ishioroshi, 2008</marker>
<rawString>Tatsunori Mori, Takuya Okubo, and Madoka Ishioroshi. 2008. A QA system that can answer any class of Japanese non-factoid questions and its application to CCLQA EN-JA task. In Proc. ofNTCIR2008, Tokyo, pp 41-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
</authors>
<title>Answering Complex Questions with Random Walk Models.</title>
<date>2006</date>
<booktitle>In Proc. of the 29th SIGIR,</booktitle>
<pages>220--227</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3201" citStr="Harabagiu et al., 2006" startWordPosition="483" endWordPosition="486">omatically construct training data by utilizing social Q&amp;A collections crawled from the Web, which contains millions of user-generated Q&amp;A pairs. Many studies (Surdeanu et al., 2008) (Duan et al., 2008) have been done on retrieving similar Q&amp;A pairs from social QA websites as answers to test questions. Our study, however, regards social Q&amp;A websites as a knowledge repository and aims to mine knowledge from them for synthesizing answers to questions from multiple documents. There is very little literature on this aspect. Our work can be seen as a kind of query-based summarization (Dang, 2006) (Harabagiu et al., 2006) (Erkan and Radev, 2004), and can also be employed to answer questions that have not been answered in social Q&amp;A websites. This paper mainly focuses on the following three steps: (1) automatically constructing question - type-specific training Q&amp;A pairs from the social Q&amp;A collection; (2) extracting cue expressions for each type of question from the collected training data, and (3) building questiontype-specific classifiers to filer out noise sentences before using a state-of-the-art IR formula to select answers. We evaluate our system on 10 types of Chinese questions by using the Pourpre eval</context>
</contexts>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu, Finley Lacatusu, Andrew Hickl. 2006. Answering Complex Questions with Random Walk Models. In Proc. of the 29th SIGIR, pp 220-227, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ves Stoyanov</author>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multi-Perspective Question Answering Using the OpQA Corpus.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP 2005, Canada,</booktitle>
<pages>923--930</pages>
<contexts>
<context position="1789" citStr="Stoyanov et al., 2005" startWordPosition="257" endWordPosition="260">es have been found more effective in constructing QA components from scratch. Yet these supervised techniques require a certain scale of (question, answer), short for Q&amp;A, pairs as training data. For example, (Echihabi et al., 2003) and (Sasaki, 2005) constructed 90,000 English Q&amp;A pairs and 2,000 Japanese Q&amp;A pairs, respectively for their factoid QA systems. (Cui et al., 2004) constructed 1Complex questions cannot be answered by simply extracting named entities. In this paper complex questions do not include definitional questions. 76 term-definition pairs for their definitional QA systems. (Stoyanov et al., 2005) required a known subjective vocabulary for their opinion QA. (Higashinaka and Isozaki, 2008) used 4,849 positive and 521,177 negative examples in their reason QA system. Among complex QA systems, many other types of questions have not been well studied, apart from reason and definitional questions. Appendix A lists 10 types of complex Chinese questions and their examples we discussed in this paper. According to the related studies on QA, supervised machine-learning technique may be effective for answering these questions. To employ the supervised approach, we need to reconstruct training Q&amp;A </context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>Ves Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-Perspective Question Answering Using the OpQA Corpus. In Proc. of HLT/EMNLP 2005, Canada, pp 923-930.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
</authors>
<title>Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji and Noriko Kando.</title>
<date>2008</date>
<booktitle>In Proc. of NTCIR</booktitle>
<marker>Mitamura, 2008</marker>
<rawString>Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji and Noriko Kando. 2008. Overview of the NTCIR-7 ACLIA Tasks: Advanced CrossLingual Information Access. In Proc. of NTCIR 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>A Support Vector Method for Multivariate Performance Measures.</title>
<date>2005</date>
<booktitle>In Proc. ofICML2005,</booktitle>
<pages>383--390</pages>
<contexts>
<context position="12975" citStr="Joachims, 2005" startWordPosition="2057" endWordPosition="2058">A pairs, respectively. Qtype Top Unigrams Hazard-type ˆ3/hazard -. —/lead to 1Ä/cause Zå/give rise to —_t/bring about W Ana/influence »3/damage Casualty-type f� }/casualty �E }/death 7-- fVhurt �kr/missing ±A/wrecked j}/die in battle ‹Ú/wounded Table 2: Top unigrams learned from hazard-type and casualty-type Q&amp;A pairs 3.3 Classifiers As mentioned above, we use the extracted cue expressions and the collected Q&amp;A pairs to build question-type-specific classifiers, which is used to remove noise sentences from answer candidates. For classifiers, we employ multivariate classification SVMs (Thorsten Joachims, 2005) that can directly optimize a large class of performance measures like F1-Score, prec@k (precision of a classifier that predicts exactly k = 100 examples to be positive) and error-rate (percentage of errors in predictions). Instead of learning a univariate rule that predicts the label of a single example in conventional SVMs (Vapnik, 1998), multivariate SVMs formulate the learning problem as a multivariate prediction of all examples in the data set. Considering hypotheses h that map a tuple x of n feature vectors x = (x1, ..., xn) to a tuple y of n labels y = (y1, ..., yn), multivariate SVMs l</context>
<context position="16990" citStr="Joachims, 2005" startWordPosition="2763" endWordPosition="2764"> the question term w, which is obtained by using the GIZA++ toolkit (Och and Ney, 2003). We use six million Q&amp;A pairs to train IBM model 1 for obtaining word-to-word probability P(w|t). Ourserrorrate and Ourspre@k denote our models that are based on classifiers optimizing performance measure error-rate and prec@k, respectively. Ourslin, a linear interpolation model, that combines scores of classifiers and the baseline, which is similar to (Mori et al., 2008) and can be expressed by the equation: sim(q, s)′ = sim(q, s) + α × φ(s) (6) where, φ(s) is the score calculated by classifiers (Thorsten Joachims, 2005) and α denotes the weight of the score. This experiment shows that: (1) Questiontype-specific classifiers can greatly outperform the baseline; for example, the F3 improvements of Ourserrorrate and Ourspre@k over the baseline in terms of N=10 are 5.8% and 7.9%, respectively. (2) Ourserrorrate is better than Ourspre@k when N &lt; 10. The average numbers of sentences retained in Ourserrorrate and Ourspre@k are 130, and 217, respectively. That means the precision of the classifier optimizing errorrate is superior to the classifier optimizing prec@k, while the recall is relatively inferior. (3) Oursli</context>
</contexts>
<marker>Joachims, 2005</marker>
<rawString>Thorsten Joachims. 2005. A Support Vector Method for Multivariate Performance Measures. In Proc. ofICML2005, pp 383-390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical learning theory.</title>
<date>1998</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="13316" citStr="Vapnik, 1998" startWordPosition="2114" endWordPosition="2115">ioned above, we use the extracted cue expressions and the collected Q&amp;A pairs to build question-type-specific classifiers, which is used to remove noise sentences from answer candidates. For classifiers, we employ multivariate classification SVMs (Thorsten Joachims, 2005) that can directly optimize a large class of performance measures like F1-Score, prec@k (precision of a classifier that predicts exactly k = 100 examples to be positive) and error-rate (percentage of errors in predictions). Instead of learning a univariate rule that predicts the label of a single example in conventional SVMs (Vapnik, 1998), multivariate SVMs formulate the learning problem as a multivariate prediction of all examples in the data set. Considering hypotheses h that map a tuple x of n feature vectors x = (x1, ..., xn) to a tuple y of n labels y = (y1, ..., yn), multivariate SVMs learn a classifier hw(x) = argmaxy′EY {wTp(x, y′)1 (1) by solving the following optimization problem. minw,ξ&gt;0 211w112 + Cξ 1 (2) s.t.: by′ E Y \y : wT [p(x, y) − p(x, y′)] &gt; A(y′, y) − ξ (3) where, w is a parameter vector, p is a function that returns a feature vector describing the match between (x1, ..., xn) and (y′1,..., y′n), A denotes</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik 1998. Statistical learning theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaobing Xue</author>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
</authors>
<title>Retrieval Models for Question and Answer Archives.</title>
<date>2008</date>
<booktitle>In Proc. of SIGIR</booktitle>
<pages>475--482</pages>
<contexts>
<context position="15988" citStr="Xue, et al., 2008" startWordPosition="2593" endWordPosition="2596">.10 27.74 12.05 26.98 37.03 33.22 26.48 18.67 Ourspre@k 8.96 22.85 29.85 8.72 25.67 38.78 26.28 28.82 20.45 Table 3: Overall performance for the test data 4.1 Overall Results Table 3 summarizes the evaluation results for several N values. The baseline refers to the conventional method introduced in Section 3, which does not employ question-type-specific classifiers before the Answer Extraction. The baseline can be expressed by the formula: sim(q, s) = hVq · Vsi (4) kVqk × kVsk where, Vq and Vs are the vectors of the question and candidate answer. The TransM denotes a translation model for QA (Xue, et al., 2008) (Bernhard et al., 2009), which uses Q&amp;A pairs as the parallel corpus, with questions to the “source” language and answers corresponding to the “target” language. This model can be expressed by: P(q|S) = � ((1 − γ)Pmx(w|S) + γPml(w|C)) w∈q Pmx(w|S) = (1 − ζ)Pml(w|S)+ P(w|t)Pml(t|S) (5) where, q is the question, S the sentence, P(w|t) the probability of translating a sentence term t to the question term w, which is obtained by using the GIZA++ toolkit (Och and Ney, 2003). We use six million Q&amp;A pairs to train IBM model 1 for obtaining word-to-word probability P(w|t). Ourserrorrate and Ourspre@k</context>
<context position="22714" citStr="Xue, et al., 2008" startWordPosition="3709" endWordPosition="3712">h of the research aims to retrieve answers to queried questions from the social Q&amp;A collection. For example, (Surdeanu et al., 2008) proposed an answer ranking engine for nonfactoid questions by incorporating textual features into a machine learning approach. (Duan et al., 2008) proposed searching questions semantically equivalent or close to the queried question for a question recommendation system. (Agichtein et al., 2008) investigated techniques of finding high-quality content in the social Q&amp;A collection, and indicated that 94% of answers to questions with high quality have high quality. (Xue, et al., 2008) proposed a retrieval model that combines a translation-based language model for the question part with a query likelihood approach for the answer part. Another category of study regards the social Q&amp;A collection as a kind of knowledge repository and aims to mine knowledge from it for generating answers to questions. To the best of our knowledge, there is very limited work reported on this aspect. This paper is similar to (Mori et al., 2008), but different from it as follows. (1) (Mori et al., 2008) collects training data for each test question using 7-grams for which centers are interrogative</context>
</contexts>
<marker>Xue, Jeon, Croft, 2008</marker>
<rawString>Xiaobing Xue, Jiwoon Jeon, W.Bruce Croft. 2008. Retrieval Models for Question and Answer Archives. In Proc. of SIGIR 2008, pp 475-482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yutaka Sasaki</author>
</authors>
<title>Question Answering as Question-biased Term Extraction: A New Approach toward Multilingual QA.</title>
<date>2005</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>215--222</pages>
<contexts>
<context position="1418" citStr="Sasaki, 2005" startWordPosition="203" endWordPosition="204">uestions, for instance, the F3 improvement of our system over the baseline and translationbased model reaches 7.9% and 5.1%, respectively. 1 Introduction Research on the topic of QA systems has mainly concentrated on answering factoid, definitional, reason and opinion questions. Among the approaches proposed to answer these questions, machine learning techniques have been found more effective in constructing QA components from scratch. Yet these supervised techniques require a certain scale of (question, answer), short for Q&amp;A, pairs as training data. For example, (Echihabi et al., 2003) and (Sasaki, 2005) constructed 90,000 English Q&amp;A pairs and 2,000 Japanese Q&amp;A pairs, respectively for their factoid QA systems. (Cui et al., 2004) constructed 1Complex questions cannot be answered by simply extracting named entities. In this paper complex questions do not include definitional questions. 76 term-definition pairs for their definitional QA systems. (Stoyanov et al., 2005) required a known subjective vocabulary for their opinion QA. (Higashinaka and Isozaki, 2008) used 4,849 positive and 521,177 negative examples in their reason QA system. Among complex QA systems, many other types of questions ha</context>
</contexts>
<marker>Sasaki, 2005</marker>
<rawString>Yutaka Sasaki. 2005. Question Answering as Question-biased Term Extraction: A New Approach toward Multilingual QA. In Proc. of ACL 2005, pp 215-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youzheng Wu</author>
<author>Jun Zhao</author>
<author>Bo Xu</author>
<author>Hao Yu</author>
</authors>
<title>Chinese Named Entity Recognition Model based on Multiple Features.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP 2005, Canada,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="10333" citStr="Wu et al., 2005" startWordPosition="1633" endWordPosition="1636">rs X are regarded as the positive training data of X-type questions. For instance, 10,362 Q&amp;A pairs grouped via informer X (=“hazard”) are regarded as positive training data of answering hazard-type questions. Table 1 lists some questions, which, together with their best answers, are employed as the training data of the corresponding type of questions. For each type of question, we also randomly select some Q&amp;A pairs that do not contain informers in questions as negative training data. Preprocessing of the training data, including word segmentation, PoS tagging, and named entity (NE) tagging (Wu et al., 2005), is conducted. We also replace each NE with its tag type. Qtype Questions of Q&amp;A pairs Hazard- What are the hazards of the trotype jan.psw.misc.kah virus? What are the hazards of RMB appreciation on China’s economy? Hazards of smoke What are the hazards of contact lenses? What are the hazards of waste accumulation? Casualty- What were the casualties on either side from type the U.S.-Iraq war? What were the casualties of the Sino-French War? What were the casualties of the Sichuan earthquake in 2008? What were the casualties of highway accidents over the years? What were the casualties of the </context>
</contexts>
<marker>Wu, Zhao, Xu, Yu, 2005</marker>
<rawString>Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu. 2005. Chinese Named Entity Recognition Model based on Multiple Features. In Proc. of HLT/EMNLP 2005, Canada, pp 427-434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanjie Liu</author>
<author>Shasha Li</author>
<author>Yunbo Cao</author>
<author>Chin-Yew Lin</author>
<author>Dingyi Han</author>
<author>Yong Yu</author>
</authors>
<title>Understanding and Summarizing Answers in Community-Based Question Answering Services.</title>
<date>2008</date>
<booktitle>In Proc. of COLING 2008,</booktitle>
<pages>497--504</pages>
<location>Manchester,</location>
<marker>Liu, Li, Cao, Lin, Han, Yu, 2008</marker>
<rawString>Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, Yong Yu. 2008. Understanding and Summarizing Answers in Community-Based Question Answering Services. In Proc. of COLING 2008, Manchester, pp 497-504.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>