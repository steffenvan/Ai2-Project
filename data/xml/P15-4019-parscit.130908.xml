<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005664">
<title confidence="0.987549">
End-to-end Argument Generation System in Debating
</title>
<author confidence="0.856612">
Misa Sato Kohsuke Yanai Toshihiko Yanase
Toshinori Miyoshi Makoto Iwayama Qinghua Sun Yoshiki Niwa
</author>
<affiliation confidence="0.944964">
Hitachi Ltd. Research &amp; Development Group
</affiliation>
<address confidence="0.733924333333333">
1-280, Higashi-koigakubo, Kokubunji-shi, Tokyo 185-8601 Japan
{misa.sato.mw, kohsuke.yanai.cs, toshihiko.yanase.gm,
toshinori.miyoshi.pd, makoto.iwayama.nw,
</address>
<email confidence="0.974944">
qinghua.sun.ap, yoshiki.niwa.tx}@hitachi.com
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999738722222222">
We introduce an argument generation sys-
tem in debating, one that is based on sen-
tence retrieval. Users can specify a motion
such as This house should ban gambling,
and a stance on whether the system agrees
or disagrees with the motion. Then the
system outputs three argument paragraphs
based on “values” automatically decided
by the system. The “value” indicates a
topic that is considered as a positive or
negative for people or communities, such
as health and education. Each paragraph
is related to one value and composed of
about seven sentences. An evaluation over
50 motions from a popular debate web-
site showed that the generated arguments
are understandable in 64 paragraphs out of
150.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959">
This paper describes our end-to-end argument
generation system, developed to participate in En-
glish debating games as an AI debater. When users
give a “motion” like This house should ban gam-
bling and a “stance” on whether the system should
agree or disagree with the motion, the system gen-
erates argument scripts in the first constructive
round of a debate.
Among NLP communities, interest is growing
in argumentation, such as argumentation mining
and claim detection (Levy et al., 2014; Mizuno et
al., 2012; Park et al., 2014). However, argument
generation is still as hard a task as other text gener-
ation tasks; no standard methods or systems exist,
as far as we know.
We assume that argument generation systems
are helpful in a variety of decision-making situ-
ations such as business, law, politics and medical
care. This is because people usually investigate
existing arguments on the Internet, newspapers, or
research papers before reaching conclusions. In
this research, we focus on debating game style
because there is similarity in argument construc-
tion between debating games and actual decision-
making.
The difficulty in argument generation is that ar-
gument scripts have to be persuasive. We ex-
plain this need by comparing argument generation
with multi-document summarization. In the two
tasks, one practical approach is combining partial
texts retrieved from multiple documents. Gener-
ated scripts in both tasks should be natural and
have sufficient content. Because the summariza-
tion task is to generate summary scripts of multi-
ple documents, the essential basis of its evaluation
is coverage, that is, how much content in the origi-
nal documents is included in the generated scripts.
However, as the role of argument scripts is to per-
suade people, persuasiveness is more important
than coverage.
We believe that the following three points are
required to generate persuasive argument scripts:
</bodyText>
<listItem confidence="0.999066666666667">
1. Consistency with a given stance
2. Cause and effect relationships
3. Relevance to people’s values
</listItem>
<bodyText confidence="0.9994055">
For example, when debaters focus on an agree
stance with a motion of This house should ban
gambling, one persuasive argument would discuss
the negative effects of gambling. To reach a dis-
cussion about the negative effects under this con-
dition, we need to consider the three points.
</bodyText>
<listItem confidence="0.993644444444444">
1. Consistency means that the stance of argu-
ment scripts must be equal to the given stance and
consistent in the overall arguments. For example,
because the gambling motion implies the claim
that gambling is negative, the generated argument
should include only negative aspects of gambling.
2. Causality makes argumentation persua-
sive. To capture causality, we focus on pro-
moting/suppressing relationships. Hashimoto et
</listItem>
<page confidence="0.984953">
109
</page>
<note confidence="0.839225">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 109–114,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<figureCaption confidence="0.999896">
Figure 1: Screenshot and Sample Input &amp; Output Script
</figureCaption>
<bodyText confidence="0.969337857142857">
al. (2012) also showed that the relationships are
useful for causality extractions. The claim gam-
bling promotes negative issues would be persua-
sive in an argumentation that agrees with a ban on
gambling.
3. Values There are topics obviously considered
to be positive or negative and highly relevant to
people’s values. For instance, health, education
and natural environment are considered to be pos-
itive values, while crime, pollution and high cost
are considered to be negative. It is possible to gen-
erate scripts about negative effects by collecting
partial texts describing negative values linked to
gambling, such as crime.
</bodyText>
<sectionHeader confidence="0.998469" genericHeader="introduction">
2 Overview
</sectionHeader>
<subsectionHeader confidence="0.998302">
2.1 Demo Description
</subsectionHeader>
<bodyText confidence="0.999971956521739">
Visitors will have the opportunity to select a mo-
tion and a stance and to run the system to generate
argument scripts automatically.
Each argument script generated by the system
consists of three topics corresponding to values,
such as health, education and revenue. This ap-
proach comes from our observations that persua-
sive arguments would be related to multiple val-
ues. Figure 1 shows the interface of the system and
an example of generated argument scripts. First,
users give text scripts about the “motion” and se-
lect the “stance” whether agree or disagree. In
the figure, the given motion is This house should
ban smoking in public spaces, and the given stance
is an agree side. When users click the start but-
ton, the system begins processing. Users can see
how many sentences or documents are processed
and how many sentences belong to each value in
the graphs in the upper right corner. Finally, the
system provides three generated paragraphs with
their value titles such as poverty, pollution, and
disease while the generated argument scripts are
read aloud by our text-to-speech system.
</bodyText>
<subsectionHeader confidence="0.999613">
2.2 System Overview
</subsectionHeader>
<bodyText confidence="0.999105066666667">
Figure 2 shows the overview of the system.
As discussed above, the key of constructing ar-
guments is to find positive/negative effects of a tar-
get in the motion. In this paper, we call the target
“a motion keyphrase”.
Positive/negative effects appear in the form of
affect relationships like something affects some-
thing. Main elements of arguments are sentences
that contain affect relationships whose subject is
a motion keyphrase and whose object represents a
value.
We have two types of affect predicates: affect+
and affect−. Affect+ means a promoting predi-
cate such as create, enhance, expand, improve, in-
crease. On the other hand, affect− means a sup-
</bodyText>
<page confidence="0.995873">
110
</page>
<figureCaption confidence="0.999767">
Figure 2: System Overview
</figureCaption>
<bodyText confidence="0.998070166666667">
pressing predicate such as decrease, discourage,
eliminate, limit, threaten. The system stored the
affect relationships in text data as automatically
added annotations described in Section 4.
Though the system consists of 21 algorithms,
we describe four main components in this paper:
</bodyText>
<listItem confidence="0.985824">
(1) A motion analysis component decides a
motion keyphrase and a polarity of arguments
to be generated.
(2) A value selection component decides mul-
tiple values relevant to the given motion by
retrieving sentences that contain affect rela-
tionships.
(3) A sentence retrieval component retrieves
sentences relevant to each value from the
stored text data.
(4) A sentence ordering and rephrasing
component combines and arranges the re-
trieved sentences to construct natural argu-
ment scripts.
</listItem>
<bodyText confidence="0.9848404">
They are processed in a pipeline and some of the
algorithms are processed in parallel on a cluster of
10 machines. We describe key functions of the
components in Section 3.
The system uses large text data from Gigaword
corpus (Napoles et al., 2012) and the annotations
to the text data. The annotations are added auto-
matically in a preprocessing step. Section 4 de-
scribes what kinds of annotations exploited in the
system. The text and annotation data are stored us-
ing Cassandra1, which is an open-source database
management system designed for handling large
amounts of data across commodity servers. They
are indexed into Solr2, open source enterprise
search platform, that enables full-text search.
</bodyText>
<subsectionHeader confidence="0.991023">
2.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.9993495">
We evaluated the generated argument scripts on
the basis of subjective evaluations.
</bodyText>
<footnote confidence="0.9999545">
1Cassandra: http://cassandra.apache.org
2Apache Solr: http://lucene.apache.org/solr
</footnote>
<tableCaption confidence="0.6803395">
Table 1: Evaluation Results
Evaluation Score Num of paragraphs
</tableCaption>
<listItem confidence="0.7784788">
0: make-no-sense 86
1: understandable 38
2: +clear 16
3: +persuasive 10
4: +natural 0
</listItem>
<bodyText confidence="0.999871590909091">
We used 50 motions from a popular debate web-
site Debatabase 3 as inputs to the system. The
system outputs three paragraphs per motion, and
each paragraph is composed of seven sentences,
totaling 150 paragraphs for 50 motions. The para-
graphs are rated by authors on a five point scale
ranging from 0 to 4. Each evaluator judges 30
paragraphs in 10 motions. The paragraphs that do
not include any claims or supporting evidence re-
lated to the motion, are given a rating of 0 points.
A 1 point rating is given when the argument is un-
derstood by the evaluator, despite a number of ir-
relevant sentences. If more than four of the seven
sentences in the paragraph are relevant to the given
motion and consistent to the stance, it is given a 2
point rating. If the evaluator feels it is persuasive,
it is given a 3 point rating. When it satisfies the
above conditions and is presented as a natural ar-
gument, it is given a 4 point rating.
Table 1 shows the results. We found that the ar-
gumentations are understandable in 64 paragraphs
(= 38+16+10+0) out of 150.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="method">
3 Pipeline Components
</sectionHeader>
<subsectionHeader confidence="0.999671">
3.1 Motion Analysis Component
</subsectionHeader>
<bodyText confidence="0.9998918">
In the beginning of processing, the system an-
alyzes the given motion text, and extracts a
keyphrase, a motion polarity, a predicate, an at-
titude, and contexts. A predicate is a phrase which
gives positive/negative sign to a keyphrase, and an
</bodyText>
<footnote confidence="0.967682">
3Debatabase: http://idebate.org/debatabase
</footnote>
<page confidence="0.9975">
111
</page>
<tableCaption confidence="0.994099">
Table 2: Motion Analysis Results
</tableCaption>
<table confidence="0.9311592">
motion keyphrase pol. predicate attitude contexts
This house believes that casino is harmful for the city casino −1 harmful believe the city
This house would create a single EU army a single EU army +1 – create –
This house should ban gambling gambling −1 – ban –
This house believes that assisted suicide should be legalized assisted suicide +1 – legalize –
</table>
<tableCaption confidence="0.994788">
Table 3: Motion Analysis Rules. K = motion keyphrase, C = contexts.
</tableCaption>
<table confidence="0.9142285">
priority rule predicate instances
1 K be modify-ed for C modify: good(+1), honor(+1), popular(+1), harmful(−1), negative(−1), weak(−1)
2 affect K affect: create(+1), enhance(+1), increase(+1), cut(−1), discourage(−1), eliminate(−1)
3 believe K believe: allow(+1), legalize(+1), permit(+1), support(+1), ban(−1), oppose(−1)
4 K be believe-ed believe: allow(+1), legalize(+1), permit(+1), support(+1), ban(−1), oppose(−1)
. . . . . . . . .
</table>
<bodyText confidence="0.994851346153846">
attitude is a predicate of this house. Table 2 shows
results of motion analysis.
To analyze a motion, the system has 22 rules
with their priority. Table 3 shows a part of the
rules. The rules are applied in the order by their
priority, until a motion keyphrase is extracted.
Suppose that the given motion is This house be-
lieves that casino is harmful for the city and the
given stance is agree(+1) (corresponding to the
first line of Table 2). The first rule “K be modify-
ed for C” in Table 3 matches the motion. As
harmful is a modifying predicate, casinos is K and
the city is C. An attitude of this house is believe.
A motion polarity is −1 because of the negative
predicate harmful(−1). In the same way, from the
second to the fourth rules in Table 3 can analyze
the other three motion examples in Table 2.
The system calculates an argument polarity by
multiplying the sign of the given stance and the
motion polarity. The system constructs arguments
that discuss the motion keyphrase, in accordance
with the argument polarity. For example, if the
given stance is agree(+1) and the motion polarity
is negative(−1), then the system decides an argu-
ment polarity is −1 and constructs arguments that
claim “the motion keyphrase is negative(−1)”.
</bodyText>
<subsectionHeader confidence="0.99682">
3.2 Value Selection Component
</subsectionHeader>
<bodyText confidence="0.999966108695652">
The value selection component decides multiple
values relevant to the given motion by using a
value dictionary. The value dictionary formulates
a set of values that represents what is important
in human’s life, what is harmful in communities,
etc. Each value is regarded as a viewpoint of the
generated argument.
Table 4 describes an example of the value dic-
tionary. As shown in Table 4, each value (e.g., dis-
ease, investment) belongs to a field (e.g., health,
economy, respectively), and has three attributes: a
value polarity, representative phrases, and context
phrases. The value polarity +1(−1) means that
the value is something good(bad). The represen-
tative phrases are linguistic expressions of the val-
ues, and the numbers are their weights calculated
by IDF in Gigaword corpus. The context phrases
are phrases that are likely to appear in neighbor
of the value in text documents. They are used to
solve ambiguity of the linguistic expressions. The
value dictionary of the current system contains 16
fields and 61 values.
The procedure of the value selection is below:
Step 1 Retrieves sentences that contain affect re-
lationships between the motion keyphrase
and one of representative phrases in the value
dictionary. For instance, it retrieves Gam-
bling increases the number of crimes.
Step 2 Calculates a polarity to the keyphrase in
each sentence, and filters out the sentences
where the polarity is not equivalent to the ar-
gument polarity. For instance, the polarity for
Gambling increases the number of crimes is
−1 by multiplying +1 (increase in the affect
dictionary) and −1 (crime in the value dictio-
nary), which equals to the argument polarity.
Step 3 Sums weights of found values and selects
the top five values.
The value dictionary was created manually.
First, fields of the dictionary were determined by
the authors in reference to the roles of govern-
ment agencies, and then value entries related to
each field were chosen manually from Debatabase.
Second, a rule-based extractor that extracts values
discussed in a document was constructed using the
dictionary, and the extractor applied to each docu-
</bodyText>
<page confidence="0.999082">
112
</page>
<tableCaption confidence="0.996311">
Table 4: Value Dictionary
</tableCaption>
<figure confidence="0.666850357142857">
field value
polarity representative phrases context phrases
economy investment
finance cost
finance income
health disease
safety crime
... ...
+1 investment:27.8, development aid:48.2 asset, bank, capital, fund, profit, stock, ...
−1 expense:35.9, expenditure:55.7 budget, dollar, fuel, lower, price, share,...
+1 revenue:35.4, wage:39.8 budget, company, earnings, higher, gain, ...
−1 disease:36.6, complication:40.1 AIDS, Alzheimer, blood, cancer, death, ...
−1 crime:31.5, prostitution:56.2 arrest, gun, jail, kidnapping, victim, ...
... ... ...
</figure>
<bodyText confidence="0.997461333333333">
ment in Debatabase. Third, we manually added
new entries to the dictionary. If a value is ex-
tracted from a document, we extracted represen-
tative/context phrases corresponding to the value
from the document. If no value is extracted, we
extracted new values that were contained in the
document. We continued these steps of classify-
ing documents and adding entries to the dictionary
like a Bootstrapping method.
</bodyText>
<subsectionHeader confidence="0.99922">
3.3 Sentence Retrieval Component
</subsectionHeader>
<bodyText confidence="0.999912823529412">
This component retrieves sentences relevant to
each value from the stored text data.
It first retrieves documents using a query com-
posed of weighted phrases. The retrieved docu-
ments should contain both the motion keyphrase
and more than one representative phrases of the
decided values. While the motion keyphrases
can be replaced with their synonyms or hy-
ponyms, their weights are smaller than the orig-
inal keyphrases; those of synonyms are 0.75 and
those of hyponyms are 0.5. The synonyms and hy-
ponyms are acquired by WordNet (Miller, 1995).
Because short documents don’t usually contains
informative scripts, the length of retrieved docu-
ments are limited to more than 200 words.
For example, when the motion keyphrase is
gambling, a search query for a health value is
</bodyText>
<equation confidence="0.9226776">
(gambling#49.53 OR gaming#22.87)
AND (health#27.48 OR disease#36.60
OR addiction#52.39
OR hospital#29.76)
AND (length:[200 TO *]).
</equation>
<bodyText confidence="0.999981625">
The real numbers following sharp signs are
weights of the former phrases, calculated by mul-
tiplying the IDF of the phrase and a synonym or
hyponym reduction rate.
The retrieval step prefers sentences that contain
promote/suppress relationships. The polarities of
the retrieved sentences must be equal to the argu-
ment polarity. The polarity of each sentence is
calculated by the product of the signs of related
phrases, such as the predicate of the keyphrase,
the promote/suppress verb, and the representative
value phrase. In the example of gambling ban(−1)
decrease(−1) the number of crimes(−1), the po-
larity of the sentence is −1.
The system uses about 10,000,000 newswire
documents and retrieves 500 per value in this step.
</bodyText>
<subsectionHeader confidence="0.9949955">
3.4 Sentence Ordering and Rephrasing
Component
</subsectionHeader>
<bodyText confidence="0.999980807692307">
This component processes the sentence set of each
value separately.
The sentence ordering step orders the retrieved
sentences in the natural order in debating by the
method reported in (Yanase et al., 2015). The
method employs an assumpsion that a constructive
speech item in debating consists of a claim sen-
tence and one or more supporting sentences, and
that the claim sentence lies in the first position of
the paragraph. The assumption is implemented as
two machine learning problems: claim sentence
selection and supporting sentence ordering. The
claim sentence selection problem is formulated as
a binary-classification problem where a given sen-
tence is a claim or not. In the supporting sentence
ordering problem, the method orders the other sen-
tences on the basis of connectivity of pairs of sen-
tences. This problem is formulated as a ranking
problem, similarly to (Tan et al., 2013). Features
for machine learning models in both problems are
extracted not only from the sentences to be ordered
but also from the motion text.
Finally, the rephrasing step trims or replaces
surplus phrases referring to too many details
for argument scripts, such as dates and people’s
names. Several simple rules are used.
</bodyText>
<sectionHeader confidence="0.998664" genericHeader="method">
4 Data Preprocessing: Annotations
</sectionHeader>
<bodyText confidence="0.9996202">
The system adds annotations automatically in pre-
processing into the stored text data by using dic-
tionaries and syntax information by Stanford Core
NLP (Manning et al., 2014). In the current system,
about 250 million annotations are stored. Users
</bodyText>
<page confidence="0.997407">
113
</page>
<bodyText confidence="0.999940333333334">
can add the annotations manually. A list of main
semantic annotations is below:
affect: promoting/suppressing relationships.
For example, it adds an annotation of “affect+:
casino —* the number of crimes” into a text
casino increases the number of crimes. The affect
dictionary, which is manually created, contains
608 positive phrases and 371 negative phrases.
modify: phrases which gives positive/negative
sign to words governed by them. For example,
it adds an annotation of “modify−: environment”
into a text harmful environment. The modification
dictionary contains 79 positive phrases and 134
negative phrases.
believe: relationships which represents attitudes
of a subject to its object. For example, it adds
an annotation of “believe−: smoking” into a text
The government bans smoking in public spaces be-
cause of a negative believe phrase ban. The be-
lieve dictionary contains 30 positive phrases, 47
negative phrases and 15 neutral phrases.
</bodyText>
<sectionHeader confidence="0.999606" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.96765903030303">
We describe three major problems of the system
here: (1) identification errors, (2) polarity errors,
(3) motion format limitation.
(1) Identification errors occur on recognizing
a motion keyphrase in text data on sentence re-
trieval step. The system can incorrectly retrieve
sentences including mentions whose expressions
are the same as or similar to the motion keyphrase
but different in their meanings. In the screenshot
of Figure 1, for example, although “smoking” in
the motion refers to “tobacco smoking,” the first
sentence in the pollution paragraph argues about
“smoking caused by a fire.”
The identification problem is especially obvi-
ous in the case a motion keyphrase forms a com-
pound noun. For instance, on the motion of This
House should ban cosmetic surgery, it is not clear
if surgery in some text is equal to cosmetic surgery
or not. The errors would show requirements of
more precise word sense disambiguation or coref-
erence resolution among multiple documents.
(2) Polarity errors are not so rare. Regarding the
disease paragraph in Figure 1, the second sentence
would contain an argument on the opposite stance
in error.
(3) Motion format limitation is that the system
can process only motions in formats which ask
people if its motion keyphrase should be banned or
permitted. Representative examples of unaccept-
able motions are comparison like This house be-
lieves that capitalism is better than socialism and
questions of an adequate degree like This House
should lower the drinking age.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999854">
We described a demonstration of our argument
generation system. Our system can generate un-
derstandable arguments on a given motion for a
given stance. Our next work is to generate coun-
terarguments, which argue against the opponents.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999381">
We would like to thank Prof. Kentaro Inui from
Tohoku University for valuable discussion.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999064725">
Chikara Hashimoto, Kentaro Torisawa and Stijn De
Saeger. 2012. Excitatory or Inhibitory: A New
Semantic Orientation Extracts Contradiction and
Causality from the Web, In Proceedings of EMNLP-
CoNLL 2012, pages 619–630.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard and David Mc-
Closky. 2014. The Stanford CoreNLP Natural Lan-
guage Processing Toolkit, In Proceedings of ACL
2014 System Demonstrations, pages 55–60.
George A. Miller. 1995. WordNet: A Lexical Database
for English In Communications of the ACM, 38(11):
pages 39–41.
Joonsuk Park and Claire Cardie. 2014. Identifying Ap-
propriate Support for Propositions in Online User
Comments In Proceedings of the First Workshop on
Argumentation Mining, pages 29–38.
Junta Mizuno, Eric Nichols, Yotaro Watanabe and Ken-
taro Inui. 2012. Organizing Information on the Web
through Agreement-Conflict Relation Classification
In Proceedings of the Eighth Asia Information Re-
trieval Societies Conference , pages 126–137.
Napoles, Courtney, Matthew Gormley and Benjamin
Van Durme. 2012. Annotated English Gigaword
LDC2012T21. Web Download, Philadelphia: Lin-
guistic Data Consortium.
Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud
Aharoni and Noam Slonim. 2014. Context Depen-
dent Claim Detection In Proceedings of COLING
2014: Technical Papers, pages 1489–1500.
Jiwei Tan, XiaojunWan and Jianguo Xiao 2013.
Learning to order natural language texts In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 87-91.
Toshihiko Yanase, Toshinori Miyoshi, Kohsuke Yanai,
Misa Sato, Makoto Iwayama, Yoshiki Niwa, Paul
Reisert and Kentaro Inui 2015. Learning Sen-
tence Ordering for Opinion Generation of Debate
In Proceedings of the 2nd Workshop on Argumenta-
tion Mining, pages 94–103.
</reference>
<page confidence="0.998596">
114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337538">
<title confidence="0.7474175">End-to-end Argument Generation System in Debating Misa Sato Kohsuke Yanai Toshihiko Yanase</title>
<author confidence="0.985335">Toshinori Miyoshi Makoto Iwayama Qinghua Sun Yoshiki Niwa</author>
<affiliation confidence="0.997094">Hitachi Ltd. Research &amp; Development</affiliation>
<address confidence="0.981686">1-280, Higashi-koigakubo, Kokubunji-shi, Tokyo 185-8601</address>
<email confidence="0.951137">kohsuke.yanai.cs,toshinori.miyoshi.pd,</email>
<abstract confidence="0.986832789473684">We introduce an argument generation system in debating, one that is based on sentence retrieval. Users can specify a motion as house should ban a stance on whether the system the motion. Then the system outputs three argument paragraphs based on “values” automatically decided by the system. The “value” indicates a topic that is considered as a positive or negative for people or communities, such Each paragraph is related to one value and composed of about seven sentences. An evaluation over 50 motions from a popular debate website showed that the generated arguments are understandable in 64 paragraphs out of 150.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
</authors>
<title>Kentaro Torisawa and Stijn De Saeger.</title>
<date>2012</date>
<booktitle>Proceedings of EMNLPCoNLL 2012,</booktitle>
<pages>619--630</pages>
<marker>Hashimoto, 2012</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa and Stijn De Saeger. 2012. Excitatory or Inhibitory: A New Semantic Orientation Extracts Contradiction and Causality from the Web, In Proceedings of EMNLPCoNLL 2012, pages 619–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit,</title>
<date>2014</date>
<booktitle>In Proceedings of ACL 2014 System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="18073" citStr="Manning et al., 2014" startWordPosition="2866" endWordPosition="2869">sentences. This problem is formulated as a ranking problem, similarly to (Tan et al., 2013). Features for machine learning models in both problems are extracted not only from the sentences to be ordered but also from the motion text. Finally, the rephrasing step trims or replaces surplus phrases referring to too many details for argument scripts, such as dates and people’s names. Several simple rules are used. 4 Data Preprocessing: Annotations The system adds annotations automatically in preprocessing into the stored text data by using dictionaries and syntax information by Stanford Core NLP (Manning et al., 2014). In the current system, about 250 million annotations are stored. Users 113 can add the annotations manually. A list of main semantic annotations is below: affect: promoting/suppressing relationships. For example, it adds an annotation of “affect+: casino —* the number of crimes” into a text casino increases the number of crimes. The affect dictionary, which is manually created, contains 608 positive phrases and 371 negative phrases. modify: phrases which gives positive/negative sign to words governed by them. For example, it adds an annotation of “modify−: environment” into a text harmful en</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit, In Proceedings of ACL 2014 System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English In</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>39--41</pages>
<contexts>
<context position="15518" citStr="Miller, 1995" startWordPosition="2466" endWordPosition="2467">ctionary like a Bootstrapping method. 3.3 Sentence Retrieval Component This component retrieves sentences relevant to each value from the stored text data. It first retrieves documents using a query composed of weighted phrases. The retrieved documents should contain both the motion keyphrase and more than one representative phrases of the decided values. While the motion keyphrases can be replaced with their synonyms or hyponyms, their weights are smaller than the original keyphrases; those of synonyms are 0.75 and those of hyponyms are 0.5. The synonyms and hyponyms are acquired by WordNet (Miller, 1995). Because short documents don’t usually contains informative scripts, the length of retrieved documents are limited to more than 200 words. For example, when the motion keyphrase is gambling, a search query for a health value is (gambling#49.53 OR gaming#22.87) AND (health#27.48 OR disease#36.60 OR addiction#52.39 OR hospital#29.76) AND (length:[200 TO *]). The real numbers following sharp signs are weights of the former phrases, calculated by multiplying the IDF of the phrase and a synonym or hyponym reduction rate. The retrieval step prefers sentences that contain promote/suppress relationsh</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English In Communications of the ACM, 38(11): pages 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joonsuk Park</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying Appropriate Support for Propositions in Online User Comments</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Argumentation Mining,</booktitle>
<pages>29--38</pages>
<marker>Park, Cardie, 2014</marker>
<rawString>Joonsuk Park and Claire Cardie. 2014. Identifying Appropriate Support for Propositions in Online User Comments In Proceedings of the First Workshop on Argumentation Mining, pages 29–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junta Mizuno</author>
</authors>
<title>Eric Nichols, Yotaro Watanabe and Kentaro Inui.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth Asia Information Retrieval Societies Conference ,</booktitle>
<pages>126--137</pages>
<marker>Mizuno, 2012</marker>
<rawString>Junta Mizuno, Eric Nichols, Yotaro Watanabe and Kentaro Inui. 2012. Organizing Information on the Web through Agreement-Conflict Relation Classification In Proceedings of the Eighth Asia Information Retrieval Societies Conference , pages 126–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated English Gigaword LDC2012T21. Web Download, Philadelphia: Linguistic Data Consortium.</title>
<date>2012</date>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Napoles, Courtney, Matthew Gormley and Benjamin Van Durme. 2012. Annotated English Gigaword LDC2012T21. Web Download, Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ran Levy</author>
<author>Yonatan Bilu</author>
<author>Daniel Hershcovich</author>
<author>Ehud Aharoni</author>
<author>Noam Slonim</author>
</authors>
<title>Context Dependent Claim Detection In</title>
<date>2014</date>
<booktitle>Proceedings of COLING 2014: Technical Papers,</booktitle>
<pages>1489--1500</pages>
<contexts>
<context position="1598" citStr="Levy et al., 2014" startWordPosition="233" endWordPosition="236">s from a popular debate website showed that the generated arguments are understandable in 64 paragraphs out of 150. 1 Introduction This paper describes our end-to-end argument generation system, developed to participate in English debating games as an AI debater. When users give a “motion” like This house should ban gambling and a “stance” on whether the system should agree or disagree with the motion, the system generates argument scripts in the first constructive round of a debate. Among NLP communities, interest is growing in argumentation, such as argumentation mining and claim detection (Levy et al., 2014; Mizuno et al., 2012; Park et al., 2014). However, argument generation is still as hard a task as other text generation tasks; no standard methods or systems exist, as far as we know. We assume that argument generation systems are helpful in a variety of decision-making situations such as business, law, politics and medical care. This is because people usually investigate existing arguments on the Internet, newspapers, or research papers before reaching conclusions. In this research, we focus on debating game style because there is similarity in argument construction between debating games an</context>
</contexts>
<marker>Levy, Bilu, Hershcovich, Aharoni, Slonim, 2014</marker>
<rawString>Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni and Noam Slonim. 2014. Context Dependent Claim Detection In Proceedings of COLING 2014: Technical Papers, pages 1489–1500.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jiwei Tan</author>
</authors>
<title>XiaojunWan and Jianguo Xiao 2013. Learning to order natural language texts</title>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>87--91</pages>
<marker>Tan, </marker>
<rawString>Jiwei Tan, XiaojunWan and Jianguo Xiao 2013. Learning to order natural language texts In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 87-91.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Toshihiko Yanase</author>
<author>Toshinori Miyoshi</author>
</authors>
<title>Kohsuke Yanai, Misa Sato, Makoto Iwayama, Yoshiki Niwa, Paul Reisert and Kentaro Inui 2015. Learning Sentence Ordering for Opinion Generation of Debate</title>
<booktitle>In Proceedings of the 2nd Workshop on Argumentation Mining,</booktitle>
<pages>94--103</pages>
<marker>Yanase, Miyoshi, </marker>
<rawString>Toshihiko Yanase, Toshinori Miyoshi, Kohsuke Yanai, Misa Sato, Makoto Iwayama, Yoshiki Niwa, Paul Reisert and Kentaro Inui 2015. Learning Sentence Ordering for Opinion Generation of Debate In Proceedings of the 2nd Workshop on Argumentation Mining, pages 94–103.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>