<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.994114">
Modeling Category Structures with a Kernel Function
</title>
<author confidence="0.990689">
Hiroya Takamura
</author>
<affiliation confidence="0.993246">
Precision and Intelligence Laboratory
Tokyo Institute of Technology
</affiliation>
<address confidence="0.7822905">
4259 Nagatsuta Midori-ku Yokohama,
226-8503 Japan
</address>
<email confidence="0.997787">
takamura@pi.titech.ac.jp
</email>
<author confidence="0.991947">
Yuji Matsumoto
</author>
<affiliation confidence="0.999812">
Department of Information Technology
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.8388635">
8516-9 Takayama Ikoma Nara,
630-0101 Japan
</address>
<email confidence="0.99852">
matsu@is.aist-nara.ac.jp
</email>
<author confidence="0.992267">
Hiroyasu Yamada
</author>
<affiliation confidence="0.828522">
School of Information Science
Japan Advanced Institute of Science and Technology
1-1 Asahidai Tatsunokuchi Ishikawa, 923-1292 Japan
</affiliation>
<email confidence="0.99026">
h-yamada@jaist.ac.jp
</email>
<sectionHeader confidence="0.998557" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923238095238">
We propose one type of TOP (Tangent vector
Of the Posterior log-odds) kernel and apply it to
text categorization. In a number of categoriza-
tion tasks including text categorization, nega-
tive examples are usually more common than
positive examples and there may be several dif-
ferent types of negative examples. Therefore,
we construct a TOP kernel, regarding the prob-
abilistic model of negative examples as a mix-
ture of several component models respectively
corresponding to given categories. Since each
component model of our mixture model is ex-
pressed using a one-dimensional Gaussian-type
function, the proposed kernel has an advantage
in computational time. We also show that the
computational advantage is shared by a more
general class of models. In our experiments,
the proposed kernel used with Support Vector
Machines outperformed the linear kernel and
the Fisher kernel based on the Probabilistic La-
tent Semantic Indexing model.
</bodyText>
<sectionHeader confidence="0.999626" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999756928571429">
Recently, Support Vector Machines (SVMs) have been
actively studied because of their high generalization abil-
ity (Vapnik, 1998). In the formulation of SVMs, func-
tions which measure the similarity of two examples take
an important role. These functions are called kernelfunc-
tions. The usual dot-product of two vectors respectively
corresponding to two examples is often used. Although
some variants to the usual dot-product are sometimes
used (for example, higher-order polynomial kernels and
RBF kernels), the distribution of examples is not taken
into account in such kernels.
However, new types of kernels have more recently
been proposed; they are based on the probability distri-
bution of examples. One is Fisher kernels (Jaakkola and
Haussler, 1998). The other is TOP (Tangent vector Of the
Posterior log-odds) kernels (Tsuda et al., 2002). While
Fisher kernels are constructed on the basis of a genera-
tive model of data, TOP kernels are based on the class-
posterior probability, that is, the probability that the pos-
itive class occurs given an example. However, in order to
use those kernels, we have to select a probabilistic model
of data. The selection of a model will affect categoriza-
tion result. The present paper provides one solution to
this issue. Specifically, we proposed one type of TOP
kernel, because it has been reported that TOP kernels per-
form better than Fisher kernels in terms of categorization
accuracy.
We briefly explain our kernel. We focus on negative
examples in binary classification. Negative examples are
usually more common than positive examples. There
may be several different types of negative examples. Fur-
thermore, the categories of negative examples are some-
times explicitly given (for example, the situation where
we are given documents, each of which has one of three
categories “sports”,”politics” and “economics”, and we
are to extract documents with “politics”). In such a situa-
tion, the probabilistic model of negative examples can be
regarded as a mixture of several component models. We
effectively use this property. Although many other mod-
els can be used, we propose a model based on the sepa-
rating hyperplanes in the original feature space. Specif-
ically, a one-dimensional Gaussian-type function normal
to a hyperplane corresponds to a category. The negative
class is then expressed as a kind of Gaussian mixture.
The reason for the selection of this model is that the re-
sulting kernel has an advantage in computational time.
The kernel based on this mixture model, what we call
Hyperplane-based TOP (HP-TOP) kernel, can be com-
puted efficiently in spite of its high dimensionality. We
later show that the computational advantage is shared by
a more general class of models.
In the experiments of text categorization, in which
SVMs are used as classifiers, our kernel outperformed
the linear kernel and the Fisher kernel based on the Prob-
abilistic Latent Semantic Indexing model proposed by
Hofmann (2000) in terms of categorization accuracy.
</bodyText>
<sectionHeader confidence="0.934166" genericHeader="introduction">
2 SVMs and Kernel Method
</sectionHeader>
<bodyText confidence="0.999832">
In this section, we explain SVMs and the kernel method,
which are the basis of our research. SVMs have achieved
high accuracy in various tasks including text categoriza-
tion (Joachims, 1998; Dumais et al., 1998).
Suppose a set Dl of ordered pairs consisting of a fea-
ture vector and its label
</bodyText>
<equation confidence="0.998401">
Dl = {(x1, y1), (x2, y2), ··· , (xl, yl)},
(∀i, xi ∈ R|I|, yi ∈ {−1,1}) (1)
</equation>
<bodyText confidence="0.998126">
is given. Dl is called training data. I is the set of feature
indices. In SVMs, a separating hyperplane (f(x) = w ·
x − b) with the largest margin (the distance between the
hyperplane and its nearest vectors) is constructed.
Skipping the details of SVMs’ formulation, here we
just show the conclusion that, using some real numbers
α∗i (∀i) and b∗, the optimal hyperplane is expressed as
follows:
</bodyText>
<equation confidence="0.997909">
f(x) = � α∗i yixi · x − b∗. (2)
i
</equation>
<bodyText confidence="0.998991333333333">
We should note that only dot-products of examples are
used in the above expression.
Since SVMs are linear classifiers, their separating abil-
ity is limited. To compensate for this limitation, the
kernel method is usually combined with SVMs (Vapnik,
1998).
In the kernel method, the dot-products in (2) are re-
placed with more general inner-products K(xi, x) (kernel
functions). The polynomial kernel (xi·xj +1)d (d ∈ N+)
and the RBF kernel exp{−kxi − xjk2/2σ2} are often
used. Using the kernel method means that feature vectors
are mapped into a (higher dimensional) Hilbert space and
linearly separated there. This mapping structure makes
non-linear separation possible, although SVMs are basi-
cally linear classifiers.
Another advantage of the kernel method is that al-
though it deals with a high dimensional (possibly infinite)
space, explicit computation of high dimensional vectors
is not required. Only the general inner-products of two
vectors need to be computed. This advantage leads to a
relatively small computational overhead.
</bodyText>
<sectionHeader confidence="0.979807" genericHeader="method">
3 Kernels from Probabilistic Models
</sectionHeader>
<bodyText confidence="0.9999088">
Recently new type of kernels which connect genera-
tive models of data and discriminative classifiers such as
SVMs, have been proposed: the Fisher kernel (Jaakkola
and Haussler, 1998) and the TOP (Tangent vector Of the
Posterior log-odds) kernel (Tsuda et al., 2002).
</bodyText>
<subsectionHeader confidence="0.998319">
3.1 Fisher Kernel
</subsectionHeader>
<bodyText confidence="0.999835">
Suppose we have a probabilistic generative model p(x|O)
of the data (we denote an example by x). The Fisher score
of x is defined as ∇o log p(x|O), where ∇o means par-
tial differentiation with respect to the parameters O. The
Fisher information matrix is denoted by I(O) (this ma-
trix defines the geometric structure of the model space).
Then, the Fisher kernel at an estimate Oˆ is given by:
</bodyText>
<equation confidence="0.980526">
K(x1,x2)
= (∇o log p(x1|ˆO))tI−1(ˆO)(∇o log p(x2|ˆO)) (3)
</equation>
<bodyText confidence="0.999944875">
The Fisher score of an example approximately indicates
how the model will change if the example is added to the
training data used in the estimation of the model. That
means, the Fisher kernel between two examples will be
large, if the influences of the two examples to the model
are similar and large (Tsuda and Kawanabe, 2002).
The matrix I(O) is often approximated by the identity
matrix to avoid large computational overhead.
</bodyText>
<subsectionHeader confidence="0.998526">
3.2 TOP Kernel
</subsectionHeader>
<bodyText confidence="0.991312454545454">
On the basis of a probabilistic model of the data, TOP
kernels are designed to extract feature vectors fˆo which
are considered to be useful for categorization with a sep-
arating hyperplane.
We begin with the proposition that, between the gener-
alization error R(fˆo) and the expected error of the poste-
rior probability D(fˆo), the relation R(fˆo) − L∗ ≤ 2D(fˆo)
holds, where L∗ is the Bayes error. This inequality means
that minimizing D(fˆo) leads to reducing the generaliza-
tion error R(fˆo). D(fˆo) is expressed, using a logistic
function F(t) = 1/(1 + exp(−t)), as
</bodyText>
<equation confidence="0.942132666666667">
D(fˆo)
= min
w,b Ex|F(w · fˆo − b) − P(y = +1|x, O∗)|, (4)
</equation>
<bodyText confidence="0.99955625">
where O∗ denotes the actual parameters of the model.
The TOP kernel consists of features which can minimize
D(fˆo). In other words, we would like to have feature vec-
tors fˆo that satisfy the following:
</bodyText>
<equation confidence="0.633461">
∀x, w · fˆo(x) − b = F−1(P(y = +1|x, O∗)). (5)
</equation>
<bodyText confidence="0.887773">
for certain values of w and b.
For that purpose, we first define a function v(x, O):
v(x, O) ≡ F−1(P(y = +1|x, O))
</bodyText>
<equation confidence="0.880845111111111">
= log P(y = +1|x, O) − log P(y = −1|x, O). (6)
The first-order Taylor expansion of v(x, θ∗) around the
estimate
ˆθ)
(θ∗ i − ˆθi)∂v(x, . (7)
∂θi
If fˆθ is of the following form:
fˆθ(x) = ¡v(x, ˆθ), ∂v(x, ˆθ)/∂θ1, · · · , ∂v(x, ˆθ)/∂θp¢,
(8)
</equation>
<bodyText confidence="0.9136">
and if w and b are properly chosen as
</bodyText>
<equation confidence="0.98937275">
w = (1, θ∗1 − ˆθ1, ··· ,θ∗p − ˆθp), b = 0, (9)
then (5) is approximately satisfied. Thus, the TOP kernel
is defined as
K(x1,x2) = fˆθ(x1) · fˆθ(x2). (10)
</equation>
<bodyText confidence="0.999922">
A detailed discussion of the TOP kernel and its theoreti-
cal analysis have been given by Tsuda et al (Tsuda et al.,
2002).
</bodyText>
<sectionHeader confidence="0.999955" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.998939">
Hofmann (2000) applied Fisher kernels to text catego-
rization under the Probabilistic Latent Semantic Indexing
(PLSI) model (Hofmann, 1999).
In PLSI, the joint probability of document d and word
wis:
</bodyText>
<equation confidence="0.9937575">
P (d, w) = X P(zk)P(d|zk)P(w|zk), (11)
k
</equation>
<bodyText confidence="0.9984986">
where variables zk correspond to latent classes. After
the estimation of the model using the EM algorithm, the
Fisher kernel for this model is computed. The average
log-likelihood of document d normalized by the docu-
ment length is given by
</bodyText>
<equation confidence="0.9813265">
l(d) = X Pˆ (wj|d) log X P(wj|zk)P(zk|d), (12)
j k
</equation>
<bodyText confidence="0.848442">
where
</bodyText>
<equation confidence="0.9868185">
Pˆ(wj|d) = freq(wj,d).(13)
Pm f req(wm, d)
</equation>
<bodyText confidence="0.80761275">
They use spherical parameterization (Kass and Vos,
1997) instead of the original parameters in the model.
p
They define parameters ρjk = 2 P (wj|zk) and ρk =
</bodyText>
<equation confidence="0.892131">
p
2 P (zk), and obtained
Pˆ (wj|d)P(zk|d, wj)
= p (14) P(wj|zk) ,
P(zk|d) ≈(15)
pP(zk) .
</equation>
<bodyText confidence="0.999359428571428">
Thus, the Fisher kernel for this model is obtained as de-
scribed in Appendix A.
The first term of (31) corresponds to the similarity
through latent spaces. The second term corresponds to
the similarity through the distribution of each word. The
number of latent classes zk can affect the value of the
kernel function. In the experiment of (Hofmann, 2000),
they computed the kernels with the different numbers (1
to 64) of zk and added them together to make a robust
kernel instead of deciding one specific number of latent
classes zk.
They concluded that the Fisher kernel based on PLSI
is effective when a large amount of unlabeled examples
are available for the estimation of the PLSI model.
</bodyText>
<sectionHeader confidence="0.987787" genericHeader="method">
5 Hyperplane-based TOP Kernel
</sectionHeader>
<bodyText confidence="0.753455">
In this section, we explain our TOP kernel.
</bodyText>
<subsectionHeader confidence="0.994818">
5.1 Derivation of HP-TOP kernel
</subsectionHeader>
<bodyText confidence="0.978855">
Suppose we have obtained the parameters wc and bc
of the separating hyperplane for each category c ∈
Ccategory in the original feature space, where Ccategory
denotes the set of categories.
We assume that the class-posteriors Pc(+1|d) and
Pc(−1|d) are expressed as1
</bodyText>
<equation confidence="0.999478">
P(c)q(d|c)
Pc(+1|d) = (16)
Pc, P(c0)q(d |c0) ,
Pc (−1|d)= Pe6=c P(e)q(d |e) (17)
Pc, P(c0)q(d |c0)
</equation>
<bodyText confidence="0.991816">
where, for any category x, component function q(d|x) is
of Gaussian-type:
</bodyText>
<equation confidence="0.854541333333333">
2
σ2 x
(18)
</equation>
<bodyText confidence="0.993048333333333">
with the mean µx of a random variable wx · d − bx and
the variance σx. Those parameters are estimated with the
maximum likelihood estimation, as follows:
</bodyText>
<equation confidence="0.9949545">
P(d,y)∈Dl,y=x{wx·d−bx}
|{(d,y)∈Dl|y=x} |, (19)
P(d,y)∈Dl,y=x{wx·d−bx−µx}2
|{(d,y)∈Dl|y=x} |. (20)
</equation>
<bodyText confidence="0.996813666666667">
We choose the Gaussian-type function as an exam-
ple.However, this choice is open to argument, since some
other models also have the same computational advan-
tage as described in Section 5.4.
We set θx1 = µx/σ2x, θx2 = −1/2σ2x. Although θx1
and θx2 are not the natural parameters of this model,
</bodyText>
<footnote confidence="0.996236">
1We cannot say q(dIx) is a generative probability of d given
class x, because it is one-dimensional and not valid as a proba-
bility density in the original feature space.
</footnote>
<equation confidence="0.997164357142857">
θˆ is
v(x,θ∗) ≈ v(x, ˆθ) + X
i
∂l(d)
∂ρjk
∂l(d)
∂ρk
q(d|x) = p exp{−
2πσ2 2
},
x
1 ((wx · d − bx) − µx)
µx =
σx =
</equation>
<bodyText confidence="0.998303">
we parameterize this model using the parameters θx1,
θx2, wx, bx and P(x) (dx E Ccategory) for simplic-
ity. Using this probabilistic model,we compute func-
tion v(d, θ) as described in Appendix B (θ denotes
1wx, bx, θx1, θx2|x E Ccategory} and wxi denotes the i-
th element of the weight vector wx).
The partial derivatives of this function with respect to
the parameters are in Appendix C.
Then we can follow the definition (10) to obtain our
version of the TOP kernel. We call this new kernel a
hyperplane-based TOP (HP-TOP) kernel.
</bodyText>
<subsectionHeader confidence="0.999194">
5.2 Properties of HP-TOP kernel
</subsectionHeader>
<bodyText confidence="0.9999798">
In the derivatives (39), which provide the largest number
of features, original features di are accompanied by other
factors computed from probability distributions. This
form suggests that two vectors are considered to be more
similar, if they have similar distributions over categories.
In other words, an occurrence of a word can have dif-
ferent contribution to the classification result, depending
on the context (i.e., the other words in the document).
This property of the HP-TOP kernel can lead to the ef-
fect of word sense disambiguation, because “bank” in a
financial document is treated differently from “bank” in a
document related to a river-side park.
The derivatives (34) and (35) correspond to the first-
order differences, respectively for the positive class and
the negative class. Similarly, the derivatives (36) and (37)
for the second-order differences. The derivatives (40) and
(41) are for the first-order differences normalized by the
variances.
The derivatives other than (38) and (38) directly de-
pend on the distance from a hyperplane, rather than on
the value of each feature. These derivatives enrich the
feature set, when there are few active words, by which
we mean the words that do not occur in the training data.
For this reason, we expect that the HP-TOP kernel works
well for a small training dataset.
</bodyText>
<subsectionHeader confidence="0.994029">
5.3 Computational issue
</subsectionHeader>
<bodyText confidence="0.999025666666667">
Computing the kernel in this form is time-consuming, be-
cause the number of components of type (39) can be very
large:
</bodyText>
<equation confidence="0.628007">
O(|I |x |Ccategory|), (21)
</equation>
<bodyText confidence="0.999702222222222">
where I denotes the set of indices for original features.
However, we can avoid this heavy computational cost
as follows. Let us compute the dot-product of deriva-
tives (39) of two vectors d1 and d2, which is shown in
Appendix D. The last expression (45) is regarded as the
scalar product of two dot-products. Thus, by preserving
vectors d and
we can efficiently compute the dot-product in (39); the
computational complexity of a kernel function is
</bodyText>
<equation confidence="0.671069">
O(|I|), (23)
</equation>
<bodyText confidence="0.965053620689655">
on the condition that the original dimension is larger than
the number of categories. Thus, from the viewpoint of
computational time, our kernel has an advantage over
some other kernels such as the PLSI-based Fisher kernel
in Section 4, which requires the computational complex-
ity of O(|I |x |Ccluster|), where Ccluster denotes the set
of clusters.
In the PLSI-based Fisher kernel, each word has a prob-
ability distribution over latent classes. In this sense, the
PLSI-based Fisher kernel is more detailed, but detailed
models are sometimes suffer overfitting to the training
data and have the computational disadvantage as men-
tioned above.
The PLSI-based Fisher kernel can be extended to a
TOP kernel by using given categories as latent classes.
However, the problem of computational time still re-
mains.
5.4 General statement about the computational
advantage
So far, we have discussed the computational time for
the kernel constructed on the Gaussian mixture. How-
ever, the computational advantage of the kernel, in fact,
is shared by a more general class of models.
We examine the required conditions for the computa-
tional advantage. Suppose the class-posteriors have the
mixture form as Equations (16) and (17), but function
q(d|x) does not have to be a Gaussian-type function. In-
stead, function q(d|x) is supposed to be represented using
some function r parametrized by we and b, as:
</bodyText>
<equation confidence="0.996252">
q(d|x) = r(fx(d)|x), (24)
</equation>
<bodyText confidence="0.997370666666667">
where fx is a scalar function. Then, let us obtain the
derivative of v(d, θ) with respect to wei, which is the bot-
tleneck of kernel computation:
</bodyText>
<equation confidence="0.995608333333334">
∂v(d,θ)
∂wei
∂r(fe(d)|e)
P_c(d)
∂fe(d) .(25)
∂wei
</equation>
<bodyText confidence="0.999918666666667">
The first two factors of (25) do not depend on i. There-
fore, if the last factor of (25) is variable-separable with
respect to e and i:
</bodyText>
<equation confidence="0.983648375">
−P(e)q(d|e)
=
−P(e)q(d|e)
= P_c(d)
∂wei
∂r(fe(d)|e)
∂fe(d)
C�−P(e)q(d|e) µe − (we · d − be) ,(22)
σ2
e
c,e∈Ccategory
:A
P_c(d)
e
∂fe(d) = S(e)T(i), (26)
∂wei
</equation>
<bodyText confidence="0.999893">
where S and T are some function, then the derivative
(25) is also variable-separable. In such cases, the effi-
cient computation described in Section 5.3 is possible by
preserving the vectors:
</bodyText>
<equation confidence="0.9707205">
(T(i))i∈I , (27)
C− P(e)q(d |e) ∂r(fe(d)  |e) S(e)I . (28)
P−c(d) ∂fe(d) / e∈C
e#cI category
</equation>
<tableCaption confidence="0.998113">
Table 1: The categories and their sizes of Reuters-21578
</tableCaption>
<figure confidence="0.97875884">
category
test texts
training texts
1051
earn
644
acq
141
135
164
133
100
87
66
48
money-fx
grain
crude
trade
interest
ship
wheat
corn
2725
1490
</figure>
<page confidence="0.9879625">
464
399
353
339
291
197
199
161
</page>
<bodyText confidence="0.9983094">
We have now obtained the required conditions for the
efficient computation: Equation (24) and the variable-
separability.
In case of Gaussian-type functions, function fe and its
derivative with respect to wei are
</bodyText>
<equation confidence="0.996830666666667">
fe(d) = we · d − be, (29)
afe(d)
awei
</equation>
<bodyText confidence="0.713202">
Thus, the conditions are satisfied.
</bodyText>
<sectionHeader confidence="0.998408" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999958448275862">
Through experiments of text categorization, we empiri-
cally compare the HP-TOP kernel with the linear kernel
and the PLSI-based Fisher kernel. We use Reuters-21578
dataset2 with ModApte-split (Dumais et al., 1998). In ad-
dition, we delete some texts from the result of ModApte-
split, because those texts have no text body. After the
deletion, we obtain 8815 training examples and 3023 test
examples. The words that occur less than five times in the
whole training set are excluded from the original feature
set.
We do not use all the 8815 training examples. The
size of the actual training data ranges from 1000 to 8000.
For each dataset size, experiments are executed 10 times
with different training sets.The result is evaluated with F-
measures for the most frequent 10 categories (Table 1).
The total number of categories is actually 116. How-
ever, for small categories, reliable statistics cannot be ob-
tained. For this reason, we regard the remaining cate-
gories other than the 10 most frequent categories as one
category. Therefore, the model for negative examples is
a mixture of 10 component models (9 out of the 10 most
frequent categories and the new category consisting of the
remaining categories).
We assume uniform priors for categories as in (Tsuda
et al., 2002). We computed the Fisher kernels with differ-
ent numbers (10, 20 and 30) of latent classes and added
them together to make a robust kernel (Hofmann, 2000).
After the learning in the original feature space, the param-
eters for the probability distributions are estimated with
</bodyText>
<footnote confidence="0.898511">
2Available from
http://www.daviddlewis.com/resources/.
</footnote>
<bodyText confidence="0.999752694444444">
maximum likelihood estimation as in Equations (19) and
(20), followed by the learning with the proposed kernel.
We used an SVM package, TinySVM3, for SVM com-
putation. The soft-margin parameter C was set to 1.0
(other values of C showed no significant changes in re-
sults).
The result is shown in Figure 1 (for macro-average)
and Figure 2 (for micro-average). The HP-TOP kernel
outperforms the linear kernel and the PLSI-based Fisher
kernel for every number of examples.
At each number of examples, we conducted a
Wilcoxon Signed Rank test with 5% significance-level,
for the HP-TOP kernel and the linear kernel, since these
two are better than the other. The test shows that the dif-
ference between the two methods is significant for the
training data sizes 1000 to 5000. The superiority of the
HP-TOP kernel for small training datasets supports our
expectation that the enrichment of feature set will lead to
better performance for few active words. Although we
also expected that the effect of word sense disambigua-
tion would improve accuracy for large training datasets,
the experiments do not provide us with an empirical ev-
idence for the expectation. One possible reason is that
Gaussian-type functions do not reflect the actual distribu-
tion of data. We leave its further investigation as future
research.
In this experimental setting, the PLSI-based Fisher ker-
nel did not work well in terms of categorization accuracy.
However, this Fisher kernel will perform better when the
number of labeled examples is small and a number of
unlabeled examples are available, as reported by Hof-
mann (2000).
We also measured computational time of each method
(Figure 3). The vertical axis indicates the average com-
putational time over 100 runs of experiments (10 runs for
each category). Please note that training time in this fig-
</bodyText>
<footnote confidence="0.7357585">
3Available from
http://cl.aist-nara.ac.jp/˜taku-ku/software/TinySVM/.
</footnote>
<figure confidence="0.9966118">
= di. (30)
10000
1000
100
0.1
10
1
HP-TOP Kernel
Linear Kernel
PLSI-based Fisher Kernel
82
80
78
76
74
72
70
68
66
64
HP-TOP Kernel
Linear Kernel
PLSI-based Fisher Kernel
1000 2000 3000 4000 5000 6000 7000 8000
Number of Labeled Examples
</figure>
<figureCaption confidence="0.996733">
Figure 1: Macro-average of F-measure
</figureCaption>
<figure confidence="0.993747583333333">
90
89
88
87
86
85
84
HP-TOP Kernel
Linear Kernel
PLSI-based Fisher Kernel
1000 2000 3000 4000 5000 6000 7000 8000
Number of Labeled Examples
</figure>
<figureCaption confidence="0.982653">
Figure 2: Micro-average of F-measure
</figureCaption>
<figure confidence="0.9614055">
1000 2000 3000 4000 5000 6000 7000 8000
Number of Labeled Examples
</figure>
<figureCaption confidence="0.999904">
Figure 3: Computational time of each method
</figureCaption>
<bodyText confidence="0.9999588">
ure does not include the computational time required for
feature extraction4. This result empirically shows that the
HP-TOP kernel outperforms the PLSI-based Fisher ker-
nel in terms of computational time as theoretically ex-
pected in Section 5.3.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="method">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99995845">
We proposed a TOP kernel based on separating hy-
perplanes. The proposed kernel is created from one-
dimensional Gaussians along the normal directions of the
hyperplanes. We showed that the computational advan-
tage that the proposed kernel has is shared by a more
general class of models. We empirically showed that the
proposed kernel outperforms the linear kernel in text cat-
egorization.
Although the superiority of the proposed method to the
linear kernel was shown, the proposed method has to be
further investigated. Firstly, for large data sizes (namely
7000 and 8000), the proposed method was not signifi-
cantly better than the linear kernel. The effectiveness of
the proposed method should be confirmed by more ex-
periments and theoretical analysis. Secondly, we have to
compare the proposed method with other kernels in or-
der to check the effectiveness of the kernel function con-
sisting of one-dimensional Gaussians normal to the hy-
perplanes. The use of Gaussians is open to argument,
because their symmetric form is somewhat against our
</bodyText>
<footnote confidence="0.633666666666667">
4If the computational time required for feature extraction is
included, the HP-TOP kernel cannot be faster than the linear
kernel.
</footnote>
<bodyText confidence="0.998494444444444">
intuition.
This model can be extended to incorporate unlabeled
examples, for example, using the EM algorithm. In that
sense, the combination of PLSI and the semi-supervised
EM algorithm is also one promising model. When the
category structure of the negative examples is not given,
the proposed method is not applicable. We should inves-
tigate whether unsupervised clustering can substitute for
the category structure.
</bodyText>
<sectionHeader confidence="0.997713" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.914200111111111">
Susan T. Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algo-
rithms and representations for text categorization. In
Proceedings of the Seventh International Conference
on Information and Knowledge Management (ACM-
CIKM98), pages 148–155.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual ACM
Conference on Research and Development in Informa-
tion Retrieval, pages 50–57, Berkeley, California, Au-
gust.
Thomas Hofmann. 2000. Learning the similarity of doc-
uments: An information geometric approach to docu-
ment retrieval and categorization. In Advances in Neu-
ral Information Processing Systems, 12, pages 914–
920.
Tommi Jaakkola and David Haussler. 1998. Exploiting
generative models in discriminative classifiers. In Ad-
vances in Neural Information Processing Systems 11,
pages 487–493.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the 10th European Con-
ference on Machine Learning, pages 137–142.
Robert E. Kass and Paul W. Vos. 1997. Geometrical
foundations of asymptotic inference. New York : Wi-
ley.
Koji Tsuda and Motoaki Kawanabe. 2002. The leave-
one-out kernel. In Proceedings of International Con-
ference on Artificial Neural Networks, pages 727–732.
Koji Tsuda, Motoaki Kawanabe, Gunnar R¨atsch, S¨oren
Sonnenburg, and Klaus-Robert M¨uller. 2002. A new
discriminative kernel from probabilistic models. Neu-
ral Computation, 14(10):2397–2414.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley, New York.
</reference>
<title confidence="0.403255">
A Fisher Kernel based on PLSI
</title>
<equation confidence="0.993376129032258">
X
K(d1, d2) =
k
X
P(zk) +
Pˆ(wj  |d1) Pˆ(wj |d 2)
X
k
P(zk|d1)P(zk|d2)
j
P(zk|d1, wj)P(zk|d2, wj) (31)
,
P(wj|zk)
where P(zk|d, wj) =
Ã
P(zk)P(d|zk)P(wj|zk) P(zk)P(d|zk)P(wj|zk)
Pl P(zl)P(d|zl)P(wj|zl) P(d, wj)
. (32)
B Function v for HP-TOP Kernel
v(d, α, w, b) = log P(+1|d) − log P(−1|d)
= log P(c)q(d|c) − log Pe6=c P(e)q(d|e)
Pc0 P(c0)q(d |c0) Pc0 P(c0)q(d|c0)
= log P(c)q(d|c) − log X P(e)q(d|e)
e6=c
2
= log P(c) exp{θc1 (wc · d) + θc2 (wc · d)2 + θ
c1 − 1 log −π }
4θc2 2 θc2
X− log 2
e6=c P(e) exp{ θe1 (we · d) + θe2 (we · d)2 + θe1 − 1 log −π }, (33)
4θe2 2 θe2
</equation>
<bodyText confidence="0.885454">
where θx1 = µx/σ2x, θx2 = −1/2σ2x.
</bodyText>
<sectionHeader confidence="0.913719" genericHeader="method">
C Partial Derivatives
</sectionHeader>
<equation confidence="0.994745585365854">
∂v(d, θ)
∂θc1
∂v(d, θ)
∂θe1
∂v(d, θ)
∂θc2
∂v(d, θ)
∂θe2
∂v(d, θ)
∂wci
= wc · d − bc − µc, (34)
P(e)q(d|e) we · d − be − µe), (35)
Pc06=c P(c0)q(d|c0)
= (wc · d − bc)2 − µ2c − σ2c, (36)
P(e)q(d|e) {(we · d − be)2 − µ2e − σ2 e}, (37)
µc − (wc · d − bc) =di, (38)
σ2c
P
c06=c P(c0)q(d|c0)
∂v(d, θ) P(e)q(d|e) µe − (we · d − be) di, (39)
∂wei σ2e
P
c06=c P(c0)q(d|c0)
P(e)q(d|e)
P
c06=c P(c0)q(d|c0)
we · d − be − µe ,(41)
σ2e
∂v(d, θ)
∂bc
wc · d − bc − µc = ,(40)
σ2c
∂v(d, θ)
∂be
∂v(d, θ) 1 = (42)
P(c) P(c)
P(d|e) (43)
∂v(d, θ)
P(e)
P
c06=c P(c0)q(d|c0)
</equation>
<sectionHeader confidence="0.512811" genericHeader="method">
D Dot-product of Derivatives (39) in Appendix C
</sectionHeader>
<equation confidence="0.923793833333333">
X X ∂v(d1, θ) ∂v(d2, θ) X= X P(e)2q(d1|e)q(d2|e) µe − (we · d − be) µe − (we · d − be) d1 i d2 i (44)
e6=c i e6=c i σ2 e
∂wei ∂wei P−c(d1)P−c(d2) σ2e
µX = P(e)2q(d1|e)q(d2|e) µe − (we · d − be) µe − (we · d − be) ¶ d1 · d2, (45)
e6=c
P−c(d1)P−c(d2) σ2e σ2e
</equation>
<bodyText confidence="0.94247">
where P−c(d) denotes P c06=c P (c0)q(d|c0).
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.119952">
<title confidence="0.877155666666667">Modeling Category Structures with a Kernel Function Hiroya Precision and Intelligence</title>
<affiliation confidence="0.880875">Tokyo Institute of</affiliation>
<address confidence="0.827697">4259 Nagatsuta Midori-ku 226-8503</address>
<email confidence="0.969543">takamura@pi.titech.ac.jp</email>
<author confidence="0.825241">Yuji</author>
<affiliation confidence="0.9999195">Department of Information Nara Institute of Science and</affiliation>
<address confidence="0.996845">8516-9 Takayama Ikoma</address>
<phone confidence="0.680241">630-0101</phone>
<email confidence="0.941513">matsu@is.aist-nara.ac.jp</email>
<author confidence="0.692852">Hiroyasu Yamada</author>
<affiliation confidence="0.9974055">School of Information Science Japan Advanced Institute of Science and Technology</affiliation>
<address confidence="0.980957">1-1 Asahidai Tatsunokuchi Ishikawa, 923-1292 Japan</address>
<email confidence="0.965298">h-yamada@jaist.ac.jp</email>
<abstract confidence="0.990347409090909">We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization. In a number of categorization tasks including text categorization, negative examples are usually more common than positive examples and there may be several different types of negative examples. Therefore, we construct a TOP kernel, regarding the probabilistic model of negative examples as a mixture of several component models respectively corresponding to given categories. Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time. We also show that the computational advantage is shared by a more general class of models. In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susan T Dumais</author>
<author>John Platt</author>
<author>David Heckerman</author>
<author>Mehran Sahami</author>
</authors>
<title>Inductive learning algorithms and representations for text categorization.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh International Conference on Information and Knowledge Management (ACMCIKM98),</booktitle>
<pages>148--155</pages>
<contexts>
<context position="4730" citStr="Dumais et al., 1998" startWordPosition="724" endWordPosition="727">its high dimensionality. We later show that the computational advantage is shared by a more general class of models. In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by Hofmann (2000) in terms of categorization accuracy. 2 SVMs and Kernel Method In this section, we explain SVMs and the kernel method, which are the basis of our research. SVMs have achieved high accuracy in various tasks including text categorization (Joachims, 1998; Dumais et al., 1998). Suppose a set Dl of ordered pairs consisting of a feature vector and its label Dl = {(x1, y1), (x2, y2), ··· , (xl, yl)}, (∀i, xi ∈ R|I|, yi ∈ {−1,1}) (1) is given. Dl is called training data. I is the set of feature indices. In SVMs, a separating hyperplane (f(x) = w · x − b) with the largest margin (the distance between the hyperplane and its nearest vectors) is constructed. Skipping the details of SVMs’ formulation, here we just show the conclusion that, using some real numbers α∗i (∀i) and b∗, the optimal hyperplane is expressed as follows: f(x) = � α∗i yixi · x − b∗. (2) i We should not</context>
<context position="17481" citStr="Dumais et al., 1998" startWordPosition="2909" endWordPosition="2912">3 100 87 66 48 money-fx grain crude trade interest ship wheat corn 2725 1490 464 399 353 339 291 197 199 161 We have now obtained the required conditions for the efficient computation: Equation (24) and the variableseparability. In case of Gaussian-type functions, function fe and its derivative with respect to wei are fe(d) = we · d − be, (29) afe(d) awei Thus, the conditions are satisfied. 6 Experiments Through experiments of text categorization, we empirically compare the HP-TOP kernel with the linear kernel and the PLSI-based Fisher kernel. We use Reuters-21578 dataset2 with ModApte-split (Dumais et al., 1998). In addition, we delete some texts from the result of ModAptesplit, because those texts have no text body. After the deletion, we obtain 8815 training examples and 3023 test examples. The words that occur less than five times in the whole training set are excluded from the original feature set. We do not use all the 8815 training examples. The size of the actual training data ranges from 1000 to 8000. For each dataset size, experiments are executed 10 times with different training sets.The result is evaluated with Fmeasures for the most frequent 10 categories (Table 1). The total number of ca</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>Susan T. Dumais, John Platt, David Heckerman, and Mehran Sahami. 1998. Inductive learning algorithms and representations for text categorization. In Proceedings of the Seventh International Conference on Information and Knowledge Management (ACMCIKM98), pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic Latent Semantic Indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual ACM Conference on Research and Development in Information Retrieval,</booktitle>
<pages>50--57</pages>
<location>Berkeley, California,</location>
<contexts>
<context position="9263" citStr="Hofmann, 1999" startWordPosition="1521" endWordPosition="1522">round the estimate ˆθ) (θ∗ i − ˆθi)∂v(x, . (7) ∂θi If fˆθ is of the following form: fˆθ(x) = ¡v(x, ˆθ), ∂v(x, ˆθ)/∂θ1, · · · , ∂v(x, ˆθ)/∂θp¢, (8) and if w and b are properly chosen as w = (1, θ∗1 − ˆθ1, ··· ,θ∗p − ˆθp), b = 0, (9) then (5) is approximately satisfied. Thus, the TOP kernel is defined as K(x1,x2) = fˆθ(x1) · fˆθ(x2). (10) A detailed discussion of the TOP kernel and its theoretical analysis have been given by Tsuda et al (Tsuda et al., 2002). 4 Related Work Hofmann (2000) applied Fisher kernels to text categorization under the Probabilistic Latent Semantic Indexing (PLSI) model (Hofmann, 1999). In PLSI, the joint probability of document d and word wis: P (d, w) = X P(zk)P(d|zk)P(w|zk), (11) k where variables zk correspond to latent classes. After the estimation of the model using the EM algorithm, the Fisher kernel for this model is computed. The average log-likelihood of document d normalized by the document length is given by l(d) = X Pˆ (wj|d) log X P(wj|zk)P(zk|d), (12) j k where Pˆ(wj|d) = freq(wj,d).(13) Pm f req(wm, d) They use spherical parameterization (Kass and Vos, 1997) instead of the original parameters in the model. p They define parameters ρjk = 2 P (wj|zk) and ρk = </context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd Annual ACM Conference on Research and Development in Information Retrieval, pages 50–57, Berkeley, California, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Learning the similarity of documents: An information geometric approach to document retrieval and categorization.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems, 12,</booktitle>
<pages>914--920</pages>
<contexts>
<context position="4457" citStr="Hofmann (2000)" startWordPosition="681" endWordPosition="682">nd of Gaussian mixture. The reason for the selection of this model is that the resulting kernel has an advantage in computational time. The kernel based on this mixture model, what we call Hyperplane-based TOP (HP-TOP) kernel, can be computed efficiently in spite of its high dimensionality. We later show that the computational advantage is shared by a more general class of models. In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by Hofmann (2000) in terms of categorization accuracy. 2 SVMs and Kernel Method In this section, we explain SVMs and the kernel method, which are the basis of our research. SVMs have achieved high accuracy in various tasks including text categorization (Joachims, 1998; Dumais et al., 1998). Suppose a set Dl of ordered pairs consisting of a feature vector and its label Dl = {(x1, y1), (x2, y2), ··· , (xl, yl)}, (∀i, xi ∈ R|I|, yi ∈ {−1,1}) (1) is given. Dl is called training data. I is the set of feature indices. In SVMs, a separating hyperplane (f(x) = w · x − b) with the largest margin (the distance between t</context>
<context position="9139" citStr="Hofmann (2000)" startWordPosition="1504" endWordPosition="1505"> v(x, O) ≡ F−1(P(y = +1|x, O)) = log P(y = +1|x, O) − log P(y = −1|x, O). (6) The first-order Taylor expansion of v(x, θ∗) around the estimate ˆθ) (θ∗ i − ˆθi)∂v(x, . (7) ∂θi If fˆθ is of the following form: fˆθ(x) = ¡v(x, ˆθ), ∂v(x, ˆθ)/∂θ1, · · · , ∂v(x, ˆθ)/∂θp¢, (8) and if w and b are properly chosen as w = (1, θ∗1 − ˆθ1, ··· ,θ∗p − ˆθp), b = 0, (9) then (5) is approximately satisfied. Thus, the TOP kernel is defined as K(x1,x2) = fˆθ(x1) · fˆθ(x2). (10) A detailed discussion of the TOP kernel and its theoretical analysis have been given by Tsuda et al (Tsuda et al., 2002). 4 Related Work Hofmann (2000) applied Fisher kernels to text categorization under the Probabilistic Latent Semantic Indexing (PLSI) model (Hofmann, 1999). In PLSI, the joint probability of document d and word wis: P (d, w) = X P(zk)P(d|zk)P(w|zk), (11) k where variables zk correspond to latent classes. After the estimation of the model using the EM algorithm, the Fisher kernel for this model is computed. The average log-likelihood of document d normalized by the document length is given by l(d) = X Pˆ (wj|d) log X P(wj|zk)P(zk|d), (12) j k where Pˆ(wj|d) = freq(wj,d).(13) Pm f req(wm, d) They use spherical parameterizatio</context>
<context position="18692" citStr="Hofmann, 2000" startWordPosition="3118" endWordPosition="3119"> categories is actually 116. However, for small categories, reliable statistics cannot be obtained. For this reason, we regard the remaining categories other than the 10 most frequent categories as one category. Therefore, the model for negative examples is a mixture of 10 component models (9 out of the 10 most frequent categories and the new category consisting of the remaining categories). We assume uniform priors for categories as in (Tsuda et al., 2002). We computed the Fisher kernels with different numbers (10, 20 and 30) of latent classes and added them together to make a robust kernel (Hofmann, 2000). After the learning in the original feature space, the parameters for the probability distributions are estimated with 2Available from http://www.daviddlewis.com/resources/. maximum likelihood estimation as in Equations (19) and (20), followed by the learning with the proposed kernel. We used an SVM package, TinySVM3, for SVM computation. The soft-margin parameter C was set to 1.0 (other values of C showed no significant changes in results). The result is shown in Figure 1 (for macro-average) and Figure 2 (for micro-average). The HP-TOP kernel outperforms the linear kernel and the PLSI-based </context>
<context position="20462" citStr="Hofmann (2000)" startWordPosition="3399" endWordPosition="3401">he effect of word sense disambiguation would improve accuracy for large training datasets, the experiments do not provide us with an empirical evidence for the expectation. One possible reason is that Gaussian-type functions do not reflect the actual distribution of data. We leave its further investigation as future research. In this experimental setting, the PLSI-based Fisher kernel did not work well in terms of categorization accuracy. However, this Fisher kernel will perform better when the number of labeled examples is small and a number of unlabeled examples are available, as reported by Hofmann (2000). We also measured computational time of each method (Figure 3). The vertical axis indicates the average computational time over 100 runs of experiments (10 runs for each category). Please note that training time in this fig3Available from http://cl.aist-nara.ac.jp/˜taku-ku/software/TinySVM/. = di. (30) 10000 1000 100 0.1 10 1 HP-TOP Kernel Linear Kernel PLSI-based Fisher Kernel 82 80 78 76 74 72 70 68 66 64 HP-TOP Kernel Linear Kernel PLSI-based Fisher Kernel 1000 2000 3000 4000 5000 6000 7000 8000 Number of Labeled Examples Figure 1: Macro-average of F-measure 90 89 88 87 86 85 84 HP-TOP Ker</context>
</contexts>
<marker>Hofmann, 2000</marker>
<rawString>Thomas Hofmann. 2000. Learning the similarity of documents: An information geometric approach to document retrieval and categorization. In Advances in Neural Information Processing Systems, 12, pages 914– 920.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi Jaakkola</author>
<author>David Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers.</title>
<date>1998</date>
<booktitle>In Advances in Neural Information Processing Systems 11,</booktitle>
<pages>487--493</pages>
<contexts>
<context position="2255" citStr="Jaakkola and Haussler, 1998" startWordPosition="322" endWordPosition="325">8). In the formulation of SVMs, functions which measure the similarity of two examples take an important role. These functions are called kernelfunctions. The usual dot-product of two vectors respectively corresponding to two examples is often used. Although some variants to the usual dot-product are sometimes used (for example, higher-order polynomial kernels and RBF kernels), the distribution of examples is not taken into account in such kernels. However, new types of kernels have more recently been proposed; they are based on the probability distribution of examples. One is Fisher kernels (Jaakkola and Haussler, 1998). The other is TOP (Tangent vector Of the Posterior log-odds) kernels (Tsuda et al., 2002). While Fisher kernels are constructed on the basis of a generative model of data, TOP kernels are based on the classposterior probability, that is, the probability that the positive class occurs given an example. However, in order to use those kernels, we have to select a probabilistic model of data. The selection of a model will affect categorization result. The present paper provides one solution to this issue. Specifically, we proposed one type of TOP kernel, because it has been reported that TOP kern</context>
<context position="6569" citStr="Jaakkola and Haussler, 1998" startWordPosition="1030" endWordPosition="1033">ructure makes non-linear separation possible, although SVMs are basically linear classifiers. Another advantage of the kernel method is that although it deals with a high dimensional (possibly infinite) space, explicit computation of high dimensional vectors is not required. Only the general inner-products of two vectors need to be computed. This advantage leads to a relatively small computational overhead. 3 Kernels from Probabilistic Models Recently new type of kernels which connect generative models of data and discriminative classifiers such as SVMs, have been proposed: the Fisher kernel (Jaakkola and Haussler, 1998) and the TOP (Tangent vector Of the Posterior log-odds) kernel (Tsuda et al., 2002). 3.1 Fisher Kernel Suppose we have a probabilistic generative model p(x|O) of the data (we denote an example by x). The Fisher score of x is defined as ∇o log p(x|O), where ∇o means partial differentiation with respect to the parameters O. The Fisher information matrix is denoted by I(O) (this matrix defines the geometric structure of the model space). Then, the Fisher kernel at an estimate Oˆ is given by: K(x1,x2) = (∇o log p(x1|ˆO))tI−1(ˆO)(∇o log p(x2|ˆO)) (3) The Fisher score of an example approximately ind</context>
</contexts>
<marker>Jaakkola, Haussler, 1998</marker>
<rawString>Tommi Jaakkola and David Haussler. 1998. Exploiting generative models in discriminative classifiers. In Advances in Neural Information Processing Systems 11, pages 487–493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="4708" citStr="Joachims, 1998" startWordPosition="722" endWordPosition="723">tly in spite of its high dimensionality. We later show that the computational advantage is shared by a more general class of models. In the experiments of text categorization, in which SVMs are used as classifiers, our kernel outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model proposed by Hofmann (2000) in terms of categorization accuracy. 2 SVMs and Kernel Method In this section, we explain SVMs and the kernel method, which are the basis of our research. SVMs have achieved high accuracy in various tasks including text categorization (Joachims, 1998; Dumais et al., 1998). Suppose a set Dl of ordered pairs consisting of a feature vector and its label Dl = {(x1, y1), (x2, y2), ··· , (xl, yl)}, (∀i, xi ∈ R|I|, yi ∈ {−1,1}) (1) is given. Dl is called training data. I is the set of feature indices. In SVMs, a separating hyperplane (f(x) = w · x − b) with the largest margin (the distance between the hyperplane and its nearest vectors) is constructed. Skipping the details of SVMs’ formulation, here we just show the conclusion that, using some real numbers α∗i (∀i) and b∗, the optimal hyperplane is expressed as follows: f(x) = � α∗i yixi · x − b</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the 10th European Conference on Machine Learning, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Kass</author>
<author>Paul W Vos</author>
</authors>
<title>Geometrical foundations of asymptotic inference.</title>
<date>1997</date>
<publisher>Wiley.</publisher>
<location>New York :</location>
<contexts>
<context position="9761" citStr="Kass and Vos, 1997" startWordPosition="1605" endWordPosition="1608">plied Fisher kernels to text categorization under the Probabilistic Latent Semantic Indexing (PLSI) model (Hofmann, 1999). In PLSI, the joint probability of document d and word wis: P (d, w) = X P(zk)P(d|zk)P(w|zk), (11) k where variables zk correspond to latent classes. After the estimation of the model using the EM algorithm, the Fisher kernel for this model is computed. The average log-likelihood of document d normalized by the document length is given by l(d) = X Pˆ (wj|d) log X P(wj|zk)P(zk|d), (12) j k where Pˆ(wj|d) = freq(wj,d).(13) Pm f req(wm, d) They use spherical parameterization (Kass and Vos, 1997) instead of the original parameters in the model. p They define parameters ρjk = 2 P (wj|zk) and ρk = p 2 P (zk), and obtained Pˆ (wj|d)P(zk|d, wj) = p (14) P(wj|zk) , P(zk|d) ≈(15) pP(zk) . Thus, the Fisher kernel for this model is obtained as described in Appendix A. The first term of (31) corresponds to the similarity through latent spaces. The second term corresponds to the similarity through the distribution of each word. The number of latent classes zk can affect the value of the kernel function. In the experiment of (Hofmann, 2000), they computed the kernels with the different numbers (</context>
</contexts>
<marker>Kass, Vos, 1997</marker>
<rawString>Robert E. Kass and Paul W. Vos. 1997. Geometrical foundations of asymptotic inference. New York : Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Tsuda</author>
<author>Motoaki Kawanabe</author>
</authors>
<title>The leaveone-out kernel.</title>
<date>2002</date>
<booktitle>In Proceedings of International Conference on Artificial Neural Networks,</booktitle>
<pages>727--732</pages>
<contexts>
<context position="7449" citStr="Tsuda and Kawanabe, 2002" startWordPosition="1183" endWordPosition="1186">e ∇o means partial differentiation with respect to the parameters O. The Fisher information matrix is denoted by I(O) (this matrix defines the geometric structure of the model space). Then, the Fisher kernel at an estimate Oˆ is given by: K(x1,x2) = (∇o log p(x1|ˆO))tI−1(ˆO)(∇o log p(x2|ˆO)) (3) The Fisher score of an example approximately indicates how the model will change if the example is added to the training data used in the estimation of the model. That means, the Fisher kernel between two examples will be large, if the influences of the two examples to the model are similar and large (Tsuda and Kawanabe, 2002). The matrix I(O) is often approximated by the identity matrix to avoid large computational overhead. 3.2 TOP Kernel On the basis of a probabilistic model of the data, TOP kernels are designed to extract feature vectors fˆo which are considered to be useful for categorization with a separating hyperplane. We begin with the proposition that, between the generalization error R(fˆo) and the expected error of the posterior probability D(fˆo), the relation R(fˆo) − L∗ ≤ 2D(fˆo) holds, where L∗ is the Bayes error. This inequality means that minimizing D(fˆo) leads to reducing the generalization erro</context>
</contexts>
<marker>Tsuda, Kawanabe, 2002</marker>
<rawString>Koji Tsuda and Motoaki Kawanabe. 2002. The leaveone-out kernel. In Proceedings of International Conference on Artificial Neural Networks, pages 727–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Tsuda</author>
<author>Motoaki Kawanabe</author>
<author>Gunnar R¨atsch</author>
<author>S¨oren Sonnenburg</author>
<author>Klaus-Robert M¨uller</author>
</authors>
<title>A new discriminative kernel from probabilistic models.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>10</issue>
<marker>Tsuda, Kawanabe, R¨atsch, Sonnenburg, M¨uller, 2002</marker>
<rawString>Koji Tsuda, Motoaki Kawanabe, Gunnar R¨atsch, S¨oren Sonnenburg, and Klaus-Robert M¨uller. 2002. A new discriminative kernel from probabilistic models. Neural Computation, 14(10):2397–2414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="1629" citStr="Vapnik, 1998" startWordPosition="228" endWordPosition="229">ing to given categories. Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time. We also show that the computational advantage is shared by a more general class of models. In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model. 1 Introduction Recently, Support Vector Machines (SVMs) have been actively studied because of their high generalization ability (Vapnik, 1998). In the formulation of SVMs, functions which measure the similarity of two examples take an important role. These functions are called kernelfunctions. The usual dot-product of two vectors respectively corresponding to two examples is often used. Although some variants to the usual dot-product are sometimes used (for example, higher-order polynomial kernels and RBF kernels), the distribution of examples is not taken into account in such kernels. However, new types of kernels have more recently been proposed; they are based on the probability distribution of examples. One is Fisher kernels (Ja</context>
<context position="5570" citStr="Vapnik, 1998" startWordPosition="880" endWordPosition="881"> In SVMs, a separating hyperplane (f(x) = w · x − b) with the largest margin (the distance between the hyperplane and its nearest vectors) is constructed. Skipping the details of SVMs’ formulation, here we just show the conclusion that, using some real numbers α∗i (∀i) and b∗, the optimal hyperplane is expressed as follows: f(x) = � α∗i yixi · x − b∗. (2) i We should note that only dot-products of examples are used in the above expression. Since SVMs are linear classifiers, their separating ability is limited. To compensate for this limitation, the kernel method is usually combined with SVMs (Vapnik, 1998). In the kernel method, the dot-products in (2) are replaced with more general inner-products K(xi, x) (kernel functions). The polynomial kernel (xi·xj +1)d (d ∈ N+) and the RBF kernel exp{−kxi − xjk2/2σ2} are often used. Using the kernel method means that feature vectors are mapped into a (higher dimensional) Hilbert space and linearly separated there. This mapping structure makes non-linear separation possible, although SVMs are basically linear classifiers. Another advantage of the kernel method is that although it deals with a high dimensional (possibly infinite) space, explicit computatio</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. John Wiley, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>