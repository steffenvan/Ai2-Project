<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994562">
Connotation Lexicon:
A Dash of Sentiment Beneath the Surface Meaning
</title>
<author confidence="0.994779">
Song Feng Jun Seok Kang Polina Kuznetsova Yejin Choi
</author>
<affiliation confidence="0.959671">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.914119">
Stony Brook, NY 11794-4400
</address>
<email confidence="0.984054">
songfeng, junkang, pkuznetsova, ychoi@cs.stonybrook.edu
</email>
<sectionHeader confidence="0.994262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840166666667">
Understanding the connotation of words
plays an important role in interpreting sub-
tle shades of sentiment beyond denotative
or surface meaning of text, as seemingly
objective statements often allude nuanced
sentiment of the writer, and even purpose-
fully conjure emotion from the readers’
minds. The focus of this paper is draw-
ing nuanced, connotative sentiments from
even those words that are objective on the
surface, such as “intelligence”, “human”,
and “cheesecake”. We propose induction
algorithms encoding a diverse set of lin-
guistic insights (semantic prosody, distri-
butional similarity, semantic parallelism of
coordination) and prior knowledge drawn
from lexical resources, resulting in the first
broad-coverage connotation lexicon.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999964980769231">
There has been a substantial body of research
in sentiment analysis over the last decade (Pang
and Lee, 2008), where a considerable amount of
work has focused on recognizing sentiment that is
generally explicit and pronounced rather than im-
plied and subdued. However in many real-world
texts, even seemingly objective statements can be
opinion-laden in that they often allude nuanced
sentiment of the writer (Greene and Resnik, 2009),
or purposefully conjure emotion from the readers’
minds (Mohammad and Turney, 2010). Although
some researchers have explored formal and statis-
tical treatments of those implicit and implied sen-
timents (e.g. Wiebe et al. (2005), Esuli and Sebas-
tiani (2006), Greene and Resnik (2009), Davidov
et al. (2010)), automatic analysis of them largely
remains as a big challenge.
In this paper, we concentrate on understanding
the connotative sentiments of words, as they play
an important role in interpreting subtle shades of
sentiment beyond denotative or surface meaning
of text. For instance, consider the following:
Geothermal replaces oil-heating; it helps re-
ducing greenhouse emissions.1
Although this sentence could be considered as a
factual statement from the general standpoint, the
subtle effect of this sentence may not be entirely
objective: this sentence is likely to have an influ-
ence on readers’ minds in regard to their opinion
toward “geothermal”. In order to sense the subtle
overtone of sentiments, one needs to know that the
word “emissions” has generally negative connota-
tion, which geothermal reduces. In fact, depend-
ing on the pragmatic contexts, it could be precisely
the intention of the author to transfer his opinion
into the readers’ minds.
The main contribution of this paper is a broad-
coverage connotation lexicon that determines the
connotative polarity of even those words with ever
so subtle connotation beneath their surface mean-
ing, such as “Literature”, “Mediterranean”, and
“wine”. Although there has been a number of
previous work that constructed sentiment lexicons
(e.g., Esuli and Sebastiani (2006), Wilson et al.
(2005a), Kaji and Kitsuregawa (2007), Qiu et
al. (2009)), which seem to be increasingly and
inevitably expanding over words with (strongly)
connotative sentiments rather than explicit senti-
ments alone (e.g., “gun”), little prior work has di-
rectly tackled this problem of learning connota-
tion,2 and much of the subtle connotation of many
seemingly objective words is yet to be determined.
</bodyText>
<footnote confidence="0.997995">
1Our learned lexicon correctly assigns negative polarity to
emission.
2A notable exception would be the work of Feng et al.
</footnote>
<page confidence="0.863206">
1774
</page>
<note confidence="0.990007166666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774–1784,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
POSITIVE NEGATIVE
FEMA, Mandela, Intel, Google, Python, Sony, Pulitzer, Katrina, Monsanto, Halliburton, Enron, Teflon, Hi-
Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney, roshima, Holocaust, Afghanistan, Mugabe, Hutu, Sad-
Hoover, Goldman, Swarovski, Hawaii, Yellowstone dam, Osama, Qaeda, Kosovo, Helicobacter, HIV
</note>
<tableCaption confidence="0.99778">
Table 1: Example Named Entities (Proper Nouns) with Polar Connotation.
</tableCaption>
<bodyText confidence="0.995345925000001">
A central premise to our approach is that it is
collocational statistics of words that affect and
shape the polarity of connotation. Indeed, the ety-
mology of “connotation” is from the Latin “com-
” (“together or with”) and “notare” (“to mark”).
It is important to clarify, however, that we do not
simply assume that words that collocate share the
same polarity of connotation. Although such an
assumption played a key role in previous work for
the analogous task of learning sentiment lexicon
(Velikovich et al., 2010), we expect that the same
assumption would be less reliable in drawing sub-
tle connotative sentiments of words. As one ex-
ample, the predicate “cure”, which has a positive
connotation typically takes arguments with nega-
tive connotation, e.g., “disease”, when used as the
“relieve” sense.3
Therefore, in order to attain a broad cover-
age lexicon while maintaining good precision, we
guide the induction algorithm with multiple, care-
fully selected linguistic insights: [1] distributional
similarity, [2] semantic parallelism of coordina-
tion, [3] selectional preference, and [4] seman-
tic prosody (e.g., Sinclair (1991), Louw (1993),
Stubbs (1995), Stefanowitsch and Gries (2003))),
and also exploit existing lexical resources as an ad-
ditional inductive bias.
We cast the connotation lexicon induction task
as a collective inference problem, and consider ap-
proaches based on three distinct types of algorith-
mic framework that have been shown successful
for conventional sentiment lexicon induction:
Random walk based on HITS/PageRank (e.g.,
Kleinberg (1999), Page et al. (1999), Feng
et al. (2011) Heerschop et al. (2011),
Montejo-R´aez et al. (2012))
Label/Graph propagation (e.g., Zhu and Ghahra-
(2011) but with practical limitations. See §3 for detailed dis-
cussion.
3Note that when “cure” is used as the “preserve” sense, it
expects objects with non-negative connotation. Hence word-
sense-disambiguation (WSD) presents a challenge, though
not unexpectedly. In this work, we assume the general conno-
tation of each word over statistically prevailing senses, leav-
ing a more cautious handling of WSD as future work.
mani (2002), Velikovich et al. (2010))
Constraint optimization (e.g., Roth and Yih
(2004), Choi and Cardie (2009), Lu et al.
(2011)).
We provide comparative empirical results over
several variants of these approaches with compre-
hensive evaluations including lexicon-based, hu-
man judgments, and extrinsic evaluations.
It is worthwhile to note that not all words have
connotative meanings that are distinct from deno-
tational meanings, and in some cases, it can be dif-
ficult to determine whether the overall sentiment is
drawn from denotational or connotative meanings
exclusively, or both. Therefore, we encompass any
sentiment from either type of meanings into the
lexicon, where non-neutral polarity prevails over
neutral one if some meanings lead to neutral while
others to non-neutral.4
Our work results in the first broad-coverage
connotation lexicon,5 significantly improving both
the coverage and the precision of Feng et al.
(2011). As an interesting by-product, our algo-
rithm can be also used as a proxy to measure the
general connotation of real-world named entities
based on their collocational statistics. Table 1
highlights some example proper nouns included in
the final lexicon.
The rest of the paper is structured as follows.
In §2 we describe three types of induction algo-
rithms followed by evaluation in §3. Then we re-
visit the induction algorithms based on constraint
optimization in §4 to enhance quality and scala-
bility. §5 presents comprehensive evaluation with
human judges and extrinsic evaluations. Related
work and conclusion are in §6 and §7.
</bodyText>
<footnote confidence="0.997513">
4In general, polysemous words do not seem to have con-
flicting non-neutral polarities over different senses, though
there are many exceptions, e.g., “heat”, or “fine”. We treat
each word in each part-of-speech as a separate word to reduce
such cases, otherwise aim to learn the most prevalent polar-
ity in the corpus with respect to each part-of-speech of each
word.
5Available at http://www.cs.stonybrook.edu/
˜ychoi/connotation.
</footnote>
<page confidence="0.988051">
1775
</page>
<figure confidence="0.6928125">
pred-arg
distr sim
</figure>
<figureCaption confidence="0.999903">
Figure 1: Graph for Graph Propagation (§2.2).
Figure 2: Graph for ILP/LP (§2.3, §4.2).
</figureCaption>
<figure confidence="0.998429161290323">
Prod-Arg
enjoy
thank
Arg-Arg
writing
profit
help
...
investment
reading
aid
prevent
suffer
enjoy
thank
tax
loss
writing
profit
preventing
...
...
investment
flu
bonus
cold
gain
pred-arg
distr sim
synonyms
antonyms
</figure>
<sectionHeader confidence="0.956534" genericHeader="method">
2 Connotation Induction Algorithms
</sectionHeader>
<bodyText confidence="0.999767222222222">
We develop induction algorithms based on three
distinct types of algorithmic framework that have
been shown successful for the analogous task of
sentiment lexicon induction: HITS &amp; PageRank
(§2.1), Label/Graph Propagation (§2.2), and Con-
straint Optimization via Integer Linear Program-
ming (§2.3). As will be shown, each of these ap-
proaches will incorporate additional, more diverse
linguistic insights.
</bodyText>
<subsectionHeader confidence="0.974231">
2.1 HITS &amp; PageRank
</subsectionHeader>
<bodyText confidence="0.999985083333333">
The work of Feng et al. (2011) explored the use
of HITS (Kleinberg, 1999) and PageRank (Page
et al., 1999) to induce the general connotation
of words hinging on the linguistic phenomena of
selectional preference and semantic prosody, i.e.,
connotative predicates influencing the connotation
of their arguments. For example, the object of
a negative connotative predicate “cure” is likely
to have negative connotation, e.g., “disease” or
“cancer”. The bipartite graph structure for this
approach corresponds to the left-most box (labeled
as “pred-arg”) in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999367">
2.2 Label Propagation
</subsectionHeader>
<bodyText confidence="0.997486142857143">
With the goal of obtaining a broad-coverage lexi-
con in mind, we find that relying only on the struc-
ture of semantic prosody is limiting, due to rel-
atively small sets of connotative predicates avail-
able.6 Therefore, we extend the graph structure
as an overlay of two sub-graphs (Figure 1) as de-
scribed below:
</bodyText>
<footnote confidence="0.991562333333333">
6For connotative predicates, we use the seed predicate set
of Feng et al. (2011), which comprises of 20 positive and 20
negative predicates.
</footnote>
<subsectionHeader confidence="0.584026">
Sub-graph #1: Predicate–Argument Graph
</subsectionHeader>
<bodyText confidence="0.998658555555556">
This sub-graph is the bipartite graph that encodes
the selectional preference of connotative predi-
cates over their arguments. In this graph, conno-
tative predicates p reside on one side of the graph
and their co-occurring arguments a reside on the
other side of the graph based on Google Web 1T
corpus.7 The weight on the edges between the
predicates p and arguments a are defined using
Point-wise Mutual Information (PMI) as follows:
</bodyText>
<equation confidence="0.999459666666667">
P(p, a)
w(p → a) := PMI(p, a) = log,
P(p)P(a)
</equation>
<bodyText confidence="0.998616333333333">
PMI scores have been widely used in previous
studies to measure association between words
(e.g., Turney (2001), Church and Hanks (1990)).
</bodyText>
<subsectionHeader confidence="0.857276">
Sub-graph #2: Argument–Argument Graph
</subsectionHeader>
<bodyText confidence="0.99880425">
The second sub-graph is based on the distribu-
tional similarities among the arguments. One pos-
sible way of constructing such a graph is simply
connecting all nodes and assign edge weights pro-
portionate to the word association scores, such as
PMI, or distributional similarity. However, such a
completely connected graph can be susceptible to
propagating noise, and does not scale well over a
very large set of vocabulary.
We therefore reduce the graph connectivity by
exploiting semantic parallelism of coordination
(Bock (1986), Hatzivassiloglou and McKeown
7We restrict predicte-argument pairs to verb-object pairs
in this study. Note that Google Web 1T dataset consists of
n-grams upto n = 5. Since n-gram sequences are too short
to apply a parser, we extract verb-object pairs approximately
by matching part-of-speech tags. Empirically, when overlaid
with the second sub-graph, we found that it is better to keep
the connectivity of this sub-graph as uni-directional. That is,
we only allow edges to go from a predicate to an argument.
</bodyText>
<page confidence="0.701467">
1776
</page>
<bodyText confidence="0.967835285714286">
POSITIVE NEGATIVE NEUTRAL
n. avatar, adrenaline, keynote, debut, unbeliever, delay, shortfall, gun- header, mark, clothing, outline,
stakeholder, sunshine, cooperation shot, misdemeanor, mutiny, rigor grid, gasoline, course, preview
v. handcraft, volunteer, party, ac- sentence, cough, trap, scratch, de- state, edit, send, put, arrive, type,
credit, personalize, nurse, google bunk, rip, misspell, overcharge drill, name, stay, echo, register
a. floral, vegetarian, prepared, age- debilitating, impaired, swollen, same, cerebral, west, uncut, auto-
less, funded, contemporary intentional, jarring, unearned matic, hydrated, unheated, routine
</bodyText>
<tableCaption confidence="0.955627">
Table 2: Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a).
</tableCaption>
<bodyText confidence="0.9986086">
(1997), Pickering and Branigan (1998)). In par-
ticular, we consider an undirected edge between a
pair of arguments a1 and a2 only if they occurred
together in the “a1 and a2” or “a2 and a1” coor-
dination, and assign edge weights as:
</bodyText>
<equation confidence="0.970842">
w(a1 − a2) = CosineSim(−→a1, →−a2) = ||−→a1  |−→a2||
</equation>
<bodyText confidence="0.999863857142857">
where →−a1 and →−a2 are co-occurrence vectors for a1
and a2 respectively. The co-occurrence vector for
each word is computed using PMI scores with re-
spect to the top n co-occurring words.8 n (=50)
is selected empirically. The edge weights in two
sub-graphs are normalized so that they are in the
comparable range.9
</bodyText>
<subsectionHeader confidence="0.990018">
Limitations of Graph-based Algorithms
</subsectionHeader>
<bodyText confidence="0.708760545454545">
Although graph-based algorithms (§2.1, §2.2) pro-
vide an intuitive framework to incorporate various
lexical relations, limitations include:
1. They allow only non-negative edge weights.
Therefore, we can encode only positive (sup-
portive) relations among words (e.g., distri-
butionally similar words will endorse each
other with the same polarity), while miss-
ing on exploiting negative relations (e.g.,
antonyms may drive each other into the op-
posite polarity).
</bodyText>
<listItem confidence="0.993553571428571">
2. They induce positive and negative polarities
in isolation via separate graphs. However, we
expect that a more effective algorithm should
induce both polarities simultaneously.
3. The framework does not readily allow incor-
porating a diverse set of soft and hard con-
straints.
</listItem>
<bodyText confidence="0.884338222222222">
8We discard edges with cosine similarity ≤ 0, as those
indicate either independence or the opposite of similarity.
9Note that cosine similarity does not make sense for the
first sub-graph as there is no reason why a predicate and an ar-
gument should be distributionally similar. We experimented
with many different variations on the graph structure and
edge weights, including ones that include any word pairs that
occurred frequently enough together. For brevity, we present
the version that achieved the best results here.
</bodyText>
<subsectionHeader confidence="0.998994">
2.3 Constraint Optimization
</subsectionHeader>
<bodyText confidence="0.981161181818182">
Addressing limitations of graph-based algorithms
(§2.2), we propose an induction algorithm based
on Integer Linear Programming (ILP). Figure 2
provides the pictorial overview. In comparison to
Figure 1, two new components are: (1) dictionary-
driven relations targeting enhanced precision, and
(2) dictionary-driven words (i.e., unseen words
with respect to those relations explored in Figure
1) targeting enhanced coverage. We formulate in-
sights in Figure 2 using ILP as follows:
Definition of sets of words:
</bodyText>
<listItem confidence="0.99488025">
1. P+: the set of positive seed predicates.
P−: the set of negative seed predicates.
2. S: the set of seed sentiment words.
3. Rsyn: word pairs in synonyms relation.
Rant: word pairs in antonyms relation.
Rcoord: word pairs in coordination relation.
Rpred: word pairs in pred-arg relation.
Rpred+(−): Rpred based on P+ (P−).
</listItem>
<bodyText confidence="0.994052">
Definition of variables: For each word i, we
define binary variables xi, yi, zi E {0, 1}, where
xi = 1 (yi = 1, zi = 1) if and only if i has a pos-
itive (negative, neutral) connotation respectively.
For every pair of word i and j, we define binary
variables dpq ijwhere p, q E {+, −, 0} and dpq ij= 1
if and only if the polarity of i and j are p and q
respectively.
</bodyText>
<equation confidence="0.72505">
Objective function: We aim to maximize:
F = Φprosody + Φcoord + Φneu
</equation>
<bodyText confidence="0.9966348">
where Φprosody is the scores based on semantic
prosody, Φcoord captures the distributional similar-
ity over coordination, and Φneu controls the sen-
sitivity of connotation detection between positive
(negative) and neutral. In particular,
</bodyText>
<equation confidence="0.968901157894737">
wpred
i,j (d++
i,j + d−−
i,j − d+−
i,j − d−+
i,j )
wi,j (d++
coord i,j + d−−
i,j + d00
i,j)
a2→−
→−a1 ·
Rpred
�
i,j
Φprosody =
Rcoord
Φcoord = E
i,j
</equation>
<page confidence="0.751663">
1777
</page>
<figure confidence="0.696582333333333">
Φneu = α Rpred wpred 3.1 Comparison against Conventional
E i,j ·zj Sentiment Lexicon
i,j
</figure>
<bodyText confidence="0.780735">
Soft constraints (edge weights): The weights in
the objective function are set as follows:
</bodyText>
<equation confidence="0.985917666666667">
freq(p, a)
wpred(p, a) =
||−→a1  |−→a2||
</equation>
<bodyText confidence="0.992676">
Note that the same wcoord(a1,a2) has been used
in graph propagation described in Section 2.2. α
controls the sensitivity of connotation detection
such that higher value of α will promote neutral
connotation over polar ones.
Hard constrains for variable consistency:
</bodyText>
<listItem confidence="0.9987885">
1. Each word i has one of {+, −, ø1 as polarity:
bi, xi + yi + zi = 1
2. Variable consistency between dpq
ij and
</listItem>
<equation confidence="0.9975505">
xi, yi, zi:
xi + xj − 1 &lt; 2d++ i,j&lt; xi + xj
yi + yj − 1 &lt; 2d−−
i,j &lt; yi + yj
zi + zj − 1 &lt; 2d00
i,j &lt; zi + zj
xi + yj − 1 &lt; 2d+−
i,j &lt; xi + yj
yi + xj − 1 &lt; 2d−+
i,j &lt; yi + xj
</equation>
<bodyText confidence="0.804198">
Hard constrains for WordNet relations:
</bodyText>
<listItem confidence="0.776588666666667">
1. Cant: Antonym pairs will not have the same
positive or negative polarity:
b(i, j) E Rant, xi + xj &lt; 1, yi + yj &lt; 1
</listItem>
<bodyText confidence="0.962286166666667">
For this constraint, we only consider
antonym pairs that share the same root, e.g.,
“sufficient” and “insufficient”, as those pairs
are more likely to have the opposite polarities
than pairs without sharing the same root, e.g.,
“east” and “west”.
</bodyText>
<listItem confidence="0.815353666666667">
2. Csyn: Synonym pairs will not have the oppo-
site polarity:
b(i, j) E Rsyn, xi + yj &lt; 1, xj + yi &lt; 1
</listItem>
<sectionHeader confidence="0.982193" genericHeader="method">
3 Experimental Result I
</sectionHeader>
<bodyText confidence="0.999881816326531">
We provide comprehensive comparisons over vari-
ants of three types of algorithms proposed in §2.
We use the Google Web 1T data (Brants and Franz
(2006)), and POS-tagged ngrams using Stanford
POS Tagger (Toutanova and Manning (2000)). We
filter out the ngrams with punctuations and other
special characters to reduce the noise.
Note that we consider the connotation lexicon to
be inclusive of a sentiment lexicon for two prac-
tical reasons: first, it is highly unlikely that any
word with non-neutral sentiment (i.e., positive or
negative) would carry connotation of the oppo-
site, i.e., conflicting10 polarity. Second, for some
words with distinct sentiment or strong connota-
tion, it can be difficult or even unnatural to draw a
precise distinction between connotation and senti-
ment, e.g., “efficient”. Therefore, sentiment lexi-
cons can serve as a surrogate to measure a subset
of connotation words induced by the algorithms,
as shown in Table 3 with respect to General In-
quirer (Stone and Hunt (1963)) and MPQA (Wil-
son et al. (2005b)).11
Discussion Table 3 shows the agreement statis-
tics with respect to two conventional sentiment
lexicons. We find that the use of label propaga-
tion alone [PRED-ARG (CP)] improves the per-
formance substantially over the comparable graph
construction with different graph analysis algo-
rithms, in particular, HITS and PageRank ap-
proaches of Feng et al. (2011). The two com-
pletely connected variants of the graph propa-
gation on the Pred-Arg graph, [® PRED-ARG
(PMI)] and [® PRED-ARG (CP)], do not neces-
sarily improve the performance over the simpler
and computationally lighter alternative, [PRED-
ARG (CP)]. The [OVERLAY], which is based
on both Pred-Arg and Arg-Arg subgraphs (§2.2),
achieves the best performance among graph-based
algorithms, significantly improving the precision
over all other baselines. This result suggests:
1 The sub-graph #2, based on the semantic par-
allelism of coordination, is simple and yet
very powerful as an inductive bias.
2 The performance of graph propagation varies
significantly depending on the graph topol-
ogy and the corresponding edge weights.
Note that a direct comparison against ILP for top
N words is tricky, as ILP does not rank results.
Only for comparison purposes however, we assign
</bodyText>
<footnote confidence="0.9149326">
10We consider “positive” and “negative” polarities conflict,
but “neutral” polarity does not conflict with any.
11In the case of General Inquirer, we use words in POSITIV
and NEGATIV sets as words with positive and negative labels
respectively.
</footnote>
<equation confidence="0.8153232">
E freq(p, x)
(p,x)∈Rpred
wcoord(a1, a2) = CosSim(−→a1, →−a2) =
→−a2
→−a1 ·
</equation>
<page confidence="0.768509">
1778
</page>
<table confidence="0.999904">
100 GENINQ EVAL ALL 100 MPQA EVAL ALL
1,000 5,000 10,000 1,000 5,000 10,000
ILP 97.6 94.5 84.5 80.8 80.4 98.0 89.7 84.6 81.2 78.4
OVERLAY 97.0 95.1 78.8 (78.3) 78.3 98.0 93.4 82.1 77.7 77.7
N PRED-ARG (PMI) 91.0 91.4 76.1 (76.1) 76.1 88.0 89.1 78.8 75.1 75.1
NPRED-ARG (CP) 88.0 85.4 76.2 (76.2) 76.2 87.0 82.6 78.0 76.3 76.3
PRED-ARG (CP) 91.0 91.0 81.0 (81.0) 81.0 88.0 91.5 80.0 78.3 78.3
HITS-ASYMT 77.0 68.8 - - 66.5 86.3 81.3 - - 72.2
PAGERANK-ASYMF 77.0 68.5 - - 65.7 87.2 80.3 - - 72.3
</table>
<tableCaption confidence="0.999947">
Table 3: Evaluation of Induction Algorithms (§2) with respect to Sentiment Lexicons (precision%).
</tableCaption>
<bodyText confidence="0.999867428571429">
ranks based on the frequency of words for ILP. Be-
cause of this issue, the performance of top ∼1k
words of ILP should be considered only as a con-
servative measure. Importantly, when evaluated
over more than top 5k words, ILP is overall the
top performer considering both precision (shown
in Table 3) and coverage (omitted for brevity).12
</bodyText>
<sectionHeader confidence="0.944195" genericHeader="method">
4 Precision, Coverage, and Efficiency
</sectionHeader>
<bodyText confidence="0.999886846153846">
In this section, we address three important aspects
of an ideal induction algorithm: precision, cover-
age, and efficiency. For brevity, the remainder of
the paper will focus on the algorithms based on
constraint optimization, as it turned out to be the
most effective one from the empirical results in §3.
Precision In order to see the effectiveness of the
induction algorithms more sharply, we had used a
limited set of seed words in §3. However to build a
lexicon with substantially enhanced precision, we
will use as a large seed set as possible, e.g., entire
sentiment lexicons13.
Broad coverage Although statistics in Google
1T corpus represent a very large amount of text,
words that appear in pred-arg and coordination re-
lations are still limited. To substantially increase
the coverage, we will leverage dictionary words
(that are not in the corpus) as described in §2.3
and Figure 2.
Efficiency One practical problem with ILP is ef-
ficiency and scalability. In particular, we found
that it becomes nearly impractical to run the ILP
formulation including all words in WordNet plus
all words in the argument position in Google Web
1T. We therefore explore an alternative approach
based on Linear Programming in what follows.
</bodyText>
<footnote confidence="0.9984378">
12In fact, the performance of PRED-ARG variants for top
10K w.r.t. GENINQ is not meaningful as no additional word
was matched beyond top 5k words.
13Note that doing so will prevent us from evaluating
against the same sentiment lexicon used as a seed set.
</footnote>
<subsectionHeader confidence="0.999235">
4.1 Induction using Linear Programming
</subsectionHeader>
<bodyText confidence="0.965772444444444">
One straightforward option for Linear Program-
ming formulation may seem like using the same
Integer Linear Programming formulation intro-
duced in §2.3, only changing the variable defini-
tions to be real values E [0, 1] rather than integers.
However, because the hard constraints in §2.3 are
defined based on the assumption that all the vari-
ables are binary integers, those constraints are not
as meaningful when considered for real numbers.
Therefore we revise those hard constraints to en-
code various semantic relations (WordNet and se-
mantic coordination) more directly.
Definition of variables: For each word i, we de-
fine variables xi, yi, zi E [0, 1]. i has a positive
(negative) connotation if and only if the xi (yi) is
assigned the greatest value among the three vari-
ables; otherwise, i is neutral.
Objective function: We aim to maximize:
</bodyText>
<equation confidence="0.9985596">
F = Φprosody + Φcoord + Φsyn + Φant + Φneu
Φprosody = Rpred+ wpred+ Rpred− p red− yj
X i,j · xj + X wi,j
i,j i,j
wcoord
i,j · (dc++
i,j + dc−−
i,j )
Φsyn = Wsyn
Φant = Want
</equation>
<bodyText confidence="0.895367636363636">
Hard constraints We add penalties to the
objective function if the polarity of a pair of words
is not consistent with its corresponding semantic
relations. For example, for synonyms i and j, we
introduce a penalty Wsyn (a positive constant) for
ds
i,j , ds−−
i,j E [−1, 0], where we set the upper
bound of ds
i,j (ds−−
i,j ) as the signed distance of
</bodyText>
<equation confidence="0.987453368421053">
Rcoord
Φcoord = X
i,j
(ds++
i,j + ds−−
i,j )
(da++
i,j + da−−i,j )
Rsyn X
i,j
Rant
X
i,j
Rpred
X
i,j
Φneu = α
wpred
i,j ·zj
</equation>
<page confidence="0.984985">
1779
</page>
<table confidence="0.9993922">
FORMULA POSITIVE NEGATIVE ALL
R P F R P F R P F
ILP Φprosody + Csyn + Cant 51.4 85.7 64.3 44.7 87.9 59.3 48.0 86.8 61.8
Φprosody + Csyn + Cant + CS 61.2 93.3 73.9 52.4 92.2 66.8 56.8 92.8 70.5
Φprosody + Φcoord + Csyn + Cant 67.3 75.0 70.9 53.7 84.4 65.6 60.5 79.7 68.8
Φprosody + Φcoord + Csyn + Cant + CS 62.2 96.0 75.5 51.5 89.5 65.4 56.9 92.8 70.5
LP Φprosody + Φsyn + Φant 24.4 76.0 36.9 23.6 78.8 36.3 24.0 77.4 36.6
Φprosody + Φsyn + Φant + ΦS 71.6 87.8 78.9 68.8 84.6 75.9 70.2 86.2 77.4
Φprosody + Φcoord + Φsyn + Φant 67.9 92.6 78.3 64.6 89.1 74.9 66.3 90.8 76.6
Φprosody + Φcoord + Φsyn + Φant + ΦS 78.6 90.5 84.1 73.3 87.1 79.6 75.9 88.8 81.8
</table>
<tableCaption confidence="0.997702">
Table 4: ILP/LP Comparison on MQPA0 (%).
</tableCaption>
<bodyText confidence="0.863761">
xi and xj (yi and yj) as shown below:
</bodyText>
<equation confidence="0.955148857142857">
For (i, j) E Rsyn,
ds++
i,j &lt; xi − xj, ds++
i,j &lt; xj − xi
ds−−
i,j &lt; yi − yj, ds−−
i,j &lt; yj − yi
</equation>
<bodyText confidence="0.936164583333333">
Notice that ds++
i,j , ds−−
i,j satisfying above inequal-
ities will be always of negative values, hence in
order to maximize the objective function, the LP
solver will try to minimize the absolute values of
ds++
i,j , ds−−
i,j , effectively pushing i and j toward
the same polarity. Constraints for semantic coor-
dination Rcoord can be defined similarly. Lastly,
following constraints encode antonym relations:
</bodyText>
<equation confidence="0.976443666666667">
For (i, j) E Rant ,
da++
i,j &lt; xi − (1 − xj), da++
i,j &lt; (1 − xj) − xi
da−−
i,j &lt; yi − (1 − yj), da−−
</equation>
<bodyText confidence="0.985016923076923">
i,j &lt; (1 − yj) − yi
Interpretation Unlike ILP, some of the vari-
ables result in fractional values. We consider a
word has positive or negative polarity only if the
assignment indicates 1 for the corresponding po-
larity and 0 for the rest. In other words, we treat
all words with fractional assignments over differ-
ent polarities as neutral. Because the optimal so-
lutions of LP correspond to extreme points in the
convex polytope formed by the constraints, we ob-
tain a large portion of words with non-fractional
assignments toward non-neutral polarities. Alter-
natively, one can round up fractional values.
</bodyText>
<subsectionHeader confidence="0.905081">
4.2 Empirical Comparisons: ILP v.s. LP
</subsectionHeader>
<bodyText confidence="0.9992026">
To solve the ILP/LP, we run ILOG CPLEX Opti-
mizer (CPLEX, 2009)) on a 3.5GHz 6 core CPU
machine with 96GB RAM. Efficiency-wise, LP
runs within 10 minutes while ILP takes several
hours. Table 4 shows the results evaluated against
MPQA for different variations of ILP and LP.
We find that LP variants much better recall and
F-score, while maintaining comparable precision.
Therefore, we choose the connotation lexicon by
LP (C-LP) in the following evaluations in §5.
</bodyText>
<sectionHeader confidence="0.99712" genericHeader="method">
5 Experimental Results II
</sectionHeader>
<bodyText confidence="0.984378625">
In this section, we present comprehensive intrin-
sic §5.1 and extrinsic §5.2 evaluations comparing
three representative lexicons from §2 &amp; §4: C-
LP, OVERLAY, PRED-ARG (CP), and two popular
sentiment lexicons: SentiWordNet (Baccianella et
al., 2010) and GI+MPQA.14 Note that C-LP is the
largest among all connotation lexicons, including
70,000 polar words.15
</bodyText>
<subsectionHeader confidence="0.964686">
5.1 Intrinsic Evaluation: Human Judgements
</subsectionHeader>
<bodyText confidence="0.880997172413793">
We evaluate 4000 words16 using Amazon Me-
chanical Turk (AMT). Because we expect that
judging a connotation can be dependent on one’s
cultural background, personality and value sys-
tems, we gather judgements from 5 people for
each word, from which we hope to draw a more
general judgement of connotative polarity. About
300 unique Turkers participated the evaluation
tasks. We gather gold standard only for those
words for which more than half of the judges
agreed on the same polarity. Otherwise we treat
them as ambiguous cases.17 Figure 3 shows a part
of the AMT task, where Turkers are presented with
questions that help judges to determine the subtle
connotative polarity of each word, then asked to
rate the degree of connotation on a scale from -
5 (most negative) and 5 (most positive). To draw
14GI+MPQA is the union of General Inquirer and MPQA.
The GI, we use words in the “Positiv” &amp; “Negativ” set. For
SentiWordNet, to retrieve the polarity of a given word, we
sum over the polarity scores over all senses, where positive
(negative) values correspond to positive (negative) polarity.
15-13k adj, -6k verbs, -28k nouns, -22k proper nouns.
16We choose words that are not already in GI+MPQA and
obtain most frequent 10,000 words based on the unigram fre-
quency in Google-Ngram, then randomly select 4000 words.
17We allow Turkers to mark words that can be used with
both positive and negative connotation, which results in about
7% of words that are excluded from the gold standard set.
</bodyText>
<page confidence="0.988565">
1780
</page>
<figureCaption confidence="0.996281">
Figure 3: A Part of AMT Task Design.
</figureCaption>
<table confidence="0.997531">
QUESTION YES NO Avg
% Avg %
“Enjoyable or pleasant” 43.3 2.9 16.3 -2.4
“Of a good quality” 56.7 2.5 6.1 -2.7
“Respectable / honourable” 21.0 3.3 14.0 -1.1
“Would like to do or have” 52.5 2.8 11.5 -2.4
</table>
<tableCaption confidence="0.995444">
Table 5: Distribution of Answers from AMT.
</tableCaption>
<listItem confidence="0.821402875">
the gold standard, we consider two different voting
schemes:
• QV ote: The judgement of each Turker is
mapped to neutral for −1 &lt; score &lt; 1, pos-
itive for score &gt; 2, negative for score &lt; 2,
then we take the majority vote.
• QS&amp;quot;e: Let Q(i) be the sum (weighted vote)
of the scores given by 5 judges for word i.
</listItem>
<bodyText confidence="0.805616">
Then we determine the polarity label l(i) of i
as:
</bodyText>
<equation confidence="0.956247333333333">
positive if Q(i) &gt; 1
negative if Q(i) &lt; −1
neutral if −1 &lt; Q(i) &lt; 1
</equation>
<bodyText confidence="0.92729965">
The resulting distribution of judgements is shown
in Table 5 &amp; 6. Interestingly, we observe
that among the relatively frequently used English
words, there are overwhelmingly more positively
connotative words than negative ones.
In Table 7, we show the percentage of words
with the same label over the mutual words by the
two lexicon. The highest agreement is 77% by
C-LP and the gold standard by AMTV ote. How
good is this? It depends on what is the natural de-
gree of agreement over subtle connotation among
people. Therefore, we also report the degree of
agreement among human judges in Table 7, where
we compute the agreement of one Turker with re-
spect to the gold standard drawn from the rest of
the Turkers, and take the average across over all
five Turkers18. Interestingly, the performance of
18In order to draw the gold standard from the 4 remaining
Turkers, we consider adjusted versions of ΩV ote and ΩScore
schemes described above.
</bodyText>
<table confidence="0.742580333333333">
POS NEG NEU UNDETERMINED
ΩV ote
ΩScore
</table>
<tableCaption confidence="0.670483">
Table 6: Distribution of Connotative Polarity from
AMT.
</tableCaption>
<table confidence="0.984371666666667">
C-LP SENTIWN HUMAN JUDGES
77.0 71.5 66.0
73.0 69.0 69.0
</table>
<tableCaption confidence="0.9351305">
Table 7: Agreement (Accuracy) against AMT-
driven Gold Standard.
</tableCaption>
<bodyText confidence="0.992723285714286">
Turkers is not as good as that of C-LP lexicon. We
conjecture that this could be due to generally vary-
ing perception of different people on the connota-
tive polarity,19 while the corpus-driven induction
algorithms focus on the general connotative po-
larity corresponding to the most prevalent senses
of words in the corpus.
</bodyText>
<subsectionHeader confidence="0.997639">
5.2 Extrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.991619">
We conduct lexicon-based binary sentiment clas-
sification on the following two corpora.
SemEval From the SemEval task, we obtain a
set of news headlines with annotated scores (rang-
ing from -100 to 87). The positive/negative scores
indicate the degree of positive/negative polarity
orientation. We construct several sets of the posi-
tive and negative texts by setting thresholds on the
scores as shown in Table 8. “≶ n” indicates that
the positive set consists of the texts with scores
&gt; n and the negative set consists of the texts with
scores &lt; −n.
Emoticon tweets The sentiment Twitter data20
consists of tweets containing either a smiley
emoticon (positive sentiment) or a frowny emoti-
con (negative sentiment). We filter out the tweets
with question marks or more than 30 words, and
keep the ones with at least two words in the union
of all polar words in the five lexicons in Table 8,
and then randomly select 10000 per class.
We denote the short text (e.g., content of tweets
or headline texts from SemEval) by t. w repre-
sents the word in t. W+/W− is the set of posi-
19Pearson correlation coefficient among turkers is 0.28,
which corresponds to a positive small to medium correlation.
Note that when the annotation of turkers is aggregated, we
observe agreement as high as 77% with respect to the learned
connotation lexicon.
</bodyText>
<footnote confidence="0.9217205">
20http://www.stanford.edu/˜alecmgo/
cs224n/twitterdata.2009.05.25.c.zip
</footnote>
<figure confidence="0.652712125">
⎧
⎨
⎩
l(i) =
50.4 14.6 24.1 10.9
67.9 20.6 11.5 n/a
ΩV ote
ΩScore
</figure>
<page confidence="0.612479">
1781
</page>
<table confidence="0.999398">
DATA
LEXICON TWEET SEMEVAL
≶20 ≶40 ≶60 ≶80
C-LP 70.1 70.8 74.6 80.8 93.5
OVERLAY 68.5 70.0 72.9 76.8 89.6
PRED-ARG (CP) 60.5 64.2 69.3 70.3 79.2
SENTIWN 67.4 61.0 64.5 70.5 79.0
GI+MPQA 65.0 64.5 69.0 74.0 80.5
</table>
<tableCaption confidence="0.7978095">
Table 8: Accuracy on Sentiment Classification
(%).
</tableCaption>
<bodyText confidence="0.99922125">
tive/negative words of the lexicon. We define the
weight of w as s(w). If w is adjective, s(w) = 2;
otherwise s(w) = 1. Then the polarity of each text
is determined as follows:
</bodyText>
<equation confidence="0.5840725">
pol(t) = { positive if
negative if
</equation>
<bodyText confidence="0.999936285714286">
As shown in Table 8, C-LP generally performs
better than the other lexicons on both corpora.
Considering that only very simple classification
strategy is applied, the result by the connotation
lexicon is quite promising.
Finally, Table 1 highlights interesting exam-
ples of proper nouns with connotative polarity,
e.g., “Mandela”, “Google”, “Hawaii” with pos-
itive connotation, and “Monsanto”, “Hallibur-
ton”, “Enron” with negative connotation, sug-
gesting that our algorithms could potentially serve
as a proxy to track the general connotation of real
world entities. Table 2 shows example common
nouns with connotative polarity.
</bodyText>
<subsectionHeader confidence="0.994721">
5.3 Practical Remarks on WSD and MWEs
</subsectionHeader>
<bodyText confidence="0.9857349375">
In this work we aim to find the polarity of most
prevalent senses of each word, in part because it
is not easy to perform unsupervised word sense
disambiguation (WSD) on a large corpus in a reli-
able way, especially when the corpus consists pri-
marily of short n-grams. Although the resulting
lexicon loses on some of the polysemous words
with potentially opposite polarities, per-word con-
notation (rather than per-sense connotation) does
have a practical value: it provides a convenient
option for users who wish to avoid the burden of
WSD before utilizing the lexicon. Future work in-
cludes handling of WSD and multi-word expres-
sions (MWEs), e.g., “Great Leader” (for Kim
Jong-Il), “Inglourious Basterds” (a movie title).21
21These examples credit to an anonymous reviewer.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999952032258065">
A very interesting work of Mohammad and Tur-
ney (2010) uses Mechanical Turk in order to build
the lexicon of emotions evoked by words. In con-
trast, we present an automatic approach that in-
fers the general connotation of words. Velikovich
et al. (2010) use graph propagation algorithms for
constructing a web-scale polarity lexicon for sen-
timent analysis. Although we employ the same
graph propagation algorithm, our graph construc-
tion is fundamentally different in that we integrate
stronger inductive biases into the graph topology
and the corresponding edge weights. As shown
in our experimental results, we find that judicious
construction of graph structure, exploiting multi-
ple complementing linguistic phenomena can en-
hance both the performance and the efficiency of
the algorithm substantially. Other interesting ap-
proaches include one based on min-cut (Dong et
al., 2012) or LDA (Xie and Li, 2012). Our pro-
posed approaches are more suitable for encoding
a much diverse set of linguistic phenomena how-
ever. But our work use a few seed predicates with
selectional preference instead of relying on word
similarity. Some recent work explored the use
of constraint optimization framework for inducing
domain-dependent sentiment lexicon (Choi and
Cardie (2009), Lu et al. (2011)). Our work dif-
fers in that we provide comprehensive insights into
different formulations of ILP and LP, aiming to
learn the much different task of learning the gen-
eral connotation of words.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999935666666666">
We presented a broad-coverage connotation lexi-
con that determines the subtle nuanced sentiment
of even those words that are objective on the sur-
face, including the general connotation of real-
world named entities. Via a comprehensive eval-
uation, we provided empirical insights into three
different types of induction algorithms, and pro-
posed one with good precision, coverage, and effi-
ciency.
</bodyText>
<sectionHeader confidence="0.998026" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991874">
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. We thank reviewers for many insightful
comments and suggestions, and for providing us
with several very inspiring examples to work with.
</bodyText>
<figure confidence="0.907946333333333">
W+ W−
E s(w) ≥ E s(w)
wEt wEt
W+ W−
E s(w) &lt; E s(w)
wEt wEt
</figure>
<page confidence="0.988741">
1782
</page>
<sectionHeader confidence="0.981608" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999861027272728">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC’10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
J. Kathryn Bock. 1986. Syntactic persistence
in language production. Cognitive psychology,
18(3):355–387.
Thorsten Brants and Alex Franz. 2006. {Web 1T 5-
gram Version 1}.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ’09, pages 590–598, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16:22–29, March.
ILOG CPLEX. 2009. High-performance software for
mathematical programming and optimization. U RL
http://www.ilog.com/products/cplex.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning, CoNLL ’10, pages 107–116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xishuang Dong, Qibo Zou, and Yi Guan. 2012. Set-
similarity joins based semi-supervised sentiment
analysis. In Neural Information Processing, pages
176–183. Springer.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417–422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1092–1103. Association for Computa-
tional Linguistics.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503–511, Boulder, Colorado, June.
Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on
European chapter of the Association for Computa-
tional Linguistics, pages 174–181. Association for
Computational Linguistics.
Bas Heerschop, Alexander Hogenboom, and Flavius
Frasincar. 2011. Sentiment lexicon creation from
lexical resources. In Business Information Systems,
pages 185–196. Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604–632.
Bill Louw. 1993. Irony in the text or insincerity in
the writer. Text and technology: In honour of John
Sinclair, pages 157–176.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction
of a context-aware sentiment lexicon: an optimiza-
tion approach. In Proceedings of the 20th interna-
tional conference on World wide web, pages 347–
356. ACM.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26–34, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
Arturo Montejo-R´aez, Eugenio Martinez-C´amara,
M. Teresa Martin-Valdivia, and L. Alfonso Ure˜na
L´opez. 2012. Random walk weighting over sen-
tiwordnet for sentiment polarity detection on twit-
ter. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 3–10, Jeju, Korea, July. Association
for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1–135.
Martin J Pickering and Holly P Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39(4):633–651.
</reference>
<page confidence="0.571434">
1783
</page>
<reference confidence="0.999750893939394">
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI’09, pages 1199–1204, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford Univer-
sity Press.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words
and constructions. International journal of corpus
linguistics, 8(2):209–243.
Philip J. Stone and Earl B. Hunt. 1963. A computer
approach to content analysis: studies using the gen-
eral inquirer system. In Proceedings of the May 21-
23, 1963, spring joint computer conference, AFIPS
’63 (Spring), pages 241–256, New York, NY, USA.
ACM.
Michael Stubbs. 1995. Collocations and semantic pro-
files: on the cause of the trouble with quantitative
studies. Functions of language, 2(1):23–55.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In In
EMNLP/VLC 2000, pages 63–70.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164–210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34–35, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ’05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 347–354, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Rui Xie and Chunping Li. 2012. Lexicon construc-
tion: A topic model approach. In Systems and Infor-
matics (ICSAI), 2012 International Conference on,
pages 2299–2303. IEEE.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
</reference>
<page confidence="0.993661">
1784
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.817848">
<title confidence="0.9983625">Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</title>
<author confidence="0.994274">Song Feng Jun Seok Kang Polina Kuznetsova Yejin</author>
<affiliation confidence="0.989336">Department of Computer</affiliation>
<author confidence="0.931277">Stony Brook Stony Brook</author>
<author confidence="0.931277">NY</author>
<email confidence="0.999301">songfeng,junkang,pkuznetsova,ychoi@cs.stonybrook.edu</email>
<abstract confidence="0.996371473684211">Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purposefully conjure emotion from the readers’ minds. The focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the such as We propose induction algorithms encoding a diverse set of linguistic insights (semantic prosody, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first lexicon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="26780" citStr="Baccianella et al., 2010" startWordPosition="4372" endWordPosition="4375">fficiency-wise, LP runs within 10 minutes while ILP takes several hours. Table 4 shows the results evaluated against MPQA for different variations of ILP and LP. We find that LP variants much better recall and F-score, while maintaining comparable precision. Therefore, we choose the connotation lexicon by LP (C-LP) in the following evaluations in §5. 5 Experimental Results II In this section, we present comprehensive intrinsic §5.1 and extrinsic §5.2 evaluations comparing three representative lexicons from §2 &amp; §4: CLP, OVERLAY, PRED-ARG (CP), and two popular sentiment lexicons: SentiWordNet (Baccianella et al., 2010) and GI+MPQA.14 Note that C-LP is the largest among all connotation lexicons, including 70,000 polar words.15 5.1 Intrinsic Evaluation: Human Judgements We evaluate 4000 words16 using Amazon Mechanical Turk (AMT). Because we expect that judging a connotation can be dependent on one’s cultural background, personality and value systems, we gather judgements from 5 people for each word, from which we hope to draw a more general judgement of connotative polarity. About 300 unique Turkers participated the evaluation tasks. We gather gold standard only for those words for which more than half of the</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kathryn Bock</author>
</authors>
<title>Syntactic persistence in language production.</title>
<date>1986</date>
<booktitle>Cognitive psychology,</booktitle>
<pages>18--3</pages>
<contexts>
<context position="11365" citStr="Bock (1986)" startWordPosition="1723" endWordPosition="1724">.g., Turney (2001), Church and Hanks (1990)). Sub-graph #2: Argument–Argument Graph The second sub-graph is based on the distributional similarities among the arguments. One possible way of constructing such a graph is simply connecting all nodes and assign edge weights proportionate to the word association scores, such as PMI, or distributional similarity. However, such a completely connected graph can be susceptible to propagating noise, and does not scale well over a very large set of vocabulary. We therefore reduce the graph connectivity by exploiting semantic parallelism of coordination (Bock (1986), Hatzivassiloglou and McKeown 7We restrict predicte-argument pairs to verb-object pairs in this study. Note that Google Web 1T dataset consists of n-grams upto n = 5. Since n-gram sequences are too short to apply a parser, we extract verb-object pairs approximately by matching part-of-speech tags. Empirically, when overlaid with the second sub-graph, we found that it is better to keep the connectivity of this sub-graph as uni-directional. That is, we only allow edges to go from a predicate to an argument. 1776 POSITIVE NEGATIVE NEUTRAL n. avatar, adrenaline, keynote, debut, unbeliever, delay,</context>
</contexts>
<marker>Bock, 1986</marker>
<rawString>J. Kathryn Bock. 1986. Syntactic persistence in language production. Cognitive psychology, 18(3):355–387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>{Web 1T 5-gram Version 1}.</booktitle>
<contexts>
<context position="17642" citStr="Brants and Franz (2006)" startWordPosition="2777" endWordPosition="2780">pairs will not have the same positive or negative polarity: b(i, j) E Rant, xi + xj &lt; 1, yi + yj &lt; 1 For this constraint, we only consider antonym pairs that share the same root, e.g., “sufficient” and “insufficient”, as those pairs are more likely to have the opposite polarities than pairs without sharing the same root, e.g., “east” and “west”. 2. Csyn: Synonym pairs will not have the opposite polarity: b(i, j) E Rsyn, xi + yj &lt; 1, xj + yi &lt; 1 3 Experimental Result I We provide comprehensive comparisons over variants of three types of algorithms proposed in §2. We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). We filter out the ngrams with punctuations and other special characters to reduce the noise. Note that we consider the connotation lexicon to be inclusive of a sentiment lexicon for two practical reasons: first, it is highly unlikely that any word with non-neutral sentiment (i.e., positive or negative) would carry connotation of the opposite, i.e., conflicting10 polarity. Second, for some words with distinct sentiment or strong connotation, it can be difficult or even unnatural to draw a precise distinction betw</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. {Web 1T 5-gram Version 1}.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 -Volume 2, EMNLP ’09,</booktitle>
<pages>590--598</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6440" citStr="Choi and Cardie (2009)" startWordPosition="957" endWordPosition="960">hop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (2009), Lu et al. (2011)). We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations. It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutral polarity prevails over</context>
<context position="35302" citStr="Choi and Cardie (2009)" startWordPosition="5795" endWordPosition="5798">on of graph structure, exploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially. Other interesting approaches include one based on min-cut (Dong et al., 2012) or LDA (Xie and Li, 2012). Our proposed approaches are more suitable for encoding a much diverse set of linguistic phenomena however. But our work use a few seed predicates with selectional preference instead of relying on word similarity. Some recent work explored the use of constraint optimization framework for inducing domain-dependent sentiment lexicon (Choi and Cardie (2009), Lu et al. (2011)). Our work differs in that we provide comprehensive insights into different formulations of ILP and LP, aiming to learn the much different task of learning the general connotation of words. 7 Conclusion We presented a broad-coverage connotation lexicon that determines the subtle nuanced sentiment of even those words that are objective on the surface, including the general connotation of realworld named entities. Via a comprehensive evaluation, we provided empirical insights into three different types of induction algorithms, and proposed one with good precision, coverage, an</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Yejin Choi and Claire Cardie. 2009. Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 -Volume 2, EMNLP ’09, pages 590–598, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<journal>Comput. Linguist.,</journal>
<pages>16--22</pages>
<contexts>
<context position="10797" citStr="Church and Hanks (1990)" startWordPosition="1635" endWordPosition="1638">t Graph This sub-graph is the bipartite graph that encodes the selectional preference of connotative predicates over their arguments. In this graph, connotative predicates p reside on one side of the graph and their co-occurring arguments a reside on the other side of the graph based on Google Web 1T corpus.7 The weight on the edges between the predicates p and arguments a are defined using Point-wise Mutual Information (PMI) as follows: P(p, a) w(p → a) := PMI(p, a) = log, P(p)P(a) PMI scores have been widely used in previous studies to measure association between words (e.g., Turney (2001), Church and Hanks (1990)). Sub-graph #2: Argument–Argument Graph The second sub-graph is based on the distributional similarities among the arguments. One possible way of constructing such a graph is simply connecting all nodes and assign edge weights proportionate to the word association scores, such as PMI, or distributional similarity. However, such a completely connected graph can be susceptible to propagating noise, and does not scale well over a very large set of vocabulary. We therefore reduce the graph connectivity by exploiting semantic parallelism of coordination (Bock (1986), Hatzivassiloglou and McKeown 7</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Comput. Linguist., 16:22–29, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ILOG CPLEX</author>
</authors>
<title>High-performance software for mathematical programming and optimization.</title>
<date>2009</date>
<note>U RL http://www.ilog.com/products/cplex.</note>
<contexts>
<context position="26106" citStr="CPLEX, 2009" startWordPosition="4269" endWordPosition="4270">alues. We consider a word has positive or negative polarity only if the assignment indicates 1 for the corresponding polarity and 0 for the rest. In other words, we treat all words with fractional assignments over different polarities as neutral. Because the optimal solutions of LP correspond to extreme points in the convex polytope formed by the constraints, we obtain a large portion of words with non-fractional assignments toward non-neutral polarities. Alternatively, one can round up fractional values. 4.2 Empirical Comparisons: ILP v.s. LP To solve the ILP/LP, we run ILOG CPLEX Optimizer (CPLEX, 2009)) on a 3.5GHz 6 core CPU machine with 96GB RAM. Efficiency-wise, LP runs within 10 minutes while ILP takes several hours. Table 4 shows the results evaluated against MPQA for different variations of ILP and LP. We find that LP variants much better recall and F-score, while maintaining comparable precision. Therefore, we choose the connotation lexicon by LP (C-LP) in the following evaluations in §5. 5 Experimental Results II In this section, we present comprehensive intrinsic §5.1 and extrinsic §5.2 evaluations comparing three representative lexicons from §2 &amp; §4: CLP, OVERLAY, PRED-ARG (CP), a</context>
</contexts>
<marker>CPLEX, 2009</marker>
<rawString>ILOG CPLEX. 2009. High-performance software for mathematical programming and optimization. U RL http://www.ilog.com/products/cplex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>107--116</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1762" citStr="Davidov et al. (2010)" startWordPosition="254" endWordPosition="257">a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. In this paper, we concentrate on understanding the connotative sentiments of words, as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text. For instance, consider the following: Geothermal replaces oil-heating; it helps reducing greenhouse emissions.1 Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sentence is likely to have an influence on readers’ </context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 107–116, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xishuang Dong</author>
<author>Qibo Zou</author>
<author>Yi Guan</author>
</authors>
<title>Setsimilarity joins based semi-supervised sentiment analysis.</title>
<date>2012</date>
<booktitle>In Neural Information Processing,</booktitle>
<pages>176--183</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="34919" citStr="Dong et al., 2012" startWordPosition="5735" endWordPosition="5738"> propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis. Although we employ the same graph propagation algorithm, our graph construction is fundamentally different in that we integrate stronger inductive biases into the graph topology and the corresponding edge weights. As shown in our experimental results, we find that judicious construction of graph structure, exploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially. Other interesting approaches include one based on min-cut (Dong et al., 2012) or LDA (Xie and Li, 2012). Our proposed approaches are more suitable for encoding a much diverse set of linguistic phenomena however. But our work use a few seed predicates with selectional preference instead of relying on word similarity. Some recent work explored the use of constraint optimization framework for inducing domain-dependent sentiment lexicon (Choi and Cardie (2009), Lu et al. (2011)). Our work differs in that we provide comprehensive insights into different formulations of ILP and LP, aiming to learn the much different task of learning the general connotation of words. 7 Conclu</context>
</contexts>
<marker>Dong, Zou, Guan, 2012</marker>
<rawString>Xishuang Dong, Qibo Zou, and Yi Guan. 2012. Setsimilarity joins based semi-supervised sentiment analysis. In Neural Information Processing, pages 176–183. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining. In</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06),</booktitle>
<pages>417--422</pages>
<contexts>
<context position="1713" citStr="Esuli and Sebastiani (2006)" startWordPosition="245" endWordPosition="249">lysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. In this paper, we concentrate on understanding the connotative sentiments of words, as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text. For instance, consider the following: Geothermal replaces oil-heating; it helps reducing greenhouse emissions.1 Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be entirely objective: this sen</context>
<context position="3090" citStr="Esuli and Sebastiani (2006)" startWordPosition="458" endWordPosition="461">ents, one needs to know that the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1Our learned lexicon correctly assigns negative polarity to emission. 2A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Compu</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06), pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Feng</author>
<author>Ritwik Bose</author>
<author>Yejin Choi</author>
</authors>
<title>Learning general connotation of words using graph-based algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1092--1103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5811" citStr="Feng et al. (2011)" startWordPosition="862" endWordPosition="865">ributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (</context>
<context position="7264" citStr="Feng et al. (2011)" startWordPosition="1080" endWordPosition="1083"> is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutral polarity prevails over neutral one if some meanings lead to neutral while others to non-neutral.4 Our work results in the first broad-coverage connotation lexicon,5 significantly improving both the coverage and the precision of Feng et al. (2011). As an interesting by-product, our algorithm can be also used as a proxy to measure the general connotation of real-world named entities based on their collocational statistics. Table 1 highlights some example proper nouns included in the final lexicon. The rest of the paper is structured as follows. In §2 we describe three types of induction algorithms followed by evaluation in §3. Then we revisit the induction algorithms based on constraint optimization in §4 to enhance quality and scalability. §5 presents comprehensive evaluation with human judges and extrinsic evaluations. Related work an</context>
<context position="9137" citStr="Feng et al. (2011)" startWordPosition="1369" endWordPosition="1372">suffer enjoy thank tax loss writing profit preventing ... ... investment flu bonus cold gain pred-arg distr sim synonyms antonyms 2 Connotation Induction Algorithms We develop induction algorithms based on three distinct types of algorithmic framework that have been shown successful for the analogous task of sentiment lexicon induction: HITS &amp; PageRank (§2.1), Label/Graph Propagation (§2.2), and Constraint Optimization via Integer Linear Programming (§2.3). As will be shown, each of these approaches will incorporate additional, more diverse linguistic insights. 2.1 HITS &amp; PageRank The work of Feng et al. (2011) explored the use of HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) to induce the general connotation of words hinging on the linguistic phenomena of selectional preference and semantic prosody, i.e., connotative predicates influencing the connotation of their arguments. For example, the object of a negative connotative predicate “cure” is likely to have negative connotation, e.g., “disease” or “cancer”. The bipartite graph structure for this approach corresponds to the left-most box (labeled as “pred-arg”) in Figure 1. 2.2 Label Propagation With the goal of obtaining a broad-coverage</context>
<context position="18880" citStr="Feng et al. (2011)" startWordPosition="2974" endWordPosition="2977">sentiment, e.g., “efficient”. Therefore, sentiment lexicons can serve as a surrogate to measure a subset of connotation words induced by the algorithms, as shown in Table 3 with respect to General Inquirer (Stone and Hunt (1963)) and MPQA (Wilson et al. (2005b)).11 Discussion Table 3 shows the agreement statistics with respect to two conventional sentiment lexicons. We find that the use of label propagation alone [PRED-ARG (CP)] improves the performance substantially over the comparable graph construction with different graph analysis algorithms, in particular, HITS and PageRank approaches of Feng et al. (2011). The two completely connected variants of the graph propagation on the Pred-Arg graph, [® PRED-ARG (PMI)] and [® PRED-ARG (CP)], do not necessarily improve the performance over the simpler and computationally lighter alternative, [PREDARG (CP)]. The [OVERLAY], which is based on both Pred-Arg and Arg-Arg subgraphs (§2.2), achieves the best performance among graph-based algorithms, significantly improving the precision over all other baselines. This result suggests: 1 The sub-graph #2, based on the semantic parallelism of coordination, is simple and yet very powerful as an inductive bias. 2 The</context>
</contexts>
<marker>Feng, Bose, Choi, 2011</marker>
<rawString>Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learning general connotation of words using graph-based algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1092–1103. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Greene</author>
<author>Philip Resnik</author>
</authors>
<title>More than words: Syntactic packaging and implicit sentiment.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>503--511</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1457" citStr="Greene and Resnik, 2009" startWordPosition="207" endWordPosition="210">, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. 1 Introduction There has been a substantial body of research in sentiment analysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. In this paper, we concentrate on understanding the connotative sentiments of words, as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text. For instance, consider th</context>
</contexts>
<marker>Greene, Resnik, 2009</marker>
<rawString>Stephan Greene and Philip Resnik. 2009. More than words: Syntactic packaging and implicit sentiment. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 503–511, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R McKeown. 1997. Predicting the semantic orientation of adjectives. In Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, pages 174–181. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bas Heerschop</author>
<author>Alexander Hogenboom</author>
<author>Flavius Frasincar</author>
</authors>
<title>Sentiment lexicon creation from lexical resources.</title>
<date>2011</date>
<booktitle>In Business Information Systems,</booktitle>
<pages>185--196</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5835" citStr="Heerschop et al. (2011)" startWordPosition="866" endWordPosition="869">ty, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (</context>
</contexts>
<marker>Heerschop, Hogenboom, Frasincar, 2011</marker>
<rawString>Bas Heerschop, Alexander Hogenboom, and Flavius Frasincar. 2011. Sentiment lexicon creation from lexical resources. In Business Information Systems, pages 185–196. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nobuhiro Kaji</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Building lexicon for sentiment analysis from massive collection of html documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<contexts>
<context position="3142" citStr="Kaji and Kitsuregawa (2007)" startWordPosition="466" endWordPosition="469">s generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1Our learned lexicon correctly assigns negative polarity to emission. 2A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774–1784, Sofia, Bulgar</context>
</contexts>
<marker>Kaji, Kitsuregawa, 2007</marker>
<rawString>Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Building lexicon for sentiment analysis from massive collection of html documents. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>JOURNAL OF THE ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="5771" citStr="Kleinberg (1999)" startWordPosition="856" endWordPosition="857">selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Const</context>
<context position="9180" citStr="Kleinberg, 1999" startWordPosition="1378" endWordPosition="1379">eventing ... ... investment flu bonus cold gain pred-arg distr sim synonyms antonyms 2 Connotation Induction Algorithms We develop induction algorithms based on three distinct types of algorithmic framework that have been shown successful for the analogous task of sentiment lexicon induction: HITS &amp; PageRank (§2.1), Label/Graph Propagation (§2.2), and Constraint Optimization via Integer Linear Programming (§2.3). As will be shown, each of these approaches will incorporate additional, more diverse linguistic insights. 2.1 HITS &amp; PageRank The work of Feng et al. (2011) explored the use of HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) to induce the general connotation of words hinging on the linguistic phenomena of selectional preference and semantic prosody, i.e., connotative predicates influencing the connotation of their arguments. For example, the object of a negative connotative predicate “cure” is likely to have negative connotation, e.g., “disease” or “cancer”. The bipartite graph structure for this approach corresponds to the left-most box (labeled as “pred-arg”) in Figure 1. 2.2 Label Propagation With the goal of obtaining a broad-coverage lexicon in mind, we find that relying only</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. JOURNAL OF THE ACM, 46(5):604–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Louw</author>
</authors>
<title>Irony in the text or insincerity in the writer. Text and technology:</title>
<date>1993</date>
<booktitle>In honour of John Sinclair,</booktitle>
<pages>157--176</pages>
<contexts>
<context position="5346" citStr="Louw (1993)" startWordPosition="795" endWordPosition="796">at the same assumption would be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate “cure”, which has a positive connotation typically takes arguments with negative connotation, e.g., “disease”, when used as the “relieve” sense.3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitatio</context>
</contexts>
<marker>Louw, 1993</marker>
<rawString>Bill Louw. 1993. Irony in the text or insincerity in the writer. Text and technology: In honour of John Sinclair, pages 157–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>Malu Castellanos</author>
<author>Umeshwar Dayal</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Automatic construction of a context-aware sentiment lexicon: an optimization approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World wide web,</booktitle>
<pages>347--356</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6458" citStr="Lu et al. (2011)" startWordPosition="961" endWordPosition="964">jo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (2009), Lu et al. (2011)). We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations. It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutral polarity prevails over neutral one if so</context>
<context position="35320" citStr="Lu et al. (2011)" startWordPosition="5799" endWordPosition="5802">xploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially. Other interesting approaches include one based on min-cut (Dong et al., 2012) or LDA (Xie and Li, 2012). Our proposed approaches are more suitable for encoding a much diverse set of linguistic phenomena however. But our work use a few seed predicates with selectional preference instead of relying on word similarity. Some recent work explored the use of constraint optimization framework for inducing domain-dependent sentiment lexicon (Choi and Cardie (2009), Lu et al. (2011)). Our work differs in that we provide comprehensive insights into different formulations of ILP and LP, aiming to learn the much different task of learning the general connotation of words. 7 Conclusion We presented a broad-coverage connotation lexicon that determines the subtle nuanced sentiment of even those words that are objective on the surface, including the general connotation of realworld named entities. Via a comprehensive evaluation, we provided empirical insights into three different types of induction algorithms, and proposed one with good precision, coverage, and efficiency. Ackn</context>
</contexts>
<marker>Lu, Castellanos, Dayal, Zhai, 2011</marker>
<rawString>Yue Lu, Malu Castellanos, Umeshwar Dayal, and ChengXiang Zhai. 2011. Automatic construction of a context-aware sentiment lexicon: an optimization approach. In Proceedings of the 20th international conference on World wide web, pages 347– 356. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Peter Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>26--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA,</location>
<contexts>
<context position="1542" citStr="Mohammad and Turney, 2010" startWordPosition="219" endWordPosition="222">dge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. 1 Introduction There has been a substantial body of research in sentiment analysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. In this paper, we concentrate on understanding the connotative sentiments of words, as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text. For instance, consider the following: Geothermal replaces oil-heating; it helps reducing greenhouse emissions.</context>
<context position="34094" citStr="Mohammad and Turney (2010)" startWordPosition="5609" endWordPosition="5613">when the corpus consists primarily of short n-grams. Although the resulting lexicon loses on some of the polysemous words with potentially opposite polarities, per-word connotation (rather than per-sense connotation) does have a practical value: it provides a convenient option for users who wish to avoid the burden of WSD before utilizing the lexicon. Future work includes handling of WSD and multi-word expressions (MWEs), e.g., “Great Leader” (for Kim Jong-Il), “Inglourious Basterds” (a movie title).21 21These examples credit to an anonymous reviewer. 6 Related Work A very interesting work of Mohammad and Turney (2010) uses Mechanical Turk in order to build the lexicon of emotions evoked by words. In contrast, we present an automatic approach that infers the general connotation of words. Velikovich et al. (2010) use graph propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis. Although we employ the same graph propagation algorithm, our graph construction is fundamentally different in that we integrate stronger inductive biases into the graph topology and the corresponding edge weights. As shown in our experimental results, we find that judicious construction of graph st</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif Mohammad and Peter Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 26–34, Los Angeles, CA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arturo Montejo-R´aez</author>
<author>Eugenio Martinez-C´amara</author>
<author>M Teresa Martin-Valdivia</author>
<author>L Alfonso Ure˜na L´opez</author>
</authors>
<title>Random walk weighting over sentiwordnet for sentiment polarity detection on twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,</booktitle>
<pages>3--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Jeju, Korea,</location>
<marker>Montejo-R´aez, Martinez-C´amara, Martin-Valdivia, L´opez, 2012</marker>
<rawString>Arturo Montejo-R´aez, Eugenio Martinez-C´amara, M. Teresa Martin-Valdivia, and L. Alfonso Ure˜na L´opez. 2012. Random walk weighting over sentiwordnet for sentiment polarity detection on twitter. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis, pages 3–10, Jeju, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1999</date>
<tech>Technical Report 1999-66,</tech>
<institution>Stanford InfoLab,</institution>
<contexts>
<context position="5791" citStr="Page et al. (1999)" startWordPosition="858" endWordPosition="861">c insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (</context>
<context position="9213" citStr="Page et al., 1999" startWordPosition="1382" endWordPosition="1385"> bonus cold gain pred-arg distr sim synonyms antonyms 2 Connotation Induction Algorithms We develop induction algorithms based on three distinct types of algorithmic framework that have been shown successful for the analogous task of sentiment lexicon induction: HITS &amp; PageRank (§2.1), Label/Graph Propagation (§2.2), and Constraint Optimization via Integer Linear Programming (§2.3). As will be shown, each of these approaches will incorporate additional, more diverse linguistic insights. 2.1 HITS &amp; PageRank The work of Feng et al. (2011) explored the use of HITS (Kleinberg, 1999) and PageRank (Page et al., 1999) to induce the general connotation of words hinging on the linguistic phenomena of selectional preference and semantic prosody, i.e., connotative predicates influencing the connotation of their arguments. For example, the object of a negative connotative predicate “cure” is likely to have negative connotation, e.g., “disease” or “cancer”. The bipartite graph structure for this approach corresponds to the left-most box (labeled as “pred-arg”) in Figure 1. 2.2 Label Propagation With the goal of obtaining a broad-coverage lexicon in mind, we find that relying only on the structure of semantic pro</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bringing order to the web. Technical Report 1999-66, Stanford InfoLab, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<journal>Found. Trends Inf. Retr.,</journal>
<pages>2--1</pages>
<contexts>
<context position="1133" citStr="Pang and Lee, 2008" startWordPosition="158" endWordPosition="161">sefully conjure emotion from the readers’ minds. The focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the surface, such as “intelligence”, “human”, and “cheesecake”. We propose induction algorithms encoding a diverse set of linguistic insights (semantic prosody, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon. 1 Introduction There has been a substantial body of research in sentiment analysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Pickering</author>
<author>Holly P Branigan</author>
</authors>
<title>The representation of verbs: Evidence from syntactic priming in language production.</title>
<date>1998</date>
<journal>Journal of Memory and Language,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="12644" citStr="Pickering and Branigan (1998)" startWordPosition="1897" endWordPosition="1900">stakeholder, sunshine, cooperation shot, misdemeanor, mutiny, rigor grid, gasoline, course, preview v. handcraft, volunteer, party, ac- sentence, cough, trap, scratch, de- state, edit, send, put, arrive, type, credit, personalize, nurse, google bunk, rip, misspell, overcharge drill, name, stay, echo, register a. floral, vegetarian, prepared, age- debilitating, impaired, swollen, same, cerebral, west, uncut, autoless, funded, contemporary intentional, jarring, unearned matic, hydrated, unheated, routine Table 2: Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a). (1997), Pickering and Branigan (1998)). In particular, we consider an undirected edge between a pair of arguments a1 and a2 only if they occurred together in the “a1 and a2” or “a2 and a1” coordination, and assign edge weights as: w(a1 − a2) = CosineSim(−→a1, →−a2) = ||−→a1 |−→a2|| where →−a1 and →−a2 are co-occurrence vectors for a1 and a2 respectively. The co-occurrence vector for each word is computed using PMI scores with respect to the top n co-occurring words.8 n (=50) is selected empirically. The edge weights in two sub-graphs are normalized so that they are in the comparable range.9 Limitations of Graph-based Algorithms A</context>
</contexts>
<marker>Pickering, Branigan, 1998</marker>
<rawString>Martin J Pickering and Holly P Branigan. 1998. The representation of verbs: Evidence from syntactic priming in language production. Journal of Memory and Language, 39(4):633–651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Expanding domain sentiment lexicon through double propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09,</booktitle>
<pages>1199--1204</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3161" citStr="Qiu et al. (2009)" startWordPosition="470" endWordPosition="473">ion, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1Our learned lexicon correctly assigns negative polarity to emission. 2A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774–1784, Sofia, Bulgaria, August 4-9 2013</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2009</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09, pages 1199–1204, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center.</title>
<date>2004</date>
<contexts>
<context position="6416" citStr="Roth and Yih (2004)" startWordPosition="953" endWordPosition="956"> et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (2009), Lu et al. (2011)). We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations. It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either type of meanings into the lexicon, where non-neutra</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Corpus, concordance, collocation. Describing English language.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5333" citStr="Sinclair (1991)" startWordPosition="793" endWordPosition="794">10), we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate “cure”, which has a positive connotation typically takes arguments with negative connotation, e.g., “disease”, when used as the “relieve” sense.3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practi</context>
</contexts>
<marker>Sinclair, 1991</marker>
<rawString>John Sinclair. 1991. Corpus, concordance, collocation. Describing English language. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anatol Stefanowitsch</author>
<author>Stefan Th Gries</author>
</authors>
<title>Collostructions: Investigating the interaction of words and constructions.</title>
<date>2003</date>
<booktitle>International journal of corpus linguistics,</booktitle>
<pages>8--2</pages>
<contexts>
<context position="5393" citStr="Stefanowitsch and Gries (2003)" startWordPosition="799" endWordPosition="802"> be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate “cure”, which has a positive connotation typically takes arguments with negative connotation, e.g., “disease”, when used as the “relieve” sense.3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that </context>
</contexts>
<marker>Stefanowitsch, Gries, 2003</marker>
<rawString>Anatol Stefanowitsch and Stefan Th Gries. 2003. Collostructions: Investigating the interaction of words and constructions. International journal of corpus linguistics, 8(2):209–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Earl B Hunt</author>
</authors>
<title>A computer approach to content analysis: studies using the general inquirer system.</title>
<date>1963</date>
<journal>AFIPS</journal>
<booktitle>In Proceedings of the</booktitle>
<volume>63</volume>
<pages>241--256</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="18490" citStr="Stone and Hunt (1963)" startWordPosition="2912" endWordPosition="2915"> inclusive of a sentiment lexicon for two practical reasons: first, it is highly unlikely that any word with non-neutral sentiment (i.e., positive or negative) would carry connotation of the opposite, i.e., conflicting10 polarity. Second, for some words with distinct sentiment or strong connotation, it can be difficult or even unnatural to draw a precise distinction between connotation and sentiment, e.g., “efficient”. Therefore, sentiment lexicons can serve as a surrogate to measure a subset of connotation words induced by the algorithms, as shown in Table 3 with respect to General Inquirer (Stone and Hunt (1963)) and MPQA (Wilson et al. (2005b)).11 Discussion Table 3 shows the agreement statistics with respect to two conventional sentiment lexicons. We find that the use of label propagation alone [PRED-ARG (CP)] improves the performance substantially over the comparable graph construction with different graph analysis algorithms, in particular, HITS and PageRank approaches of Feng et al. (2011). The two completely connected variants of the graph propagation on the Pred-Arg graph, [® PRED-ARG (PMI)] and [® PRED-ARG (CP)], do not necessarily improve the performance over the simpler and computationally </context>
</contexts>
<marker>Stone, Hunt, 1963</marker>
<rawString>Philip J. Stone and Earl B. Hunt. 1963. A computer approach to content analysis: studies using the general inquirer system. In Proceedings of the May 21-23, 1963, spring joint computer conference, AFIPS ’63 (Spring), pages 241–256, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Stubbs</author>
</authors>
<title>Collocations and semantic profiles: on the cause of the trouble with quantitative studies. Functions of language,</title>
<date>1995</date>
<pages>2--1</pages>
<contexts>
<context position="5361" citStr="Stubbs (1995)" startWordPosition="797" endWordPosition="798">ssumption would be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate “cure”, which has a positive connotation typically takes arguments with negative connotation, e.g., “disease”, when used as the “relieve” sense.3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sinclair (1991), Louw (1993), Stubbs (1995), Stefanowitsch and Gries (2003))), and also exploit existing lexical resources as an additional inductive bias. We cast the connotation lexicon induction task as a collective inference problem, and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction: Random walk based on HITS/PageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for </context>
</contexts>
<marker>Stubbs, 1995</marker>
<rawString>Michael Stubbs. 1995. Collocations and semantic profiles: on the cause of the trouble with quantitative studies. Functions of language, 2(1):23–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In In EMNLP/VLC</booktitle>
<pages>63--70</pages>
<contexts>
<context position="17722" citStr="Toutanova and Manning (2000)" startWordPosition="2788" endWordPosition="2791"> xi + xj &lt; 1, yi + yj &lt; 1 For this constraint, we only consider antonym pairs that share the same root, e.g., “sufficient” and “insufficient”, as those pairs are more likely to have the opposite polarities than pairs without sharing the same root, e.g., “east” and “west”. 2. Csyn: Synonym pairs will not have the opposite polarity: b(i, j) E Rsyn, xi + yj &lt; 1, xj + yi &lt; 1 3 Experimental Result I We provide comprehensive comparisons over variants of three types of algorithms proposed in §2. We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). We filter out the ngrams with punctuations and other special characters to reduce the noise. Note that we consider the connotation lexicon to be inclusive of a sentiment lexicon for two practical reasons: first, it is highly unlikely that any word with non-neutral sentiment (i.e., positive or negative) would carry connotation of the opposite, i.e., conflicting10 polarity. Second, for some words with distinct sentiment or strong connotation, it can be difficult or even unnatural to draw a precise distinction between connotation and sentiment, e.g., “efficient”. Therefore, sentiment lexicons </context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In In EMNLP/VLC 2000, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: Pmi-ir versus lsa on toefl.</title>
<date>2001</date>
<contexts>
<context position="10772" citStr="Turney (2001)" startWordPosition="1633" endWordPosition="1634">edicate–Argument Graph This sub-graph is the bipartite graph that encodes the selectional preference of connotative predicates over their arguments. In this graph, connotative predicates p reside on one side of the graph and their co-occurring arguments a reside on the other side of the graph based on Google Web 1T corpus.7 The weight on the edges between the predicates p and arguments a are defined using Point-wise Mutual Information (PMI) as follows: P(p, a) w(p → a) := PMI(p, a) = log, P(p)P(a) PMI scores have been widely used in previous studies to measure association between words (e.g., Turney (2001), Church and Hanks (1990)). Sub-graph #2: Argument–Argument Graph The second sub-graph is based on the distributional similarities among the arguments. One possible way of constructing such a graph is simply connecting all nodes and assign edge weights proportionate to the word association scores, such as PMI, or distributional similarity. However, such a completely connected graph can be susceptible to propagating noise, and does not scale well over a very large set of vocabulary. We therefore reduce the graph connectivity by exploiting semantic parallelism of coordination (Bock (1986), Hatzi</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: Pmi-ir versus lsa on toefl.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Velikovich</author>
<author>Sasha Blair-Goldensohn</author>
<author>Kerry Hannan</author>
<author>Ryan McDonald</author>
</authors>
<title>The viability of web-derived polarity lexicons.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4721" citStr="Velikovich et al., 2010" startWordPosition="700" endWordPosition="703">ma, Qaeda, Kosovo, Helicobacter, HIV Table 1: Example Named Entities (Proper Nouns) with Polar Connotation. A central premise to our approach is that it is collocational statistics of words that affect and shape the polarity of connotation. Indeed, the etymology of “connotation” is from the Latin “com” (“together or with”) and “notare” (“to mark”). It is important to clarify, however, that we do not simply assume that words that collocate share the same polarity of connotation. Although such an assumption played a key role in previous work for the analogous task of learning sentiment lexicon (Velikovich et al., 2010), we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words. As one example, the predicate “cure”, which has a positive connotation typically takes arguments with negative connotation, e.g., “disease”, when used as the “relieve” sense.3 Therefore, in order to attain a broad coverage lexicon while maintaining good precision, we guide the induction algorithm with multiple, carefully selected linguistic insights: [1] distributional similarity, [2] semantic parallelism of coordination, [3] selectional preference, and [4] semantic prosody (e.g., Sin</context>
<context position="6364" citStr="Velikovich et al. (2010)" startWordPosition="946" endWordPosition="949">ageRank (e.g., Kleinberg (1999), Page et al. (1999), Feng et al. (2011) Heerschop et al. (2011), Montejo-R´aez et al. (2012)) Label/Graph propagation (e.g., Zhu and Ghahra(2011) but with practical limitations. See §3 for detailed discussion. 3Note that when “cure” is used as the “preserve” sense, it expects objects with non-negative connotation. Hence wordsense-disambiguation (WSD) presents a challenge, though not unexpectedly. In this work, we assume the general connotation of each word over statistically prevailing senses, leaving a more cautious handling of WSD as future work. mani (2002), Velikovich et al. (2010)) Constraint optimization (e.g., Roth and Yih (2004), Choi and Cardie (2009), Lu et al. (2011)). We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon-based, human judgments, and extrinsic evaluations. It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings, and in some cases, it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively, or both. Therefore, we encompass any sentiment from either</context>
<context position="34291" citStr="Velikovich et al. (2010)" startWordPosition="5644" endWordPosition="5647">ense connotation) does have a practical value: it provides a convenient option for users who wish to avoid the burden of WSD before utilizing the lexicon. Future work includes handling of WSD and multi-word expressions (MWEs), e.g., “Great Leader” (for Kim Jong-Il), “Inglourious Basterds” (a movie title).21 21These examples credit to an anonymous reviewer. 6 Related Work A very interesting work of Mohammad and Turney (2010) uses Mechanical Turk in order to build the lexicon of emotions evoked by words. In contrast, we present an automatic approach that infers the general connotation of words. Velikovich et al. (2010) use graph propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis. Although we employ the same graph propagation algorithm, our graph construction is fundamentally different in that we integrate stronger inductive biases into the graph topology and the corresponding edge weights. As shown in our experimental results, we find that judicious construction of graph structure, exploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially. Other interesting approaches include one based on</context>
</contexts>
<marker>Velikovich, Blair-Goldensohn, Hannan, McDonald, 2010</marker>
<rawString>Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald. 2010. The viability of web-derived polarity lexicons. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<booktitle>Language Resources and Evaluation (formerly Computers and the Humanities),</booktitle>
<pages>39--2</pages>
<contexts>
<context position="1684" citStr="Wiebe et al. (2005)" startWordPosition="241" endWordPosition="244">arch in sentiment analysis over the last decade (Pang and Lee, 2008), where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued. However in many real-world texts, even seemingly objective statements can be opinion-laden in that they often allude nuanced sentiment of the writer (Greene and Resnik, 2009), or purposefully conjure emotion from the readers’ minds (Mohammad and Turney, 2010). Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments (e.g. Wiebe et al. (2005), Esuli and Sebastiani (2006), Greene and Resnik (2009), Davidov et al. (2010)), automatic analysis of them largely remains as a big challenge. In this paper, we concentrate on understanding the connotative sentiments of words, as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text. For instance, consider the following: Geothermal replaces oil-heating; it helps reducing greenhouse emissions.1 Although this sentence could be considered as a factual statement from the general standpoint, the subtle effect of this sentence may not be</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation (formerly Computers and the Humanities), 39(2/3):164–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Paul Hoffmann</author>
<author>Swapna Somasundaran</author>
<author>Jason Kessler</author>
<author>Janyce Wiebe</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Opinionfinder: a system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP on Interactive Demonstrations,</booktitle>
<pages>34--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3111" citStr="Wilson et al. (2005" startWordPosition="462" endWordPosition="465">the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1Our learned lexicon correctly assigns negative polarity to emission. 2A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</context>
<context position="18521" citStr="Wilson et al. (2005" startWordPosition="2918" endWordPosition="2922"> for two practical reasons: first, it is highly unlikely that any word with non-neutral sentiment (i.e., positive or negative) would carry connotation of the opposite, i.e., conflicting10 polarity. Second, for some words with distinct sentiment or strong connotation, it can be difficult or even unnatural to draw a precise distinction between connotation and sentiment, e.g., “efficient”. Therefore, sentiment lexicons can serve as a surrogate to measure a subset of connotation words induced by the algorithms, as shown in Table 3 with respect to General Inquirer (Stone and Hunt (1963)) and MPQA (Wilson et al. (2005b)).11 Discussion Table 3 shows the agreement statistics with respect to two conventional sentiment lexicons. We find that the use of label propagation alone [PRED-ARG (CP)] improves the performance substantially over the comparable graph construction with different graph analysis algorithms, in particular, HITS and PageRank approaches of Feng et al. (2011). The two completely connected variants of the graph propagation on the Pred-Arg graph, [® PRED-ARG (PMI)] and [® PRED-ARG (CP)], do not necessarily improve the performance over the simpler and computationally lighter alternative, [PREDARG (</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Theresa Wilson, Paul Hoffmann, Swapna Somasundaran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005a. Opinionfinder: a system for subjectivity analysis. In Proceedings of HLT/EMNLP on Interactive Demonstrations, pages 34–35, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3111" citStr="Wilson et al. (2005" startWordPosition="462" endWordPosition="465">the word “emissions” has generally negative connotation, which geothermal reduces. In fact, depending on the pragmatic contexts, it could be precisely the intention of the author to transfer his opinion into the readers’ minds. The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning, such as “Literature”, “Mediterranean”, and “wine”. Although there has been a number of previous work that constructed sentiment lexicons (e.g., Esuli and Sebastiani (2006), Wilson et al. (2005a), Kaji and Kitsuregawa (2007), Qiu et al. (2009)), which seem to be increasingly and inevitably expanding over words with (strongly) connotative sentiments rather than explicit sentiments alone (e.g., “gun”), little prior work has directly tackled this problem of learning connotation,2 and much of the subtle connotation of many seemingly objective words is yet to be determined. 1Our learned lexicon correctly assigns negative polarity to emission. 2A notable exception would be the work of Feng et al. 1774 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</context>
<context position="18521" citStr="Wilson et al. (2005" startWordPosition="2918" endWordPosition="2922"> for two practical reasons: first, it is highly unlikely that any word with non-neutral sentiment (i.e., positive or negative) would carry connotation of the opposite, i.e., conflicting10 polarity. Second, for some words with distinct sentiment or strong connotation, it can be difficult or even unnatural to draw a precise distinction between connotation and sentiment, e.g., “efficient”. Therefore, sentiment lexicons can serve as a surrogate to measure a subset of connotation words induced by the algorithms, as shown in Table 3 with respect to General Inquirer (Stone and Hunt (1963)) and MPQA (Wilson et al. (2005b)).11 Discussion Table 3 shows the agreement statistics with respect to two conventional sentiment lexicons. We find that the use of label propagation alone [PRED-ARG (CP)] improves the performance substantially over the comparable graph construction with different graph analysis algorithms, in particular, HITS and PageRank approaches of Feng et al. (2011). The two completely connected variants of the graph propagation on the Pred-Arg graph, [® PRED-ARG (PMI)] and [® PRED-ARG (CP)], do not necessarily improve the performance over the simpler and computationally lighter alternative, [PREDARG (</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Xie</author>
<author>Chunping Li</author>
</authors>
<title>Lexicon construction: A topic model approach.</title>
<date>2012</date>
<booktitle>In Systems and Informatics (ICSAI), 2012 International Conference on,</booktitle>
<pages>2299--2303</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="34945" citStr="Xie and Li, 2012" startWordPosition="5741" endWordPosition="5744"> constructing a web-scale polarity lexicon for sentiment analysis. Although we employ the same graph propagation algorithm, our graph construction is fundamentally different in that we integrate stronger inductive biases into the graph topology and the corresponding edge weights. As shown in our experimental results, we find that judicious construction of graph structure, exploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially. Other interesting approaches include one based on min-cut (Dong et al., 2012) or LDA (Xie and Li, 2012). Our proposed approaches are more suitable for encoding a much diverse set of linguistic phenomena however. But our work use a few seed predicates with selectional preference instead of relying on word similarity. Some recent work explored the use of constraint optimization framework for inducing domain-dependent sentiment lexicon (Choi and Cardie (2009), Lu et al. (2011)). Our work differs in that we provide comprehensive insights into different formulations of ILP and LP, aiming to learn the much different task of learning the general connotation of words. 7 Conclusion We presented a broad-</context>
</contexts>
<marker>Xie, Li, 2012</marker>
<rawString>Rui Xie and Chunping Li. 2012. Lexicon construction: A topic model approach. In Systems and Informatics (ICSAI), 2012 International Conference on, pages 2299–2303. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation. In</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107.</tech>
<institution>CarnegieMellon University.</institution>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. In Technical Report CMU-CALD-02-107. CarnegieMellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>