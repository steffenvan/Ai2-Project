<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000728">
<title confidence="0.9774165">
Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-Speech
Information
</title>
<author confidence="0.961894">
Serguei PAKHOMOV
</author>
<affiliation confidence="0.792798">
Division of Medical Informatics
Research, Mayo Clinic
</affiliation>
<address confidence="0.77195">
Rochester, MN
</address>
<email confidence="0.985647">
Pakhomov.Serguei@mayo.edu
</email>
<note confidence="0.6984595">
Anni CODEN
IBM, T.J. Watson Research
Center,
Hawthorne, NY 10532
</note>
<email confidence="0.933882">
anni@us.ibm.com
</email>
<author confidence="0.926808">
Christopher CHUTE
</author>
<affiliation confidence="0.81735">
Division of Medical
Informatics Research, Mayo
</affiliation>
<address confidence="0.647729">
Clinic Rochester, MN
</address>
<email confidence="0.98874">
Chute@mayo.edu
</email>
<sectionHeader confidence="0.992412" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999696615384616">
This paper presents a project whose main goal
is to construct a corpus of clinical text
manually annotated for part-of-speech
information. We describe and discuss the
process of training three domain experts to
perform linguistic annotation. We list some of
the challenges as well as encouraging results
pertaining to inter-rater agreement and
consistency of annotation. We also present
preliminary experimental results indicating the
necessity for adapting state-of-the-art POS
taggers to the sublanguage domain of medical
text.
</bodyText>
<sectionHeader confidence="0.998488" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917905405406">
Having reliable part-of-speech (POS)
information is critical to successful implementation
of Natural Language Processing (NLP) techniques
for processing unrestricted text in the biomedical
domain. State-of-the-art automated POS taggers
achieve accuracy of 93% - 98% and the most
successful implementations are based on statistical
approaches to POS tagging. Taggers based on
Hidden Markoff Model (HMM) technology
currently appear to be in the lead. The prime public
domain examples of such implementations include
the Trigrams’n’Tags tagger (Brandts 2000), Xerox
tagger (Cutting et al. 1992) and LT POS tagger
(Mikheev 1997). Maximum Entropy (MaxEnt)
based taggers also seem to perform very well
(Ratnaparkhi 1996, Jason Baldridge, Tom Morton,
and Gann Bierner http://maxent.sourceforge.net ).
One of the issues with statistical POS taggers is
that most of them need a representative amount of
hand-labeled training data either in the form of a
comprehensive lexicon and a corpus of untagged
data or a large corpus of text annotated for POS or
a combination of the two. Currently, most of the
POS tagger accuracy reports are based on the
experiments involving Penn Treebank data
(Marcus, 1993). The texts in Treebank represent
the general English domain. It is not entirely clear
how representative the general English language
vocabulary and structure are of a specialized sub-
domain such as clinical reports.
A well-recognized problem is that the accuracy
of all current POS taggers drops dramatically on
unknown words. For example, while the TnT
tagger performs at 97% accuracy on known words
in the Treebank, the accuracy drops to 89% on
unknown words (Brandts, 2000). The LT POS
tagger is reported to perform at 93.6-94.3%
accuracy on known words and at 87.7-88.7% on
unknown words using a cascading unknown word
“guesser” (Mikheev, 1997). The overall results for
both of these taggers are much closer to the high
end of the spectrum because the rate of the
unknown words in the tests performed on the Penn
Treebank corpus is generally relatively low – 2.9%
(Brandts, 2000). From these results, we can
conclude that the higher the rate of unknown
vocabulary, the lower the overall accuracy will be,
necessitating the adaptation of the taggers trained
on Penn Treebank to sublanguage domains with
vocabulary that is substantially different from the
one represented by the Penn Treebank corpus.
Based on the observable differences between
the clinical and the general English discourse and
POS tagging accuracy results on unknown
vocabulary, it is reasonable to assume that a tagger
trained on general English may not perform as well
on clinical notes, where the percentage of unknown
words will increase. To test this assumption, a
“gold standard” corpus of clinical notes needs to be
manually annotated for POS information. The
issues with the annotation process constitute the
primary focus of this paper.
We describe an effort to train three medical
coding experts to mark the text of clinical notes for
part-of-speech information. The motivation for
using medical coders rather than trained linguists is
threefold. First of all, due to confidentiality
restrictions, in order to develop a corpus of hand
labeled data from clinical notes one can only use
personnel authorized to access patient information.
The only way to avoid it, is to anonymize the notes
prior to POS tagging which in itself is a difficult
and expensive process (Ruch et al. 2000). Second,
medical coding experts are well familiar with
</bodyText>
<page confidence="0.998449">
62
</page>
<bodyText confidence="0.9998846875">
clinical discourse, which helps especially with
annotating medicine specific vocabulary. Third,
the fact that POS tagging can be viewed as a
classification task makes the medical coding
experts highly suitable because their primary
occupation and expertise is in classifying patient
records for subsequent retrieval.
We show that, given a good set of guidelines,
medical coding experts can be trained in a limited
amount of time to perform a linguistic task such as
POS annotation at a high level of agreement on
both clinical notes and Penn Treebank data.
Finally, we report on a set of training experiments
performed with the TnT tagger (Brandts, 2000)
using the Penn Treebank as well as the newly
developed medical corpus..
</bodyText>
<sectionHeader confidence="0.973062" genericHeader="introduction">
2 Annotation
</sectionHeader>
<bodyText confidence="0.99998280952381">
Prior to this study, the three annotators who
participated in it had a substantial experience in
coding clinical diagnoses but virtually no
experience in POS markup. The training process
consisted of a general and rather superficial
introduction to the issues in linguistics as well as
some formal training using the POS tagging
guidelines developed by Santoriny (1991) for
tagging Penn Treebank data. The formal training
was followed by informal discussions of the data
and difficult cases pertinent to the clinical notes
domain which often resulted in slight
modifications to the Penn Treebank guidelines.
The annotation process consisted of
preprocessing and editing. The pre-processing
includes sentence boundary detection, tokenization
and priming with part-of-speech tags generated by
a MaxEnt tagger (Maxent 1.2.4 package (Baldridge
et al.)) trained on Penn Treebank data.
Automatically annotated notes were then presented
to the domain experts for editing.
</bodyText>
<sectionHeader confidence="0.982429" genericHeader="method">
3 Annotator agreement
</sectionHeader>
<bodyText confidence="0.999986882352941">
In order to establish reliability of the data, we
need to ensure internal as well as external
consistency of the annotation. First of all, we need
to make sure that the annotators agree amongst
themselves (internal consistency) on how they
mark up text for part-of-speech information.
Second, we need to find out how closely the
annotators generating data for this study agree with
the annotators of an established project such as
Penn Treebank (external consistency). If both tests
show relatively high levels of agreement, then we
can safely assume that the annotators in this study
are able to generate part-of-speech tags for
biomedical data that will be consistent with a
widely recognized standard and can work
independently of each other thus tripling the
amount of manually annotated data.
</bodyText>
<subsectionHeader confidence="0.986203">
3.1 Methods
</subsectionHeader>
<bodyText confidence="0.999935">
Two types of measures of consistency were
computed – absolute agreement and Kappa
coefficient. The absolute agreement (Abs Agr) was
calculated by dividing the total number of times all
annotators agreed on a tag over the total number
of tags.
</bodyText>
<equation confidence="0.968941">
Kappa coefficient is given in (1) (Carletta 1996)
P(A) − P(E)
1( )
−P E
</equation>
<bodyText confidence="0.999948285714286">
where P(A) is the proportion of times the
annotators actually agree and P(E) is the
proportion of times the annotators are expected to
agree due to chance3.
The Absolute Agreement is most informative
when computed over several sets of labels and
where one of the sets represents the “authoritative”
set. In this case, the ratio of matches among all the
sets including the “authoritative” set to the total
number of labels shows how close the other sets
are to the “authoritative” one. The Kappa statistic
is useful in measuring how consistent the
annotators are compared to each other as opposed
to an authority standard.
</bodyText>
<subsectionHeader confidence="0.999771">
3.2 Annotator consistency
</subsectionHeader>
<bodyText confidence="0.999321">
In order to test for internal consistency, we
analyzed inter-annotator agreement where the three
annotators tagged the same small corpus of clinical
dictations.
</bodyText>
<table confidence="0.999044">
Fil Abs agr. Kappa N Samples
e ID
1137689 93.24% 0.9527 755
1165875 94.59% 0.9622 795
1283904 89.79% 0.9302 392
1284881 90.42% 0.9328 397
1307526 84.43% 0.8943 347
Total 2686
Average 90.49% 0.9344
</table>
<tableCaption confidence="0.7377265">
Table 1. Annotator agreement results based on 5
clinical notes
</tableCaption>
<footnote confidence="0.68572175">
3 A very detailed explanation of the terms used in the formula for
Kappa computation as well as concrete examples of how it is
computed are pro
vided in Poessio and Vieira (1988).
</footnote>
<equation confidence="0.699950333333333">
=
(1)
Kappa
</equation>
<page confidence="0.985427">
63
</page>
<bodyText confidence="0.999628285714286">
The results were compared and the Kappa-
statistic was used to calculate the inter-annotator
agreement. The results of this experiment are
summarized in Table 1. For the absolute
agreement, we computed the ratio of how many
times all three annotators agreed on a tag for a
given token to the total number of tags.
Based on the small pilot sample of 5 clinical
notes (2686 words), the Kappa test showed a very
high agreement coefficient – 0.93. An acceptable
agreement for most NLP classification tasks lies
between 0.7 and 0.8 (Carletta 1996, Poessio and
Vieira 1988). Absolute agreement numbers are
consistent with high Kappa as they show an
average of 90% of all tags in the test documents
assigned exactly the same way by all three
annotators.
The external consistency with the Penn Treebank
annotation was computed using a small random
sample of 939 words from the Penn Treebank
Corpus annotated for POS information.
</bodyText>
<table confidence="0.9946974">
Annotator Abs agr
A1 88.17%
A2 87.85%
A3 87.85%
Average 87.95%
</table>
<tableCaption confidence="0.999718">
Table 2. Absolute agreement results based on 5
</tableCaption>
<bodyText confidence="0.938899">
clinical notes with an “authority” label set.
The results in Table 2 show that the three
annotators are on average 88% consistent with the
annotators of the Penn Treebank corpus.
</bodyText>
<subsectionHeader confidence="0.8893135">
3.3 Descriptive statistics for the corpus of
clinical notes
</subsectionHeader>
<bodyText confidence="0.999074166666667">
The annotation process resulted in a corpus of
273 clinical notes annotated with POS tags. The
corpus contains 100650 tokens from 8702 types
distributed across 7299 sentences. Table 3 displays
frequency counts for the top most frequent
syntactic categories.
</bodyText>
<table confidence="0.998954166666667">
Category Count % total
NN 18372 18%
IN 8963 9%
JJ 8851 9%
DT 6796 7%
NNP 4794 5%
</table>
<tableCaption confidence="0.7970115">
Table 3 Syntactic category distribution in the
corpus of clinical notes.
</tableCaption>
<bodyText confidence="0.999674294117647">
The distribution of syntactic categories suggests
the predominance of nominal categories, which is
consistent with the nature of clinical notes
reporting on various patient characteristics such as
disorders, signs and symptoms.
Another important descriptive characteristic of
this corpus is that the average sentence length is
13.79 tokens per sentence, which is relatively short
as compared to the Treebank corpus where the
average sentence length is 24.16 tokens per
sentence. This supports our informal observation
of the clinical notes data containing multiple
sentence fragments and short diagnostic
statements. Shorter sentence length implies greater
number of inter-sentential transitions and therefore
is likely to present a challenge for a stochastic
process.
</bodyText>
<sectionHeader confidence="0.612768" genericHeader="method">
4 Training a POS tagger on medical data
</sectionHeader>
<bodyText confidence="0.995425631578947">
In order to test some of our assumptions
regarding how the differences between general
English language and the language of clinical notes
may affect POS tagging, we have trained the
HMM-based TnT tagger (Brandts, 2000) with
default parameters at the tri-gram level both on
Penn Treebank and the clinical notes data. We
should also note that the tagger relies on a
sophisticated “unknown” word guessing algorithm
which computes the likelihood of a tag based on
the N last letters of the word, which is meant to
leverage the word’s morphology in a purely
statistical manner.
The clinical notes data was split at random 10
times in 80/20 fashion where 80% of the sentences
were used for training and 20% were used for
testing. This technique is a variation on the classic
10-fold validation and appears to be more suitable
for smaller amounts of data.
We conducted two experiments. First, we
computed the correctness of the Treebank model
on each fold of the clinical notes data. We tested
the Treebank model on the 10 folds rather than the
whole corpus of clinical notes in order to produce
correctness results on exactly the same test data as
would be used for validation tests of models build
from the clinical notes data. Then, we computed
the correctness of each of the 10 models trained on
each training fold of the clinical notes data using
the corresponding testing fold of the same data for
testing.
Table 4 Correctness results for the Treebank
model.
Correctness was computed simply as the
percentage of correct tag assignments of the POS
tagger (hits) to the total number of tokens in the
test set. Table 4 summarizes the results of testing
the Treebank model, while Table 5 summarizes the
</bodyText>
<figure confidence="0.948261625">
Split
Hits
Total
Correctness
Average
21826.3
24309
89.79%
</figure>
<page confidence="0.997301">
64
</page>
<bodyText confidence="0.989759818181818">
testing results for the models trained on the clinical
notes.
The average correctness of the Treebank model
tested on clinical notes is ~88%, which is
considerably lower than the state-of-the-art
performance of the TnT tagger - ~96%. Training
the tagger on a relatively small amount of clinical
notes data brings the performance much closer to
the state-of-the-art – ~95%.
Table 5 Correctness results for the clinical notes
model.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999989290322581">
The results of this pilot project are encouraging.
It is clear that with appropriate supervision, people
who are well familiar with medical content can be
reliably trained to carry out some of the tasks
traditionally done by trained linguists.
This study also indicates that an automatic POS
tagger trained on data that does not include clinical
documents may not perform as well as a tagger
trained on data from the same domain. A
comparison between the Treebank and the clinical
notes data shows that the clinical notes corpus
contains 3,239 lexical items that are not found in
Treebank. The Treebank corpus contains over
40,000 lexical items that are not found in the
corpus of clinical notes. 5,463 lexical items are
found in both corpora. In addition to this 37% out-
of-vocabulary rate (words in clinical notes but not
the Treebank corpus), the picture is further
complicated by the differences between the n-gram
tag transitions within the two corpora. For
example, the likelihood of a DT 4 NN bigram is 1
in Treebank and 0.75 in the clinical notes corpus.
On the other hand, JJ 4 NN transition in the
clinical notes is 1 but in the Treebank corpus it has
a likelihood of 0.73. This is just to illustrate the
fact that not only the “unknown” out-of-vocabulary
items may be responsible for the decreased
accuracy of POS taggers trained on general
English domain and tested on the clinical notes
domain, but the actual n-gram statistics may be a
major contributing factor.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999905846153846">
Several questions remain unresolved. First of all,
it is unclear how much domain specific data is
enough to achieve state-of-the-art performance on
POS tagging. Second, given that it is somewhat
easier to develop lexicons for POS tagging than to
annotate corpora, we need to find out how
important the corpus statistics are as opposed to a
domain specific lexicon. In other words, can we
achieve state-of-the-art performance in a
specialized domain by simply adding the
vocabulary from the domain to the POS tagger’s
lexicon? We intend to address both of these
questions with further experimentation.
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999936">
Our thanks go to Barbara Abbot, Pauline Funk
and Debora Albrecht for their persistent efforts in
the difficult task of corpus annotation. This work
has been carried out under the NLM Training
Grant # T15 LM07041-19.
</bodyText>
<sectionHeader confidence="0.999304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999544297297297">
Baldridge, J., Morton, T., and Bierner, G URL:
http://maxent.sourceforge.net
Brandts, T (2000) “TnT – A Statistical Part-of-Speech
Tagger.” In Proc. NAACL/ANLP-2000.
Carletta, J. (1996). Assiessing agreement on
classification tasks: The Kappa statistic.
Computational Linguistics, 22(2) pp. 249-254.
Cutting, D., Kupiec, J., Pedersen, J, and Sibun, P. A
(1992). Practical POS Tagger. In Proc. ANLP’92.
Jurafski D. and Martin J. (2000). Speech and Language
Processing. Prentice Hall, NJ.
Manning, C. and Shutze H. (1999). Foundations of
Statistical Natural Language Processing. MIT Press,
Cambridge, MA.
Marcus, M., B. Santorini, and M. A. Marcinkiewicz
(1993). Building a large annotated corpus of English:
the Penn Treebank. Computational Linguistics 19,
297-352.
Mikheev, A. (1997). Automatic Rule Induction for
Unknown-Word Guessing. Computational Linguistics
23(3): 405-423
Poessio, M. and Vieira, R. (1988). “A corpus based
investigation of definite description use”
Computational Linguistics, pp 186-215.
Ratnaparkhi A. (1996). A maximum entropy part of
speech tagger. In Proceedings of the conference on
empirical methods in natural language processing,
May 1996, University of Pennsylvania
Ruch P, Baud RH, Rassinoux AM, Bouillon P, Robert
G. Medical document anonymization with a semantic
lexicon. Proc AMIA Symp. 2000; 729-33.
Santorini B. (1991). Part-of-Speech Tagging Guidelines
for the Penn Treebank Project. Technical Report.
Department of Computer and Information Science,
University of Pennsylvania.
UMLS. (2001). UMLS Knowledge Sources (12th ed.).
Bethesda (MD): National Library of Medicine.
</reference>
<figure confidence="0.99507625">
Split
Hits
Total
Correctness
Average
23018.4
24309
94.69%
</figure>
<page confidence="0.991297">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.267488">
<title confidence="0.9991265">Creating a Test Corpus of Clinical Notes Manually Tagged for Part-of-Speech Information</title>
<author confidence="0.701364">Serguei PAKHOMOV</author>
<affiliation confidence="0.6948535">Division of Medical Informatics Research, Mayo Clinic</affiliation>
<address confidence="0.722269">Rochester, MN</address>
<email confidence="0.831251">Pakhomov.Serguei@mayo.edu</email>
<author confidence="0.968741">Anni CODEN</author>
<affiliation confidence="0.9908125">IBM, T.J. Watson Research Center,</affiliation>
<address confidence="0.980639">Hawthorne, NY</address>
<email confidence="0.998709">anni@us.ibm.com</email>
<author confidence="0.999025">Christopher CHUTE</author>
<affiliation confidence="0.9509935">Division of Medical Informatics Research, Mayo</affiliation>
<address confidence="0.822885">Clinic Rochester, MN</address>
<email confidence="0.918338">Chute@mayo.edu</email>
<abstract confidence="0.9913805">This paper presents a project whose main goal is to construct a corpus of clinical text manually annotated for part-of-speech information. We describe and discuss the process of training three domain experts to perform linguistic annotation. We list some of the challenges as well as encouraging results pertaining to inter-rater agreement and consistency of annotation. We also present preliminary experimental results indicating the necessity for adapting state-of-the-art POS taggers to the sublanguage domain of medical text.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>J Baldridge</author>
<author>T Morton</author>
<author>G Bierner</author>
</authors>
<note>URL: http://maxent.sourceforge.net</note>
<marker>Baldridge, Morton, Bierner, </marker>
<rawString>Baldridge, J., Morton, T., and Bierner, G URL: http://maxent.sourceforge.net</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brandts</author>
</authors>
<title>TnT – A Statistical Part-of-Speech Tagger.” In</title>
<date>2000</date>
<booktitle>Proc. NAACL/ANLP-2000.</booktitle>
<contexts>
<context position="1490" citStr="Brandts 2000" startWordPosition="200" endWordPosition="201">to the sublanguage domain of medical text. 1 Introduction Having reliable part-of-speech (POS) information is critical to successful implementation of Natural Language Processing (NLP) techniques for processing unrestricted text in the biomedical domain. State-of-the-art automated POS taggers achieve accuracy of 93% - 98% and the most successful implementations are based on statistical approaches to POS tagging. Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams’n’Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997). Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net ). One of the issues with statistical POS taggers is that most of them need a representative amount of hand-labeled training data either in the form of a comprehensive lexicon and a corpus of untagged data or a large corpus of text annotated for POS or a combination of the two. Currently, most of the POS tagger accuracy reports are based on the experiments invol</context>
<context position="3012" citStr="Brandts, 2000" startWordPosition="449" endWordPosition="450"> current POS taggers drops dramatically on unknown words. For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words (Brandts, 2000). The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word “guesser” (Mikheev, 1997). The overall results for both of these taggers are much closer to the high end of the spectrum because the rate of the unknown words in the tests performed on the Penn Treebank corpus is generally relatively low – 2.9% (Brandts, 2000). From these results, we can conclude that the higher the rate of unknown vocabulary, the lower the overall accuracy will be, necessitating the adaptation of the taggers trained on Penn Treebank to sublanguage domains with vocabulary that is substantially different from the one represented by the Penn Treebank corpus. Based on the observable differences between the clinical and the general English discourse and POS tagging accuracy results on unknown vocabulary, it is reasonable to assume that a tagger trained on general English may not perform as well on clinical notes, where the percentage o</context>
<context position="5104" citStr="Brandts, 2000" startWordPosition="782" endWordPosition="783">ly with annotating medicine specific vocabulary. Third, the fact that POS tagging can be viewed as a classification task makes the medical coding experts highly suitable because their primary occupation and expertise is in classifying patient records for subsequent retrieval. We show that, given a good set of guidelines, medical coding experts can be trained in a limited amount of time to perform a linguistic task such as POS annotation at a high level of agreement on both clinical notes and Penn Treebank data. Finally, we report on a set of training experiments performed with the TnT tagger (Brandts, 2000) using the Penn Treebank as well as the newly developed medical corpus.. 2 Annotation Prior to this study, the three annotators who participated in it had a substantial experience in coding clinical diagnoses but virtually no experience in POS markup. The training process consisted of a general and rather superficial introduction to the issues in linguistics as well as some formal training using the POS tagging guidelines developed by Santoriny (1991) for tagging Penn Treebank data. The formal training was followed by informal discussions of the data and difficult cases pertinent to the clinic</context>
<context position="11274" citStr="Brandts, 2000" startWordPosition="1767" endWordPosition="1768"> where the average sentence length is 24.16 tokens per sentence. This supports our informal observation of the clinical notes data containing multiple sentence fragments and short diagnostic statements. Shorter sentence length implies greater number of inter-sentential transitions and therefore is likely to present a challenge for a stochastic process. 4 Training a POS tagger on medical data In order to test some of our assumptions regarding how the differences between general English language and the language of clinical notes may affect POS tagging, we have trained the HMM-based TnT tagger (Brandts, 2000) with default parameters at the tri-gram level both on Penn Treebank and the clinical notes data. We should also note that the tagger relies on a sophisticated “unknown” word guessing algorithm which computes the likelihood of a tag based on the N last letters of the word, which is meant to leverage the word’s morphology in a purely statistical manner. The clinical notes data was split at random 10 times in 80/20 fashion where 80% of the sentences were used for training and 20% were used for testing. This technique is a variation on the classic 10-fold validation and appears to be more suitabl</context>
</contexts>
<marker>Brandts, 2000</marker>
<rawString>Brandts, T (2000) “TnT – A Statistical Part-of-Speech Tagger.” In Proc. NAACL/ANLP-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assiessing agreement on classification tasks: The Kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<contexts>
<context position="7279" citStr="Carletta 1996" startWordPosition="1118" endWordPosition="1119">relatively high levels of agreement, then we can safely assume that the annotators in this study are able to generate part-of-speech tags for biomedical data that will be consistent with a widely recognized standard and can work independently of each other thus tripling the amount of manually annotated data. 3.1 Methods Two types of measures of consistency were computed – absolute agreement and Kappa coefficient. The absolute agreement (Abs Agr) was calculated by dividing the total number of times all annotators agreed on a tag over the total number of tags. Kappa coefficient is given in (1) (Carletta 1996) P(A) − P(E) 1( ) −P E where P(A) is the proportion of times the annotators actually agree and P(E) is the proportion of times the annotators are expected to agree due to chance3. The Absolute Agreement is most informative when computed over several sets of labels and where one of the sets represents the “authoritative” set. In this case, the ratio of matches among all the sets including the “authoritative” set to the total number of labels shows how close the other sets are to the “authoritative” one. The Kappa statistic is useful in measuring how consistent the annotators are compared to eac</context>
<context position="9105" citStr="Carletta 1996" startWordPosition="1427" endWordPosition="1428">it is computed are pro vided in Poessio and Vieira (1988). = (1) Kappa 63 The results were compared and the Kappastatistic was used to calculate the inter-annotator agreement. The results of this experiment are summarized in Table 1. For the absolute agreement, we computed the ratio of how many times all three annotators agreed on a tag for a given token to the total number of tags. Based on the small pilot sample of 5 clinical notes (2686 words), the Kappa test showed a very high agreement coefficient – 0.93. An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8 (Carletta 1996, Poessio and Vieira 1988). Absolute agreement numbers are consistent with high Kappa as they show an average of 90% of all tags in the test documents assigned exactly the same way by all three annotators. The external consistency with the Penn Treebank annotation was computed using a small random sample of 939 words from the Penn Treebank Corpus annotated for POS information. Annotator Abs agr A1 88.17% A2 87.85% A3 87.85% Average 87.95% Table 2. Absolute agreement results based on 5 clinical notes with an “authority” label set. The results in Table 2 show that the three annotators are on ave</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, J. (1996). Assiessing agreement on classification tasks: The Kappa statistic. Computational Linguistics, 22(2) pp. 249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>A</title>
<date>1992</date>
<booktitle>Proc. ANLP’92.</booktitle>
<contexts>
<context position="1526" citStr="Cutting et al. 1992" startWordPosition="204" endWordPosition="207">medical text. 1 Introduction Having reliable part-of-speech (POS) information is critical to successful implementation of Natural Language Processing (NLP) techniques for processing unrestricted text in the biomedical domain. State-of-the-art automated POS taggers achieve accuracy of 93% - 98% and the most successful implementations are based on statistical approaches to POS tagging. Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams’n’Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997). Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net ). One of the issues with statistical POS taggers is that most of them need a representative amount of hand-labeled training data either in the form of a comprehensive lexicon and a corpus of untagged data or a large corpus of text annotated for POS or a combination of the two. Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 199</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Cutting, D., Kupiec, J., Pedersen, J, and Sibun, P. A (1992). Practical POS Tagger. In Proc. ANLP’92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafski</author>
<author>J Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice Hall, NJ.</publisher>
<marker>Jurafski, Martin, 2000</marker>
<rawString>Jurafski D. and Martin J. (2000). Speech and Language Processing. Prentice Hall, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Shutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Shutze, 1999</marker>
<rawString>Manning, C. and Shutze H. (1999). Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>297--352</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., B. Santorini, and M. A. Marcinkiewicz (1993). Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics 19, 297-352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>Automatic Rule Induction for Unknown-Word Guessing.</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<issue>3</issue>
<pages>405--423</pages>
<contexts>
<context position="1559" citStr="Mikheev 1997" startWordPosition="212" endWordPosition="213">able part-of-speech (POS) information is critical to successful implementation of Natural Language Processing (NLP) techniques for processing unrestricted text in the biomedical domain. State-of-the-art automated POS taggers achieve accuracy of 93% - 98% and the most successful implementations are based on statistical approaches to POS tagging. Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams’n’Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997). Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net ). One of the issues with statistical POS taggers is that most of them need a representative amount of hand-labeled training data either in the form of a comprehensive lexicon and a corpus of untagged data or a large corpus of text annotated for POS or a combination of the two. Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993). The texts in Treebank represe</context>
<context position="2777" citStr="Mikheev, 1997" startWordPosition="407" endWordPosition="408">he general English domain. It is not entirely clear how representative the general English language vocabulary and structure are of a specialized subdomain such as clinical reports. A well-recognized problem is that the accuracy of all current POS taggers drops dramatically on unknown words. For example, while the TnT tagger performs at 97% accuracy on known words in the Treebank, the accuracy drops to 89% on unknown words (Brandts, 2000). The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word “guesser” (Mikheev, 1997). The overall results for both of these taggers are much closer to the high end of the spectrum because the rate of the unknown words in the tests performed on the Penn Treebank corpus is generally relatively low – 2.9% (Brandts, 2000). From these results, we can conclude that the higher the rate of unknown vocabulary, the lower the overall accuracy will be, necessitating the adaptation of the taggers trained on Penn Treebank to sublanguage domains with vocabulary that is substantially different from the one represented by the Penn Treebank corpus. Based on the observable differences between t</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Mikheev, A. (1997). Automatic Rule Induction for Unknown-Word Guessing. Computational Linguistics 23(3): 405-423</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poessio</author>
<author>R Vieira</author>
</authors>
<title>A corpus based investigation of definite description use” Computational Linguistics,</title>
<date>1988</date>
<pages>186--215</pages>
<contexts>
<context position="8549" citStr="Poessio and Vieira (1988)" startWordPosition="1329" endWordPosition="1332">3.2 Annotator consistency In order to test for internal consistency, we analyzed inter-annotator agreement where the three annotators tagged the same small corpus of clinical dictations. Fil Abs agr. Kappa N Samples e ID 1137689 93.24% 0.9527 755 1165875 94.59% 0.9622 795 1283904 89.79% 0.9302 392 1284881 90.42% 0.9328 397 1307526 84.43% 0.8943 347 Total 2686 Average 90.49% 0.9344 Table 1. Annotator agreement results based on 5 clinical notes 3 A very detailed explanation of the terms used in the formula for Kappa computation as well as concrete examples of how it is computed are pro vided in Poessio and Vieira (1988). = (1) Kappa 63 The results were compared and the Kappastatistic was used to calculate the inter-annotator agreement. The results of this experiment are summarized in Table 1. For the absolute agreement, we computed the ratio of how many times all three annotators agreed on a tag for a given token to the total number of tags. Based on the small pilot sample of 5 clinical notes (2686 words), the Kappa test showed a very high agreement coefficient – 0.93. An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8 (Carletta 1996, Poessio and Vieira 1988). Absolute agreeme</context>
</contexts>
<marker>Poessio, Vieira, 1988</marker>
<rawString>Poessio, M. and Vieira, R. (1988). “A corpus based investigation of definite description use” Computational Linguistics, pp 186-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part of speech tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the conference on empirical methods in natural language processing,</booktitle>
<institution>University of Pennsylvania</institution>
<contexts>
<context position="1648" citStr="Ratnaparkhi 1996" startWordPosition="225" endWordPosition="226">ral Language Processing (NLP) techniques for processing unrestricted text in the biomedical domain. State-of-the-art automated POS taggers achieve accuracy of 93% - 98% and the most successful implementations are based on statistical approaches to POS tagging. Taggers based on Hidden Markoff Model (HMM) technology currently appear to be in the lead. The prime public domain examples of such implementations include the Trigrams’n’Tags tagger (Brandts 2000), Xerox tagger (Cutting et al. 1992) and LT POS tagger (Mikheev 1997). Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net ). One of the issues with statistical POS taggers is that most of them need a representative amount of hand-labeled training data either in the form of a comprehensive lexicon and a corpus of untagged data or a large corpus of text annotated for POS or a combination of the two. Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993). The texts in Treebank represent the general English domain. It is not entirely clear how representative the general En</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi A. (1996). A maximum entropy part of speech tagger. In Proceedings of the conference on empirical methods in natural language processing, May 1996, University of Pennsylvania</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
<author>Baud RH</author>
<author>Rassinoux AM</author>
<author>P Bouillon</author>
<author>G Robert</author>
</authors>
<title>Medical document anonymization with a semantic lexicon.</title>
<date>2000</date>
<booktitle>Proc AMIA Symp.</booktitle>
<pages>729--33</pages>
<contexts>
<context position="4391" citStr="Ruch et al. 2000" startWordPosition="667" endWordPosition="670">with the annotation process constitute the primary focus of this paper. We describe an effort to train three medical coding experts to mark the text of clinical notes for part-of-speech information. The motivation for using medical coders rather than trained linguists is threefold. First of all, due to confidentiality restrictions, in order to develop a corpus of hand labeled data from clinical notes one can only use personnel authorized to access patient information. The only way to avoid it, is to anonymize the notes prior to POS tagging which in itself is a difficult and expensive process (Ruch et al. 2000). Second, medical coding experts are well familiar with 62 clinical discourse, which helps especially with annotating medicine specific vocabulary. Third, the fact that POS tagging can be viewed as a classification task makes the medical coding experts highly suitable because their primary occupation and expertise is in classifying patient records for subsequent retrieval. We show that, given a good set of guidelines, medical coding experts can be trained in a limited amount of time to perform a linguistic task such as POS annotation at a high level of agreement on both clinical notes and Penn</context>
</contexts>
<marker>Ruch, RH, AM, Bouillon, Robert, 2000</marker>
<rawString>Ruch P, Baud RH, Rassinoux AM, Bouillon P, Robert G. Medical document anonymization with a semantic lexicon. Proc AMIA Symp. 2000; 729-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Santorini</author>
</authors>
<title>Part-of-Speech Tagging Guidelines for the Penn Treebank Project.</title>
<date>1991</date>
<tech>Technical Report.</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<marker>Santorini, 1991</marker>
<rawString>Santorini B. (1991). Part-of-Speech Tagging Guidelines for the Penn Treebank Project. Technical Report. Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>UMLS</author>
</authors>
<date>2001</date>
<booktitle>UMLS Knowledge Sources (12th ed.). Bethesda (MD): National Library of Medicine.</booktitle>
<marker>UMLS, 2001</marker>
<rawString>UMLS. (2001). UMLS Knowledge Sources (12th ed.). Bethesda (MD): National Library of Medicine.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>