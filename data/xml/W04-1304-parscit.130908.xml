<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.077459">
<title confidence="0.816529">
Grammatical Inference and First Language Acquisition
</title>
<author confidence="0.555524">
Alexander Clark (asc@aclark.demon.co.uk)
</author>
<affiliation confidence="0.475106">
ISSCO / TIM, University of Geneva
</affiliation>
<note confidence="0.716282">
UNI-MAIL, Boulevard du Pont-d’Arve,
CH-1211 Geneve 4, Switzerland
</note>
<sectionHeader confidence="0.971414" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994112">
One argument for parametric models of language
has been learnability in the context of first language
acquisition. The claim is made that “logical” ar-
guments from learnability theory require non-trivial
constraints on the class of languages. Initial formal-
isations of the problem (Gold, 1967) are however
inapplicable to this particular situation. In this pa-
per we construct an appropriate formalisation of the
problem using a modern vocabulary drawn from sta-
tistical learning theory and grammatical inference
and looking in detail at the relevant empirical facts.
We claim that a variant of the Probably Approxi-
mately Correct (PAC) learning framework (Valiant,
1984) with positive samples only, modified so it is
not completely distribution free is the appropriate
choice. Some negative results derived from crypto-
graphic problems (Kearns et al., 1994) appear to ap-
ply in this situation but the existence of algorithms
with provably good performance (Ron et al., 1995)
and subsequent work, shows how these negative re-
sults are not as strong as they initially appear, and
that recent algorithms for learning regular languages
partially satisfy our criteria. We then discuss the
applicability of these results to parametric and non-
parametric models.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968736842106">
For some years, the relevance of formal results
in grammatical inference to the empirical question
of first language acquisition by infant children has
been recognised (Wexler and Culicover, 1980). Un-
fortunately, for many researchers, with a few no-
table exceptions (Abe, 1988), this begins and ends
with Gold’s famous negative results in the identifi-
cation in the limit paradigm. This paradigm, though
still widely used in the grammatical inference com-
munity, is clearly of limited relevance to the issue
at hand, since it requires the model to be able to
exactly identify the target language even when an
adversary can pick arbitrarily misleading sequences
of examples to provide. Moreover, the paradigm as
stated has no bounds on the amount of data or com-
putation required for the learner. In spite of the inap-
plicability of this particular paradigm, in a suitable
analysis there are quite strong arguments that bear
directly on this problem.
Grammatical inference is the study of machine
learning of formal languages. It has a vast formal
vocabulary and has been applied to a wide selec-
tion of different problems, where the “languages”
under study can be (representations of) parts of nat-
ural languages, sequences of nucleotides, moves of
a robot, or some other sequence data. For any con-
clusions that we draw from formal discussions to
have any applicability to the real world, we must
be sure to select, or construct, from the rich set of
formal devices available an appropriate formalisa-
tion. Even then, we should be very cautious about
making inferences about how the infant child must
or cannot learn language: subsequent developments
in GI might allow a more nuanced description in
which these conclusions are not valid. The situation
is complicated by the fact that the field of grammti-
cal inference, much like the wider field of machine
learning in general, is in a state of rapid change.
In this paper we hope to address this problem by
justifying the selection of the appropriate learning
framework starting by looking at the actual situa-
tion the child is in, rather than from an a priori deci-
sion about the right framework. We will not attempt
a survey of grammatical inference techniques; nor
shall we provide proofs of the theorems we use here.
Arguments based on formal learnability have been
used to support the idea of parameter based theo-
ries of language (Chomsky, 1986). As we shall see
below, under our analysis of the problem these ar-
guments are weak. Indeed, they are more pertinent
to questions about the autonomy and modularity of
language learning: the question whether learning of
some level of linguistic knowledge – morphology
or syntax, for example – can take place in isolation
from other forms of learning, such as the acquisition
of word meaning, and without interaction, ground-
ing and so on.
</bodyText>
<page confidence="0.996327">
25
</page>
<bodyText confidence="0.999733678571428">
Positive results can help us to understand how hu-
mans might learn languages by outlining the class of
algorithms that might be used by humans, consid-
ered as computational systems at a suitable abstract
level. Conversely, negative results might be help-
ful if they could demonstrate that no algorithms of a
certain class could perform the task – in this case we
could know that the human child learns his language
in some other way.
We shall proceed as follows: after briefly de-
scribing FLA, we describe the various elements of
a model of learning, or framework. We then make
a series of decisions based on the empirical facts
about FLA, to construct an appropriate model or
models, avoiding unnecessary idealisation wherever
possible. We proceed to some strong negative re-
sults, well-known in the GI community that bear on
the questions at hand. The most powerful of these
(Kearns et al., 1994) appears to apply quite directly
to our chosen model. We then discuss an interest-
ing algorithm (Ron et al., 1995) which shows that
this can be circumvented, at least for a subclass of
regular languages. Finally, after discussing the pos-
sibilities for extending this result to all regular lan-
guages, and beyond, we conclude with a discussion
of the implications of the results presented for the
distinction between parametric and non-parametric
models.
</bodyText>
<sectionHeader confidence="0.892929" genericHeader="method">
2 First Language Acquisition
</sectionHeader>
<bodyText confidence="0.999973972222222">
Let us first examine the phenomenon we are con-
cerned with: first language acquisition. In the space
of a few years, children almost invariably acquire,
in the absence of explicit instruction, one or more of
the languages that they are exposed to. A multitude
of subsidiary debates have sprung up around this
central issue covering questions about critical peri-
ods – the ages at which this can take place, the ex-
act nature of the evidence available to the child, and
the various phases of linguistic use through which
the infant child passes. In the opinion of many re-
searchers, explaining this ability is one of the most
important challenges facing linguists and cognitive
scientists today.
A difficulty for us in this paper is that many of
the idealisations made in the study of this field are
in fact demonstrably false. Classical assumptions,
such as the existence of uniform communities of
language users, are well-motivated in the study of
the “steady state” of a system, but less so when
studying acquisition and change. There is a regret-
table tendency to slip from viewing these idealisa-
tions correctly – as counter-factual idealizations – to
viewing them as empirical facts that need to be ex-
plained. Thus, when looking for an appropriate for-
mulation of the problem, we should recall for exam-
ple the fact that different children do not converge to
exactly the same knowledge of language as is some-
times claimed, nor do all of them acquire a language
competently at all, since there is a small proportion
of children who though apparently neurologically
normal fail to acquire language. In the context of
our discussion later on, these observations lead us
to accept slightly less stringent criteria where we al-
low a small probability of failure and do not demand
perfect equality of hypothesis and target.
</bodyText>
<sectionHeader confidence="0.990704" genericHeader="method">
3 Grammatical Inference
</sectionHeader>
<bodyText confidence="0.99998155">
The general field of machine learning has a spe-
cialised subfield that deals with the learning of for-
mal languages. This field, Grammatical Inference
(GI), is characterised above all by an interest in for-
mal results, both in terms of formal characterisa-
tions of the target languages, and in terms of formal
proofs either that particular algorithms can learn ac-
cording to particular definitions, or that sets of lan-
guage cannot be learnt. In spite of its theoretical
bent, GI algorithms have also been applied with
some success. Natural language, however is not the
only source of real-world applications for GI. Other
domains include biological sequence data, artificial
languages, such as discovering XML schemas, or
sequences of moves of a robot. The field is also
driven by technical motives and the intrinsic ele-
gance and interest of the mathematical ideas em-
ployed. In summary it is not just about language,
and accordingly it has developed a rich vocabulary
to deal with the wide range of its subject matter.
In particular, researchers are often concerned
with formal results – that is we want algorithms
where we can prove that they will perform in a cer-
tain way. Often, we may be able to empirically es-
tablish that a particular algorithm performs well, in
the sense of reliably producing an accurate model,
while we may be unable to prove formally that the
algorithm will always perform in this way. This
can be for a number of reasons: the mathematics
required in the derivation of the bounds on the er-
rors may be difficult or obscure, or the algorithm
may behave strangely when dealing with sets of data
which are ill-behaved in some way.
The basic framework can be considered as a
game played between two players. One player, the
teacher, provides information to another, the learner,
and from that information the learner must identify
the underlying language. We can break down this
situation further into a number of elements. We as-
sume that the languages to be learned are drawn
</bodyText>
<page confidence="0.992244">
26
</page>
<bodyText confidence="0.991734611111111">
in some way from a possibly infinite class of lan-
guages, L, which is a set of formal mathematical
objects. The teacher selects one of these languages,
which we call the target, and then gives the learner
a certain amount of information of various types
about the target. After a while, the learner then re-
turns its guess, the hypothesis, which in general will
be a language drawn from the same class L. Ide-
ally the learner has been able to deduce or induce
or abduce something about the target from the in-
formation we have given it, and in this case the hy-
pothesis it returns will be identical to, or close in
some technical sense, to the target. If the learner
can conistently do this, under whatever constraints
we choose, then we say it can learn that class of lan-
guages. To turn this vague description into some-
thing more concrete requires us to specify a number
of things.
</bodyText>
<listItem confidence="0.99204975">
• What sort of mathematical object should we
use to represent a language?
• What is the target class of languages?
• What information is the learner given?
• What computational constraints does the
learner operate under?
• How close must the target be to the hypothesis,
and how do we measure it?
</listItem>
<bodyText confidence="0.999946555555556">
This paper addresses the extent to which negative
results in GI could be relevant to this real world sit-
uation. As always, when negative results from the-
ory are being applied, a certain amount of caution
is appropriate in examining the underlying assump-
tions of the theory and the extent to which these are
applicable. As we shall see, in our opinion, none
of the current negative results, though powerful, are
applicable to the empirical situation. We shall ac-
cordingly, at various points, make strong pessimistic
assumptions about the learning environment of the
child, and show that even under these unrealistically
stringent stipulations, the negative results are still
inapplicable. This will make the conclusions we
come to a little sharper. Conversely, if we wanted
to show that the negative results did apply, to be
convincing we would have to make rather optimistic
assumptions about the learning environment.
</bodyText>
<sectionHeader confidence="0.97935" genericHeader="method">
4 Applying GI to FLA
</sectionHeader>
<bodyText confidence="0.99998645">
We now have the delicate task of selecting, or rather
constructing, a formal model by identifying the vari-
ous components we have identified above. We want
to choose the model that is the best representation
of the learning task or tasks that the infant child
must perform. We consider that some of the em-
pirical questions do not yet have clear answers. In
those cases, we shall make the choice that makes the
learning task more difficult. In other cases, we may
not have a clear idea of how to formalise some in-
formation source. We shall start by making a signif-
icant idealisation: we consider language acquisition
as being a single task. Natural languages as tradi-
tionally describe have different levels. At the very
least we have morphology and syntax; one might
also consider inter-sentential or discourse as an ad-
ditional level. We conflate all of these into a single
task: learning a formal language; in the discussion
below, for the sake of concreteness and clarity, we
shall talk in terms of learning syntax.
</bodyText>
<subsectionHeader confidence="0.996715">
4.1 The Language
</subsectionHeader>
<bodyText confidence="0.999963054054054">
The first question we must answer concerns the lan-
guage itself. A formal language is normally defined
as follows. Given a finite alphabet E, we define the
set of all strings (the free monoid) over E as E*.
We want to learn a language L C E*. The alpha-
bet E could be a set of phonemes, or characters, or
a set of words, or a set of lexical categories (part
of speech tags). The language could be the set of
well-formed sentences, or the set of words that obey
the phonotactics of the language, and so on. We re-
duce all of the different learning tasks in language
to a single abstract task – identifying a possibly in-
finite set of strings. This is overly simplistic since
transductions, i.e. mappings from one string to an-
other, are probably also necessary. We are using
here a standard definition of a language where every
string is unambiguously either in or not in the lan-
guage.. This may appear unrealistic – if the formal
language is meant to represent the set of grammati-
cal sentences, there are well-known methodological
problems with deciding where exactly to draw the
line between grammatical and ungrammatical sen-
tences. An alternative might be to consider accept-
ability rather than grammaticality as the defining
criterion for inclusion in the set. Moreover, there
is a certain amount of noise in the input – There
are other possibilities. We could for example use a
fuzzy set – i.e. a function from E* —&gt; [0, 1] where
each string has a degree of membership between 0
and 1. This would seem to create more problems
than it solves. A more appealing option is to learn
distributions, again functions f from E* —&gt; [0, 1]
but where &amp;CL f(s) = 1. This is of course the
classic problem of language modelling, and is com-
pelling for two reasons. First, it is empirically well
grounded – the probability of a string is related to its
frequency of occurrence, and secondly, we can de-
</bodyText>
<page confidence="0.987555">
27
</page>
<bodyText confidence="0.9999295">
duce from the speech recognition capability of hu-
mans that they must have some similar capability.
Both possibilities – crisp languages, and distri-
butions – are reasonable. The choice depends on
what one considers the key phenomena to be ex-
plained are – grammaticality judgments by native
speakers, or natural use and comprehension of the
language. We favour the latter, and accordingly
think that learning distributions is a more accurate
and more difficult choice.
</bodyText>
<subsectionHeader confidence="0.994479">
4.2 The class of languages
</subsectionHeader>
<bodyText confidence="0.99996856">
A common confusion in some discussions of this
topic is between languages and classes of lan-
guages. Learnability is a property of classes of
languages. If there is only one language in the
class of languages to be learned then the learner
can just guess that language and succeed. A class
with two languages is again trivially learnable if
you have an efficient algorithm for testing member-
ship. It is only when the set of languages is expo-
nentially large or infinite, that the problem becomes
non-trivial, from a theoretical point of view. The
class of languages we need is a class of languages
that includes all attested human languages and ad-
ditionally all “possible” human languages. Natu-
ral languages are thought to fall into the class of
mildly context-sensitive languages, (Vijay-Shanker
and Weir, 1994), so clearly this class is large
enough. It is, however, not necessary that our class
be this large. Indeed it is essential for learnability
that it is not. As we shall see below, even the class
of regular languages contains some subclasses that
are computationally hard to learn. Indeed, we claim
it is reasonable to define our class so it does not con-
tain languages that are clearly not possible human
languages.
</bodyText>
<subsectionHeader confidence="0.987947">
4.3 Information sources
</subsectionHeader>
<bodyText confidence="0.999983979591836">
Next we must specify the information that our learn-
ing algorithm has access to. Clearly the primary
source of data is the primary linguistic data (PLD),
namely the utterances that occur in the child’s envi-
ronment. These will consist of both child-directed
speech and adult-to-adult speech. These are gen-
erally acceptable sentences that is to say sentences
that are in the language to be learned. These are
called positive samples. One of the most long-
running debates in this field is over whether the
child has access to negative data – unacceptable sen-
tences that are marked in some way as such. The
consensus (Marcus, 1993) appears to be that they do
not. In middle-class Western families, children are
provided with some sort of feedback about the well-
formedness of their utterances, but this is unreliable
and erratic, not a universal of global child-raising.
Furthermore this appears to have no effect on the
child. Children do also get indirect pragmatic feed-
back if their utterances are incomprehensible. In our
opinion, both of these would be better modelled by
what is called a membership query: the algorithm
may generate a string and be informed whether that
string is in the language or not. However, we feel
that this is too erratic to be considered an essential
part of the process. Another question is whether the
input data is presented as a flat string or annotated
with some sort of structural evidence, which might
be derived from prosodic or semantic information.
Unfortunately there is little agreement on what the
constituent structure should be – indeed many lin-
guistic theories do not have a level of constituent
structure at all, but just dependency structure.
Semantic information is also claimed as an im-
portant source. The hypothesis is that children can
use lexical semantics, coupled with rich sources of
real-world knowlege to infer the meaning of utter-
ances from the situational context. That would be
an extremely powerful piece of information, but it is
clearly absurd to claim that the meaning of an utter-
ance is uniquely specified by the situational context.
If true, there would be no need for communication
or information transfer at all. Of course the context
puts some constraints on the sentences that will be
uttered, but it is not clear how to incorporate this
fact without being far too generous. In summary it
appears that only positive evidence can be unequiv-
ocally relied upon though this may seem a harsh and
unrealistic environment.
</bodyText>
<subsectionHeader confidence="0.98814">
4.4 Presentation
</subsectionHeader>
<bodyText confidence="0.999884263157895">
We have now decided that the only evidence avail-
able to the learner will be unadorned positive sam-
ples drawn from the target language. There are var-
ious possibilities for how the samples are selected.
The choice that is most favourable for the learner is
where they are slected by a helpful teacher to make
the learning process as easy as possible (Goldman
and Mathias, 1996). While it is certainly true that
carers speak to small children in sentences of sim-
ple structure (Motherese), this is not true for all of
the data that the child has access to, nor is it uni-
versally valid. Moreover, there are serious techni-
cal problems with formalising this, namely what is
called ’collusion’ where the teacher provides exam-
ples that encode the grammar itself, thus trivialising
the learning process. Though attempts have been
made to limit this problem, they are not yet com-
pletely satisfactory. The next alternative is that the
examples are selected randomly from some fixed
</bodyText>
<page confidence="0.993058">
28
</page>
<bodyText confidence="0.9999808">
distribution. This appears to us to be the appropri-
ate choice, subject to some limitations on the dis-
tributions that we discuss below. The final option,
the most difficult for the learner, is where the se-
quence of samples can be selected by an intelli-
gent adversary, in an attempt to make the learner
fail, subject only to the weak requirement that each
string in the language appears at least once. This is
the approach taken in the identification in the limit
paradigm (Gold, 1967), and is clearly too stringent.
The remaining question then regards the distribu-
tion from which the samples are drawn: whether the
learner has to be able to learn for every possible dis-
tribution, or only for distributions from a particular
class, or only for one particular distribution.
</bodyText>
<subsectionHeader confidence="0.789942">
4.5 Resources
</subsectionHeader>
<bodyText confidence="0.9999894375">
Beyond the requirement of computability we will
wish to place additional limitations on the computa-
tional resources that the learner can use. Since chil-
dren learn the language in a limited period of time,
which limits both the amount of data they have ac-
cess to and the amount of computation they can use,
it seems appropriate to disallow algorithms that use
unbounded or very large amounts of data or time.
As normal, we shall formalise this by putting poly-
nomial bounds on the sample complexity and com-
putational complexity. Since the individual samples
are of varying length, we need to allow the compu-
tational complexity to depend on the total length of
the sample. A key question is what the parameters
of the sample complexity polynomial should be. We
shall discuss this further below.
</bodyText>
<subsectionHeader confidence="0.997043">
4.6 Convergence Criteria
</subsectionHeader>
<bodyText confidence="0.999972583333333">
Next we address the issue of reliability: the extent
to which all children acquire language. First, vari-
ability in achievement of particular linguistic mile-
stones is high. There are numerous causes including
deafness, mental retardation, cerebral palsy, specific
language impairment and autism. Generally, autis-
tic children appear neurologically and physically
normal, but about half may never speak. Autism,
on some accounts, has an incidence of about 0.2%.
Therefore we can require learning to happen with
arbitrarily high probability, but requiring it to hap-
pen with probability one is unreasonable. A related
question concerns convergence: the extent to which
children exposed to a linguistic environment end
up with the same language as others. Clearly they
are very close since otherwise communication could
not happen, but there is ample evidence from stud-
ies of variation (Labov, 1975), that there are non-
trivial differences between adults, who have grown
up with near-identical linguistic experiences, about
the interpretation and syntactic acceptability of sim-
ple sentences, quite apart from the wide purely lex-
ical variation that is easily detected. A famous ex-
ample in English is “Each of the boys didn’t come”.
Moreover, language change requires some chil-
dren to end up with slightly different grammars
from the older generation. At the very most, we
should require that the hypothesis should be close
to the target. The function we use to measure the
’distance’ between hypothesis and target depends on
whether we are learnng crisp languages or distribu-
tions. If we are learning distributions then the ob-
vious choice is the Kullback-Leibler divergence – a
very strict measure. For crisp languages, the prob-
ability of the symmetric difference with respect to
some distribution is natural.
</bodyText>
<subsectionHeader confidence="0.993202">
4.7 PAC-learning
</subsectionHeader>
<bodyText confidence="0.999981416666667">
These considerations lead us to some variant of the
Probably Approximately Correct (PAC) model of
learning (Valiant, 1984). We require the algorithm
to produce with arbitrarily high probability a good
hypothesis. We formalise this by saying that for any
5 &gt; 0 it must produce a good hypothesis with prob-
ability more than 1 − 5. Next we require a good
hypothesis to be arbitrarily close to the target, so we
have a precision E and we say that for any E &gt; 0, the
hypothesis must be less than E away from the target.
We allow the amount of data it can use to increase as
the confidence and precision get smaller. We define
PAC-learning in the following way: given a finite
alphabet E, and a class of languages L over E, an
algorithm PAC-learns the class L, if there is a poly-
nomial q, such that for every confidence 5 &gt; 0 and
precision E &gt; 0, for every distribution D over E*,
for every language L in L, whenever the number of
samples exceeds q(1/E,1/5, |E|, |L|), the algorithm
must produce a hypothesis H such that with prob-
ability greater than 1 − 5, PrD(HAL &gt; E). Here
we use AAB to mean the symmetric difference be-
tween two sets. The polynomial q is called the
sample complexity polynomial. We also limit the
amount of computation to some polynomial in the
total length of the data it has seen. Note first of all
that this is a worst case bound – we are not requiring
merely that on average it comes close. Additionally
this model is what is called ’distribution-free’. This
means that the algorithm must work for every com-
bination of distribution and language. This is a very
stringent requirement, only mitigated by the fact
that the error is calculated with respect to the same
distribution that the samples are drawn from. Thus,
if there is a subset of E* with low aggregate proba-
bility under D, the algorithm will not get many sam-
</bodyText>
<page confidence="0.996966">
29
</page>
<bodyText confidence="0.999931804878049">
ples from this region but will not be penalised very
much for errors in that region. From our point of
view, there are two problems with this framework:
first, we only want to draw positive samples, but the
distributions are over all strings in E*, and include
some that give a zero probability to all strings in
the language concerned. Secondly, this is too pes-
simistic because the distribution has no relation to
the language: intuitively it’s reasonable to expect
the distribution to be derived in some way from the
language, or the structure of a grammar generating
the language. Indeed there is a causal connection
in reality since the sample of the language the child
is exposed to is generated by people who do in fact
know the language.
One alternative that has been suggested is the
PAC learning with simple distributions model intro-
duced by (Denis, 2001). This is based on ideas from
complexity theory where the samples are drawn ac-
cording to a universal distribution defined by the
conditional Kolmogorov complexity. While math-
ematically correct this is inappropriate as a model
of FLA for a number of reasons. First, learnability
is proven only on a single very unusual distribution,
and relies on particular properties of this distribu-
tion, and secondly there are some very large con-
stants in the sample complexity polynomial.
The solution we favour is to define some natu-
ral class of distributions based on a grammar or au-
tomaton generating the language. Given a class of
languages defined by some generative device, there
is normally a natural stochastic variant of the de-
vice which defines a distribution over that language.
Thus regular languages can be defined by a finite-
state automaton, and these can be naturally ex-
tended to Probabilistic finite state automaton. Sim-
ilarly context free languages are normally defined
by context-free grammmars which can be extended
again to to Probabilistic or stochastic CFG. We
therefore propose a slight modification of the PAC-
framework. For every class of languages L, defined
by some formal device define a class of distribu-
tions defined by a stochastic variant of that device.
D. Then for each language L, we select the set of
distributions whose support is equal to the language
and subject to a polynomial bound (q)on the com-
plexity of the distribution in terms of the complex-
ity of the target language: DL = {D E D : L =
supp(D) n |D |&lt; q(|L|)�. Samples are drawn from
one of these distributions.
There are two technical problems here: first, this
doesn’t penalise over-generalisation. Since the dis-
tribution is over positive examples, negative exam-
ples have zero weight, so we need some penalty
function over negative examples or alternatively
require the hypothesis to be a subset of the tar-
get. Secondly, this definition is too vague. The
exact way in which you extend the “crisp” lan-
guage to a stochastic one can have serious con-
sequences. When dealing with regular languages,
for example, though the class of languages defined
by deterministic automata is the same as that de-
fined by non-deterministic languages, the same is
not true for their stochastic variants. Additionally,
one can have exponential blow-ups in the number
of states when determinising automata. Similarly,
with CFGs, (Abney et al., 1999) showed that con-
verting between two parametrisations of stochastic
Context Free languages are equivalent but that there
are blow-ups in both directions. We do not have a
completely satisfactory solution to this problem at
the moment; an alternative is to consider learning
the distributions rather than the languages.
In the case of learning distributions, we have the
same framework, but the samples are drawn accord-
ing to the distribution being learned T, and we re-
quire that the hypothesis H has small divergence
from the target: D(T ||H) &lt; E. Since the divergence
is infinite if the hypothesis gives probability zero to
a string in the target, this will have the consequence
that the target must assign a non-zero probability to
every string.
</bodyText>
<sectionHeader confidence="0.990303" genericHeader="method">
5 Negative Results
</sectionHeader>
<bodyText confidence="0.999952083333333">
Now that we have a fairly clear idea of various ways
of formalising the situation we can consider the ex-
tent to which formal results apply. We start by con-
sidering negative results, which in Machine Learn-
ing come in two types. First, there are information-
theoretic bounds on sample complexity, derived
from the Vapnik-Chervonenkis (VC) dimension of
the space of languages, a measure of the complex-
ity of the set of hypotheses. If we add a parameter
to the sample complexity polynomial that represents
the complexity of the concept to be learned then this
will remove these problems. This can be the size of
a representation of the target which will be a poly-
nomial in the number of states, or simply the num-
ber of non-terminals or states. This is very standard
in most fields of machine learning.
The second problem relates not to the amount
of information but to the computation involved.
Results derived from cryptographic limitations on
computational complexity, can be proved based on
widely held and well supported assumptions that
certain hard cryptographic problems are insoluble.
In what follows we assume that there are no effi-
cient algorithms for common cryptographic prob-
</bodyText>
<page confidence="0.995826">
30
</page>
<bodyText confidence="0.999981925">
lems such as factoring Blum integers, inverting RSA
function, recognizing quadratic residues or learning
noisy parity functions.
There may be algorithms that will learn with rea-
sonable amounts of data but that require unfeasibly
large amounts of computation to find. There are
a number of powerful negative results on learning
in the purely distribution-free situation we consid-
ered and rejected above. (Kearns and Valiant, 1989)
showed that acyclic deterministic automata are not
learnable even with positive and negative exam-
ples. Similarly, (Abe and Warmuth, 1992) showed
a slightly weaker representation dependent result on
learning with a large alphabet for non-deterministic
automata, by showing that there are strings such that
maximising the likelihood of the string is NP-hard.
Again this does not strictly apply to the partially dis-
tribution free situation we have chosen.
However there is one very strong result that ap-
pears to apply. A straightforward consequence of
(Kearns et al., 1994) shows that Acyclic Determinis-
tic Probabilistic FSA over a two letter alphabet can-
not be learned under another cryptographic assump-
tion (the noisy parity assumption). Therefore any
class of languages that includes this comparatively
weak family will not be learnable in out framework.
But this rests upon the assumption that the class
of possible human languages must include some
cryptographically hard functions. It appears that
our formal apparatus does not distinguish between
these cryptographic functions which hav been con-
sciously designed to be hard to learn, and natu-
ral languages which presumably have evolved to be
easy to learn since there is no evolutionary pressure
to make them hard to decrypt – no intelligent preda-
tors eavesdropping for example. Clearly this is a
flaw in our analysis: we need to find some more
nuanced description for the class of possible human
languages that excludes these hard languages or dis-
tributions.
</bodyText>
<sectionHeader confidence="0.995507" genericHeader="method">
6 Positive results
</sectionHeader>
<bodyText confidence="0.999980720930232">
There is a positive result that shows a way forward.
A PDFA is µ-distinguishable the distributions gen-
erated from any two states differ by at least µ in
the L,,-norm, i.e. there is a string with a differ-
ence in probability of at least µ. (Ron et al., 1995)
showed that µ-distinguishable acyclic PDFAs can
be PAC-learned using the KLD as error function
in time polynomial in n,1/E,1/8,1/µ, El. They
use a variant of a standard state-merging algorithm.
Since these are acyclic the languages they define
are always finite. This additional criterion of distin-
guishability suffices to guarantee learnability. This
work can be extended to cyclic automata (Clark and
Thollard, 2004a; Clark and Thollard, 2004b), and
thus the class of all regular languages, with the ad-
dition of a further parameter which bounds the ex-
pected length of a string generated from any state.
The use of distinguishability seems innocuous; in
syntactic terms it is a consequence of the plausible
condition that for any pair of distinct non-terminals
there is some fairly likely string generated by one
and not the other. Similarly strings of symbols in
natural language tend to have limited length. An
alternate way of formalising this is to define a class
of distinguishable automata, where the distinguisha-
bility of the automata is lower bounded by an in-
verse polynomial in the number of states. This is
formally equivalent, but avoids adding terms to the
sample complexity polynomial. In summary this
would be a valid solution if all human languages
actually lay within the class of regular languages.
Note also the general properties of this kind of al-
gorithm: provably learning an infinite class of lan-
guages with infinite support using only polynomial
amounts of data and computation.
It is worth pointing out that the algorithm does
not need to “know” the values of the parameters.
Define a new parameter t, and set, for example n =
t, L = t, 8 = e−t, E = t−1 and µ = t−1. This gives
a sample complexity polynomial in one parameter
q(t). Given a certain amount of data N we can just
choose the largest value of t such that q(t) &lt; N,
and set the parameters accordingly.
</bodyText>
<sectionHeader confidence="0.996057" genericHeader="method">
7 Parametric models
</sectionHeader>
<bodyText confidence="0.999968714285714">
We can now examine the relevance of these re-
sults to the distinction between parametric and non-
parametric languages. Parametric models are those
where the class of languages is parametrised by a
small set of finite-valued (binary) parameters, where
the number of paameters is small compared to the
log2 of the complexity of the languages. Without
this latter constraint the notion is mathematically
vacuous, since, for example, any context free gram-
mar in Chomsky normal form can be parametrised
with N3 + NM + 1 binary parameters where N
is the number of non-terminals and M the num-
ber of terminals. This constraint is also necessary
for parametric models to make testable empirical
predictions both about language universals, devel-
opmental evidence and relationships between the
two (Hyams, 1986). We neglect here the important
issue of lexical learning: we assume, implausibly,
that lexical learning can take place completely be-
fore syntax learning commences. It has in the past
been stated that the finiteness of a language class
</bodyText>
<page confidence="0.999581">
31
</page>
<bodyText confidence="0.999973612903226">
suffices to guarantee learnability even under a PAC-
learning criterion (Bertolo, 2001). This is, in gen-
eral, false, and arises from neglecting constraints on
the sample complexity and the computational com-
plexities both of learning and of parsing. The neg-
ative result of (Kearns et al., 1994) discussed above
applies also to parametric models. The specific class
of noisy parity functions that they prove are unlearn-
able, are parametrised by a number of binary pa-
rameters in a way very reminiscent of a parametric
model of language. The mere fact that there are a
finite number of parameters does not suffice to guar-
antee learnability, if the resulting class of languages
is exponentially large, or if there is no polynomial
algorithm for parsing. This does not imply that all
parametrised classes of languages will be unlearn-
able, only that having a small number of parame-
ters is neither necessary nor sufficient to guarantee
efficient learnability. If the parameters are shallow
and relate to easily detectable properties of the lan-
guages and are independent then learning can oc-
cur efficiently (Yang, 2002). If they are “deep” and
inter-related, learning may be impossible. Learn-
ability depends more on simple statistical properties
of the distributions of the samples than on the struc-
ture of the class of languages.
Our conclusion then is ultimately that the theory
of learnability will not be able to resolve disputes
about the nature of first language acquisition: these
problems will have to be answered by empirical re-
search, rather than by mathematical analysis.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999685">
This work was supported in part by the IST
Programme of the European Community, under
the PASCAL Network of Excellence, IST-2002-
506778, funded in part by the Swiss Federal Office
for Education and Science (OFES). This publication
only reflects the authors’ views.
</bodyText>
<sectionHeader confidence="0.999197" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999536353846154">
N. Abe and M. K. Warmuth. 1992. On the com-
putational complexity of approximating distribu-
tions by probabilistic automata. Machine Learn-
ing, 9:205–260.
N. Abe. 1988. Feasible learnability of formal gram-
mars and the theory of natural language acquisi-
tion. In Proceedings of COLING 1988, pages 1–
6.
S. Abney, D. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In
Proceedings ofACL ’99.
Stefano Bertolo. 2001. A brief overview of learn-
ability. In Stefano Bertolo, editor, Language Ac-
quisition and Learnability. Cambridge University
Press.
Noam Chomsky. 1986. Knowledge of Language:
Its Nature, Origin, and Use. Praeger.
Alexander Clark and Franck Thollard. 2004a.
PAC-learnability of probabilistic deterministic fi-
nite state automata. Journal ofMachine Learning
Research, 5:473–497, May.
Alexander Clark and Franck Thollard. 2004b. Par-
tially distribution-free learning of regular lan-
guages from positive samples. In Proceedings of
COLING, Geneva, Switzerland.
F. Denis. 2001. Learning regular languages from
simple positive examples. Machine Learning,
44(1/2):37–66.
E. M. Gold. 1967. Language indentification in the
limit. Information and control, 10(5):447 – 474.
S. A. Goldman and H. D. Mathias. 1996. Teach-
ing a smarter learner. Journal of Computer and
System Sciences, 52(2):255–267.
N. Hyams. 1986. Language Acquisition and the
Theory ofParameters. D. Reidel.
M. Kearns and G. Valiant. 1989. Cryptographic
limitations on learning boolean formulae and fi-
nite automata. In 21st annual ACM symposium
on Theory of computation, pages 433–444, New
York. ACM, ACM.
M.J. Kearns, Y. Mansour, D. Ron, R. Rubinfeld,
R.E. Schapire, and L. Sellie. 1994. On the learn-
ability of discrete distributions. In Proc. of the
25th Annual ACM Symposium on Theory of Com-
puting, pages 273–282.
W. Labov. 1975. Empirical foundations of linguis-
tic theory. In R. Austerlitz, editor, The Scope of
American Linguistics. Peter de Ridder Press.
G. F. Marcus. 1993. Negative evidence in language
acquisition. Cognition, 46:53–85.
D. Ron, Y. Singer, and N. Tishby. 1995. On the
learnability and usage of acyclic probabilistic fi-
nite automata. In COLT 1995, pages 31–40,
Santa Cruz CA USA. ACM.
L. Valiant. 1984. A theory of the learnable. Com-
munications of the ACM, 27(11):1134 – 1142.
K. Vijay-Shanker and David J. Weir. 1994.
The equivalence of four extensions of context-
free grammars. Mathematical Systems Theory,
27(6):511–546.
Kenneth Wexler and Peter W. Culicover. 1980. For-
mal Principles of Language Acquisition. MIT
Press.
C. Yang. 2002. Knowledge and Learning in Natu-
ral Language. Oxford.
</reference>
<page confidence="0.9993">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816487">
<title confidence="0.99984">Grammatical Inference and First Language Acquisition</title>
<author confidence="0.998359">Alexander Clark</author>
<affiliation confidence="0.9629135">ISSCO / TIM, University of UNI-MAIL, Boulevard du</affiliation>
<address confidence="0.952917">CH-1211 Geneve 4, Switzerland</address>
<abstract confidence="0.995718307692308">One argument for parametric models of language has been learnability in the context of first language acquisition. The claim is made that “logical” arguments from learnability theory require non-trivial constraints on the class of languages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>M K Warmuth</author>
</authors>
<title>On the computational complexity of approximating distributions by probabilistic automata.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<pages>9--205</pages>
<contexts>
<context position="30808" citStr="Abe and Warmuth, 1992" startWordPosition="5168" endWordPosition="5171">o efficient algorithms for common cryptographic prob30 lems such as factoring Blum integers, inverting RSA function, recognizing quadratic residues or learning noisy parity functions. There may be algorithms that will learn with reasonable amounts of data but that require unfeasibly large amounts of computation to find. There are a number of powerful negative results on learning in the purely distribution-free situation we considered and rejected above. (Kearns and Valiant, 1989) showed that acyclic deterministic automata are not learnable even with positive and negative examples. Similarly, (Abe and Warmuth, 1992) showed a slightly weaker representation dependent result on learning with a large alphabet for non-deterministic automata, by showing that there are strings such that maximising the likelihood of the string is NP-hard. Again this does not strictly apply to the partially distribution free situation we have chosen. However there is one very strong result that appears to apply. A straightforward consequence of (Kearns et al., 1994) shows that Acyclic Deterministic Probabilistic FSA over a two letter alphabet cannot be learned under another cryptographic assumption (the noisy parity assumption). </context>
</contexts>
<marker>Abe, Warmuth, 1992</marker>
<rawString>N. Abe and M. K. Warmuth. 1992. On the computational complexity of approximating distributions by probabilistic automata. Machine Learning, 9:205–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Abe</author>
</authors>
<title>Feasible learnability of formal grammars and the theory of natural language acquisition.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING 1988,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="1743" citStr="Abe, 1988" startWordPosition="258" endWordPosition="259">bly good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. 1 Introduction For some years, the relevance of formal results in grammatical inference to the empirical question of first language acquisition by infant children has been recognised (Wexler and Culicover, 1980). Unfortunately, for many researchers, with a few notable exceptions (Abe, 1988), this begins and ends with Gold’s famous negative results in the identification in the limit paradigm. This paradigm, though still widely used in the grammatical inference community, is clearly of limited relevance to the issue at hand, since it requires the model to be able to exactly identify the target language even when an adversary can pick arbitrarily misleading sequences of examples to provide. Moreover, the paradigm as stated has no bounds on the amount of data or computation required for the learner. In spite of the inapplicability of this particular paradigm, in a suitable analysis </context>
</contexts>
<marker>Abe, 1988</marker>
<rawString>N. Abe. 1988. Feasible learnability of formal grammars and the theory of natural language acquisition. In Proceedings of COLING 1988, pages 1– 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>D McAllester</author>
<author>F Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL ’99.</booktitle>
<contexts>
<context position="28290" citStr="Abney et al., 1999" startWordPosition="4759" endWordPosition="4762">y function over negative examples or alternatively require the hypothesis to be a subset of the target. Secondly, this definition is too vague. The exact way in which you extend the “crisp” language to a stochastic one can have serious consequences. When dealing with regular languages, for example, though the class of languages defined by deterministic automata is the same as that defined by non-deterministic languages, the same is not true for their stochastic variants. Additionally, one can have exponential blow-ups in the number of states when determinising automata. Similarly, with CFGs, (Abney et al., 1999) showed that converting between two parametrisations of stochastic Context Free languages are equivalent but that there are blow-ups in both directions. We do not have a completely satisfactory solution to this problem at the moment; an alternative is to consider learning the distributions rather than the languages. In the case of learning distributions, we have the same framework, but the samples are drawn according to the distribution being learned T, and we require that the hypothesis H has small divergence from the target: D(T ||H) &lt; E. Since the divergence is infinite if the hypothesis gi</context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>S. Abney, D. McAllester, and F. Pereira. 1999. Relating probabilistic grammars and automata. In Proceedings ofACL ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Bertolo</author>
</authors>
<title>A brief overview of learnability.</title>
<date>2001</date>
<booktitle>Language Acquisition and Learnability.</booktitle>
<editor>In Stefano Bertolo, editor,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="35491" citStr="Bertolo, 2001" startWordPosition="5942" endWordPosition="5943"> + 1 binary parameters where N is the number of non-terminals and M the number of terminals. This constraint is also necessary for parametric models to make testable empirical predictions both about language universals, developmental evidence and relationships between the two (Hyams, 1986). We neglect here the important issue of lexical learning: we assume, implausibly, that lexical learning can take place completely before syntax learning commences. It has in the past been stated that the finiteness of a language class 31 suffices to guarantee learnability even under a PAClearning criterion (Bertolo, 2001). This is, in general, false, and arises from neglecting constraints on the sample complexity and the computational complexities both of learning and of parsing. The negative result of (Kearns et al., 1994) discussed above applies also to parametric models. The specific class of noisy parity functions that they prove are unlearnable, are parametrised by a number of binary parameters in a way very reminiscent of a parametric model of language. The mere fact that there are a finite number of parameters does not suffice to guarantee learnability, if the resulting class of languages is exponential</context>
</contexts>
<marker>Bertolo, 2001</marker>
<rawString>Stefano Bertolo. 2001. A brief overview of learnability. In Stefano Bertolo, editor, Language Acquisition and Learnability. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Knowledge of Language: Its Nature, Origin, and Use.</title>
<date>1986</date>
<publisher>Praeger.</publisher>
<contexts>
<context position="3857" citStr="Chomsky, 1986" startWordPosition="614" endWordPosition="615">the field of grammtical inference, much like the wider field of machine learning in general, is in a state of rapid change. In this paper we hope to address this problem by justifying the selection of the appropriate learning framework starting by looking at the actual situation the child is in, rather than from an a priori decision about the right framework. We will not attempt a survey of grammatical inference techniques; nor shall we provide proofs of the theorems we use here. Arguments based on formal learnability have been used to support the idea of parameter based theories of language (Chomsky, 1986). As we shall see below, under our analysis of the problem these arguments are weak. Indeed, they are more pertinent to questions about the autonomy and modularity of language learning: the question whether learning of some level of linguistic knowledge – morphology or syntax, for example – can take place in isolation from other forms of learning, such as the acquisition of word meaning, and without interaction, grounding and so on. 25 Positive results can help us to understand how humans might learn languages by outlining the class of algorithms that might be used by humans, considered as com</context>
</contexts>
<marker>Chomsky, 1986</marker>
<rawString>Noam Chomsky. 1986. Knowledge of Language: Its Nature, Origin, and Use. Praeger.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Franck Thollard</author>
</authors>
<title>PAC-learnability of probabilistic deterministic finite state automata.</title>
<date>2004</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>5--473</pages>
<contexts>
<context position="32878" citStr="Clark and Thollard, 2004" startWordPosition="5500" endWordPosition="5503">d. A PDFA is µ-distinguishable the distributions generated from any two states differ by at least µ in the L,,-norm, i.e. there is a string with a difference in probability of at least µ. (Ron et al., 1995) showed that µ-distinguishable acyclic PDFAs can be PAC-learned using the KLD as error function in time polynomial in n,1/E,1/8,1/µ, El. They use a variant of a standard state-merging algorithm. Since these are acyclic the languages they define are always finite. This additional criterion of distinguishability suffices to guarantee learnability. This work can be extended to cyclic automata (Clark and Thollard, 2004a; Clark and Thollard, 2004b), and thus the class of all regular languages, with the addition of a further parameter which bounds the expected length of a string generated from any state. The use of distinguishability seems innocuous; in syntactic terms it is a consequence of the plausible condition that for any pair of distinct non-terminals there is some fairly likely string generated by one and not the other. Similarly strings of symbols in natural language tend to have limited length. An alternate way of formalising this is to define a class of distinguishable automata, where the distingui</context>
</contexts>
<marker>Clark, Thollard, 2004</marker>
<rawString>Alexander Clark and Franck Thollard. 2004a. PAC-learnability of probabilistic deterministic finite state automata. Journal ofMachine Learning Research, 5:473–497, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Franck Thollard</author>
</authors>
<title>Partially distribution-free learning of regular languages from positive samples.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="32878" citStr="Clark and Thollard, 2004" startWordPosition="5500" endWordPosition="5503">d. A PDFA is µ-distinguishable the distributions generated from any two states differ by at least µ in the L,,-norm, i.e. there is a string with a difference in probability of at least µ. (Ron et al., 1995) showed that µ-distinguishable acyclic PDFAs can be PAC-learned using the KLD as error function in time polynomial in n,1/E,1/8,1/µ, El. They use a variant of a standard state-merging algorithm. Since these are acyclic the languages they define are always finite. This additional criterion of distinguishability suffices to guarantee learnability. This work can be extended to cyclic automata (Clark and Thollard, 2004a; Clark and Thollard, 2004b), and thus the class of all regular languages, with the addition of a further parameter which bounds the expected length of a string generated from any state. The use of distinguishability seems innocuous; in syntactic terms it is a consequence of the plausible condition that for any pair of distinct non-terminals there is some fairly likely string generated by one and not the other. Similarly strings of symbols in natural language tend to have limited length. An alternate way of formalising this is to define a class of distinguishable automata, where the distingui</context>
</contexts>
<marker>Clark, Thollard, 2004</marker>
<rawString>Alexander Clark and Franck Thollard. 2004b. Partially distribution-free learning of regular languages from positive samples. In Proceedings of COLING, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Denis</author>
</authors>
<title>Learning regular languages from simple positive examples.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>44--1</pages>
<contexts>
<context position="25878" citStr="Denis, 2001" startWordPosition="4362" endWordPosition="4363"> include some that give a zero probability to all strings in the language concerned. Secondly, this is too pessimistic because the distribution has no relation to the language: intuitively it’s reasonable to expect the distribution to be derived in some way from the language, or the structure of a grammar generating the language. Indeed there is a causal connection in reality since the sample of the language the child is exposed to is generated by people who do in fact know the language. One alternative that has been suggested is the PAC learning with simple distributions model introduced by (Denis, 2001). This is based on ideas from complexity theory where the samples are drawn according to a universal distribution defined by the conditional Kolmogorov complexity. While mathematically correct this is inappropriate as a model of FLA for a number of reasons. First, learnability is proven only on a single very unusual distribution, and relies on particular properties of this distribution, and secondly there are some very large constants in the sample complexity polynomial. The solution we favour is to define some natural class of distributions based on a grammar or automaton generating the langu</context>
</contexts>
<marker>Denis, 2001</marker>
<rawString>F. Denis. 2001. Learning regular languages from simple positive examples. Machine Learning, 44(1/2):37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language indentification in the limit.</title>
<date>1967</date>
<journal>Information and control,</journal>
<volume>10</volume>
<issue>5</issue>
<pages>474</pages>
<contexts>
<context position="20236" citStr="Gold, 1967" startWordPosition="3405" endWordPosition="3406">m, they are not yet completely satisfactory. The next alternative is that the examples are selected randomly from some fixed 28 distribution. This appears to us to be the appropriate choice, subject to some limitations on the distributions that we discuss below. The final option, the most difficult for the learner, is where the sequence of samples can be selected by an intelligent adversary, in an attempt to make the learner fail, subject only to the weak requirement that each string in the language appears at least once. This is the approach taken in the identification in the limit paradigm (Gold, 1967), and is clearly too stringent. The remaining question then regards the distribution from which the samples are drawn: whether the learner has to be able to learn for every possible distribution, or only for distributions from a particular class, or only for one particular distribution. 4.5 Resources Beyond the requirement of computability we will wish to place additional limitations on the computational resources that the learner can use. Since children learn the language in a limited period of time, which limits both the amount of data they have access to and the amount of computation they c</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. M. Gold. 1967. Language indentification in the limit. Information and control, 10(5):447 – 474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Goldman</author>
<author>H D Mathias</author>
</authors>
<title>Teaching a smarter learner.</title>
<date>1996</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>52</volume>
<issue>2</issue>
<contexts>
<context position="19155" citStr="Goldman and Mathias, 1996" startWordPosition="3218" endWordPosition="3221">be uttered, but it is not clear how to incorporate this fact without being far too generous. In summary it appears that only positive evidence can be unequivocally relied upon though this may seem a harsh and unrealistic environment. 4.4 Presentation We have now decided that the only evidence available to the learner will be unadorned positive samples drawn from the target language. There are various possibilities for how the samples are selected. The choice that is most favourable for the learner is where they are slected by a helpful teacher to make the learning process as easy as possible (Goldman and Mathias, 1996). While it is certainly true that carers speak to small children in sentences of simple structure (Motherese), this is not true for all of the data that the child has access to, nor is it universally valid. Moreover, there are serious technical problems with formalising this, namely what is called ’collusion’ where the teacher provides examples that encode the grammar itself, thus trivialising the learning process. Though attempts have been made to limit this problem, they are not yet completely satisfactory. The next alternative is that the examples are selected randomly from some fixed 28 di</context>
</contexts>
<marker>Goldman, Mathias, 1996</marker>
<rawString>S. A. Goldman and H. D. Mathias. 1996. Teaching a smarter learner. Journal of Computer and System Sciences, 52(2):255–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hyams</author>
</authors>
<title>Language Acquisition and the Theory ofParameters.</title>
<date>1986</date>
<journal>D. Reidel.</journal>
<contexts>
<context position="35167" citStr="Hyams, 1986" startWordPosition="5891" endWordPosition="5892">sed by a small set of finite-valued (binary) parameters, where the number of paameters is small compared to the log2 of the complexity of the languages. Without this latter constraint the notion is mathematically vacuous, since, for example, any context free grammar in Chomsky normal form can be parametrised with N3 + NM + 1 binary parameters where N is the number of non-terminals and M the number of terminals. This constraint is also necessary for parametric models to make testable empirical predictions both about language universals, developmental evidence and relationships between the two (Hyams, 1986). We neglect here the important issue of lexical learning: we assume, implausibly, that lexical learning can take place completely before syntax learning commences. It has in the past been stated that the finiteness of a language class 31 suffices to guarantee learnability even under a PAClearning criterion (Bertolo, 2001). This is, in general, false, and arises from neglecting constraints on the sample complexity and the computational complexities both of learning and of parsing. The negative result of (Kearns et al., 1994) discussed above applies also to parametric models. The specific class</context>
</contexts>
<marker>Hyams, 1986</marker>
<rawString>N. Hyams. 1986. Language Acquisition and the Theory ofParameters. D. Reidel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kearns</author>
<author>G Valiant</author>
</authors>
<title>Cryptographic limitations on learning boolean formulae and finite automata.</title>
<date>1989</date>
<booktitle>In 21st annual ACM symposium on Theory of computation,</booktitle>
<pages>433--444</pages>
<publisher>ACM, ACM.</publisher>
<location>New York.</location>
<contexts>
<context position="30670" citStr="Kearns and Valiant, 1989" startWordPosition="5148" endWordPosition="5151">widely held and well supported assumptions that certain hard cryptographic problems are insoluble. In what follows we assume that there are no efficient algorithms for common cryptographic prob30 lems such as factoring Blum integers, inverting RSA function, recognizing quadratic residues or learning noisy parity functions. There may be algorithms that will learn with reasonable amounts of data but that require unfeasibly large amounts of computation to find. There are a number of powerful negative results on learning in the purely distribution-free situation we considered and rejected above. (Kearns and Valiant, 1989) showed that acyclic deterministic automata are not learnable even with positive and negative examples. Similarly, (Abe and Warmuth, 1992) showed a slightly weaker representation dependent result on learning with a large alphabet for non-deterministic automata, by showing that there are strings such that maximising the likelihood of the string is NP-hard. Again this does not strictly apply to the partially distribution free situation we have chosen. However there is one very strong result that appears to apply. A straightforward consequence of (Kearns et al., 1994) shows that Acyclic Determini</context>
</contexts>
<marker>Kearns, Valiant, 1989</marker>
<rawString>M. Kearns and G. Valiant. 1989. Cryptographic limitations on learning boolean formulae and finite automata. In 21st annual ACM symposium on Theory of computation, pages 433–444, New York. ACM, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Kearns</author>
<author>Y Mansour</author>
<author>D Ron</author>
<author>R Rubinfeld</author>
<author>R E Schapire</author>
<author>L Sellie</author>
</authors>
<title>On the learnability of discrete distributions.</title>
<date>1994</date>
<booktitle>In Proc. of the 25th Annual ACM Symposium on Theory of Computing,</booktitle>
<pages>273--282</pages>
<contexts>
<context position="1056" citStr="Kearns et al., 1994" startWordPosition="149" endWordPosition="152">nguages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. 1 Introduction For some years, the relevance of formal results in grammatical inference to the empirical question of first language acquisition by infant children has been recognised (Wexler and Culicover</context>
<context position="5190" citStr="Kearns et al., 1994" startWordPosition="838" endWordPosition="841">nstrate that no algorithms of a certain class could perform the task – in this case we could know that the human child learns his language in some other way. We shall proceed as follows: after briefly describing FLA, we describe the various elements of a model of learning, or framework. We then make a series of decisions based on the empirical facts about FLA, to construct an appropriate model or models, avoiding unnecessary idealisation wherever possible. We proceed to some strong negative results, well-known in the GI community that bear on the questions at hand. The most powerful of these (Kearns et al., 1994) appears to apply quite directly to our chosen model. We then discuss an interesting algorithm (Ron et al., 1995) which shows that this can be circumvented, at least for a subclass of regular languages. Finally, after discussing the possibilities for extending this result to all regular languages, and beyond, we conclude with a discussion of the implications of the results presented for the distinction between parametric and non-parametric models. 2 First Language Acquisition Let us first examine the phenomenon we are concerned with: first language acquisition. In the space of a few years, chi</context>
<context position="31241" citStr="Kearns et al., 1994" startWordPosition="5236" endWordPosition="5239">ed and rejected above. (Kearns and Valiant, 1989) showed that acyclic deterministic automata are not learnable even with positive and negative examples. Similarly, (Abe and Warmuth, 1992) showed a slightly weaker representation dependent result on learning with a large alphabet for non-deterministic automata, by showing that there are strings such that maximising the likelihood of the string is NP-hard. Again this does not strictly apply to the partially distribution free situation we have chosen. However there is one very strong result that appears to apply. A straightforward consequence of (Kearns et al., 1994) shows that Acyclic Deterministic Probabilistic FSA over a two letter alphabet cannot be learned under another cryptographic assumption (the noisy parity assumption). Therefore any class of languages that includes this comparatively weak family will not be learnable in out framework. But this rests upon the assumption that the class of possible human languages must include some cryptographically hard functions. It appears that our formal apparatus does not distinguish between these cryptographic functions which hav been consciously designed to be hard to learn, and natural languages which pres</context>
<context position="35697" citStr="Kearns et al., 1994" startWordPosition="5975" endWordPosition="5978">anguage universals, developmental evidence and relationships between the two (Hyams, 1986). We neglect here the important issue of lexical learning: we assume, implausibly, that lexical learning can take place completely before syntax learning commences. It has in the past been stated that the finiteness of a language class 31 suffices to guarantee learnability even under a PAClearning criterion (Bertolo, 2001). This is, in general, false, and arises from neglecting constraints on the sample complexity and the computational complexities both of learning and of parsing. The negative result of (Kearns et al., 1994) discussed above applies also to parametric models. The specific class of noisy parity functions that they prove are unlearnable, are parametrised by a number of binary parameters in a way very reminiscent of a parametric model of language. The mere fact that there are a finite number of parameters does not suffice to guarantee learnability, if the resulting class of languages is exponentially large, or if there is no polynomial algorithm for parsing. This does not imply that all parametrised classes of languages will be unlearnable, only that having a small number of parameters is neither nec</context>
</contexts>
<marker>Kearns, Mansour, Ron, Rubinfeld, Schapire, Sellie, 1994</marker>
<rawString>M.J. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. 1994. On the learnability of discrete distributions. In Proc. of the 25th Annual ACM Symposium on Theory of Computing, pages 273–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Labov</author>
</authors>
<title>Empirical foundations of linguistic theory.</title>
<date>1975</date>
<booktitle>The Scope of American Linguistics. Peter de</booktitle>
<editor>In R. Austerlitz, editor,</editor>
<publisher>Ridder Press.</publisher>
<contexts>
<context position="22248" citStr="Labov, 1975" startWordPosition="3727" endWordPosition="3728">t and autism. Generally, autistic children appear neurologically and physically normal, but about half may never speak. Autism, on some accounts, has an incidence of about 0.2%. Therefore we can require learning to happen with arbitrarily high probability, but requiring it to happen with probability one is unreasonable. A related question concerns convergence: the extent to which children exposed to a linguistic environment end up with the same language as others. Clearly they are very close since otherwise communication could not happen, but there is ample evidence from studies of variation (Labov, 1975), that there are nontrivial differences between adults, who have grown up with near-identical linguistic experiences, about the interpretation and syntactic acceptability of simple sentences, quite apart from the wide purely lexical variation that is easily detected. A famous example in English is “Each of the boys didn’t come”. Moreover, language change requires some children to end up with slightly different grammars from the older generation. At the very most, we should require that the hypothesis should be close to the target. The function we use to measure the ’distance’ between hypothesi</context>
</contexts>
<marker>Labov, 1975</marker>
<rawString>W. Labov. 1975. Empirical foundations of linguistic theory. In R. Austerlitz, editor, The Scope of American Linguistics. Peter de Ridder Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G F Marcus</author>
</authors>
<title>Negative evidence in language acquisition.</title>
<date>1993</date>
<journal>Cognition,</journal>
<pages>46--53</pages>
<contexts>
<context position="16906" citStr="Marcus, 1993" startWordPosition="2844" endWordPosition="2845"> specify the information that our learning algorithm has access to. Clearly the primary source of data is the primary linguistic data (PLD), namely the utterances that occur in the child’s environment. These will consist of both child-directed speech and adult-to-adult speech. These are generally acceptable sentences that is to say sentences that are in the language to be learned. These are called positive samples. One of the most longrunning debates in this field is over whether the child has access to negative data – unacceptable sentences that are marked in some way as such. The consensus (Marcus, 1993) appears to be that they do not. In middle-class Western families, children are provided with some sort of feedback about the wellformedness of their utterances, but this is unreliable and erratic, not a universal of global child-raising. Furthermore this appears to have no effect on the child. Children do also get indirect pragmatic feedback if their utterances are incomprehensible. In our opinion, both of these would be better modelled by what is called a membership query: the algorithm may generate a string and be informed whether that string is in the language or not. However, we feel that</context>
</contexts>
<marker>Marcus, 1993</marker>
<rawString>G. F. Marcus. 1993. Negative evidence in language acquisition. Cognition, 46:53–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ron</author>
<author>Y Singer</author>
<author>N Tishby</author>
</authors>
<title>On the learnability and usage of acyclic probabilistic finite automata.</title>
<date>1995</date>
<booktitle>In COLT</booktitle>
<pages>31--40</pages>
<publisher>ACM.</publisher>
<location>Santa Cruz CA USA.</location>
<contexts>
<context position="1172" citStr="Ron et al., 1995" startWordPosition="169" endWordPosition="172">his paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. 1 Introduction For some years, the relevance of formal results in grammatical inference to the empirical question of first language acquisition by infant children has been recognised (Wexler and Culicover, 1980). Unfortunately, for many researchers, with a few notable exceptions (Abe, 1988), this begins and ends with G</context>
<context position="5303" citStr="Ron et al., 1995" startWordPosition="858" endWordPosition="861">ld learns his language in some other way. We shall proceed as follows: after briefly describing FLA, we describe the various elements of a model of learning, or framework. We then make a series of decisions based on the empirical facts about FLA, to construct an appropriate model or models, avoiding unnecessary idealisation wherever possible. We proceed to some strong negative results, well-known in the GI community that bear on the questions at hand. The most powerful of these (Kearns et al., 1994) appears to apply quite directly to our chosen model. We then discuss an interesting algorithm (Ron et al., 1995) which shows that this can be circumvented, at least for a subclass of regular languages. Finally, after discussing the possibilities for extending this result to all regular languages, and beyond, we conclude with a discussion of the implications of the results presented for the distinction between parametric and non-parametric models. 2 First Language Acquisition Let us first examine the phenomenon we are concerned with: first language acquisition. In the space of a few years, children almost invariably acquire, in the absence of explicit instruction, one or more of the languages that they a</context>
<context position="32460" citStr="Ron et al., 1995" startWordPosition="5438" endWordPosition="5441">ably have evolved to be easy to learn since there is no evolutionary pressure to make them hard to decrypt – no intelligent predators eavesdropping for example. Clearly this is a flaw in our analysis: we need to find some more nuanced description for the class of possible human languages that excludes these hard languages or distributions. 6 Positive results There is a positive result that shows a way forward. A PDFA is µ-distinguishable the distributions generated from any two states differ by at least µ in the L,,-norm, i.e. there is a string with a difference in probability of at least µ. (Ron et al., 1995) showed that µ-distinguishable acyclic PDFAs can be PAC-learned using the KLD as error function in time polynomial in n,1/E,1/8,1/µ, El. They use a variant of a standard state-merging algorithm. Since these are acyclic the languages they define are always finite. This additional criterion of distinguishability suffices to guarantee learnability. This work can be extended to cyclic automata (Clark and Thollard, 2004a; Clark and Thollard, 2004b), and thus the class of all regular languages, with the addition of a further parameter which bounds the expected length of a string generated from any s</context>
</contexts>
<marker>Ron, Singer, Tishby, 1995</marker>
<rawString>D. Ron, Y. Singer, and N. Tishby. 1995. On the learnability and usage of acyclic probabilistic finite automata. In COLT 1995, pages 31–40, Santa Cruz CA USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<volume>27</volume>
<issue>11</issue>
<pages>1142</pages>
<contexts>
<context position="870" citStr="Valiant, 1984" startWordPosition="123" endWordPosition="124">en learnability in the context of first language acquisition. The claim is made that “logical” arguments from learnability theory require non-trivial constraints on the class of languages. Initial formalisations of the problem (Gold, 1967) are however inapplicable to this particular situation. In this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts. We claim that a variant of the Probably Approximately Correct (PAC) learning framework (Valiant, 1984) with positive samples only, modified so it is not completely distribution free is the appropriate choice. Some negative results derived from cryptographic problems (Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. 1 Introduction For</context>
<context position="23296" citStr="Valiant, 1984" startWordPosition="3892" endWordPosition="3893">der generation. At the very most, we should require that the hypothesis should be close to the target. The function we use to measure the ’distance’ between hypothesis and target depends on whether we are learnng crisp languages or distributions. If we are learning distributions then the obvious choice is the Kullback-Leibler divergence – a very strict measure. For crisp languages, the probability of the symmetric difference with respect to some distribution is natural. 4.7 PAC-learning These considerations lead us to some variant of the Probably Approximately Correct (PAC) model of learning (Valiant, 1984). We require the algorithm to produce with arbitrarily high probability a good hypothesis. We formalise this by saying that for any 5 &gt; 0 it must produce a good hypothesis with probability more than 1 − 5. Next we require a good hypothesis to be arbitrarily close to the target, so we have a precision E and we say that for any E &gt; 0, the hypothesis must be less than E away from the target. We allow the amount of data it can use to increase as the confidence and precision get smaller. We define PAC-learning in the following way: given a finite alphabet E, and a class of languages L over E, an al</context>
</contexts>
<marker>Valiant, 1984</marker>
<rawString>L. Valiant. 1984. A theory of the learnable. Communications of the ACM, 27(11):1134 – 1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
</authors>
<title>The equivalence of four extensions of contextfree grammars.</title>
<date>1994</date>
<booktitle>Mathematical Systems Theory,</booktitle>
<pages>27--6</pages>
<contexts>
<context position="15842" citStr="Vijay-Shanker and Weir, 1994" startWordPosition="2661" endWordPosition="2664"> in the class of languages to be learned then the learner can just guess that language and succeed. A class with two languages is again trivially learnable if you have an efficient algorithm for testing membership. It is only when the set of languages is exponentially large or infinite, that the problem becomes non-trivial, from a theoretical point of view. The class of languages we need is a class of languages that includes all attested human languages and additionally all “possible” human languages. Natural languages are thought to fall into the class of mildly context-sensitive languages, (Vijay-Shanker and Weir, 1994), so clearly this class is large enough. It is, however, not necessary that our class be this large. Indeed it is essential for learnability that it is not. As we shall see below, even the class of regular languages contains some subclasses that are computationally hard to learn. Indeed, we claim it is reasonable to define our class so it does not contain languages that are clearly not possible human languages. 4.3 Information sources Next we must specify the information that our learning algorithm has access to. Clearly the primary source of data is the primary linguistic data (PLD), namely t</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1994</marker>
<rawString>K. Vijay-Shanker and David J. Weir. 1994. The equivalence of four extensions of contextfree grammars. Mathematical Systems Theory, 27(6):511–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Wexler</author>
<author>Peter W Culicover</author>
</authors>
<title>Formal Principles of Language Acquisition.</title>
<date>1980</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1663" citStr="Wexler and Culicover, 1980" startWordPosition="243" endWordPosition="246">Kearns et al., 1994) appear to apply in this situation but the existence of algorithms with provably good performance (Ron et al., 1995) and subsequent work, shows how these negative results are not as strong as they initially appear, and that recent algorithms for learning regular languages partially satisfy our criteria. We then discuss the applicability of these results to parametric and nonparametric models. 1 Introduction For some years, the relevance of formal results in grammatical inference to the empirical question of first language acquisition by infant children has been recognised (Wexler and Culicover, 1980). Unfortunately, for many researchers, with a few notable exceptions (Abe, 1988), this begins and ends with Gold’s famous negative results in the identification in the limit paradigm. This paradigm, though still widely used in the grammatical inference community, is clearly of limited relevance to the issue at hand, since it requires the model to be able to exactly identify the target language even when an adversary can pick arbitrarily misleading sequences of examples to provide. Moreover, the paradigm as stated has no bounds on the amount of data or computation required for the learner. In s</context>
</contexts>
<marker>Wexler, Culicover, 1980</marker>
<rawString>Kenneth Wexler and Peter W. Culicover. 1980. Formal Principles of Language Acquisition. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yang</author>
</authors>
<title>Knowledge and Learning in Natural Language.</title>
<date>2002</date>
<location>Oxford.</location>
<contexts>
<context position="36514" citStr="Yang, 2002" startWordPosition="6111" endWordPosition="6112">of a parametric model of language. The mere fact that there are a finite number of parameters does not suffice to guarantee learnability, if the resulting class of languages is exponentially large, or if there is no polynomial algorithm for parsing. This does not imply that all parametrised classes of languages will be unlearnable, only that having a small number of parameters is neither necessary nor sufficient to guarantee efficient learnability. If the parameters are shallow and relate to easily detectable properties of the languages and are independent then learning can occur efficiently (Yang, 2002). If they are “deep” and inter-related, learning may be impossible. Learnability depends more on simple statistical properties of the distributions of the samples than on the structure of the class of languages. Our conclusion then is ultimately that the theory of learnability will not be able to resolve disputes about the nature of first language acquisition: these problems will have to be answered by empirical research, rather than by mathematical analysis. Acknowledgements This work was supported in part by the IST Programme of the European Community, under the PASCAL Network of Excellence,</context>
</contexts>
<marker>Yang, 2002</marker>
<rawString>C. Yang. 2002. Knowledge and Learning in Natural Language. Oxford.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>