<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.153298">
<title confidence="0.972711">
BOUNCE: Sentiment Classification in Twitter using Rich Feature Sets
</title>
<author confidence="0.997367">
Nadin K¨okciyan†, Arda C¸ elebi†, Arzucan ¨Ozg¨ur, Suzan ¨Usk¨udarlı
</author>
<affiliation confidence="0.931566">
Department of Computer Engineering
Bogazici University
Istanbul, Turkey
</affiliation>
<email confidence="0.994559">
inadin.kokciyan,arda.celebi,arzucan.ozgur,suzan.uskudarlil@boun.edu.tr
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962285714286">
The widespread use of Twitter makes it very
interesting to determine the opinions and the
sentiments expressed by its users. The short-
ness of the length and the highly informal na-
ture of tweets render it very difficult to auto-
matically detect such information. This paper
reports the results to a challenge, set forth by
SemEval-2013 Task 2, to determine the posi-
tive, neutral, or negative sentiments of tweets.
Two systems are explained: System A for de-
termining the sentiment of a phrase within a
tweet and System B for determining the senti-
ment of a tweet. Both approaches rely on rich
feature sets, which are explained in detail.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996306085106384">
Twitter consists of a massive number of posts on a
wide range of subjects, making it very interesting to
extract information and sentiments from them. For
example, answering questions like ‘What do Twitter
users feel about the brand X?’ are quite interesting.
The constrained length and highly informal nature
of tweets presents a serious challenge for the auto-
mated extraction of such sentiments.
Twitter supports special tokens (i.e. mentions and
hashtags), which have been utilized to determine the
sentiment of tweets. In (Go et al., 2009), emoticons
are used to label tweets. In (Davidov et al., 2010),
Twitter emoticons as well as hashtags are used to la-
bel tweets. O’Connor et al. (2010) demonstrated
a correlation between sentiments identified in pub-
lic opinion polls and those in tweets. A subjectivity
† These authors contributed equally to this work
lexicon was used to identify the positive and nega-
tive words in a tweet. In (Barbosa and Feng, 2010),
subjective tweets are used for sentiment classifica-
tion. They propose the use of word specific (e.g.
POS tags) and tweet specific (e.g. presence of a link)
features. Most of these studies use their own anno-
tated data sets for evaluation, which makes it diffi-
cult to compare the performances of their proposed
approaches.
Sentiment Analysis in Twitter 2013 (SemEval
2013 Task 2) (Wilson et al., 2013) presented a chal-
lenge for exploring different approaches examin-
ing sentiments conveyed in tweets: interval-level
(phrase-level) sentiment classification (TaskA) and
message-level sentiment classification (TaskB). Sen-
timent are considered as positive, negative, or neu-
tral. For TaskA, the goal is to determine the sen-
timent of an interval (consecutive word sequence)
within a tweet. For TaskB, the goal is to determine
sentiment of an entire tweet. For example, let’s con-
sider a tweet like ‘Can’t wait until the DLC for ME3
comes out tomorrow. :-)’. For TaskA, the interval
0-1 (Can’t wait) is ‘positive’ and the interval 10-10
(:-)) is ‘positive’. For TaskB, this tweet is ‘positive’.
In this paper, we present two systems, one for
TaskA and one for TaskB. In both cases machine
learning methods were utilized with rich feature sets
based on the characteristics of tweets. Our results
suggest that our approach is promising for sentiment
classification in Twitter.
</bodyText>
<sectionHeader confidence="0.994897" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.998041">
The task of detecting the sentiments of a tweet or
an interval therein, is treated as a classification of
</bodyText>
<page confidence="0.978114">
554
</page>
<note confidence="0.7326465">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 554–561, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.998698407407407">
TaskA
TaskB
Positive
Classifier
Negative
Classifier
Interval Classifier
Tweet
Classifier
Classified
Tweet Intervals
Classified
Tweets
+/-/0
+/-/0
Multiple Binary Classifier
Preprocessor
+
Feature
Generator
TaskA
Tweets
with
Intervals
TaskB
Tweets
Lexicons
</figure>
<figureCaption confidence="0.999996">
Figure 1: The Overview of BOUNCE System
</figureCaption>
<bodyText confidence="0.999981769230769">
tweets into positive, negative, or neutral sets. Fig-
ure 1 gives the overview of our approach. The Pre-
processor module tokenizes the tweets that are used
by the Feature Generator. At this stage, the tweets
are represented as feature vectors. For TaskA, the
feature vectors are used by the Interval Classifier
that predicts the labels of the tweet intervals. For
TaskB, the feature vectors are used by the Positive
Classifier and the Negative Classifier which report
on the positivity and negativity of the tweets. The
Tweet Classifier determines the tweet labels using a
rule-based method. Each step is described in detail
in the following subsections.
</bodyText>
<subsectionHeader confidence="0.986397">
2.1 Lexicons
</subsectionHeader>
<bodyText confidence="0.999316888888889">
The core of our approach to sentiment analysis relies
on word lists that are used to determine the positive
and negative words or phrases. Several acquired lists
are used in addition to one that we curated. AFINN
(Nielsen, 2011) is the main sentiment word list in-
cluding 2477 words rated between -5 to 5 for va-
lence. SentiWordNet (Baccianella et al., 2010), de-
rived from the Princeton English WordNet (Miller,
1995), assigns positive, negative, or objective scores
to each synset in WordNet. We considered the av-
erage of a word’s synsets as its SentiWordNet score.
Thus, synsets are disregarded and no disambiguation
of the sense of a word in a given context is done.
The SentiWordNet score of a word is not used if it
has objective synsets, since it indicates that the word
might have been used in an objective sense. We use
a list of emotion words and categories that is created
by DeRose1. Furthermore, a slang dictionary down-
</bodyText>
<footnote confidence="0.891847">
1http://derose.net/steve/resources/emotionwords/ewords.html
</footnote>
<bodyText confidence="0.995643222222222">
loaded from the Urban Dictionary2 containing over
16,000 phrases (with no sentiment) is used. Finally,
we curated a sentiment word list initiated with a list
of positive and negative words obtained from Gen-
eral Inquirer (Stone et al., 1966), and refined by sen-
timent emitting words from a frequency-based or-
dered word list generated from the training data set
of SemEval-2013 Task A. Naturally, this list is more
specialized to the Twitter domain.
</bodyText>
<subsectionHeader confidence="0.999266">
2.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999365166666667">
Prior to feature generation, tweets were prepro-
cessed to yield text with more common wording.
For this, CMU’s Ark Tokenizer and Part-of-Speech
(POS) Tagger (Gimpel et al., 2011), which has been
specifically trained for tweets, was used. Tweets are
tokenized and POS tagged.
</bodyText>
<subsectionHeader confidence="0.998958">
2.3 Feature Sets
</subsectionHeader>
<bodyText confidence="0.999959846153846">
In addition to the lexical or syntactic characteristics,
the manner in which tweets are written may reveal
sentiment. Orthogonal shapes of words (esp. fully
or partially capitalized words), expressions of a sin-
gle word or a phrase in the form of a hashtag, posi-
tions of certain tokens in a tweet are prominent char-
acteristics of tweets. In addition to these, tweets may
convey multiple sentiments. This leads to sequence-
based features, where we append features for each
sentiment emitted by a word or a phrase in a tweet.
Moreover, since TaskA asks for sentiment of inter-
vals in a tweet, we also engineer features to catch
clues from the surrounding context of the interval,
</bodyText>
<footnote confidence="0.988637">
2http://www.urbandictionary.com
</footnote>
<page confidence="0.997301">
555
</page>
<bodyText confidence="0.999932">
such as the sentiments and lengths of the neighbor-
ing intervals. For TaskB, the usage of hashtags and
last words in tweets were occasionally sentimental,
thus we considered them as features as well. We ex-
plain all features in detail in Section 3.
</bodyText>
<subsectionHeader confidence="0.993184">
2.4 Classification
</subsectionHeader>
<bodyText confidence="0.99997736">
Maximum entropy models (Berger et al., 1996) have
been used in sentiment analysis (Fei et al., 2010).
They model all given data and treat the remainder as
uniform as possible making no assumptions about
what is not provided. For this, TaskA system uses
the MaxEnt tool (Zhang, 2011).
Naive Bayes is a simple probabilistic model based
on Bayes’ Theorem that assumes independence be-
tween features. It has performed well in sentiment
classification of Twitter data (Go et al., 2009; Bifet
and Frank, 2010). TaskB data was not evenly dis-
tributed. There were very few negative tweets com-
pared to positive tweets. Using a single classifier
to distinguish the classes from each other resulted
in poor performance in identifying negative tweets.
Therefore, TaskB system utilizes multiple binary
classifiers that use the one-vs-all strategy. Maximum
Entropy and Naive Bayes models were considered
and the model that performed best on the develop-
ment set was chosen for each classifier. As a result,
the positive classifier (Bpo3) is based on the Max-
imum Entropy model, whereas the negative classi-
fier (Bneg) is based on Naive Bayes. TaskB system
uses the Natural Language Toolkit (Loper and Bird,
2002).
</bodyText>
<sectionHeader confidence="0.996803" genericHeader="method">
3 Systems
</sectionHeader>
<bodyText confidence="0.999964">
In this section, TaskA and TaskB systems are ex-
plained in detail. All features used in the final ex-
periments for both tasks are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999549">
3.1 TaskA System
</subsectionHeader>
<bodyText confidence="0.996603846153846">
TaskA is a classification task where we classify a
given interval as having positive, negative or neutral
sentiment. TaskA feature sets are shown in Table 1.
lexical features: These features use directly
words (or tokens) from tweets as features. single-
word feature uses the word of the single-word inter-
vals, whereas slang features are created for match-
ing uni-grams and bi-grams from our slang dictio-
nary. We also use emoticons as features, as well as
the words or phrases that emit emotion according to
the lexicons described in Section 2.1.
score-based features: These features use the
scores obtained from the AFINN and SentiWordNet
(SWN) lexicons. We use separate scores for the pos-
itive and negative sentiments, since one interval may
contain multiple words with opposite sentiment. In
case of multiple positive or negative occurances, we
take the arithmetic mean of those.
shape-based features: These features capture the
length of an interval, whether it contains a capital-
ized word or all words are capitalized, whether it
contains a URL, or ends with an exclamation mark.
tag-based features: In addition to numeric val-
ues of sentiments, we use the tokens ‘positive’ and
‘negative’ to express the type of sentiment. When
multiple words emit a sentiment in a given interval,
their corresponding tokens are appended to create a
single feature out of it, sequences. Moreover, we
have another set of features which also contains the
POS tags of these sentiment words.
indicator features: These features are used in or-
der to expose how many sentiment emitting words
from our currated large lexicon exist in a given inter-
val. hasNegation indicates the presence of a nega-
tion word like not or can’t in the interval, whereas
numOfPosIndicators and numOfNegIndicators gives
the number of tokens that convey positive and nega-
tive sentiment, respectively.
context features: In addition to the features gen-
erated from the given interval, these features capture
the context information from the neighboring inter-
vals. Feature surroundings combines the length of
the interval along with the lengths of the intervals on
both sides, whereas surrounding-shape and extra-
surrounding-shape features use number of positive
and negative sentiment indicators for the intervals.
We also use their normalized forms (those starting
with norm-) where we divide the number of indi-
cators by the length of the interval. Features with
-extra- use two adjacent intervals from both sides.
Intervals that are not available are represented with
NA.
</bodyText>
<subsectionHeader confidence="0.998804">
3.2 TaskB System
</subsectionHeader>
<bodyText confidence="0.993257">
TaskB is a classification task where we determine
the sentiment (positive, negative, or neutral) of a
tweet. TaskB system uses a rule-based method to
</bodyText>
<page confidence="0.986748">
556
</page>
<table confidence="0.999889576923077">
Feature Set Feature Example Feature Instance used by
lexical-based single-word-* single-word-worst A, B
slang-* slang-shit A, Bpos
emoticons-* emoticons-:) A
emitted-emotions-* emitted-emotions-angry A, B
score-based afinn-positive:#, afinn-negative:# afinn-positive:4, afinn-negative:-2 A, B
swn-positive:#, swn-negative:# swn-positive:2, swn-negative:-3 A
shape-based length-# length-10 A
hasAllCap-T/F hasAllCap-T A
fullCap-T/F fullCap-T A
hasURL-T/F hasURL-F A, B
endsWExlamation-T/F endsWExlamation-T A, Bneg
tag-based our-seq-* our-seq-positive-positive A, B
our-tag-seq-*, swn-seq-*, swn-tag-seq-* afinn-seq-positive-a-positive-n A
afinn-seq-*, afinn-tag-seq-* afinn-seq-positive-a-negative-n A
indicators hasNegation-T/F hasNegation-F A
numOfPosIndicators-# numOfPosIndicators-2 A
numOfNegIndicators-# numOfNegIndicators-0 A
context surroundings-#-#-# surroundings-1-2-NA A
surr-shape-#-#-# surrounding-shape-NA-2-1 A
extra-surr-shape-#-#-#-#-# extra-surr-shape-NA-2-1-0-1 A
norm-surr-shape-#-#-# norm-surr-shape-0.5-0.2-0.0 A
norm-extra-surr-shape-#-#-#-#-# norm-extra-surr-shape-NA-0.5-0.2-0.0-0.2 A
left-sentiment-*, right-sentiment-* left-sentiment-positive A
twitter-tags hasEmoticon-T/F hasEmoticon-T B
hasMention-T/F hasMention-T B
hasHashtag-T/F hasHashtag-F B
[emoticon|mention|hash]-count-# mention-count-3 B
repetition unigram-*n unigram-[no+] B
$character-count-# o-count-7 B
lastword lastword-*n lastword-[OMG+] B
lastwordshape-* lastwordshape-XXXX B
chat chatword-* for word ‘gz’: chatword-congratulations B
interjection interjection-*n interjection-[lo+l] B
negation negword-*n negword-never Bneg
negword-count-# negword-count-3 Bneg
negcapword-count-# negcapword-count-1 Bneg
hash hashword-* hashword-good B
hashtag-#* hashtag-#good B
hash-sentiment-[positive|negative] hash-sentiment-positive B
lingemotion [noun|verb|adverb|adjective]-$emotion noun-fear B
oursent oursent-* for tweet: a nice morning.. I hate work.. damn! B
oursent-longseq-* oursent-nice, oursent-hate, oursent-damn B
oursent-shortseq-* oursent-longseq-pnn B
oursent-first-last-* oursent-shortseq-pn B
oursent-first-last-pn
afinn-phrases phrase-firstsense-[positive|negative] phrase-firstsense-positive B
phrase-lastsense-[positive|negative] phrase-lastsense-negative B
afinnword-* afinnword-nice, afinnword-hate, afinnword-damn B
afinn-firstsense-[positive|negative] afinn-firstsense-positive B
afinn-lastsense-[positive|negative] afinn-lastsense-positive B
emo emo-pattern-* for =) : emo-pattern-HAPPY B
</table>
<tableCaption confidence="0.999587">
Table 1: Feature sets used in TaskA and TaskB
</tableCaption>
<page confidence="0.936786">
557
</page>
<table confidence="0.999849285714286">
Dataset Type Positive Negative Neutral+Objective Tot. No. of Instances
Training 5290 (5865) 2771(3120) 16118 (17943) 24179 (26928)
TaskA Development 589 (648) 392 (430) 1993 (2202) 2974 (3280)
Test 2734 1541 160 4435
Training 3274 (3640) 1291 (1458) 4155 (4586) 8720 (9684)
TaskB Development 523 (575) 309 (340) 674 (739) 1506 (1654)
Test 1572 601 1640 3813
</table>
<tableCaption confidence="0.999468">
Table 2: Number of instances used in TaskA and TaskB
</tableCaption>
<bodyText confidence="0.999435346666667">
decide on the sentiment label of a tweet. For each
tweet, the probabilities of belonging to the posi-
tive class (Probpos) and negative class (Probneg)
are computed by the Bpos and Bneg classifiers, re-
spectively. If Probpos is greater than Probneg, and
greater than a predefined threshold, then the tweet
is classified as ‘positive’, otherwise it is classified
as ‘neutral’. On the other hand, if Probneg is
greater than Probpos, and greater than the prede-
fined threshold, then the tweet is classified as ‘neg-
ative’, otherwise it is classified as ‘neutral’. The
threshold is set to 0.45, since it gives the optimal F-
score on the development set. TaskB features along
with examples are shown in Table 1.
twitter-tags: hasEmoticon, hasMention, ha-
sURL, and hasHashtag indicate whether the corre-
sponding term (e.g. mention) exists in the tweet.
repetition: Words with repeating letters are
added as a feature *n. *n represents the normalized
version (i.e., no repeating letters) of a word. For ex-
ample, ‘nooooooo’ is shortened to [no+]. We also
keep the count of the repeated character.
wordshape: Shape of each word in a tweet is con-
sidered. For example, the shape of ‘NOoOo!!’ is
‘XXxXx!!’.
lastword: The normalized form and the shape of
the last word are used as features. For example, if
the lastword is ‘OMGG’, then lastword ‘[OMG+]’
and lastwordshape ‘XXXX’ are used as features.
chat: A list of chat abbreviations that express sen-
timent is manually created. Each abbreviation is re-
placed by its corresponding word.
interjection: An interjection is a word that ex-
presses an emotion or sentiment (e.g. hurraah,
loool). Interjection wordn is used as a feature.
negation: We manually created a negation list ex-
tended by word clusters from (Owoputi et al., 2013).
A negation word is represented by spellings such
as not, n0t, and naht. Each negation wordn (e.g
neve[r+]) is considered. We keep the count of nega-
tion words and all capitalized negation words.
hash: If the hashtag is ‘#good’ then #good and
good become hash features. If the hashtag is a sen-
timent expressing word according to our sentiment
word list, then we keep the sentiment information.
lingemotion: Nodebox Linguistics3 package
gives emotional values of words for expressions of
emotions such as fear and sadness. POS augmented
expression information is used as a feature.
oursent: Each word in a tweet that exists in our
sentiment word list is considered. When multiple
sentiment expressing words are found, a sentiment
sequence feature is used. oursent-longseq keeps
the long sequence, whereas oursent-shortseq keeps
same sequence without repetitive sentiments. We
also consider the first and last sentiments emitted by
a tweet.
afinn: We consider each word that exists in
AFINN. If a negation exists before this word, the
opposite sentiment is considered. For example, if a
tweet contains the bigram ‘not good’, then the senti-
ment of the bigram is set to ‘negative’. The AFINN
scores of the positive and negative words, as well as
the first and last sentiments emitted by the tweet are
considered.
phrases: Each n-gram (n &gt; 1) of a tweet that
exists in our sentiment phrase list is considered.
afinn-phrases: Phrases are retrieved using the
phrases feature. Each sentiment that appears in
a phrase is kept, hence we obtain a sentiment se-
quence. The first and last sentiments of this se-
quence are also considered. Then, the phrases are
removed from the tweet text and the afinn feature is
applied.
emo: We manually created an emoticon list where
</bodyText>
<footnote confidence="0.948431">
3http://nodebox.net/code/index.php/Linguistics
</footnote>
<page confidence="0.993654">
558
</page>
<bodyText confidence="0.999658666666667">
each term is associated with an emotion pattern such
as HAPPY. These emotion patterns are used as a fea-
ture.
others: Bp,,, uses the slang feature from the lexi-
cal feature set, and B,,eg uses endsWExlamation fea-
ture from the indicators feature set.
</bodyText>
<sectionHeader confidence="0.991822" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.990585">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999748266666667">
The data set provided by the task organizers was an-
notated by using Amazon Mechanical Turk4. The
annotations of the tweets in the training and devel-
opment sets were provided to the task participants.
However, the tweets had to be downloaded from
Twitter by using the script made available by the or-
ganizers. We were unable to download all the tweets
in the training and development sets, since some
tweets were deleted and others were not publicly
accessible due to their updated authorization status.
The number of actual tweets (numbers in parenthe-
ses) and the number of collected tweets are shown in
Table 2. Almost 10% of the data for both tasks are
missing. For the test data, however, the tweets were
directly provided to the participants.
</bodyText>
<subsectionHeader confidence="0.954394">
4.2 Results on TaskA
</subsectionHeader>
<bodyText confidence="0.999994578947369">
We start our experiments with features generated
from lexicons and emoticons. Called our baseline,
it achieved an f-score of 47.8 on the devset in Ta-
ble 3. As we add other features at each step, we
reach an average f-score of 81.6 on the devset at
the end. Among those features, the most contribut-
ing ones are lexical feature single-word, indicator
feature hasNegation, and especially shape feature
length. The success of the length feature is mostly
due to the nature of intervals, where the long ones
tend to be neutral, and the rest are mostly positive
or negative. Another noteworthy result is that our
curated word list contributed more compared to the
others. When the final model is used on the test set,
we get the results in Table 5. Having low neutral f-
score might be due to the fact that there were only a
few neutral intervals in the test set, which might in-
dicate that their characteristics may not be the same
as the ones in the devset.
</bodyText>
<footnote confidence="0.966744">
4https://www.mturk.com/mturk/
</footnote>
<table confidence="0.9999116">
Added Features Avg. F-Score
afinn-positive, afinn-negetive 47.8
swn-positive, swn-negative,
emoticons, emitted-emotions
+ hasAllCap, fullCap, hasURL, 50.1
endsWExclamation
+ slang 51.5
+ single-word 56.8
+ afinn-seq, swn-seq, afinn-tag-seq, 57.7
swn-tag-seq
+ our-seq, our-tag-seq 60.2
+ hasNegation 64.8
+ numOfPosIndicators, 65.3
numOfNegIndicators
+ length 75.2
+ left-sentiment, right-sentiment 76.5
+ surroundings, surrounding-shape 78.9
+ extra-surrounding-shape 80.6
+ norm-surrounding-shape, 81.6
norm-extra-surrounding-shape
</table>
<tableCaption confidence="0.996149">
Table 3: Macro-averaged F-Score on the TaskA dev. set
</tableCaption>
<table confidence="0.999717111111111">
Added Features Average
F-Score
oursent (baseline) 58.59
+ afinn-phrases 64.64
+ tags + hash 65.43
+ interjection + chat 65.53
+ emo + lingemotion 65.92
+ repetition + lastword 66.01
+ negation + others 66.32
</table>
<tableCaption confidence="0.999833">
Table 4: Macro-averaged F-Score on the TaskB dev. set
</tableCaption>
<subsectionHeader confidence="0.965508">
4.3 Results on TaskB
</subsectionHeader>
<bodyText confidence="0.978810909090909">
The baseline model is considered to include oursent
feature that gives an average f-score of 58.59. Next,
we added the afinn-phrases feature which increased
the average f-score to 64.64. This increase can be
explained by the sentiment scores and sequence pat-
terns that afinn-phrases is based on. Following that
model, the other added features slightly increased
the average f-score to 66.32 as shown in Table 4.
The final model is used over the test set of TaskB,
where we obtained an f-score of 63.53 as shown in
Table 5.
</bodyText>
<page confidence="0.981125">
559
</page>
<table confidence="0.999886222222222">
Class Precision Recall F-Score
positive 89.7 88.3 89.0
TestA negative 86.6 82.7 84.6
neutral 10.7 18.1 13.4
average(pos+neg) 88.15 85.5 86.8
positive 82.3 55.6 66.4
TestB negative 48.7 80.2 60.6
neutral 68.2 73.3 70.7
average(pos+neg) 65.56 67.93 63.53
</table>
<tableCaption confidence="0.99981">
Table 5: Results on the test sets for both tasks
</tableCaption>
<sectionHeader confidence="0.996481" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999652">
We presented two systems one for TaskA (a Maxi-
mum Entropy model) and one for TaskB (Maximum
Entropy + Naive Bayes models) based on using rich
feature sets. For Task A, we started with a baseline
system that just uses ordinary features like sentiment
scores of words. As we added new features, we ob-
served that lexical features and shape-based features
are the ones that contribute most to the performance
of the system. Including the context features and the
indicator feature for negations led to considerable
improvement in performance as well. For TaskB,
we first created a baseline model that uses sentiment
words and phrases from the AFINN lexicon as fea-
tures. Each feature that we added to the system re-
sulted in improvement in performance. The nega-
tion and endsWExclamation features only improved
the performance of the negative classifier, whereas
the slang feature only improved the performance of
the positive classifier.
Our results show that using rich feature sets with
machine learning algorithms is a promising ap-
proach for sentiment classification in Twitter. Our
TaskA system ranked 3rd among 23 systems and
TaskB system ranked 4th among 35 systems partici-
pating in SemEval 2013 Task 2.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99974495">
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evaluation
(LREC’10), Valletta, Malta, May. European Language
Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ’10,
pages 36–44, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22:39–71.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Proceed-
ings of the 13th international conference on Discov-
ery science, DS’10, pages 1–15, Berlin, Heidelberg.
Springer-Verlag.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10, pages 241–249, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Xiaoxu Fei, Huizhen Wang, and Jingbo Zhu. 2010. Sen-
timent word identification using the maximum entropy
model. In International Conference on Natural Lan-
guage Processing and Knowledge Engineering (NLP-
KE), pages 1–4.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
twitter: annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
’11, pages 42–47. Association for Computational Lin-
guistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical report, Stanford University.
Edward Loper and Steven Bird. 2002. Nltk: the natural
language toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and methodologies for teach-
ing natural language processing and computational
linguistics - Volume 1, ETMTNLP ’02, pages 63–70,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
George A. Miller. 1995. Wordnet: A lexical database for
english. Communications of the ACM, 38:39–41.
Finn ˚A. Nielsen. 2011. A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ‘Making
Sense of Microposts’: Big things come in small pack-
ages.
</reference>
<page confidence="0.964502">
560
</page>
<reference confidence="0.999395541666667">
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conver-
sational text with word clusters. In Proceedings of
NAACL.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. MIT
Press.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ’13, June.
Le Zhang. 2011. Maximum entropy modeling toolkit for
python and c++. http://homepages.inf.ed.
ac.uk/lzhang10/maxent_toolkit.html.
Accessed: 2013-04-13.
</reference>
<page confidence="0.997876">
561
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767436">
<title confidence="0.999535">BOUNCE: Sentiment Classification in Twitter using Rich Feature Sets</title>
<author confidence="0.999825">Arda Arzucan Suzan</author>
<affiliation confidence="0.907673">Department of Computer Bogazici</affiliation>
<address confidence="0.977161">Istanbul, Turkey</address>
<email confidence="0.990568">inadin.kokciyan,arda.celebi,arzucan.ozgur,suzan.uskudarlil@boun.edu.tr</email>
<abstract confidence="0.9982762">The widespread use of Twitter makes it very interesting to determine the opinions and the sentiments expressed by its users. The shortness of the length and the highly informal nature of tweets render it very difficult to automatically detect such information. This paper reports the results to a challenge, set forth by SemEval-2013 Task 2, to determine the positive, neutral, or negative sentiments of tweets. Two systems are explained: System A for determining the sentiment of a phrase within a tweet and System B for determining the sentiment of a tweet. Both approaches rely on rich feature sets, which are explained in detail.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="4955" citStr="Baccianella et al., 2010" startWordPosition="769" endWordPosition="772"> by the Positive Classifier and the Negative Classifier which report on the positivity and negativity of the tweets. The Tweet Classifier determines the tweet labels using a rule-based method. Each step is described in detail in the following subsections. 2.1 Lexicons The core of our approach to sentiment analysis relies on word lists that are used to determine the positive and negative words or phrases. Several acquired lists are used in addition to one that we curated. AFINN (Nielsen, 2011) is the main sentiment word list including 2477 words rated between -5 to 5 for valence. SentiWordNet (Baccianella et al., 2010), derived from the Princeton English WordNet (Miller, 1995), assigns positive, negative, or objective scores to each synset in WordNet. We considered the average of a word’s synsets as its SentiWordNet score. Thus, synsets are disregarded and no disambiguation of the sense of a word in a given context is done. The SentiWordNet score of a word is not used if it has objective synsets, since it indicates that the word might have been used in an objective sense. We use a list of emotion words and categories that is created by DeRose1. Furthermore, a slang dictionary down1http://derose.net/steve/re</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. In Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1899" citStr="Barbosa and Feng, 2010" startWordPosition="292" endWordPosition="295"> challenge for the automated extraction of such sentiments. Twitter supports special tokens (i.e. mentions and hashtags), which have been utilized to determine the sentiment of tweets. In (Go et al., 2009), emoticons are used to label tweets. In (Davidov et al., 2010), Twitter emoticons as well as hashtags are used to label tweets. O’Connor et al. (2010) demonstrated a correlation between sentiments identified in public opinion polls and those in tweets. A subjectivity † These authors contributed equally to this work lexicon was used to identify the positive and negative words in a tweet. In (Barbosa and Feng, 2010), subjective tweets are used for sentiment classification. They propose the use of word specific (e.g. POS tags) and tweet specific (e.g. presence of a link) features. Most of these studies use their own annotated data sets for evaluation, which makes it difficult to compare the performances of their proposed approaches. Sentiment Analysis in Twitter 2013 (SemEval 2013 Task 2) (Wilson et al., 2013) presented a challenge for exploring different approaches examining sentiments conveyed in tweets: interval-level (phrase-level) sentiment classification (TaskA) and message-level sentiment classific</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 36–44, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--39</pages>
<contexts>
<context position="7366" citStr="Berger et al., 1996" startWordPosition="1159" endWordPosition="1162">s leads to sequencebased features, where we append features for each sentiment emitted by a word or a phrase in a tweet. Moreover, since TaskA asks for sentiment of intervals in a tweet, we also engineer features to catch clues from the surrounding context of the interval, 2http://www.urbandictionary.com 555 such as the sentiments and lengths of the neighboring intervals. For TaskB, the usage of hashtags and last words in tweets were occasionally sentimental, thus we considered them as features as well. We explain all features in detail in Section 3. 2.4 Classification Maximum entropy models (Berger et al., 1996) have been used in sentiment analysis (Fei et al., 2010). They model all given data and treat the remainder as uniform as possible making no assumptions about what is not provided. For this, TaskA system uses the MaxEnt tool (Zhang, 2011). Naive Bayes is a simple probabilistic model based on Bayes’ Theorem that assumes independence between features. It has performed well in sentiment classification of Twitter data (Go et al., 2009; Bifet and Frank, 2010). TaskB data was not evenly distributed. There were very few negative tweets compared to positive tweets. Using a single classifier to disting</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Eibe Frank</author>
</authors>
<title>Sentiment knowledge discovery in twitter streaming data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 13th international conference on Discovery science, DS’10,</booktitle>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="7824" citStr="Bifet and Frank, 2010" startWordPosition="1235" endWordPosition="1238">mental, thus we considered them as features as well. We explain all features in detail in Section 3. 2.4 Classification Maximum entropy models (Berger et al., 1996) have been used in sentiment analysis (Fei et al., 2010). They model all given data and treat the remainder as uniform as possible making no assumptions about what is not provided. For this, TaskA system uses the MaxEnt tool (Zhang, 2011). Naive Bayes is a simple probabilistic model based on Bayes’ Theorem that assumes independence between features. It has performed well in sentiment classification of Twitter data (Go et al., 2009; Bifet and Frank, 2010). TaskB data was not evenly distributed. There were very few negative tweets compared to positive tweets. Using a single classifier to distinguish the classes from each other resulted in poor performance in identifying negative tweets. Therefore, TaskB system utilizes multiple binary classifiers that use the one-vs-all strategy. Maximum Entropy and Naive Bayes models were considered and the model that performed best on the development set was chosen for each classifier. As a result, the positive classifier (Bpo3) is based on the Maximum Entropy model, whereas the negative classifier (Bneg) is </context>
</contexts>
<marker>Bifet, Frank, 2010</marker>
<rawString>Albert Bifet and Eibe Frank. 2010. Sentiment knowledge discovery in twitter streaming data. In Proceedings of the 13th international conference on Discovery science, DS’10, pages 1–15, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>241--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1544" citStr="Davidov et al., 2010" startWordPosition="232" endWordPosition="235">1 Introduction Twitter consists of a massive number of posts on a wide range of subjects, making it very interesting to extract information and sentiments from them. For example, answering questions like ‘What do Twitter users feel about the brand X?’ are quite interesting. The constrained length and highly informal nature of tweets presents a serious challenge for the automated extraction of such sentiments. Twitter supports special tokens (i.e. mentions and hashtags), which have been utilized to determine the sentiment of tweets. In (Go et al., 2009), emoticons are used to label tweets. In (Davidov et al., 2010), Twitter emoticons as well as hashtags are used to label tweets. O’Connor et al. (2010) demonstrated a correlation between sentiments identified in public opinion polls and those in tweets. A subjectivity † These authors contributed equally to this work lexicon was used to identify the positive and negative words in a tweet. In (Barbosa and Feng, 2010), subjective tweets are used for sentiment classification. They propose the use of word specific (e.g. POS tags) and tweet specific (e.g. presence of a link) features. Most of these studies use their own annotated data sets for evaluation, which</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 241–249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoxu Fei</author>
<author>Huizhen Wang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Sentiment word identification using the maximum entropy model.</title>
<date>2010</date>
<booktitle>In International Conference on Natural Language Processing and Knowledge Engineering (NLPKE),</booktitle>
<pages>1--4</pages>
<contexts>
<context position="7422" citStr="Fei et al., 2010" startWordPosition="1169" endWordPosition="1172"> for each sentiment emitted by a word or a phrase in a tweet. Moreover, since TaskA asks for sentiment of intervals in a tweet, we also engineer features to catch clues from the surrounding context of the interval, 2http://www.urbandictionary.com 555 such as the sentiments and lengths of the neighboring intervals. For TaskB, the usage of hashtags and last words in tweets were occasionally sentimental, thus we considered them as features as well. We explain all features in detail in Section 3. 2.4 Classification Maximum entropy models (Berger et al., 1996) have been used in sentiment analysis (Fei et al., 2010). They model all given data and treat the remainder as uniform as possible making no assumptions about what is not provided. For this, TaskA system uses the MaxEnt tool (Zhang, 2011). Naive Bayes is a simple probabilistic model based on Bayes’ Theorem that assumes independence between features. It has performed well in sentiment classification of Twitter data (Go et al., 2009; Bifet and Frank, 2010). TaskB data was not evenly distributed. There were very few negative tweets compared to positive tweets. Using a single classifier to distinguish the classes from each other resulted in poor perfor</context>
</contexts>
<marker>Fei, Wang, Zhu, 2010</marker>
<rawString>Xiaoxu Fei, Huizhen Wang, and Jingbo Zhu. 2010. Sentiment word identification using the maximum entropy model. In International Conference on Natural Language Processing and Knowledge Engineering (NLPKE), pages 1–4.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for twitter: annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1481" citStr="Go et al., 2009" startWordPosition="221" endWordPosition="224">rely on rich feature sets, which are explained in detail. 1 Introduction Twitter consists of a massive number of posts on a wide range of subjects, making it very interesting to extract information and sentiments from them. For example, answering questions like ‘What do Twitter users feel about the brand X?’ are quite interesting. The constrained length and highly informal nature of tweets presents a serious challenge for the automated extraction of such sentiments. Twitter supports special tokens (i.e. mentions and hashtags), which have been utilized to determine the sentiment of tweets. In (Go et al., 2009), emoticons are used to label tweets. In (Davidov et al., 2010), Twitter emoticons as well as hashtags are used to label tweets. O’Connor et al. (2010) demonstrated a correlation between sentiments identified in public opinion polls and those in tweets. A subjectivity † These authors contributed equally to this work lexicon was used to identify the positive and negative words in a tweet. In (Barbosa and Feng, 2010), subjective tweets are used for sentiment classification. They propose the use of word specific (e.g. POS tags) and tweet specific (e.g. presence of a link) features. Most of these </context>
<context position="7800" citStr="Go et al., 2009" startWordPosition="1231" endWordPosition="1234">ccasionally sentimental, thus we considered them as features as well. We explain all features in detail in Section 3. 2.4 Classification Maximum entropy models (Berger et al., 1996) have been used in sentiment analysis (Fei et al., 2010). They model all given data and treat the remainder as uniform as possible making no assumptions about what is not provided. For this, TaskA system uses the MaxEnt tool (Zhang, 2011). Naive Bayes is a simple probabilistic model based on Bayes’ Theorem that assumes independence between features. It has performed well in sentiment classification of Twitter data (Go et al., 2009; Bifet and Frank, 2010). TaskB data was not evenly distributed. There were very few negative tweets compared to positive tweets. Using a single classifier to distinguish the classes from each other resulted in poor performance in identifying negative tweets. Therefore, TaskB system utilizes multiple binary classifiers that use the one-vs-all strategy. Maximum Entropy and Naive Bayes models were considered and the model that performed best on the development set was chosen for each classifier. As a result, the positive classifier (Bpo3) is based on the Maximum Entropy model, whereas the negati</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>Nltk: the natural language toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics - Volume 1, ETMTNLP ’02,</booktitle>
<pages>63--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8515" citStr="Loper and Bird, 2002" startWordPosition="1345" endWordPosition="1348">tweets compared to positive tweets. Using a single classifier to distinguish the classes from each other resulted in poor performance in identifying negative tweets. Therefore, TaskB system utilizes multiple binary classifiers that use the one-vs-all strategy. Maximum Entropy and Naive Bayes models were considered and the model that performed best on the development set was chosen for each classifier. As a result, the positive classifier (Bpo3) is based on the Maximum Entropy model, whereas the negative classifier (Bneg) is based on Naive Bayes. TaskB system uses the Natural Language Toolkit (Loper and Bird, 2002). 3 Systems In this section, TaskA and TaskB systems are explained in detail. All features used in the final experiments for both tasks are shown in Table 1. 3.1 TaskA System TaskA is a classification task where we classify a given interval as having positive, negative or neutral sentiment. TaskA feature sets are shown in Table 1. lexical features: These features use directly words (or tokens) from tweets as features. singleword feature uses the word of the single-word intervals, whereas slang features are created for matching uni-grams and bi-grams from our slang dictionary. We also use emoti</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. Nltk: the natural language toolkit. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics - Volume 1, ETMTNLP ’02, pages 63–70, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<pages>38--39</pages>
<contexts>
<context position="5014" citStr="Miller, 1995" startWordPosition="780" endWordPosition="781"> the positivity and negativity of the tweets. The Tweet Classifier determines the tweet labels using a rule-based method. Each step is described in detail in the following subsections. 2.1 Lexicons The core of our approach to sentiment analysis relies on word lists that are used to determine the positive and negative words or phrases. Several acquired lists are used in addition to one that we curated. AFINN (Nielsen, 2011) is the main sentiment word list including 2477 words rated between -5 to 5 for valence. SentiWordNet (Baccianella et al., 2010), derived from the Princeton English WordNet (Miller, 1995), assigns positive, negative, or objective scores to each synset in WordNet. We considered the average of a word’s synsets as its SentiWordNet score. Thus, synsets are disregarded and no disambiguation of the sense of a word in a given context is done. The SentiWordNet score of a word is not used if it has objective synsets, since it indicates that the word might have been used in an objective sense. We use a list of emotion words and categories that is created by DeRose1. Furthermore, a slang dictionary down1http://derose.net/steve/resources/emotionwords/ewords.html loaded from the Urban Dict</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM, 38:39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚A Nielsen</author>
</authors>
<title>A new ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things</booktitle>
<note>come in small packages.</note>
<contexts>
<context position="4827" citStr="Nielsen, 2011" startWordPosition="748" endWordPosition="749">d by the Interval Classifier that predicts the labels of the tweet intervals. For TaskB, the feature vectors are used by the Positive Classifier and the Negative Classifier which report on the positivity and negativity of the tweets. The Tweet Classifier determines the tweet labels using a rule-based method. Each step is described in detail in the following subsections. 2.1 Lexicons The core of our approach to sentiment analysis relies on word lists that are used to determine the positive and negative words or phrases. Several acquired lists are used in addition to one that we curated. AFINN (Nielsen, 2011) is the main sentiment word list including 2477 words rated between -5 to 5 for valence. SentiWordNet (Baccianella et al., 2010), derived from the Princeton English WordNet (Miller, 1995), assigns positive, negative, or objective scores to each synset in WordNet. We considered the average of a word’s synsets as its SentiWordNet score. Thus, synsets are disregarded and no disambiguation of the sense of a word in a given context is done. The SentiWordNet score of a word is not used if it has objective synsets, since it indicates that the word might have been used in an objective sense. We use a </context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚A. Nielsen. 2011. A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big things come in small packages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<date>2010</date>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010.</rawString>
</citation>
<citation valid="false">
<title>From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series.</title>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker></marker>
<rawString>From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series. In Proceedings of the International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<date>2013</date>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013.</rawString>
</citation>
<citation valid="false">
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<booktitle>In Proceedings of NAACL.</booktitle>
<marker></marker>
<rawString>Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
<author>Daniel M Ogilvie</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5828" citStr="Stone et al., 1966" startWordPosition="908" endWordPosition="911">uation of the sense of a word in a given context is done. The SentiWordNet score of a word is not used if it has objective synsets, since it indicates that the word might have been used in an objective sense. We use a list of emotion words and categories that is created by DeRose1. Furthermore, a slang dictionary down1http://derose.net/steve/resources/emotionwords/ewords.html loaded from the Urban Dictionary2 containing over 16,000 phrases (with no sentiment) is used. Finally, we curated a sentiment word list initiated with a list of positive and negative words obtained from General Inquirer (Stone et al., 1966), and refined by sentiment emitting words from a frequency-based ordered word list generated from the training data set of SemEval-2013 Task A. Naturally, this list is more specialized to the Twitter domain. 2.2 Preprocessing Prior to feature generation, tweets were preprocessed to yield text with more common wording. For this, CMU’s Ark Tokenizer and Part-of-Speech (POS) Tagger (Gimpel et al., 2011), which has been specifically trained for tweets, was used. Tweets are tokenized and POS tagged. 2.3 Feature Sets In addition to the lexical or syntactic characteristics, the manner in which tweets</context>
</contexts>
<marker>Stone, Dunphy, Smith, Ogilvie, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<title>Zornitsa Kozareva, Preslav Nakov,</title>
<date>2013</date>
<location>Sara Rosenthal, Veselin</location>
<marker>Wilson, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.</rawString>
</citation>
<citation valid="true">
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date></date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<marker></marker>
<rawString>SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for python and c++.</title>
<date>2011</date>
<booktitle>http://homepages.inf.ed. ac.uk/lzhang10/maxent_toolkit.html. Accessed:</booktitle>
<pages>2013--04</pages>
<marker>Le Zhang, 2011</marker>
<rawString>Le Zhang. 2011. Maximum entropy modeling toolkit for python and c++. http://homepages.inf.ed. ac.uk/lzhang10/maxent_toolkit.html. Accessed: 2013-04-13.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>