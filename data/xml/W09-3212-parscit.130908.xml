<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000609">
<title confidence="0.959738">
Quantitative analysis of treebanks using frequent subtree mining methods
</title>
<author confidence="0.931684">
Scott Martens
</author>
<affiliation confidence="0.713015">
Centrum voor Computerlinguistiek, KU Leuven
</affiliation>
<address confidence="0.740766">
Blijde-Inkomststraat 13, bus 3315
3000 Leuven Belgium
</address>
<email confidence="0.997575">
scott@ccl.kuleuven.be
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988857142857">
The first task of statistical computational
linguistics, or any other type of data-
driven processing of language, is the ex-
traction of counts and distributions of phe-
nomena. This is much more difficult for
the type of complex structured data found
in treebanks and in corpora with sophisti-
cated annotation than for tokenized texts.
Recent developments in data mining, par-
ticularly in the extraction of frequent sub-
trees from treebanks, offer some solutions.
We have applied a modified version of the
TreeMiner algorithm to a small treebank
and present some promising results.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998956931034483">
Statistical corpus linguistics and many natural lan-
guage processing applications rely on extracting
the frequencies and distributions of phenomena
from natural language data sources. This is rela-
tively simple when language data is treated as bags
of tokens or as n-grams, but much more compli-
cated for corpora annotated with complex feature
schemes and for treebanks where syntactic depen-
dencies are marked. A great deal of useful infor-
mation is encoded in these more complex struc-
tured corpora, but access to it is very limited using
the traditional algorithms and analytical tools of
computational linguistics. Many of the most pow-
erful techniques available to natural language pro-
cessing have been built on the basis of n-gram and
bag of words models, but we already know that
these methods are inadequate to fully model the
information in texts or we would have little use
for treebanks or annotation schemes.
Suffix trees provide some improvement over
n-grams and bag-of-words schemes by identify-
ing all frequently occurring sequences regard-
less of length (Weiner, 1973; McCreight, 1976;
Ravichandran and Hovy, 2002). While this has
value in identifying some multi-word phenomena,
any algorithm that models languages on the basis
of frequent contiguous string discovery will have
trouble modeling a number of pervasive phenom-
ena in natural language. In particular:
</bodyText>
<listItem confidence="0.9866255625">
• Long distance dependencies – i.e., dependen-
cies between words that are too far apart to be
accessible to n-gram models.
• Flexible word orders – languages usually
have contexts where word order can vary.
• Languages with very rich morphologies that
must be taken into account or where too much
important information is lost through lemma-
tization.
• Correlations between different levels of ab-
straction in annotation, such as between the
lemma of a verb and the semantic or syntac-
tic class of its arguments.
• Extra-syntactic correlations that may involve
any nearby word, such as semantic priming
effects.
</listItem>
<bodyText confidence="0.99290775">
In treebanks and other annotated corpora that
can be converted into rooted, directed graphs,
many of these phenomena are accessible as fre-
quently recurring subtrees. For example, consider
the Dutch idiom “naar huis gaan”, (to go home).
The components of this phrase can appear in a va-
riety of orders and with words inserted between
the constituents:
</bodyText>
<listItem confidence="0.989683666666667">
1. Ik zou naar huis kunnen gaan. (I could go
home.)
2. We gaan naar huis. (We’re going home.)
</listItem>
<bodyText confidence="0.99970025">
In a treebank, these two sentences would share
a common subtree that encompasses the phrase
“naar huis gaan”, as in Figure 1. Note that for this
purpose, two subtrees are treated as identical if the
</bodyText>
<page confidence="0.986704">
84
</page>
<note confidence="0.999883">
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 84–92,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.998838">
Figure 1: The two Dutch sentences Ik zou naar
</figureCaption>
<bodyText confidence="0.988602272727273">
huis kunnen gaan and We gaan naar huis, parsed,
and with the frequent section highlighted. Note
that these two subtrees are identical except for the
order of the nodes. (N.B.: This tree does not take
the difference between the infinitive and conju-
gated forms into account.)
only difference between them is the order of the
children of some or all the nodes.
Most theories of syntax use trees to represent in-
terlexical dependencies, and generally theories of
morphology and phonology use either hierarchical
tree structures to represent their formalisms, or use
unstructured bags that can be trivially represented
as trees. Most types of linguistic feature systems
are at least in part hierarchical and representable in
tree form. Because so many linguistic phenomena
are manifest as frequent subtrees within hierarchi-
cal representations that are motivated by linguistic
theories, efficient methods for extracting frequent
subtrees from treebanks are therefore potentially
very valuable to corpus and computational linguis-
tics.
</bodyText>
<sectionHeader confidence="0.922123" genericHeader="introduction">
2 Previous and Related Work
</sectionHeader>
<bodyText confidence="0.998639875">
Tree mining research is a subset of graph min-
ing focused specifically on rooted, directed acyclic
graphs. Although there is research into extracting
frequent subtrees from free (unrooted and undi-
rected) trees, free tree mining usually proceeds by
deciding which node in any particular tree will
be treated as a root, and then treating it as if it
was a rooted and directed tree (Chi et al., 2003).
</bodyText>
<figureCaption confidence="0.799530666666667">
Figure 2: Tree (a) and different types of subtree:
(b) a bottom-up subtree of (a), (c) an induced sub-
tree of (a), and (d) an embedded subtree of (a).
</figureCaption>
<bodyText confidence="0.999484">
Research on frequent subtree discovery generally
draws heavily on early work by Zaki (2002) and
Asai et al. (2002) who roughly simultaneously be-
gan applying the Apriori algorithm to frequent tree
discovery (Agrawal et al., 1993). For a summary
of Apriori, which is widely used in data mining,
and a short review of its extensive literature, see
Kotsiantis and Kanellopoulos (2006). A broad
summary of algorithms for frequent subtree min-
ing can be found in Chi et al. (2004).
Research into frequent substructures in compu-
tational linguistics is quite limited. The Data Ori-
ented Processing model (Bod et al., 2003) along
with its extension into machine translation - the
Data Oriented Translation model (Poutsma, 2000;
Poutsma, 2003; Hearne and Way, 2003) - is the
most developed approach to using frequent sub-
tree statistics to do natural language processing.
There is also growing work, largely stemming out
of DOP research, into subtree alignment in bilin-
gual parsed treebanks as an aid in the development
of statistical and example-based machine transla-
tions systems (Hassan et al., 2006; Tinsley et al.,
2007; Zhechev and Way, 2008).
</bodyText>
<sectionHeader confidence="0.983619" genericHeader="method">
3 Key concepts
</sectionHeader>
<bodyText confidence="0.997970166666667">
Among the key concepts in tree mining is the dif-
ference between bottom-up subtrees, induced sub-
trees and embedded subtrees. A bottom-up subtree
T&apos; of a tree T is a subtree where, for every node
in T&apos;, if its corresponding node in T has children,
then all those children are also in T&apos;. An induced
</bodyText>
<figure confidence="0.994019">
(a)
(b) (c) (d)
</figure>
<page confidence="0.956327">
85
</page>
<figureCaption confidence="0.890918">
Figure 3: “...between the European Commission and the government of the [German] Federal Repub-
</figureCaption>
<bodyText confidence="0.989048357142857">
lic...” This structure is a subtree of one of the sentences in the Alpino corpus of Dutch where a node has
two children with the same labels - two NPs. This often occurs with conjunctions and can prevent the
algorithm from discovering some frequent subtrees.
subtree T0 of T is a subtree where every node in
T0 is either the root of T0 or its parent in T is also
its parent in T�. An embedded subtree T0 of T is
a subtree where every node in T0 is either the root
of T0 or its parent in T0 is one of its ancestors in
T. See Figure 2 for an example of these different
types of subtrees.
Linear time solutions exist for finding all fre-
quent bottom-up subtrees in a treebank because
this problem can be transformed into finding all
frequent substrings in a string, a problem for
which fast solutions are well known (Luccio et al.,
2001; Luccio et al., 2004).
Solutions for induced and embedded subtrees
draw heavily on Zaki (2002) (the TreeMiner al-
gorithm) and Asai et al. (2002) (the FREQT al-
gorithm), both of whom propose Apriori-style ap-
proaches. This type of solution has the general
property that runtime is proportionate to the size
of the output: the sum of the number of times
each frequent subtree appears in the treebank. This
is not readily predictable, because the number
and frequencies of subtrees is not formally de-
terminable from the size of the treebank and can
grow very rapidly.
</bodyText>
<subsectionHeader confidence="0.998497">
3.1 Ordered and unordered trees
</subsectionHeader>
<bodyText confidence="0.999986846153846">
TreeMiner/FREQT approaches require all trees to
be ordered so that the nodes of any frequent sub-
tree will always appear in the same order every
time it appears. The children of each non-leaf
node are sorted into a lexicographic order, but
this only guarantees that frequent subtrees will al-
ways appear with the same ordering if no node
has more than one non-leaf child node with the
same label. This is not uniformly true of natural
language parse trees, as shown in Figure 3. So-
lutions exist that remove this limitation - notably
Chi et al. (2003) - but they come at a significantly
increased processing cost.
</bodyText>
<subsectionHeader confidence="0.999318">
3.2 Closed trees
</subsectionHeader>
<bodyText confidence="0.999893625">
Given that this type of approach to subtree discov-
ery has runtime bounds proportionate to the unpre-
dictable size of the output, one way to keep subtree
discovery within manageable bounds is to restrict
the output. Many of the frequent trees present in
treebanks are redundant, since they are identically
distributed with other, larger trees, as in Figure 4.
If a corpus has a sequence of tokens ABCDE
that appears f times, then that corpus also con-
tains at least f instances of the sequences A, B, C,
D, E, AB, BC, CD, DE, ABC, BCD, CDE,
ABCD, and BCDE. If any of these sequences
appears only in the context of ABCDE, then they
are redundant, because they have the same count
and distribution as the longer sequence ABCDE.
If a set of sequences is identically distributed -
appearing in all the same places - then the longest
of those sequences is called a closed sequence. In
more formal terms, a sequence 5 that appears f
times in a corpus is called closed if and only if
there is no prefix or suffix a such that a5 or 5a
also appears f times in the corpus. This definition
extends easily to trees: A subtree T in a treebank
is closed if and only if there is no node that can be
added to it to produce a new subtree T0 such that
the frequency of T0 is equal to the frequency of T.
All subtrees in a corpus are either closed subtrees
or are subtrees of closed subtrees that appear in
exactly the same places in the treebank. The set of
closed subtrees in a treebank is the smallest set of
subtrees that encompasses all the distributions of
subtrees in the treebank. Any subtree that is not in
the list of closed subtrees is either a subtree of one
of the closed subtrees that appears exactly as often
and in all the same places, or does not appear in
the treebank at all.
There are algorithms that extract only
closed subtrees from treebanks - notably
Chi et al. (2005a) - and thereby increase
their speed dramatically without producing less
</bodyText>
<page confidence="0.981283">
86
</page>
<figureCaption confidence="0.805825666666667">
(a) The common subtree
of the two parse trees in
Figure 1: “naar huis gaan”
(b) Redundant subtrees of tree (a). There
are many more such structures.
Figure 4: Closed and non-closed subtrees in just
</figureCaption>
<bodyText confidence="0.8924724">
the two sentences in Figure 1. In a larger treebank,
some of these might not be redundant.
information, since any non-closed subtree present
in the treebank is a subtree of a closed one and
shares its distribution.
</bodyText>
<sectionHeader confidence="0.934995" genericHeader="method">
4 Algorithm and data structures
</sectionHeader>
<bodyText confidence="0.99987325">
The algorithm used in this research is an extension
of the TreeMiner algorithm (Zaki, 2002), modified
to extract only closed subtrees. It takes a minimum
frequency threshold as a parameter and extracts
only those subtrees which are closed and whose
frequency is at least equal to the threshold. This
algorithm suffers from the same shortcoming of
Zaki’s original algorithm in that it is only guar-
anteed to find all frequent subtrees among ordered
trees where no node has two non-leaf children with
the same label.
It has one novel property which it appears not
to share with any other subtree extraction scheme
to date: This algorithm outputs subtrees in order
from the most frequent to the least. Given the
difficulty of predicting in advance how large the
output will be, and the large size of many natural
language data sources, this can be a real boon. If
output size or memory usage grow too large, or too
much time has passed, the program can be stopped
while still guaranteeing that it has not missed any
more frequent subtree than the last one outputted.
This section can only very briefly describe the
algorithm.
</bodyText>
<subsectionHeader confidence="0.987784">
4.1 Definitions
</subsectionHeader>
<bodyText confidence="0.99998919047619">
A treebank is any collection of trees where each
node bears a label and each node is uniquely ad-
dressable in such a way that the address an of a
node n is always greater than the address ap of its
parent p. This is accomplished by representing all
trees as ordered depth-first canonical strings. (See
Chi et al. (2005b).)
Each appearance of a subtree within a treebank
is characterized by the address of its root in the
treebank and the address of its rightmost node.
This data structure will be called a Hit. The list
of all Hits corresponding to all the appearances
of some subtree in the treebank will be called a
HitList. So, for each subtree there is a correspond-
ing HitList and vice-versa. HitLists are always
constructed in sequential order, from first instance
in the treebank to last, and can never contain du-
plicates.
We will define the function queueKey on
HitLists to output an array of four numbers in a
specific order, given a HitList as input:
</bodyText>
<listItem confidence="0.98624975">
1. The number of Hits in the HitList.
2. The distance from the address of the root of
the first Hit to the end of the treebank.
3. The distance from the address of the right-
most node of the first Hit to the end of the
treebank.
4. The number of nodes in the subtree associ-
ated with that HitList.
</listItem>
<bodyText confidence="0.83390775">
These keys are sortable and designed to ensure
that HitLists from a single treebank can always be
sorted into a fixed order such that, for two HitLists
A and B, if A &gt; B then:
</bodyText>
<listItem confidence="0.936921166666667">
1. A has more Hits than B.
2. If A has the same number of Hits as B, then
the root of the first Hit in A precedes the root
of the first Hit in B.
3. If A’s first root is identical to B’s, then the ad-
dress of the rightmost node of the first Hit in
</listItem>
<bodyText confidence="0.867019833333333">
A precedes the address of the rightmost node
of the first Hit in B.
4. If the first Hit in A is exactly the same the first
Hit in B, then the subtree associated with A
has more nodes than the subtree associated
with B.
</bodyText>
<page confidence="0.995147">
87
</page>
<bodyText confidence="0.999870888888889">
A self-sorting queue is any data structure that
stores key-data pairs and stores the keys in order
from greatest to least. The data structure used to
implement a self-sorting queue in this research is
an AVL tree (Adelson-Velskii and Landis, 1962),
however, other structures could equally well have
been used. The self-sorting queue will be used to
maintain a sorted list of HitLists, sorted in the or-
der of their queueKeys as described above.
</bodyText>
<subsectionHeader confidence="0.908061">
4.2 Initialization
</subsectionHeader>
<bodyText confidence="0.999904461538461">
Fix a minimum frequency threshold t for the sub-
trees you wish to extract from the treebank. Start
processing by initializing one HitList for each
unique label in the treebank with the set of Hits
that corresponds to each occurrence of that label.
We will treat each as a HitList with an associ-
ated subtree containing only one node. This set
is constructed in linear time by iterating over all
the nodes in the treebank.
Of the initial HitLists, throw away all those with
fewer than threshold frequency t Hits in them.
The remaining HitLists are inserted into the self-
sorting queue.
</bodyText>
<subsectionHeader confidence="0.815946">
4.3 Extracting induced subtrees without
checking for closure
</subsectionHeader>
<bodyText confidence="0.951896333333333">
Extracting all the subtrees above a fixed frequency
- not just the closed subtrees - in order from the
most frequent to the least, proceeds as follows:
</bodyText>
<listItem confidence="0.998887066666667">
1. Initialize as described in Section 4.2.
2. Pop the top HitList hl and its associated sub-
tree s from the queue.
3. Extend hl:
(a) Visit each Hit in hl and find all the
nodes that can be added to the right side
of s to produce new induced subtrees.
(b) Generate new HitLists for all subtrees
that extend s by one node to the right.
(c) Test each new HitList to make sure it
appears more than threshold frequency
t times, and if it does, insert it into the
queue.
(d) Output s and hl.
4. Repeat until the queue is empty.
</listItem>
<bodyText confidence="0.9987012">
This is essentially identical to the TreeMiner
and FREQT algorithms already published by
Zaki (2002) and by Asai et al. (2002), except that
it outputs frequent subtrees in order from the most
frequent to the least.
</bodyText>
<subsectionHeader confidence="0.998984">
4.4 Extracting only closed induced subtrees
</subsectionHeader>
<bodyText confidence="0.999620137931035">
By controlling the order in which HitLists reach
the top of the queue, it is possible to efficiently
prevent any subtree which is not a closed sub-
tree or a prefix of a closed subtree from being
extended, and to prevent any subtree that is not
closed from being outputted.
Every subtree with a frequency of f is either
a closed subtree, a prefix of a closed subtree that
also has a frequency of f and can be constructed
by adding more nodes to the right, or is a redun-
dant non-closed subtree that need not be extended
or stored. Consider a redundant, non-closed sub-
tree x and a closed subtree or prefix of a closed
subtree y which has the same frequency, and has
the same set of addresses for the rightmost node of
each of its appearances in the treebank. The sort
order of the self-sorting queue (see Section 4.1)
ensures that if a prefix of a closed subtree y is in
the queue and some subtree of it x is also in the
queue, then y is closer to the top of the queue than
x is. Furthermore, it can be proven that the pre-
fix of a closed subtree with the same distribution
as any non-closed, redundant subtree will be gen-
erated, inserted into the queue, and removed from
the top of the queue before x can reach the top.
So, to prevent x from being extended or stored,
all that is necessary is to check to see there is some
closed subtree or prefix of a closed subtree y such
that:
</bodyText>
<listItem confidence="0.999601166666667">
• y has already been at the top of the queue.
• y has the same frequency as x.
• The set of rightmost nodes of every Hit in
y’s HitList is identical to the set of rightmost
nodes of every Hit in x’s HitList.
• x is a subtree of y
</listItem>
<bodyText confidence="0.999794357142857">
This can be checked by constructing a hash
value for each HitList based on its frequency and
some subset of the set of rightmost nodes of ev-
ery Hit. In our experiments, we used only the first
node of each HitList. If x’s hash value matches
some previously processed y’s hash value, then
check if x is a subtree of y and reject it if it is.
The result is to only instantiate closed subtrees and
their prefixes, and subtrees which are one node ex-
tensions of closed subtrees and their prefixes.
Like TreeMiner, worst case space and time
bounds are proportionate to the number of sub-
trees instantiated and the number of times each
appears in the corpus. This is smaller than the
</bodyText>
<page confidence="0.997564">
88
</page>
<bodyText confidence="0.999882125">
worst case bounds for TreeMiner because it does
not instantiate all frequent subtrees. There is addi-
tional approximately constant time processing for
each instantiated subtree to check for closure and
to store it in the self-sorting queue. At the lowest
frequency thresholds, this can take up the major-
ity of runtime, but is generally negligible at high
frequencies.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999930866666667">
We applied this algorithm to a parsed and hand-
corrected 7137 sentence subset of the Alpino Tree-
bank of Dutch.1 The average sentence length in
this small treebank is roughly 20 words, and the
corresponding trees have an average of approx-
imately 32 nodes for a total of 230,673 nodes.
With the minimum frequency set to 2, this algo-
rithm extracted 342,401 closed subtrees in about
2000 seconds on a conventional workstation run-
ning Linux2. The same implementation but with-
out testing for closure - which makes this algo-
rithm equivalent to TreeMiner - extracted some 4.2
million trees in roughly 11,000 seconds. Closed
tree extraction contrasts quite favorably to extrac-
tion without closure, even over a small dataset.
</bodyText>
<table confidence="0.849356066666667">
Min. Freq. Subtrees extracted Runtime
Threshold
2 342401 1952.33s
3 243484 1004.30s
4 176885 588.58s
5 134495 402.26s
8 72732 209.51s
10 53842 163.22s
15 30681 112.39s
20 20610 85.24s
30 11516 66.05s
40 7620 54.14s
50 5549 47.98s
60 4219 43.24s
70 3365 39.97s
</table>
<tableCaption confidence="0.988076">
Table 1: Runtime and closed trees extracted at dif-
</tableCaption>
<bodyText confidence="0.60870425">
ferent minimum frequency thresholds, using the
7137 sentence sample of the Alpino Treebank.
Runtime and the number of trees produced fall
very dramatically as thresholds rise - so much so
</bodyText>
<footnote confidence="0.99149725">
1http://www.let.rug.nl/vannoord/trees/
2A Dell Precision 490 workstation with an Intel Dual-
Core Xeon processor and 8GB of memory. The algorithm
was not implemented to use two processors.
</footnote>
<table confidence="0.999843857142857">
Sentences Total Subtrees Runtime
nodes extracted
2500 94528 37607 61.08s
5000 189170 98538 260.91s
10000 379980 264616 1495.19s
15000 573629 477750 3829.29s
20000 763502 704018 7998.57s
</table>
<tableCaption confidence="0.987213">
Table 2: Runtime and closed trees extracted from
</tableCaption>
<bodyText confidence="0.992467487804878">
automatically parsed samples of the Europarl
Dutch corpus, keeping the minimum frequency
threshold constant at 5 for all sizes of treebank.
that setting the minimum frequency to 3 instead of
2 halved the runtime. This pattern is characteristic
of a power law distribution like Zipf’s law. (See
Table 1 and Figure 5.) Given the pervasiveness
of power law distributions in word frequencies, it
should perhaps not be surprising to discover that
frequent closed subtrees in treebanks are similarly
distributed. This research may be the first effort
to empirically support such a conclusion, although
admittedly only very tentatively.
To test the impact of varying the size of the tree-
bank, but keeping the minimum frequency thresh-
old constant, we used a section of the Dutch por-
tion of the Europarl corpus (Koehn, 2005) auto-
matically parsed using the Alpino Dutch parser
(van Noord, 2006) without any manual correction.
Random samples of 2500, 5000, 10000, 15000
and 20000 sentences were selected, and all sub-
trees of frequency 5 or higher were extracted from
each, as summarized in Table 2. As treebank size
grows, the number of subtrees extracted at the
same minimum frequency threshold, and the time
and memory used extracting them, grows expo-
nentially. This is in sharp contrast to algorithms
that extract frequently recurring strings, which in-
crease linearly in time and memory usage as the
data grows.
However, if the minimum frequency threshold
is kept constant as a proportion of the size of the
treebank, then the number of trees extracted re-
mains roughly constant and the time and memory
used to extract them grows roughly linearly with
the size of the treebank. Table 3 shows the result
for different sized random samples of the parsed
Europarl corpus.
Lastly, since this algorithm has known difficul-
ties when presented with trees where more than
one non-leaf child of a node can have the same
</bodyText>
<page confidence="0.997682">
89
</page>
<figure confidence="0.813866">
(a) Runtime by minimum fre- (b) Subtrees extracted by mini- (c) Log-log plot of (b).
quency threshold. mum frequency threshold.
</figure>
<figureCaption confidence="0.985222333333333">
Figure 5: Runtime (a) and subtrees extracted (b) from the Alpino sample using different minimum fre-
quency thresholds. Figure (c) is a log-log plot of (b). Figure (c) looks close to a straight line, which is
characteristic of a power law distribution.
</figureCaption>
<table confidence="0.999728375">
Sents Total Min. Subtrees Run
nodes Freq. extracted time
Thres.
2500 99323 5 42905 72.95s
5000 194736 10 42783 122.18s
10000 382022 20 41988 216.23s
15000 574632 30 43078 325.86s
20000 770240 40 44416 435.19s
</table>
<tableCaption confidence="0.992332">
Table 3: Runtime and closed trees extracted from
</tableCaption>
<bodyText confidence="0.984213695652174">
automatically parsed samples of the Europarl
Dutch corpus, with minimum frequency thresh-
olds kept roughly constant as a proportion of the
sample size.
label (see sections 3.1 and 4), we attempted to
determine if this problem is marginal or perva-
sive. The 7137 sentence Alpino Treebank sample
contains 3833 nodes with more than one non-leaf
child node with identical labels or roughly 1.7% of
all nodes. Furthermore, these nodes are present in
2666 sentences - some 37% of all sentences! This
is a very large minority.
In order to estimate the effect this phenomenon
has on the extraction of closed trees, we looked
for outputted trees that are not closed by compar-
ing the HitLists of all outputted trees to all other
outputted trees with the same frequency. Table 4
shows the number of trees with identical distribu-
tions to other outputted trees - i.e. trees that ap-
peared to be closed to this algorithm, but in fact
are not. The number was surprisingly large, but
distributed overwhelmingly at the very lowest fre-
quencies.
</bodyText>
<table confidence="0.995529454545454">
Min. Freq. Non-closed as a % of
Threshold trees all trees extracted
2 2874 0.84%
3 693 0.28%
4 225 0.13%
5 101 0.08%
8 18 0.02%
10 11 0.02%
15 6 0.02%
20 3 0.01%
30 0 0.00%
</table>
<tableCaption confidence="0.993458">
Table 4: Non-closed trees from the 7137 sentence
</tableCaption>
<bodyText confidence="0.99472775">
sample of the Alpino Treebank, produced erro-
neously as closed trees because of repeated labels.
There were no non-closed trees extracted at fre-
quencies over 30.
</bodyText>
<sectionHeader confidence="0.996467" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999945785714286">
The algorithm presented above opens up tree-
banks and annotated corpora to much more de-
tailed quantitative analysis, and extends the tools
available for data-driven natural language process-
ing. This makes a number of new applications
possible. We are developing treebank indexing for
fast retrieval by tree similarity, in order to make
full treebanks available for example-based parsing
and machine translation in real time. This algo-
rithm also has applications in constructing concor-
dances of syntactic, morphological and semantic
structures - types of information that are not tradi-
tionally amenable to indexing. Furthermore, sta-
tistical models of natural language data can take
</bodyText>
<page confidence="0.992868">
90
</page>
<bodyText confidence="0.999990968253968">
advantage of comprehensive subtree censuses to
become fully syntax-aware, instead of relying on
bag of words and n-gram models.
However, there are a number of drawbacks and
caveats that must be highlighted.
Runtime, memory usage and output size are dif-
ficult to estimate in advance. This is mitigated in
part by the order in which subtrees are outputted,
making it possible to extract only the most fre-
quent subset of subtrees given fixed time and space
bounds. Empirically, it appears that resource re-
quirements and output size can also be estimated
by sampling, if minimum frequency thresholds
can be kept constant as a proportion of total tree-
bank size.
The formalisms used in most theories of syn-
tax allow nodes to have multiple non-leaf chil-
dren with the same labels. Although errors caused
by non-unique labels are overwhelmingly present
only among the lowest frequency subtrees, er-
rors appear often enough to pose a non-negligible
problem for this algorithm.
We are investigating the degree to which this
can be mitigated by making different choices of
linguistic formalism. Syntax trees that contain
only binary trees - for example, those constructed
using Chomsky Normal Form rules (Jurafsky and
Martin, 2009) - cannot have identically labelled
non-leaf children, but must suffer some loss of
generality in their frequent subtrees because if it.
Other theories can reduce the size of this source of
error, notably dependency syntax which often uses
fewer abstract labels (Tesni`ere, 1959; Mel’ˇcuk,
1988; Sugayama and Hudson, 2006), but will most
likely be poor sources of highly general rules as a
consequence.
Furthermore, tree mining algorithms exist that
eliminate this problem, but at some cost. We are
investigating a hybrid solution to the non-unique
label problem that identifies only those subtrees
where more resource-intensive closure checking is
necessary. This will guarantee the correct extrac-
tion of closed subtrees in all cases while minimiz-
ing the additional processing burden.
Among the open problems suggested by this re-
search is the degree to which the empirical results
obtained above are dependent on the language of
the underlying data and the linguistic formalisms
used to produce treebanks. Different linguistic
theories use different abstractions and use their ab-
stract categories differently. This has an immedi-
ate effect on the number of nodes in a treebank
and on the topology of the trees. Some theories
produce more compact trees than others. Some
produce deep trees, others produce shallow trees.
It is likely that the formalisms used in treebanks
have a pervasive influence on the number and kind
of frequent subtrees extracted. By doing quantita-
tive research on the structures found in treebanks,
it may become possible to make reliable opera-
tional choices about the linguistic formalisms used
in treebanks on the basis of the kinds of structures
one hopes to get out of them.
</bodyText>
<sectionHeader confidence="0.997391" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.97947575">
This research is supported by the AMASS++
Project3 directly funded by the Institute for the
Promotion of Innovation by Science and Technol-
ogy in Flanders (IWT) (SBO IWT 060051).
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998024066666667">
Georgiy M. Adelson-Velskii and Yevgeniy M. Landis.
1962. An algorithm for the organization of informa-
tion. Proceedings of the USSR Academy ofSciences,
146(2):263–266.
Rakesh Agrawal, Tomasz Imielinski and Arun Swami.
1993. Mining association rules between sets of
items in large databases. Proceedings of the 1993
ACM SIGMOD International Conference on Man-
agement ofData, 207–216.
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hi-
roki Arimura, Hiroshi Sakamoto and Set-
suo Arikawa. 2002. Efficient substructure
discovery from large semi-structured data. Proceed-
ings of the Second SIAM International Conference
on Data Mining, 158–174.
Rens Bod, Khalil Sima’an and Remko Scha, editors.
2003. Data-Oriented Parsing. CLSI Publicatons,
Stanford, CA.
Yun Chi, Yirong Yang and Richard R. Muntz. 2003.
Indexing and Mining Free Trees. UCLA Computer
Science Department Technical Report No. 030041.
Yun Chi, Richard R. Muntz, Siegfried Nijssen and
Joost N. Kok. 2004. Frequent Subtree Mining – An
Overview. Fundamenta Informaticae, 66(1-2):161–
198.
Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz.
2005a. Mining Closed and Maximal Frequent
Subtrees from Databases of Labeled Rooted Trees.
IEEE Transactions on Knowledge and Data Engi-
neering, 17(2):190–202.
</reference>
<footnote confidence="0.735723">
3http://www.cs.kuleuven.be/˜liir/
projects/amass/
</footnote>
<page confidence="0.994163">
91
</page>
<reference confidence="0.99964424">
Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz.
2005b. Canonical forms for labelled trees and
their applications in frequent subtree mining. IEEE
Transactions on Knowledge and Data Engineering,
8(2):203–234.
Hany Hassan, Mary Hearne, Khalil Sima’an and
Andy Way. 2006. Syntactic Phrase-Based Statisti-
cal Machine Translation. Proceedings of the IEEE
2006 Workshop on Spoken Language Translation,
238–241.
Mary Hearne and Andy Way. 2003. Seeing the Wood
for the Trees: Data-Oriented Translation. Proceed-
ings of the 9th Machine Translation Summit, 165–
172.
Daniel Jurafsky and James H. Martin. 2009. Speech
and Language Processing. Pearson Prentice Hall,
Upper Saddle River, NJ.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th Machine Translation Summit, 79–86.
Sotiris Kotsiantis and Dimitris Kanellopoulos. 2006.
Association Rules Mining: A Recent Overview.
GESTS International Transactions on Computer
Science and Engineering, 32(1):71–82.
Fabrizio Luccio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2001. Exact Rooted Subtree
Matching in Sublinear Time. Universit`a Di Pisa
Technical Report TR-01-14.
Fabrizio Luccio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2004. Bottom-up subtree isomor-
phism for unordered labeled trees. Universit`a Di
Pisa Technical Report TR-04-13.
Edward M. McCreight. 1976. A Space-Economical
Suffix Tree Construction Algorithm. Journal of the
Association for Computing Machinery, 23(2):262–
272.
Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press,
Albany, NY.
Arjen Poutsma. 2000. Data-Oriented Translation.
Proc. of the 18th International Conference on Com-
putational Linguistics (COLING 2000), 635–641.
Arjen Poutsma. 2003. Machine Translation with Tree-
DOP. Data-Oriented Parsing, 63–81.
Deepak Ravichandran and Eduard Hovy 2002. Learn-
ing surface text patterns for a question answering
system. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics
(ACL-02), 41–47.
Kensei Sugayama and Richard Hudson, editors. 2006.
Word Grammar: New Perspectives on a Theory of
Language Structure. Continuum International Pub-
lishing, London.
Lucien Tesni`ere. 1959. El´ements de la syntaxe struc-
turale. Editions Klincksieck, Paris.
John Tinsley, Mary Hearne and Andy Way. 2007.
Exploiting Parallel Treebanks for use in Statisti-
cal Machine Translation. Proceedings of Treebanks
and Linguistic Theories (TLT ’07), Bergen, Norway
175–187.
Gertjan van Noord. 2006. At last parsing is now
operational. Verbum Ex Machina. Actes de la
13e conf´erence sur le traitement automatique des
langues naturelles (TALN6), 20–42.
Peter Weiner. 1973 Linear pattern matching algorithm.
14th Annual IEEE Symposium on Switching and Au-
tomata Theory, 1–11.
Mohammed J. Zaki. 2002. Efficiently mining fre-
quent trees in a forest. Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, 1021–1035.
Ventsislav Zhechev and Andy Way. 2008. Auto-
matic Generation of Parallel Treebanks. Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (COLING 2008), 1105–1112.
</reference>
<page confidence="0.9959">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.723075">
<title confidence="0.989373">Quantitative analysis of treebanks using frequent subtree mining methods</title>
<author confidence="0.90073">Scott</author>
<affiliation confidence="0.770503">Centrum voor Computerlinguistiek, KU</affiliation>
<address confidence="0.9241655">Blijde-Inkomststraat 13, bus 3000 Leuven</address>
<email confidence="0.984678">scott@ccl.kuleuven.be</email>
<abstract confidence="0.999828666666667">The first task of statistical computational linguistics, or any other type of datadriven processing of language, is the extraction of counts and distributions of phenomena. This is much more difficult for the type of complex structured data found in treebanks and in corpora with sophisticated annotation than for tokenized texts. Recent developments in data mining, particularly in the extraction of frequent subtrees from treebanks, offer some solutions. We have applied a modified version of the to a small treebank and present some promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Georgiy M Adelson-Velskii</author>
<author>Yevgeniy M Landis</author>
</authors>
<title>An algorithm for the organization of information.</title>
<date>1962</date>
<booktitle>Proceedings of the USSR Academy ofSciences,</booktitle>
<pages>146--2</pages>
<contexts>
<context position="14546" citStr="Adelson-Velskii and Landis, 1962" startWordPosition="2506" endWordPosition="2509">of the first Hit in A precedes the root of the first Hit in B. 3. If A’s first root is identical to B’s, then the address of the rightmost node of the first Hit in A precedes the address of the rightmost node of the first Hit in B. 4. If the first Hit in A is exactly the same the first Hit in B, then the subtree associated with A has more nodes than the subtree associated with B. 87 A self-sorting queue is any data structure that stores key-data pairs and stores the keys in order from greatest to least. The data structure used to implement a self-sorting queue in this research is an AVL tree (Adelson-Velskii and Landis, 1962), however, other structures could equally well have been used. The self-sorting queue will be used to maintain a sorted list of HitLists, sorted in the order of their queueKeys as described above. 4.2 Initialization Fix a minimum frequency threshold t for the subtrees you wish to extract from the treebank. Start processing by initializing one HitList for each unique label in the treebank with the set of Hits that corresponds to each occurrence of that label. We will treat each as a HitList with an associated subtree containing only one node. This set is constructed in linear time by iterating </context>
</contexts>
<marker>Adelson-Velskii, Landis, 1962</marker>
<rawString>Georgiy M. Adelson-Velskii and Yevgeniy M. Landis. 1962. An algorithm for the organization of information. Proceedings of the USSR Academy ofSciences, 146(2):263–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rakesh Agrawal</author>
</authors>
<title>Tomasz Imielinski and Arun Swami.</title>
<date>1993</date>
<booktitle>Proceedings of the 1993 ACM SIGMOD International Conference on Management ofData,</booktitle>
<pages>207--216</pages>
<marker>Agrawal, 1993</marker>
<rawString>Rakesh Agrawal, Tomasz Imielinski and Arun Swami. 1993. Mining association rules between sets of items in large databases. Proceedings of the 1993 ACM SIGMOD International Conference on Management ofData, 207–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsuya Asai</author>
</authors>
<title>Kenji Abe, Shinji Kawasoe, Hiroki Arimura, Hiroshi Sakamoto and Setsuo Arikawa.</title>
<date>2002</date>
<booktitle>Proceedings of the Second SIAM International Conference on Data Mining,</booktitle>
<pages>158--174</pages>
<marker>Asai, 2002</marker>
<rawString>Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroki Arimura, Hiroshi Sakamoto and Setsuo Arikawa. 2002. Efficient substructure discovery from large semi-structured data. Proceedings of the Second SIAM International Conference on Data Mining, 158–174.</rawString>
</citation>
<citation valid="false">
<editor>Rens Bod, Khalil Sima’an and Remko Scha, editors. 2003. Data-Oriented Parsing. CLSI Publicatons,</editor>
<location>Stanford, CA.</location>
<marker></marker>
<rawString>Rens Bod, Khalil Sima’an and Remko Scha, editors. 2003. Data-Oriented Parsing. CLSI Publicatons, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Chi</author>
<author>Yirong Yang</author>
<author>Richard R Muntz</author>
</authors>
<title>Indexing and Mining Free Trees.</title>
<date>2003</date>
<tech>Technical Report No. 030041.</tech>
<institution>UCLA Computer Science Department</institution>
<contexts>
<context position="5101" citStr="Chi et al., 2003" startWordPosition="804" endWordPosition="807">ns that are motivated by linguistic theories, efficient methods for extracting frequent subtrees from treebanks are therefore potentially very valuable to corpus and computational linguistics. 2 Previous and Related Work Tree mining research is a subset of graph mining focused specifically on rooted, directed acyclic graphs. Although there is research into extracting frequent subtrees from free (unrooted and undirected) trees, free tree mining usually proceeds by deciding which node in any particular tree will be treated as a root, and then treating it as if it was a rooted and directed tree (Chi et al., 2003). Figure 2: Tree (a) and different types of subtree: (b) a bottom-up subtree of (a), (c) an induced subtree of (a), and (d) an embedded subtree of (a). Research on frequent subtree discovery generally draws heavily on early work by Zaki (2002) and Asai et al. (2002) who roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be f</context>
<context position="8790" citStr="Chi et al. (2003)" startWordPosition="1449" endWordPosition="1452"> treebank and can grow very rapidly. 3.1 Ordered and unordered trees TreeMiner/FREQT approaches require all trees to be ordered so that the nodes of any frequent subtree will always appear in the same order every time it appears. The children of each non-leaf node are sorted into a lexicographic order, but this only guarantees that frequent subtrees will always appear with the same ordering if no node has more than one non-leaf child node with the same label. This is not uniformly true of natural language parse trees, as shown in Figure 3. Solutions exist that remove this limitation - notably Chi et al. (2003) - but they come at a significantly increased processing cost. 3.2 Closed trees Given that this type of approach to subtree discovery has runtime bounds proportionate to the unpredictable size of the output, one way to keep subtree discovery within manageable bounds is to restrict the output. Many of the frequent trees present in treebanks are redundant, since they are identically distributed with other, larger trees, as in Figure 4. If a corpus has a sequence of tokens ABCDE that appears f times, then that corpus also contains at least f instances of the sequences A, B, C, D, E, AB, BC, CD, D</context>
</contexts>
<marker>Chi, Yang, Muntz, 2003</marker>
<rawString>Yun Chi, Yirong Yang and Richard R. Muntz. 2003. Indexing and Mining Free Trees. UCLA Computer Science Department Technical Report No. 030041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Chi</author>
<author>Richard R Muntz</author>
<author>Siegfried Nijssen</author>
<author>Joost N Kok</author>
</authors>
<title>Frequent Subtree Mining – An Overview. Fundamenta Informaticae,</title>
<date>2004</date>
<pages>66--1</pages>
<contexts>
<context position="5726" citStr="Chi et al. (2004)" startWordPosition="912" endWordPosition="915"> 2: Tree (a) and different types of subtree: (b) a bottom-up subtree of (a), (c) an induced subtree of (a), and (d) an embedded subtree of (a). Research on frequent subtree discovery generally draws heavily on early work by Zaki (2002) and Asai et al. (2002) who roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as an aid in the development of statistical and example-based machine translations systems (Has</context>
</contexts>
<marker>Chi, Muntz, Nijssen, Kok, 2004</marker>
<rawString>Yun Chi, Richard R. Muntz, Siegfried Nijssen and Joost N. Kok. 2004. Frequent Subtree Mining – An Overview. Fundamenta Informaticae, 66(1-2):161– 198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Chi</author>
<author>Yi Xia</author>
<author>Yirong Yang</author>
<author>Richard R Muntz</author>
</authors>
<title>Mining Closed and Maximal Frequent Subtrees from Databases of Labeled Rooted Trees.</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="10739" citStr="Chi et al. (2005" startWordPosition="1811" endWordPosition="1814"> equal to the frequency of T. All subtrees in a corpus are either closed subtrees or are subtrees of closed subtrees that appear in exactly the same places in the treebank. The set of closed subtrees in a treebank is the smallest set of subtrees that encompasses all the distributions of subtrees in the treebank. Any subtree that is not in the list of closed subtrees is either a subtree of one of the closed subtrees that appears exactly as often and in all the same places, or does not appear in the treebank at all. There are algorithms that extract only closed subtrees from treebanks - notably Chi et al. (2005a) - and thereby increase their speed dramatically without producing less 86 (a) The common subtree of the two parse trees in Figure 1: “naar huis gaan” (b) Redundant subtrees of tree (a). There are many more such structures. Figure 4: Closed and non-closed subtrees in just the two sentences in Figure 1. In a larger treebank, some of these might not be redundant. information, since any non-closed subtree present in the treebank is a subtree of a closed one and shares its distribution. 4 Algorithm and data structures The algorithm used in this research is an extension of the TreeMiner algorithm</context>
<context position="12716" citStr="Chi et al. (2005" startWordPosition="2153" endWordPosition="2156">n be a real boon. If output size or memory usage grow too large, or too much time has passed, the program can be stopped while still guaranteeing that it has not missed any more frequent subtree than the last one outputted. This section can only very briefly describe the algorithm. 4.1 Definitions A treebank is any collection of trees where each node bears a label and each node is uniquely addressable in such a way that the address an of a node n is always greater than the address ap of its parent p. This is accomplished by representing all trees as ordered depth-first canonical strings. (See Chi et al. (2005b).) Each appearance of a subtree within a treebank is characterized by the address of its root in the treebank and the address of its rightmost node. This data structure will be called a Hit. The list of all Hits corresponding to all the appearances of some subtree in the treebank will be called a HitList. So, for each subtree there is a corresponding HitList and vice-versa. HitLists are always constructed in sequential order, from first instance in the treebank to last, and can never contain duplicates. We will define the function queueKey on HitLists to output an array of four numbers in a </context>
</contexts>
<marker>Chi, Xia, Yang, Muntz, 2005</marker>
<rawString>Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz. 2005a. Mining Closed and Maximal Frequent Subtrees from Databases of Labeled Rooted Trees. IEEE Transactions on Knowledge and Data Engineering, 17(2):190–202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun Chi</author>
<author>Yi Xia</author>
<author>Yirong Yang</author>
<author>Richard R Muntz</author>
</authors>
<title>Canonical forms for labelled trees and their applications in frequent subtree mining.</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>8</volume>
<issue>2</issue>
<contexts>
<context position="10739" citStr="Chi et al. (2005" startWordPosition="1811" endWordPosition="1814"> equal to the frequency of T. All subtrees in a corpus are either closed subtrees or are subtrees of closed subtrees that appear in exactly the same places in the treebank. The set of closed subtrees in a treebank is the smallest set of subtrees that encompasses all the distributions of subtrees in the treebank. Any subtree that is not in the list of closed subtrees is either a subtree of one of the closed subtrees that appears exactly as often and in all the same places, or does not appear in the treebank at all. There are algorithms that extract only closed subtrees from treebanks - notably Chi et al. (2005a) - and thereby increase their speed dramatically without producing less 86 (a) The common subtree of the two parse trees in Figure 1: “naar huis gaan” (b) Redundant subtrees of tree (a). There are many more such structures. Figure 4: Closed and non-closed subtrees in just the two sentences in Figure 1. In a larger treebank, some of these might not be redundant. information, since any non-closed subtree present in the treebank is a subtree of a closed one and shares its distribution. 4 Algorithm and data structures The algorithm used in this research is an extension of the TreeMiner algorithm</context>
<context position="12716" citStr="Chi et al. (2005" startWordPosition="2153" endWordPosition="2156">n be a real boon. If output size or memory usage grow too large, or too much time has passed, the program can be stopped while still guaranteeing that it has not missed any more frequent subtree than the last one outputted. This section can only very briefly describe the algorithm. 4.1 Definitions A treebank is any collection of trees where each node bears a label and each node is uniquely addressable in such a way that the address an of a node n is always greater than the address ap of its parent p. This is accomplished by representing all trees as ordered depth-first canonical strings. (See Chi et al. (2005b).) Each appearance of a subtree within a treebank is characterized by the address of its root in the treebank and the address of its rightmost node. This data structure will be called a Hit. The list of all Hits corresponding to all the appearances of some subtree in the treebank will be called a HitList. So, for each subtree there is a corresponding HitList and vice-versa. HitLists are always constructed in sequential order, from first instance in the treebank to last, and can never contain duplicates. We will define the function queueKey on HitLists to output an array of four numbers in a </context>
</contexts>
<marker>Chi, Xia, Yang, Muntz, 2005</marker>
<rawString>Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz. 2005b. Canonical forms for labelled trees and their applications in frequent subtree mining. IEEE Transactions on Knowledge and Data Engineering, 8(2):203–234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Mary Hearne</author>
<author>Khalil Sima’an</author>
<author>Andy Way</author>
</authors>
<title>Syntactic Phrase-Based Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>Proceedings of the IEEE 2006 Workshop on Spoken Language Translation,</booktitle>
<pages>238--241</pages>
<marker>Hassan, Hearne, Sima’an, Way, 2006</marker>
<rawString>Hany Hassan, Mary Hearne, Khalil Sima’an and Andy Way. 2006. Syntactic Phrase-Based Statistical Machine Translation. Proceedings of the IEEE 2006 Workshop on Spoken Language Translation, 238–241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Seeing the Wood for the Trees: Data-Oriented Translation.</title>
<date>2003</date>
<booktitle>Proceedings of the 9th Machine Translation Summit,</booktitle>
<volume>165</volume>
<pages>172</pages>
<contexts>
<context position="6006" citStr="Hearne and Way, 2003" startWordPosition="954" endWordPosition="957">taneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as an aid in the development of statistical and example-based machine translations systems (Hassan et al., 2006; Tinsley et al., 2007; Zhechev and Way, 2008). 3 Key concepts Among the key concepts in tree mining is the difference between bottom-up subtrees, induced subtrees and embedded subtrees. A bottom-up subtree T&apos; of a tree T is a subtree where, for every node in T&apos;, </context>
</contexts>
<marker>Hearne, Way, 2003</marker>
<rawString>Mary Hearne and Andy Way. 2003. Seeing the Wood for the Trees: Data-Oriented Translation. Proceedings of the 9th Machine Translation Summit, 165– 172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2009</date>
<publisher>Pearson Prentice Hall,</publisher>
<location>Upper Saddle River, NJ.</location>
<contexts>
<context position="26497" citStr="Jurafsky and Martin, 2009" startWordPosition="4559" endWordPosition="4562">lds can be kept constant as a proportion of total treebank size. The formalisms used in most theories of syntax allow nodes to have multiple non-leaf children with the same labels. Although errors caused by non-unique labels are overwhelmingly present only among the lowest frequency subtrees, errors appear often enough to pose a non-negligible problem for this algorithm. We are investigating the degree to which this can be mitigated by making different choices of linguistic formalism. Syntax trees that contain only binary trees - for example, those constructed using Chomsky Normal Form rules (Jurafsky and Martin, 2009) - cannot have identically labelled non-leaf children, but must suffer some loss of generality in their frequent subtrees because if it. Other theories can reduce the size of this source of error, notably dependency syntax which often uses fewer abstract labels (Tesni`ere, 1959; Mel’ˇcuk, 1988; Sugayama and Hudson, 2006), but will most likely be poor sources of highly general rules as a consequence. Furthermore, tree mining algorithms exist that eliminate this problem, but at some cost. We are investigating a hybrid solution to the non-unique label problem that identifies only those subtrees w</context>
</contexts>
<marker>Jurafsky, Martin, 2009</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2009. Speech and Language Processing. Pearson Prentice Hall, Upper Saddle River, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="21449" citStr="Koehn, 2005" startWordPosition="3724" endWordPosition="3725"> runtime. This pattern is characteristic of a power law distribution like Zipf’s law. (See Table 1 and Figure 5.) Given the pervasiveness of power law distributions in word frequencies, it should perhaps not be surprising to discover that frequent closed subtrees in treebanks are similarly distributed. This research may be the first effort to empirically support such a conclusion, although admittedly only very tentatively. To test the impact of varying the size of the treebank, but keeping the minimum frequency threshold constant, we used a section of the Dutch portion of the Europarl corpus (Koehn, 2005) automatically parsed using the Alpino Dutch parser (van Noord, 2006) without any manual correction. Random samples of 2500, 5000, 10000, 15000 and 20000 sentences were selected, and all subtrees of frequency 5 or higher were extracted from each, as summarized in Table 2. As treebank size grows, the number of subtrees extracted at the same minimum frequency threshold, and the time and memory used extracting them, grows exponentially. This is in sharp contrast to algorithms that extract frequently recurring strings, which increase linearly in time and memory usage as the data grows. However, if</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. Proceedings of the 10th Machine Translation Summit, 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sotiris Kotsiantis</author>
<author>Dimitris Kanellopoulos</author>
</authors>
<title>Association Rules Mining: A Recent Overview.</title>
<date>2006</date>
<journal>GESTS International Transactions on Computer Science and Engineering,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="5633" citStr="Kotsiantis and Kanellopoulos (2006)" startWordPosition="894" endWordPosition="897">l be treated as a root, and then treating it as if it was a rooted and directed tree (Chi et al., 2003). Figure 2: Tree (a) and different types of subtree: (b) a bottom-up subtree of (a), (c) an induced subtree of (a), and (d) an embedded subtree of (a). Research on frequent subtree discovery generally draws heavily on early work by Zaki (2002) and Asai et al. (2002) who roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as</context>
</contexts>
<marker>Kotsiantis, Kanellopoulos, 2006</marker>
<rawString>Sotiris Kotsiantis and Dimitris Kanellopoulos. 2006. Association Rules Mining: A Recent Overview. GESTS International Transactions on Computer Science and Engineering, 32(1):71–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Luccio</author>
<author>Antonio Enriquez</author>
<author>Pablo Rieumont</author>
<author>Linda Pagli</author>
</authors>
<title>Exact Rooted Subtree Matching in Sublinear Time. Universit`a Di Pisa</title>
<date>2001</date>
<tech>Technical Report TR-01-14.</tech>
<contexts>
<context position="7647" citStr="Luccio et al., 2001" startWordPosition="1252" endWordPosition="1255">ithm from discovering some frequent subtrees. subtree T0 of T is a subtree where every node in T0 is either the root of T0 or its parent in T is also its parent in T�. An embedded subtree T0 of T is a subtree where every node in T0 is either the root of T0 or its parent in T0 is one of its ancestors in T. See Figure 2 for an example of these different types of subtrees. Linear time solutions exist for finding all frequent bottom-up subtrees in a treebank because this problem can be transformed into finding all frequent substrings in a string, a problem for which fast solutions are well known (Luccio et al., 2001; Luccio et al., 2004). Solutions for induced and embedded subtrees draw heavily on Zaki (2002) (the TreeMiner algorithm) and Asai et al. (2002) (the FREQT algorithm), both of whom propose Apriori-style approaches. This type of solution has the general property that runtime is proportionate to the size of the output: the sum of the number of times each frequent subtree appears in the treebank. This is not readily predictable, because the number and frequencies of subtrees is not formally determinable from the size of the treebank and can grow very rapidly. 3.1 Ordered and unordered trees TreeM</context>
</contexts>
<marker>Luccio, Enriquez, Rieumont, Pagli, 2001</marker>
<rawString>Fabrizio Luccio, Antonio Enriquez, Pablo Rieumont and Linda Pagli. 2001. Exact Rooted Subtree Matching in Sublinear Time. Universit`a Di Pisa Technical Report TR-01-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Luccio</author>
<author>Antonio Enriquez</author>
<author>Pablo Rieumont</author>
<author>Linda Pagli</author>
</authors>
<title>Bottom-up subtree isomorphism for unordered labeled trees. Universit`a Di Pisa</title>
<date>2004</date>
<tech>Technical Report TR-04-13.</tech>
<contexts>
<context position="7669" citStr="Luccio et al., 2004" startWordPosition="1256" endWordPosition="1259"> some frequent subtrees. subtree T0 of T is a subtree where every node in T0 is either the root of T0 or its parent in T is also its parent in T�. An embedded subtree T0 of T is a subtree where every node in T0 is either the root of T0 or its parent in T0 is one of its ancestors in T. See Figure 2 for an example of these different types of subtrees. Linear time solutions exist for finding all frequent bottom-up subtrees in a treebank because this problem can be transformed into finding all frequent substrings in a string, a problem for which fast solutions are well known (Luccio et al., 2001; Luccio et al., 2004). Solutions for induced and embedded subtrees draw heavily on Zaki (2002) (the TreeMiner algorithm) and Asai et al. (2002) (the FREQT algorithm), both of whom propose Apriori-style approaches. This type of solution has the general property that runtime is proportionate to the size of the output: the sum of the number of times each frequent subtree appears in the treebank. This is not readily predictable, because the number and frequencies of subtrees is not formally determinable from the size of the treebank and can grow very rapidly. 3.1 Ordered and unordered trees TreeMiner/FREQT approaches </context>
</contexts>
<marker>Luccio, Enriquez, Rieumont, Pagli, 2004</marker>
<rawString>Fabrizio Luccio, Antonio Enriquez, Pablo Rieumont and Linda Pagli. 2004. Bottom-up subtree isomorphism for unordered labeled trees. Universit`a Di Pisa Technical Report TR-04-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward M McCreight</author>
</authors>
<title>A Space-Economical Suffix Tree Construction Algorithm.</title>
<date>1976</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>272</pages>
<contexts>
<context position="1897" citStr="McCreight, 1976" startWordPosition="290" endWordPosition="291">structured corpora, but access to it is very limited using the traditional algorithms and analytical tools of computational linguistics. Many of the most powerful techniques available to natural language processing have been built on the basis of n-gram and bag of words models, but we already know that these methods are inadequate to fully model the information in texts or we would have little use for treebanks or annotation schemes. Suffix trees provide some improvement over n-grams and bag-of-words schemes by identifying all frequently occurring sequences regardless of length (Weiner, 1973; McCreight, 1976; Ravichandran and Hovy, 2002). While this has value in identifying some multi-word phenomena, any algorithm that models languages on the basis of frequent contiguous string discovery will have trouble modeling a number of pervasive phenomena in natural language. In particular: • Long distance dependencies – i.e., dependencies between words that are too far apart to be accessible to n-gram models. • Flexible word orders – languages usually have contexts where word order can vary. • Languages with very rich morphologies that must be taken into account or where too much important information is </context>
</contexts>
<marker>McCreight, 1976</marker>
<rawString>Edward M. McCreight. 1976. A Space-Economical Suffix Tree Construction Algorithm. Journal of the Association for Computing Machinery, 23(2):262– 272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>Albany, NY.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor A. Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, Albany, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<title>Data-Oriented Translation.</title>
<date>2000</date>
<booktitle>Proc. of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<pages>635--641</pages>
<contexts>
<context position="5968" citStr="Poutsma, 2000" startWordPosition="950" endWordPosition="951">t al. (2002) who roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as an aid in the development of statistical and example-based machine translations systems (Hassan et al., 2006; Tinsley et al., 2007; Zhechev and Way, 2008). 3 Key concepts Among the key concepts in tree mining is the difference between bottom-up subtrees, induced subtrees and embedded subtrees. A bottom-up subtree T&apos; of a tree T is a</context>
</contexts>
<marker>Poutsma, 2000</marker>
<rawString>Arjen Poutsma. 2000. Data-Oriented Translation. Proc. of the 18th International Conference on Computational Linguistics (COLING 2000), 635–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjen Poutsma</author>
</authors>
<date>2003</date>
<booktitle>Machine Translation with TreeDOP. Data-Oriented Parsing,</booktitle>
<pages>63--81</pages>
<contexts>
<context position="5983" citStr="Poutsma, 2003" startWordPosition="952" endWordPosition="953">o roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as an aid in the development of statistical and example-based machine translations systems (Hassan et al., 2006; Tinsley et al., 2007; Zhechev and Way, 2008). 3 Key concepts Among the key concepts in tree mining is the difference between bottom-up subtrees, induced subtrees and embedded subtrees. A bottom-up subtree T&apos; of a tree T is a subtree where,</context>
</contexts>
<marker>Poutsma, 2003</marker>
<rawString>Arjen Poutsma. 2003. Machine Translation with TreeDOP. Data-Oriented Parsing, 63–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>41--47</pages>
<contexts>
<context position="1927" citStr="Ravichandran and Hovy, 2002" startWordPosition="292" endWordPosition="295">a, but access to it is very limited using the traditional algorithms and analytical tools of computational linguistics. Many of the most powerful techniques available to natural language processing have been built on the basis of n-gram and bag of words models, but we already know that these methods are inadequate to fully model the information in texts or we would have little use for treebanks or annotation schemes. Suffix trees provide some improvement over n-grams and bag-of-words schemes by identifying all frequently occurring sequences regardless of length (Weiner, 1973; McCreight, 1976; Ravichandran and Hovy, 2002). While this has value in identifying some multi-word phenomena, any algorithm that models languages on the basis of frequent contiguous string discovery will have trouble modeling a number of pervasive phenomena in natural language. In particular: • Long distance dependencies – i.e., dependencies between words that are too far apart to be accessible to n-gram models. • Flexible word orders – languages usually have contexts where word order can vary. • Languages with very rich morphologies that must be taken into account or where too much important information is lost through lemmatization. • </context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy 2002. Learning surface text patterns for a question answering system. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), 41–47.</rawString>
</citation>
<citation valid="true">
<title>Word Grammar: New Perspectives on a Theory of Language Structure.</title>
<date>2006</date>
<editor>Kensei Sugayama and Richard Hudson, editors.</editor>
<publisher>Continuum International Publishing,</publisher>
<location>London.</location>
<contexts>
<context position="5633" citStr="(2006)" startWordPosition="897" endWordPosition="897">hen treating it as if it was a rooted and directed tree (Chi et al., 2003). Figure 2: Tree (a) and different types of subtree: (b) a bottom-up subtree of (a), (c) an induced subtree of (a), and (d) an embedded subtree of (a). Research on frequent subtree discovery generally draws heavily on early work by Zaki (2002) and Asai et al. (2002) who roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as</context>
</contexts>
<marker>2006</marker>
<rawString>Kensei Sugayama and Richard Hudson, editors. 2006. Word Grammar: New Perspectives on a Theory of Language Structure. Continuum International Publishing, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de la syntaxe structurale. Editions Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Lucien Tesni`ere. 1959. El´ements de la syntaxe structurale. Editions Klincksieck, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Tinsley</author>
<author>Mary Hearne</author>
<author>Andy Way</author>
</authors>
<title>Exploiting Parallel Treebanks for use in Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>Proceedings of Treebanks and Linguistic Theories (TLT ’07),</booktitle>
<location>Bergen,</location>
<contexts>
<context position="6364" citStr="Tinsley et al., 2007" startWordPosition="1012" endWordPosition="1015">equent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as an aid in the development of statistical and example-based machine translations systems (Hassan et al., 2006; Tinsley et al., 2007; Zhechev and Way, 2008). 3 Key concepts Among the key concepts in tree mining is the difference between bottom-up subtrees, induced subtrees and embedded subtrees. A bottom-up subtree T&apos; of a tree T is a subtree where, for every node in T&apos;, if its corresponding node in T has children, then all those children are also in T&apos;. An induced (a) (b) (c) (d) 85 Figure 3: “...between the European Commission and the government of the [German] Federal Republic...” This structure is a subtree of one of the sentences in the Alpino corpus of Dutch where a node has two children with the same labels - two NP</context>
</contexts>
<marker>Tinsley, Hearne, Way, 2007</marker>
<rawString>John Tinsley, Mary Hearne and Andy Way. 2007. Exploiting Parallel Treebanks for use in Statistical Machine Translation. Proceedings of Treebanks and Linguistic Theories (TLT ’07), Bergen, Norway 175–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At last parsing is now operational. Verbum Ex Machina. Actes de la 13e conf´erence sur le traitement automatique des langues naturelles (TALN6),</title>
<date>2006</date>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At last parsing is now operational. Verbum Ex Machina. Actes de la 13e conf´erence sur le traitement automatique des langues naturelles (TALN6), 20–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Weiner</author>
</authors>
<title>Linear pattern matching algorithm.</title>
<date>1973</date>
<booktitle>14th Annual IEEE Symposium on Switching and Automata Theory,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1880" citStr="Weiner, 1973" startWordPosition="288" endWordPosition="289"> more complex structured corpora, but access to it is very limited using the traditional algorithms and analytical tools of computational linguistics. Many of the most powerful techniques available to natural language processing have been built on the basis of n-gram and bag of words models, but we already know that these methods are inadequate to fully model the information in texts or we would have little use for treebanks or annotation schemes. Suffix trees provide some improvement over n-grams and bag-of-words schemes by identifying all frequently occurring sequences regardless of length (Weiner, 1973; McCreight, 1976; Ravichandran and Hovy, 2002). While this has value in identifying some multi-word phenomena, any algorithm that models languages on the basis of frequent contiguous string discovery will have trouble modeling a number of pervasive phenomena in natural language. In particular: • Long distance dependencies – i.e., dependencies between words that are too far apart to be accessible to n-gram models. • Flexible word orders – languages usually have contexts where word order can vary. • Languages with very rich morphologies that must be taken into account or where too much importan</context>
</contexts>
<marker>Weiner, 1973</marker>
<rawString>Peter Weiner. 1973 Linear pattern matching algorithm. 14th Annual IEEE Symposium on Switching and Automata Theory, 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed J Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>1021--1035</pages>
<contexts>
<context position="5344" citStr="Zaki (2002)" startWordPosition="849" endWordPosition="850">bset of graph mining focused specifically on rooted, directed acyclic graphs. Although there is research into extracting frequent subtrees from free (unrooted and undirected) trees, free tree mining usually proceeds by deciding which node in any particular tree will be treated as a root, and then treating it as if it was a rooted and directed tree (Chi et al., 2003). Figure 2: Tree (a) and different types of subtree: (b) a bottom-up subtree of (a), (c) an induced subtree of (a), and (d) an embedded subtree of (a). Research on frequent subtree discovery generally draws heavily on early work by Zaki (2002) and Asai et al. (2002) who roughly simultaneously began applying the Apriori algorithm to frequent tree discovery (Agrawal et al., 1993). For a summary of Apriori, which is widely used in data mining, and a short review of its extensive literature, see Kotsiantis and Kanellopoulos (2006). A broad summary of algorithms for frequent subtree mining can be found in Chi et al. (2004). Research into frequent substructures in computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translat</context>
<context position="7742" citStr="Zaki (2002)" startWordPosition="1269" endWordPosition="1270">ther the root of T0 or its parent in T is also its parent in T�. An embedded subtree T0 of T is a subtree where every node in T0 is either the root of T0 or its parent in T0 is one of its ancestors in T. See Figure 2 for an example of these different types of subtrees. Linear time solutions exist for finding all frequent bottom-up subtrees in a treebank because this problem can be transformed into finding all frequent substrings in a string, a problem for which fast solutions are well known (Luccio et al., 2001; Luccio et al., 2004). Solutions for induced and embedded subtrees draw heavily on Zaki (2002) (the TreeMiner algorithm) and Asai et al. (2002) (the FREQT algorithm), both of whom propose Apriori-style approaches. This type of solution has the general property that runtime is proportionate to the size of the output: the sum of the number of times each frequent subtree appears in the treebank. This is not readily predictable, because the number and frequencies of subtrees is not formally determinable from the size of the treebank and can grow very rapidly. 3.1 Ordered and unordered trees TreeMiner/FREQT approaches require all trees to be ordered so that the nodes of any frequent subtree</context>
<context position="11352" citStr="Zaki, 2002" startWordPosition="1916" endWordPosition="1917"> - and thereby increase their speed dramatically without producing less 86 (a) The common subtree of the two parse trees in Figure 1: “naar huis gaan” (b) Redundant subtrees of tree (a). There are many more such structures. Figure 4: Closed and non-closed subtrees in just the two sentences in Figure 1. In a larger treebank, some of these might not be redundant. information, since any non-closed subtree present in the treebank is a subtree of a closed one and shares its distribution. 4 Algorithm and data structures The algorithm used in this research is an extension of the TreeMiner algorithm (Zaki, 2002), modified to extract only closed subtrees. It takes a minimum frequency threshold as a parameter and extracts only those subtrees which are closed and whose frequency is at least equal to the threshold. This algorithm suffers from the same shortcoming of Zaki’s original algorithm in that it is only guaranteed to find all frequent subtrees among ordered trees where no node has two non-leaf children with the same label. It has one novel property which it appears not to share with any other subtree extraction scheme to date: This algorithm outputs subtrees in order from the most frequent to the </context>
<context position="16178" citStr="Zaki (2002)" startWordPosition="2799" endWordPosition="2800">ize as described in Section 4.2. 2. Pop the top HitList hl and its associated subtree s from the queue. 3. Extend hl: (a) Visit each Hit in hl and find all the nodes that can be added to the right side of s to produce new induced subtrees. (b) Generate new HitLists for all subtrees that extend s by one node to the right. (c) Test each new HitList to make sure it appears more than threshold frequency t times, and if it does, insert it into the queue. (d) Output s and hl. 4. Repeat until the queue is empty. This is essentially identical to the TreeMiner and FREQT algorithms already published by Zaki (2002) and by Asai et al. (2002), except that it outputs frequent subtrees in order from the most frequent to the least. 4.4 Extracting only closed induced subtrees By controlling the order in which HitLists reach the top of the queue, it is possible to efficiently prevent any subtree which is not a closed subtree or a prefix of a closed subtree from being extended, and to prevent any subtree that is not closed from being outputted. Every subtree with a frequency of f is either a closed subtree, a prefix of a closed subtree that also has a frequency of f and can be constructed by adding more nodes t</context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed J. Zaki. 2002. Efficiently mining frequent trees in a forest. Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1021–1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ventsislav Zhechev</author>
<author>Andy Way</author>
</authors>
<title>Automatic Generation of Parallel Treebanks.</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd International Conference on Computational Linguistics (COLING</booktitle>
<pages>1105--1112</pages>
<contexts>
<context position="6388" citStr="Zhechev and Way, 2008" startWordPosition="1016" endWordPosition="1019">n computational linguistics is quite limited. The Data Oriented Processing model (Bod et al., 2003) along with its extension into machine translation - the Data Oriented Translation model (Poutsma, 2000; Poutsma, 2003; Hearne and Way, 2003) - is the most developed approach to using frequent subtree statistics to do natural language processing. There is also growing work, largely stemming out of DOP research, into subtree alignment in bilingual parsed treebanks as an aid in the development of statistical and example-based machine translations systems (Hassan et al., 2006; Tinsley et al., 2007; Zhechev and Way, 2008). 3 Key concepts Among the key concepts in tree mining is the difference between bottom-up subtrees, induced subtrees and embedded subtrees. A bottom-up subtree T&apos; of a tree T is a subtree where, for every node in T&apos;, if its corresponding node in T has children, then all those children are also in T&apos;. An induced (a) (b) (c) (d) 85 Figure 3: “...between the European Commission and the government of the [German] Federal Republic...” This structure is a subtree of one of the sentences in the Alpino corpus of Dutch where a node has two children with the same labels - two NPs. This often occurs wit</context>
</contexts>
<marker>Zhechev, Way, 2008</marker>
<rawString>Ventsislav Zhechev and Andy Way. 2008. Automatic Generation of Parallel Treebanks. Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), 1105–1112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>