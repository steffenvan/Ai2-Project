<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989083">
Uncertainty reduction as a measure of cognitive processing effort
</title>
<author confidence="0.999158">
Stefan L. Frank
</author>
<affiliation confidence="0.9041705">
University of Amsterdam
Amsterdam, The Netherlands
</affiliation>
<email confidence="0.977324">
s.l.frank@uva.nl
</email>
<sectionHeader confidence="0.997169" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935066666667">
The amount of cognitive effort required to
process a word has been argued to depend
on the word’s effect on the uncertainty
about the incoming sentence, as quanti-
fied by the entropy over sentence probabil-
ities. The current paper tests this hypoth-
esis more thoroughly than has been done
before by using recurrent neural networks
for entropy-reduction estimation. A com-
parison between these estimates and word-
reading times shows that entropy reduc-
tion is positively related to processing ef-
fort, confirming the entropy-reduction hy-
pothesis. This effect is independent from
the effect of surprisal.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999824518518519">
In the field of computational psycholinguistics, a
currently popular approach is to account for read-
ing times on a sentence’s words by estimates of the
amount of information conveyed by these words.
Processing a word that conveys more information
is assumed to involve more cognitive effort, which
is reflected in the time required to read the word.
In this context, the most common formaliza-
tion of a word’s information content is its sur-
prisal (Hale, 2001; Levy, 2008). If word string
wt1 (short for w1, w2, ... wt) is the sentence so
far and P(wt+1jwt1) the occurrence probability of
the next word wt+1, then that word’s surprisal is
defined as − log P(wt+1jwt1). It is well estab-
lished by now that word-reading times indeed cor-
relate positively with surprisal values as estimated
by any sufficiently accurate generative language
model (Boston et al., 2008; Demberg and Keller,
2008; Frank, 2009; Roark et al., 2009; Smith and
Levy, 2008).
A lesser known alternative operationalization of
a word’s information content is based on the un-
certainty about the rest of the sentence, quantified
by Hale (2003, 2006) as the entropy of the prob-
ability distribution over possible sentence struc-
tures. The reduction in entropy that results from
processing a word is taken to be the amount of
information conveyed by that word, and was ar-
gued by Hale to be predictive of word-reading
time. However, this entropy-reduction hypothesis
has not yet been comprehensively tested, possibly
because of the difficulty of computing the required
entropies. Although Hale (2006) shows how sen-
tence entropy can be computed given a PCFG, this
computation is not feasible when the grammar is
of realistic size.
Here, we empirically investigate the entropy-
reduction hypothesis more thoroughly than has
been done before, by using recurrent neural net-
works as language models. Since these networks
do not derive any structure, they provide estimates
of sentence entropy rather than sentence-structure
entropy. In practice, these two entropies will gen-
erally be similar: If the rest of the sentence is
highly uncertain, so is its structure. Sentence en-
tropy can therefore be viewed as a simplification
of structure entropy; one that is less theory depen-
dent since it does not rely on any particular gram-
mar. The distinction between entropy over sen-
tences and entropy over structures will simply be
ignored in the remainder of this paper.
Results show that, indeed, a significant fraction
of variance in reading-time data is accounted for
by entropy reduction, over and above surprisal.
</bodyText>
<sectionHeader confidence="0.990693" genericHeader="introduction">
2 Entropy and sentence processing
</sectionHeader>
<subsectionHeader confidence="0.978588">
2.1 Sentence entropy
</subsectionHeader>
<bodyText confidence="0.999870333333333">
Let W be the set of words in the language and Wi
the set of all word strings of length i. The set of
complete sentences, denoted S, contains all word
strings of any length (i.e., U°°0 Wi), except that a
special end-of-sentence marker &lt;/s&gt; is attached
to the end of each string.
</bodyText>
<page confidence="0.987862">
81
</page>
<note confidence="0.9791975">
Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 81–89,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.996941">
A generative language model defines a proba-
bility distribution over S. The entropy of this dis-
tribution is
</bodyText>
<equation confidence="0.9514165">
H = − X P(wj1) log P(wj1).
wj1ES
</equation>
<bodyText confidence="0.9999566">
As words are processed one by one, the sen-
tence probabilities change. When the first t words
(i.e., the string wt 1 E Wt) of a sentence have been
processed, the entropy of the probability distribu-
tion over sentences is
</bodyText>
<equation confidence="0.978823">
H(t) = − X P(wj1jwt1)log P(wj1jwt1). (1)
wj1ES
</equation>
<bodyText confidence="0.995883666666667">
In order to simplify later equations, we define
the function h(yjx) = −P(yjx) log P(yjx), such
that Eq. 1 becomes
</bodyText>
<equation confidence="0.9882795">
H(t) = X h(wj1jwt1).
wj1ES
</equation>
<bodyText confidence="0.9995226">
If the first t words of wj1 do not equal wt1 (or wj1
has fewer than t + 1 words),1 then P(wj1jwt1) = 0
so h(wj1jwt1) = 0. This means that, for computing
H(t), only the words from t + 1 onwards need to
be taken into account:
</bodyText>
<equation confidence="0.856292">
H(t) = X h(wjt+1jwt1).
wjt+1ES
The reduction in entropy due to processing the
next word, wt+1, is
AH(t + 1) = H(t) − H(t + 1). (2)
</equation>
<bodyText confidence="0.9932075">
Note that positive AH corresponds to a
decrease in entropy. According to Hale
(2006), the nonnegative reduction in entropy (i.e.,
max{0, AH}) reflects the cognitive effort in-
volved in processing wt+1 and should therefore be
predictive of reading time on that word.
</bodyText>
<subsectionHeader confidence="0.999563">
2.2 Suffix entropy
</subsectionHeader>
<bodyText confidence="0.99940325">
Computing H(t) is computationally feasible only
when there are very few sentences in S, or when
the language can be described by a small grammar.
To estimate entropy in more realistic situations, an
</bodyText>
<footnote confidence="0.684639333333333">
1Since wj1 ends with &lt; /s &gt; and wt1 does not, the two
strings must be different. Consequently, if wj1 is t words long,
then P(wj1|wt1� = 0.
</footnote>
<bodyText confidence="0.998585583333333">
obvious solution is to look only at the next few
words instead of all complete continuations of wt1.
Let Sm be the subset of S containing all (and
only) sentences of length m or less, counting also
the &lt;/s&gt; at the end of each sentence. Note that
this set includes the ‘empty sentence’ consisting
of only &lt;/s&gt;. The set of length-m word strings
that do not end in &lt;/s&gt; is Wm. Together, these
sets form Wm = Wm U Sm, which contains all
the relevant strings for defining the entropy over
strings up to length m.2 After processing wt1, the
entropy over strings up to length t + n is:
</bodyText>
<equation confidence="0.943699">
Hn(t) = X Xh(wj1jwt1) = h(wjt+1jwt1).
wj1EWt+n wjt+1EWn
</equation>
<bodyText confidence="0.99967625">
It now seems straightforward to define suffix-
entropy reduction by analogy with sentence-
entropy reduction as expressed in Eq. 2: Simply
replace H by Hn to obtain
</bodyText>
<equation confidence="0.939062">
AHsuf
n (t + 1) = Hn(t) − Hn(t + 1). (3)
</equation>
<bodyText confidence="0.974418">
As indicated by its superscript label, AHsuf
</bodyText>
<subsectionHeader confidence="0.472721">
n
</subsectionHeader>
<bodyText confidence="0.934855833333333">
quantifies the reduction in uncertainty about the
upcoming n-word suffix. However, this is concep-
tually different from the original AH of Eq. 2,
which is the reduction in uncertainty about the
identity of the current sentence. The difference
becomes clear when we view the sentence proces-
sor’s task as that of selecting the correct element
from S. If this set of complete sentences is ap-
proximated by Wt+n, and the task is to select one
element from that set, an alternative definition of
suffix-entropy reduction arises:
AHsent
</bodyText>
<equation confidence="0.9969449">
n (t + 1)
X
− h(wj 1jwt+1
1 )
wj1EWt+n
= h(wjt+1jwt1) −
X X h(wj t+2jwt+1
1 )
wjt+1EWn wjt+2EWn−1
= Hn(t) − Hn−1(t + 1). (4)
</equation>
<bodyText confidence="0.983920428571429">
The label ‘sent’ indicates that AHsent
n quantifies
the reduction in uncertainty about which sentence
forms the current input. This uncertainty is ap-
proximated by marginalizing over all word strings
longer than t + n.
It is easy to see that
</bodyText>
<footnote confidence="0.87620525">
lim AHnsuf = lim AHsent = AH,
n→oo n→oo n
2The probability of a string wm1 E Wm is the summed
probability of all sentences with prefix wm1 .
</footnote>
<equation confidence="0.975331">
X= h(wj1jwt1)
wj1EWt+n
</equation>
<page confidence="0.967799">
82
</page>
<bodyText confidence="0.9957426">
so both approximations of entropy reduction ap-
propriately converge to AH in the limit. Nev-
ertheless, they formalize different quantities and
may well correspond to different cognitive factors.
If it is true that cognitive effort is predicted by
the reduction in uncertainty about the identity of
the incoming sentence, we should find that word-
reading times are predicted more accurately by
AHsent
� than by AHsuf �.
</bodyText>
<subsectionHeader confidence="0.998911">
2.3 Relation to next-word entropy
</subsectionHeader>
<bodyText confidence="0.998733">
In the extreme case of n = 1, Eq. 4 reduces to
</bodyText>
<equation confidence="0.931399">
AHsent
1 (t + 1) =
</equation>
<bodyText confidence="0.899861444444444">
so the reduction of entropy over the single next
word wt+1 equals the next-word entropy just be-
fore processing that word. Note that AHsent
1 (t+1)
is independent of the word at t + 1, making it a
severely impoverished measure of the uncertainty
reduction caused by that word. We would there-
fore expect reading times to be predicted more ac-
curately by AHsent
</bodyText>
<equation confidence="0.63115">
� with n &gt; 1, and possibly even
by AHsuf
1 .
</equation>
<bodyText confidence="0.994787727272727">
Roark et al. (2009) investigated the relation be-
tween H1(t + 1) and reading time on wt+1, and
found a significant positive effect: Larger next-
word entropy directly after processing wt+1 cor-
responded to longer reading time on that word.
This is of particular interest because H1(t + 1)
necessarily correlates negatively with entropy re-
duction AHsent
� (t + 1): If entropy is large after
wt+1, chances are that it did not reduce much
through processing of wt+1. Indeed, in our data
</bodyText>
<equation confidence="0.7704595">
set, H1(t + 1) and AHsent
� (t + 1) correlate be-
</equation>
<bodyText confidence="0.967126">
tween r = −.29 and r = −.26 (for n = 2 to
n = 4) which is highly significantly (p Pz� 0) dif-
ferent from 0. Roark et al.’s finding of a positive
relation between H1(t + 1) and reading time on
wt+1 therefore seems to disconfirm the entropy-
reduction hypothesis.
</bodyText>
<sectionHeader confidence="0.980809" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.97732975">
A set of language models was trained on a corpus
of POS tags of sentences. The advantage of using
POS tags rather than words is that their probabil-
ities can be estimated much more accurately and,
consequently, more accurate prediction of word-
reading time is possible (Demberg and Keller,
2008; Roark et al., 2009). Subsequent to training,
the models were made to generate estimates of sur-
prisal and entropy reductions AHsuf
� and AHsent
�
over a test corpus. These estimates were then com-
pared to reading times measured over the words
of the same test corpus. This section presents the
data sets that were used, language-model details,
and the evaluation metric.
</bodyText>
<subsectionHeader confidence="0.99676">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.99998736">
The models were trained on the POS tag se-
quences of the full WSJ corpus (Marcus et al.,
1993). They were evaluated on the POS-tagged
Dundee corpus (Kennedy and Pynte, 2005), which
has been used in several studies that investigate the
relation between word surprisal and reading time
(Demberg and Keller, 2008; Frank, 2009; Smith
and Levy, 2008). This 2368-sentence (51501
words) collection of British newspaper editorials
comes with eye-tracking data of 10 participants.
POS tags for the Dundee corpus were taken from
Frank (2009).
For each word and each participant, reading
time was defined as the total fixation time on that
word before any fixation on a later word of the
same sentence. Following Demberg and Keller
(2008), data points (i.e., word/participant pairs)
were removed if the word was not fixated, was
presented as the first or last on a line, contained
more than one capital letter or a non-letter (e.g.,
the apostrophe in a clitic), or was attached to punc-
tuation. Mainly due to the large number (over
46%) of nonfixations, 62.8% of data points were
removed, leaving 191380 data points (between
16 469 and 21770 per participant).
</bodyText>
<subsectionHeader confidence="0.998354">
3.2 Language model
</subsectionHeader>
<bodyText confidence="0.999989777777778">
Entropy is more time consuming to compute than
surprisal, even for n = 1, because it requires es-
timates of the occurrence probabilities at t + 1 of
all word types, rather than just of the actual next
word. Moreover, the number of suffixes rises ex-
ponentially as suffix length n grows, and, conse-
quently, so does computation time.
Roark et al. (2009) used an incremental PCFG
parser to obtain H1 but this method rapidly be-
comes infeasible as n grows. Low-order Markov
models (e.g., a bigram model) are more efficient
and can be used for larger n but they do not form
particularly accurate language models. Moreover,
Markov models lack cognitive plausibility.
Here, Simple Recurrent Networks (SRNs) (El-
man, 1990) are used as language models. When
trained to predict the upcoming input in a word se-
quence, these networks can generate estimates of
</bodyText>
<equation confidence="0.954679">
H1(t) − H0(t + 1) = H1(t),
</equation>
<page confidence="0.959315">
83
</page>
<bodyText confidence="0.996228818181818">
P(wt+1|wt1) efficiently and relatively accurately.
They thereby allow to approximate sentence en-
tropy more closely than the incremental parsers
used in previous studies. Unlike Markov models,
SRNs have been claimed to form cognitively re-
alistic sentence-processing models (Christiansen
and MacDonald, 2009). Moreover, it has been
shown that SRN-based surprisal estimates can cor-
relate more strongly to reading times than surprisal
values estimated by a phrase-structure grammar
(Frank, 2009).
</bodyText>
<subsectionHeader confidence="0.886417">
3.2.1 Network architecture and processing
</subsectionHeader>
<bodyText confidence="0.9997675">
The SRNs comprised three layers of units: the in-
put layer, the recurrent (hidden) layer, and the out-
put layer. Each input unit corresponds to one POS
tag, making 45 input units since there are 45 dif-
ferent POS tags in the WSJ corpus. The network’s
output units represent predictions of subsequent
inputs. The output layer also has one unit for each
POS tag, plus an extra unit that represents &lt;/s&gt;,
that is, the absence of any further input. Hence,
there were 46 output units. The number of recur-
rent units was fairly arbitrarily set to 100.
As is common in these networks, the input layer
was fully connected to the recurrent layer, which
in turn was fully connected to the output layer.
Also, there were time-delayed connections from
the recurrent layer to itself. In addition, each re-
current and output unit received a bias input.
The vectors of recurrent- and output-layer ac-
tivations after processing wt1 are denoted arec(t)
and aout(t), respectively. At the beginning of each
sentence, arec(0) = 0.5.
The input vector aiin, representing POS tag i,
consists of zeros except for a single element (cor-
responding to i) that equals one. When input i is
processed, the recurrent layer’s state is updated ac-
cording to:
</bodyText>
<equation confidence="0.972381">
arec(t) = frec(Wrecarec(t − 1) + Winaiin + brec),
</equation>
<bodyText confidence="0.998998166666667">
where matrices Win and Wrec contain the net-
work’s input and recurrent connection weights, re-
spectively; brec is the vector of recurrent-layer bi-
ases; and activation function frec(x) is the logistic
function f(x) = (1+e−x)−1 applied elementwise
to x. The new output vector is now given by
</bodyText>
<equation confidence="0.99401">
aout(t) = fout(Woutarec(t) + bout),
</equation>
<bodyText confidence="0.998518">
where Wout is the matrix of output connection
weights; bout the vector of output-layer biases; and
fout(x) the softmax function
</bodyText>
<equation confidence="0.628128">
e�z
fi,out(x1, ... , x46) = e i j
exi .
</equation>
<bodyText confidence="0.999940428571429">
This function makes sure that aout sums to one
and can therefore be viewed as a probability dis-
tribution: The i-th element of aout(t) is the SRN’s
estimate of the probability that the i-th POS tag
will be the input at t + 1, or, in case i corresponds
to &lt; /s &gt;, the probability that the sentence ends
after t POS tags.
</bodyText>
<subsubsectionHeader confidence="0.673358">
3.2.2 Network training
</subsubsectionHeader>
<bodyText confidence="0.999937909090909">
Ten SRNs, differing only in their random initial
connection weights and biases, were trained us-
ing the standard backpropagation algorithm. Each
string of WSJ POS tags was presented once, with
the sentences in random order. After each POS in-
put, connection weights were updated to minimize
the cross-entropy between the network outputs and
a 46-element vector that encoded the next input (or
marked the end of the sentence) by the correspond-
ing element having a value of one and all others
being zero.
</bodyText>
<subsectionHeader confidence="0.9690415">
3.3 Evaluation
3.3.1 Obtaining surprisal and entropy
</subsectionHeader>
<bodyText confidence="0.99983125">
Since aout(t) is basically the probability distribu-
tion P(wt+1|wt1), surprisal and H1 can be read off
directly. To obtain H2, H3, and H4, we use the
fact that
</bodyText>
<equation confidence="0.964221">
P(wt+i|wt+i−1
1 ). (5)
</equation>
<bodyText confidence="0.987465352941176">
Surprisal and entropy estimates were averaged
over the ten SRNs. So, for each POS tag of the
Dundee corpus, there was one estimate of surprisal
and four of entropy (for n = 1 to n = 4).
Since Hn(t) approximates H(t) more closely
as n grows, it would be natural to expect a better
fit to reading times for larger n. On the other hand,
it goes without saying that Hn is only a very rough
measure of a reader’s actual uncertainty about the
upcoming n inputs, no matter how accurate the
language model that was used to compute these
entropies. Crucially, the correspondence between
Hn and the uncertainty experienced by a reader
will grow even weaker with larger n. This is ap-
parent from the fact that, as proven in the Ap-
pendix, Hn can be expressed in terms of H1 and
Hn−1:
</bodyText>
<equation confidence="0.9981388">
Hn(t) = H1(t) + E(Hn−1(t + 1)),
P(wt+n
t+1 |wt1) =
n
i=1
</equation>
<page confidence="0.992802">
84
</page>
<figure confidence="0.99299975">
0.5
0.25
0
correlation with surprisal
100
Significance (p−value)
10−1
10−2
10−3
10−4
1 2 3 4
suffix length n
</figure>
<figureCaption confidence="0.934518666666667">
Figure 1: Coefficient of correlation between es-
timates of surprisal and entropy reduction, as a
function of suffix length n.
</figureCaption>
<bodyText confidence="0.999984285714286">
where E(x) is the expected value of x. Obviously,
the expected value of Hn_1 is less appropriate as
an uncertainty measure than is Hn_1 itself. Hence,
Hn can be less accurate than Hn_1 as a quantifi-
cation of the actual cognitive uncertainty. For this
reason, we may expect larger n to result in worse
fit to reading-time data.3
</bodyText>
<subsectionHeader confidence="0.837349">
3.3.2 Negative entropy reduction
</subsectionHeader>
<bodyText confidence="0.971971807692308">
Hale (2006) argued for nonnegative entropy re-
duction max{0, AH}, rather than AH itself, as
a measure of processing effort. For AHsent, the
difference between the two is negligible because
only about 0.03% of entropy reductions are neg-
ative. As for AHsuf, approximately 42% of val-
ues are negative so whether these are left out
makes quite a difference. Since preliminary ex-
periments showed that word-reading times are pre-
dicted much more accurately by AHsuf than by
max{0, AHsuf}, only AHsuf and AHsent were
used here, that is, negative values were included.
3.3.3 Relation between information measures
Both surprisal and entropy reduction can be taken
as measures for the amount of information con-
veyed by a word, so it is to be expected that they
are positively correlated. However, as shown in
Figure 1, this correlation is in fact quite weak,
ranging from .14 for AHsuf
4 to .38 for AHsent
1 .
In contrast, AHsuf
n and AHsent
n correlate very
strongly to each other: The coefficients of correla-
tion range from .73 when n = 1 to .97 for n = 4.
</bodyText>
<footnote confidence="0.862111">
3Not to mention the realistic possibility that the cognitive
sentence-processing system does not abide by the normative
chain rule expressed in Eq. 5.
</footnote>
<figure confidence="0.936615333333333">
p = .05 3.84
0 4 8 12
Effect size
</figure>
<figureCaption confidence="0.811359333333333">
Figure 2: Cumulative k2 distribution with 1 de-
gree of freedom, plotting statistical significance
(p-value) as a function of effect size.
</figureCaption>
<subsectionHeader confidence="0.4762">
3.3.4 Fit to reading times
</subsectionHeader>
<bodyText confidence="0.999961368421053">
A generalized linear regression model for gamma-
distributed data was fitted to the reading times.4
This model contained several well-known predic-
tors of word-reading time: the number of letters
in the word, the word’s position in the sentence,
whether the next word was fixated, whether the
previous word was fixated, log of the word’s rel-
ative frequency, log of the word’s forward and
backward transitional probabilities,5 and surprisal
of the part-of-speech. Next, one set of entropy-
reduction estimates was added to the regression.
The effect size is the resulting decrease in the re-
gression model’s deviance, which is indicative of
the amount of variance in reading time accounted
for by those estimates of entropy reduction. Fig-
ure 2 shows how effect size is related to statis-
tical significance: A factor forms a significant
(p &lt; .05) predictor of reading time if its effect
size is greater than 3.84.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="method">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999883">
4.1 Effect of entropy reduction
</subsectionHeader>
<bodyText confidence="0.999782">
Figure 3 shows the effect sizes for both measures
of entropy reduction, and their relation to suffix
length n. All effects are in the correct direction,
that is, larger entropy reduction corresponds to
longer reading time. These results clearly support
the entropy-reduction hypothesis: A significant
</bodyText>
<footnote confidence="0.860728333333333">
4The reading times, which are approximately gamma dis-
tributed, were first normalized to make the scale parameters
of the gamma distributions the same across participants.
5These are, respectively, the relative frequency of the
word given the previous word, and its relative frequency
given the next word.
</footnote>
<figure confidence="0.851854666666667">
ΔH
sent
suf
ΔH
n
n
</figure>
<page confidence="0.60607">
85
</page>
<figure confidence="0.9779695">
1 2 3 4
suffix length n
</figure>
<figureCaption confidence="0.998204">
Figure 3: Size of the effect of AHsuf �and AHsent
</figureCaption>
<figure confidence="0.837262666666667">
�
as a function of suffix length n.
suffix length n
</figure>
<figureCaption confidence="0.998044">
Figure 4: Effect size of entropy reduction
</figureCaption>
<bodyText confidence="0.789314333333333">
(AHsent
� ), next-word entropy (H1), or surprisal,
over and above the other two predictors.
</bodyText>
<figure confidence="0.991817590909091">
suf
ΔH
ΔH
n
n
sent
1 2 3 4
20
15
10
5
H1
surprisal
ΔH
sent
n
10
5
effect size
ΔHn
0
effect size
</figure>
<bodyText confidence="0.932785619047619">
fraction of variance in reading time is accounted
for by the entropy-reduction estimates AHsent
� ,
over and above what is explained by the other fac-
tors in the regression analysis, including surprisal.
Moreover, the effect of AHsent
� is larger than that
of AHsuf
� , indicating that it is indeed uncertainty
about the identity of the current sentence, rather
than uncertainty about the upcoming input(s), that
matters for cognitive processing effort. Only at
n = 1 was the effect size of AHsent
� smaller than
that of AHsuf
� , but it should be kept in mind that
AHsent
1 is independent of the incoming word and
is therefore quite impoverished as a measure of the
effort involved in processing the word. Moreover,
the difference between AHsent
</bodyText>
<sectionHeader confidence="0.591955" genericHeader="method">
1 and AHsuf
</sectionHeader>
<bodyText confidence="0.970250823529412">
1 is not
significant (p &gt; .4), as determined by the boot-
strap method (Efron and Tibshirani, 1986). In con-
trast, the differences are significant when n &gt; 1
(all p &lt; .01), in spite of the high correlation be-
tween AHsent
� and AHsuf
� .
Another indication that cognitive processing ef-
fort is modeled more accurately by AHsent
� than by
AHsuf
� is that the effect size of AHsent
� seems less
affected by n. Even though AH, the reduction in
entropy over complete sentences, is approximated
more closely as suffix length grows, increasing n
is strongly detrimental to the effect of AHsuf
� : It
is no longer significant for n &gt; 2. Presumably,
this can be (partly) attributed to the impoverished
relation between formal entropy and psychologi-
cal uncertainty, as explained in Section 3.3.1. In
any case, the effect of AHsent
� is more stable. Al-
though AHsuf
� and AHsent
� necessarily converge as
n —* oc, the two effect sizes seem to diverge up to
n = 3: The difference between the effect sizes
of AHsent
� and AHsuf
� is marginally significantly
(p &lt; .07) larger for n = 3 than for n = 2.
</bodyText>
<subsectionHeader confidence="0.999457">
4.2 Effects of other factors
</subsectionHeader>
<bodyText confidence="0.995442035714285">
It is also of interest that surprisal has a significant
effect over and above entropy reduction, in the cor-
rect (i.e., positive) direction. When surprisal esti-
mates are added to a regression model that already
contains AHsent
� , the effect size ranges from 8.7
for n = 1 to 13.9 for n = 4. This show that there
exist independent effects of surprisal and entropy
reduction on processing effort.
Be reminded from Section 2.3 that Roark et al.
(2009) found a positive relation between reading
time on wt+1 and H1(t + 1), the next-word en-
tropy after processing wt+1. When that value is
added as a predictor in the regression model that
already contains surprisal and entropy reduction
AHsent
� , model fit greatly improves. In fact, as can
be seen from comparing Figures 3 and 4, the ef-
fect of AHsent
� is strengthened by including next-
word entropy in the regression model. Moreover,
each of the factors surprisal, entropy reduction,
and next-word entropy has a significant effect over
and above the other two. In all cases, these ef-
fects were in the positive direction. This confirms
Roark et al.’s finding and shows that it is in fact
compatible with the entropy-reduction hypothesis,
in contrast to what was suggested in Section 2.3.
</bodyText>
<page confidence="0.99582">
86
</page>
<sectionHeader confidence="0.996796" genericHeader="method">
5 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999997054347827">
The current results contribute to a growing body of
evidence that the amount of information conveyed
by a word in sentence context is indicative of the
amount of cognitive effort required for processing,
as can be observed from reading time on the word.
Several previous studies have shown that surprisal
can serve as a cognitively relevant measure for a
word’s information content. In contrast, the rele-
vance of entropy reduction as a cognitive measure
has not been investigated this thoroughly before.
Hale (2003; 2006) presents entropy-reduction ac-
counts of particular psycholinguistic phenomena,
but does not show that entropy reduction gener-
ally correlates with word-reading times. Roark et
al. (2009) presented data that could be taken as ev-
idence against the entropy-reduction hypothesis,
but the current paper showed that the next-word
entropy effect, found by Roark et al., is indepen-
dent of the entropy-reduction effect.
It is tempting to take the independent effects
of surprisal and entropy reduction as evidence
for two distinct cognitive representations or pro-
cesses, one related to surprisal, the other to en-
tropy reduction. However, it is very well possible
that these two information measures are merely
complementary formalizations of a single, cogni-
tively relevant notion of word information. Since
the quantitative results presented here provide no
evidence for either view, a more detailed qualita-
tive analysis is needed.
In addition, the relation between reading time
and the two measures of word information may
be further clarified by the development of mech-
anistic sentence-processing models. Both the sur-
prisal and entropy-reduction theories provide only
functional-level descriptions (Marr, 1982) of the
relation between information content and process-
ing effort, so the question remains which under-
lying mechanism is responsible for longer read-
ing times on words that convey more information.
That is, we are still without a model that pro-
poses, at Marr’s computational level, some spe-
cific sentence-processing mechanism that takes
longer to process a word that has higher surprisal
or leads to greater reduction in sentence entropy.
For surprisal, Levy (2008) makes a first step in
that direction by presenting a mechanistic account
of why surprisal would predict word-reading time:
If the state of the sentence-processing system is
viewed as a probability distribution over all possi-
ble interpretations of complete sentences, and pro-
cessing a word comes down to updating this distri-
bution to incorporate the new information, then the
word’s surprisal equals the Kullback-Leibler di-
vergence from the old distribution to the new. This
divergence is presumed to quantify the amount of
work (and, therefore, time) needed to update the
distribution. Likewise, Smith and Levy (2008) ex-
plain the surprisal effect in terms of a reader’s opti-
mal preparation to incoming input. When it comes
to entropy reduction, however, no reading-time
predicting mechanism has been proposed. Ideally,
of course, there should be a single computational-
level model that predicts the effects of both sur-
prisal and entropy reduction.
One recent model (Frank, 2010) shows that the
reading-time effects of both surprisal and entropy
reduction can indeed result from a single pro-
cessing mechanism. The model simulates sen-
tence comprehension as the incremental and dy-
namical update of a non-linguistic representation
of the state-of-affairs described by the sentence.
In this framework, surprisal and entropy reduc-
tion are defined with respect to a probabilistic
model of the world, rather than a model of the
language: The amount of information conveyed
by a word depends on what is asserted by the
sentence-so-far, and not on how the sentence’s
form matches the statistical patterns of the lan-
guage. As it turns out, word-processing times in
the sentence-comprehension model correlate pos-
itively with both surprisal and entropy reduction.
The model thereby forms a computational-level
account of the relation between reading time and
both measures of word information. According
to this account, the two information measures do
not correspond to two distinct cognitive processes.
Rather, there is one comprehension mechanism
that is responsible for the incremental revision of
a mental representation. Surprisal and entropy re-
duction form two complementary quantifications
of the extent of this revision.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997795">
The research presented here was supported by
grant 277-70-006 of the Netherlands Organization
for Scientific Research (NWO). I would like to
thank Rens Bod, Reut Tsarfaty, and two anony-
mous reviewers for their helpful comments.
</bodyText>
<page confidence="0.998528">
87
</page>
<sectionHeader confidence="0.998348" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.999959393442623">
M. F. Boston, J. Hale, U. Patil, R. Kliegl, and S. Va-
sishth. 2008. Parsing costs as predictors of read-
ing difficulty: An evaluation using the Potsdam Sen-
tence Corpus. Journal of Eye Movement Research,
2:1–12.
M. H. Christiansen and M. C. MacDonald. 2009. A
usage-based approach to recursion in sentence pro-
cessing. Language Learning, 59:129–164.
V. Demberg and F. Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109:193–210.
B. Efron and R. Tibshirani. 1986. Bootstrap methods
for standard errors, confidence intervals, and other
measures of statistical accuracy. Statistical Science,
1:54–75.
J. L. Elman. 1990. Finding structure in time. Cogni-
tive Science, 14:179–211.
S. L. Frank. 2009. Surprisal-based comparison be-
tween a symbolic and a connectionist model of sen-
tence processing. In N. A. Taatgen and H. van Rijn,
editors, Proceedings of the 31st Annual Conference
of the Cognitive Science Society, pages 1139–1144.
Austin, TX: Cognitive Science Society.
S. L. Frank. 2010. The role of world knowledge in
sentence comprehension: an information-theoretic
analysis and a connectionist simulation. Manuscript
in preparation.
J. Hale. 2001. A probabilistic Early parser as a psy-
cholinguistic model. In Proceedings of the sec-
ond conference of the North American chapter of
the Association for Computational Linguistics, vol-
ume 2, pages 159–166. Pittsburgh, PA: Association
for Computational Linguistics.
J. Hale. 2003. The information conveyed by words.
Journal of Psycholinguistic Research, 32:101–123.
J. Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30:643–672.
A. Kennedy and J. Pynte. 2005. Parafoveal-on-foveal
effects in normal reading. Vision Research, 45:153–
168.
R. Levy. 2008. Expectation-based syntactic compre-
hension. Cognition, 106:1126–1177.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the Penn Treebank. Computational Linguis-
tics, 19:313–330.
D. Marr. 1982. Vision. San Francisco: W.H. Freeman
and Company.
B. Roark, A. Bachrach, C. Cardenas, and C. Pallier.
2009. Deriving lexical and syntactic expectation-
based measures for psycholinguistic modeling via
incremental top-down parsing. In Proceedings of
the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 324–333. Associ-
ation for Computational Linguistics.
N. J. Smith and R. Levy. 2008. Optimal processing
times in reading: a formal model and empirical in-
vestigation. In B. C. Love, K. McRae, and V. M.
Sloutsky, editors, Proceedings of the 30th Annual
Conference of the Cognitive Science Society, pages
595–600. Austin, TX: Cognitive Science Society.
</reference>
<page confidence="0.999743">
88
</page>
<sectionHeader confidence="0.997622" genericHeader="references">
Appendix
</sectionHeader>
<bodyText confidence="0.999676">
It is of some interest that Hn can be expressed in
terms of H1 and the expected value of Hn−1. First,
note that
</bodyText>
<equation confidence="0.99721959375">
h(wj t+1|wt 1) = −P(wj t+1|wt 1) log P(wj t+1|wt 1) = −P(wt+1 |wt1)P(wjt+2 |w1+1) log (P(wt+1  |wt1)P(wjt+2 |wi+1))
= P(wj t+2|wt+1
1 )h(wt+1|wt 1) + P(wt+1|wt 1)h(wj t+2|wt+1
1 ).
For entropy Hn(t), this makes
Hn(t) = � h(wjt+1|wt1)
wjt+1EWn � P(wt+1|wt1)h(wjt+2|wt+1
� + 1 )
= P(wj t+2|wt+1
1 )h(wt+1|wt 1)
wjt+1EWn wjt+1EWn
+ � P(wt+1|wt1)Hn−1(t + 1)
wt+1EW1
�
� �
+ �
P(wt+1 |wt1) 1:h(wjt+2 |wi+1)
wt+1EW1 wjt+2EWn−1
�
���h(wt+1|wt1)
wjt+2EW
h(wt+1|wt1)
�
�
P (wj t+2|wt+1
1 ) �
n−1
�=
wt+1EW1
�=
wt+1EW1
= H1(t) + E(Hn−1(t + 1)).
</equation>
<page confidence="0.997591">
89
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.583015">
<title confidence="0.998761">Uncertainty reduction as a measure of cognitive processing effort</title>
<author confidence="0.999948">L Stefan</author>
<affiliation confidence="0.998075">University of</affiliation>
<address confidence="0.603661">Amsterdam, The</address>
<email confidence="0.986368">s.l.frank@uva.nl</email>
<abstract confidence="0.997392875">The amount of cognitive effort required to process a word has been argued to depend on the word’s effect on the uncertainty about the incoming sentence, as quantified by the entropy over sentence probabilities. The current paper tests this hypothesis more thoroughly than has been done before by using recurrent neural networks for entropy-reduction estimation. A comparison between these estimates and wordreading times shows that entropy reduction is positively related to processing effort, confirming the entropy-reduction hypothesis. This effect is independent from the effect of surprisal.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M F Boston</author>
<author>J Hale</author>
<author>U Patil</author>
<author>R Kliegl</author>
<author>S Vasishth</author>
</authors>
<title>Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus.</title>
<date>2008</date>
<journal>Journal of Eye Movement Research,</journal>
<pages>2--1</pages>
<contexts>
<context position="1629" citStr="Boston et al., 2008" startWordPosition="255" endWordPosition="258">mation is assumed to involve more cognitive effort, which is reflected in the time required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively t</context>
</contexts>
<marker>Boston, Hale, Patil, Kliegl, Vasishth, 2008</marker>
<rawString>M. F. Boston, J. Hale, U. Patil, R. Kliegl, and S. Vasishth. 2008. Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. Journal of Eye Movement Research, 2:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Christiansen</author>
<author>M C MacDonald</author>
</authors>
<title>A usage-based approach to recursion in sentence processing.</title>
<date>2009</date>
<booktitle>Language Learning,</booktitle>
<pages>59--129</pages>
<contexts>
<context position="12058" citStr="Christiansen and MacDonald, 2009" startWordPosition="2064" endWordPosition="2067">t they do not form particularly accurate language models. Moreover, Markov models lack cognitive plausibility. Here, Simple Recurrent Networks (SRNs) (Elman, 1990) are used as language models. When trained to predict the upcoming input in a word sequence, these networks can generate estimates of H1(t) − H0(t + 1) = H1(t), 83 P(wt+1|wt1) efficiently and relatively accurately. They thereby allow to approximate sentence entropy more closely than the incremental parsers used in previous studies. Unlike Markov models, SRNs have been claimed to form cognitively realistic sentence-processing models (Christiansen and MacDonald, 2009). Moreover, it has been shown that SRN-based surprisal estimates can correlate more strongly to reading times than surprisal values estimated by a phrase-structure grammar (Frank, 2009). 3.2.1 Network architecture and processing The SRNs comprised three layers of units: the input layer, the recurrent (hidden) layer, and the output layer. Each input unit corresponds to one POS tag, making 45 input units since there are 45 different POS tags in the WSJ corpus. The network’s output units represent predictions of subsequent inputs. The output layer also has one unit for each POS tag, plus an extra</context>
</contexts>
<marker>Christiansen, MacDonald, 2009</marker>
<rawString>M. H. Christiansen and M. C. MacDonald. 2009. A usage-based approach to recursion in sentence processing. Language Learning, 59:129–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Demberg</author>
<author>F Keller</author>
</authors>
<title>Data from eyetracking corpora as evidence for theories of syntactic processing complexity.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>109--193</pages>
<contexts>
<context position="1655" citStr="Demberg and Keller, 2008" startWordPosition="259" endWordPosition="262">involve more cognitive effort, which is reflected in the time required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of</context>
<context position="9332" citStr="Demberg and Keller, 2008" startWordPosition="1614" endWordPosition="1617">ed, in our data set, H1(t + 1) and AHsent � (t + 1) correlate between r = −.29 and r = −.26 (for n = 2 to n = 4) which is highly significantly (p Pz� 0) different from 0. Roark et al.’s finding of a positive relation between H1(t + 1) and reading time on wt+1 therefore seems to disconfirm the entropyreduction hypothesis. 3 Method A set of language models was trained on a corpus of POS tags of sentences. The advantage of using POS tags rather than words is that their probabilities can be estimated much more accurately and, consequently, more accurate prediction of wordreading time is possible (Demberg and Keller, 2008; Roark et al., 2009). Subsequent to training, the models were made to generate estimates of surprisal and entropy reductions AHsuf � and AHsent � over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric. 3.1 Data The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993). They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that in</context>
</contexts>
<marker>Demberg, Keller, 2008</marker>
<rawString>V. Demberg and F. Keller. 2008. Data from eyetracking corpora as evidence for theories of syntactic processing complexity. Cognition, 109:193–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
</authors>
<title>Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy.</title>
<date>1986</date>
<journal>Statistical Science,</journal>
<pages>1--54</pages>
<contexts>
<context position="20748" citStr="Efron and Tibshirani, 1986" startWordPosition="3570" endWordPosition="3573">Hsent � is larger than that of AHsuf � , indicating that it is indeed uncertainty about the identity of the current sentence, rather than uncertainty about the upcoming input(s), that matters for cognitive processing effort. Only at n = 1 was the effect size of AHsent � smaller than that of AHsuf � , but it should be kept in mind that AHsent 1 is independent of the incoming word and is therefore quite impoverished as a measure of the effort involved in processing the word. Moreover, the difference between AHsent 1 and AHsuf 1 is not significant (p &gt; .4), as determined by the bootstrap method (Efron and Tibshirani, 1986). In contrast, the differences are significant when n &gt; 1 (all p &lt; .01), in spite of the high correlation between AHsent � and AHsuf � . Another indication that cognitive processing effort is modeled more accurately by AHsent � than by AHsuf � is that the effect size of AHsent � seems less affected by n. Even though AH, the reduction in entropy over complete sentences, is approximated more closely as suffix length grows, increasing n is strongly detrimental to the effect of AHsuf � : It is no longer significant for n &gt; 2. Presumably, this can be (partly) attributed to the impoverished relation</context>
</contexts>
<marker>Efron, Tibshirani, 1986</marker>
<rawString>B. Efron and R. Tibshirani. 1986. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical Science, 1:54–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive Science,</journal>
<pages>14--179</pages>
<contexts>
<context position="11588" citStr="Elman, 1990" startWordPosition="1993" endWordPosition="1995">mates of the occurrence probabilities at t + 1 of all word types, rather than just of the actual next word. Moreover, the number of suffixes rises exponentially as suffix length n grows, and, consequently, so does computation time. Roark et al. (2009) used an incremental PCFG parser to obtain H1 but this method rapidly becomes infeasible as n grows. Low-order Markov models (e.g., a bigram model) are more efficient and can be used for larger n but they do not form particularly accurate language models. Moreover, Markov models lack cognitive plausibility. Here, Simple Recurrent Networks (SRNs) (Elman, 1990) are used as language models. When trained to predict the upcoming input in a word sequence, these networks can generate estimates of H1(t) − H0(t + 1) = H1(t), 83 P(wt+1|wt1) efficiently and relatively accurately. They thereby allow to approximate sentence entropy more closely than the incremental parsers used in previous studies. Unlike Markov models, SRNs have been claimed to form cognitively realistic sentence-processing models (Christiansen and MacDonald, 2009). Moreover, it has been shown that SRN-based surprisal estimates can correlate more strongly to reading times than surprisal value</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>J. L. Elman. 1990. Finding structure in time. Cognitive Science, 14:179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
</authors>
<title>Surprisal-based comparison between a symbolic and a connectionist model of sentence processing.</title>
<date>2009</date>
<booktitle>Proceedings of the 31st Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1139--1144</pages>
<editor>In N. A. Taatgen and H. van Rijn, editors,</editor>
<publisher>Cognitive Science Society.</publisher>
<location>Austin, TX:</location>
<contexts>
<context position="1668" citStr="Frank, 2009" startWordPosition="263" endWordPosition="264">ort, which is reflected in the time required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficul</context>
<context position="10033" citStr="Frank, 2009" startWordPosition="1734" endWordPosition="1735">of surprisal and entropy reductions AHsuf � and AHsent � over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric. 3.1 Data The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993). They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that investigate the relation between word surprisal and reading time (Demberg and Keller, 2008; Frank, 2009; Smith and Levy, 2008). This 2368-sentence (51501 words) collection of British newspaper editorials comes with eye-tracking data of 10 participants. POS tags for the Dundee corpus were taken from Frank (2009). For each word and each participant, reading time was defined as the total fixation time on that word before any fixation on a later word of the same sentence. Following Demberg and Keller (2008), data points (i.e., word/participant pairs) were removed if the word was not fixated, was presented as the first or last on a line, contained more than one capital letter or a non-letter (e.g., </context>
<context position="12243" citStr="Frank, 2009" startWordPosition="2093" endWordPosition="2094">to predict the upcoming input in a word sequence, these networks can generate estimates of H1(t) − H0(t + 1) = H1(t), 83 P(wt+1|wt1) efficiently and relatively accurately. They thereby allow to approximate sentence entropy more closely than the incremental parsers used in previous studies. Unlike Markov models, SRNs have been claimed to form cognitively realistic sentence-processing models (Christiansen and MacDonald, 2009). Moreover, it has been shown that SRN-based surprisal estimates can correlate more strongly to reading times than surprisal values estimated by a phrase-structure grammar (Frank, 2009). 3.2.1 Network architecture and processing The SRNs comprised three layers of units: the input layer, the recurrent (hidden) layer, and the output layer. Each input unit corresponds to one POS tag, making 45 input units since there are 45 different POS tags in the WSJ corpus. The network’s output units represent predictions of subsequent inputs. The output layer also has one unit for each POS tag, plus an extra unit that represents &lt;/s&gt;, that is, the absence of any further input. Hence, there were 46 output units. The number of recurrent units was fairly arbitrarily set to 100. As is common i</context>
</contexts>
<marker>Frank, 2009</marker>
<rawString>S. L. Frank. 2009. Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. In N. A. Taatgen and H. van Rijn, editors, Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 1139–1144. Austin, TX: Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Frank</author>
</authors>
<title>The role of world knowledge in sentence comprehension: an information-theoretic analysis and a connectionist simulation. Manuscript in preparation.</title>
<date>2010</date>
<contexts>
<context position="26194" citStr="Frank, 2010" startWordPosition="4469" endWordPosition="4470"> then the word’s surprisal equals the Kullback-Leibler divergence from the old distribution to the new. This divergence is presumed to quantify the amount of work (and, therefore, time) needed to update the distribution. Likewise, Smith and Levy (2008) explain the surprisal effect in terms of a reader’s optimal preparation to incoming input. When it comes to entropy reduction, however, no reading-time predicting mechanism has been proposed. Ideally, of course, there should be a single computationallevel model that predicts the effects of both surprisal and entropy reduction. One recent model (Frank, 2010) shows that the reading-time effects of both surprisal and entropy reduction can indeed result from a single processing mechanism. The model simulates sentence comprehension as the incremental and dynamical update of a non-linguistic representation of the state-of-affairs described by the sentence. In this framework, surprisal and entropy reduction are defined with respect to a probabilistic model of the world, rather than a model of the language: The amount of information conveyed by a word depends on what is asserted by the sentence-so-far, and not on how the sentence’s form matches the stat</context>
</contexts>
<marker>Frank, 2010</marker>
<rawString>S. L. Frank. 2010. The role of world knowledge in sentence comprehension: an information-theoretic analysis and a connectionist simulation. Manuscript in preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>A probabilistic Early parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of the second conference of the North American chapter of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>159--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Pittsburgh, PA:</location>
<contexts>
<context position="1227" citStr="Hale, 2001" startWordPosition="190" endWordPosition="191">ively related to processing effort, confirming the entropy-reduction hypothesis. This effect is independent from the effect of surprisal. 1 Introduction In the field of computational psycholinguistics, a currently popular approach is to account for reading times on a sentence’s words by estimates of the amount of information conveyed by these words. Processing a word that conveys more information is assumed to involve more cognitive effort, which is reflected in the time required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about th</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>J. Hale. 2001. A probabilistic Early parser as a psycholinguistic model. In Proceedings of the second conference of the North American chapter of the Association for Computational Linguistics, volume 2, pages 159–166. Pittsburgh, PA: Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>The information conveyed by words.</title>
<date>2003</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>32--101</pages>
<contexts>
<context position="1875" citStr="Hale (2003" startWordPosition="297" endWordPosition="298">rt for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficulty of computing the required entropies. Although Hale (2006) shows how sentence entropy can be computed given a PCFG, this computation is not feasible when the grammar is of realistic size. Here, we empirica</context>
<context position="23542" citStr="Hale (2003" startWordPosition="4064" endWordPosition="4065">on hypothesis, in contrast to what was suggested in Section 2.3. 86 5 Discussion and conclusion The current results contribute to a growing body of evidence that the amount of information conveyed by a word in sentence context is indicative of the amount of cognitive effort required for processing, as can be observed from reading time on the word. Several previous studies have shown that surprisal can serve as a cognitively relevant measure for a word’s information content. In contrast, the relevance of entropy reduction as a cognitive measure has not been investigated this thoroughly before. Hale (2003; 2006) presents entropy-reduction accounts of particular psycholinguistic phenomena, but does not show that entropy reduction generally correlates with word-reading times. Roark et al. (2009) presented data that could be taken as evidence against the entropy-reduction hypothesis, but the current paper showed that the next-word entropy effect, found by Roark et al., is independent of the entropy-reduction effect. It is tempting to take the independent effects of surprisal and entropy reduction as evidence for two distinct cognitive representations or processes, one related to surprisal, the ot</context>
</contexts>
<marker>Hale, 2003</marker>
<rawString>J. Hale. 2003. The information conveyed by words. Journal of Psycholinguistic Research, 32:101–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hale</author>
</authors>
<title>Uncertainty about the rest of the sentence.</title>
<date>2006</date>
<journal>Cognitive Science,</journal>
<pages>30--643</pages>
<contexts>
<context position="2328" citStr="Hale (2006)" startWordPosition="368" endWordPosition="369">sser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficulty of computing the required entropies. Although Hale (2006) shows how sentence entropy can be computed given a PCFG, this computation is not feasible when the grammar is of realistic size. Here, we empirically investigate the entropyreduction hypothesis more thoroughly than has been done before, by using recurrent neural networks as language models. Since these networks do not derive any structure, they provide estimates of sentence entropy rather than sentence-structure entropy. In practice, these two entropies will generally be similar: If the rest of the sentence is highly uncertain, so is its structure. Sentence entropy can therefore be viewed as </context>
<context position="4840" citStr="Hale (2006)" startWordPosition="804" endWordPosition="805">jwt1)log P(wj1jwt1). (1) wj1ES In order to simplify later equations, we define the function h(yjx) = −P(yjx) log P(yjx), such that Eq. 1 becomes H(t) = X h(wj1jwt1). wj1ES If the first t words of wj1 do not equal wt1 (or wj1 has fewer than t + 1 words),1 then P(wj1jwt1) = 0 so h(wj1jwt1) = 0. This means that, for computing H(t), only the words from t + 1 onwards need to be taken into account: H(t) = X h(wjt+1jwt1). wjt+1ES The reduction in entropy due to processing the next word, wt+1, is AH(t + 1) = H(t) − H(t + 1). (2) Note that positive AH corresponds to a decrease in entropy. According to Hale (2006), the nonnegative reduction in entropy (i.e., max{0, AH}) reflects the cognitive effort involved in processing wt+1 and should therefore be predictive of reading time on that word. 2.2 Suffix entropy Computing H(t) is computationally feasible only when there are very few sentences in S, or when the language can be described by a small grammar. To estimate entropy in more realistic situations, an 1Since wj1 ends with &lt; /s &gt; and wt1 does not, the two strings must be different. Consequently, if wj1 is t words long, then P(wj1|wt1� = 0. obvious solution is to look only at the next few words instea</context>
<context position="16562" citStr="Hale (2006)" startWordPosition="2849" endWordPosition="2850">1 84 0.5 0.25 0 correlation with surprisal 100 Significance (p−value) 10−1 10−2 10−3 10−4 1 2 3 4 suffix length n Figure 1: Coefficient of correlation between estimates of surprisal and entropy reduction, as a function of suffix length n. where E(x) is the expected value of x. Obviously, the expected value of Hn_1 is less appropriate as an uncertainty measure than is Hn_1 itself. Hence, Hn can be less accurate than Hn_1 as a quantification of the actual cognitive uncertainty. For this reason, we may expect larger n to result in worse fit to reading-time data.3 3.3.2 Negative entropy reduction Hale (2006) argued for nonnegative entropy reduction max{0, AH}, rather than AH itself, as a measure of processing effort. For AHsent, the difference between the two is negligible because only about 0.03% of entropy reductions are negative. As for AHsuf, approximately 42% of values are negative so whether these are left out makes quite a difference. Since preliminary experiments showed that word-reading times are predicted much more accurately by AHsuf than by max{0, AHsuf}, only AHsuf and AHsent were used here, that is, negative values were included. 3.3.3 Relation between information measures Both surp</context>
</contexts>
<marker>Hale, 2006</marker>
<rawString>J. Hale. 2006. Uncertainty about the rest of the sentence. Cognitive Science, 30:643–672.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kennedy</author>
<author>J Pynte</author>
</authors>
<title>Parafoveal-on-foveal effects in normal reading.</title>
<date>2005</date>
<journal>Vision Research,</journal>
<volume>45</volume>
<pages>168</pages>
<contexts>
<context position="9884" citStr="Kennedy and Pynte, 2005" startWordPosition="1709" endWordPosition="1712">ate prediction of wordreading time is possible (Demberg and Keller, 2008; Roark et al., 2009). Subsequent to training, the models were made to generate estimates of surprisal and entropy reductions AHsuf � and AHsent � over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric. 3.1 Data The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993). They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that investigate the relation between word surprisal and reading time (Demberg and Keller, 2008; Frank, 2009; Smith and Levy, 2008). This 2368-sentence (51501 words) collection of British newspaper editorials comes with eye-tracking data of 10 participants. POS tags for the Dundee corpus were taken from Frank (2009). For each word and each participant, reading time was defined as the total fixation time on that word before any fixation on a later word of the same sentence. Following Demberg and Keller (2008), data points (i.e., word/participant pairs) w</context>
</contexts>
<marker>Kennedy, Pynte, 2005</marker>
<rawString>A. Kennedy and J. Pynte. 2005. Parafoveal-on-foveal effects in normal reading. Vision Research, 45:153– 168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
</authors>
<title>Expectation-based syntactic comprehension.</title>
<date>2008</date>
<journal>Cognition,</journal>
<pages>106--1126</pages>
<contexts>
<context position="1240" citStr="Levy, 2008" startWordPosition="192" endWordPosition="193">d to processing effort, confirming the entropy-reduction hypothesis. This effect is independent from the effect of surprisal. 1 Introduction In the field of computational psycholinguistics, a currently popular approach is to account for reading times on a sentence’s words by estimates of the amount of information conveyed by these words. Processing a word that conveys more information is assumed to involve more cognitive effort, which is reflected in the time required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the</context>
<context position="10056" citStr="Levy, 2008" startWordPosition="1738" endWordPosition="1739">y reductions AHsuf � and AHsent � over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric. 3.1 Data The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993). They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that investigate the relation between word surprisal and reading time (Demberg and Keller, 2008; Frank, 2009; Smith and Levy, 2008). This 2368-sentence (51501 words) collection of British newspaper editorials comes with eye-tracking data of 10 participants. POS tags for the Dundee corpus were taken from Frank (2009). For each word and each participant, reading time was defined as the total fixation time on that word before any fixation on a later word of the same sentence. Following Demberg and Keller (2008), data points (i.e., word/participant pairs) were removed if the word was not fixated, was presented as the first or last on a line, contained more than one capital letter or a non-letter (e.g., the apostrophe in a cli</context>
<context position="25216" citStr="Levy (2008)" startWordPosition="4316" endWordPosition="4317"> mechanistic sentence-processing models. Both the surprisal and entropy-reduction theories provide only functional-level descriptions (Marr, 1982) of the relation between information content and processing effort, so the question remains which underlying mechanism is responsible for longer reading times on words that convey more information. That is, we are still without a model that proposes, at Marr’s computational level, some specific sentence-processing mechanism that takes longer to process a word that has higher surprisal or leads to greater reduction in sentence entropy. For surprisal, Levy (2008) makes a first step in that direction by presenting a mechanistic account of why surprisal would predict word-reading time: If the state of the sentence-processing system is viewed as a probability distribution over all possible interpretations of complete sentences, and processing a word comes down to updating this distribution to incorporate the new information, then the word’s surprisal equals the Kullback-Leibler divergence from the old distribution to the new. This divergence is presumed to quantify the amount of work (and, therefore, time) needed to update the distribution. Likewise, Smi</context>
</contexts>
<marker>Levy, 2008</marker>
<rawString>R. Levy. 2008. Expectation-based syntactic comprehension. Cognition, 106:1126–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="9805" citStr="Marcus et al., 1993" startWordPosition="1697" endWordPosition="1700">ilities can be estimated much more accurately and, consequently, more accurate prediction of wordreading time is possible (Demberg and Keller, 2008; Roark et al., 2009). Subsequent to training, the models were made to generate estimates of surprisal and entropy reductions AHsuf � and AHsent � over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric. 3.1 Data The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993). They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that investigate the relation between word surprisal and reading time (Demberg and Keller, 2008; Frank, 2009; Smith and Levy, 2008). This 2368-sentence (51501 words) collection of British newspaper editorials comes with eye-tracking data of 10 participants. POS tags for the Dundee corpus were taken from Frank (2009). For each word and each participant, reading time was defined as the total fixation time on that word before any fixation on a later word of the same sentence. Fo</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of english: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marr</author>
</authors>
<date>1982</date>
<publisher>W.H. Freeman and Company.</publisher>
<location>Vision. San Francisco:</location>
<contexts>
<context position="24751" citStr="Marr, 1982" startWordPosition="4242" endWordPosition="4243">other to entropy reduction. However, it is very well possible that these two information measures are merely complementary formalizations of a single, cognitively relevant notion of word information. Since the quantitative results presented here provide no evidence for either view, a more detailed qualitative analysis is needed. In addition, the relation between reading time and the two measures of word information may be further clarified by the development of mechanistic sentence-processing models. Both the surprisal and entropy-reduction theories provide only functional-level descriptions (Marr, 1982) of the relation between information content and processing effort, so the question remains which underlying mechanism is responsible for longer reading times on words that convey more information. That is, we are still without a model that proposes, at Marr’s computational level, some specific sentence-processing mechanism that takes longer to process a word that has higher surprisal or leads to greater reduction in sentence entropy. For surprisal, Levy (2008) makes a first step in that direction by presenting a mechanistic account of why surprisal would predict word-reading time: If the stat</context>
</contexts>
<marker>Marr, 1982</marker>
<rawString>D. Marr. 1982. Vision. San Francisco: W.H. Freeman and Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>A Bachrach</author>
<author>C Cardenas</author>
<author>C Pallier</author>
</authors>
<title>Deriving lexical and syntactic expectationbased measures for psycholinguistic modeling via incremental top-down parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>324--333</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1688" citStr="Roark et al., 2009" startWordPosition="265" endWordPosition="268"> reflected in the time required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficulty of computing the </context>
<context position="8263" citStr="Roark et al. (2009)" startWordPosition="1419" endWordPosition="1422">we should find that wordreading times are predicted more accurately by AHsent � than by AHsuf �. 2.3 Relation to next-word entropy In the extreme case of n = 1, Eq. 4 reduces to AHsent 1 (t + 1) = so the reduction of entropy over the single next word wt+1 equals the next-word entropy just before processing that word. Note that AHsent 1 (t+1) is independent of the word at t + 1, making it a severely impoverished measure of the uncertainty reduction caused by that word. We would therefore expect reading times to be predicted more accurately by AHsent � with n &gt; 1, and possibly even by AHsuf 1 . Roark et al. (2009) investigated the relation between H1(t + 1) and reading time on wt+1, and found a significant positive effect: Larger nextword entropy directly after processing wt+1 corresponded to longer reading time on that word. This is of particular interest because H1(t + 1) necessarily correlates negatively with entropy reduction AHsent � (t + 1): If entropy is large after wt+1, chances are that it did not reduce much through processing of wt+1. Indeed, in our data set, H1(t + 1) and AHsent � (t + 1) correlate between r = −.29 and r = −.26 (for n = 2 to n = 4) which is highly significantly (p Pz� 0) di</context>
<context position="11227" citStr="Roark et al. (2009)" startWordPosition="1934" endWordPosition="1937">r or a non-letter (e.g., the apostrophe in a clitic), or was attached to punctuation. Mainly due to the large number (over 46%) of nonfixations, 62.8% of data points were removed, leaving 191380 data points (between 16 469 and 21770 per participant). 3.2 Language model Entropy is more time consuming to compute than surprisal, even for n = 1, because it requires estimates of the occurrence probabilities at t + 1 of all word types, rather than just of the actual next word. Moreover, the number of suffixes rises exponentially as suffix length n grows, and, consequently, so does computation time. Roark et al. (2009) used an incremental PCFG parser to obtain H1 but this method rapidly becomes infeasible as n grows. Low-order Markov models (e.g., a bigram model) are more efficient and can be used for larger n but they do not form particularly accurate language models. Moreover, Markov models lack cognitive plausibility. Here, Simple Recurrent Networks (SRNs) (Elman, 1990) are used as language models. When trained to predict the upcoming input in a word sequence, these networks can generate estimates of H1(t) − H0(t + 1) = H1(t), 83 P(wt+1|wt1) efficiently and relatively accurately. They thereby allow to ap</context>
<context position="22211" citStr="Roark et al. (2009)" startWordPosition="3840" endWordPosition="3843">n = 3: The difference between the effect sizes of AHsent � and AHsuf � is marginally significantly (p &lt; .07) larger for n = 3 than for n = 2. 4.2 Effects of other factors It is also of interest that surprisal has a significant effect over and above entropy reduction, in the correct (i.e., positive) direction. When surprisal estimates are added to a regression model that already contains AHsent � , the effect size ranges from 8.7 for n = 1 to 13.9 for n = 4. This show that there exist independent effects of surprisal and entropy reduction on processing effort. Be reminded from Section 2.3 that Roark et al. (2009) found a positive relation between reading time on wt+1 and H1(t + 1), the next-word entropy after processing wt+1. When that value is added as a predictor in the regression model that already contains surprisal and entropy reduction AHsent � , model fit greatly improves. In fact, as can be seen from comparing Figures 3 and 4, the effect of AHsent � is strengthened by including nextword entropy in the regression model. Moreover, each of the factors surprisal, entropy reduction, and next-word entropy has a significant effect over and above the other two. In all cases, these effects were in the </context>
<context position="23734" citStr="Roark et al. (2009)" startWordPosition="4088" endWordPosition="4091">tion conveyed by a word in sentence context is indicative of the amount of cognitive effort required for processing, as can be observed from reading time on the word. Several previous studies have shown that surprisal can serve as a cognitively relevant measure for a word’s information content. In contrast, the relevance of entropy reduction as a cognitive measure has not been investigated this thoroughly before. Hale (2003; 2006) presents entropy-reduction accounts of particular psycholinguistic phenomena, but does not show that entropy reduction generally correlates with word-reading times. Roark et al. (2009) presented data that could be taken as evidence against the entropy-reduction hypothesis, but the current paper showed that the next-word entropy effect, found by Roark et al., is independent of the entropy-reduction effect. It is tempting to take the independent effects of surprisal and entropy reduction as evidence for two distinct cognitive representations or processes, one related to surprisal, the other to entropy reduction. However, it is very well possible that these two information measures are merely complementary formalizations of a single, cognitively relevant notion of word informa</context>
</contexts>
<marker>Roark, Bachrach, Cardenas, Pallier, 2009</marker>
<rawString>B. Roark, A. Bachrach, C. Cardenas, and C. Pallier. 2009. Deriving lexical and syntactic expectationbased measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 324–333. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Smith</author>
<author>R Levy</author>
</authors>
<title>Optimal processing times in reading: a formal model and empirical investigation.</title>
<date>2008</date>
<booktitle>Proceedings of the 30th Annual Conference of the Cognitive Science Society,</booktitle>
<pages>595--600</pages>
<editor>In B. C. Love, K. McRae, and V. M. Sloutsky, editors,</editor>
<publisher>Cognitive Science Society.</publisher>
<location>Austin, TX:</location>
<contexts>
<context position="1711" citStr="Smith and Levy, 2008" startWordPosition="269" endWordPosition="272">me required to read the word. In this context, the most common formalization of a word’s information content is its surprisal (Hale, 2001; Levy, 2008). If word string wt1 (short for w1, w2, ... wt) is the sentence so far and P(wt+1jwt1) the occurrence probability of the next word wt+1, then that word’s surprisal is defined as − log P(wt+1jwt1). It is well established by now that word-reading times indeed correlate positively with surprisal values as estimated by any sufficiently accurate generative language model (Boston et al., 2008; Demberg and Keller, 2008; Frank, 2009; Roark et al., 2009; Smith and Levy, 2008). A lesser known alternative operationalization of a word’s information content is based on the uncertainty about the rest of the sentence, quantified by Hale (2003, 2006) as the entropy of the probability distribution over possible sentence structures. The reduction in entropy that results from processing a word is taken to be the amount of information conveyed by that word, and was argued by Hale to be predictive of word-reading time. However, this entropy-reduction hypothesis has not yet been comprehensively tested, possibly because of the difficulty of computing the required entropies. Alt</context>
<context position="10056" citStr="Smith and Levy, 2008" startWordPosition="1736" endWordPosition="1739">and entropy reductions AHsuf � and AHsent � over a test corpus. These estimates were then compared to reading times measured over the words of the same test corpus. This section presents the data sets that were used, language-model details, and the evaluation metric. 3.1 Data The models were trained on the POS tag sequences of the full WSJ corpus (Marcus et al., 1993). They were evaluated on the POS-tagged Dundee corpus (Kennedy and Pynte, 2005), which has been used in several studies that investigate the relation between word surprisal and reading time (Demberg and Keller, 2008; Frank, 2009; Smith and Levy, 2008). This 2368-sentence (51501 words) collection of British newspaper editorials comes with eye-tracking data of 10 participants. POS tags for the Dundee corpus were taken from Frank (2009). For each word and each participant, reading time was defined as the total fixation time on that word before any fixation on a later word of the same sentence. Following Demberg and Keller (2008), data points (i.e., word/participant pairs) were removed if the word was not fixated, was presented as the first or last on a line, contained more than one capital letter or a non-letter (e.g., the apostrophe in a cli</context>
<context position="25834" citStr="Smith and Levy (2008)" startWordPosition="4410" endWordPosition="4413">08) makes a first step in that direction by presenting a mechanistic account of why surprisal would predict word-reading time: If the state of the sentence-processing system is viewed as a probability distribution over all possible interpretations of complete sentences, and processing a word comes down to updating this distribution to incorporate the new information, then the word’s surprisal equals the Kullback-Leibler divergence from the old distribution to the new. This divergence is presumed to quantify the amount of work (and, therefore, time) needed to update the distribution. Likewise, Smith and Levy (2008) explain the surprisal effect in terms of a reader’s optimal preparation to incoming input. When it comes to entropy reduction, however, no reading-time predicting mechanism has been proposed. Ideally, of course, there should be a single computationallevel model that predicts the effects of both surprisal and entropy reduction. One recent model (Frank, 2010) shows that the reading-time effects of both surprisal and entropy reduction can indeed result from a single processing mechanism. The model simulates sentence comprehension as the incremental and dynamical update of a non-linguistic repres</context>
</contexts>
<marker>Smith, Levy, 2008</marker>
<rawString>N. J. Smith and R. Levy. 2008. Optimal processing times in reading: a formal model and empirical investigation. In B. C. Love, K. McRae, and V. M. Sloutsky, editors, Proceedings of the 30th Annual Conference of the Cognitive Science Society, pages 595–600. Austin, TX: Cognitive Science Society.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>