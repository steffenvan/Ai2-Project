<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000337">
<title confidence="0.960612">
Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers
</title>
<author confidence="0.924196">
Andr´e F. T. Martins*† Miguel B. Almeida*† Noah A. Smith#
</author>
<affiliation confidence="0.933232">
*Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal
#School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.998901">
{atm,mba}@priberam.pt, nasmith@cs.cmu.edu
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998744272727273">
We present fast, accurate, direct non-
projective dependency parsers with third-
order features. Our approach uses AD3,
an accelerated dual decomposition algo-
rithm which we extend to handle special-
ized head automata and sequential head
bigram models. Experiments in fourteen
languages yield parsing speeds competi-
tive to projective parsers, with state-of-
the-art accuracies for the largest datasets
(English, Czech, and German).
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999852925">
Dependency parsing has become a prominent ap-
proach to syntax in the last few years, with in-
creasingly fast and accurate models being devised
(K¨ubler et al., 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Rush and Petrov, 2012).
In projective parsing, the arcs in the dependency
tree are constrained to be nested, and the problem
of finding the best tree can be addressed with dy-
namic programming. This results in cubic-time
decoders for arc-factored and sibling second-order
models (Eisner, 1996; McDonald and Pereira,
2006), and quartic-time for grandparent models
(Carreras, 2007) and third-order models (Koo and
Collins, 2010). Recently, Rush and Petrov (2012)
trained third-order parsers with vine pruning cas-
cades, achieving runtimes only a small factor
slower than first-order systems. Third-order fea-
tures have also been included in transition systems
(Zhang and Nivre, 2011) and graph-based parsers
with cube-pruning (Zhang and McDonald, 2012).
Unfortunately, non-projective dependency
parsers (appropriate for languages with a more
flexible word order, such as Czech, Dutch, and
German) lag behind these recent advances. The
main obstacle is that non-projective parsing is
NP-hard beyond arc-factored models (McDonald
and Satta, 2007). Approximate parsers have there-
fore been introduced, based on belief propagation
(Smith and Eisner, 2008), dual decomposition
(Koo et al., 2010), or multi-commodity flows
(Martins et al., 2009, 2011). These are all in-
stances of turbo parsers, as shown by Martins et
al. (2010): the underlying approximations come
from the fact that they run global inference in
factor graphs ignoring loop effects. While this
line of research has led to accuracy gains, none of
these parsers use third-order contexts, and their
speeds are well behind those of projective parsers.
This paper bridges the gap above by presenting
the following contributions:
</bodyText>
<listItem confidence="0.838675777777778">
• We apply the third-order feature models of Koo
and Collins (2010) to non-projective parsing.
• This extension is non-trivial since exact dy-
namic programming is not applicable. Instead,
we adapt AD3, the dual decomposition algo-
rithm proposed by Martins et al. (2011), to han-
dle third-order features, by introducing special-
ized head automata.
• We make our parser substantially faster than the
</listItem>
<bodyText confidence="0.9089506">
many-components approach of Martins et al.
(2011). While AD3 requires solving quadratic
subproblems as an intermediate step, recent re-
sults (Martins et al., 2012) show that they can be
addressed with the same oracles used in the sub-
gradient method (Koo et al., 2010). This enables
AD3 to exploit combinatorial subproblems like
the the head automata above.
Along with this paper, we provide a free distribu-
tion of our parsers, including training code.1
</bodyText>
<sectionHeader confidence="0.954243" genericHeader="method">
2 Dependency Parsing with AD3
</sectionHeader>
<bodyText confidence="0.991671">
Dual decomposition is a class of optimization
techniques that tackle the dual of combinatorial
</bodyText>
<footnote confidence="0.9983305">
1Released as TurboParser 2.1, and publicly available at
http://www.ark.cs.cmu.edu/TurboParser.
</footnote>
<page confidence="0.854948">
617
</page>
<note confidence="0.6342825">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.846386">
Figure 1: Parts considered in this paper. First-
order models factor over arcs (Eisner, 1996; Mc-
Donald et al., 2005), and second-order models in-
clude also consecutive siblings and grandparents
</figureCaption>
<bodyText confidence="0.989582666666667">
(Carreras, 2007). Our parsers add also arbitrary
siblings (not necessarily consecutive) and head bi-
grams, as in Martins et al. (2011), in addition
to third-order features for grand- and tri-siblings
(Koo and Collins, 2010).
problems in a modular and extensible manner (Ko-
modakis et al., 2007; Rush et al., 2010). In this
paper, we employ alternating directions dual de-
composition (AD3; Martins et al., 2011). Like
the subgradient algorithm of Rush et al. (2010),
AD3 splits the original problem into local sub-
problems, and seeks an agreement on the over-
lapping variables. The difference is that the AD3
subproblems have an additional quadratic term to
accelerate consensus. Recent analysis (Martins et
al., 2012) has shown that: (i) AD3 converges at
a faster rate,2 and (ii) the quadratic subproblems
can be solved using the same combinatorial ma-
chinery that is used in the subgradient algorithm.
This opens the door for larger subproblems (such
as the combination of trees and head automata in
Koo et al., 2010) instead of a many-components
approach (Martins et al., 2011), while still enjoy-
ing faster convergence.
</bodyText>
<subsectionHeader confidence="0.993945">
2.1 Our Setup
</subsectionHeader>
<bodyText confidence="0.99409364516129">
Given a sentence with L words, to which we
prepend a root symbol $, let A := {hh, mi  |h ∈
{0, ... , L}, m ∈ {1, ... , L}, h =6 m} be the
set of possible dependency arcs. We parame-
terize a dependency tree via an indicator vector
u := huaiaEA, where ua is 1 if the arc a is in the
tree, and 0 otherwise, and we denote by Y ⊆ R|A|
the set of such vectors that are indicators of well-
2Concretely, AD3 needs O(1/e) iterations to converge to
a e-accurate solution, while subgradient needs O(1/e2).
formed trees. Let {As}Ss=1 be a cover of A, where
each As ⊆ A. We assume that the score of a parse
tree u ∈ Y decomposes as f(u) := ESs=1 fs(zs),
where each zs := hzs,aiaEAs is a “partial view” of
u, and each local score function fs comes from a
feature-based linear model.
Past work in dependency parsing considered ei-
ther (i) a few “large” components, such as trees
and head automata (Smith and Eisner, 2008; Koo
et al., 2010), or (ii) many “small” components,
coming from a multi-commodity flow formulation
(Martins et al., 2009, 2011). Let Ys ⊆ R|As |de-
note the set of feasible realizations of zs, i.e., those
that are partial views of an actual parse tree. A tu-
ple of views hz1, ... , zSi ∈ HSs=1 Ys is said to be
globally consistent if zs,a = zs0,a holds for every
a, s and s&apos; such that a ∈ As∩As0. We assume each
parse u ∈ Y corresponds uniquely to a globally
consistent tuple of views, and vice-versa. Follow-
ing Martins et al. (2011), the problem of obtaining
the best-scored tree can be written as follows:
</bodyText>
<equation confidence="0.650359333333333">
maximize ESs=1 fs(zs)
w.r.t. u ∈ R|A|, zs ∈ Ys, ∀s
s.t. zs,a = ua, ∀s, ∀a ∈ As, (1)
</equation>
<bodyText confidence="0.999733">
where the equality constraint ensures that the par-
tial views “glue” together to form a coherent parse
tree.3
</bodyText>
<subsectionHeader confidence="0.999704">
2.2 Dual Decomposition and AD3
</subsectionHeader>
<bodyText confidence="0.954981333333333">
Dual decomposition methods dualize out the
equality constraint in Eq. 1 by introducing La-
grange multipliers λs,a. In doing so, they solve a
relaxation where the combinatorial sets Ys are re-
placed by their convex hulls Zs := conv(Ys).4 All
that is necessary is the following assumption:
Assumption 1 (Local-Max Oracle). Every s ∈
{1, ... , S} has an oracle that solves efficiently any
instance of the following subproblem:
</bodyText>
<equation confidence="0.976207">
maximize fs(zs) + EaEAs λs,azs,a
w.r.t. zs ∈ Ys. (2)
</equation>
<bodyText confidence="0.862383833333333">
Typically, Assumption 1 is met whenever the max-
imization of fs over Ys is tractable, since the ob-
jective in Eq. 2 just adds a linear function to fs.
3Note that any tuple (z1, ... , zS) E f1Ss=1 Ys satisfying
the equality constraints will be globally consistent; this fact,
due the assumptions above, will imply u E Y.
</bodyText>
<footnote confidence="0.93869925">
4Let Δ|&apos;Js |:= {α E R|&apos;Js  ||α &gt; 0, ,s∈&apos;Js α,s = 11
be the probability simplex. The convex hull of Ys is the set
conv(Ys) := {E,s∈&apos;Js α,sys  |α E Δ|&apos;Js|1. Its members
represent marginal probabilities over the arcs in As.
</footnote>
<page confidence="0.987657">
618
</page>
<bodyText confidence="0.981965333333333">
average runtime (sec.)
The AD3 algorithm (Martins et al., 2011) alter-
nates among the following iterative updates:
</bodyText>
<listItem confidence="0.787918">
• z-updates, which decouple over s = 1, ... , S,
and solve a penalized version of Eq. 2:
</listItem>
<equation confidence="0.9974315">
fs(zs) + Ea∈As λ(t)s,azs,a
−2 Ea∈As(zs,a − u(t))2. (3)
</equation>
<bodyText confidence="0.978768">
Above, ρ is a constant and the quadratic term
penalizes deviations from the current global so-
lution (stored in u(t)).5 We will see (Prop. 2)
that this problem can be solved iteratively using
only the Local-Max Oracle (Eq. 2).
</bodyText>
<listItem confidence="0.960076">
• u-updates, a simple averaging operation:
</listItem>
<equation confidence="0.987829666666667">
u(t+1)1
a:=is: (t+1)
|{ Es : aEA3 Zs,a . (4)
</equation>
<listItem confidence="0.9965545">
• A-updates, where the Lagrange multipliers are
adjusted to penalize disagreements:
</listItem>
<equation confidence="0.9985645">
λ(t+1)
s,a := λ(t) s,a − ρ(z(t+1)
s,a − u(t+1)
a ). (5)
</equation>
<bodyText confidence="0.998160375">
In sum, the only difference between AD3 and
the subgradient method is in the z-updates, which
in AD3 require solving a quadratic problem.
While closed-form solutions have been developed
for some specialized components (Martins et al.,
2011), this problem is in general more difficult
than the one arising in the subgradient algorithm.
However, the following result, proved in Martins
et al. (2012), allows to expand the scope of AD3
to any problem which satisfies Assumption 1.
Proposition 2. The problem in Eq. 3 admits a
solution z∗s which is spanned by a sparse basis
W ⊆ Ys with cardinality at most |W |≤ O(|As|).
In other words, there is a distribution α with sup-
port in W such that z∗s = Eys∈W αysys.6
Prop. 2 has motivated an active set algorithm
(Martins et al., 2012) that maintains an estimate
of W by iteratively adding and removing elements
computed through the oracle in Eq. 2.7 Typically,
very few iterations are necessary and great speed-
ups are achieved by warm-starting W with the ac-
tive set computed in the previous AD3 iteration.
This has a huge impact in practice and is crucial to
obtain the fast runtimes in §4 (see Fig. 2).
</bodyText>
<footnote confidence="0.978580857142857">
5In our experiments (§4), we set p = 0.05.
6Note that JYsJ = O(2|A.|) in general. What Prop. 2
tells us is that the solution of Eq. 3 can be represented as a
distribution over Ys with a very sparse support.
7The algorithm is a specialization of Nocedal and Wright
(1999), §16.4, which effectively exploits the sparse represen-
tation of z∗s. For details, see Martins et al. (2012).
</footnote>
<figure confidence="0.982386166666667">
0.20
Subgrad.
AD3
0.10
0.000 10 20 30 40 50
sentence length (words)
</figure>
<figureCaption confidence="0.980693">
Figure 2: Comparison between AD3 and subgra-
</figureCaption>
<bodyText confidence="0.89244925">
dient. We show averaged runtimes in PTB §22 as
a function of the sentence length. For subgradi-
ent, we chose for each sentence the most favorable
stepsize in {0.001, 0.01, 0.1,1}.
</bodyText>
<sectionHeader confidence="0.965853" genericHeader="method">
3 Solving the Subproblems
</sectionHeader>
<bodyText confidence="0.95243647368421">
We next describe the actual components used in
our third-order parsers.
Tree component. We use an arc-factored score
function (McDonald et al., 2005):
ELm=1 σARC(π(m), m), where π(m) is the parent
of the mth word according to the parse tree z,
and σARC(h, m) is the score of an individual arc.
The parse tree that maximizes this function can be
found in time O(L3) via the Chu-Liu-Edmonds’
algorithm (Chu and Liu, 1965; Edmonds, 1967).8
Grand-sibling head automata. Let Ainh and
Aout
h denote respectively the sets of incoming and
outgoing candidate arcs for the hth word, where
the latter subdivides into arcs pointing to the right,
Aout
h,→, and to the left, Aout
h,←. Define the sets
AGSIB h,→ = Ain h ∪Aout
</bodyText>
<equation confidence="0.9256695">
h,→ and AGSIB
h,← = Ain h ∪Aout
</equation>
<bodyText confidence="0.997417">
h,←. We
describe right-side grand-sibling head automata;
their left-side counterparts are analogous. For
each head word h in the parse tree z, define
g := π(h), and let hm0, m1, ... , mp+1i be the se-
quence of right modifiers of h, with m0 = START
and mp+1 = END. Then, we have the following
grand-sibling component:
</bodyText>
<equation confidence="0.970544">
fh,SIB → (z|AhS� �) = Ek+=1 (σSIB(h, mk−1, mk)
σGP(g, h, mk) + σGSIB(g, h, mk−1, mk)),
</equation>
<bodyText confidence="0.9999364">
where we use the shorthand z|B to denote the
subvector of z indexed by the arcs in B ⊆ A.
Note that this score function absorbs grandparent
and consecutive sibling scores, in addition to the
grand-sibling scores.9 For each h
</bodyText>
<footnote confidence="0.9886875">
8In fact, there is an asymptotically faster O(L2) algorithm
(Tarjan, 1977). Moreover, if the set of possible arcs is reduced
to a subset B C_ A (via pruning), then the fastest known al-
gorithm (Gabow et al., 1986) runs in O(JBJ + L log L) time.
9Koo et al. (2010) used an identical automaton for their
second-order model, but leaving out the grand-sibling scores.
</footnote>
<equation confidence="0.993200166666667">
z(t+1) := argmax
s
zs∈Zs
fTREE(z) =
, fGSIB
h,→ can be
</equation>
<page confidence="0.992303">
619
</page>
<table confidence="0.997811285714286">
No pruning |Ainm |≤ K same, + |Aout
h  |≤ J
TREE O(KL + L log L) O(KL + L log L)
GSIB O(K2L2) O(JK2L)
TSIB O(L4) O(KL3) O(J2KL)
SEQ O(K2L) O(K2L)
ASIB O(L3) O(KL2) O(JKL)
</table>
<tableCaption confidence="0.993799">
Table 1: Theoretical runtimes of each subproblem
</tableCaption>
<bodyText confidence="0.983750818181818">
without pruning, limiting the number of candidate
heads, and limiting (in addition) the number of
modifiers. Note the O(L log L) total runtime per
AD3 iteration in the latter case.
maximized in time O(L3) with dynamic program-
ming, yielding O(L4) total runtime.
Tri-sibling head automata. In addition, we de-
fine left and right-side tri-sibling head automata
that remember the previous two modifiers of a
head word. This corresponds to the following
component function (for the right-side case):
</bodyText>
<equation confidence="0.990565">
1
fhs� (z  |Ahut, ) = Ek=2 σTSIB (h, mk−2, mk−1, mk).
</equation>
<bodyText confidence="0.99917075">
Again, each of these functions can be maximized
in time O(L3), yielding O(L4) runtime.
Sequential head bigram model. Head bigrams
can be captured with a simple sequence model:
</bodyText>
<equation confidence="0.901395">
fSEQ(z) = ELm=2 σHB(m, π(m), π(m − 1)).
</equation>
<bodyText confidence="0.996986588235294">
Each score σHB(m, h, h&apos;) is obtained via features
that look at the heads of consecutive words (as in
Martins et al. (2011)). This function can be maxi-
mized in time O(L3) with the Viterbi algorithm.
Arbitrary siblings. We handle arbitrary siblings
as in Martins et al. (2011), defining O(L3) compo-
nent functions of the form fh sm s (z(h,m), z(h,s)) =
σASIB (h, m, s). In this case, the quadratic problem
in Eq. 3 can be solved directly in constant time.
Tab. 1 details the time complexities of each sub-
problem. Without pruning, each iteration of AD3
has O(L4) runtime. With a simple strategy that
limits the number of candidate heads per word to
a constant K, this drops to cubic time.10 Further
speed-ups are possible with more pruning: by lim-
iting the number of possible modifiers to a con-
stant J, the runtime would reduce to O(L log L).
</bodyText>
<footnote confidence="0.6146034">
10In our experiments, we employed this strategy with K =
10, by pruning with a first-order probabilistic model. Fol-
lowing Koo and Collins (2010), for each word m, we also
pruned away incoming arcs (h, m) with posterior probability
less than 0.0001 times the probability of the most likely head.
</footnote>
<table confidence="0.754704714285714">
UAS Tok/sec
PTB-YM §22, 1st ord 91.38 4,063
PTB-YM §22, 2nd ord 93.15 1,338
PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018
PTB-YM §22, 3rd ord 93.29 709
PTB-YM §22, 3rd ord, gold tags 94.01 722
This work (PTB-YM §23, 3rd ord) 93.07 735
Koo et al. (2010) 92.46 112†
Huang and Sagae (2010) 92.1– 587†
Zhang and Nivre (2011) 92.9– 680†
Martins et al. (2011) 92.53 66†
Zhang and McDonald (2012) 93.06 220
This work (PTB-S §23, 3rd ord) 92.82 604
Rush and Petrov (2012) 92.7– 4,460
</table>
<tableCaption confidence="0.935211">
Table 2: Results for the projective English dataset.
</tableCaption>
<bodyText confidence="0.991998">
We report unlabeled attachment scores (UAS) ig-
noring punctuation, and parsing speeds in tokens
per second. Our speeds include the time necessary
for pruning, evaluating features, and decoding, as
measured on a Intel Core i7 processor @3.4 GHz.
The others are speeds reported in the cited papers;
those marked with † were converted from times per
sentence.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99991788">
We first evaluated our non-projective parser in a
projective English dataset, to see how its speed and
accuracy compares with recent projective parsers,
which can take advantage of dynamic program-
ming. To this end, we converted the Penn Tree-
bank to dependencies through (i) the head rules
of Yamada and Matsumoto (2003) (PTB-YM) and
(ii) basic dependencies from the Stanford parser
2.0.5 (PTB-S).11 We trained by running 10 epochs
of cost-augmented MIRA (Crammer et al., 2006).
To ensure valid parse trees at test time, we rounded
fractional solutions as in Martins et al. (2009)—
yet, solutions were integral ≈ 95% of the time.
Tab. 2 shows the results in the dev-set (top
block) and in the test-set (two bottom blocks). In
the dev-set, we see consistent gains when more ex-
pressive features are added, the best accuracies be-
ing achieved with the full third-order model; this
comes at the cost of a 6-fold drop in runtime com-
pared with a first-order model. By looking at the
two bottom blocks, we observe that our parser
has slightly better accuracies than recent projec-
tive parsers, with comparable speed levels (with
the exception of the highly optimized vine cascade
approach of Rush and Petrov, 2012).
</bodyText>
<footnote confidence="0.97447375">
11We train on sections §02–21, use §22 as validation data,
and test on §23. We trained a simple 2nd-order tagger with
10-fold jackknifing to obtain automatic part-of-speech tags
for §22–23, with accuracies 97.2% and 96.9%, respectively.
</footnote>
<page confidence="0.984988">
620
</page>
<table confidence="0.9981056875">
First Ord. Sec. Ord. Third Ord. Best published UAS RP12 ZM12
UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS
Arabic 77.23 2,481 78.50 388 79.64 197 81.12 - Ma11 - - -
Bulgarian 91.76 5,678 92.82 2,049 93.10 1,273 93.50 - Ma11 91.9 3,980 93.08
Chinese 88.49 18,094 90.14 4,284 89.98 2,592 91.89 - Ma10 90.9 7,800 -
Czech 87.66 1,840 90.00 751 90.32 501 89.46 - Ma11 - - -
Danish 89.42 4,110 91.20 1,053 91.48 650 91.86 - Ma11 - -
Dutch 83.61 3,884 86.37 1,294 86.19 599 85.81 121 Ko10 - - -
German 90.52 5,331 91.85 1,788 92.41 965 91.89 - Ma11 90.8 2,880 91.35
English 91.21 3,127 93.03 1,317 93.22 785 92.68 - Ma11 - - -
Japanese 92.78 23,895 93.14 5,660 93.52 2,996 93.72 - Ma11 92.3 8,600 93.24
Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69
Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - -
Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48
Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44
Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - -
</table>
<tableCaption confidence="0.99242">
Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008.
</tableCaption>
<bodyText confidence="0.963614941176471">
“Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al.
(2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald
(2012). The last two are shown separately in the rightmost columns.
In our second experiment (Tab. 3), we used 14
datasets, most of which are non-projective, from
the CoNLL 2006 and 2008 shared tasks (Buch-
holz and Marsi, 2006; Surdeanu et al., 2008).
Our third-order model achieved the best reported
scores for English, Czech, German, and Dutch—
which includes the three largest datasets and the
ones with the most non-projective dependencies—
and is on par with the state of the art for the
remaining languages. To our knowledge, the
speeds are the highest reported among higher-
order non-projective parsers, and only about 3–
4 times slower than the vine parser of Rush and
Petrov (2012), which has lower accuracies.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999963166666667">
We presented new third-order non-projective
parsers which are both fast and accurate. We de-
coded with AD3, an accelerated dual decomposi-
tion algorithm which we adapted to handle large
components, including specialized head automata
for the third-order features, and a sequence model
for head bigrams. Results are above the state of
the art for large datasets and non-projective lan-
guages. In the hope that other researchers may find
our implementation useful or are willing to con-
tribute with further improvements, we made our
parsers publicly available as open source software.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99992225">
We thank all reviewers for their insightful com-
ments and Lingpeng Kong for help in converting
the Penn Treebank to Stanford dependencies. This
work was partially supported by the EU/FEDER
programme, QREN/POR Lisboa (Portugal), under
the Intelligo project (contract 2012/24803), by a
FCT grant PTDC/EEI-SII/2312/2012, and by NSF
grant IIS-1054319.
</bodyText>
<sectionHeader confidence="0.998164" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99927548">
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Inter-
national Conference on Natural Language Learn-
ing.
X. Carreras. 2007. Experiments with a higher-order
projective dependency parser. In International Con-
ference on Natural Language Learning.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551–585.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233–240.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc.
of International Conference on Computational Lin-
guistics, pages 340–345.
H. N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan.
1986. Efficient algorithms for finding minimum
spanning trees in undirected and directed graphs.
Combinatorica, 6(2):109–122.
</reference>
<page confidence="0.990888">
621
</page>
<reference confidence="0.999354777777777">
L. Huang and K. Sagae. 2010. Dynamic programming
for linear-time incremental parsing. In Proc. of An-
nual Meeting of the Association for Computational
Linguistics, pages 1077–1086.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition:
Message-passing revisited. In Proc. of International
Conference on Computer Vision.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of Annual Meeting of the
Association for Computational Linguistics, pages 1–
11.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and
D. Sontag. 2010. Dual decomposition for parsing
with non-projective head automata. In Proc. of Em-
pirical Methods for Natural Language Processing.
S. K¨ubler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Morgan &amp; Claypool Publishers.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proc. ofAnnual Meeting
of the Association for Computational Linguistics.
A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T.
Figueiredo, and P. M. Q. Aguiar. 2010. Turbo
parsers: Dependency parsing by approximate vari-
ational inference. In Proc. of Empirical Methods for
Natural Language Processing.
A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and
M. A. T. Figueiredo. 2011. Dual decomposition
with many overlapping components. In Proc. of Em-
pirical Methods for Natural Language Processing.
A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,
N. A. Smith, and E. P. Xing. 2012. Alternat-
ing directions dual decomposition. Arxiv preprint
arXiv:1212.6550.
R. T. McDonald and F. C. N. Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of Annual Meeting of the European
Chapter of the Association for Computational Lin-
guistics.
R. McDonald and G. Satta. 2007. On the complex-
ity of non-projective data-driven dependency pars-
ing. In Proc. of International Conference on Parsing
Technologies.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Ha-
jic. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proc. of Empirical
Methods for Natural Language Processing.
R. McDonald, K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In Proc. of International Confer-
ence on Natural Language Learning.
J. Nivre, J. Hall, J. Nilsson, G. Eryiˇgit, and S. Marinov.
2006. Labeled pseudo-projective dependency pars-
ing with support vector machines. In Procs. of In-
ternational Conference on Natural Language Learn-
ing.
J. Nocedal and S. J. Wright. 1999. Numerical opti-
mization. Springer-Verlag.
Alexander M Rush and Slav Petrov. 2012. Vine prun-
ing for efficient multi-pass dependency parsing. In
Proc. of Conference of the North American Chapter
of the Association for Computational Linguistics.
A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010.
On dual decomposition and linear programming re-
laxations for natural language processing. In Proc.
of Empirical Methods for Natural Language Pro-
cessing.
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of Empirical Methods
for Natural Language Processing.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez,
and J. Nivre. 2008. The CoNLL-2008 shared task
on joint parsing of syntactic and semantic dependen-
cies. Proc. of International Conference on Natural
Language Learning.
R.E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25–36.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proc. of International Conference on Parsing Tech-
nologies.
H. Zhang and R. McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of Empirical Methods in Natural Language
Processing.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc.
of the Annual Meeting of the Association for Com-
putational Linguistics.
</reference>
<page confidence="0.997964">
622
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.387772">
<title confidence="0.999172">Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</title>
<author confidence="0.953049">F T B A</author>
<affiliation confidence="0.95243">Labs, Alameda D. Afonso Henriques, 41, 1000-123 Lisboa, de Instituto Superior T´ecnico, 1049-001 Lisboa,</affiliation>
<address confidence="0.978865">of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213,</address>
<email confidence="0.998198">nasmith@cs.cmu.edu</email>
<abstract confidence="0.993672454545454">We present fast, accurate, direct nonprojective dependency parsers with thirdfeatures. Our approach uses an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets</abstract>
<note confidence="0.515035">(English, Czech, and German).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In International Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="18457" citStr="Buchholz and Marsi, 2006" startWordPosition="3138" endWordPosition="3142">1.14 684 91.44 - ZM12 90.1 5,320 91.44 Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported scores for English, Czech, German, and Dutch— which includes the three largest datasets and the ones with the most non-projective dependencies— and is on par with the state of the art for the remaining languages. To our knowledge, the speeds are the highest reported among higherorder non-projective parsers, and only about 3– 4 times slower than the vine parser of Rush and Petrov (2012), which has lower accuracies. 5 Conclusions We presented new third-order non-projective parsers which are both fast and accurate. We deco</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In International Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In International Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="1455" citStr="Carreras, 2007" startWordPosition="207" endWordPosition="208">nd German). 1 Introduction Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is </context>
<context position="4220" citStr="Carreras, 2007" startWordPosition="621" endWordPosition="622">ependency Parsing with AD3 Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial 1Released as TurboParser 2.1, and publicly available at http://www.ark.cs.cmu.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems h</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>X. Carreras. 2007. Experiments with a higher-order projective dependency parser. In International Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="11053" citStr="Chu and Liu, 1965" startWordPosition="1837" endWordPosition="1840"> We show averaged runtimes in PTB §22 as a function of the sentence length. For subgradient, we chose for each sentence the most favorable stepsize in {0.001, 0.01, 0.1,1}. 3 Solving the Subproblems We next describe the actual components used in our third-order parsers. Tree component. We use an arc-factored score function (McDonald et al., 2005): ELm=1 σARC(π(m), m), where π(m) is the parent of the mth word according to the parse tree z, and σARC(h, m) is the score of an individual arc. The parse tree that maximizes this function can be found in time O(L3) via the Chu-Liu-Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967).8 Grand-sibling head automata. Let Ainh and Aout h denote respectively the sets of incoming and outgoing candidate arcs for the hth word, where the latter subdivides into arcs pointing to the right, Aout h,→, and to the left, Aout h,←. Define the sets AGSIB h,→ = Ain h ∪Aout h,→ and AGSIB h,← = Ain h ∪Aout h,←. We describe right-side grand-sibling head automata; their left-side counterparts are analogous. For each head word h in the parse tree z, define g := π(h), and let hm0, m1, ... , mp+1i be the sequence of right modifiers of h, with m0 = START and mp+1 = END. Then, we hav</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y. J. Chu and T. H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="15917" citStr="Crammer et al., 2006" startWordPosition="2687" endWordPosition="2690">rocessor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4 Experiments We first evaluated our non-projective parser in a projective English dataset, to see how its speed and accuracy compares with recent projective parsers, which can take advantage of dynamic programming. To this end, we converted the Penn Treebank to dependencies through (i) the head rules of Yamada and Matsumoto (2003) (PTB-YM) and (ii) basic dependencies from the Stanford parser 2.0.5 (PTB-S).11 We trained by running 10 epochs of cost-augmented MIRA (Crammer et al., 2006). To ensure valid parse trees at test time, we rounded fractional solutions as in Martins et al. (2009)— yet, solutions were integral ≈ 95% of the time. Tab. 2 shows the results in the dev-set (top block) and in the test-set (two bottom blocks). In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runtime compared with a first-order model. By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective p</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Edmonds</author>
</authors>
<title>Optimum branchings.</title>
<date>1967</date>
<journal>Journal of Research of the National Bureau of Standards,</journal>
<pages>71--233</pages>
<contexts>
<context position="11069" citStr="Edmonds, 1967" startWordPosition="1841" endWordPosition="1842">untimes in PTB §22 as a function of the sentence length. For subgradient, we chose for each sentence the most favorable stepsize in {0.001, 0.01, 0.1,1}. 3 Solving the Subproblems We next describe the actual components used in our third-order parsers. Tree component. We use an arc-factored score function (McDonald et al., 2005): ELm=1 σARC(π(m), m), where π(m) is the parent of the mth word according to the parse tree z, and σARC(h, m) is the score of an individual arc. The parse tree that maximizes this function can be found in time O(L3) via the Chu-Liu-Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967).8 Grand-sibling head automata. Let Ainh and Aout h denote respectively the sets of incoming and outgoing candidate arcs for the hth word, where the latter subdivides into arcs pointing to the right, Aout h,→, and to the left, Aout h,←. Define the sets AGSIB h,→ = Ain h ∪Aout h,→ and AGSIB h,← = Ain h ∪Aout h,←. We describe right-side grand-sibling head automata; their left-side counterparts are analogous. For each head word h in the parse tree z, define g := π(h), and let hm0, m1, ... , mp+1i be the sequence of right modifiers of h, with m0 = START and mp+1 = END. Then, we have the following </context>
</contexts>
<marker>Edmonds, 1967</marker>
<rawString>J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. of International Conference on Computational Linguistics,</booktitle>
<pages>340--345</pages>
<contexts>
<context position="1368" citStr="Eisner, 1996" startWordPosition="196" endWordPosition="197">parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German)</context>
<context position="4103" citStr="Eisner, 1996" startWordPosition="604" endWordPosition="605">utomata above. Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing with AD3 Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial 1Released as TurboParser 2.1, and publicly available at http://www.ark.cs.cmu.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. of International Conference on Computational Linguistics, pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H N Gabow</author>
<author>Z Galil</author>
<author>T Spencer</author>
<author>R E Tarjan</author>
</authors>
<title>Efficient algorithms for finding minimum spanning trees in undirected and directed graphs.</title>
<date>1986</date>
<journal>Combinatorica,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="12218" citStr="Gabow et al., 1986" startWordPosition="2048" endWordPosition="2051">ers of h, with m0 = START and mp+1 = END. Then, we have the following grand-sibling component: fh,SIB → (z|AhS� �) = Ek+=1 (σSIB(h, mk−1, mk) σGP(g, h, mk) + σGSIB(g, h, mk−1, mk)), where we use the shorthand z|B to denote the subvector of z indexed by the arcs in B ⊆ A. Note that this score function absorbs grandparent and consecutive sibling scores, in addition to the grand-sibling scores.9 For each h 8In fact, there is an asymptotically faster O(L2) algorithm (Tarjan, 1977). Moreover, if the set of possible arcs is reduced to a subset B C_ A (via pruning), then the fastest known algorithm (Gabow et al., 1986) runs in O(JBJ + L log L) time. 9Koo et al. (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. z(t+1) := argmax s zs∈Zs fTREE(z) = , fGSIB h,→ can be 619 No pruning |Ainm |≤ K same, + |Aout h |≤ J TREE O(KL + L log L) O(KL + L log L) GSIB O(K2L2) O(JK2L) TSIB O(L4) O(KL3) O(J2KL) SEQ O(K2L) O(K2L) ASIB O(L3) O(KL2) O(JKL) Table 1: Theoretical runtimes of each subproblem without pruning, limiting the number of candidate heads, and limiting (in addition) the number of modifiers. Note the O(L log L) total runtime per AD3 iteration in the lat</context>
</contexts>
<marker>Gabow, Galil, Spencer, Tarjan, 1986</marker>
<rawString>H. N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan. 1986. Efficient algorithms for finding minimum spanning trees in undirected and directed graphs. Combinatorica, 6(2):109–122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="1052" citStr="Huang and Sagae, 2010" startWordPosition="144" endWordPosition="147">present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-or</context>
<context position="14828" citStr="Huang and Sagae (2010)" startWordPosition="2510" endWordPosition="2513">J, the runtime would reduce to O(L log L). 10In our experiments, we employed this strategy with K = 10, by pruning with a first-order probabilistic model. Following Koo and Collins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and Petrov (2012) 92.7– 4,460 Table 2: Results for the projective English dataset. We report unlabeled attachment scores (UAS) ignoring punctuation, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proc. of Annual Meeting of the Association for Computational Linguistics, pages 1077–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Komodakis</author>
<author>N Paragios</author>
<author>G Tziritas</author>
</authors>
<title>MRF optimization via dual decomposition: Message-passing revisited.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Computer Vision.</booktitle>
<contexts>
<context position="4495" citStr="Komodakis et al., 2007" startWordPosition="662" endWordPosition="666">ssociation for Computational Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) AD3 converges at a faster rate,2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algor</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>N. Komodakis, N. Paragios, and G. Tziritas. 2007. MRF optimization via dual decomposition: Message-passing revisited. In Proc. of International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1502" citStr="Koo and Collins, 2010" startWordPosition="212" endWordPosition="215">rsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald an</context>
<context position="2823" citStr="Koo and Collins (2010)" startWordPosition="410" endWordPosition="413">ith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3, the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach of Martins et al. (2011). While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al., 2010). This </context>
<context position="4426" citStr="Koo and Collins, 2010" startWordPosition="651" endWordPosition="654">.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) AD3 converges at a faster rate,2 and (ii) the quadratic subproblems can be solved using t</context>
<context position="14393" citStr="Koo and Collins (2010)" startWordPosition="2430" endWordPosition="2433">, s). In this case, the quadratic problem in Eq. 3 can be solved directly in constant time. Tab. 1 details the time complexities of each subproblem. Without pruning, each iteration of AD3 has O(L4) runtime. With a simple strategy that limits the number of candidate heads per word to a constant K, this drops to cubic time.10 Further speed-ups are possible with more pruning: by limiting the number of possible modifiers to a constant J, the runtime would reduce to O(L log L). 10In our experiments, we employed this strategy with K = 10, by pruning with a first-order probabilistic model. Following Koo and Collins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and P</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. of Annual Meeting of the Association for Computational Linguistics, pages 1– 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="2261" citStr="Koo et al., 2010" startWordPosition="319" endWordPosition="322">first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. • This ext</context>
<context position="5216" citStr="Koo et al., 2010" startWordPosition="782" endWordPosition="785">t al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) AD3 converges at a faster rate,2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al., 2010) instead of a many-components approach (Martins et al., 2011), while still enjoying faster convergence. 2.1 Our Setup Given a sentence with L words, to which we prepend a root symbol $, let A := {hh, mi |h ∈ {0, ... , L}, m ∈ {1, ... , L}, h =6 m} be the set of possible dependency arcs. We parameterize a dependency tree via an indicator vector u := huaiaEA, where ua is 1 if the arc a is in the tree, and 0 otherwise, and we denote by Y ⊆ R|A| the set of such vectors that are indicators of well2Concretely, AD3 needs O(1/e) iterations to converge to a e-accurate solution, while subgradient needs </context>
<context position="12268" citStr="Koo et al. (2010)" startWordPosition="2060" endWordPosition="2063">ave the following grand-sibling component: fh,SIB → (z|AhS� �) = Ek+=1 (σSIB(h, mk−1, mk) σGP(g, h, mk) + σGSIB(g, h, mk−1, mk)), where we use the shorthand z|B to denote the subvector of z indexed by the arcs in B ⊆ A. Note that this score function absorbs grandparent and consecutive sibling scores, in addition to the grand-sibling scores.9 For each h 8In fact, there is an asymptotically faster O(L2) algorithm (Tarjan, 1977). Moreover, if the set of possible arcs is reduced to a subset B C_ A (via pruning), then the fastest known algorithm (Gabow et al., 1986) runs in O(JBJ + L log L) time. 9Koo et al. (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. z(t+1) := argmax s zs∈Zs fTREE(z) = , fGSIB h,→ can be 619 No pruning |Ainm |≤ K same, + |Aout h |≤ J TREE O(KL + L log L) O(KL + L log L) GSIB O(K2L2) O(JK2L) TSIB O(L4) O(KL3) O(J2KL) SEQ O(K2L) O(K2L) ASIB O(L3) O(KL2) O(JKL) Table 1: Theoretical runtimes of each subproblem without pruning, limiting the number of candidate heads, and limiting (in addition) the number of modifiers. Note the O(L log L) total runtime per AD3 iteration in the latter case. maximized in time O(L3) with dynamic pro</context>
<context position="14794" citStr="Koo et al. (2010)" startWordPosition="2504" endWordPosition="2507">ible modifiers to a constant J, the runtime would reduce to O(L log L). 10In our experiments, we employed this strategy with K = 10, by pruning with a first-order probabilistic model. Following Koo and Collins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and Petrov (2012) 92.7– 4,460 Table 2: Results for the projective English dataset. We report unlabeled attachment scores (UAS) ignoring punctuation, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were co</context>
<context position="18187" citStr="Koo et al. (2010)" startWordPosition="3093" endWordPosition="3096">6 93.72 - Ma11 92.3 8,600 93.24 Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69 Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - - Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48 Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44 Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported scores for English, Czech, German, and Dutch— which includes the three largest datasets and the ones with the most non-projective dependencies— and is on par with the state of the art for the remaining languages. To our knowledge, the speeds are the highe</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Dependency parsing.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>S. K¨ubler, R. McDonald, and J. Nivre. 2009. Dependency parsing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. ofAnnual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2309" citStr="Martins et al., 2009" startWordPosition="326" endWordPosition="329">e also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. • This extension is non-trivial since exact dynamic progra</context>
<context position="6356" citStr="Martins et al., 2009" startWordPosition="998" endWordPosition="1001">O(1/e) iterations to converge to a e-accurate solution, while subgradient needs O(1/e2). formed trees. Let {As}Ss=1 be a cover of A, where each As ⊆ A. We assume that the score of a parse tree u ∈ Y decomposes as f(u) := ESs=1 fs(zs), where each zs := hzs,aiaEAs is a “partial view” of u, and each local score function fs comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few “large” components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many “small” components, coming from a multi-commodity flow formulation (Martins et al., 2009, 2011). Let Ys ⊆ R|As |denote the set of feasible realizations of zs, i.e., those that are partial views of an actual parse tree. A tuple of views hz1, ... , zSi ∈ HSs=1 Ys is said to be globally consistent if zs,a = zs0,a holds for every a, s and s&apos; such that a ∈ As∩As0. We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be written as follows: maximize ESs=1 fs(zs) w.r.t. u ∈ R|A|, zs ∈ Ys, ∀s s.t. zs,a = ua, ∀s, ∀a ∈ As, (1) where the equality constraint e</context>
<context position="16020" citStr="Martins et al. (2009)" startWordPosition="2705" endWordPosition="2708">ed from times per sentence. 4 Experiments We first evaluated our non-projective parser in a projective English dataset, to see how its speed and accuracy compares with recent projective parsers, which can take advantage of dynamic programming. To this end, we converted the Penn Treebank to dependencies through (i) the head rules of Yamada and Matsumoto (2003) (PTB-YM) and (ii) basic dependencies from the Stanford parser 2.0.5 (PTB-S).11 We trained by running 10 epochs of cost-augmented MIRA (Crammer et al., 2006). To ensure valid parse trees at test time, we rounded fractional solutions as in Martins et al. (2009)— yet, solutions were integral ≈ 95% of the time. Tab. 2 shows the results in the dev-set (top block) and in the test-set (two bottom blocks). In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runtime compared with a first-order model. By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. ofAnnual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>M A T Figueiredo</author>
<author>P M Q Aguiar</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="2393" citStr="Martins et al. (2010)" startWordPosition="342" endWordPosition="345">parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3, the dual decomposition algorithm pro</context>
<context position="18161" citStr="Martins et al. (2010" startWordPosition="3088" endWordPosition="3091">23,895 93.14 5,660 93.52 2,996 93.72 - Ma11 92.3 8,600 93.24 Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69 Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - - Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48 Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44 Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported scores for English, Czech, German, and Dutch— which includes the three largest datasets and the ones with the most non-projective dependencies— and is on par with the state of the art for the remaining languages. To our knowledge</context>
</contexts>
<marker>Martins, Smith, Xing, Figueiredo, Aguiar, 2010</marker>
<rawString>A. F. T. Martins, N. A. Smith, E. P. Xing, M. A. T. Figueiredo, and P. M. Q. Aguiar. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="3023" citStr="Martins et al. (2011)" startWordPosition="441" endWordPosition="444">erlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3, the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach of Martins et al. (2011). While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al., 2010). This enables AD3 to exploit combinatorial subproblems like the the head automata above. Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing </context>
<context position="4337" citStr="Martins et al. (2011)" startWordPosition="638" endWordPosition="641">binatorial 1Released as TurboParser 2.1, and publicly available at http://www.ark.cs.cmu.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) </context>
<context position="6766" citStr="Martins et al. (2011)" startWordPosition="1081" endWordPosition="1084">her (i) a few “large” components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many “small” components, coming from a multi-commodity flow formulation (Martins et al., 2009, 2011). Let Ys ⊆ R|As |denote the set of feasible realizations of zs, i.e., those that are partial views of an actual parse tree. A tuple of views hz1, ... , zSi ∈ HSs=1 Ys is said to be globally consistent if zs,a = zs0,a holds for every a, s and s&apos; such that a ∈ As∩As0. We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be written as follows: maximize ESs=1 fs(zs) w.r.t. u ∈ R|A|, zs ∈ Ys, ∀s s.t. zs,a = ua, ∀s, ∀a ∈ As, (1) where the equality constraint ensures that the partial views “glue” together to form a coherent parse tree.3 2.2 Dual Decomposition and AD3 Dual decomposition methods dualize out the equality constraint in Eq. 1 by introducing Lagrange multipliers λs,a. In doing so, they solve a relaxation where the combinatorial sets Ys are replaced by their convex hulls Zs := conv(Ys).4 All that is necessary is the following assumption: Assumption 1 (L</context>
<context position="8144" citStr="Martins et al., 2011" startWordPosition="1328" endWordPosition="1331">.t. zs ∈ Ys. (2) Typically, Assumption 1 is met whenever the maximization of fs over Ys is tractable, since the objective in Eq. 2 just adds a linear function to fs. 3Note that any tuple (z1, ... , zS) E f1Ss=1 Ys satisfying the equality constraints will be globally consistent; this fact, due the assumptions above, will imply u E Y. 4Let Δ|&apos;Js |:= {α E R|&apos;Js ||α &gt; 0, ,s∈&apos;Js α,s = 11 be the probability simplex. The convex hull of Ys is the set conv(Ys) := {E,s∈&apos;Js α,sys |α E Δ|&apos;Js|1. Its members represent marginal probabilities over the arcs in As. 618 average runtime (sec.) The AD3 algorithm (Martins et al., 2011) alternates among the following iterative updates: • z-updates, which decouple over s = 1, ... , S, and solve a penalized version of Eq. 2: fs(zs) + Ea∈As λ(t)s,azs,a −2 Ea∈As(zs,a − u(t))2. (3) Above, ρ is a constant and the quadratic term penalizes deviations from the current global solution (stored in u(t)).5 We will see (Prop. 2) that this problem can be solved iteratively using only the Local-Max Oracle (Eq. 2). • u-updates, a simple averaging operation: u(t+1)1 a:=is: (t+1) |{ Es : aEA3 Zs,a . (4) • A-updates, where the Lagrange multipliers are adjusted to penalize disagreements: λ(t+1) </context>
<context position="13532" citStr="Martins et al. (2011)" startWordPosition="2278" endWordPosition="2281">-sibling head automata. In addition, we define left and right-side tri-sibling head automata that remember the previous two modifiers of a head word. This corresponds to the following component function (for the right-side case): 1 fhs� (z |Ahut, ) = Ek=2 σTSIB (h, mk−2, mk−1, mk). Again, each of these functions can be maximized in time O(L3), yielding O(L4) runtime. Sequential head bigram model. Head bigrams can be captured with a simple sequence model: fSEQ(z) = ELm=2 σHB(m, π(m), π(m − 1)). Each score σHB(m, h, h&apos;) is obtained via features that look at the heads of consecutive words (as in Martins et al. (2011)). This function can be maximized in time O(L3) with the Viterbi algorithm. Arbitrary siblings. We handle arbitrary siblings as in Martins et al. (2011), defining O(L3) component functions of the form fh sm s (z(h,m), z(h,s)) = σASIB (h, m, s). In this case, the quadratic problem in Eq. 3 can be solved directly in constant time. Tab. 1 details the time complexities of each subproblem. Without pruning, each iteration of AD3 has O(L4) runtime. With a simple strategy that limits the number of candidate heads per word to a constant K, this drops to cubic time.10 Further speed-ups are possible with</context>
<context position="14895" citStr="Martins et al. (2011)" startWordPosition="2522" endWordPosition="2525">employed this strategy with K = 10, by pruning with a first-order probabilistic model. Following Koo and Collins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and Petrov (2012) 92.7– 4,460 Table 2: Results for the projective English dataset. We report unlabeled attachment scores (UAS) ignoring punctuation, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4 Experiments We first evaluated our non-projective parser in a proj</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>A. F. T. Martins, N. A. Smith, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A F T Martins</author>
<author>M A T Figueiredo</author>
<author>P M Q Aguiar</author>
</authors>
<marker>Martins, Figueiredo, Aguiar, </marker>
<rawString>A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar,</rawString>
</citation>
<citation valid="true">
<authors>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Alternating directions dual decomposition. Arxiv preprint arXiv:1212.6550.</title>
<date>2012</date>
<marker>Smith, Xing, 2012</marker>
<rawString>N. A. Smith, and E. P. Xing. 2012. Alternating directions dual decomposition. Arxiv preprint arXiv:1212.6550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F C N Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proc. of Annual Meeting of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1397" citStr="McDonald and Pereira, 2006" startWordPosition="198" endWordPosition="201">state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent adva</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>R. T. McDonald and F. C. N. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proc. of Annual Meeting of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>G Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="2116" citStr="McDonald and Satta, 2007" startWordPosition="298" endWordPosition="301">lins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by </context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>R. McDonald and G. Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proc. of International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="4127" citStr="McDonald et al., 2005" startWordPosition="606" endWordPosition="610"> Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing with AD3 Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial 1Released as TurboParser 2.1, and publicly available at http://www.ark.cs.cmu.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and </context>
<context position="10784" citStr="McDonald et al., 2005" startWordPosition="1788" endWordPosition="1791">s a specialization of Nocedal and Wright (1999), §16.4, which effectively exploits the sparse representation of z∗s. For details, see Martins et al. (2012). 0.20 Subgrad. AD3 0.10 0.000 10 20 30 40 50 sentence length (words) Figure 2: Comparison between AD3 and subgradient. We show averaged runtimes in PTB §22 as a function of the sentence length. For subgradient, we chose for each sentence the most favorable stepsize in {0.001, 0.01, 0.1,1}. 3 Solving the Subproblems We next describe the actual components used in our third-order parsers. Tree component. We use an arc-factored score function (McDonald et al., 2005): ELm=1 σARC(π(m), m), where π(m) is the parent of the mth word according to the parse tree z, and σARC(h, m) is the score of an individual arc. The parse tree that maximizes this function can be found in time O(L3) via the Chu-Liu-Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967).8 Grand-sibling head automata. Let Ainh and Aout h denote respectively the sets of incoming and outgoing candidate arcs for the hth word, where the latter subdivides into arcs pointing to the right, Aout h,→, and to the left, Aout h,←. Define the sets AGSIB h,→ = Ain h ∪Aout h,→ and AGSIB h,← = Ain h ∪Aout h,←. W</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proc. of International Conference on Natural Language Learning.</booktitle>
<contexts>
<context position="18139" citStr="McDonald et al. (2006)" startWordPosition="3084" endWordPosition="3087">11 - - - Japanese 92.78 23,895 93.14 5,660 93.52 2,996 93.72 - Ma11 92.3 8,600 93.24 Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69 Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - - Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48 Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44 Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported scores for English, Czech, German, and Dutch— which includes the three largest datasets and the ones with the most non-projective dependencies— and is on par with the state of the art for the remaining langu</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proc. of International Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>G Eryiˇgit</author>
<author>S Marinov</author>
</authors>
<title>Labeled pseudo-projective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Procs. of International Conference on Natural Language Learning.</booktitle>
<marker>Nivre, Hall, Nilsson, Eryiˇgit, Marinov, 2006</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, G. Eryiˇgit, and S. Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Procs. of International Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nocedal</author>
<author>S J Wright</author>
</authors>
<title>Numerical optimization.</title>
<date>1999</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="10209" citStr="Nocedal and Wright (1999)" startWordPosition="1693" endWordPosition="1696"> an estimate of W by iteratively adding and removing elements computed through the oracle in Eq. 2.7 Typically, very few iterations are necessary and great speedups are achieved by warm-starting W with the active set computed in the previous AD3 iteration. This has a huge impact in practice and is crucial to obtain the fast runtimes in §4 (see Fig. 2). 5In our experiments (§4), we set p = 0.05. 6Note that JYsJ = O(2|A.|) in general. What Prop. 2 tells us is that the solution of Eq. 3 can be represented as a distribution over Ys with a very sparse support. 7The algorithm is a specialization of Nocedal and Wright (1999), §16.4, which effectively exploits the sparse representation of z∗s. For details, see Martins et al. (2012). 0.20 Subgrad. AD3 0.10 0.000 10 20 30 40 50 sentence length (words) Figure 2: Comparison between AD3 and subgradient. We show averaged runtimes in PTB §22 as a function of the sentence length. For subgradient, we chose for each sentence the most favorable stepsize in {0.001, 0.01, 0.1,1}. 3 Solving the Subproblems We next describe the actual components used in our third-order parsers. Tree component. We use an arc-factored score function (McDonald et al., 2005): ELm=1 σARC(π(m), m), wh</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>J. Nocedal and S. J. Wright. 1999. Numerical optimization. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine pruning for efficient multi-pass dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1099" citStr="Rush and Petrov, 2012" startWordPosition="152" endWordPosition="155">ependency parsers with thirdorder features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also bee</context>
<context position="15005" citStr="Rush and Petrov (2012)" startWordPosition="2542" endWordPosition="2545">ins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and Petrov (2012) 92.7– 4,460 Table 2: Results for the projective English dataset. We report unlabeled attachment scores (UAS) ignoring punctuation, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4 Experiments We first evaluated our non-projective parser in a projective English dataset, to see how its speed and accuracy compares with recent projective parsers, which can t</context>
<context position="16645" citStr="Rush and Petrov, 2012" startWordPosition="2813" endWordPosition="2816">et, solutions were integral ≈ 95% of the time. Tab. 2 shows the results in the dev-set (top block) and in the test-set (two bottom blocks). In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runtime compared with a first-order model. By looking at the two bottom blocks, we observe that our parser has slightly better accuracies than recent projective parsers, with comparable speed levels (with the exception of the highly optimized vine cascade approach of Rush and Petrov, 2012). 11We train on sections §02–21, use §22 as validation data, and test on §23. We trained a simple 2nd-order tagger with 10-fold jackknifing to obtain automatic part-of-speech tags for §22–23, with accuracies 97.2% and 96.9%, respectively. 620 First Ord. Sec. Ord. Third Ord. Best published UAS RP12 ZM12 UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Tok/sec UAS Arabic 77.23 2,481 78.50 388 79.64 197 81.12 - Ma11 - - - Bulgarian 91.76 5,678 92.82 2,049 93.10 1,273 93.50 - Ma11 91.9 3,980 93.08 Chinese 88.49 18,094 90.14 4,284 89.98 2,592 91.89 - Ma10 90.9 7,800 - Czech 87.66 1,840 90.00 751</context>
<context position="18211" citStr="Rush and Petrov (2012)" startWordPosition="3097" endWordPosition="3100"> 8,600 93.24 Portuguese 91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69 Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - - Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48 Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44 Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported scores for English, Czech, German, and Dutch— which includes the three largest datasets and the ones with the most non-projective dependencies— and is on par with the state of the art for the remaining languages. To our knowledge, the speeds are the highest reported among higher</context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander M Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Proc. of Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rush</author>
<author>D Sontag</author>
<author>M Collins</author>
<author>T Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="4515" citStr="Rush et al., 2010" startWordPosition="667" endWordPosition="670">onal Linguistics, pages 617–622, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) AD3 converges at a faster rate,2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens the</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>A. Rush, D. Sontag, M. Collins, and T. Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>J Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="2222" citStr="Smith and Eisner, 2008" startWordPosition="313" endWordPosition="316">ing runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: • We apply the third-order feature models of Koo and Collins (2010</context>
<context position="6234" citStr="Smith and Eisner, 2008" startWordPosition="979" endWordPosition="982"> tree, and 0 otherwise, and we denote by Y ⊆ R|A| the set of such vectors that are indicators of well2Concretely, AD3 needs O(1/e) iterations to converge to a e-accurate solution, while subgradient needs O(1/e2). formed trees. Let {As}Ss=1 be a cover of A, where each As ⊆ A. We assume that the score of a parse tree u ∈ Y decomposes as f(u) := ESs=1 fs(zs), where each zs := hzs,aiaEAs is a “partial view” of u, and each local score function fs comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few “large” components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many “small” components, coming from a multi-commodity flow formulation (Martins et al., 2009, 2011). Let Ys ⊆ R|As |denote the set of feasible realizations of zs, i.e., those that are partial views of an actual parse tree. A tuple of views hz1, ... , zSi ∈ HSs=1 Ys is said to be globally consistent if zs,a = zs0,a holds for every a, s and s&apos; such that a ∈ As∩As0. We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be written as fo</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>D. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>Proc. of International Conference on Natural Language Learning.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. Proc. of International Conference on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="12080" citStr="Tarjan, 1977" startWordPosition="2023" endWordPosition="2024"> analogous. For each head word h in the parse tree z, define g := π(h), and let hm0, m1, ... , mp+1i be the sequence of right modifiers of h, with m0 = START and mp+1 = END. Then, we have the following grand-sibling component: fh,SIB → (z|AhS� �) = Ek+=1 (σSIB(h, mk−1, mk) σGP(g, h, mk) + σGSIB(g, h, mk−1, mk)), where we use the shorthand z|B to denote the subvector of z indexed by the arcs in B ⊆ A. Note that this score function absorbs grandparent and consecutive sibling scores, in addition to the grand-sibling scores.9 For each h 8In fact, there is an asymptotically faster O(L2) algorithm (Tarjan, 1977). Moreover, if the set of possible arcs is reduced to a subset B C_ A (via pruning), then the fastest known algorithm (Gabow et al., 1986) runs in O(JBJ + L log L) time. 9Koo et al. (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. z(t+1) := argmax s zs∈Zs fTREE(z) = , fGSIB h,→ can be 619 No pruning |Ainm |≤ K same, + |Aout h |≤ J TREE O(KL + L log L) O(KL + L log L) GSIB O(K2L2) O(JK2L) TSIB O(L4) O(KL3) O(J2KL) SEQ O(K2L) O(K2L) ASIB O(L3) O(KL2) O(JKL) Table 1: Theoretical runtimes of each subproblem without pruning, limiting the num</context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>R.E. Tarjan. 1977. Finding optimum branchings. Networks, 7(1):25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of International Conference on Parsing Technologies.</booktitle>
<contexts>
<context position="15760" citStr="Yamada and Matsumoto (2003)" startWordPosition="2663" endWordPosition="2666">on, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4 Experiments We first evaluated our non-projective parser in a projective English dataset, to see how its speed and accuracy compares with recent projective parsers, which can take advantage of dynamic programming. To this end, we converted the Penn Treebank to dependencies through (i) the head rules of Yamada and Matsumoto (2003) (PTB-YM) and (ii) basic dependencies from the Stanford parser 2.0.5 (PTB-S).11 We trained by running 10 epochs of cost-augmented MIRA (Crammer et al., 2006). To ensure valid parse trees at test time, we rounded fractional solutions as in Martins et al. (2009)— yet, solutions were integral ≈ 95% of the time. Tab. 2 shows the results in the dev-set (top block) and in the test-set (two bottom blocks). In the dev-set, we see consistent gains when more expressive features are added, the best accuracies being achieved with the full third-order model; this comes at the cost of a 6-fold drop in runti</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of International Conference on Parsing Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>R McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1824" citStr="Zhang and McDonald, 2012" startWordPosition="258" endWordPosition="261">em of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations</context>
<context position="14931" citStr="Zhang and McDonald (2012)" startWordPosition="2528" endWordPosition="2531">10, by pruning with a first-order probabilistic model. Following Koo and Collins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and Petrov (2012) 92.7– 4,460 Table 2: Results for the projective English dataset. We report unlabeled attachment scores (UAS) ignoring punctuation, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4 Experiments We first evaluated our non-projective parser in a projective English dataset, to see how i</context>
<context position="18238" citStr="Zhang and McDonald (2012)" startWordPosition="3101" endWordPosition="3104">91.14 4,273 92.71 1,316 92.69 740 93.03 79 Ko10 91.5 2,900 91.69 Slovene 82.81 4,315 85.21 722 86.01 366 86.95 - Ma11 - - - Spanish 83.61 4,347 84.97 623 85.59 318 87.48 - ZM12 - - 87.48 Swedish 89.36 5,622 90.98 1,387 91.14 684 91.44 - ZM12 90.1 5,320 91.44 Turkish 75.98 6,418 76.50 1,721 76.90 793 77.55 258 Ko10 - - - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported scores for English, Czech, German, and Dutch— which includes the three largest datasets and the ones with the most non-projective dependencies— and is on par with the state of the art for the remaining languages. To our knowledge, the speeds are the highest reported among higherorder non-projective parser</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>H. Zhang and R. McDonald. 2012. Generalized higher-order dependency parsing with cube pruning. In Proc. of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>J Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1075" citStr="Zhang and Nivre, 2011" startWordPosition="148" endWordPosition="151"> direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3, an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-orde</context>
<context position="14862" citStr="Zhang and Nivre (2011)" startWordPosition="2516" endWordPosition="2519"> log L). 10In our experiments, we employed this strategy with K = 10, by pruning with a first-order probabilistic model. Following Koo and Collins (2010), for each word m, we also pruned away incoming arcs (h, m) with posterior probability less than 0.0001 times the probability of the most likely head. UAS Tok/sec PTB-YM §22, 1st ord 91.38 4,063 PTB-YM §22, 2nd ord 93.15 1,338 PTB-YM §22, 2nd ord, +ASIB, +HB 93.28 1,018 PTB-YM §22, 3rd ord 93.29 709 PTB-YM §22, 3rd ord, gold tags 94.01 722 This work (PTB-YM §23, 3rd ord) 93.07 735 Koo et al. (2010) 92.46 112† Huang and Sagae (2010) 92.1– 587† Zhang and Nivre (2011) 92.9– 680† Martins et al. (2011) 92.53 66† Zhang and McDonald (2012) 93.06 220 This work (PTB-S §23, 3rd ord) 92.82 604 Rush and Petrov (2012) 92.7– 4,460 Table 2: Results for the projective English dataset. We report unlabeled attachment scores (UAS) ignoring punctuation, and parsing speeds in tokens per second. Our speeds include the time necessary for pruning, evaluating features, and decoding, as measured on a Intel Core i7 processor @3.4 GHz. The others are speeds reported in the cited papers; those marked with † were converted from times per sentence. 4 Experiments We first evaluated ou</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Y. Zhang and J. Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>