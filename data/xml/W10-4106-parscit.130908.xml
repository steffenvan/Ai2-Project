<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.576229">
Semantic class induction and its application for a Chinese voice
search system
</note>
<author confidence="0.976245">
Yali Li
</author>
<affiliation confidence="0.9856435">
ThinkIT laboratory,
Institute of
Acoustics, Chinese
Academy of Sciences
</affiliation>
<email confidence="0.964783">
liyali@hccl.ioa.ac.cn
</email>
<author confidence="0.982893">
Weiqun Xu
</author>
<affiliation confidence="0.982593666666667">
ThinkIT laboratory,
Institute of Acoustics,
Chinese Academy of
</affiliation>
<address confidence="0.874735">
Sciences
</address>
<email confidence="0.99622">
xuweiqun@hccl.ioa.ac.cn
</email>
<author confidence="0.968062">
Yonghong Yan
</author>
<affiliation confidence="0.982429">
ThinkIT laboratory,
Institute of
Acoustics, Chinese
Academy of Sciences
</affiliation>
<email confidence="0.994169">
yyan@hccl.ioa.ac.cn
</email>
<sectionHeader confidence="0.996785" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9939824">
In this paper, we propose a novel
similarity measure based on
co-occurrence probabilities for inducing
semantic classes. Clustering with the new
similarity measure outperformed that
with the widely used distance measure
based on Kullback-Leibler divergence in
precision, recall and F1 evaluation. We
then use the induced semantic classes and
structures by the new similarity measure
to generate in-domain data. At last, we
use the generated data to do language
model adaptation and improve the result
of character recognition from 85.2% to
91%.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999671035714286">
Voice search (e.g. Wang et al., 2008) has
recently become one of the major foci in spoken
dialogue system research and development. In
main stream large vocabulary ASR engines,
statistical language models (n-grams in
particular), usually trained with plenty of data,
are widely used and proved very effective. But
for a voice search system, we have to deal with
the case where there is no or very little relevant
data for language modeling. One of the
conventional solutions to this problem is to
collect and use some human-human or
Wizard-of-Oz (WOZ) dialogue data. Once the
initial system is up running, the performance can
be further improved with human-computer data
in a system-in-the-loop style. Another practical
approach is to handcraft some grammar rules and
generate some artificial data. But writing
grammars manually is tedious and
time-consuming and requires some linguistic
expertise.
In this paper, we introduced a new similarity
measure to induce semantic classes and
structures. We then generated a large number of
data using the induced semantic classes and
structures to make language model adaptation.
At the end, we give the conclusion and implied
the future work.
</bodyText>
<sectionHeader confidence="0.988821" genericHeader="introduction">
2 Semantic Class Induction
</sectionHeader>
<bodyText confidence="0.999949884615385">
The studies on semantic class induction in spoken
language (or spoken language acquisition in
general) have received some attention since the
middle 90&apos;s. One of the earlier works is carried
out by Gorin (1995), who employed an
information -theoretic connectionist network
embedded in a feedback control system to acquire
spoken language. Later on Arai et al. (1999)
further studied how to acquire grammar
fragments in fluent speech through clustering
similar phrases using Kullback-Leibler distance.
Meng and Siu (2002) proposed to
semi-automatically induce language structures
from unannotated corpora for spoken language
understanding, mainly using Kullback-Liebler
divergence and mutual information. Pargellis et
al. (2004) used similar measures (plus three
others) to induce semantic classes for comparing
domain concept independence and porting
concepts across domains. Potamianos (2005,
2006, 2007) and colleagues conducted a series of
studies to further improve semantic class
induction, including combining wide and narrow
context similarity measures, and adopting a
soft-clustering algorithm (via a probabilistic
class-membership function).
</bodyText>
<subsectionHeader confidence="0.969507">
2.1 Clustering
</subsectionHeader>
<bodyText confidence="0.97909024">
In general, words and phrases which appear in
similar context usually share similar semantics.
E.g., 清华大学(Tsinghua University) and 北京
大学(Peking University) in the following two
utterances (literal translations are given in
brackets) are both names of place or
organisation.
请 找 清华大学 附近 的 银行。
Please/look for/Tsinghua University/near//bank
(Please look for banks near Tsinghua
University.)
请 找 北京大学 附近 的 体育馆。
Please/look for/Peking University/nearby//gym
(Please look for gyms near Peking University.)
To automatically discover that the above two
words have similar semantics from unannotated
corpus, we try unsupervised clustering based on
some similarity measures to induce semantic
classes. Further details about similarity measures
are given in section 2.2.
Before clustering, the utterances are
segmented into phrases using a simple maximum
matching against a lexicon. Clustering are
conducted on phrases, which may be of a single
word.
</bodyText>
<subsectionHeader confidence="0.998641">
2.2 Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999873777777778">
For lexical distributional similarity, several
measures have been proposed and adopted, e.g.,
Meng and Siu (2002), Lin(1998), Dagan et al.
(1999), Weeds et al. (2004).
We use two kinds of similarity measures in
the experiments. One is similarity measure based
on distance, and the other is a new similarity
measure directly using the co-occurrence
probabilities.
</bodyText>
<subsectionHeader confidence="0.990046">
2.3 Distance based similarity measures
</subsectionHeader>
<bodyText confidence="0.967862">
The relative entropy between two probability
mass functions p(x) and q(x) is defined by
(Cover and Thomas, 2006) as:
</bodyText>
<equation confidence="0.996257">
p x
( )
(  ||) = ∑ p x
( ) log = Ep
D p q
x
q ( )
</equation>
<bodyText confidence="0.999690833333334">
The relative entropy, as an asymmetric
distance between two distributions, measures the
inefficiency of assuming that the distribution is
q when the true distribution is p .
It is commonly used as a statistical distance
and can be symmetry as follows:
</bodyText>
<equation confidence="0.976814">
div p q = D p q + D q p
( , ) (  ||) (  ||) (2)
</equation>
<bodyText confidence="0.828901666666667">
For two words in a similar context, e.g., in
the sequence { ..., w −1, w, w1 ,... },
where w can be word a or b , the right
</bodyText>
<equation confidence="0.992505909090909">
bigram 1(  ||)
D aR bR and 1(  ||)
D bR aR are
defined as:
D1 (a bR) =∑p(w1|a)log p(w1|a) (3)
w1
1∈ W p(w|b)
and
D1(bR  ||aR) = ∑ p(w  |b)log p(w  |b) (4) 1∈ W p(w1 |a)
1
w
</equation>
<bodyText confidence="0.9878625">
where W is the set of words or phrases.
And the symmetric divergence is
</bodyText>
<equation confidence="0.998211">
div1 (aR,bR) =D1(aR  ||bR)+D1(bR ||aR)(5)
</equation>
<bodyText confidence="0.931781666666667">
The left bigram symmetric divergence can be
similarly defined.
Using both left and right symmetric
divergences, the distance between a and b
is
d a b = div aL bL + div aR bR
</bodyText>
<equation confidence="0.99228">
1( , ) 1 ( , ) 1 ( , ) (6)
</equation>
<bodyText confidence="0.893365166666667">
So the KL distance becomes:
This is the widely used distance measure for
lexical semantic similarity, e.g., Dagan et al.
(1999); Meng and Siu (2002); Pargellis et al
(2004). We can also see the IR distance and L1
distance below:
</bodyText>
<equation confidence="0.962375108108108">
p(w  |a) log
− 1
W p(w |b)
−1
p(w  |b) log
− 1
W p(w |a)
−1
−
log
W p(w  |b)
1
log
W p(w |a)
1
=
∑
+
w
+
w ∈
1
∑
∈
1
∑
p(w |
1
a)
+ ∑ p(w  |b)
1
1
−
p(w
w
1∈
∈
1
w−
1
p(w
 |a)
1
p(w
|b)
1
−
(7)
p(w
KL(a, b)
= div(a , b
L L
) + div(a , b )
R R
x X
∈
(1)
p ( )
x
log
q ( )
x
,...
..., w−2,w−1, w,w1,w2
+
w
∑
∈ +
W p(w  |a)
2p(w−1
 |b)
1
−
p(w|
</equation>
<figure confidence="0.912733964285714">
−1
b)log
2p(w  |a)
1
∑
p(w|
1
a)l
+
og
p (w  |b)
1
w1
∈ p (w  |a)+
W 1
2p(w  |b)
1
∑
p(w|
1
b)l
+
og
p(w  |b)
1
w1
∈ p(w  |a)+
W 1
</figure>
<equation confidence="0.982786294117647">
IR(a,b)
w ∈+
W p(w  |a)
− 1
− 1
∑
p(w|
−1
a)log
=
p(w  |b)
− 1
p(w  |b)
− 1
p(w − 1
2
 |a)
</equation>
<bodyText confidence="0.878486666666667">
We can see from the IR metric that it is
similar to the KL distance. Manhattan-norm (L1)
distance :
</bodyText>
<equation confidence="0.9668153125">
L (a,b)
1
= |p(w1|a)−p(w1|b)|
1∈
|
(w1
a) − p(w  |b) |
1
w1∈
∑
W
+ ∑ |
W
p
w
−
</equation>
<bodyText confidence="0.9988966">
In Pargellis et al. (2004), the lexical context
is further extended from bigrams to trigrams as
follows. For the sequence:
where w can be word a or b , the trigram
KL between a and b is:
</bodyText>
<equation confidence="0.998855571428571">
p(w w  |a)
− −
2 1
,wW
∈ p(w w  |b)
w − −
2 1
− −
2 2
∑ a) log
− −
2 1
KL (a,b)= p(w w |
2
p(w w  |b)
− −
2 1
+ ∑p(w w  |b) log
− −
2 1
− −
w 2,wW pw−2w−1 |a)
+∑p(Aw2 |a)logp(�2  |a)
,w W
∈ p(w w  |b)
w 1 2
1 2
p(w w  |b)
1 2
+ ∑ p(w w  |b) log
1 2
,w W
∈ p(w w  |a)
w 1 2
1 2
</equation>
<bodyText confidence="0.9993518">
Since more information is taken into account
in KL2(a,b), more constraints are imposed on the
similarity measure. This is expected to improve
the precision of clustering but may lead to a lower
recall.
</bodyText>
<footnote confidence="0.966598">
2.4 Co-occurrence Probability based
similarity measures
After a close investigation of the corpus, we
came up with an intuitive similarity measure
directly based on the co-occurrence probability.
</footnote>
<figure confidence="0.929161733333333">
=
S1
S (a,b)
3
W
w−1,
w1∈
∑
mi
=
n(p(w −
S (a,b)
1
 |a),p(w  |b))
1 −1
</figure>
<bodyText confidence="0.953994444444444">
The key idea is that the more common
neighbouring words or phrases any two words or
phrases in question share, the more similar they
are to each other. Therefore, for each left or right
neighboring word or phrase, we take the lower
conditional probability into account.
Thus we have the following similarity
measures:
Similarity using the bigram context
</bodyText>
<figure confidence="0.997932866666667">
W
+ ∑min(p(w  |a),
1
w1
Similarity using the trigram context
(11)
S (a,b)= min (p
2
(w w  |a),p(w w  |b))
− −
2 1 − −
2 1
∑
w − w − W
∈
2, 1
+ min(p(w w  |a),p(w w
1 2 1 2
w1
∑
,w W
2∈
∈
w−1
 |b))
p(w1
∈
W
(12)
 |b))
</figure>
<bodyText confidence="0.890327">
Similarity extending S1(a,b), taking both left
and right contexts into account simultaneously
</bodyText>
<equation confidence="0.975667">
+ ∑ min(p(w w  |a),p(w w  |b))(13)
− 1 1 −1 1
</equation>
<bodyText confidence="0.998750333333333">
After pairs of words or phrases are clustered
above, those pairs with common members are
further merged.
</bodyText>
<equation confidence="0.969509666666667">
)
P
(w1  |b
</equation>
<bodyText confidence="0.907881666666667">
seeing P(w1  |a) as x and seeing
as , the equation changed to:
y
</bodyText>
<equation confidence="0.997544857142857">
KL (a, b) =
R ∑
a)log
w1∈ W p(w1  |b)
(
p(w |
1
p(w1
 |a)
(14)
)
 |b)
 |b)
p(w1
log
 |a)
+ p(w1
p(w1
KL R (x, y) =∑(x log x + ylog y (15)
)y x
− 1
</equation>
<footnote confidence="0.472263">
and SR (x, y) becomes to:
S R (x, y) = ∑ min( x, y) (16)
We can also get the IRR (x, y) and L1R (x, y)|
</footnote>
<subsectionHeader confidence="0.514918">
2.5 Comparison of measures
</subsectionHeader>
<bodyText confidence="0.7073062">
The KL distances emphasize on the difference of
(10) two probability but the new measure take the
probability itself into account. Take the right
bigram context the similarity measure for
example:
</bodyText>
<equation confidence="0.759357">
x=y=z (20)
</equation>
<bodyText confidence="0.999940363636364">
We can see from the four figures (the space
distribution of four bigram metrics) that four
curve surface are all symmetric. The curve
surface of the three distance (KL,IR, L1) all
contain the curve of (19), and curve surface of
the minimum similarity contains the curve of
(20). We say that the KL distances, IR distances
and L1 distances all emphasize only on the
distances and don&apos;t take the probability itself into
account.
We take the right context of two pairs
</bodyText>
<equation confidence="0.9464555">
(a1 , b1) and (a 2 , b 2) for example. If
p(w  |a1)=0. 1, p(w1 |a1)=0.9
p(w  |b1)=0. 1, p(w2|b1)=0. 9
SR(a2, b2) =min(p(w |a2), p(w |b2))
min(0.9,0.9
0.9
</equation>
<bodyText confidence="0.9781118">
The KL calculation result of two pairs is the
same but the new similarity calculated that
(a 2 , b2) is more similar than (a1 , b1 )
because they have more similar context
probability 0.9.
</bodyText>
<sectionHeader confidence="0.996167" genericHeader="background">
3 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.985533">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9997385">
In our experiments, four types of corpora are
exploited in different stages and different ways.
</bodyText>
<listItem confidence="0.999945333333333">
• T: A large collection of text corpus is used
to train a general n-gram language model.
• H: Some WOZ dialogues were collected
</listItem>
<bodyText confidence="0.789897">
before the system is built, using a similar
scenario where users talked in Chinese to a
service provider (human) via telephone to
search for local information, or information
about some local points of interest (POI).
These dialogues were manually transcribed
and used for language model training. This
is the best data we could get before the
</bodyText>
<figure confidence="0.56510425">
IR R(x, y)=∑(x log 2x + ylog 2y ) (17)
x+y x+
and L1R (x,y)= |x − y  |(18)
We can see the space distribution in Figure.1.
</figure>
<figureCaption confidence="0.957293">
Figure 1. Space distribution of different metrics
</figureCaption>
<figure confidence="0.99917671">
y
x
y
z=0
(19)
= 0.9, p(w3  |a2
p w &apos; b = 0. 1
(  |2) 0.9, ( 4  |2 )
p w b =
The calculation is shown as follows:
)
p(w&apos;
 |a2
)
=
0.1
*
*
=
=
+
0.9
0.9
0.9
0.9
0
0.9
0.9
log
log
)
min(0. 1,0. 1
*
*
=
=
+
0.1
0.1
0.1
0
0.1
0.1
0.1
log
log
p(w
p(w
 |a )
1
 |b )
1
p(w
p(w
 |b )
1
 |a )
1
+ p(w  |b ) log
1
=
p(w
(a
b1
 |a )log
1
KLR
1
,
)
S (a , b ) =min (p(w |a ), p(w  |b ))
R 1 1 1 1
0. 1
&apos;
p(w &apos;
|a 2
)
=
&apos;
2
KL R( a2, b
 |a )log
2
p(w
)
)
p(w
 |b 2
&apos;
&apos;
p(w
&apos;
|b 2
)
)
+ p(w  |b 2
)log
p(w
 |a 2
)
</figure>
<bodyText confidence="0.5315985">
system is built though it is not the real but
near in-domain data.
</bodyText>
<listItem confidence="0.985212">
• C: After the initial system was up running,
some real human-computer dialogues were
collected and transcribed. These dialogues
were split into three sets. One (C1) is used
for semantic class and structure induction.
One (C2) is used as test data. The other (C3)
is reserved.
• A: Domain information (domain entities) is
used in conjunction with the induced
semantic classes and structures from C1 to
generate a large amount of in-domain
</listItem>
<bodyText confidence="0.6446646">
corpus for language model adaptation. In
Table 1, we give some statistics in terms
of the number of utterances(no. u) and
Chinese characters(no. c) for the above
corpora.
</bodyText>
<table confidence="0.998541857142857">
corpus no.u no.c
T 38,636 8,706,340
H 6,652 151,460
C1 658 15,434
C2 1,000 19,284
C3 411 8,014
A 14,205 365,576
</table>
<tableCaption confidence="0.999935">
Table 1. statistics of different corpus
</tableCaption>
<subsectionHeader confidence="0.999308">
3.2 Semantic Clustering
</subsectionHeader>
<bodyText confidence="0.9778218">
We conducted clustering with the above
similarity measures on the data set C1.
During the clustering, it is required that all the
probabilities involved in calculating similarity be
larger than 0. We have no threshold except this
constraint.
The outcomes are pairs of phrases.
It is noticed that most of the clustered words
and phrases are domain entities.
In our experiments, we merged the induced
similar pairs into large clusters. For example, if
a is similar to b and b is similar to c, then
(a , b , c) are merged into one category. In the
end we use the categories to replace those words
and phrases in corpus C1 and obtained templates.
Examples of the results are given below.
$ask $toponym $near $wh-word $sevice
[麻烦] $ask $toponym $near 有 $sevice 吗
我 在 $toponym $ask 怎么t $poi
where:
</bodyText>
<equation confidence="0.99991">
$ask = 请问  |问--F |查询--F  |...
$toponym = 清华大学  |Utf路  |...
$sevice = VTT  |加油站  |体-ft馆  |...
$near = ffDd  |IBJ围  |...
$wh-word = 有没有  |有ft么  |有哪些  |...
$poi = 北京饭店  |Q家体-ft馆  |...
</equation>
<bodyText confidence="0.9999585">
To evaluate the induction performance, we
compare the induced word pairs against manual
annotation. We manually annotated each phrase
with a tag like $toponym, $poi and so on. If a
and b are calculated as a pairs and the
annotation is the same, we see that they are
correctly induced which is referred to Pangos
(2006).
We compute the metrics of precision P ,
recall R and f-score F1 as follows:
</bodyText>
<equation confidence="0.990227">
P=x100% (21)
</equation>
<bodyText confidence="0.9340245">
where m is the number of correctly induced
pairs, and M is the number of induced pairs.
</bodyText>
<equation confidence="0.99751">
R _ N x 100% (22)
</equation>
<bodyText confidence="0.999915666666667">
where n is the number of correctly induced
words and phrases, and N is the number of
words and phrases in the annotation.
</bodyText>
<equation confidence="0.9678045">
F1 _ 2xPxRx100%23
P+R ( )
</equation>
<bodyText confidence="0.664008">
which is a harmonic mean of P and R .
</bodyText>
<figureCaption confidence="0.996423">
Figure 2. Induction process
</figureCaption>
<bodyText confidence="0.997879444444444">
The iterate process we adopted is as in
Pargellis et al. (2004). In the first iteration, we
calculated the similarity and use the largest
similarity pairs to generate large classes which
can be called semantic generalizer. Then we use
these semantic classes to replace the corpus, and
obtained new corpus just as the example
presented above. Then we duplicate this process
for the second iteration and so on.
</bodyText>
<figureCaption confidence="0.99127775">
Figure 3. Precision according to iterations
induced by KL and S1 similarity measure
Figure 4. Recall according to iterations induced
by KL and S1 similarity measure
Figure 5. F1 according to iterations induced by
KL and S1 similarity measure
Figure 6. F1 according to iterations induced by
all bigram similarity measure
</figureCaption>
<bodyText confidence="0.999491235294118">
From figures (Figure 3-6), we can see that
clustering with our new co-occurrence
probability based similarity measures
outperforms that with the widely used relative
entropy based distance measure consistently for
both bigram and trigram contexts. This confirms
the effectiveness of our new and simple measure.
Regarding the context size, the results from
using the bigram context outperforms that from
using the trigram context in precision. But recall
and F1 drops a lot. This is due to that larger
contexts bring more constraints. The context size
effect holds for both types of similarity measures.
And the best performance is achieved with the
similarity measure S3. It is based on S1 and
takes both left and right contexts into account at
the same time.
</bodyText>
<subsectionHeader confidence="0.998795">
3.3 Corpus Generation
</subsectionHeader>
<bodyText confidence="0.995533444444444">
Since the number of the domain entities
(terminals) we can collect from the dialogues is
very limited, we have to expand those variables
(non-terminals) in the induced templates with
domain information from the application
database and relevant web sites. For example, we
used all the words and phrases in the toponym
cluster, e.g., ̏MA* i lk  |U 4 M  |...&amp;quot;, to
replace $toponym in the templates above. Then
we generated a large collection of artificial data
which has a good coverage in both the utterance
structures (the way people speak) and the domain
entities. This resulted in the generated corpus A
in Table 1. In generation we used the semantic
classes and structures induced with S3 and
manually corrected some obvious errors. In the
generated data, there are 14,205 utterances and
365,576 Chinese characters.:
</bodyText>
<subsectionHeader confidence="0.681405">
3.4 Language Model Adaptation
</subsectionHeader>
<bodyText confidence="0.999989684210526">
There are some language model adaptation
(LMA) work oriented to the dialogue systems e.g.
Wang et al(2006), Hakkani-Tür et al.(2006),
Bellegarda(2004). So far major effort has been
spent on adaptation for large vocabulary speech
recognition or transcription tasks. But recently
there have been a few studies that are oriented
toward dialogue systems, e.g. Wang et al(2006),
Hakkani-Tür et al.(2006). In our experiments,
three trigram language models were built, each
trained separately on the large text collection (T),
on the WOZ data (H) and on the artificially
generated data (A). These trigram models were
then combined through model interpolation as
follows: We used the linear interpolation to adapt
language model. The formula is shown as follows.
T is the out-of-domain data, H is the
humane-to-humane dialogues, and A is the
corpus generated by grammars
</bodyText>
<equation confidence="0.984609916666667">
P(w  |w w )= λ P (w  |w w )
i i − −
1 2
i T T i i− −
1 2
i
+ λ P (w  |w w )
H H i i− −
1 2
i
+ λ P (w  |w
A A i i
</equation>
<bodyText confidence="0.929962806451613">
where 0 &lt; AT,AH,AA &lt;1 and AT +AH +AA =1 .
The weights were determined empirically on
the held-out data (C3 in Table 1}).
All the language models were built with the
Stolcke(2002)’s {SRILM} toolkit.
Why we did not use the C corpus directly is that it
does&apos;t have a good covering on the
domain-entities and other users usually say
utterances similar to C in structures but different
domain entities. So we use the good covering
generated data to make LMA.
We evaluated the different language models
with both intrinsic and extrinsic metrics. For
intrinsic evaluation, we computed the perplexity.
For extrinsic evaluation, we ran speech
recognition experiments on the test data C2 and
calculated the character error rate (CER).
We can see that corpus A is useful to make
model adaptation and it is closer to the in-domain
data than the human-human data for
human-computer dialogues. By using these
generated sentences, our domain-specific
Chinese speech recognition have a growth from
85.2% to 91.4%.
λ , 1, 0.2, 0.2, 0.2,
T 0, 0.8, 0, 0.4,
λ ,
H
λA 0 0 0.8 0.4
PP 984 95.4 33.6 23.3
CER(%) 32.3 14.8 10.7 9.0
</bodyText>
<tableCaption confidence="0.794093">
Table 2. perplexity and character error rate
according to model interpolation
</tableCaption>
<bodyText confidence="0.999972037037037">
The optimized weights (0.2,0.4,0.4) is
obtained from the develop sets C3. From Table 2,
we can see that language models built using
additional dialogue related data, either
human-human/WOZ dialogues or data
generated from human-computer dialogues,
shows significant improvement in both
perplexity and speech recognition performance
over the one built with the general text data only.
For the two dialogue related data, the generated
data is better than the WOZ data or closer to the
test data, since perplexity further drops from
103.5 to 38.1 and CER drops from 14.8 to 10.7.
This confirms our conjecture that human-human
WOZ dialogue data is near in-domain and not
very proper for human-computer dialogues.
Therefore, to effectively improve language
modeling for human-computer dialogues, we
need more in-domain data, even if it is generated
or artificial. The best language model is obtained
through interpolation of both language models
from dialogue related data with the one from
general text data. This may be because there is
still some mismatch between data sets C1 (for
induction and generation) and C2 (for test).
And some of the missing bits in C1 appeared in
the WOZ data (corpus A).
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
4 Related Works
</sectionHeader>
<bodyText confidence="0.999943947368421">
The most relevant work to ours is done by Wang
et al. (2006), who generated in-domain data
through out-of-domain data transformation. First
some artificial sentences are generated through
parsing and reconstructing out-of-domain data
and the illegal ones are filtered out. Then the
synthetic corpus is sampled to achieve a desired
probability distribution, based on either
simulated dialogues or semantic information
extracted from development data. But we used a
different approach in producing more in-domain
data. First semantic classes and structures are
induced from limited human-computer dialogues.
Then large amount of artificial in-domain corpus
is generated with the induced semantic classes
and patterns augmented with domain entities.
The main difference between the two works lies
in how the data is generated and how the
generated data helped.
</bodyText>
<figure confidence="0.83295825">
(24)
)
1 2
w i −
</figure>
<sectionHeader confidence="0.931352" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999984928571429">
In this paper, we described our work on
generating in-domain corpus using the
auto-induced semantic classes and structures for
language model adaptation in a Chinese voice
search dialogue system. In inducing semantic
classes we proposed a novel co-occurrence
probability based similarity measure. Our
experiments show that the simple co-occurrence
probability based similarity measure is effective
for semantic clustering which is used in our
experiment. For interpolation based language
model adaptation, the data generated using the
induced semantic classes and structures
enhanced with domain entities helped a lot for
human-computer dialogues. Despite that we
dealt with the language of Chinese, we believe
that that approaches we employed are language
independent and can be applied to other
languages as well.
In our experiment we noticed that the
performance of semantic clustering was affected
quite a lot by the noises in the data. For future
work, we would like to investigate how to
further improve the robustness of semantic
clustering in noisy spoken language. The
semantic structures induced above are very
shallow. We would like to investigate how to
find deep semantics and relations in the data.
</bodyText>
<sectionHeader confidence="0.975054" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9769926">
This work is partially supported by The National
Science &amp; Technology Pillar Program
(2008BAI50B03), National Natural Science
Foundation of China (No. 10925419, 90920302,
10874203, 60875014).
</bodyText>
<sectionHeader confidence="0.998031" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999227705882353">
Arai, K. J. H., Wright, G. Riccardi, and Gorin, A. L.
“Grammar fragment acquisition using syntactic
and semantic clustering,” Speech Communication,
vol. 27, iss. 1, pp. 43–62, 1999
Bellegarda, J. R. Statistical language model
adaptation: review and perspectives, Speech
Communication, vol. 42, iss. 1, pp. 93–108, 2004
Cover, T. M. and Thomas, J. A., Elements of
Information Theory. Wiley-Interscience, 2006
Dagan, I., Lee, L. and Pereira, F. C. N.
“Similarity-Based Models of Word Cooccurrence
Probabilities,” Machine Learning, 1999
Gorin, A. L. “On automated language acquisition,”
Acoustical Society of America Journal, vol. 97, pp.
3441–3461, 1995
Hakkani-Tür, D. Z., Riccardi, G. and Tur, G. An
active approach to spoken language processing,
ACM Transactions on Speech and Language
Processing (TSLP), vol. 3, iss. 3, pp. 1–31, 2006
Lin, D. “An information-theoretic definition of
similarity,” in Proc. ICML ’98: Proceedings of the
Fifteenth International Conference on Machine
Learning, 1998
Meng, H. M. and Siu, K.-C. “Semiautomatic
Acquisition of Semantic Structures for
Understanding Domain-Specific Natural Language
Queries,” IEEE Trans. Knowl. Data Eng. 2002
Pargellis, A. N., Fosler-Lussier, E., Fosler-Lussier,
Lee, C.-H., Potamianos, A. and Tsai, A.
“Auto-induced semantic classes,” Speech
Communication, vol. 43, iss. 3, pp. 183–203, 2004
Pangos, A Combining statistical similarity measures
for automatic induction of semantic classes, 2005
Pangos, A., Iosif, E. and Tegos, A. Unsupervised
combination of metrics for semantic class
induction, SLT 2006, 2006
Pangos, A. and Iosif, E., A Soft-Clustering Algorithm
for Automatic Induction of Semantic Classes,
interspeech07, 2007
Stolcke, A. SRILM – an extensible language
modeling toolkit, in Proc. ICSLP, 2002
Wang, C. Chung, G. and Seneff, S. Automatic
induction of language model data for a spoken
dialogue system, Language Resources and
Evaluation, vol. 40, iss. 1, pp. 25–46, 2006
Wang, Y.-Y. and Dong Yu, E. A., An introduction to
voice search, Signal Processing Magazine, IEEE,
vol. 25, iss. 3, pp. 28–38, 2008
Weeds, J., Weir, D. and McCarthy, D.
“Characterising measures of lexical distributional
similarity,” in Proc. in Proc. COLING ’04, 2004,
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.019190">
<title confidence="0.971418">Semantic class induction and its application for a Chinese voice search system</title>
<author confidence="0.999779">Yali Li</author>
<affiliation confidence="0.913423">ThinkIT laboratory, Institute Acoustics, Academy of Sciences</affiliation>
<email confidence="0.65476">liyali@hccl.ioa.ac.cn</email>
<title confidence="0.3237">Weiqun ThinkIT Institute of Chinese Academy xuweiqun@hccl.ioa.ac.cn</title>
<author confidence="0.451686">Yonghong</author>
<affiliation confidence="0.9283975">ThinkIT laboratory, Institute Acoustics, Academy of Sciences</affiliation>
<email confidence="0.804002">yyan@hccl.ioa.ac.cn</email>
<abstract confidence="0.9840325625">In this paper, we propose a novel similarity measure based co-occurrence probabilities for inducing semantic classes. Clustering with the new similarity measure outperformed that with the widely used distance measure based on Kullback-Leibler divergence in precision, recall and F1 evaluation. We then use the induced semantic classes and structures by the new similarity measure to generate in-domain data. At last, we use the generated data to do language model adaptation and improve the result of character recognition from 85.2% to 91%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K J H Arai</author>
<author>G Riccardi Wright</author>
<author>A L Gorin</author>
</authors>
<title>Grammar fragment acquisition using syntactic and semantic clustering,”</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<volume>27</volume>
<pages>43--62</pages>
<contexts>
<context position="2540" citStr="Arai et al. (1999)" startWordPosition="377" endWordPosition="380">e semantic classes and structures. We then generated a large number of data using the induced semantic classes and structures to make language model adaptation. At the end, we give the conclusion and implied the future work. 2 Semantic Class Induction The studies on semantic class induction in spoken language (or spoken language acquisition in general) have received some attention since the middle 90&apos;s. One of the earlier works is carried out by Gorin (1995), who employed an information -theoretic connectionist network embedded in a feedback control system to acquire spoken language. Later on Arai et al. (1999) further studied how to acquire grammar fragments in fluent speech through clustering similar phrases using Kullback-Leibler distance. Meng and Siu (2002) proposed to semi-automatically induce language structures from unannotated corpora for spoken language understanding, mainly using Kullback-Liebler divergence and mutual information. Pargellis et al. (2004) used similar measures (plus three others) to induce semantic classes for comparing domain concept independence and porting concepts across domains. Potamianos (2005, 2006, 2007) and colleagues conducted a series of studies to further impr</context>
</contexts>
<marker>Arai, Wright, Gorin, 1999</marker>
<rawString>Arai, K. J. H., Wright, G. Riccardi, and Gorin, A. L. “Grammar fragment acquisition using syntactic and semantic clustering,” Speech Communication, vol. 27, iss. 1, pp. 43–62, 1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>Statistical language model adaptation: review and perspectives,</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>42</volume>
<pages>93--108</pages>
<marker>Bellegarda, 2004</marker>
<rawString>Bellegarda, J. R. Statistical language model adaptation: review and perspectives, Speech Communication, vol. 42, iss. 1, pp. 93–108, 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory. Wiley-Interscience,</title>
<date>2006</date>
<contexts>
<context position="4819" citStr="Cover and Thomas, 2006" startWordPosition="696" endWordPosition="699">ainst a lexicon. Clustering are conducted on phrases, which may be of a single word. 2.2 Similarity Measures For lexical distributional similarity, several measures have been proposed and adopted, e.g., Meng and Siu (2002), Lin(1998), Dagan et al. (1999), Weeds et al. (2004). We use two kinds of similarity measures in the experiments. One is similarity measure based on distance, and the other is a new similarity measure directly using the co-occurrence probabilities. 2.3 Distance based similarity measures The relative entropy between two probability mass functions p(x) and q(x) is defined by (Cover and Thomas, 2006) as: p x ( ) ( ||) = ∑ p x ( ) log = Ep D p q x q ( ) The relative entropy, as an asymmetric distance between two distributions, measures the inefficiency of assuming that the distribution is q when the true distribution is p . It is commonly used as a statistical distance and can be symmetry as follows: div p q = D p q + D q p ( , ) ( ||) ( ||) (2) For two words in a similar context, e.g., in the sequence { ..., w −1, w, w1 ,... }, where w can be word a or b , the right bigram 1( ||) D aR bR and 1( ||) D bR aR are defined as: D1 (a bR) =∑p(w1|a)log p(w1|a) (3) w1 1∈ W p(w|b) and D1(bR ||aR) =</context>
</contexts>
<marker>Cover, Thomas, 2006</marker>
<rawString>Cover, T. M. and Thomas, J. A., Elements of Information Theory. Wiley-Interscience, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F C N Pereira</author>
</authors>
<title>Similarity-Based Models of Word Cooccurrence Probabilities,” Machine Learning,</title>
<date>1999</date>
<contexts>
<context position="4450" citStr="Dagan et al. (1999)" startWordPosition="639" endWordPosition="642">versity.) To automatically discover that the above two words have similar semantics from unannotated corpus, we try unsupervised clustering based on some similarity measures to induce semantic classes. Further details about similarity measures are given in section 2.2. Before clustering, the utterances are segmented into phrases using a simple maximum matching against a lexicon. Clustering are conducted on phrases, which may be of a single word. 2.2 Similarity Measures For lexical distributional similarity, several measures have been proposed and adopted, e.g., Meng and Siu (2002), Lin(1998), Dagan et al. (1999), Weeds et al. (2004). We use two kinds of similarity measures in the experiments. One is similarity measure based on distance, and the other is a new similarity measure directly using the co-occurrence probabilities. 2.3 Distance based similarity measures The relative entropy between two probability mass functions p(x) and q(x) is defined by (Cover and Thomas, 2006) as: p x ( ) ( ||) = ∑ p x ( ) log = Ep D p q x q ( ) The relative entropy, as an asymmetric distance between two distributions, measures the inefficiency of assuming that the distribution is q when the true distribution is p . It </context>
<context position="5904" citStr="Dagan et al. (1999)" startWordPosition="936" endWordPosition="939">the right bigram 1( ||) D aR bR and 1( ||) D bR aR are defined as: D1 (a bR) =∑p(w1|a)log p(w1|a) (3) w1 1∈ W p(w|b) and D1(bR ||aR) = ∑ p(w |b)log p(w |b) (4) 1∈ W p(w1 |a) 1 w where W is the set of words or phrases. And the symmetric divergence is div1 (aR,bR) =D1(aR ||bR)+D1(bR ||aR)(5) The left bigram symmetric divergence can be similarly defined. Using both left and right symmetric divergences, the distance between a and b is d a b = div aL bL + div aR bR 1( , ) 1 ( , ) 1 ( , ) (6) So the KL distance becomes: This is the widely used distance measure for lexical semantic similarity, e.g., Dagan et al. (1999); Meng and Siu (2002); Pargellis et al (2004). We can also see the IR distance and L1 distance below: p(w |a) log − 1 W p(w |b) −1 p(w |b) log − 1 W p(w |a) −1 − log W p(w |b) 1 log W p(w |a) 1 = ∑ + w + w ∈ 1 ∑ ∈ 1 ∑ p(w | 1 a) + ∑ p(w |b) 1 1 − p(w w 1∈ ∈ 1 w− 1 p(w |a) 1 p(w |b) 1 − (7) p(w KL(a, b) = div(a , b L L ) + div(a , b ) R R x X ∈ (1) p ( ) x log q ( ) x ,... ..., w−2,w−1, w,w1,w2 + w ∑ ∈ + W p(w |a) 2p(w−1 |b) 1 − p(w| −1 b)log 2p(w |a) 1 ∑ p(w| 1 a)l + og p (w |b) 1 w1 ∈ p (w |a)+ W 1 2p(w |b) 1 ∑ p(w| 1 b)l + og p(w |b) 1 w1 ∈ p(w |a)+ W 1 IR(a,b) w ∈+ W p(w |a) − 1 − 1 ∑ p(w| </context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Dagan, I., Lee, L. and Pereira, F. C. N. “Similarity-Based Models of Word Cooccurrence Probabilities,” Machine Learning, 1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
</authors>
<title>On automated language acquisition,”</title>
<date>1995</date>
<journal>Acoustical Society of America Journal,</journal>
<volume>97</volume>
<pages>3441--3461</pages>
<contexts>
<context position="2384" citStr="Gorin (1995)" startWordPosition="356" endWordPosition="357">grammars manually is tedious and time-consuming and requires some linguistic expertise. In this paper, we introduced a new similarity measure to induce semantic classes and structures. We then generated a large number of data using the induced semantic classes and structures to make language model adaptation. At the end, we give the conclusion and implied the future work. 2 Semantic Class Induction The studies on semantic class induction in spoken language (or spoken language acquisition in general) have received some attention since the middle 90&apos;s. One of the earlier works is carried out by Gorin (1995), who employed an information -theoretic connectionist network embedded in a feedback control system to acquire spoken language. Later on Arai et al. (1999) further studied how to acquire grammar fragments in fluent speech through clustering similar phrases using Kullback-Leibler distance. Meng and Siu (2002) proposed to semi-automatically induce language structures from unannotated corpora for spoken language understanding, mainly using Kullback-Liebler divergence and mutual information. Pargellis et al. (2004) used similar measures (plus three others) to induce semantic classes for comparing</context>
</contexts>
<marker>Gorin, 1995</marker>
<rawString>Gorin, A. L. “On automated language acquisition,” Acoustical Society of America Journal, vol. 97, pp. 3441–3461, 1995</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Z Hakkani-Tür</author>
<author>G Riccardi</author>
<author>G Tur</author>
</authors>
<title>An active approach to spoken language processing,</title>
<date>2006</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>3</volume>
<pages>1--31</pages>
<marker>Hakkani-Tür, Riccardi, Tur, 2006</marker>
<rawString>Hakkani-Tür, D. Z., Riccardi, G. and Tur, G. An active approach to spoken language processing, ACM Transactions on Speech and Language Processing (TSLP), vol. 3, iss. 3, pp. 1–31, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity,” in</title>
<date>1998</date>
<booktitle>Proc. ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning,</booktitle>
<marker>Lin, 1998</marker>
<rawString>Lin, D. “An information-theoretic definition of similarity,” in Proc. ICML ’98: Proceedings of the Fifteenth International Conference on Machine Learning, 1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>H M Meng</author>
<author>K-C Siu</author>
</authors>
<title>Semiautomatic Acquisition of Semantic Structures for Understanding Domain-Specific Natural Language Queries,”</title>
<date>2002</date>
<journal>IEEE Trans. Knowl. Data Eng.</journal>
<contexts>
<context position="2694" citStr="Meng and Siu (2002)" startWordPosition="398" endWordPosition="401">tation. At the end, we give the conclusion and implied the future work. 2 Semantic Class Induction The studies on semantic class induction in spoken language (or spoken language acquisition in general) have received some attention since the middle 90&apos;s. One of the earlier works is carried out by Gorin (1995), who employed an information -theoretic connectionist network embedded in a feedback control system to acquire spoken language. Later on Arai et al. (1999) further studied how to acquire grammar fragments in fluent speech through clustering similar phrases using Kullback-Leibler distance. Meng and Siu (2002) proposed to semi-automatically induce language structures from unannotated corpora for spoken language understanding, mainly using Kullback-Liebler divergence and mutual information. Pargellis et al. (2004) used similar measures (plus three others) to induce semantic classes for comparing domain concept independence and porting concepts across domains. Potamianos (2005, 2006, 2007) and colleagues conducted a series of studies to further improve semantic class induction, including combining wide and narrow context similarity measures, and adopting a soft-clustering algorithm (via a probabilist</context>
<context position="4418" citStr="Meng and Siu (2002)" startWordPosition="634" endWordPosition="637">se look for gyms near Peking University.) To automatically discover that the above two words have similar semantics from unannotated corpus, we try unsupervised clustering based on some similarity measures to induce semantic classes. Further details about similarity measures are given in section 2.2. Before clustering, the utterances are segmented into phrases using a simple maximum matching against a lexicon. Clustering are conducted on phrases, which may be of a single word. 2.2 Similarity Measures For lexical distributional similarity, several measures have been proposed and adopted, e.g., Meng and Siu (2002), Lin(1998), Dagan et al. (1999), Weeds et al. (2004). We use two kinds of similarity measures in the experiments. One is similarity measure based on distance, and the other is a new similarity measure directly using the co-occurrence probabilities. 2.3 Distance based similarity measures The relative entropy between two probability mass functions p(x) and q(x) is defined by (Cover and Thomas, 2006) as: p x ( ) ( ||) = ∑ p x ( ) log = Ep D p q x q ( ) The relative entropy, as an asymmetric distance between two distributions, measures the inefficiency of assuming that the distribution is q when </context>
<context position="5925" citStr="Meng and Siu (2002)" startWordPosition="940" endWordPosition="943">|) D aR bR and 1( ||) D bR aR are defined as: D1 (a bR) =∑p(w1|a)log p(w1|a) (3) w1 1∈ W p(w|b) and D1(bR ||aR) = ∑ p(w |b)log p(w |b) (4) 1∈ W p(w1 |a) 1 w where W is the set of words or phrases. And the symmetric divergence is div1 (aR,bR) =D1(aR ||bR)+D1(bR ||aR)(5) The left bigram symmetric divergence can be similarly defined. Using both left and right symmetric divergences, the distance between a and b is d a b = div aL bL + div aR bR 1( , ) 1 ( , ) 1 ( , ) (6) So the KL distance becomes: This is the widely used distance measure for lexical semantic similarity, e.g., Dagan et al. (1999); Meng and Siu (2002); Pargellis et al (2004). We can also see the IR distance and L1 distance below: p(w |a) log − 1 W p(w |b) −1 p(w |b) log − 1 W p(w |a) −1 − log W p(w |b) 1 log W p(w |a) 1 = ∑ + w + w ∈ 1 ∑ ∈ 1 ∑ p(w | 1 a) + ∑ p(w |b) 1 1 − p(w w 1∈ ∈ 1 w− 1 p(w |a) 1 p(w |b) 1 − (7) p(w KL(a, b) = div(a , b L L ) + div(a , b ) R R x X ∈ (1) p ( ) x log q ( ) x ,... ..., w−2,w−1, w,w1,w2 + w ∑ ∈ + W p(w |a) 2p(w−1 |b) 1 − p(w| −1 b)log 2p(w |a) 1 ∑ p(w| 1 a)l + og p (w |b) 1 w1 ∈ p (w |a)+ W 1 2p(w |b) 1 ∑ p(w| 1 b)l + og p(w |b) 1 w1 ∈ p(w |a)+ W 1 IR(a,b) w ∈+ W p(w |a) − 1 − 1 ∑ p(w| −1 a)log = p(w |b) − </context>
</contexts>
<marker>Meng, Siu, 2002</marker>
<rawString>Meng, H. M. and Siu, K.-C. “Semiautomatic Acquisition of Semantic Structures for Understanding Domain-Specific Natural Language Queries,” IEEE Trans. Knowl. Data Eng. 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Pargellis</author>
<author>E Fosler-Lussier</author>
<author>Lee Fosler-Lussier</author>
<author>C-H Potamianos</author>
<author>A</author>
<author>A Tsai</author>
</authors>
<title>Auto-induced semantic classes,”</title>
<date>2004</date>
<journal>Speech Communication,</journal>
<volume>43</volume>
<pages>183--203</pages>
<contexts>
<context position="2901" citStr="Pargellis et al. (2004)" startWordPosition="422" endWordPosition="425">ave received some attention since the middle 90&apos;s. One of the earlier works is carried out by Gorin (1995), who employed an information -theoretic connectionist network embedded in a feedback control system to acquire spoken language. Later on Arai et al. (1999) further studied how to acquire grammar fragments in fluent speech through clustering similar phrases using Kullback-Leibler distance. Meng and Siu (2002) proposed to semi-automatically induce language structures from unannotated corpora for spoken language understanding, mainly using Kullback-Liebler divergence and mutual information. Pargellis et al. (2004) used similar measures (plus three others) to induce semantic classes for comparing domain concept independence and porting concepts across domains. Potamianos (2005, 2006, 2007) and colleagues conducted a series of studies to further improve semantic class induction, including combining wide and narrow context similarity measures, and adopting a soft-clustering algorithm (via a probabilistic class-membership function). 2.1 Clustering In general, words and phrases which appear in similar context usually share similar semantics. E.g., 清华大学(Tsinghua University) and 北京 大学(Peking University) in th</context>
<context position="5949" citStr="Pargellis et al (2004)" startWordPosition="944" endWordPosition="947"> D bR aR are defined as: D1 (a bR) =∑p(w1|a)log p(w1|a) (3) w1 1∈ W p(w|b) and D1(bR ||aR) = ∑ p(w |b)log p(w |b) (4) 1∈ W p(w1 |a) 1 w where W is the set of words or phrases. And the symmetric divergence is div1 (aR,bR) =D1(aR ||bR)+D1(bR ||aR)(5) The left bigram symmetric divergence can be similarly defined. Using both left and right symmetric divergences, the distance between a and b is d a b = div aL bL + div aR bR 1( , ) 1 ( , ) 1 ( , ) (6) So the KL distance becomes: This is the widely used distance measure for lexical semantic similarity, e.g., Dagan et al. (1999); Meng and Siu (2002); Pargellis et al (2004). We can also see the IR distance and L1 distance below: p(w |a) log − 1 W p(w |b) −1 p(w |b) log − 1 W p(w |a) −1 − log W p(w |b) 1 log W p(w |a) 1 = ∑ + w + w ∈ 1 ∑ ∈ 1 ∑ p(w | 1 a) + ∑ p(w |b) 1 1 − p(w w 1∈ ∈ 1 w− 1 p(w |a) 1 p(w |b) 1 − (7) p(w KL(a, b) = div(a , b L L ) + div(a , b ) R R x X ∈ (1) p ( ) x log q ( ) x ,... ..., w−2,w−1, w,w1,w2 + w ∑ ∈ + W p(w |a) 2p(w−1 |b) 1 − p(w| −1 b)log 2p(w |a) 1 ∑ p(w| 1 a)l + og p (w |b) 1 w1 ∈ p (w |a)+ W 1 2p(w |b) 1 ∑ p(w| 1 b)l + og p(w |b) 1 w1 ∈ p(w |a)+ W 1 IR(a,b) w ∈+ W p(w |a) − 1 − 1 ∑ p(w| −1 a)log = p(w |b) − 1 p(w |b) − 1 p(w − 1 2 </context>
<context position="13687" citStr="Pargellis et al. (2004)" startWordPosition="2587" endWordPosition="2590">so on. If a and b are calculated as a pairs and the annotation is the same, we see that they are correctly induced which is referred to Pangos (2006). We compute the metrics of precision P , recall R and f-score F1 as follows: P=x100% (21) where m is the number of correctly induced pairs, and M is the number of induced pairs. R _ N x 100% (22) where n is the number of correctly induced words and phrases, and N is the number of words and phrases in the annotation. F1 _ 2xPxRx100%23 P+R ( ) which is a harmonic mean of P and R . Figure 2. Induction process The iterate process we adopted is as in Pargellis et al. (2004). In the first iteration, we calculated the similarity and use the largest similarity pairs to generate large classes which can be called semantic generalizer. Then we use these semantic classes to replace the corpus, and obtained new corpus just as the example presented above. Then we duplicate this process for the second iteration and so on. Figure 3. Precision according to iterations induced by KL and S1 similarity measure Figure 4. Recall according to iterations induced by KL and S1 similarity measure Figure 5. F1 according to iterations induced by KL and S1 similarity measure Figure 6. F1</context>
</contexts>
<marker>Pargellis, Fosler-Lussier, Fosler-Lussier, Potamianos, A, Tsai, 2004</marker>
<rawString>Pargellis, A. N., Fosler-Lussier, E., Fosler-Lussier, Lee, C.-H., Potamianos, A. and Tsai, A. “Auto-induced semantic classes,” Speech Communication, vol. 43, iss. 3, pp. 183–203, 2004</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pangos</author>
</authors>
<title>A Combining statistical similarity measures for automatic induction of semantic classes,</title>
<date>2005</date>
<marker>Pangos, 2005</marker>
<rawString>Pangos, A Combining statistical similarity measures for automatic induction of semantic classes, 2005</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pangos</author>
<author>E Iosif</author>
<author>A Tegos</author>
</authors>
<title>Unsupervised combination of metrics for semantic class induction, SLT</title>
<date>2006</date>
<marker>Pangos, Iosif, Tegos, 2006</marker>
<rawString>Pangos, A., Iosif, E. and Tegos, A. Unsupervised combination of metrics for semantic class induction, SLT 2006, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pangos</author>
<author>E Iosif</author>
</authors>
<title>A Soft-Clustering Algorithm for Automatic Induction of Semantic Classes,</title>
<date>2007</date>
<marker>Pangos, Iosif, 2007</marker>
<rawString>Pangos, A. and Iosif, E., A Soft-Clustering Algorithm for Automatic Induction of Semantic Classes, interspeech07, 2007</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit, in</title>
<date>2002</date>
<booktitle>Proc. ICSLP,</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. SRILM – an extensible language modeling toolkit, in Proc. ICSLP, 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chung Wang</author>
<author>G</author>
<author>S Seneff</author>
</authors>
<title>Automatic induction of language model data for a spoken dialogue system,</title>
<date>2006</date>
<journal>Language Resources and Evaluation,</journal>
<volume>40</volume>
<pages>25--46</pages>
<contexts>
<context position="19414" citStr="Wang et al. (2006)" startWordPosition="3549" endWordPosition="3552">ain and not very proper for human-computer dialogues. Therefore, to effectively improve language modeling for human-computer dialogues, we need more in-domain data, even if it is generated or artificial. The best language model is obtained through interpolation of both language models from dialogue related data with the one from general text data. This may be because there is still some mismatch between data sets C1 (for induction and generation) and C2 (for test). And some of the missing bits in C1 appeared in the WOZ data (corpus A). 4 Related Works The most relevant work to ours is done by Wang et al. (2006), who generated in-domain data through out-of-domain data transformation. First some artificial sentences are generated through parsing and reconstructing out-of-domain data and the illegal ones are filtered out. Then the synthetic corpus is sampled to achieve a desired probability distribution, based on either simulated dialogues or semantic information extracted from development data. But we used a different approach in producing more in-domain data. First semantic classes and structures are induced from limited human-computer dialogues. Then large amount of artificial in-domain corpus is ge</context>
</contexts>
<marker>Wang, G, Seneff, 2006</marker>
<rawString>Wang, C. Chung, G. and Seneff, S. Automatic induction of language model data for a spoken dialogue system, Language Resources and Evaluation, vol. 40, iss. 1, pp. 25–46, 2006</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-Y Wang</author>
<author>Dong Yu</author>
<author>E A</author>
</authors>
<title>An introduction to voice search,</title>
<date>2008</date>
<journal>Signal Processing Magazine, IEEE,</journal>
<volume>25</volume>
<pages>28--38</pages>
<contexts>
<context position="998" citStr="Wang et al., 2008" startWordPosition="136" endWordPosition="139">l.ioa.ac.cn Abstract In this paper, we propose a novel similarity measure based on co-occurrence probabilities for inducing semantic classes. Clustering with the new similarity measure outperformed that with the widely used distance measure based on Kullback-Leibler divergence in precision, recall and F1 evaluation. We then use the induced semantic classes and structures by the new similarity measure to generate in-domain data. At last, we use the generated data to do language model adaptation and improve the result of character recognition from 85.2% to 91%. 1 Introduction Voice search (e.g. Wang et al., 2008) has recently become one of the major foci in spoken dialogue system research and development. In main stream large vocabulary ASR engines, statistical language models (n-grams in particular), usually trained with plenty of data, are widely used and proved very effective. But for a voice search system, we have to deal with the case where there is no or very little relevant data for language modeling. One of the conventional solutions to this problem is to collect and use some human-human or Wizard-of-Oz (WOZ) dialogue data. Once the initial system is up running, the performance can be further </context>
</contexts>
<marker>Wang, Yu, A, 2008</marker>
<rawString>Wang, Y.-Y. and Dong Yu, E. A., An introduction to voice search, Signal Processing Magazine, IEEE, vol. 25, iss. 3, pp. 28–38, 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
<author>D McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity,” in</title>
<date>2004</date>
<booktitle>Proc. in Proc. COLING ’04,</booktitle>
<contexts>
<context position="4471" citStr="Weeds et al. (2004)" startWordPosition="643" endWordPosition="646">cally discover that the above two words have similar semantics from unannotated corpus, we try unsupervised clustering based on some similarity measures to induce semantic classes. Further details about similarity measures are given in section 2.2. Before clustering, the utterances are segmented into phrases using a simple maximum matching against a lexicon. Clustering are conducted on phrases, which may be of a single word. 2.2 Similarity Measures For lexical distributional similarity, several measures have been proposed and adopted, e.g., Meng and Siu (2002), Lin(1998), Dagan et al. (1999), Weeds et al. (2004). We use two kinds of similarity measures in the experiments. One is similarity measure based on distance, and the other is a new similarity measure directly using the co-occurrence probabilities. 2.3 Distance based similarity measures The relative entropy between two probability mass functions p(x) and q(x) is defined by (Cover and Thomas, 2006) as: p x ( ) ( ||) = ∑ p x ( ) log = Ep D p q x q ( ) The relative entropy, as an asymmetric distance between two distributions, measures the inefficiency of assuming that the distribution is q when the true distribution is p . It is commonly used as a</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Weeds, J., Weir, D. and McCarthy, D. “Characterising measures of lexical distributional similarity,” in Proc. in Proc. COLING ’04, 2004,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>