<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<note confidence="0.691825">
OPEN MIND WORD EXPERT:
</note>
<title confidence="0.97983">
Creating Large Annotated Data Collections with Web Users&apos; Help
</title>
<author confidence="0.998307">
Rada Mihalcea Timothy Chklovski
</author>
<affiliation confidence="0.999754">
Department of Computer Science Artificial Intelligence Laboratory
University of North Texas Massachusetts Institute of Technology
</affiliation>
<email confidence="0.99856">
rada@cs.unt.edu timc@mit.edu
</email>
<sectionHeader confidence="0.998581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999500294117647">
Open Mind Word Expert is an imple-
mented active learning system that aims
to create large annotated corpora by tap-
ping into the world&apos;s vast pool of knowl-
edge. It does this by relying on the
vast number of Web users who con-
tribute their knowledge to data anno-
tation. Open Mind Word Expert fo-
cuses on building semantically anno-
tated corpora, by collecting word sense
tagging from the general public over
the Web. It is available at http://teach-
computers.org. During the first nine
months of activity, the system yielded
90,000 high quality tagged items at a
much lower cost than the traditional
method of hiring lexicographers.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.967821469387755">
A large range of Natural Language Processing
(NLP) applications require large amounts of anno-
tated data in order to ensure good performance and
high accuracy. While recent advances in NLP re-
search brought significant improvement in the per-
formance of NLP methods and algorithms, there
has been relatively little progress on addressing
the problem of obtaining annotated data required
by some of the highest-performing algorithms. As
a consequence, many of today&apos;s NLP applications
experience severe training data bottlenecks.
One notoriously difficult problem in under-
standing text has been Word Sense Disambigua-
tion (WSD). Ambiguity is very common (espe-
cially among the most common words â€” consider
the words &amp;quot;table,&amp;quot; or the phrase &amp;quot;computer fan&amp;quot;).
Humans, however, are so competent at figuring out
word senses from context that they usually do not
even notice the ambiguities. While a large num-
ber of efficient WSD algorithms have been de-
signed and implemented to date within the recent
SENSE VAL evaluation frameworks (Kilgarriff and
Palmer, 2000), (Preiss and Yarowsky, 2001), and
elsewhere, the availability of sense tagged data is
still a significant problem.
Most of the efforts in WSD have focused
on supervised learning algorithms, which usually
achieve the best performance at the cost of low re-
call. The main weakness of these methods is the
lack of widely available semantically tagged cor-
pora and the strong dependence of disambigua-
tion accuracy on the size of the training corpus.
For instance, one study reports that high preci-
sion WSD requires at least 500 examples per am-
biguous word (Ng, 1997)1. At a throughput of
one tagged example per minute (Edmonds, 2000),
and with about 20,000 ambiguous words in the
common English vocabulary, a simple calculation
leads to about 160,000 hours of tagging, which is
nothing less than 80 man-years of human annota-
tion work.2 Since the tagging process is usually
&apos;The number of examples required for a word is highly
connected to the word entropy. 500 represents an average.
2Similar data bottleneck problems are faced by many
other NLP applications. High quality part of speech tagging
for English requires about 3 million words annotated with
their part of speech. The state-of-the-art in syntactic pars-
ing in English is close to 88-89% , performance attainable by
training parser models on a corpus of about 600,000 words
</bodyText>
<page confidence="0.997834">
53
</page>
<bodyText confidence="0.9993906">
done by trained lexicographers, it is very expen-
sive, and limits the size of such corpora to a hand-
ful of tagged texts.
In this paper, we present Open Mind Word Ex-
pert, a Web-based system that aims to create large
sense tagged corpora with the help of Web users.
The annotation workload is distributed among
millions of potential human annotators, which is
likely to significantly reduce the cost and the du-
ration of the annotation process. We investigate
the amount and quality of the data produced dur-
ing nine months of deployment of the activity, and
present results obtained during preliminary WSD
experiments that rely on this sense tagged data.
Open Mind Word Expert is a project that fol-
lows the Open Mind initiative (Stork, 1999). The
basic idea behind broad Open Mind initiative is
to use the information and knowledge obtainable
from the millions of existing Web users, to the end
of creating more intelligent software. Other Open
Mind projects related to natural language and
world knowledge include Open Mind 1001 Ques-
tions (Chklovski, 2003), which acquires knowl-
edge from millions of users, and Open Mind Com-
mon Sense (Singh, 2002).
</bodyText>
<sectionHeader confidence="0.858557" genericHeader="method">
2 Sense Tagged Corpora
</sectionHeader>
<bodyText confidence="0.9027479">
The availability of large amounts of semantically
tagged data is crucial for creating successful WSD
systems. Yet, as of today, only few sense tagged
corpora are publicly available.3
One of the first large scale hand tagging efforts
is reported in (Miller et al., 1993), where a sub-
set of the Brown corpus was tagged with Word-
Net(Miller, 1995) senses. The corpus includes a
total of 234,136 tagged word occurrences, out of
which 186,575 are polysemous. There are 88,058
noun occurrences of which 70,214 are polyse-
mous.
The next significant hand tagging task was re-
ported in (Bruce and Wiebe, 1994), where 2,476
usages of interest were manually assigned with
manually parsed within the Penn Treebank project, an anno-
tation effort that required approximately 2 man-years of work
(Marcus et al., 1993). Information extraction, automatic sum-
marization, anaphora resolution, and other tasks also strongly
require large annotated corpora.
</bodyText>
<footnote confidence="0.763503">
3See http://www.senseval.org for a complete list of re-
sources.
</footnote>
<bodyText confidence="0.999853621621622">
sense tags from the Longman Dictionary of Con-
temporary English (LDOCE). This corpus was
used in various experiments, with classification
accuracies ranging from 75% to 90%, depending
on the algorithm and features employed.
The high accuracy of the LEXAS system (Ng
and Lee, 1996) is due in part to the use of large
corpora. For this system, 192,800 word occur-
rences have been manually tagged with senses
from WordNet. The set of tagged words consists
of the 191 most frequently occurring nouns and
verbs. The authors mention that approximately
one man-year of effort was spent in tagging the
data set.
Recently, the SENSEVAL competitions have
been providing a good environment for the de-
velopment of supervised WSD systems, making
freely available large amounts of sense tagged data
for about 100 words. During S EN S EVA L- 1 (Kil-
garriff and Palmer, 2000), data for 35 words was
made available adding up to about 20,000 ex-
amples tagged with respect to the Hector dictio-
nary. The size of the tagged corpus increased with
S EN S E vAL-2 (Preiss and Yarowsky, 2001), when
13,000 additional examples were released for 73
polysemous words. This time, the semantic anno-
tations were performed with respect to WordNet.
Additionally, (Kilgarriff, 1998) mentions the
Hector corpus, which comprises about 300 word
types with 300-1000 tagged instances for each
word, selected from a 17 million word corpus.
With Open Mind Word Expert we aim to cre-
ate a very large sense tagged corpus by making
use of the incredible resource of knowledge con-
stituted by the millions of Web users. We use tech-
niques for active learning to utilize this resource
efficiently.
</bodyText>
<sectionHeader confidence="0.992978" genericHeader="method">
3 Open Mind Word Expert
</sectionHeader>
<bodyText confidence="0.998813444444445">
Open Mind Word Expert is a Web-based appli-
cation that allows contributors to annotate words
with their WordNet senses. Tagging is organized
by word: for each ambiguous word for which we
want to build a sense tagged corpus, users are pre-
sented with a set of natural language (English)
sentences that include an instance of the ambigu-
ous word.
The overall process proceeds as follows. mi-
</bodyText>
<page confidence="0.99697">
54
</page>
<bodyText confidence="0.999981586206896">
tially, example sentences are extracted from a
large textual corpus. If other training data is not
available, a number of these sentences are pre-
sented to the users for tagging in Stage 1. Next,
this tagged collection is used as training data, and
active learning is used to identify in the remain-
ing corpus the examples that are &amp;quot;hard to tag&amp;quot;.
These are the examples that are presented to the
contributors for tagging in Stage 2. For all tag-
ging, users are asked to select the sense they find
to be the most appropriate in the given sentence.
The selection is made from a drop-down list con-
taining all WordNet senses of the current word,
plus two additional choices, &amp;quot;unclear&amp;quot; and &amp;quot;none
of the above.&amp;quot; The results of any automatic clas-
sification or the classification submitted by other
users are not presented so as to not bias the con-
tributor&apos;s decisions. Based on early feedback from
both researchers and contributors, a future version
of Open Mind Word Expert may allow contribu-
tors to specify more than one sense for a given in-
stance. As will be elaborated below, the current
approach of collecting redundant tagging already
addresses this to some degree.
A prototype of the system has been imple-
mented and is available at http://www.teach-
computers.org. Figure 1 shows a screen shot from
the system interface, illustrating the screen pre-
sented to users when tagging the noun &amp;quot;plane&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.99786">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.998169615384615">
The starting corpus we use is formed by a mix of
three different sources of data, namely the Penn
Treebank corpus (Marcus et al., 1993), the Los An-
geles Times collection, as provided during TREC
conferences,4 and Open Mind Common Sense5 , a
collection of about 400,000 commonsense asser-
tions in English as contributed by volunteers over
the Web (Singh, 2002)6. A mix of several sources,
each covering a different spectrum of usage, is
used to increase the coverage of word senses and
writing styles.
Future versions of Open Mind Word Expert
will include example sentences extracted from the
</bodyText>
<footnote confidence="0.912716666666667">
4http://trec.nist.gov
5http://commonsense.media.mit.edu
6See also (Singh et al., 2002) for additional details regard-
</footnote>
<bodyText confidence="0.923648">
ing the quality of free-form entered information, evaluation,
bias, and the level of difficulty of the collected knowledge.
British National Corpus,7 and the American Na-
tional Corpus8 (the latter as soon as it will become
available).
</bodyText>
<subsectionHeader confidence="0.99985">
3.2 Active Learning
</subsectionHeader>
<bodyText confidence="0.999985097560976">
To minimize the amount of human annotation ef-
fort needed to build a tagged corpus for a given
ambiguous word, Open Mind Word Expert in-
cludes an active learning component that has the
role of selecting for annotation only those exam-
ples that are the most informative.
According to (Dagan et al., 1995), there are two
main types of active learning. The first one uses
memberships queries, in which the learner con-
structs examples and asks a user to label them. In
natural language processing tasks, this approach is
not always applicable, since it is hard and not al-
ways possible to construct meaningful unlabeled
examples for training Instead, a second type of
active learning can be applied to these tasks, which
is selective sampling. In this case, several classi-
fiers examine the unlabeled data and identify only
those examples that are the most informative, that
is the examples where a certain level of disagree-
ment is measured among the classifiers.
We use a simplified form of active learning
with selective sampling, where the instances to be
tagged are selected as those instances where there
is a disagreement between the labels assigned by
two different classifiers. The two classifiers are
trained on a relatively small corpus of tagged data,
which is formed either with (1) Senseval training
examples, in the case of Senseval words, or (2) ex-
amples obtained with the Open Mind Word Expert
system itself, when no other training data is avail-
able.
The first classifier is a Semantic Tagger with
Active Feature Selection (STAFS). This system is
one of the top ranked systems in the English lexi-
cal sample task at SENSEVAL-2. The system con-
sists of an instance based learning algorithm im-
proved with a scheme for automatic feature se-
lection. It relies on the fact that different sets of
features have different effects depending on the
ambiguous word considered. Rather than creat-
ing a general learning model for all polysemous
</bodyText>
<footnote confidence="0.999095">
7http://www.hcu.ox.ac.uldBNC/
8http://americannationalcorpus.org/
</footnote>
<page confidence="0.997622">
55
</page>
<subsectionHeader confidence="0.870442">
Learning about PLANE OPEN MIND
</subsectionHeader>
<bodyText confidence="0.998303">
The topic 061,t, has 5 senses
</bodyText>
<listItem confidence="0.658995714285714">
1) afroplans, airplane, plans - la kind of heavier-than-air craft) -- an aircraft that has a fixed wing and is powered by propellers or jets &amp;quot;the flight was
delayed due to trouble with the airplane&amp;quot;
2} shoot, plane - to kind of shape) -- (mathematics) an unbounded two-dimensional shape &amp;quot;we will refer to the plane of the graph as the X-Y plane&amp;quot;
&amp;quot;any line Joining two points on a plane lies wholly an that plane&amp;quot;
3) wane - la kind of degreei --a level of existence or development he lived on a worklly plane&amp;quot;
4) Oats. planing machine. planer - la kind of power moll --a power tool for smoothing or shaping wood
5) 0 woodworking plane. carpenters plane a kind of hand root edge Mon --a carpenter&apos;s hand tool with an adjustable blade for smoothing
</listItem>
<table confidence="0.957684352941176">
Anonymous Total -Score 0â– 0(session&apos;totali Login to credit your account with this contribution&apos;
Secretor plane Tau 0 Champion ,,(stkat) ace stats
Items 11-20 of about 103 available
--Select - A search was hunched Sunday several miles west of Mt Whitney in Into County for a single - engine plane that disappeared during a
flight from Redlands to the Stockton area oWicials said
ff&apos; Spending on military research and hardware including tanks combat plane, and helicopters will be cut along with military construction
k To understand the event&apos; Sally ran onto the plane as its door was closing &amp;quot; it is important to know that The plane cannot fly unless the
doors are closed
Five years ago he and his wife Jann who died last year flew a single - engine plane to Upstate New York with a second couple Roger
and Nancy Bowman
--Select - fs Something that might happen when you fly a planes die when the piane crashes
--Select - &amp;quot; I heard the Oahe as it was corning down &amp;quot;said Skeet Jackson of Abilene
â€”Select- 1, The slams in early in its development stage and the first one would be delivered to International Lease in 1995
--Select â€¢â€¢â€¢ Thomson who operated American Photo on Melrose Avenue was in a rented Air Spacers plane when he was reported missing
- 1.1 British Airways said no cracks were found in the ultrasonic checks on the alms. &apos; upper fuselage
No one wiya was supposed to have boarded Flight 811 in Honolulu failed to do sound hence might be responsible for bombing the
stanie he said
</table>
<figure confidence="0.846613">
(optional) jump to word. a-
Now
iislatkrt I buiri
</figure>
<figureCaption confidence="0.999826">
Figure 1: Screen shot from Open Mind Word Expert
</figureCaption>
<bodyText confidence="0.999978173913044">
words, STAFS builds a separate feature space for
each individual word. The features are selected
from a pool of eighteen different features that have
been previously acknowledged as good indicators
of word sense, including: part of speech of the am-
biguous word itself, surrounding words and their
parts of speech, keywords in context, noun be-
fore and after, verb before and after, and others.
An iterative forward search algorithm identifies at
each step the feature that leads to the highest cross-
validation precision computed on the training data.
More details on this system can be found in (Mi-
halcea, 2002).
The second classifier is a COnstraint-BAsed
Language Tagger (COBALT). The system treats
every training example as a set of soft constraints
on the sense of the word of interest. WordNet
glosses, hyponyms, hyponym glosses and other
WordNet data is also used to create soft con-
straints. Currently, only &amp;quot;keywords in context&amp;quot;
type of constraint is implemented, with weights
accounting for the distance from the target word.
The tagging is performed by finding the sense that
minimizes the violation of constraints in the in-
stance being tagged. COBALT generates confi-
dences in its tagging of a given instance based on
how much the constraints were satisfied and vio-
lated for that instance.
Both taggers use WordNet 1.7 dictionary
glosses and relations. The performance of the two
systems and their level of agreement were eval-
uated on the Senseval noun data set. The two
systems agreed in their classification decision in
54.96% of the cases. This low agreement level
is a good indication that the two approaches are
fairly orthogonal, and therefore we may hope for
high disambiguation precision on the agreement
set. Indeed, the tagging accuracy measured on the
set where both COBALT and STAFS assign the
same label is 82.5%, a fairly high figure.
Table 1 lists the precision for the agreement and
disagreement sets of the two taggers. The low pre-
cision on the instances in the disagreement set jus-
tifies referring to these as &amp;quot;hard to tag&amp;quot;. In Open
Mind Word Expert, these are the instances that are
presented to the users for tagging in the active
</bodyText>
<page confidence="0.98912">
56
</page>
<table confidence="0.997108571428572">
Preci si on
System (fine grained) (coarse grained)
STAFS 69.5% 76.6%
COBALT 59.2% 66.8%
STAFSnCOBALT 82.5% 86.3%
STAFS - STAFSnCOBALT 52.4% 63.3%
COBALT - STAFSnCOBALT 30.09% 42.07%
</table>
<tableCaption confidence="0.965807">
Table 1: Disambiguation precision for the two in-
dividual classifiers and their agreement and dis-
agreement sets
</tableCaption>
<bodyText confidence="0.481643">
learning stage.
</bodyText>
<subsectionHeader confidence="0.989803">
3.3 Ensuring Quality
</subsectionHeader>
<bodyText confidence="0.9997846">
Collecting from the general public holds the
promise of providing much data at low cost. It also
makes attending to two aspects of data collection
more important: (1) ensuring contribution quality,
and (2) making the contribution process engaging
to the contributors.
To ensure contribution quality, redundant tag-
ging is collected for each item. Open Mind Word
Expert currently uses the following rules in pre-
senting items to volunteer contributors:
</bodyText>
<listItem confidence="0.998454">
â€¢ Two tags per item. Once an item has two tags
associated with it, it is not presented for fur-
ther tagging.
â€¢ One tag per item per contributor. We allow
</listItem>
<bodyText confidence="0.876616105263158">
contributors to submit tagging either anony-
mously or having logged in. Anonymous
contributors are not shown any items already
tagged by contributors (anonymous or not)
from the same IP address. Logged in contrib-
utors are not shown items they have already
tagged.
In all, automatic assessment of the quality of
tagging seems possible, and, based on the expe-
rience of similar volunteer contribution projects
(Singh, 2002), the rate of maliciously misleading
or incorrect contributions has been surprisingly
low.
Moreover, since we plan to use paid, trained
taggers to create a separate test corpus for several
of the words tagged with Open Mind Word Expert,
these same paid taggers could also validate a small
percentage of the training data for which no gold
standard exists.
</bodyText>
<sectionHeader confidence="0.992269" genericHeader="evaluation">
4 Results after nine months of activity
</sectionHeader>
<bodyText confidence="0.999924590909091">
During the first nine months of activity, Open
Mind Word Expert has collected more than 90,000
individual sense taggings from contributors. Of
that number, approximately 16,500 tags came
from using Open Mind Word Expert in the class-
rooms as a teaching aid (the web site provides spe-
cial features for this). Future rate of collection de-
pends on the site being listed in various directories
and on the contributor repeat visit rate (we are also
experimenting with prizes to encourage participa-
tion).
There are two main figures that we measured to
evaluate the quality of the annotation task. One
is inter tagger agreement, which represents the
agreement between the tags assigned by two dif-
ferent annotators. The other is replicability, which
measures the degree to which an annotation exper-
iment can be replicated. According to (Kilganiff,
1999), the capability of recreating a set of anno-
tated data is an indicator for annotation quality that
is even more important than the inter-annotator
agreement.
</bodyText>
<subsectionHeader confidence="0.496955">
4.1 Inter-Tagger Agreement
</subsectionHeader>
<bodyText confidence="0.999992590909091">
In terms of inter-annotator agreement, the results
obtained so far can be directly compared with the
agreement figures previously reported in the lit-
erature. (Kilgarriff, 2002) mentions that for the
SENSEVAL-2 nouns and adjectives there was a
66.5% agreement between the first two tags col-
lected for each item. About 12% of their tagging
consisted of multi-word expressions and proper
nouns, which are usually not ambiguous, and
which are not considered during our data collec-
tion process. So far we measured a 62.8% inter-
tagger agreement for single word tagging, plus
close-to- l 00% precision in tagging multi-word ex-
pressions and proper nouns (as mentioned ear-
lier, this represents about 12% of the annotated
data). This results in an overall agreement of about
66.56% which is reasonable and closely compara-
ble with previous figures.
In addition to raw inter-tagger agreement, the
kappa statistic was also determined, which re-
moves from the agreement rate the amount of
agreement that is expected by chance (Carletta,
</bodyText>
<page confidence="0.996665">
57
</page>
<bodyText confidence="0.999986333333333">
1996). With an average of 5 senses per word,
the average value for the chance agreement is
20%9. This results in a kappa statistic of 58.2%.
Since previous sense annotation experiments have
not used this statistic to evaluate the inter-tagger
agreement, we have no base for comparison.
</bodyText>
<subsectionHeader confidence="0.978573">
4.2 Replicability
</subsectionHeader>
<bodyText confidence="0.86685685106383">
To measure the replicability of the tagging process
performed through Open Mind Word Expert, we
had to replicate a tagging experiment where the
annotation was performed with &amp;quot;trusted humans.&amp;quot;
To this end, we used the data set for the noun
&amp;quot;interest,&amp;quot; created and made available by (Bruce
and Wiebe, 1994). In this data set, consisting of
2,369 examples, the annotation was done with re-
spect to LDOCE, and therefore we had first to map
the sense entries from this dictionary to Word-
Net, which is the sense inventory used by Open
Mind Word Expert. The mapping did not pose
any particular problems, and consists of one-to-
one mappings for the six LDOCE entries, plus one
WordNet entry not defined in LDOCE, for which
we discarded all corresponding examples from the
Open Mind annotation.
Next, we identified and eliminated all the ex-
amples in the corpus that contained collocations
(e.g. &amp;quot;interest rate&amp;quot;); these examples accounted
for more than 35% of the data. Finally, the remain-
ing 1,438 examples were displayed on the Open
Mind Word Expert site for tagging.
Out of the 1,438 examples, 1,066 had two tags
that agreed, therefore a 74% inter-annotator agree-
ment for single words taggine . Out of these
1,066 items, 967 have a tag that coincides with
the tag assigned in the experiments reported in
(Bruce and Wiebe, 1994), which leads to an 90.8%
replicability for single words tagging (note that the
35% monosemous multi-word expressions are not
taken into account by this figure). This is close to
9Note that ideally the chance agreement should take into
consideration the entropy of word senses, which implies the
availability of sense annotated examples other than those that
we are evaluating. Since such examples are not available for
all the words in the Open Mind tagged collection, the chance
agreement was determined using a simplified assumption of
uniform sense distribution
mAdding the 35% monosemous multi-word expressions
tagged with 100% precision, leads to an overall 83% inter-
tagger agreement for this particular word
Table 2: Precision and error rate reduction for var-
ious sizes of the training corpus.
the 95% replicability scores mentioned in (Kilgar-
riff, 1999) for annotation experiments performed
by lexicographers.
</bodyText>
<subsectionHeader confidence="0.999848">
4.3 Word Sense Disambiguation using Open
Mind Word Expert corpus
</subsectionHeader>
<bodyText confidence="0.967238617647059">
For additional evaluations of the quality of the
data collected through the Open Mind Word Ex-
pert, we used these data sets in disambiguation ex-
periments, performed using the STAFS WSD sys-
tem with a fixed set of features, and consisting in
10-fold cross validation runs. We also computed
a simple baseline, consisting of a simple heuris-
tic that assigns the most frequent sense by de-
fault (also computed during 10-fold cross valida-
tion runs). Table 3 lists all words for which we col-
lected sense tagged data with Open Mind Word Ex-
pert, the number of items with full inter-annotator
agreement, the most frequent sense baseline, and
the precision achieved with STAFS&amp;quot;.
For the 280 words for which data was collected
using Open Mind Word Expert, the average num-
ber of examples per word is 87. The most frequent
sense heuristic yields correct results in 63.32%
overall. When disambiguation is performed using
STAFS, restricting the system to a simple set of
features consisting of the word itself, the word&apos;s
part of speech, and a surrounding context of two
(words and their corresponding parts of speech),
the overall precision is 66.23%, which represents
an error reduction of about 9% with respect to the
most frequent sense heuristic, providing additional
evidence of usefulness of this corpus.
Moreover, the average for the 72 words which
have at least 100 training examples is 75.88%
for the most frequent sense heuristic, and 80.32%
when using STAFS, resulting in an error reduc-
&amp;quot;The Open Mind Word Expert sense tagged corpora used
in these experiments is free for download at http://teach-
computers.org/download
</bodyText>
<figure confidence="0.995240222222222">
Number of
training example.,
any
&gt; 100
&gt; 200
&gt; 300
Precision
baseline STAFS
63.32% 66.23%
75.88% 80.32%
63.48% 72.18%
45.51% 69.15%
Error rate
reduction
9%
19%
24%
43%
</figure>
<page confidence="0.997867">
58
</page>
<bodyText confidence="0.9999674375">
tion of 19%. When at least 200 examples are
available per word, the most frequent sense heuris-
tic is correct 63.48% of the time, and STAFS is
correct 72.18% of the time, which represents a
24% reduction in disambiguation error. Table 2
lists the precisions obtained with the most fre-
quent sense heuristic and STAFS, as a function
of corpus size. The reduction in error rate grows
steadily with the number of training examples. For
the words for which more data was collected with
Open Mind Word Expert, the improvement over
the most frequent sense baseline was larger. This
agrees with prior work by other researchers (Ng,
1997), (Banko and Brill, 2001), who noted that
additional annotated data is likely to bring larger
improvements in disambiguation quality.
</bodyText>
<sectionHeader confidence="0.972792" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.983739103448276">
Open Mind Word Expert has the potential of cre-
ating a large sense tagged corpus. In this paper
we investigated the amount and quality of data
collected during the first nine months of deploy-
ment of the activity. The experiments performed
showed that the inter-tagger agreement, replica-
bility, and disambiguation results obtained on this
data are comparable with what can be obtained us-
ing data collected with the traditional method of
hiring lexicographers, at a much lower cost.
The English sense tagged corpus collected
with Open Mind Word Expert is continuously
growing, and will provide annotated data for the
English SENSEVAL-3 lexical sample task. Two
new editions, Romanian Open Mind Word Expert
and Bilingual Open Mind Word Expert will soon
be deployed. Other languages are also likely to be
added in the near future.
Acknowledgments
We want to thank the Open Mind Word Expert
contributors who are making all this work possi-
ble. We are also grateful to Ted Pedersen and the
NLP group at University of Minnesota at Duluth
for interesting discussions and important contri-
butions to this data collection process, to Adam
Kilgarriff for valuable suggestions, and to all the
Open Mind Word Expert users who have emailed
us with their feedback and suggestions, helping us
improve this activity.
</bodyText>
<sectionHeader confidence="0.992898" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99840413559322">
M. Banko and E. Brill. 2001. Scaling to very very large corpora for natu-
ral language disambiguation. In Proceedings of the 39th Annual Meeting
of the Association for Computational Lingusitics (ACL-2001), Toulouse,
France, July.
R. Bruce and J. Wiebe. 1994. Word sense disambiguation using decomposable
models. In Proceedings of the 32nd Annual Meeting of the Association
for Computational Linguistics (ACI,-94), pages 139-146, LasCruces, NM,
June.
J. Carletta. 1996. Assessing agreement on classification tasks: The kappa
statistic. Computational Linguistics, 22(2):249-254.
T. Chklovski. 2003. Effective Knowledge Acquisition. Ph.D. thesis, MIT.
(proposal).
I. Daganâ€ž and S.P. Engelson. 1995. Committee-based sampling for training
probabilistic classifiers. In International Conference on Machine Learn-
ing, pages 150-157.
P. Edmonds. 2000. Designing a task for Senseval-2, May. Available online at
http://www.itri.bton.ac.ukievents/senseval.
A. Kilgarriff and M. Palmer, editors. 2000. Computer and the Humanities.
Special issue: SENSEVAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
A. Kilgarriff. 1998. Gold standard datasets for evaluating word sense disam-
biguation programs. Computer Speech and Language, 12(4):453-472.
A. Kilgarriff. 1999. 95% replicability for manual word sense tagging. In
Proceedings of European Association for Computational Linguistics, pages
277-278, Bergen, Norway, June.
A. Kilganitt. 2002. English lexical sample task description. In Proceedings
of Senseval-2 Workshop, Association of Computational Linguistics, pages
17-20, Toulouse, France.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large
annotated corpus of english: the Penn Treebank. Computational Linguis-
tics, 19(21:313-330.
R. Mihalcea. 2002. Instance based learning with automatic feature selection
applied to Word Sense Disambiguation. in Proceedings of the 19th Inter-
national Conference on Computational Linguistics (COLING-ACL 2002),
Taipei, Taiwan, August.
G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993. A semantic concor-
dance. In Proceedings of the 3rd DARPA Workshop on Human Language
Technology, pages 303-308, Plainsboro, New Jersey.
G. Miller. 1995. Wordnet: A lexical database. Communication of the ACM,
38(111:39-41.
H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge sources to dis-
ambiguate word sense: An examplar-based approach. In Proceedings of
the 34th Annual Meeting of the Association for Computational Linguistics
(ACL-96), Santa Cruz.
H.T. Ng. 1997. Getting serious about word sense disambiguation. In Proceed-
ings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics:
Why; What, and How?, pages 1-7, Washington.
J. Preiss and D. Yarowsky, editors. 2001. Proceedings of SENSE VA L-2, Asso-
ciation for Computational Linguistics Workshop, Toulouse, France.
P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins, and W. Li Zhu. 2002.
Open mind common sense: Knowledge acquisition from the general pub-
lic. In Proceedings of the First International Conference on Ontologies,
Databases, and Applications ofSentantics for Large Scale Information Sys-
tems. Lecture Notes in Computer Science, Heidelberg: Springer-Verlag.
P. Singh. 2002. The public acquisition of commonsense knowledge. In Pro-
ceedings of AAAI Spring Symposium: Acquiring (and Using) Linguistic
(and World) Knowledge for information Access., Palo Alto, CA. AAAI.
D. Stork. 1999. The Open Mind initiative. IEEE Expert Systems and Their
Applications, 14(3):19-20.
</reference>
<page confidence="0.995136">
59
</page>
<table confidence="0.918862863636364">
Word Set size Baseline STAFS Word Set size Baseline STAFS Word Set size Baseline STAFS
act 119 80.00% 79.50% activity 103 90.00% 90.00% afternoon 97 100.00&apos;k 100.00%
age 62 78.75% 75.00% amount 63 58.89% 51.11% analysis 90 56.67% 46.67%
animal 60 100.00% 100.00% answer 67 51.54% 44.62% arc 59 77.14% 82.14%
area 98 60.00% 50.59% argument 82 25.(10% 57.00% arm 142 52.50% 80.62%
art 107 30.00% 63.53% aspect 64 54.00% 50.00% atmosphere 86 28.57% 49.29%
attempt 95 90.71% 90.71% attention 83 60.00% 55.45% attitude 107 100.00% 100.00%
audience 84 55.83% 68.33% author 94 62.31% 73.85Â°/e authority 11 25.00% 30.00%
award 77 61.43% 47.86% bank 160 91.88% 91.88% bar 107 61.76% 70.59%,
basis 47 98.18% 98.18% bed 142 98.12% 98.12% behavior 58 54.62% 45.38%
blood 136 91.05% 91.05% brother 101 95.45% 95.45% building 114 87.33% 88.67%
bum 47 40.91% 53.64% captain 101 47.27% 48.18% car 144 99.44% 99.44%
cell 126 89.44% 88.33% chair 38 93.64% 93.64Â°/e chalice 115 56.25% 81.88%
channel 103 84.62% 86.15% chapter 137 68.50% 71.50% child 105 55.33% 84.67%
church 93 70.00% 75.83% circuit 197 31.92% 45.77% circumstance 66 52.50% 50.83%
city 86 89.29% 85.71% claim 75 51.67% 41.67% coffee 115 95.00% 95.00%
college 84 93.33% 93.33% completion 87 56.6790 70.67% concentration 78 40.00% 56.67%
concern 84 32.5090 60.0090 condition 90 55.56% 52.22% contrast 50 64.00% 54.00%
cost 36 43.33% 48.89% country 66 59.17% 55.83% culture 59 40.71% 44.29%
day 192 34.76% 44.76% decision 86 55.71% 60.71% degree 140 71.43%, 82.14%
demand 41 10.0090 30.00% depth 60 43.33% 50.0090 detail 57 36.67% 50.83%
detention 38 65.45% 65.45% device 106 98.1290 98.12% difference 76 8.46% 57.69%
difficulty 60 30.00% 56.67% discussion 44 63.75% 53.75% distance 54 58.89% 53.33%
distribution 75 60.83% 55.83% doctor 133 100.00&apos;k 100.00&apos;k dog 130 100.00% 100.00%
door 112 54.62% 45.38% dream 75 50.83% 39.17% dust 46 63.00% 57.00%
earth 89 80.00% 80.59% edge 68 47.86% 54.2990 education 56 63.64% 53.64%
effect 72 92.22% 92.22% effort 66 15.83% 56.67% election 28 42.00% 64.00%
element 88 75.00% 68.12% enemy 41 48.00% 52.00% energy 76 64.62% 56.92%
evening 47 45.45% 66.36% event 77 32.14% 37.1490 evidence 65 52.73% 54.55%
example 42 26.67% 18.33% existence 65 85.45% 76.36% experience 87 44.67% 54.67%,
experiment 51 63.33% 68.33% extent 53 76.25% 97.50% eye 117 96.11% 96.11%
facility 205 81.60% 74.40% fact 172 53.16% 47.89% factor 89 67.65% 64.12%
family 95 61.43% 50.71% father 160 96.8890 96.88% fatigue 20 70.00% 70.00%
fear 52 75.7 I 90 75.71% feature 80 56.25% 55.00% feeling 48 45.00% 25.00%
fig 72 76.67% 78.89% film 98 86.47% 79.41% finger 78 100.00% 100.00%
flower 42 78.33% 54.81% friend 57 50.83% 50.83% function 105 24.67% 32.00%
future 73 78.00% 100.00% Ras 52 48.57% 40.00% girl 68 53.5790 66.43%
glass 93 65.83% 75.83% god 110 71.8290 81.82% government 82 79.00% 80.00%
grip 239 45.94% 61.88% growth 76 42.31% 43.85% gun 143 94.71% 94.71%
hair 147 96.67% 96.67% history 57 58.33% 47.5090 holiday 29 100.00% 100.00%
home 46 19.00% 27.00% hope 67 52.31% 42.31% horse 138 100.00% 100.00%
hour 79 95.00% 95.00% idea 41 56.00% 40.00% image 120 36.67% 71.6790
importance 64 93.00% 93.00% increase 43 44.29% 32.86% individual 103 96.15% 96.15%
industry 83 93.64% 93.64% influence 44 41.25% 40.00% information 62 56.25&apos; / 46.25&apos; /
</table>
<bodyText confidence="0.84285726">
intensity 88 85.00% 76.88% interest 1066 39.91% 71.08Â°/e item 85 74.62&apos; / 74.62&apos; /
judgment 85 19.23% 28.46% kid 106 83.75% 84.38% knee 29 80.00% 754:1e,
labor 59 34.29% 30.00% lady 9 language 76 53.08&apos; / 51.5.1&apos;.
law 106 38.12% 66.88% length 45 42.22% 62.22% letter 137 85.00% 81.00%
level 80 37.50% 33.75% lip 96 90.67% 90.67% list 102 100.00% 100.00%
literature 57 54.17% 58.33% manager 91 98.00% 98.00% manner 53 73.75% 67.50%
marriage 20 60.00% 40.00% material 196 77.60% 76.40% matter 46 16.00% 37.00%
meaning 77 55.00% 55.71% memory 54 42.22% 35.56% method 60 56.67% 61.67%
mind 57 57.50% 48.33% minute 93 59.17% 74.17% mission 14 46.0090 50.0090
moment 63 51.11% 61.11% money 46 67.00% 62.00% morning 71 76.25% 76.25%
mother 119 99.00% 99.00% mouth 151 74.38% 77.50% music 50 48.00% 70.00%
name 136 98.42% 98.42% nation 21 73.33% 70.00% nature 83 80.00% 81.82%
need 73 54.00% 61.00% neighborhood 67 39.23% 60.00% newspaper 78 84.00% 78.00%
object 183 96.19% 96.19% objective 74 100.00&apos;k 100.0090 office 209 62.7690 61.03%
officer 103 56.15% 55.38% onset 97 94.38% 94.38% organization 46 33.00% 40.00%
pain 83 65.4570 61.82% paper 81 73.33% 70.00% particle 95 74.29% 75.71%
party 95 45.71% 49.29% past 71 55.00% 52.50% people 120 99.17% 99.17%
performance 61 40.00% 42.86% phase 71 98.75% 98.75% plan 243 95.56% 95.56%
plane 46 90.0090 90.00% plant 126 98.89% 98.8990 policy 97 94.38% 94.38%
portion 5 - - possibility 61 44.29% 45.71% post 49 78.46&apos;. 74.62%
presence 5 pressure 106 72.50% 70.62% price 84 82.50&apos;. 70.83%
problem 60 20.00% 50.00% procedure 68 50.00% 47.1490 process 96 79.33&apos;. 72.00%
product 216 80.74% 81.48% production 63 48.89% 34.44% property 99 77.75&apos;. 71.67%
purpose 92 75.4590 58.18% quality 84 75.00% 70.83% question 53 75.00&apos;. 63.75%
radiation 55 72.00% 68.00% rate 94 59.23% 73.08% reaction 76 80.77&apos;. 68.46%
reality 5 reason 72 73.33% 67.78% region 66 65.00% 55.83%
relationship 5 - - religion 71 33.75% 61.25% report 101 66.36% 60.91%
requirement 51 48.33% 36.67% research 3 - rest 360 51.11% 67.22%
restraint 204 22.9290 46.2590 result 57 37.50% 69.17% road 93 99.17% 99.17%
role 66 31.67% 54.17% room 124 100.00&apos;k 100.0090 sale 87 74.6790 85.3390
sample 43 45.71% 40.00% school 82 33.0090 52.00% sea 205 90.80% 90.80%
season 102 92.50% 92.50% sense 58 21.54% 75.38% series 95 44.29% 63.57%
shape 79 52.50% 56.88% share 93 69.17% 80.00% shelter 81 85.56% 80.00%
shoulder 92 89.09% 80.00% site 66 69.17% 66.67% situation 43 47.14% 44.29%
size 83 74.55% 69.09% skill 55 66.00% 63.0090 society 94 82.31% 73.85%
soil 61 61.43% 62.86% soldier 95 98.5790 98.57% song 116 92.35% 92.35%
sort 98 62.94% 86.47% source 69 3133% 60.67% spade 11 100.00% 100.00%
statement 73 4.00% 64.00% story 77 50.71% 65.7190 street 78 45.33% 30.67%
stress 24 65.00% 60.00% structure 112 75.38% 72.31% student 75 96.67% 96.67%
success 44 38.7590 45.0090 sun 101 63.64% 66.36% surface 87 50.67% 58.67%
table 89 60.59% 68.24% team 96 100.00% 100.00% technique 64 90.00% 90.00%
term 125 71.I8&apos;k 90.59&apos;k test 9 text 48 22.50% 49.17%
theory 31 62.50% 55.0090 thought 51 65.00% 53.33% tissue 95 85.00% 82.86%
town 74 83.64% 76.36% trade 11 20.00% - training 44 100.00% 100.00%
treatment 108 67.78% 66.6790 tree 105 100.00&apos;k 100.00Â°c trial 109 87.37% 86.84%
trouble 73 65.00% 53.00% type 135 92.78% 92.78% understanding 29 37.27% 43.64%
unit 108 54.44% 46.67% use 92 85.4590 82.73% value 84 22.50% 56.67%
volume 103 63.85% 54.6290 war 86 72.14% 82.14% water 103 53.85% 72.31%
wave 80 51.25% 51.25% week 39 70.00% 78.33% window 60 33.33% 36.67%
woman 65 26.36% 37.27% work 89 51.18% 61.76% worker 33 93.33% 93.33%
</bodyText>
<tableCaption confidence="0.9838205">
Table 3: Words with examples sense tagged in Open Mind Word Expert: (1) set size, (2) precision
attainable with the most frequent sense heuristic, (3) precision attainable with the STAFS WSD system
</tableCaption>
<page confidence="0.996799">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974065">
<title confidence="0.9889185">OPEN MIND WORD EXPERT: Creating Large Annotated Data Collections with Web Users&apos; Help</title>
<author confidence="0.999377">Rada Mihalcea Timothy Chklovski</author>
<affiliation confidence="0.9999975">Department of Computer Science Artificial Intelligence Laboratory University of North Texas Massachusetts Institute of Technology</affiliation>
<email confidence="0.999409">rada@cs.unt.edutimc@mit.edu</email>
<abstract confidence="0.999844555555555">Open Mind Word Expert is an implemented active learning system that aims to create large annotated corpora by tapping into the world&apos;s vast pool of knowledge. It does this by relying on the vast number of Web users who contribute their knowledge to data annotation. Open Mind Word Expert focuses on building semantically annotated corpora, by collecting word sense tagging from the general public over the Web. It is available at http://teachcomputers.org. During the first nine months of activity, the system yielded 90,000 high quality tagged items at a much lower cost than the traditional method of hiring lexicographers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Lingusitics (ACL-2001),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="25298" citStr="Banko and Brill, 2001" startWordPosition="4172" endWordPosition="4175">examples are available per word, the most frequent sense heuristic is correct 63.48% of the time, and STAFS is correct 72.18% of the time, which represents a 24% reduction in disambiguation error. Table 2 lists the precisions obtained with the most frequent sense heuristic and STAFS, as a function of corpus size. The reduction in error rate grows steadily with the number of training examples. For the words for which more data was collected with Open Mind Word Expert, the improvement over the most frequent sense baseline was larger. This agrees with prior work by other researchers (Ng, 1997), (Banko and Brill, 2001), who noted that additional annotated data is likely to bring larger improvements in disambiguation quality. 5 Conclusions and future work Open Mind Word Expert has the potential of creating a large sense tagged corpus. In this paper we investigated the amount and quality of data collected during the first nine months of deployment of the activity. The experiments performed showed that the inter-tagger agreement, replicability, and disambiguation results obtained on this data are comparable with what can be obtained using data collected with the traditional method of hiring lexicographers, at </context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Lingusitics (ACL-2001), Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word sense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACI,-94),</booktitle>
<pages>139--146</pages>
<location>LasCruces, NM,</location>
<contexts>
<context position="5076" citStr="Bruce and Wiebe, 1994" startWordPosition="820" endWordPosition="823">. 2 Sense Tagged Corpora The availability of large amounts of semantically tagged data is crucial for creating successful WSD systems. Yet, as of today, only few sense tagged corpora are publicly available.3 One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet(Miller, 1995) senses. The corpus includes a total of 234,136 tagged word occurrences, out of which 186,575 are polysemous. There are 88,058 noun occurrences of which 70,214 are polysemous. The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with manually parsed within the Penn Treebank project, an annotation effort that required approximately 2 man-years of work (Marcus et al., 1993). Information extraction, automatic summarization, anaphora resolution, and other tasks also strongly require large annotated corpora. 3See http://www.senseval.org for a complete list of resources. sense tags from the Longman Dictionary of Contemporary English (LDOCE). This corpus was used in various experiments, with classification accuracies ranging from 75% to 90%, depending on the algorithm a</context>
<context position="20865" citStr="Bruce and Wiebe, 1994" startWordPosition="3440" endWordPosition="3443">(Carletta, 57 1996). With an average of 5 senses per word, the average value for the chance agreement is 20%9. This results in a kappa statistic of 58.2%. Since previous sense annotation experiments have not used this statistic to evaluate the inter-tagger agreement, we have no base for comparison. 4.2 Replicability To measure the replicability of the tagging process performed through Open Mind Word Expert, we had to replicate a tagging experiment where the annotation was performed with &amp;quot;trusted humans.&amp;quot; To this end, we used the data set for the noun &amp;quot;interest,&amp;quot; created and made available by (Bruce and Wiebe, 1994). In this data set, consisting of 2,369 examples, the annotation was done with respect to LDOCE, and therefore we had first to map the sense entries from this dictionary to WordNet, which is the sense inventory used by Open Mind Word Expert. The mapping did not pose any particular problems, and consists of one-toone mappings for the six LDOCE entries, plus one WordNet entry not defined in LDOCE, for which we discarded all corresponding examples from the Open Mind annotation. Next, we identified and eliminated all the examples in the corpus that contained collocations (e.g. &amp;quot;interest rate&amp;quot;); th</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>R. Bruce and J. Wiebe. 1994. Word sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACI,-94), pages 139-146, LasCruces, NM, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--2</pages>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Chklovski</author>
</authors>
<title>Effective Knowledge Acquisition.</title>
<date>2003</date>
<tech>Ph.D. thesis, MIT. (proposal).</tech>
<contexts>
<context position="4363" citStr="Chklovski, 2003" startWordPosition="702" endWordPosition="703">ss. We investigate the amount and quality of the data produced during nine months of deployment of the activity, and present results obtained during preliminary WSD experiments that rely on this sense tagged data. Open Mind Word Expert is a project that follows the Open Mind initiative (Stork, 1999). The basic idea behind broad Open Mind initiative is to use the information and knowledge obtainable from the millions of existing Web users, to the end of creating more intelligent software. Other Open Mind projects related to natural language and world knowledge include Open Mind 1001 Questions (Chklovski, 2003), which acquires knowledge from millions of users, and Open Mind Common Sense (Singh, 2002). 2 Sense Tagged Corpora The availability of large amounts of semantically tagged data is crucial for creating successful WSD systems. Yet, as of today, only few sense tagged corpora are publicly available.3 One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet(Miller, 1995) senses. The corpus includes a total of 234,136 tagged word occurrences, out of which 186,575 are polysemous. There are 88,058 noun occurrenc</context>
</contexts>
<marker>Chklovski, 2003</marker>
<rawString>T. Chklovski. 2003. Effective Knowledge Acquisition. Ph.D. thesis, MIT. (proposal).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Daganâ€ž</author>
<author>S P Engelson</author>
</authors>
<title>Committee-based sampling for training probabilistic classifiers.</title>
<date>1995</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<pages>150--157</pages>
<marker>Daganâ€ž, Engelson, 1995</marker>
<rawString>I. Daganâ€ž and S.P. Engelson. 1995. Committee-based sampling for training probabilistic classifiers. In International Conference on Machine Learning, pages 150-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Edmonds</author>
</authors>
<title>Designing a task for</title>
<date>2000</date>
<booktitle>Senseval-2,</booktitle>
<note>Available online at http://www.itri.bton.ac.ukievents/senseval.</note>
<contexts>
<context position="2605" citStr="Edmonds, 2000" startWordPosition="410" endWordPosition="411">Yarowsky, 2001), and elsewhere, the availability of sense tagged data is still a significant problem. Most of the efforts in WSD have focused on supervised learning algorithms, which usually achieve the best performance at the cost of low recall. The main weakness of these methods is the lack of widely available semantically tagged corpora and the strong dependence of disambiguation accuracy on the size of the training corpus. For instance, one study reports that high precision WSD requires at least 500 examples per ambiguous word (Ng, 1997)1. At a throughput of one tagged example per minute (Edmonds, 2000), and with about 20,000 ambiguous words in the common English vocabulary, a simple calculation leads to about 160,000 hours of tagging, which is nothing less than 80 man-years of human annotation work.2 Since the tagging process is usually &apos;The number of examples required for a word is highly connected to the word entropy. 500 represents an average. 2Similar data bottleneck problems are faced by many other NLP applications. High quality part of speech tagging for English requires about 3 million words annotated with their part of speech. The state-of-the-art in syntactic parsing in English is </context>
</contexts>
<marker>Edmonds, 2000</marker>
<rawString>P. Edmonds. 2000. Designing a task for Senseval-2, May. Available online at http://www.itri.bton.ac.ukievents/senseval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
<author>editors</author>
</authors>
<date>2000</date>
<booktitle>Computer and the Humanities. Special issue: SENSEVAL. Evaluating Word Sense Disambiguation programs,</booktitle>
<volume>34</volume>
<marker>Kilgarriff, Palmer, editors, 2000</marker>
<rawString>A. Kilgarriff and M. Palmer, editors. 2000. Computer and the Humanities. Special issue: SENSEVAL. Evaluating Word Sense Disambiguation programs, volume 34, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Gold standard datasets for evaluating word sense disambiguation programs.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<pages>12--4</pages>
<contexts>
<context position="6719" citStr="Kilgarriff, 1998" startWordPosition="1085" endWordPosition="1086">mpetitions have been providing a good environment for the development of supervised WSD systems, making freely available large amounts of sense tagged data for about 100 words. During S EN S EVA L- 1 (Kilgarriff and Palmer, 2000), data for 35 words was made available adding up to about 20,000 examples tagged with respect to the Hector dictionary. The size of the tagged corpus increased with S EN S E vAL-2 (Preiss and Yarowsky, 2001), when 13,000 additional examples were released for 73 polysemous words. This time, the semantic annotations were performed with respect to WordNet. Additionally, (Kilgarriff, 1998) mentions the Hector corpus, which comprises about 300 word types with 300-1000 tagged instances for each word, selected from a 17 million word corpus. With Open Mind Word Expert we aim to create a very large sense tagged corpus by making use of the incredible resource of knowledge constituted by the millions of Web users. We use techniques for active learning to utilize this resource efficiently. 3 Open Mind Word Expert Open Mind Word Expert is a Web-based application that allows contributors to annotate words with their WordNet senses. Tagging is organized by word: for each ambiguous word fo</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>A. Kilgarriff. 1998. Gold standard datasets for evaluating word sense disambiguation programs. Computer Speech and Language, 12(4):453-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>95% replicability for manual word sense tagging.</title>
<date>1999</date>
<booktitle>In Proceedings of European Association for Computational Linguistics,</booktitle>
<pages>277--278</pages>
<location>Bergen, Norway,</location>
<contexts>
<context position="22740" citStr="Kilgarriff, 1999" startWordPosition="3749" endWordPosition="3751">ation the entropy of word senses, which implies the availability of sense annotated examples other than those that we are evaluating. Since such examples are not available for all the words in the Open Mind tagged collection, the chance agreement was determined using a simplified assumption of uniform sense distribution mAdding the 35% monosemous multi-word expressions tagged with 100% precision, leads to an overall 83% intertagger agreement for this particular word Table 2: Precision and error rate reduction for various sizes of the training corpus. the 95% replicability scores mentioned in (Kilgarriff, 1999) for annotation experiments performed by lexicographers. 4.3 Word Sense Disambiguation using Open Mind Word Expert corpus For additional evaluations of the quality of the data collected through the Open Mind Word Expert, we used these data sets in disambiguation experiments, performed using the STAFS WSD system with a fixed set of features, and consisting in 10-fold cross validation runs. We also computed a simple baseline, consisting of a simple heuristic that assigns the most frequent sense by default (also computed during 10-fold cross validation runs). Table 3 lists all words for which we </context>
</contexts>
<marker>Kilgarriff, 1999</marker>
<rawString>A. Kilgarriff. 1999. 95% replicability for manual word sense tagging. In Proceedings of European Association for Computational Linguistics, pages 277-278, Bergen, Norway, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilganitt</author>
</authors>
<title>English lexical sample task description.</title>
<date>2002</date>
<booktitle>In Proceedings of Senseval-2 Workshop, Association of Computational Linguistics,</booktitle>
<pages>17--20</pages>
<location>Toulouse, France.</location>
<marker>Kilganitt, 2002</marker>
<rawString>A. Kilganitt. 2002. English lexical sample task description. In Proceedings of Senseval-2 Workshop, Association of Computational Linguistics, pages 17-20, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--21</pages>
<contexts>
<context position="5277" citStr="Marcus et al., 1993" startWordPosition="851" endWordPosition="854">le.3 One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet(Miller, 1995) senses. The corpus includes a total of 234,136 tagged word occurrences, out of which 186,575 are polysemous. There are 88,058 noun occurrences of which 70,214 are polysemous. The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with manually parsed within the Penn Treebank project, an annotation effort that required approximately 2 man-years of work (Marcus et al., 1993). Information extraction, automatic summarization, anaphora resolution, and other tasks also strongly require large annotated corpora. 3See http://www.senseval.org for a complete list of resources. sense tags from the Longman Dictionary of Contemporary English (LDOCE). This corpus was used in various experiments, with classification accuracies ranging from 75% to 90%, depending on the algorithm and features employed. The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. For this system, 192,800 word occurrences have been manually tagged with sense</context>
<context position="9066" citStr="Marcus et al., 1993" startWordPosition="1489" endWordPosition="1492">ibutors, a future version of Open Mind Word Expert may allow contributors to specify more than one sense for a given instance. As will be elaborated below, the current approach of collecting redundant tagging already addresses this to some degree. A prototype of the system has been implemented and is available at http://www.teachcomputers.org. Figure 1 shows a screen shot from the system interface, illustrating the screen presented to users when tagging the noun &amp;quot;plane&amp;quot;. 3.1 Data The starting corpus we use is formed by a mix of three different sources of data, namely the Penn Treebank corpus (Marcus et al., 1993), the Los Angeles Times collection, as provided during TREC conferences,4 and Open Mind Common Sense5 , a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web (Singh, 2002)6. A mix of several sources, each covering a different spectrum of usage, is used to increase the coverage of word senses and writing styles. Future versions of Open Mind Word Expert will include example sentences extracted from the 4http://trec.nist.gov 5http://commonsense.media.mit.edu 6See also (Singh et al., 2002) for additional details regarding the quality of free-form</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of english: the Penn Treebank. Computational Linguistics, 19(21:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Instance based learning with automatic feature selection applied to Word Sense Disambiguation.</title>
<date>2002</date>
<booktitle>in Proceedings of the 19th International Conference on Computational Linguistics (COLING-ACL 2002),</booktitle>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="14903" citStr="Mihalcea, 2002" startWordPosition="2473" endWordPosition="2475">pert words, STAFS builds a separate feature space for each individual word. The features are selected from a pool of eighteen different features that have been previously acknowledged as good indicators of word sense, including: part of speech of the ambiguous word itself, surrounding words and their parts of speech, keywords in context, noun before and after, verb before and after, and others. An iterative forward search algorithm identifies at each step the feature that leads to the highest crossvalidation precision computed on the training data. More details on this system can be found in (Mihalcea, 2002). The second classifier is a COnstraint-BAsed Language Tagger (COBALT). The system treats every training example as a set of soft constraints on the sense of the word of interest. WordNet glosses, hyponyms, hyponym glosses and other WordNet data is also used to create soft constraints. Currently, only &amp;quot;keywords in context&amp;quot; type of constraint is implemented, with weights accounting for the distance from the target word. The tagging is performed by finding the sense that minimizes the violation of constraints in the instance being tagged. COBALT generates confidences in its tagging of a given in</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>R. Mihalcea. 2002. Instance based learning with automatic feature selection applied to Word Sense Disambiguation. in Proceedings of the 19th International Conference on Computational Linguistics (COLING-ACL 2002), Taipei, Taiwan, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>C Leacock</author>
<author>T Randee</author>
<author>R Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, New Jersey.</location>
<contexts>
<context position="4748" citStr="Miller et al., 1993" startWordPosition="764" endWordPosition="767">knowledge obtainable from the millions of existing Web users, to the end of creating more intelligent software. Other Open Mind projects related to natural language and world knowledge include Open Mind 1001 Questions (Chklovski, 2003), which acquires knowledge from millions of users, and Open Mind Common Sense (Singh, 2002). 2 Sense Tagged Corpora The availability of large amounts of semantically tagged data is crucial for creating successful WSD systems. Yet, as of today, only few sense tagged corpora are publicly available.3 One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet(Miller, 1995) senses. The corpus includes a total of 234,136 tagged word occurrences, out of which 186,575 are polysemous. There are 88,058 noun occurrences of which 70,214 are polysemous. The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with manually parsed within the Penn Treebank project, an annotation effort that required approximately 2 man-years of work (Marcus et al., 1993). Information extraction, automatic summarization, anaphora resolution,</context>
</contexts>
<marker>Miller, Leacock, Randee, Bunker, 1993</marker>
<rawString>G. Miller, C. Leacock, T. Randee, and R. Bunker. 1993. A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 303-308, Plainsboro, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: A lexical database.</title>
<date>1995</date>
<journal>Communication of the ACM,</journal>
<pages>38--111</pages>
<contexts>
<context position="4822" citStr="Miller, 1995" startWordPosition="779" endWordPosition="781">ing more intelligent software. Other Open Mind projects related to natural language and world knowledge include Open Mind 1001 Questions (Chklovski, 2003), which acquires knowledge from millions of users, and Open Mind Common Sense (Singh, 2002). 2 Sense Tagged Corpora The availability of large amounts of semantically tagged data is crucial for creating successful WSD systems. Yet, as of today, only few sense tagged corpora are publicly available.3 One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet(Miller, 1995) senses. The corpus includes a total of 234,136 tagged word occurrences, out of which 186,575 are polysemous. There are 88,058 noun occurrences of which 70,214 are polysemous. The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with manually parsed within the Penn Treebank project, an annotation effort that required approximately 2 man-years of work (Marcus et al., 1993). Information extraction, automatic summarization, anaphora resolution, and other tasks also strongly require large annotated corpora. 3See http:</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. Miller. 1995. Wordnet: A lexical database. Communication of the ACM, 38(111:39-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96),</booktitle>
<location>Santa Cruz.</location>
<contexts>
<context position="5754" citStr="Ng and Lee, 1996" startWordPosition="920" endWordPosition="923">manually parsed within the Penn Treebank project, an annotation effort that required approximately 2 man-years of work (Marcus et al., 1993). Information extraction, automatic summarization, anaphora resolution, and other tasks also strongly require large annotated corpora. 3See http://www.senseval.org for a complete list of resources. sense tags from the Longman Dictionary of Contemporary English (LDOCE). This corpus was used in various experiments, with classification accuracies ranging from 75% to 90%, depending on the algorithm and features employed. The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. For this system, 192,800 word occurrences have been manually tagged with senses from WordNet. The set of tagged words consists of the 191 most frequently occurring nouns and verbs. The authors mention that approximately one man-year of effort was spent in tagging the data set. Recently, the SENSEVAL competitions have been providing a good environment for the development of supervised WSD systems, making freely available large amounts of sense tagged data for about 100 words. During S EN S EVA L- 1 (Kilgarriff and Palmer, 2000), data for 35 words was</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An examplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics (ACL-96), Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
</authors>
<title>Getting serious about word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why; What, and How?,</booktitle>
<pages>1--7</pages>
<location>Washington.</location>
<contexts>
<context position="2538" citStr="Ng, 1997" startWordPosition="399" endWordPosition="400">luation frameworks (Kilgarriff and Palmer, 2000), (Preiss and Yarowsky, 2001), and elsewhere, the availability of sense tagged data is still a significant problem. Most of the efforts in WSD have focused on supervised learning algorithms, which usually achieve the best performance at the cost of low recall. The main weakness of these methods is the lack of widely available semantically tagged corpora and the strong dependence of disambiguation accuracy on the size of the training corpus. For instance, one study reports that high precision WSD requires at least 500 examples per ambiguous word (Ng, 1997)1. At a throughput of one tagged example per minute (Edmonds, 2000), and with about 20,000 ambiguous words in the common English vocabulary, a simple calculation leads to about 160,000 hours of tagging, which is nothing less than 80 man-years of human annotation work.2 Since the tagging process is usually &apos;The number of examples required for a word is highly connected to the word entropy. 500 represents an average. 2Similar data bottleneck problems are faced by many other NLP applications. High quality part of speech tagging for English requires about 3 million words annotated with their part </context>
<context position="25273" citStr="Ng, 1997" startWordPosition="4170" endWordPosition="4171">t least 200 examples are available per word, the most frequent sense heuristic is correct 63.48% of the time, and STAFS is correct 72.18% of the time, which represents a 24% reduction in disambiguation error. Table 2 lists the precisions obtained with the most frequent sense heuristic and STAFS, as a function of corpus size. The reduction in error rate grows steadily with the number of training examples. For the words for which more data was collected with Open Mind Word Expert, the improvement over the most frequent sense baseline was larger. This agrees with prior work by other researchers (Ng, 1997), (Banko and Brill, 2001), who noted that additional annotated data is likely to bring larger improvements in disambiguation quality. 5 Conclusions and future work Open Mind Word Expert has the potential of creating a large sense tagged corpus. In this paper we investigated the amount and quality of data collected during the first nine months of deployment of the activity. The experiments performed showed that the inter-tagger agreement, replicability, and disambiguation results obtained on this data are comparable with what can be obtained using data collected with the traditional method of h</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>H.T. Ng. 1997. Getting serious about word sense disambiguation. In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics: Why; What, and How?, pages 1-7, Washington.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Proceedings of SENSE VA L-2, Association for Computational Linguistics Workshop,</booktitle>
<editor>J. Preiss and D. Yarowsky, editors.</editor>
<location>Toulouse, France.</location>
<marker>2001</marker>
<rawString>J. Preiss and D. Yarowsky, editors. 2001. Proceedings of SENSE VA L-2, Association for Computational Linguistics Workshop, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singh</author>
<author>T Lin</author>
<author>E Mueller</author>
<author>G Lim</author>
<author>T Perkins</author>
<author>W Li Zhu</author>
</authors>
<title>Open mind common sense: Knowledge acquisition from the general public.</title>
<date>2002</date>
<booktitle>In Proceedings of the First International Conference on Ontologies, Databases, and Applications ofSentantics for Large Scale Information Systems. Lecture Notes in Computer Science,</booktitle>
<publisher>Springer-Verlag.</publisher>
<location>Heidelberg:</location>
<contexts>
<context position="9608" citStr="Singh et al., 2002" startWordPosition="1571" endWordPosition="1574">ferent sources of data, namely the Penn Treebank corpus (Marcus et al., 1993), the Los Angeles Times collection, as provided during TREC conferences,4 and Open Mind Common Sense5 , a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web (Singh, 2002)6. A mix of several sources, each covering a different spectrum of usage, is used to increase the coverage of word senses and writing styles. Future versions of Open Mind Word Expert will include example sentences extracted from the 4http://trec.nist.gov 5http://commonsense.media.mit.edu 6See also (Singh et al., 2002) for additional details regarding the quality of free-form entered information, evaluation, bias, and the level of difficulty of the collected knowledge. British National Corpus,7 and the American National Corpus8 (the latter as soon as it will become available). 3.2 Active Learning To minimize the amount of human annotation effort needed to build a tagged corpus for a given ambiguous word, Open Mind Word Expert includes an active learning component that has the role of selecting for annotation only those examples that are the most informative. According to (Dagan et al., 1995), there are two </context>
</contexts>
<marker>Singh, Lin, Mueller, Lim, Perkins, Zhu, 2002</marker>
<rawString>P. Singh, T. Lin, E. Mueller, G. Lim, T. Perkins, and W. Li Zhu. 2002. Open mind common sense: Knowledge acquisition from the general public. In Proceedings of the First International Conference on Ontologies, Databases, and Applications ofSentantics for Large Scale Information Systems. Lecture Notes in Computer Science, Heidelberg: Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Singh</author>
</authors>
<title>The public acquisition of commonsense knowledge.</title>
<date>2002</date>
<booktitle>In Proceedings of AAAI Spring Symposium: Acquiring (and Using) Linguistic (and World) Knowledge for information Access.,</booktitle>
<publisher>AAAI.</publisher>
<location>Palo Alto, CA.</location>
<contexts>
<context position="4454" citStr="Singh, 2002" startWordPosition="718" endWordPosition="719"> of the activity, and present results obtained during preliminary WSD experiments that rely on this sense tagged data. Open Mind Word Expert is a project that follows the Open Mind initiative (Stork, 1999). The basic idea behind broad Open Mind initiative is to use the information and knowledge obtainable from the millions of existing Web users, to the end of creating more intelligent software. Other Open Mind projects related to natural language and world knowledge include Open Mind 1001 Questions (Chklovski, 2003), which acquires knowledge from millions of users, and Open Mind Common Sense (Singh, 2002). 2 Sense Tagged Corpora The availability of large amounts of semantically tagged data is crucial for creating successful WSD systems. Yet, as of today, only few sense tagged corpora are publicly available.3 One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet(Miller, 1995) senses. The corpus includes a total of 234,136 tagged word occurrences, out of which 186,575 are polysemous. There are 88,058 noun occurrences of which 70,214 are polysemous. The next significant hand tagging task was reported in (</context>
<context position="9289" citStr="Singh, 2002" startWordPosition="1527" endWordPosition="1528"> to some degree. A prototype of the system has been implemented and is available at http://www.teachcomputers.org. Figure 1 shows a screen shot from the system interface, illustrating the screen presented to users when tagging the noun &amp;quot;plane&amp;quot;. 3.1 Data The starting corpus we use is formed by a mix of three different sources of data, namely the Penn Treebank corpus (Marcus et al., 1993), the Los Angeles Times collection, as provided during TREC conferences,4 and Open Mind Common Sense5 , a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web (Singh, 2002)6. A mix of several sources, each covering a different spectrum of usage, is used to increase the coverage of word senses and writing styles. Future versions of Open Mind Word Expert will include example sentences extracted from the 4http://trec.nist.gov 5http://commonsense.media.mit.edu 6See also (Singh et al., 2002) for additional details regarding the quality of free-form entered information, evaluation, bias, and the level of difficulty of the collected knowledge. British National Corpus,7 and the American National Corpus8 (the latter as soon as it will become available). 3.2 Active Learni</context>
<context position="17807" citStr="Singh, 2002" startWordPosition="2949" endWordPosition="2950">enting items to volunteer contributors: â€¢ Two tags per item. Once an item has two tags associated with it, it is not presented for further tagging. â€¢ One tag per item per contributor. We allow contributors to submit tagging either anonymously or having logged in. Anonymous contributors are not shown any items already tagged by contributors (anonymous or not) from the same IP address. Logged in contributors are not shown items they have already tagged. In all, automatic assessment of the quality of tagging seems possible, and, based on the experience of similar volunteer contribution projects (Singh, 2002), the rate of maliciously misleading or incorrect contributions has been surprisingly low. Moreover, since we plan to use paid, trained taggers to create a separate test corpus for several of the words tagged with Open Mind Word Expert, these same paid taggers could also validate a small percentage of the training data for which no gold standard exists. 4 Results after nine months of activity During the first nine months of activity, Open Mind Word Expert has collected more than 90,000 individual sense taggings from contributors. Of that number, approximately 16,500 tags came from using Open M</context>
</contexts>
<marker>Singh, 2002</marker>
<rawString>P. Singh. 2002. The public acquisition of commonsense knowledge. In Proceedings of AAAI Spring Symposium: Acquiring (and Using) Linguistic (and World) Knowledge for information Access., Palo Alto, CA. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Stork</author>
</authors>
<title>The Open Mind initiative.</title>
<date>1999</date>
<journal>IEEE Expert Systems and Their Applications,</journal>
<pages>14--3</pages>
<contexts>
<context position="4047" citStr="Stork, 1999" startWordPosition="652" endWordPosition="653">is paper, we present Open Mind Word Expert, a Web-based system that aims to create large sense tagged corpora with the help of Web users. The annotation workload is distributed among millions of potential human annotators, which is likely to significantly reduce the cost and the duration of the annotation process. We investigate the amount and quality of the data produced during nine months of deployment of the activity, and present results obtained during preliminary WSD experiments that rely on this sense tagged data. Open Mind Word Expert is a project that follows the Open Mind initiative (Stork, 1999). The basic idea behind broad Open Mind initiative is to use the information and knowledge obtainable from the millions of existing Web users, to the end of creating more intelligent software. Other Open Mind projects related to natural language and world knowledge include Open Mind 1001 Questions (Chklovski, 2003), which acquires knowledge from millions of users, and Open Mind Common Sense (Singh, 2002). 2 Sense Tagged Corpora The availability of large amounts of semantically tagged data is crucial for creating successful WSD systems. Yet, as of today, only few sense tagged corpora are public</context>
</contexts>
<marker>Stork, 1999</marker>
<rawString>D. Stork. 1999. The Open Mind initiative. IEEE Expert Systems and Their Applications, 14(3):19-20.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>