<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.972876">
Combining a Chinese Thesaurus with a Chinese Dictionary
</title>
<author confidence="0.986161">
Ji Donghong Gong Junping
</author>
<affiliation confidence="0.949822">
Kent Ridge Digital Labs Department of Computer Science
</affiliation>
<address confidence="0.9808965">
21 Heng Mui Keng Terrace Ohio State University
Singapore, 119613 Columbus, OH
</address>
<email confidence="0.994798">
dhji@lcrdl.org.sg jgong@cis.ohio-state.edu
</email>
<author confidence="0.957228">
Huang Changning
</author>
<affiliation confidence="0.9935215">
Department of Computer Science
Tsinghua University
</affiliation>
<address confidence="0.767184">
Beijing, 100084, P. R. China
</address>
<email confidence="0.968952">
hcn@mail.tsinghua.edu.cn
</email>
<sectionHeader confidence="0.993963" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999218">
In this paper, we study the problem of combining
a Chinese thesaurus with a Chinese dictionary by
linking the word entries in the thesaurus with the
word senses in the dictionary, and propose a
similar word strategy to solve the problem. The
method is based on the definitions given in the
dictionary, but without any syntactic parsing or
sense disambiguation on them at all. As a result,
their combination makes the thesaurus specify the
similarity between senses which accounts for the
similarity between words, produces a kind of
semantic classification of the senses defined in the
dictionary, and provides reliable information
about the lexical items on which the resources
don&apos;t conform with each other.
</bodyText>
<sectionHeader confidence="0.99776" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999900826086957">
Both ((TongYiCi CiLin)) (Mei. et al, 1983) and
((XianDai HanYu CiDian)) (1978) are important
Chinese resources, and have been widely used in
various Chinese processing systems (e.g., Zhang
et al, 1995). As a thesaurus, ((TongYiCi CiLin))
defines semantic categories for words, however, it
doesn&apos;t specify which sense of a polysemous
word is involved in a semantic category. On the
other hand, ((XianDai HanYu CiDian)) is an
ordinary dictionary which provides definitions of
senses while not giving any information about
their semantic classification.
A manual effort has been made to build a
resource for English, i.e., WordNet, which
contains both definition and classification
information (Miller et al., 1990), but such
resources are not available for many other
languages, e.g. Chinese. This paper presents an
automatic method to combine the Chinese
thesaurus with the Chinese dictionary into such a
resource, by tagging the entries in the thesaurus
with appropriate senses in the dictionary,
meanwhile assigning appropriate semantic codes,
which stand for semantic categories in the
thesaurus, to the senses in the dictionary.
D.Yarowsky has considered a similar problem
to link Roget&apos;s categories, an English thesaurus,
with the senses in COBUILD, an English
dictionary (Yarowsky, 1992). He treats the
problem as a sense disambiguation one, with the
definitions in the dictionary taken as a kind of
contexts in which the headwords occur, and deals
with it based on a statistical model of Roget&apos;s
categories trained on large corpus. In our opinion,
the method, for a specific word, neglects the
difference between its definitions and the ordinary
contexts: definitions generally contain its
synonyms, hyponyms or hypernyms, etc., while
ordinary contexts generally its collocations. So
the trained model on ordinary contexts may be not
appropriate for the disambiguation problem in
definition contexts.
A seemingly reasonable method to the
problem would be common word strategy, which
has been extensively studied by many researchers
(e.g., Knight, 1993; Lesk, 1986). The solution
</bodyText>
<page confidence="0.994014">
600
</page>
<bodyText confidence="0.99991936">
would be, for a category, to select those senses
whose definitions hold most number of common
words among all those for its member words. But
the words in a category in the Chinese thesaurus
may be not similar in a strict way, although
similar to some extend, so their definitions may
only contain some similar words at most, rather
than share many words. As a result, the common
word strategy may be not appropriate for the
problem we study here.
In this paper, we extend the idea of common
word strategy further to a similar word method
based on the intuition that definitions for similar
senses generally contain similar words, if not the
same ones. Now that the words in a category in
the thesaurus are similar to some extent, some of
their definitions should contain similar words. We
see these words as marks of the category, then the
correct sense of a word involved in the category
could be identified by checking whether its
definition contains such marks. So the key of the
method is to determine the marks for a category.
Since the marks may be different word tokens, it
may be difficult to make them out only based on
their frequencies. But since they are similar words,
they would belong to the same category in the
thesaurus, or hold the same semantic code, so we
can locate them by checking their semantic codes.
In implementation, for any category, we first
compute a salience value for each code with
respect to it, which in fact provides the
information about the marks of the category, then
compute distances between the category and the
senses of its member words, which reflect
whether their definitions contain the marks and
how many, finally select those senses as tags by
checking whether their distances from the
category fall within a threshold.
The remainder of this paper is organized as
the following: in section 2, we give a formal
setting of the problem and present the tagging
procedure; in section 3, we explore the issue of
threshold estimation for the distances between
senses and categories based on an analysis of the
distances between the senses and categories of
univocal words; in section 4, we report our
experiment results and their evaluation; in section
5, we present some discussions about our
methodology; finally in section 6, we give some
conclusions.
</bodyText>
<sectionHeader confidence="0.890462" genericHeader="method">
2. Problem Setting
</sectionHeader>
<bodyText confidence="0.997016307692308">
The Chinese dictionary provides sense
distinctions for 44,389 Chinese words, on the
other hand, the Chinese thesaurus divides 64,500
word entries into 12 major, 94 medium and 1428
minor categories, which is in fact a kind of
semantic classification of the words&apos;. Intuitively,
there should be a kind of correspondence between
the senses and the entries. The main task of
combining the two resources is to locate such kind
of correspondence.
Suppose X is a category2 in the thesaurus, for
any word we X, let Sw be the set of its senses in
the dictionary, and Sx = , for any SE Sx, let
</bodyText>
<equation confidence="0.547997">
we X
DIC be the set of the definition words in its
definition, DW.--.0 DW5, and D141 A= U DWW,
seS,, we X
</equation>
<bodyText confidence="0.969845">
for any word w, let CODE(w) be the set of its
semantic codes that are given in the thesaurus&apos;,
</bodyText>
<equation confidence="0.684065">
CODEs= U CODE (w) , CODE „,= U CODE 2,
wam,
</equation>
<bodyText confidence="0.864286">
and CODE x= U CODE . For any ce CODE,r, we
</bodyText>
<subsubsectionHeader confidence="0.327889">
sesx
</subsubsectionHeader>
<footnote confidence="0.781077857142857">
&apos;The electronic versions of the two resources we use now
only contain part of the words in them, see section 4.
2 We generally use &amp;quot;category&amp;quot; to refer to minor categories in
the following text, if no confusion is involved. Furthermore,
we also use a semantic code to refer to a category.
A category is given a semantic code, a word may belong to
several categories, and hold several codes.
</footnote>
<page confidence="0.987846">
601
</page>
<figure confidence="0.85857775">
define its definition salience with respect to X in
1).
I)
IXI
</figure>
<bodyText confidence="0.941924333333333">
For example, 2) lists a category Ea02 in the
thesaurus, whose members are the synonyms or
antonyms of word At(Igaodai; high and big)4.
</bodyText>
<equation confidence="0.889555">
2) WWI tst4N *X A* AfA Itli* AM; X#F.
MR 4* R7E 7C )1 t Eld4t-t
AWL
</equation>
<bodyText confidence="0.8410595">
3) lists some semantic codes and their definition
salience with respect to the category.
</bodyText>
<equation confidence="0.990383">
3) Ea02 (0.92), Ea03 (0.76), Dn01 (0.45),
Eb04 (0.24), Dn04 (0.14).
</equation>
<bodyText confidence="0.9995558">
To define a distance between a category X and a
sense s, we first define a distance between any
two categories according to the distribution of
their member words in a corpus, which consists of
80 million Chinese characters.
For any category X, suppose its members are
w1, W29 %in, for any wi, we first compute its
mutual information with each semantic code
according to their co-occurrence in a corpus% then
select 10 top semantic codes as its environmental
codes6, which hold the biggest mutual information
with wi. Let NCi be the set of wi&apos;s environmental
codes, CT be the set of all the semantic codes
given in the thesaurus, for any ce C7-, we define its
context salience with respect to X in 4).
</bodyText>
<equation confidence="0.620278">
1{w1 lc E NCI}(
</equation>
<bodyText confidence="0.964854866666667">
Igaodar is the Pinyin of the word, and &amp;quot;high and big&amp;quot; is its
English translation.
5 We see each occurrence of a word in the corpus as one
occurrence of its codes. Each co-occurrence of a word and a
code falls within a 5-word distance.
6 The intuition behind the parameter selection (10) is that the
words which can combined with a specific word to form
collocations fall in at most 10 categories in the thesaurus.
We build a context vector for X in 5), where
k=I Cr) .
5) cvx=&lt;Sal2(c 1, Sa12(c2, X), SalAck, X)&gt;
Given two categories X and Y, suppose cvx and
CVy are their context vectors respectively, we
define their distance dis(X, Y) as 6) based on the
cosine of the two vectors.
</bodyText>
<listItem confidence="0.4312505">
6) dis(X, Y)=1-cos(cvx, CVy)
Let CE CODES, we define a distance between c
and a sense s in 7).
7) dis(c, s)= Min dis(c, c&apos;)
</listItem>
<bodyText confidence="0.836773333333333">
c&apos;eCODE,
Now we define a distance between a category X
and a sense s in 8).
</bodyText>
<equation confidence="0.9197294">
8) dis(X, s)= E (lic • dis(c, s))
ceCODE x
Sall(c, X)
ESA (c&apos; , X)
c&apos;ECODEx
</equation>
<bodyText confidence="0.9991797">
Intuitively, if CODE, contains the salient
codes with respect to X, i.e., those with higher
salience with respect to X, dis(X, s) will be
smaller due to the fact that the contribution of a
semantic code to the distance increases with its
salience, so s tends to be a correct sense tag of
some word.
For any category X, let we X and se S„,, if
dis(X, s$T, where T is some threshold, we will
tag w by s, and assign the semantic code X to s.
</bodyText>
<sectionHeader confidence="0.960025" genericHeader="method">
3. Parameter Estimation
</sectionHeader>
<bodyText confidence="0.999195375">
Now we consider the problem of estimating an
appropriate threshold for dis(X, s) to distinguish
between the senses of the words in X. To do so,
we first extract the words which hold only one
code in the thesaurus, and have only one sense in
the dictionary&apos;, then check the distances between
these senses and categories. The number of such
words is 22,028.
</bodyText>
<footnote confidence="0.7669995">
7 This means that the words are regarded as univocal ones by
both resources.
</footnote>
<table confidence="0.572209">
I{WIW E X, CE CODE n,}1
4) Sa12(c, X).
where k-
</table>
<page confidence="0.976621">
602
</page>
<bodyText confidence="0.652402">
Tab.1 lists the distribution of the words with
respect to the distance in 5 intervals.
</bodyText>
<table confidence="0.999726714285714">
Intervals Word num. Percent(%)
[0.0, 0.2) 8,274 37.56
[0.2, 0.4) 10,655 48.37
[0.4, 0.6) 339 1.54
[0.6, 0.8) 1172 5.32
[0.8, 1.0] 1588 7.21
all 22,028 100
</table>
<tableCaption confidence="0.9543295">
Tab. 1. The distribution of univocal words
with respect to disa,
</tableCaption>
<bodyText confidence="0.99771475">
From Tab.!, we can see that for most univocal
words, the distance between their senses and
categories lies in [0, 0.4].
Let Wu be the set of the univocal words we
consider here, for any univocal word we Wth let sw
be its unique sense, and Xw be its univocal
category, we call DEN,,I. ,2, point density in
interval [ti, t2) as 9), where 0511&lt;t25_1.
</bodyText>
<equation confidence="0.992080666666667">
9) DEN,,, ,,2,=
1{WIW E Wu , t, dis(X.„sw) _t2}1
t2 — t
</equation>
<bodyText confidence="0.9980675">
We define 10) as an object function, and take t*
which maximizes DEN, as the threshold.
</bodyText>
<equation confidence="0.483049">
10) DENt= DEN&lt;o, DEN&lt;LI&gt;
</equation>
<bodyText confidence="0.999940578947368">
The object function is built on the following
inference. About the explanation of the words
which are regarded as univocal by both Chinese
resources, the two resources tend to be in
accordance with each other. It means that for most
univocal words, their senses should be the correct
tags of their entries, or the distance between their
categories and senses should be smaller, falling
within the under-specified threshold. So it is
reasonable to suppose that the intervals within the
threshold hold a higher point density, furthermore
that the difference between the point density in [0,
a and that in [t*, 1] gets the biggest value.
With t falling in its value set {dis(X, s)}, we
get ts as 0.384, when for 18,653 (84.68%)
univocal words, their unique entries are tagged
with their unique senses, and for the other
univocal words, their entries not tagged with their
senses.
</bodyText>
<sectionHeader confidence="0.997376" genericHeader="evaluation">
4. Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999615888888889">
There are altogether 29,679 words shared by the two
resources, which hold 35,193 entries in the thesaurus
and 36,426 senses in the dictionary. We now
consider the 13,165 entries and 14,398 senses which
are irrelevant with the 22,028 univocal words. Tab. 2
and 3 list the distribution of the entries with respect
to the number of their sense tags, and the distribution
of the senses with respect to the number of their
code tags respectively.
</bodyText>
<table confidence="0.705935">
Tag num. Entry Percent (%)
0 1625 12.34
1 9908 75.26
2 1349 10.25
3 283 2.15
</table>
<tableCaption confidence="0.424262">
Tab. 2. The distribution of entries with respect to
their sense tags
</tableCaption>
<table confidence="0.9867174">
Tag num. Sense Percent (%)
0 1461 10.15
1 10433 72.46
2 2334 16.21
&gt;3 170 1.18
</table>
<tableCaption confidence="0.663313">
Tab. 3. The distribution of senses with respect to
their code tags
</tableCaption>
<bodyText confidence="0.977025833333333">
In order to evaluate the efficiency of our
method, we define two measures, accuracy rate
and loss rate, for a group of entries E as 11) and
12) respectively&apos;.
° We only give the evaluation on the results for entries, the
evaluation on the results for senses can be done similarly.
</bodyText>
<page confidence="0.983622">
603
</page>
<figure confidence="0.601278333333333">
I RTE CTE I
12)
CTE I
</figure>
<bodyText confidence="0.989026256410257">
where RTE is a set of the sense tags for the entries
in E produced by the tagging procedure, and CTE
is a set of the sense tags for the entries in E, which
are regarded as correct ones somehow.
What we expect for the tagging procedure is
to select the appropriate sense tags for the entries
in the thesaurus, if they really exist in the
dictionary. To evaluate the procedure directly
proves to be difficult. We turn to deal with it in an
indirect way, in particular, we explore the
efficiency of the procedure of tagging the entries,
when their appropriate sense tags don&apos;t exist in
the dictionary. This indirect evaluation, on the one
hand, can be carried out automatically in a large
scale, on the other hand, can suggest what the
direct evaluation entails in some way because that
none appropriate tags can be seen as a special tag
for the entries, say None9.
In the first experiment, let&apos;s consider the
18,653 univocal words again which are selected in
parameter estimation stage. For each of them, we
create a new entry in the thesaurus which is
different from its original one. Based on the
analysis in section 3, the senses for theses words
should only be the correct tags for their
corresponding entries, the newly created ones
have to take None as their correct tags.
When creating new entries, we adopt the
following 3 different kinds of constraints:
i) the new entry belongs to the same
medium category with the original one;
ii) the new entry belongs to the same
major category with the original one;
iii) no constraints;
With each constraint, we select 5 groups of new
A default sense tag for the entries.
entries respectively, and carry out the experiment
for each group. Tab. 4 lists average accuracy rates
and loss rates under different constraints.
</bodyText>
<table confidence="0.99323575">
Constraint Aver. accuracy(%) Aver. loss (%)
88.39 11.61
94.75 5.25
95.26 4.74
</table>
<tableCaption confidence="0.940664">
Tab. 4. Average accuracy, loss rates under different
constraints
</tableCaption>
<bodyText confidence="0.9997614">
From Tab. 4, we can see that the accuracy rate
under constraint i) is a bit less than that under
constraint ii) or iii), the reason is that with the
created new entries belonging to the same
medium category with the original ones, it may be
a bit more likely for them to be tagged with the
original senses. On the other hand, notice that the
accuracy rates and loss rates in Tab.4 are
complementary with each other, the reason is that
IRTEI equals ICTEI in such cases.
In another experiment, we select 5 groups of
0-tag, 1-tag and 2-tag entries respectively, and
each group consists of 20-30 entries. We check
their accuracy rates and loss rates manually. Tab.
5 lists the results.
</bodyText>
<table confidence="0.99655175">
Tag num. Aver. accuracy(%) Aver. loss(%)
0 94.6 7.3
1 90.1 5.2
2 87.6 2.1
</table>
<tableCaption confidence="0.7727425">
Tab. 5. Average accuracy and loss rates under
different number of tags
</tableCaption>
<bodyText confidence="0.998380428571429">
Notice that the accuracy rates and loss rates in
Tab.5 are not complementary, the reason is that
IRTEI doesn&apos;t equal ICTEI in such cases.
In order to explore the main factors affecting
accuracy and loss rates, we extract the entries
which are not correctly tagged with the senses,
and check relevant definitions and semantic codes.
</bodyText>
<figure confidence="0.940018333333333">
II)
IRTE I
ICTE —(RTE CTE)I
</figure>
<page confidence="0.996017">
604
</page>
<bodyText confidence="0.954864090909091">
The main reasons are:
i) No salient codes exist with respect to a
category, or the determined are not the expected.
This may be attributed to the fact that the words in
a category may be not strict synonyms, or that a
category may contain too less words, etc.
ii) The information provided for a word by
the resources may be incomplete. For example,
word f( (/quanshuf, all) holds one semantic
code Ka06 in the thesaurus, its definition in the
dictionary is:
</bodyText>
<equation confidence="0.910664">
[Eb02]
/quanshu/ /quanbu/
all
</equation>
<bodyText confidence="0.974821393939394">
The correct tag for the entry should be the sense
listed above, but in fact, it is tagged with None in
the experiment. The reason is that word ifil ;
(/quanbu/, all) can be an adverb or an adjective,
and should hold two semantic codes, Ka06 and
Eb02, corresponding with its adverb and adjective
usage respectively, but the thesaurus neglects its
adverb usage. If Ka06 is added as a semantic code
of word ±U (/quanbu/, all), the entry will be
successfully tagged with the expected sense.
iii) The distance defined between a sense and
a category fails to capture the information carried
by the order of salient codes, more generally, the
information carried by syntactic structures
involved. As an example, consider word t{
(/yaochuan/), which has two definitions listed in
the following.
im 1) lin[Da19] M[Ie01].
/yaochuan/ /yaoyan/ /chuanbo/
hearsay spread
the hearsay spreads.
2) 4iA[Ie01] M igli[Da19]
/chuanbo/ /de/ /yaoyan/
spread of hearsay
the hearsay which spreads
The two definitions contain the same content
words, the difference between them lies in the
order of the content words, more generally, lies in
the syntactic structures involved in the definitions:
the former presents a sub-obj structure, while the
latter with a &amp;quot;IM/de/,of)&amp;quot; structure. To distinguish
such definitions needs to give more consideration
on word order or syntactic structures.
</bodyText>
<sectionHeader confidence="0.997635" genericHeader="evaluation">
5. Discussions
</sectionHeader>
<bodyText confidence="0.998881580645161">
In the tagging procedure, we don&apos;t try to carry out
any sense disambiguation on definitions due to its
known difficulty. Undoubtedly, when the noisy
semantic codes taken by some definition words
exactly cover the salient ones of a category, they
will affect the tagging accuracy. But the
probability for such cases may be lower,
especially when more than one salient code exists
with respect to a category.
The distance between two categories is
defined according to the distribution of their
member words in a corpus. A natural alternative is
based on the shortest path from one category to
another in the thesaurus (e.g., Lee at al., 1993;
Rada et al., 1989), but it is known that the method
suffers from the problem of neglecting the wide
variability in what a link in the thesaurus entails.
Another choice may be information content
method (Resnik, 1995), although it can avoid the
difficulty faced by shortest path methods, it will
make the minor categories within a medium one
get a same distance between each other, because
the distance is defined in terms of the information
content carried by the medium category. What we
concern here is to evaluate the dissimilarity
between different categories, including those
within one medium category, so we make use of
semantic code based vectors to define their
dissimilarity, which is motivated by Shuetze&apos;s
word frequency based vectors (Shuetze, 1993).
In order to determine appropriate sense tags
</bodyText>
<page confidence="0.997091">
605
</page>
<bodyText confidence="0.999984777777778">
for a word entry in one category, we estimate a
threshold for the distance between a sense and a
category. Another natural choice may be to select
the sense holding the smallest distance from the
category as the correct tag for the entry. But this
choice, although avoiding estimation issues, will
fail to directly demonstrate the inconsistency
between the two resources, and the similarity
between two senses with respect to a category.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="conclusions">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.999963037037037">
In this paper, we propose an automatic method to
combine a Chinese thesaurus with a Chinese
dictionary. Their combination establishes the
correspondence between the entries in the
thesaurus and the senses in the dictionary, and
provides reliable information about the lexical
items on which the two resources are not in
accordance with each other. The method uses no
language-specific knowledge, and can be applied
to other languages.
The combination of the two resources can be
seen as improvement on both of them. On the one
hand, it makes the thesaurus specify the similarity
between word senses behind that between words,
on the other hand, it produces a semantic
classification for the word senses in the
dictionary.
The method is in fact appropriate for a more
general problem: given a set of similar words,
how to identify the senses, among all, which
account for their similarity. In the problem we
consider here, the words fall within a category in
the Chinese thesaurus, with similarity to some
extent between each other. The work suggests that
if the set contains more words, and they are more
similar with each other, the result will be more
sound.
</bodyText>
<sectionHeader confidence="0.999202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999543769230769">
Knight K. (1993) Building a Large Ontology for
Machine Translation. In &amp;quot;Proceedings of DARPA
Human Language Conference&amp;quot;, Princeton, USA,
185-190.
Lesk M. (1986) Automated Word Sense
Disambiguation using Machine Readable
Dictionaries: How to Tell a Pine Cone from an Ice
Cream Cone. In &amp;quot;Proceedings of the ACM SIGDOC
Conference&amp;quot;, Toronto Ontario.
Lee J. H., Kim M. H., and Lee Y. J. (1993).
Information retrieval based on concept distance in
IS-A hierarchies. Journal of Documentation, 49/2.
Mei J.J. et al. (1983) TongYiCi CiLin(A Chinese
Thesaurus). Shanghai Cishu press, Shanghai.
Miller G.A., Backwith R., Fellbaum C., Gross D. and
Miller K. J. (1990) Introduction to WordNet: An
On-line Lexical Database. International Journal of
Lexicography, 3(4) (Special Issue).
Rada R. and Bicknell E (1989) Ranking documents
with a thesaurus. JASIS, 40(5), pp. 304-310.
Resnik P. (1995) Using Information Content to
Evaluate the similarity in a Taxonomy. In
&amp;quot;Proceedings of the 14th International Joint
Conference on Artificial Intelligence&amp;quot;.
Schutze H. (1993) Part-of-speech induction from
scratch. In &amp;quot;Proceedings of the 31st Annual Meeting
of the Association for Computational Linguistics&amp;quot;,
Columbus, OH.
XianDai Han Yu CiDian(A modern Chinese Dictionary)
(1978), Shangwu press, Beijing.
Yarowsky D. (1992) Word Sense Disambiguation
Using Statistical Models of Roget&apos;s Categories
Trained on Large Corpora. In &amp;quot;Proceedings of
COLING&apos;92&amp;quot;, Nantas, France, pp. 454-460.
Zhang J, Huang C. N. Yang E. H. (1994) Construction
a Machine Tractable Dictionary from a Machine
Readable Dictionary. Communications of Chinese
and Oriental Language Information Processing
Society, 4(2), pp. 123-130.
</reference>
<page confidence="0.998764">
606
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.567934">
<title confidence="0.999813">Combining a Chinese Thesaurus with a Chinese Dictionary</title>
<author confidence="0.919401">Ji Donghong Gong Junping</author>
<affiliation confidence="0.8171855">Kent Ridge Digital Labs Department of Computer Science 21 Heng Mui Keng Terrace Ohio State University</affiliation>
<address confidence="0.999877">Singapore, 119613 Columbus, OH</address>
<email confidence="0.987758">dhji@lcrdl.org.sgjgong@cis.ohio-state.edu</email>
<author confidence="0.970738">Huang Changning</author>
<affiliation confidence="0.9998745">Department of Computer Science Tsinghua University</affiliation>
<address confidence="0.999677">Beijing, 100084, P. R. China</address>
<email confidence="0.974945">hcn@mail.tsinghua.edu.cn</email>
<abstract confidence="0.999383625">In this paper, we study the problem of combining a Chinese thesaurus with a Chinese dictionary by linking the word entries in the thesaurus with the word senses in the dictionary, and propose a similar word strategy to solve the problem. The method is based on the definitions given in the dictionary, but without any syntactic parsing or sense disambiguation on them at all. As a result, their combination makes the thesaurus specify the similarity between senses which accounts for the similarity between words, produces a kind of semantic classification of the senses defined in the dictionary, and provides reliable information about the lexical items on which the resources don&apos;t conform with each other.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Building a Large Ontology for Machine Translation. In &amp;quot;Proceedings of DARPA Human Language Conference&amp;quot;,</title>
<date>1993</date>
<pages>185--190</pages>
<location>Princeton, USA,</location>
<contexts>
<context position="3170" citStr="Knight, 1993" startWordPosition="473" endWordPosition="474">nd deals with it based on a statistical model of Roget&apos;s categories trained on large corpus. In our opinion, the method, for a specific word, neglects the difference between its definitions and the ordinary contexts: definitions generally contain its synonyms, hyponyms or hypernyms, etc., while ordinary contexts generally its collocations. So the trained model on ordinary contexts may be not appropriate for the disambiguation problem in definition contexts. A seemingly reasonable method to the problem would be common word strategy, which has been extensively studied by many researchers (e.g., Knight, 1993; Lesk, 1986). The solution 600 would be, for a category, to select those senses whose definitions hold most number of common words among all those for its member words. But the words in a category in the Chinese thesaurus may be not similar in a strict way, although similar to some extend, so their definitions may only contain some similar words at most, rather than share many words. As a result, the common word strategy may be not appropriate for the problem we study here. In this paper, we extend the idea of common word strategy further to a similar word method based on the intuition that d</context>
</contexts>
<marker>Knight, 1993</marker>
<rawString>Knight K. (1993) Building a Large Ontology for Machine Translation. In &amp;quot;Proceedings of DARPA Human Language Conference&amp;quot;, Princeton, USA, 185-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automated Word Sense Disambiguation using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>In &amp;quot;Proceedings of the ACM SIGDOC Conference&amp;quot;,</booktitle>
<location>Toronto Ontario.</location>
<contexts>
<context position="3183" citStr="Lesk, 1986" startWordPosition="475" endWordPosition="476">it based on a statistical model of Roget&apos;s categories trained on large corpus. In our opinion, the method, for a specific word, neglects the difference between its definitions and the ordinary contexts: definitions generally contain its synonyms, hyponyms or hypernyms, etc., while ordinary contexts generally its collocations. So the trained model on ordinary contexts may be not appropriate for the disambiguation problem in definition contexts. A seemingly reasonable method to the problem would be common word strategy, which has been extensively studied by many researchers (e.g., Knight, 1993; Lesk, 1986). The solution 600 would be, for a category, to select those senses whose definitions hold most number of common words among all those for its member words. But the words in a category in the Chinese thesaurus may be not similar in a strict way, although similar to some extend, so their definitions may only contain some similar words at most, rather than share many words. As a result, the common word strategy may be not appropriate for the problem we study here. In this paper, we extend the idea of common word strategy further to a similar word method based on the intuition that definitions fo</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk M. (1986) Automated Word Sense Disambiguation using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. In &amp;quot;Proceedings of the ACM SIGDOC Conference&amp;quot;, Toronto Ontario.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lee</author>
<author>M H Kim</author>
<author>Y J Lee</author>
</authors>
<title>Information retrieval based on concept distance in IS-A hierarchies.</title>
<date>1993</date>
<journal>Journal of Documentation,</journal>
<pages>49--2</pages>
<marker>Lee, Kim, Lee, 1993</marker>
<rawString>Lee J. H., Kim M. H., and Lee Y. J. (1993). Information retrieval based on concept distance in IS-A hierarchies. Journal of Documentation, 49/2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Mei</author>
</authors>
<title>TongYiCi CiLin(A Chinese Thesaurus). Shanghai Cishu press,</title>
<date>1983</date>
<location>Shanghai.</location>
<marker>Mei, 1983</marker>
<rawString>Mei J.J. et al. (1983) TongYiCi CiLin(A Chinese Thesaurus). Shanghai Cishu press, Shanghai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Backwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K J Miller</author>
</authors>
<title>Introduction to WordNet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal of Lexicography, 3(4) (Special Issue).</journal>
<contexts>
<context position="1825" citStr="Miller et al., 1990" startWordPosition="268" endWordPosition="271"> Chinese resources, and have been widely used in various Chinese processing systems (e.g., Zhang et al, 1995). As a thesaurus, ((TongYiCi CiLin)) defines semantic categories for words, however, it doesn&apos;t specify which sense of a polysemous word is involved in a semantic category. On the other hand, ((XianDai HanYu CiDian)) is an ordinary dictionary which provides definitions of senses while not giving any information about their semantic classification. A manual effort has been made to build a resource for English, i.e., WordNet, which contains both definition and classification information (Miller et al., 1990), but such resources are not available for many other languages, e.g. Chinese. This paper presents an automatic method to combine the Chinese thesaurus with the Chinese dictionary into such a resource, by tagging the entries in the thesaurus with appropriate senses in the dictionary, meanwhile assigning appropriate semantic codes, which stand for semantic categories in the thesaurus, to the senses in the dictionary. D.Yarowsky has considered a similar problem to link Roget&apos;s categories, an English thesaurus, with the senses in COBUILD, an English dictionary (Yarowsky, 1992). He treats the prob</context>
</contexts>
<marker>Miller, Backwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller G.A., Backwith R., Fellbaum C., Gross D. and Miller K. J. (1990) Introduction to WordNet: An On-line Lexical Database. International Journal of Lexicography, 3(4) (Special Issue).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rada</author>
<author>E Bicknell</author>
</authors>
<title>Ranking documents with a thesaurus.</title>
<date>1989</date>
<journal>JASIS,</journal>
<volume>40</volume>
<issue>5</issue>
<pages>304--310</pages>
<marker>Rada, Bicknell, 1989</marker>
<rawString>Rada R. and Bicknell E (1989) Ranking documents with a thesaurus. JASIS, 40(5), pp. 304-310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using Information Content to Evaluate the similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In &amp;quot;Proceedings of the 14th International Joint Conference on Artificial Intelligence&amp;quot;.</booktitle>
<contexts>
<context position="18430" citStr="Resnik, 1995" startWordPosition="3171" endWordPosition="3172"> will affect the tagging accuracy. But the probability for such cases may be lower, especially when more than one salient code exists with respect to a category. The distance between two categories is defined according to the distribution of their member words in a corpus. A natural alternative is based on the shortest path from one category to another in the thesaurus (e.g., Lee at al., 1993; Rada et al., 1989), but it is known that the method suffers from the problem of neglecting the wide variability in what a link in the thesaurus entails. Another choice may be information content method (Resnik, 1995), although it can avoid the difficulty faced by shortest path methods, it will make the minor categories within a medium one get a same distance between each other, because the distance is defined in terms of the information content carried by the medium category. What we concern here is to evaluate the dissimilarity between different categories, including those within one medium category, so we make use of semantic code based vectors to define their dissimilarity, which is motivated by Shuetze&apos;s word frequency based vectors (Shuetze, 1993). In order to determine appropriate sense tags 605 for</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik P. (1995) Using Information Content to Evaluate the similarity in a Taxonomy. In &amp;quot;Proceedings of the 14th International Joint Conference on Artificial Intelligence&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
</authors>
<title>Part-of-speech induction from scratch.</title>
<date>1993</date>
<booktitle>In &amp;quot;Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics&amp;quot;,</booktitle>
<location>Columbus, OH.</location>
<marker>Schutze, 1993</marker>
<rawString>Schutze H. (1993) Part-of-speech induction from scratch. In &amp;quot;Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics&amp;quot;, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XianDai Han</author>
</authors>
<title>Yu CiDian(A modern Chinese Dictionary)</title>
<date>1978</date>
<location>Beijing.</location>
<marker>Han, 1978</marker>
<rawString>XianDai Han Yu CiDian(A modern Chinese Dictionary) (1978), Shangwu press, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Word Sense Disambiguation Using Statistical Models of Roget&apos;s Categories Trained on Large Corpora.</title>
<date>1992</date>
<booktitle>In &amp;quot;Proceedings of COLING&apos;92&amp;quot;,</booktitle>
<pages>454--460</pages>
<location>Nantas, France,</location>
<contexts>
<context position="2405" citStr="Yarowsky, 1992" startWordPosition="356" endWordPosition="357">n information (Miller et al., 1990), but such resources are not available for many other languages, e.g. Chinese. This paper presents an automatic method to combine the Chinese thesaurus with the Chinese dictionary into such a resource, by tagging the entries in the thesaurus with appropriate senses in the dictionary, meanwhile assigning appropriate semantic codes, which stand for semantic categories in the thesaurus, to the senses in the dictionary. D.Yarowsky has considered a similar problem to link Roget&apos;s categories, an English thesaurus, with the senses in COBUILD, an English dictionary (Yarowsky, 1992). He treats the problem as a sense disambiguation one, with the definitions in the dictionary taken as a kind of contexts in which the headwords occur, and deals with it based on a statistical model of Roget&apos;s categories trained on large corpus. In our opinion, the method, for a specific word, neglects the difference between its definitions and the ordinary contexts: definitions generally contain its synonyms, hyponyms or hypernyms, etc., while ordinary contexts generally its collocations. So the trained model on ordinary contexts may be not appropriate for the disambiguation problem in defini</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky D. (1992) Word Sense Disambiguation Using Statistical Models of Roget&apos;s Categories Trained on Large Corpora. In &amp;quot;Proceedings of COLING&apos;92&amp;quot;, Nantas, France, pp. 454-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zhang</author>
<author>Huang C N Yang E H</author>
</authors>
<title>Construction a Machine Tractable Dictionary from a Machine Readable Dictionary.</title>
<date>1994</date>
<journal>Communications of Chinese and Oriental Language Information Processing Society,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>123--130</pages>
<marker>Zhang, H, 1994</marker>
<rawString>Zhang J, Huang C. N. Yang E. H. (1994) Construction a Machine Tractable Dictionary from a Machine Readable Dictionary. Communications of Chinese and Oriental Language Information Processing Society, 4(2), pp. 123-130.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>