<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.987634">
Automatically Assessing Review Helpfulness
</title>
<author confidence="0.999182">
Soo-Min Kim†, Patrick Pantel†, Tim Chklovski†, Marco Pennacchiotti‡
</author>
<affiliation confidence="0.9972345">
†Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.8234225">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.992825">
{skim,pantel,timc}@isi.edu
</email>
<author confidence="0.795805">
‡ART Group - DISP
</author>
<affiliation confidence="0.968154">
University of Rome “Tor Vergata”
</affiliation>
<address confidence="0.873708">
Viale del Politecnico 1
Rome, Italy
</address>
<email confidence="0.993257">
pennacchiotti@info.uniroma2.it
</email>
<sectionHeader confidence="0.997263" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914625">
User-supplied reviews are widely and
increasingly used to enhance e-
commerce and other websites. Because
reviews can be numerous and varying in
quality, it is important to assess how
helpful each review is. While review
helpfulness is currently assessed manu-
ally, in this paper we consider the task
of automatically assessing it. Experi-
ments using SVM regression on a vari-
ety of features over Amazon.com
product reviews show promising results,
with rank correlations of up to 0.66. We
found that the most useful features in-
clude the length of the review, its uni-
grams, and its product rating.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999471136363636">
Unbiased user-supplied reviews are solicited
ubiquitously by online retailers like Ama-
zon.com, Overstock.com, Apple.com and Epin-
ions.com, movie sites like imdb.com, traveling
sites like citysearch.com, open source software
distributors like cpanratings.perl.org, and count-
less others. Because reviews can be numerous
and varying in quality, it is important to rank
them to enhance customer experience.
In contrast with ranking search results, assess-
ing relevance when ranking reviews is of little
importance because reviews are directly associ-
ated with the relevant product or service. Instead,
a key challenge when ranking reviews is to de-
termine which reviews the customers will find
helpful.
Most websites currently rank reviews by their
recency or product rating (e.g., number of stars
in Amazon.com reviews). Recently, more sophis-
ticated ranking schemes measure reviews by their
helpfulness, which is typically estimated by hav-
ing users manually assess it. For example, on
Amazon.com, an interface allows customers to
vote whether a particular review is helpful or not.
Unfortunately, newly written reviews and re-
views with few votes cannot be ranked as several
assessments are required in order to properly es-
timate helpfulness. For example, for all MP3
player products on Amazon.com, 38% of the
20,919 reviews received three or fewer helpful-
ness votes. Another problem is that low-traffic
items may never gather enough votes. Among the
MP3 player reviews that were authored at least
three months ago on Amazon.com, still only 31%
had three or fewer helpfulness votes.
It would be useful to assess review helpfulness
automatically, as soon as the review is written.
This would accelerate determining a review’s
ranking and allow a website to provide rapid
feedback to review authors.
In this paper, we investigate the task of auto-
matically predicting review helpfulness using a
machine learning approach. Our main contribu-
tions are:
</bodyText>
<listItem confidence="0.993845461538462">
• A system for automatically ranking reviews
according to helpfulness; using state of the art
SVM regression, we empirically evaluate our
system on a real world dataset collected from
Amazon.com on the task of reconstructing the
helpfulness ranking; and
• An analysis of different classes of features
most important to capture review helpful-
ness; including structural (e.g., html tags,
punctuation, review length), lexical (e.g., n-
grams), syntactic (e.g., percentage of verbs and
nouns), semantic (e.g., product feature men-
tions), and meta-data (e.g., star rating).
</listItem>
<sectionHeader confidence="0.997576" genericHeader="introduction">
2 Relevant Work
</sectionHeader>
<bodyText confidence="0.9924325">
The task of automatically assessing product re-
view helpfulness is related to these broader areas
</bodyText>
<page confidence="0.990848">
423
</page>
<note confidence="0.8539335">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423–430,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999887094339623">
of research: automatic analysis of product re-
views, opinion and sentiment analysis, and text
classification.
In the thriving area of research on automatic
analysis and processing of product reviews (Hu
and Liu 2004; Turney 2002; Pang and Lee 2005),
little attention has been paid to the important task
studied here – assessing review helpfulness. Pang
and Lee (2005) have studied prediction of prod-
uct ratings, which may be particularly relevant
due to the correlation we find between product
rating and the helpfulness of the review (dis-
cussed in Section 5). However, a user’s overall
rating for the product is often already available.
Helpfulness, on the other hand, is valuable to
assess because it is not explicitly known in cur-
rent approaches until many users vote on the
helpfulness of a review.
In opinion and sentiment analysis, the focus is
on distinguishing between statements of fact vs.
opinion, and on detecting the polarity of senti-
ments being expressed. Many researchers have
worked in various facets of opinion analysis.
Pang et al. (2002) and Turney (2002) classified
sentiment polarity of reviews at the document
level. Wiebe et al. (1999) classified sentence
level subjectivity using syntactic classes such as
adjectives, pronouns and modal verbs as features.
Riloff and Wiebe (2003) extracted subjective
expressions from sentences using a bootstrapping
pattern learning process. Yu and Hatzivassi-
loglou (2003) identified the polarity of opinion
sentences using semantically oriented words.
These techniques were applied and examined in
different domains, such as customer reviews (Hu
and Liu 2004) and news articles (TREC novelty
track 2003 and 2004).
In text classification, systems typically use
bag-of-words models, although there is some
evidence of benefits when introducing relevant
semantic knowledge (Gabrilovich and Mark-
ovitch, 2005). In this paper, we explore the use of
some semantic features for review helpfulness
ranking. Another potential relevant classification
task is academic and commercial efforts on de-
tecting email spam messages1, which aim to cap-
ture a much broader notion of helpfulness. For an
SVM-based approach, see (Drucker et al 1999).
Finally, a related area is work on automatic es-
say scoring, which seeks to rate the quality of an
essay (Attali and Burstein 2006; Burstein et al.
2004). The task is important for reducing the
human effort required in scoring large numbers
</bodyText>
<footnote confidence="0.876407">
1 See http://www.ceas.cc/, http://spamconference.org/
</footnote>
<bodyText confidence="0.9999664375">
of student essays regularly written for standard
tests such as the GRE. The exact scoring ap-
proaches developed in commercial systems are
often not disclosed. However, more recent work
on one of the major systems, e-rater 2.0, has fo-
cused on systematizing and simplifying the set of
features used (Attali and Burstein 2006). Our
choice of features to test was partially influenced
by the features discussed by Attali and Burstein.
At the same time, due to differences in the tasks,
we did not use features aimed at assessing essay
structure such as discourse structure analysis fea-
tures. Our observations suggest that even helpful
reviews vary widely in their discourse structure.
We present the features which we have used be-
low, in Section 3.2.
</bodyText>
<sectionHeader confidence="0.941703" genericHeader="method">
3 Modeling Review Helpfulness
</sectionHeader>
<bodyText confidence="0.999544333333333">
In this section, we formally define the learning
task and we investigate several features for as-
sessing review helpfulness.
</bodyText>
<subsectionHeader confidence="0.998544">
3.1 Task Definition
</subsectionHeader>
<bodyText confidence="0.99945925">
Formally, given a set of reviews R for a particu-
lar product, our task is to rank the reviews ac-
cording to their helpfulness. We define a review
helpfulness function, h, as:
</bodyText>
<equation confidence="0.999874">
h(r∈ R)= rating+ (r) (1)
rating+ (r) +rating− ( r)
</equation>
<bodyText confidence="0.999978">
where rating+(r) is the number of people that will
find a review helpful and rating-(r) is the number
of people that will find the review unhelpful. For
evaluation, we resort to estimates of h from man-
ual review assessments on websites like Ama-
zon.com, as described in Section 4.
</bodyText>
<subsectionHeader confidence="0.987654">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999981833333333">
One aim of this paper is to investigate how well
different classes of features capture the helpful-
ness of a review. We experimented with various
features organized in five classes: Structural,
Lexical, Syntactic, Semantic, and Meta-data. Be-
low we describe each feature class in turn.
</bodyText>
<subsectionHeader confidence="0.836214">
Structural Features
</subsectionHeader>
<bodyText confidence="0.998569875">
Structural features are observations of the docu-
ment structure and formatting. Properties such as
review length and average sentence length are
hypothesized to relate structural complexity to
helpfulness. Also, HTML formatting tags could
help in making a review more readable, and con-
sequently more helpful. We experimented with
the following features:
</bodyText>
<page confidence="0.996634">
424
</page>
<listItem confidence="0.998713888888889">
• Length (LEN): The total number of tokens in a
syntactic analysis2 of the review.
• Sentential (SEN): Observations of the sen-
tences, including the number of sentences, the
average sentence length, the percentage of
question sentences, and the number of excla-
mation marks.
• HTML (HTM): Two features for the number of
bold tags &lt;b&gt; and line breaks &lt;br&gt;.
</listItem>
<subsectionHeader confidence="0.882779">
Lexical Features
</subsectionHeader>
<bodyText confidence="0.998722333333333">
Lexical features capture the words observed in
the reviews. We experimented with two sets of
features:
</bodyText>
<listItem confidence="0.99430475">
• Unigram (UGR): The tf-idf statistic of each
word occurring in a review.
• Bigram (BGR): The tf-idf statistic of each bi-
gram occurring in a review.
</listItem>
<bodyText confidence="0.98064175">
For both unigrams and bigrams, we used lemma-
tized words from a syntactic analysis of the re-
views and computed the tf-idf statistic (Salton
and McGill 1983) using the following formula:
</bodyText>
<equation confidence="0.9622585">
tf idf = tf × log(idf )
N
</equation>
<bodyText confidence="0.9095592">
where N is the number of tokens in the review.
Syntactic Features
Syntactic features aim to capture the linguistic
properties of the review. We grouped them into
the following feature set:
</bodyText>
<listItem confidence="0.903774875">
• Syntax (SYN): Includes the percentage of
parsed tokens that are open-class (i.e., nouns,
verbs, adjectives and adverbs), the percentage
of tokens that are nouns, the percentage of to-
kens that are verbs, the percentage of tokens
that are verbs conjugated in the first person,
and the percentage of tokens that are adjectives
or adverbs.
</listItem>
<subsectionHeader confidence="0.911137">
Semantic Features
</subsectionHeader>
<bodyText confidence="0.996438444444445">
Most online reviews are fairly short; their spar-
sity suggests that bigram features will not per-
form well (which is supported by our
experiments described in Section 5.3). Although
semantic features have rarely been effective in
many text classification problems (Moschitti and
Basili 2004), there is reason here to hypothesize
that a specialized vocabulary of important words
might help with the sparsity. We hypothesized
2 Reviews are analyzed using the Minipar dependency
parser (Lin 1994).
that good reviews will often contain: i) refer-
ences to the features of a product (e.g., the LCD
and resolution of a digital camera), and ii) men-
tions of sentiment words (i.e., words that express
an opinion such as “great screen”). Below we
describe two families of features that capture
these semantic observations within the reviews:
</bodyText>
<listItem confidence="0.991214173913043">
• Product-Feature (PRF): The features of prod-
ucts that occur in the review, e.g., capacity of
MP3 players and zoom of a digital camera.
This feature counts the number of lexical
matches that occur in the review for each prod-
uct feature. There is no trivial way of obtaining
a list of all the features of a product. In Section
5.1 we describe a method for automatically ex-
tracting product features from Pro/Con listings
from Epinions.com. Our assumption is that
pro/cons are the features that are important for
customers (and hence should be part of a help-
ful review).
• General-Inquirer (GIW): Positive and negative
sentiment words describing products or prod-
uct features (e.g., “amazing sound quality” and
“weak zoom”). The intuition is that reviews
that analyze product features are more helpful
than those that do not. We try to capture this
analysis by extracting sentiment words using
the publicly available list of positive and nega-
tive sentiment words from the General Inquirer
Dictionaries3.
</listItem>
<subsectionHeader confidence="0.485466">
Meta-Data Features
</subsectionHeader>
<bodyText confidence="0.99303825">
Unlike the previous four feature classes, meta-
data features capture observations which are in-
dependent of the text (i.e., unrelated with linguis-
tic features). We consider the following feature:
</bodyText>
<listItem confidence="0.87111075">
• Stars (STR): Most websites require reviewers
to include an overall rating for the products
that they review (e.g., star ratings in Ama-
zon.com). This feature set includes the rating
</listItem>
<bodyText confidence="0.932231636363636">
score (STR1) as well as the absolute value of
the difference between the rating score and the
average rating score given by all reviewers
(STR2).
We differentiate meta-data features from seman-
tic features since they require external knowl-
edge that may not be available from certain
review sites. Nowadays, however, most sites that
collect user reviews also collect some form of
product rating (e.g., Amazon.com, Over-
stock.com, and Apple.com).
</bodyText>
<footnote confidence="0.943543">
3 http://www.wjh.harvard.edu/~inquirer/homecat.htm
</footnote>
<page confidence="0.998828">
425
</page>
<bodyText confidence="0.63922225">
Table 1. Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Ama-
zon.com along with their ratings as well as their helpfulness ranks (from both the gold
standard from Amazon.com and the SVM prediction of our best performing system de-
scribed in Section 5.2).
</bodyText>
<table confidence="0.994208285714286">
REVIEW TITLE HELPFUL UNHELPFUL RANK(h)
VOTES VOTES GOLD SVM
STANDARD PREDICTION
“iPod Moves to All-color Line-up” 215 11 7 1
“iPod: It&apos;s NOT Music to My Ears” 11 13 25 30
“The best thing I ever bought” 22 32 26 27
“VERY disappointing” 1 18 40 40
</table>
<sectionHeader confidence="0.97157" genericHeader="method">
4 Ranking System
</sectionHeader>
<bodyText confidence="0.999949540540541">
In this paper, we estimate the helpfulness func-
tion in Equation 1 using user ratings extracted
from Amazon.com, where rating+(r) is the num-
ber of unique users that rated the review r as
helpful and rating-(r) is the number of unique
users that rated r as unhelpful.
Reviews from Amazon.com form a gold stan-
dard labeled dataset of {review, h(review)} pairs
that can be used to train a supervised machine
learning algorithm. In this paper, we applied an
SVM (Vapnik 1995) package on the features ex-
tracted from reviews to learn the function h.
Two natural options for learning helpfulness
according to Equation 1 are SVM Regression and
SVM Ranking (Joachims 2002). Though learning
to rank according to helpfulness requires only
SVM Ranking, the helpfulness function provides
non-uniform differences between ranks in the
training set. Also, in practice, many products
have only one review, which can serve as train-
ing data for SVM Regression but not SVM Rank-
ing. Furthermore, in large sites such as
Amazon.com, when new reviews are written it is
inefficient to re-rank all previously ranked re-
views. We therefore choose SVM Regression in
this paper. We describe the exact implementation
in Section 5.1.
After the SVM is trained, for a given product
and its set of reviews R, we rank the reviews of R
in decreasing order of h(r), r ∈ R.
Table 1 shows four sample reviews for the
iPod Photo 20GB product from Amazon.com,
their total number of helpful and unhelpful votes,
as well as their rank according to the helpfulness
score h from both the gold standard from Ama-
zon.com and using the SVM prediction of our
best performing system described in Section 5.2.
</bodyText>
<sectionHeader confidence="0.997674" genericHeader="method">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999934285714286">
We empirically evaluate our review model and
ranking system, described in Section 3 and Sec-
tion 4, by comparing the performance of various
feature combinations on products mined from
Amazon.com. Below, we describe our experi-
mental setup, present our results, and analyze
system performance.
</bodyText>
<subsectionHeader confidence="0.941407">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.984706689655172">
We describe below the datasets that we extracted
from Amazon.com, the implementation of our
SVM system, and the method we used for ex-
tracting features of reviews.
Extraction and Preprocessing of Datasets
We focused our experiments on two products
from Amazon.com: MP3 Players and Digital
Cameras.
Using Amazon Web Services API, we col-
lected reviews associated with all products in the
MP3 Players and Digital Cameras categories.
For MP3 Players, we collected 821 products and
33,016 reviews; for Digital Cameras, we col-
lected 1,104 products and 26,189 reviews.
In most retailer websites like Amazon.com,
duplicate reviews, which are quite frequent, skew
statistics and can greatly affect a learning algo-
rithm. Looking for exact string matches between
reviews is not a sufficient filter since authors of
duplicated reviews often make small changes to
the reviews to avoid detection. We built a simple
filter that compares the distribution of word bi-
grams across each pair of reviews. A pair is
deemed a duplicate if more than 80% of their
bigrams match.
Also, whole products can be duplicated. For
different product versions, such as iPods that can
come in black or white models, reviews on Ama-
zon.com are duplicated between them. We filter
</bodyText>
<page confidence="0.998941">
426
</page>
<tableCaption confidence="0.985273">
Table 2. Overview of filtered datasets extracted
from Amazon.com.
</tableCaption>
<table confidence="0.999200666666667">
MP3 DIGITAL
PLAYERS CAMERAS
Total Products 736 1066
Total Reviews 11,374 14,467
Average Reviews/Product 15.4 13.6
Min/MaxReviews/Product 1 / 375 1 / 168
</table>
<bodyText confidence="0.997981428571429">
out complete products where each of its reviews
is detected as a duplicate of another product (i.e.,
only one iPod version is retained).
The filtering of duplicate products and dupli-
cate reviews discarded 85 products and 12,097
reviews for MP3 Players and 38 products and
3,692 reviews for Digital Cameras.
In order to have accurate estimates for the
helpfulness function in Equation 1, we filtered
out any review that did not receive at least five
user ratings (i.e., reviews where less than five
users voted it as helpful or unhelpful are filtered
out). This filtering was performed before dupli-
cate detection and discarded 45.7% of the MP3
Players reviews and 32.7% of the Digital Cam-
eras reviews.
Table 2 describes statistics for the final data-
sets after the filtering steps. 10% of products for
both datasets were withheld as development cor-
pora and the remaining 90% were randomly
sorted into 10 sets for 10-fold cross validation.
</bodyText>
<sectionHeader confidence="0.562244" genericHeader="method">
SVM Regression
</sectionHeader>
<bodyText confidence="0.999989142857143">
For our regression model, we deployed the state
of the art SVM regression tool SVMlight
(Joachims 1999). We tested on the development
sets various kernels including linear, polynomial
(degrees 2, 3, and 4), and radial basis function
(RBF). The best performing kernel was RBF and
we report only these results in this paper (per-
formance was measured using Spearman’s corre-
lation coefficient, described in Section 5.2).
We tuned the RBF kernel parameters C (the
penalty parameter) and γ (the kernel width hy-
perparameter) performing full grid search over
the 110 combinations of exponentially spaced
parameter pairs (C,γ) following (Hsu et al. 2003).
</bodyText>
<subsectionHeader confidence="0.643168">
Feature Extraction
</subsectionHeader>
<bodyText confidence="0.998883166666667">
To extract the features described in Section 3.2,
we preprocessed each review using the Minipar
dependency parser (Lin 1994). We used the
parser tokenization, sentence breaker, and syn-
tactic categorizations to generate the Length,
Sentential, Unigram, Bigram, and Syntax feature
sets.
In order to count the occurrences of product
features for the Product-Feature set, we devel-
oped an automatic way of mining references to
product features from Epinions.com. On this
website, user-generated product reviews include
explicit lists of pros and cons, describing the best
and worst aspects of a product. For example, for
MP3 players, we found the pro “belt clip” and
the con “Useless FM tuner”. Our assumption is
that the pro/con lists tend to contain references to
the product features that are important to cus-
tomers, and hence their occurrence in a review
may correlate with review helpfulness. We fil-
tered out all single-word entries which were in-
frequently seen (e.g., hold, ever). After splitting
and filtering the pro/con lists, we were left with a
total of 9,110 unique features for MP3 Players
and 13,991 unique features for Digital Cameras.
The Stars feature set was created directly from
the star ratings given by each author of an Ama-
zon.com review.
For each feature measurement f, we applied
the following standard transformation:
</bodyText>
<equation confidence="0.979898">
ln(f + 1)
</equation>
<bodyText confidence="0.999954222222222">
and then scaled each feature between [0, 1] as
suggested in (Hsu et al. 2003).
We experimented with various combinations
of feature sets. Our results tables use the abbre-
viations presented in Section 3.2. For brevity, we
report the combinations which contributed to our
best performing system and those that help assess
the power of the different feature classes in cap-
turing helpfulness.
</bodyText>
<subsectionHeader confidence="0.999609">
5.2 Ranking Performance
</subsectionHeader>
<bodyText confidence="0.999904083333333">
Evaluating the quality of a particular ranking is
difficult since certain ranking intervals can be
more important than others (e.g., top-10 versus
bottom-10) We adopt the Spearman correlation
coefficient p (Spearman 1904) since it is the
most commonly used measure of correlation be-
tween two sets of ranked data points4.
For each fold in our 10-fold cross-validation
experiments, we trained our SVM system using 9
folds. For the remaining test fold, we ranked each
product’s reviews according to the SVM predic-
tion (described in Section 4) and computed the p
</bodyText>
<footnote confidence="0.98868075">
4 We used the version of Spearman’s correlation coeffi-
cient that allows for ties in rankings. See Siegel and Cas-
tellan (1988) for more on alternate rank statistics such as
Kendall’s tau.
</footnote>
<page confidence="0.998365">
427
</page>
<tableCaption confidence="0.982437">
Table 3. Evaluation of the feature combinations that make up our best performing system
(in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras accord-
ing to helpfulness.
</tableCaption>
<table confidence="0.932729727272727">
FEATURE COMBINATIONS SPEARMAN† MP3 PLAYERS DIGITAL CAMERAS
PEARSON† SPEARMAN† PEARSON†
LEN 0.575 f 0.037 0.391 f 0.038 0.521 f 0.029 0.357 f 0.029
UGR 0.593 f 0.036 0.398 f 0.038 0.499 f 0.025 0.328 f 0.029
STR1 0.589 f 0.034 0.326 f 0.038 0.507 f 0.029 0.266 f 0.030
UGR+STR1 0.644 f 0.033 0.436 f 0.038 0.490 f 0.032 0.324 f 0.032
LEN+UGR 0.582 f 0.036 0.401 f 0.038 0.553 f 0.028 0.394 f 0.029
LEN+STR1 0.652 f 0.033 0.470 f 0.038 0.577 f 0.029 0.423 f 0.031
LEN+UGR+STR1 0.656 f 0.033 0.476 f 0.038 0.595 f 0.028 0.442 f 0.031
LEN=Length; UGR=Unigram; STR=Stars
†95% confidence bounds are calculated using 10-fold cross-validation.
</table>
<bodyText confidence="0.999850222222222">
correlation between the ranking and the gold
standard ranking from the test fold5.
Although our task definition is to learn review
rankings according to helpfulness, as an interme-
diate step the SVM system learns to predict the
absolute helpfulness score for each review. To
test the correlation of this score against the gold
standard, we computed the standard Pearson cor-
relation coefficient.
Results show that the highest performing fea-
ture combination consisted of the Length, the
Unigram, and the Stars feature sets. Table 3 re-
ports the evaluation results for every combination
of these features with 95% confidence bounds.
Of the three features alone, neither was statisti-
cally more significant than the others. Examining
each pair combination, only the combination of
length with stars outperformed the others. Sur-
prisingly, adding unigram features to this combi-
nation had little effect for the MP3 Players.
Given our list of features defined in Section
3.2, helpfulness of reviews is best captured with
a combination of the Length and Stars features.
Training an RBF-kernel SVM regression model
does not necessarily make clear the exact rela-
tionship between input and output variables. To
investigate this relationship between length and
helpfulness, we inspected their Pearson correla-
tion coefficient, which was 0.45. Users indeed
tend to find short reviews less helpful than longer
ones: out of the 5,247 reviews for MP3 Players
that contained more than 1000 characters, the
average gold standard helpfulness score was
82%; the 204 reviews with fewer than 100 char-
acters had on average a score of 23%. The ex-
plicit product rating, such as Stars is also an
</bodyText>
<footnote confidence="0.7830775">
5 Recall that the gold standard is extracted directly from
user helpfulness votes on Amazon.com (see Section 4).
</footnote>
<bodyText confidence="0.999855384615385">
indicator of review helpfulness, with a Pearson
correlation coefficient of 0.48.
The low Pearson correlations of Table 3 com-
pared to the Spearman correlations suggest that
we can learn the ranking without perfectly learn-
ing the function itself. To investigate this, we
tested the ability of SVM regression to recover
the target helpfulness score, given the score itself
as the only feature. The Spearman correlation for
this test was a perfect 1.0. Interestingly, the Pear-
son correlation was only 0.798, suggesting that
the RBF kernel does learn the helpfulness rank-
ing without learning the function exactly.
</bodyText>
<subsectionHeader confidence="0.978896">
5.3 Results Analysis
</subsectionHeader>
<bodyText confidence="0.999955565217392">
Table 3 shows only the feature combinations of
our highest performing system. In Table 4, we
report several other feature combinations to show
why we selected certain features and what was
the effect of our five feature classes presented in
Section 3.2.
In the first block of six feature combinations in
Table 4, we show that the unigram features out-
perform the bigram features, which seem to be
suffering from the data sparsity of the short re-
views. Also, unigram features seem to subsume
the information carried in our semantic features
Product-Feature (PRF) and General-Inquirer
(GIW). Although both PRF and GIW perform
well as standalone features, when combined with
unigrams there is little performance difference
(for MP3 Players we see a small but insignificant
decrease in performance whereas for Digital
Cameras we see a small but insignificant im-
provement). Recall that PRF and GIW are simply
subsets of review words that are found to be
product features or sentiment words. The learn-
ing algorithm seems to discover on its own which
</bodyText>
<page confidence="0.998371">
428
</page>
<bodyText confidence="0.7848506">
Table 4. Performance evaluation of various feature combinations for ranking reviews of MP3 Players
and Digital Cameras on Amazon.com according to helpfulness. The first six lines suggest that uni-
grams subsume the semantic features; the next two support the use of the raw counts of product ratings
(stars) rather than the distance of this count from the average rating; the final six investigate the impor-
tance of auxiliary feature sets.
</bodyText>
<equation confidence="0.8655205">
LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram;
SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars
</equation>
<figure confidence="0.994700194444444">
†95% confidence bounds are calculated using 10-fold cross-validation.
FEATURE COMBINATIONS
STR2
STR1
MP3 PLAYERS
CAMERAS
DIGITAL
SPEARMAN†
PEARSON†
0.266 ± 0.030
0.326 ± 0.038
0.229 ± 0.027
0.303 ± 0.038
0.595 ± 0.028
0.476 ± 0.038
0.599 ± 0.028
0.470 ± 0.038
0.594 ± 0.028
0.459 ± 0.039
0.595 ± 0.028
0.469 ± 0.039
0.600 ± 0.028
0.453 ± 0.039
0.604 ± 0.027
0.396 ± 0.038
0.593 ± 0.036 0.398 ± 0.038
0.499 ± 0.040 0.293 ± 0.038
0.571 ± 0.036 0.381 ± 0.038
0.570 ± 0.037 0.375 ± 0.038
0.554 ± 0.037 0.358 ± 0.038
0.589 ± 0.034
0.556 ± 0.032
0.656 ± 0.033
0.653 ± 0.033
0.640 ± 0.035
0.645 ± 0.034
0.631 ± 0.035
0.601 ± 0.035
0.591± 0.037 0.400 ± 0.039
SPEARMAN† PEARSON†
0.499 ± 0.025
0.434 ± 0.032
0.527 ± 0.030
0.524 ± 0.030
0.546 ± 0.029
0.568 ± 0.031
0.507 ± 0.029
0.504 ± 0.027
0.328 ± 0.029
0.242 ± 0.029
0.316 ± 0.028
0.333 ± 0.028
0.348 ± 0.028
0.324 ± 0.029
0.442 ± 0.031
0.448 ± 0.030
0.442 ± 0.031
0.447 ± 0.030
0.452 ± 0.030
0.460 ± 0.030
UGR
BGR
PRF
GIW
UGR+PRF
UGR+GIW
LEN+UGR+STR1
LEN+UGR+STR1+SEN
LEN+UGR+STR1+HTM
LEN+UGR+STR1+SYN
LEN+UGR+STR1+SEN+HTM+SYN
LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW
</figure>
<bodyText confidence="0.998528826086956">
words are most important in a review and does
not use additional knowledge about the meaning
of the words (at least not the semantics contained
in PRF and GIW).
We tested two different versions of the Stars
feature: i) the number of star ratings, STR1; and
ii) the difference between the star rating and the
average rating of the review, STR2. The second
block of feature combinations in Table 4 shows
that neither is significantly better than the other
so we chose STR1 for our best performing sys-
tem.
Our experiments also revealed that our struc-
tural features Sentential and HTML, as well as
our syntactic features, Syntax, did not show any
significant improvement in system performance.
In the last block of feature combinations in Table
4, we report the performance of our best per-
forming features (Length, Unigram, and Stars)
along with these other features. Though none of
the features cause a performance deterioration,
neither of them significantly improves perform-
ance.
</bodyText>
<subsectionHeader confidence="0.650802">
5.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999578548387097">
In this section, we discuss the broader implica-
tions and potential impacts of our work, and pos-
sible connections with other research directions.
The usefulness of the Stars feature for deter-
mining review helpfulness suggests the need for
developing automatic methods for assessing pro-
duct ratings, e.g., (Pang and Lee 2005).
Our findings focus on predictors of helpful-
ness of reviews of tangible consumer products
(consumer electronics). Helpfulness is also solic-
ited and tracked for reviews of many other types
of entities: restaurants (citysearch.com), films
(imdb.com), reviews of open-source software
modules (cpanratings.perl.org), and countless
others. Our findings of the importance of Length,
Unigrams, and Stars may provide the basis of
comparison for assessing helpfulness of reviews
of other entity types.
Our work represents an initial step in assessing
helpfulness. In the future, we plan to investigate
other possible indicators of helpfulness such as a
reviewer’s reputation, the use of comparatives
(e.g., more and better than), and references to
other products.
Taken further, this work may have interesting
connections to work on personalization, social
networks, and recommender systems, for in-
stance by identifying the reviews that a particular
user would find helpful.
Our work on helpfulness of reviews also has
potential applications to work on automatic gen-
</bodyText>
<page confidence="0.99746">
429
</page>
<bodyText confidence="0.9999558">
eration of review information, by providing a
way to assess helpfulness of automatically gener-
ated reviews. Work on generation of reviews in-
cludes review summarization and extraction of
useful reviews from blogs and other mixed texts.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999981965517241">
Ranking reviews according to user helpfulness is
an important problem for many online sites such
as Amazon.com and Ebay.com. To date, most
websites measure helpfulness by having users
manually assess how helpful each review is to
them. In this paper, we proposed an algorithm for
automatically assessing helpfulness and ranking
reviews according to it. Exploiting the multitude
of user-rated reviews on Amazon.com, we
trained an SVM regression system to learn a
helpfulness function and then applied it to rank
unlabeled reviews. Our best system achieved
Spearman correlation coefficient scores of 0.656
and 0.604 against a gold standard for MP3 play-
ers and digital cameras.
We also performed a detailed analysis of dif-
ferent features to study the importance of several
feature classes in capturing helpfulness. We
found that the most useful features were the
length of the review, its unigrams, and its product
rating. Semantic features like mentions of prod-
uct features and sentiment words seemed to be
subsumed by the simple unigram features. Struc-
tural features (other than length) and syntactic
features had no significant impact.
It is our hope through this work to shed some
light onto what people find helpful in user-
supplied reviews and, by automatically ranking
them, to ultimately enhance user experience.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865850746269">
Attali, Y. and Burstein, J. 2006. Automated Essay
Scoring With e-rater® V.2. Journal of Technology,
Learning, and Assessment, 4(3).
Burstein, J., Chodorow, M., and Leacock, C. 2004.
Automated essay evaluation: the criterion online
writing service. AI Magazine. 25(3), pp 27–36.
Drucker,H., Wu,D. and Vapnik,V. 1999. Support vector
machines for spam categorization. IEEE Trans.
Neural Netw., 10, 1048–1054.
Gabrilovich, E. and Markovitch, S. 2005.
Feature Generation for Text Categorization Using
World Knowledge. In Proceedings of IJCAI-2005.
Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J. 2003. A
practical guide to SVM classification. Technical
report, Department of Computer Science and
Information Technology, National Taiwan University.
Hu, M. and Liu, B. 2004. Mining and summarizing
customer reviews. KDD’04. pp.168 – 177
Kim, S. and Hovy, E. 2004. Determining the Sentiment
of Opinions. Proceedings of COLING-04.
Joachims, T. 1999. Making Large-Scale SVM Learning
Practical. In B. Schölkopf, C. Burges, and A. Smola
(eds), Advances in Kernel Methods: Support Vector
Learning. MIT Press. Cambridge, MA.
Joachims, T. 2002. Optimizing Search Engines Using
Clickthrough Data. In Proceedings of ACM KDD-02.
Moschitti, A. and Basili R. 2004. Complex Linguistic
Features for Text Classification: A Comprehensive
Study. In Proceedings of ECIR 2004. Sunderland,
U.K.
Pang, B, L. Lee, and S. Vaithyanathan. 2001. Thumbs
up? Sentiment Classification using Machine Learning
Techniques. Proceedings of EMNLP 2002.
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the ACL, 2005.
Riloff , E. and J. Wiebe. 2003. Learning Extraction
Patterns for Subjective Expressions. In Proc. of
EMNLP-03.
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning
Subjective Nouns Using Extraction Pattern
Bootstrapping. Proceedings of CoNLL-03
Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003.
A Hybrid Text Classification Approach for Analysis
of Student Essays. In Proc. of the HLT-NAACL, 2003.
Salton, G. and McGill, M. J. 1983. Introduction to
Modern Information Retrieval. McGraw Hill.
Siegel, S. and Castellan, N.J. Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill.
Spearman C. 1904. The Proof and Measurement of
Association Between Two Things. American Journal
of Psychology, 15:72–101.
Turney, P. 2002. Thumbs Up or Thumbs Down?
Semantic Orientation Applied to Unsupervised
Classification of Reviews. Proceedings of the 40th
Annual Meeting of the ACL, Philadelphia, 417–424.
Vapnik, V.N. 1995. The Nature of Statistical Learning
Theory. Springer.
Wiebe, J, R. Bruce, and T. O’Hara. 1999. Development
and use of a gold standard data set for subjectivity
classifications. Proc. of the 37th Annual Meeting of the
Association for Computational Linguistics(ACL-99),
246–253.
Yu, H. and Hatzivassiloglou, V. 2003. Towards
Answering Opinion Questions: Separating Facts from
Opinions and Identifying the Polarity of Opinion
Sentences. Proceedings of EMNLP 2003.
</reference>
<page confidence="0.998226">
430
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.210850">
<title confidence="0.999934">Automatically Assessing Review Helpfulness</title>
<author confidence="0.999763">Patrick Tim Marco</author>
<affiliation confidence="0.9698375">Sciences University of Southern</affiliation>
<address confidence="0.986639">4676 Admiralty</address>
<author confidence="0.947039">Marina del Rey</author>
<author confidence="0.947039">CA</author>
<email confidence="0.993313">skim@isi.edu</email>
<email confidence="0.993313">pantel@isi.edu</email>
<email confidence="0.993313">timc@isi.edu</email>
<author confidence="0.876248">Group</author>
<affiliation confidence="0.984544">University of Rome “Tor</affiliation>
<address confidence="0.6014235">Viale del Politecnico Rome,</address>
<email confidence="0.991598">pennacchiotti@info.uniroma2.it</email>
<abstract confidence="0.986336176470588">User-supplied reviews are widely and increasingly used to enhance ecommerce and other websites. Because reviews can be numerous and varying in quality, it is important to assess how review is. While review helpfulness is currently assessed manually, in this paper we consider the task of automatically assessing it. Experiments using SVM regression on a variety of features over Amazon.com product reviews show promising results, with rank correlations of up to 0.66. We found that the most useful features include the length of the review, its unigrams, and its product rating.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Attali</author>
<author>J Burstein</author>
</authors>
<title>Automated Essay Scoring With e-rater® V.2.</title>
<date>2006</date>
<journal>Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="6102" citStr="Attali and Burstein 2006" startWordPosition="923" endWordPosition="926">tion, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial efforts on detecting email spam messages1, which aim to capture a much broader notion of helpfulness. For an SVM-based approach, see (Drucker et al 1999). Finally, a related area is work on automatic essay scoring, which seeks to rate the quality of an essay (Attali and Burstein 2006; Burstein et al. 2004). The task is important for reducing the human effort required in scoring large numbers 1 See http://www.ceas.cc/, http://spamconference.org/ of student essays regularly written for standard tests such as the GRE. The exact scoring approaches developed in commercial systems are often not disclosed. However, more recent work on one of the major systems, e-rater 2.0, has focused on systematizing and simplifying the set of features used (Attali and Burstein 2006). Our choice of features to test was partially influenced by the features discussed by Attali and Burstein. At th</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Attali, Y. and Burstein, J. 2006. Automated Essay Scoring With e-rater® V.2. Journal of Technology, Learning, and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burstein</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Automated essay evaluation: the criterion online writing service.</title>
<date>2004</date>
<journal>AI Magazine.</journal>
<volume>25</volume>
<issue>3</issue>
<pages>27--36</pages>
<contexts>
<context position="6125" citStr="Burstein et al. 2004" startWordPosition="927" endWordPosition="930">e bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial efforts on detecting email spam messages1, which aim to capture a much broader notion of helpfulness. For an SVM-based approach, see (Drucker et al 1999). Finally, a related area is work on automatic essay scoring, which seeks to rate the quality of an essay (Attali and Burstein 2006; Burstein et al. 2004). The task is important for reducing the human effort required in scoring large numbers 1 See http://www.ceas.cc/, http://spamconference.org/ of student essays regularly written for standard tests such as the GRE. The exact scoring approaches developed in commercial systems are often not disclosed. However, more recent work on one of the major systems, e-rater 2.0, has focused on systematizing and simplifying the set of features used (Attali and Burstein 2006). Our choice of features to test was partially influenced by the features discussed by Attali and Burstein. At the same time, due to dif</context>
</contexts>
<marker>Burstein, Chodorow, Leacock, 2004</marker>
<rawString>Burstein, J., Chodorow, M., and Leacock, C. 2004. Automated essay evaluation: the criterion online writing service. AI Magazine. 25(3), pp 27–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Drucker</author>
<author>D Wu</author>
<author>V Vapnik</author>
</authors>
<title>Support vector machines for spam categorization.</title>
<date>1999</date>
<journal>IEEE Trans. Neural Netw.,</journal>
<volume>10</volume>
<pages>1048--1054</pages>
<contexts>
<context position="5971" citStr="Drucker et al 1999" startWordPosition="899" endWordPosition="902">t domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004). In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial efforts on detecting email spam messages1, which aim to capture a much broader notion of helpfulness. For an SVM-based approach, see (Drucker et al 1999). Finally, a related area is work on automatic essay scoring, which seeks to rate the quality of an essay (Attali and Burstein 2006; Burstein et al. 2004). The task is important for reducing the human effort required in scoring large numbers 1 See http://www.ceas.cc/, http://spamconference.org/ of student essays regularly written for standard tests such as the GRE. The exact scoring approaches developed in commercial systems are often not disclosed. However, more recent work on one of the major systems, e-rater 2.0, has focused on systematizing and simplifying the set of features used (Attali </context>
</contexts>
<marker>Drucker, Wu, Vapnik, 1999</marker>
<rawString>Drucker,H., Wu,D. and Vapnik,V. 1999. Support vector machines for spam categorization. IEEE Trans. Neural Netw., 10, 1048–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Feature Generation for Text Categorization Using World Knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-2005.</booktitle>
<contexts>
<context position="5650" citStr="Gabrilovich and Markovitch, 2005" startWordPosition="847" endWordPosition="851">adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004). In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial efforts on detecting email spam messages1, which aim to capture a much broader notion of helpfulness. For an SVM-based approach, see (Drucker et al 1999). Finally, a related area is work on automatic essay scoring, which seeks to rate the quality of an essay (Attali and Burstein 2006; Burstein et al. 2004). The task is important for reducing the human effort required in scoring large numbers 1 See http://www.ceas.cc/, http://spa</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2005</marker>
<rawString>Gabrilovich, E. and Markovitch, S. 2005. Feature Generation for Text Categorization Using World Knowledge. In Proceedings of IJCAI-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-W Hsu</author>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>A practical guide to SVM classification.</title>
<date>2003</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science and Information Technology, National Taiwan University.</institution>
<contexts>
<context position="18053" citStr="Hsu et al. 2003" startWordPosition="2869" endWordPosition="2872">deployed the state of the art SVM regression tool SVMlight (Joachims 1999). We tested on the development sets various kernels including linear, polynomial (degrees 2, 3, and 4), and radial basis function (RBF). The best performing kernel was RBF and we report only these results in this paper (performance was measured using Spearman’s correlation coefficient, described in Section 5.2). We tuned the RBF kernel parameters C (the penalty parameter) and γ (the kernel width hyperparameter) performing full grid search over the 110 combinations of exponentially spaced parameter pairs (C,γ) following (Hsu et al. 2003). Feature Extraction To extract the features described in Section 3.2, we preprocessed each review using the Minipar dependency parser (Lin 1994). We used the parser tokenization, sentence breaker, and syntactic categorizations to generate the Length, Sentential, Unigram, Bigram, and Syntax feature sets. In order to count the occurrences of product features for the Product-Feature set, we developed an automatic way of mining references to product features from Epinions.com. On this website, user-generated product reviews include explicit lists of pros and cons, describing the best and worst as</context>
<context position="19500" citStr="Hsu et al. 2003" startWordPosition="3102" endWordPosition="3105">ers, and hence their occurrence in a review may correlate with review helpfulness. We filtered out all single-word entries which were infrequently seen (e.g., hold, ever). After splitting and filtering the pro/con lists, we were left with a total of 9,110 unique features for MP3 Players and 13,991 unique features for Digital Cameras. The Stars feature set was created directly from the star ratings given by each author of an Amazon.com review. For each feature measurement f, we applied the following standard transformation: ln(f + 1) and then scaled each feature between [0, 1] as suggested in (Hsu et al. 2003). We experimented with various combinations of feature sets. Our results tables use the abbreviations presented in Section 3.2. For brevity, we report the combinations which contributed to our best performing system and those that help assess the power of the different feature classes in capturing helpfulness. 5.2 Ranking Performance Evaluating the quality of a particular ranking is difficult since certain ranking intervals can be more important than others (e.g., top-10 versus bottom-10) We adopt the Spearman correlation coefficient p (Spearman 1904) since it is the most commonly used measure</context>
</contexts>
<marker>Hsu, Chang, Lin, 2003</marker>
<rawString>Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J. 2003. A practical guide to SVM classification. Technical report, Department of Computer Science and Information Technology, National Taiwan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hu</author>
<author>B Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>KDD’04. pp.168 –</booktitle>
<pages>177</pages>
<contexts>
<context position="4002" citStr="Hu and Liu 2004" startWordPosition="597" endWordPosition="600"> of verbs and nouns), semantic (e.g., product feature mentions), and meta-data (e.g., star rating). 2 Relevant Work The task of automatically assessing product review helpfulness is related to these broader areas 423 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423–430, Sydney, July 2006. c�2006 Association for Computational Linguistics of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification. In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here – assessing review helpfulness. Pang and Lee (2005) have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (discussed in Section 5). However, a user’s overall rating for the product is often already available. Helpfulness, on the other hand, is valuable to assess because it is not explicitly known in current approaches until many users vote on the helpfulness of a review. In opinion a</context>
<context position="5405" citStr="Hu and Liu 2004" startWordPosition="814" endWordPosition="817">various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004). In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial efforts on detecting email spam messages1, which aim to capture a much broader notion of helpfulness. For an SVM-based approach, see (Drucker et al 1999). Finally, a related area is work </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Hu, M. and Liu, B. 2004. Mining and summarizing customer reviews. KDD’04. pp.168 – 177</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the Sentiment of Opinions.</title>
<date>2004</date>
<booktitle>Proceedings of COLING-04.</booktitle>
<marker>Kim, Hovy, 2004</marker>
<rawString>Kim, S. and Hovy, E. 2004. Determining the Sentiment of Opinions. Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical. In</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="17511" citStr="Joachims 1999" startWordPosition="2786" endWordPosition="2787">st five user ratings (i.e., reviews where less than five users voted it as helpful or unhelpful are filtered out). This filtering was performed before duplicate detection and discarded 45.7% of the MP3 Players reviews and 32.7% of the Digital Cameras reviews. Table 2 describes statistics for the final datasets after the filtering steps. 10% of products for both datasets were withheld as development corpora and the remaining 90% were randomly sorted into 10 sets for 10-fold cross validation. SVM Regression For our regression model, we deployed the state of the art SVM regression tool SVMlight (Joachims 1999). We tested on the development sets various kernels including linear, polynomial (degrees 2, 3, and 4), and radial basis function (RBF). The best performing kernel was RBF and we report only these results in this paper (performance was measured using Spearman’s correlation coefficient, described in Section 5.2). We tuned the RBF kernel parameters C (the penalty parameter) and γ (the kernel width hyperparameter) performing full grid search over the 110 combinations of exponentially spaced parameter pairs (C,γ) following (Hsu et al. 2003). Feature Extraction To extract the features described in </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. 1999. Making Large-Scale SVM Learning Practical. In B. Schölkopf, C. Burges, and A. Smola (eds), Advances in Kernel Methods: Support Vector Learning. MIT Press. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Optimizing Search Engines Using Clickthrough Data.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM KDD-02.</booktitle>
<contexts>
<context position="13662" citStr="Joachims 2002" startWordPosition="2159" endWordPosition="2160">tion in Equation 1 using user ratings extracted from Amazon.com, where rating+(r) is the number of unique users that rated the review r as helpful and rating-(r) is the number of unique users that rated r as unhelpful. Reviews from Amazon.com form a gold standard labeled dataset of {review, h(review)} pairs that can be used to train a supervised machine learning algorithm. In this paper, we applied an SVM (Vapnik 1995) package on the features extracted from reviews to learn the function h. Two natural options for learning helpfulness according to Equation 1 are SVM Regression and SVM Ranking (Joachims 2002). Though learning to rank according to helpfulness requires only SVM Ranking, the helpfulness function provides non-uniform differences between ranks in the training set. Also, in practice, many products have only one review, which can serve as training data for SVM Regression but not SVM Ranking. Furthermore, in large sites such as Amazon.com, when new reviews are written it is inefficient to re-rank all previously ranked reviews. We therefore choose SVM Regression in this paper. We describe the exact implementation in Section 5.1. After the SVM is trained, for a given product and its set of </context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>Joachims, T. 2002. Optimizing Search Engines Using Clickthrough Data. In Proceedings of ACM KDD-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>R Basili</author>
</authors>
<title>Complex Linguistic Features for Text Classification: A Comprehensive Study.</title>
<date>2004</date>
<booktitle>In Proceedings of ECIR</booktitle>
<publisher>Sunderland, U.K.</publisher>
<contexts>
<context position="10033" citStr="Moschitti and Basili 2004" startWordPosition="1562" endWordPosition="1565">ludes the percentage of parsed tokens that are open-class (i.e., nouns, verbs, adjectives and adverbs), the percentage of tokens that are nouns, the percentage of tokens that are verbs, the percentage of tokens that are verbs conjugated in the first person, and the percentage of tokens that are adjectives or adverbs. Semantic Features Most online reviews are fairly short; their sparsity suggests that bigram features will not perform well (which is supported by our experiments described in Section 5.3). Although semantic features have rarely been effective in many text classification problems (Moschitti and Basili 2004), there is reason here to hypothesize that a specialized vocabulary of important words might help with the sparsity. We hypothesized 2 Reviews are analyzed using the Minipar dependency parser (Lin 1994). that good reviews will often contain: i) references to the features of a product (e.g., the LCD and resolution of a digital camera), and ii) mentions of sentiment words (i.e., words that express an opinion such as “great screen”). Below we describe two families of features that capture these semantic observations within the reviews: • Product-Feature (PRF): The features of products that occur </context>
</contexts>
<marker>Moschitti, Basili, 2004</marker>
<rawString>Moschitti, A. and Basili R. 2004. Complex Linguistic Features for Text Classification: A Comprehensive Study. In Proceedings of ECIR 2004. Sunderland, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2001</date>
<booktitle>Proceedings of EMNLP</booktitle>
<marker>Pang, Lee, Vaithyanathan, 2001</marker>
<rawString>Pang, B, L. Lee, and S. Vaithyanathan. 2001. Thumbs up? Sentiment Classification using Machine Learning Techniques. Proceedings of EMNLP 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<contexts>
<context position="4035" citStr="Pang and Lee 2005" startWordPosition="603" endWordPosition="606"> (e.g., product feature mentions), and meta-data (e.g., star rating). 2 Relevant Work The task of automatically assessing product review helpfulness is related to these broader areas 423 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423–430, Sydney, July 2006. c�2006 Association for Computational Linguistics of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification. In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here – assessing review helpfulness. Pang and Lee (2005) have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (discussed in Section 5). However, a user’s overall rating for the product is often already available. Helpfulness, on the other hand, is valuable to assess because it is not explicitly known in current approaches until many users vote on the helpfulness of a review. In opinion and sentiment analysis, the focus </context>
<context position="27855" citStr="Pang and Lee 2005" startWordPosition="4466" endWordPosition="4469">st block of feature combinations in Table 4, we report the performance of our best performing features (Length, Unigram, and Stars) along with these other features. Though none of the features cause a performance deterioration, neither of them significantly improves performance. 5.4 Discussion In this section, we discuss the broader implications and potential impacts of our work, and possible connections with other research directions. The usefulness of the Stars feature for determining review helpfulness suggests the need for developing automatic methods for assessing product ratings, e.g., (Pang and Lee 2005). Our findings focus on predictors of helpfulness of reviews of tangible consumer products (consumer electronics). Helpfulness is also solicited and tracked for reviews of many other types of entities: restaurants (citysearch.com), films (imdb.com), reviews of open-source software modules (cpanratings.perl.org), and countless others. Our findings of the importance of Length, Unigrams, and Stars may provide the basis of comparison for assessing helpfulness of reviews of other entity types. Our work represents an initial step in assessing helpfulness. In the future, we plan to investigate other </context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E</author>
<author>J Wiebe</author>
</authors>
<title>Learning Extraction Patterns for Subjective Expressions.</title>
<date>2003</date>
<booktitle>In Proc. of EMNLP-03.</booktitle>
<marker>E, Wiebe, 2003</marker>
<rawString>Riloff , E. and J. Wiebe. 2003. Learning Extraction Patterns for Subjective Expressions. In Proc. of EMNLP-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
<author>T Wilson</author>
</authors>
<title>Learning Subjective Nouns Using Extraction Pattern Bootstrapping.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL-03</booktitle>
<marker>Riloff, Wiebe, Wilson, 2003</marker>
<rawString>Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning Subjective Nouns Using Extraction Pattern Bootstrapping. Proceedings of CoNLL-03</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rose</author>
<author>A Roque</author>
<author>D Bhembe</author>
<author>K Vanlehn</author>
</authors>
<title>A Hybrid Text Classification Approach for Analysis of Student Essays.</title>
<date>2003</date>
<booktitle>In Proc. of the HLT-NAACL,</booktitle>
<marker>Rose, Roque, Bhembe, Vanlehn, 2003</marker>
<rawString>Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003. A Hybrid Text Classification Approach for Analysis of Student Essays. In Proc. of the HLT-NAACL, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="9143" citStr="Salton and McGill 1983" startWordPosition="1418" endWordPosition="1421">of sentences, the average sentence length, the percentage of question sentences, and the number of exclamation marks. • HTML (HTM): Two features for the number of bold tags &lt;b&gt; and line breaks &lt;br&gt;. Lexical Features Lexical features capture the words observed in the reviews. We experimented with two sets of features: • Unigram (UGR): The tf-idf statistic of each word occurring in a review. • Bigram (BGR): The tf-idf statistic of each bigram occurring in a review. For both unigrams and bigrams, we used lemmatized words from a syntactic analysis of the reviews and computed the tf-idf statistic (Salton and McGill 1983) using the following formula: tf idf = tf × log(idf ) N where N is the number of tokens in the review. Syntactic Features Syntactic features aim to capture the linguistic properties of the review. We grouped them into the following feature set: • Syntax (SYN): Includes the percentage of parsed tokens that are open-class (i.e., nouns, verbs, adjectives and adverbs), the percentage of tokens that are nouns, the percentage of tokens that are verbs, the percentage of tokens that are verbs conjugated in the first person, and the percentage of tokens that are adjectives or adverbs. Semantic Features</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G. and McGill, M. J. 1983. Introduction to Modern Information Retrieval. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Siegel</author>
<author>N J Jr Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="20520" citStr="Siegel and Castellan (1988)" startWordPosition="3264" endWordPosition="3268">lt since certain ranking intervals can be more important than others (e.g., top-10 versus bottom-10) We adopt the Spearman correlation coefficient p (Spearman 1904) since it is the most commonly used measure of correlation between two sets of ranked data points4. For each fold in our 10-fold cross-validation experiments, we trained our SVM system using 9 folds. For the remaining test fold, we ranked each product’s reviews according to the SVM prediction (described in Section 4) and computed the p 4 We used the version of Spearman’s correlation coefficient that allows for ties in rankings. See Siegel and Castellan (1988) for more on alternate rank statistics such as Kendall’s tau. 427 Table 3. Evaluation of the feature combinations that make up our best performing system (in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras according to helpfulness. FEATURE COMBINATIONS SPEARMAN† MP3 PLAYERS DIGITAL CAMERAS PEARSON† SPEARMAN† PEARSON† LEN 0.575 f 0.037 0.391 f 0.038 0.521 f 0.029 0.357 f 0.029 UGR 0.593 f 0.036 0.398 f 0.038 0.499 f 0.025 0.328 f 0.029 STR1 0.589 f 0.034 0.326 f 0.038 0.507 f 0.029 0.266 f 0.030 UGR+STR1 0.644 f 0.033 0.436 f 0.038 0.490 f 0.032 0.324 f 0.032 LEN+UGR 0.</context>
</contexts>
<marker>Siegel, Castellan, 1988</marker>
<rawString>Siegel, S. and Castellan, N.J. Jr. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Spearman</author>
</authors>
<title>The Proof and Measurement of Association Between Two Things.</title>
<date>1904</date>
<journal>American Journal of Psychology,</journal>
<pages>15--72</pages>
<contexts>
<context position="20057" citStr="Spearman 1904" startWordPosition="3187" endWordPosition="3188"> feature between [0, 1] as suggested in (Hsu et al. 2003). We experimented with various combinations of feature sets. Our results tables use the abbreviations presented in Section 3.2. For brevity, we report the combinations which contributed to our best performing system and those that help assess the power of the different feature classes in capturing helpfulness. 5.2 Ranking Performance Evaluating the quality of a particular ranking is difficult since certain ranking intervals can be more important than others (e.g., top-10 versus bottom-10) We adopt the Spearman correlation coefficient p (Spearman 1904) since it is the most commonly used measure of correlation between two sets of ranked data points4. For each fold in our 10-fold cross-validation experiments, we trained our SVM system using 9 folds. For the remaining test fold, we ranked each product’s reviews according to the SVM prediction (described in Section 4) and computed the p 4 We used the version of Spearman’s correlation coefficient that allows for ties in rankings. See Siegel and Castellan (1988) for more on alternate rank statistics such as Kendall’s tau. 427 Table 3. Evaluation of the feature combinations that make up our best p</context>
</contexts>
<marker>Spearman, 1904</marker>
<rawString>Spearman C. 1904. The Proof and Measurement of Association Between Two Things. American Journal of Psychology, 15:72–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>417--424</pages>
<location>Philadelphia,</location>
<contexts>
<context position="4015" citStr="Turney 2002" startWordPosition="601" endWordPosition="602">ns), semantic (e.g., product feature mentions), and meta-data (e.g., star rating). 2 Relevant Work The task of automatically assessing product review helpfulness is related to these broader areas 423 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423–430, Sydney, July 2006. c�2006 Association for Computational Linguistics of research: automatic analysis of product reviews, opinion and sentiment analysis, and text classification. In the thriving area of research on automatic analysis and processing of product reviews (Hu and Liu 2004; Turney 2002; Pang and Lee 2005), little attention has been paid to the important task studied here – assessing review helpfulness. Pang and Lee (2005) have studied prediction of product ratings, which may be particularly relevant due to the correlation we find between product rating and the helpfulness of the review (discussed in Section 5). However, a user’s overall rating for the product is often already available. Helpfulness, on the other hand, is valuable to assess because it is not explicitly known in current approaches until many users vote on the helpfulness of a review. In opinion and sentiment </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. Proceedings of the 40th Annual Meeting of the ACL, Philadelphia, 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="13470" citStr="Vapnik 1995" startWordPosition="2128" endWordPosition="2129"> 7 1 “iPod: It&apos;s NOT Music to My Ears” 11 13 25 30 “The best thing I ever bought” 22 32 26 27 “VERY disappointing” 1 18 40 40 4 Ranking System In this paper, we estimate the helpfulness function in Equation 1 using user ratings extracted from Amazon.com, where rating+(r) is the number of unique users that rated the review r as helpful and rating-(r) is the number of unique users that rated r as unhelpful. Reviews from Amazon.com form a gold standard labeled dataset of {review, h(review)} pairs that can be used to train a supervised machine learning algorithm. In this paper, we applied an SVM (Vapnik 1995) package on the features extracted from reviews to learn the function h. Two natural options for learning helpfulness according to Equation 1 are SVM Regression and SVM Ranking (Joachims 2002). Though learning to rank according to helpfulness requires only SVM Ranking, the helpfulness function provides non-uniform differences between ranks in the training set. Also, in practice, many products have only one review, which can serve as training data for SVM Regression but not SVM Ranking. Furthermore, in large sites such as Amazon.com, when new reviews are written it is inefficient to re-rank all</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, V.N. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>R Bruce</author>
<author>T O’Hara</author>
</authors>
<title>Development and use of a gold standard data set for subjectivity classifications.</title>
<date>1999</date>
<booktitle>Proc. of the 37th Annual Meeting of the Association for Computational Linguistics(ACL-99),</booktitle>
<pages>246--253</pages>
<marker>Wiebe, Bruce, O’Hara, 1999</marker>
<rawString>Wiebe, J, R. Bruce, and T. O’Hara. 1999. Development and use of a gold standard data set for subjectivity classifications. Proc. of the 37th Annual Meeting of the Association for Computational Linguistics(ACL-99), 246–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences.</title>
<date>2003</date>
<booktitle>Proceedings of EMNLP</booktitle>
<contexts>
<context position="5217" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="786" endWordPosition="790">w. In opinion and sentiment analysis, the focus is on distinguishing between statements of fact vs. opinion, and on detecting the polarity of sentiments being expressed. Many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles (TREC novelty track 2003 and 2004). In text classification, systems typically use bag-of-words models, although there is some evidence of benefits when introducing relevant semantic knowledge (Gabrilovich and Markovitch, 2005). In this paper, we explore the use of some semantic features for review helpfulness ranking. Another potential relevant classification task is academic and commercial</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Yu, H. and Hatzivassiloglou, V. 2003. Towards Answering Opinion Questions: Separating Facts from Opinions and Identifying the Polarity of Opinion Sentences. Proceedings of EMNLP 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>