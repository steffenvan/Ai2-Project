<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.167897">
<title confidence="0.998085">
Statistically-Driven Alignment-Based Multiword Expression
Identification for Technical Domains
</title>
<author confidence="0.994778">
Helena de Medeiros Caseli*, Aline Villavicencio4*, Andr´e Machado4, Maria Jos´e Finatto°
</author>
<affiliation confidence="0.99850575">
*Department of Computer Science, Federal University of S˜ao Carlos (Brazil)
4Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
*Department of Computer Sciences, Bath University (UK)
°Institute of Language and Linguistics, Federal University of Rio Grande do Sul (Brazil)
</affiliation>
<email confidence="0.9658995">
helenacaseli@dc.ufscar.br, avillavicencio@inf.ufrgs.br,
ammachado@inf.ufrgs.br, mfinatto@terra.com.br
</email>
<sectionHeader confidence="0.997322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999525444444445">
Multiword Expressions (MWEs) are one
of the stumbling blocks for more precise
Natural Language Processing (NLP) sys-
tems. Particularly, the lack of coverage
of MWEs in resources can impact nega-
tively on the performance of tasks and ap-
plications, and can lead to loss of informa-
tion or communication errors. This is es-
pecially problematic in technical domains,
where a significant portion of the vocab-
ulary is composed of MWEs. This pa-
per investigates the use of a statistically-
driven alignment-based approach to the
identification of MWEs in technical cor-
pora. We look at the use of several sources
of data, including parallel corpora, using
English and Portuguese data from a corpus
of Pediatrics, and examining how a sec-
ond language can provide relevant cues for
this tasks. We report results obtained by
a combination of statistical measures and
linguistic information, and compare these
to the reported in the literature. Such an
approach to the (semi-)automatic identifi-
cation of MWEs can considerably speed
up lexicographic work, providing a more
targeted list of MWE candidates.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761156862745">
A multiword expression (MWE) can be defined
as any word combination for which the syntac-
tic or semantic properties of the whole expres-
sion cannot be obtained from its parts (Sag et
al., 2002). Examples of MWEs are phrasal verbs
(break down, rely on), compounds (police car, cof-
fee machine), idioms (rock the boat, let the cat
out of the bag). They are very numerous in lan-
guages, as Biber et al. (1999) note, accouting for
between 30% and 45% of spoken English and 21%
of academic prose, and for Jackendoff (1997) the
number of MWEs in a speaker’s lexicon is of the
same order of magnitude as the number of single
words. However, these estimates are likely to be
underestimates if we consider that for language
from a specific domain the specialized vocabulary
is going to consist largely of MWEs (global warm-
ing, protein sequencing) and new MWEs are con-
stantly appearing (weapons of mass destruction,
axis of evil).
Multiword expressions play an important role
in Natural Language Processing (NLP) applica-
tions, which should not only identify the MWEs
but also be able to deal with them when they are
found (Fazly and Stevenson, 2007). Failing to
identify MWEs may cause serious problems for
many NLP tasks, especially those envolving some
kind of semantic processing. For parsing, for in-
stance, Baldwin et al. (2004), found that for a ran-
dom sample of 20,000 strings from the British Na-
tional Corpus (BNC) even with a broad-coverage
grammar for English (Flickinger, 2000) missing
MWEs accounted for 8% of total parsing errors.
Therefore, there is an enormous need for robust
(semi-)automated ways of acquiring lexical infor-
mation for MWEs (Villavicencio et al., 2007) that
can significantly extend the coverage of resources.
For example, one can more than double the num-
ber of verb-particle constructions (VPCs) entries
in a dictionary, such as the Alvey Natural Lan-
guage Tools (Carroll and Grover, 1989), just ex-
tracting VPCs from a corpus like the BNC (Bald-
win, 2005). Furthermore, as MWEs are language
dependent and culturally motivated, identifying
the adequate translation of MWE occurrences is an
important challenge for machine translation meth-
ods.
In this paper, we investigate experimentally the
use of an alignment-based approach for the iden-
tification of MWEs in technical corpora. We look
at the use of several sources of data, including par-
</bodyText>
<page confidence="0.822358">
1
</page>
<note confidence="0.998959">
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 1–8,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999885666666667">
allel corpora, using English and Portuguese data
from a corpus of Pediatrics, and examining how
a second language can provide relevant cues for
this tasks. In this way, cost-effective tools for the
automatic alignment of texts can generate a list
of MWE candidates with their appropriate trans-
lations. Such an approach to the (semi-)automatic
identification of MWEs can considerably speed up
lexicographic work, providing a more targeted list
of MWE candidates and their translations, for the
construction of bilingual resources, and/or with
some semantic information for monolingual re-
sources.
The remainder of this paper is structured as fol-
lows. Section 2 briefly discusses MWEs and some
previous works on methods for automatically ex-
tracting them. Section 3 presents the resources
used while section 4 describes the methods pro-
posed to extract MWEs as a statistically-driven by-
product of an automatic word alignment process.
Section 5 presents the evaluation methodology and
analyses the results and section 6 finishes this pa-
per with some conclusions and proposals for fu-
ture work.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962394736842">
The term Multiword Expression has been used to
describe a large number of distinct but related phe-
nomena, such as phrasal verbs (e.g. come along),
nominal compounds (e.g. frying pan), institution-
alised phrases (e.g. bread and butter), and many
others (Sag et al., 2002). They are very frequent in
everyday language and this is reflected in several
existing grammars and lexical resources, where
almost half of the entries are Multiword Expres-
sions.
However, due to their heterogeneous charac-
teristics, MWEs present a tough challenge for
both linguistic and computational work (Sag et
al., 2002). Some MWEs are fixed, and do not
present internal variation, such as ad hoc, while
others allow different degrees of internal vari-
ability and modification, such as touch a nerve
(touch/find a nerve) and spill beans (spill sev-
eral/musical/mountains of beans). In terms of se-
mantics, some MWEs are more opaque in their
meaning (e.g. to kick the bucket as to die), while
others have more transparent meanings that can be
inferred from the words in the MWE (e.g. eat up,
where the particle up adds a completive sense to
eat). Therefore, providing appropriate methods
for the automatic identification and treatment of
these phenomena is a real challenge for NLP sys-
tems.
A variety of approaches has been proposed for
automatically identifying MWEs, differing basi-
cally in terms of the type of MWE and lan-
guage to which they apply, and the sources of
information they use. Although some work on
MWEs is type independent (e.g. (Zhang et al.,
2006; Villavicencio et al., 2007)), given the het-
erogeneity of MWEs much of the work looks in-
stead at specific types of MWE like collocations
(Pearce, 2002), compounds (Keller and Lapata,
2003) and VPCs (Baldwin, 2005; Villavicencio,
2005; Carlos Ramisch and Aline Villavicencio and
Leonardo Moura and Marco Idiart, 2008). Some
of these works concentrate on particular languages
(e.g. (Pearce, 2002; Baldwin, 2005) for English
and (Piao et al., 2006) for Chinese), but some
work has also benefitted from asymmetries in lan-
guages, using information from one language to
help deal with MWEs in the other (e.g. (na Vil-
lada Moir´on and Tiedemann, 2006; Caseli et al.,
2009)).
As basis for helping to determine whether a
given sequence of words is in fact an MWE (e.g.
ad hoc vs the small boy) some of these works em-
ploy linguistic knowledge for the task (Villavicen-
cio, 2005), while others employ statistical meth-
ods (Pearce, 2002; Evert and Krenn, 2005; Zhang
et al., 2006; Villavicencio et al., 2007) or combine
them with some kinds of linguistic information
such as syntactic and semantic properties (Bald-
win and Villavicencio, 2002; Van de Cruys and na
Villada Moir´on, 2007) or automatic word align-
ment (na Villada Moir´on and Tiedemann, 2006).
Statistical measures of association have been
commonly used for this task, as they can be demo-
cratically applied to any language and MWE type.
However, there is no consensus about which mea-
sure is best suited for identifying MWEs in gen-
eral. Villavicencio et al. (2007) compared some of
these measures (mutual information, permutation
entropy and x2) for the type-independent detec-
tion of MWEs and found that Mutual Information
seemed to differentiate MWEs from non-MWEs,
but the same was not true of x2. In addition, Ev-
ert and Krenn (2005) found that for MWE iden-
tification the efficacy of a given measure depends
on factors like the type of MWEs being targeted
for identification, the domain and size of the cor-
</bodyText>
<page confidence="0.988268">
2
</page>
<bodyText confidence="0.999931680851064">
pora used, and the amount of low-frequency data
excluded by adopting a threshold. Nonetheless,
Villavicencio et al. (2007), discussing the influ-
ence of the corpus size and nature over the meth-
ods, found that these different measures have a
high level of agreement about MWEs, whether
in carefully constructed corpora or in more het-
erogeneous web-based ones. They also discuss
the results obtained from adopting approaches like
these for extending the coverage of resources, ar-
guing that grammar coverage can be significantly
increased if MWEs are properly identified and
treated (Villavicencio et al., 2007).
Among the methods that use additional infor-
mation along with statistics to extract MWE, the
one proposed by na Villada Moir´on and Tiede-
mann (2006) seems to be the most similar to our
approach. The main difference between them is
the way in which word alignment is used in the
MWE extraction process. In this paper, the word
alignment is the basis for the MWE extraction
process while Villada Moir´on and Tiedemann’s
method uses the alignment just for ranking the
MWE candidates which were extracted on the ba-
sis of association measures (log-likelihood and
salience) and head dependence heuristic (in parsed
data).
Our approach, as described in details by Caseli
et al. (2009), also follows to some extent that
of Zhang et al. (2006), as missing lexical en-
tries for MWEs and related constructions are de-
tected via error mining methods, and this paper fo-
cuses on the extraction of generic MWEs as a by-
product of an automatic word alignment. Another
related work is the automatic detection of non-
compositional compounds (NCC) by Melamed
(1997) in which NCCs are identified by analyz-
ing statistical translation models trained in a huge
corpus by a time-demanding process.
Given this context, our approach proposes
the use of alignment techniques for identifying
MWEs, looking at sequences detected by the
aligner as containing more than one word, which
form the MWE candidates. As a result, sequences
of two or more consecutive source words are
treated as MWE candidates regardless of whether
they are translated as one or more target words.
</bodyText>
<sectionHeader confidence="0.945353" genericHeader="method">
3 The Corpus and Reference Lists
</sectionHeader>
<bodyText confidence="0.99987074074074">
The Corpus of Pediatrics used in these experi-
ments contains 283 texts in Portuguese with a total
of 785,448 words, extracted from the Jornal de Pe-
diatria. From this corpus, the Pediatrics Glossary,
a reference list containing multiword terms and re-
curring expressions, was semi-automatically con-
structed, and manually checked.1 The primary aim
of the Pediatrics Glossary, as an online resource
for long-distance education, was to train, qualify
and support translation students on the domain of
pediatrics texts.
The Pediatrics Glossary was built from the
36,741 ngrams that occurred at least 5 times in the
corpus. These were automatically cleaned or re-
moved using some POS tag patterns (e.g. remov-
ing prepositions from terms that began or ended
with them). In addition, if an ngram was part of a
larger ngram, only the latter appeared in the Glos-
sary, as is the case of aleitamento materno (mater-
nal breastfeeding) which is excluded as it is con-
tained in aleitamento materno exclusivo (exclusive
maternal breastfeeding). This post-processing re-
sulted in 3,645 ngrams, which were manually
checked by translation students, and resulted in
2,407 terms, with 1,421 bigrams, 730 trigrams and
339 ngrams with n larger than 3 (not considered in
the experiments presented in this paper).
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="method">
4 Statistically-Driven and
Alignment-Based methods
</sectionHeader>
<subsectionHeader confidence="0.998384">
4.1 Statistically-Driven method
</subsectionHeader>
<bodyText confidence="0.99998485">
Statistical measures of association have been
widely employed in the identification of MWEs.
The idea behind their use is that they are an in-
expensive language and type independent means
of detecting recurrent patterns. As Firth famously
said a word is characterized by the company it
keeps and since we expect the component words
of an MWE to occur frequently together, then
these measures can give an indication of MWE-
ness. In this way, if a group of words co-occurs
with significantly high frequency when compared
to the frequencies of the individual words, then
they may form an MWE. Indeed, measures such
as Pointwise Mutual Information (PMI), Mutual
Information (MI), x2, log-likelihood (Press et al.,
1992) and others have been employed for this
task, and some of them seem to provide more ac-
curate predictions of MWEness than others. In
fact, in a comparison of some measures for the
type-independent detection of MWEs, MI seemed
</bodyText>
<footnote confidence="0.9983765">
1Available in the TEXTQUIM/UFRGS website: http:
//www.ufrgs.br/textquim
</footnote>
<page confidence="0.996696">
3
</page>
<bodyText confidence="0.999943736842105">
to differentiate MWEs from non-MWEs, but the
same was not true of x2 (Villavicencio et al.,
2007). In this work we use two commonly em-
ployed measures for this task: PMI and MI,
as implemented in the Ngram Statistics Package
(Banerjee and Pedersen, 2003).
From the Portuguese portion of the Corpus of
Pediatrics, 196,105 bigram and 362,663 trigram
MWE candidates were generated, after filtering
ngrams containing punctuation and numbers. In
order to evaluate how these methods perform with-
out any linguistic filtering, the only threshold em-
ployed was a frequency cut-off of 2 occurrences,
resulting in 64,839 bigrams and 54,548 trigrams.
Each of the four measures were then calculated for
these ngrams, and we ranked each n-gram accord-
ing to each of these measures. The average of all
the rankings is used as the combined measure of
the MWE candidates.
</bodyText>
<subsectionHeader confidence="0.871824">
4.2 Alignment-Based method
</subsectionHeader>
<bodyText confidence="0.99993338961039">
The second of the MWE extraction approaches
to be investigated in this paper is the alignment-
based method. The automatic word alignment of
two parallel texts — a text written in one (source)
language and its translation to another (target) lan-
guage — is the process of searching for correspon-
dences between source and target words and se-
quences of words. For each word in a source sen-
tence equivalences in the parallel target sentence
are looked for. Therefore, taking into account a
word alignment between a source word sequence
5 (5 = s1 ... s,,, with n &gt; 2) and a target word
sequence T (T = t1 ... tm with m &gt; 1), that
is 5 H T, the alignmet-based MWE extracion
method assumes that: (a) 5 and T share some se-
mantic features, and (b) 5 may be a MWE.
In other words, the alignment-based MWE ex-
traction method states that the sequence 5 will be
a MWE candidate if it is aligned with a sequence
T composed of one or more words (a n : m align-
ment with n &gt; 2 and m &gt; 1). For example,
the sequence of two Portuguese words aleitamento
materno — which occurs 202 times in the cor-
pus used in our experiments — is a MWE can-
didate because these two words were joined to be
aligned 184 times with the word breastfeeding (a
2 : 1 alignment), 8 times with the word breast-
fed (a 2 : 1 alignment), 2 times with breastfeeding
practice (a 2 : 2 alignment) and so on.
Thus, notice that the alignment-based MWE ex-
traction method does not rely on the conceptual
asymmetries between languages since it does not
expect that a source sequence of words be aligned
with a single target word. The method looks
for the sequences of source words that are fre-
quently joined together during the alignment de-
spite the number of target words involved. These
features indicate that the method priorizes preci-
sion in spite of recall.
It is also important to say that although the se-
quences of source and target words resemble the
phrases used in the phrase-based statistical ma-
chine translation (SMT), they are indeed a re-
finement of them. More specifically, although
both approaches rely on word alignments per-
formed by GIZA++2 (Och and Ney, 2000), in
the alignment-based approach not all sequences of
words are considered as phrases (and MWE can-
didates) but just those with an alignment n : m
(n &gt;= 2) with a target sequence. To confirm
this assumption a phrase-based SMT system was
trained with the same corpus used in our exper-
iments and the number of phrases extracted fol-
lowing both approaches were compared. While
the SMT extracted 819,208 source phrases, our
alignment-based approach (without applying any
part-of-speech or frequency filter) extracted only
34,277. These results show that the alignment-
based approach refines in some way the phrases
of SMT systems.
In this paper, we investigate experimentally
whether MWEs can be identified as a by-product
of the automatic word alignment of parallel texts.
We focus on Portuguese MWEs from the Corpus
of Pediatrics and the evaluation is performed us-
ing the bigrams and trigrams from the Pediatrics
Glossary as gold standard.
To perform the extraction of MWE candi-
dates following the alignment-based approach,
first, the original corpus had to be sentence and
word aligned and Part-of-Speech (POS) tagged.
For these preprocessing steps were used, re-
spectively: a version of the Translation Cor-
pus Aligner (TCA) (Hofland, 1996), the statisti-
cal word aligner GIZA++ (Och and Ney, 2000)
and the morphological analysers and POS taggers
from Apertium3 (Armentano-Oller et al., 2006).
</bodyText>
<footnote confidence="0.9929236">
2GIZA++ is a well-known statistical word aligner that can
be found at: http://www.fjoch.com/GIZA++.html
3Apertium is an open-source machine translation en-
gine and toolbox available at: http://www.apertium.
org.
</footnote>
<page confidence="0.997492">
4
</page>
<bodyText confidence="0.999898935483871">
From the preprocessed corpus, the MWE candi-
dates are extracted as those in which two or more
words have the same alignment, that is, they are
linked to the same target unit. This initial list of
MWE candidates is, then, filtered to remove those
candidates that: (a) match some sequences of POS
tags or words (patterns) defined in previous exper-
iments (Caseli et al., 2009) or (b) whose frequency
is below a certain threshold. The remaining units
in the candidate list are considered to be MWEs.
Several filtering patterns and minimum fre-
quency thresholds were tested and three of them
are presented in details here. The first one (F1)
is the same used during the manual building of
the reference lists of MWEs: (a) patterns begin-
ning with Article + Noun and beginning or finish-
ing with verbs and (b) with a minimum frequency
threshold of 5.
The second one (F2) is the same used in the
(Caseli et al., 2009), mainly: (a) patterns begin-
ning with determiner, auxiliary verb, pronoun, ad-
verb, conjunction and surface forms such as those
of the verb to be (are, is, was, were), relatives
(that, what, when, which, who, why) and prepo-
sitions (from, to, of) and (b) with a minimum fre-
quency threshold of 2.
And the third one (F3) is the same as (Caseli et
al., 2009) plus: (a) patterns beginning or finishing
with determiner, adverb, conjunction, preposition,
verb, pronoun and numeral and (b) with a mini-
mum frequency threshold of 2.
</bodyText>
<sectionHeader confidence="0.998397" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999870333333333">
Table 1 shows the top 5 and the bottom 5 ranked
candidates returned by PMI and the alignment-
based approach. Although some of the results are
good, especially the top candidates, there is still
considerable noise among the candidates, as for
instance jogar video game (lit. play video game).
From table 1 it is also possible to notice that the
alignment-based approach indeed extracts Pedi-
atrics terms such as aleitamento materno (breast-
feeding) and also other possible MWE that are not
Pediatrics terms such as estados unidos (United
States).
In table 2 we show the precision (number of
correct candidates among the proposed ones), re-
call (number of correct candidates among those in
reference lists) and F-measure ((2 * precision *
recall)/(precision + recall)) figures for the as-
sociation measures using all the candidates (on the
</bodyText>
<table confidence="0.910233666666667">
PMI alignment-based
Online Mendelian Inheritance faixa et´aria
Beta Technology Incorporated aleitamento materno
Lange Beta Technology estados unidos
Oxido Nitrico Inalatorio hipertens˜ao arterial
jogar video game leite materno
... ...
e um de couro cabeludo
eado bloqueio lact´ıferos
se que de emocional anatomia
eada neonato a termo
e de nao duplas m˜aes bebˆes
</table>
<tableCaption confidence="0.9877295">
Table 1: Top 5 and Bottom 5 MWE candidates
ranked by PMI and alignment-based approach
</tableCaption>
<table confidence="0.999923380952381">
pt MWE candidates PMI MI
# proposed bigrams 64,839 64,839
# correct MWEs 1403 1403
precision 2.16% 2.16%
recall 98.73% 98.73%
F 4.23% 4.23%
# proposed trigrams 54,548 54,548
# correct MWEs 701 701
precision 1.29% 1.29%
recall 96.03% 96.03%
F 2.55% 2.55%
# proposed bigrams 1,421 1,421
# correct MWEs 155 261
precision 10.91% 18.37%
recall 10.91% 18.37%
F 10.91% 18.37%
# proposed trigrams 730 730
# correct MWEs 44 20
precision 6.03% 2.74%
recall 6.03% 2.74%
F 6.03% 2.74%
</table>
<tableCaption confidence="0.8876835">
Table 2: Evaluation of MWE candidates - PMI and
MI
</tableCaption>
<bodyText confidence="0.999728210526316">
first half of the table) and using the top 1,421 bi-
gram and 730 trigram candidates (on the second
half). From these latter results, we can see that the
top candidates produced by these measures do not
agree with the Pediatrics Glossary, since there are
only at most 18.37% bigram and 6.03% trigram
MWEs among the top candidates, as ranked by
MI and PMI respectively. Interestingly, MI had a
better performance for bigrams while for trigrams
PMI performed better.
On the other hand, looking at the alignment-
based method, 34,277 pt MWE candidates were
extracted and Table 3 sumarizes the number of
candidates filtered following the three filters de-
scribed in 4.2: F1, F2 and F3.
To evaluate the efficacy of the alignment-based
method in identifying multiword terms of Pedi-
atrics, an automatic comparison was performed
using the Pediatrics Glossary. In this auto-
</bodyText>
<page confidence="0.991203">
5
</page>
<table confidence="0.99929">
pt MWE candidates F1 F2 F3
# filtered by POS patterns 24,996 21,544 32,644
# filtered by frequency 9,012 11,855 1,442
# final Set 269 878 191
</table>
<tableCaption confidence="0.978707">
Table 3: Number of pt MWE candidates filtered
in the alignment-based approach
</tableCaption>
<table confidence="0.999957625">
pt MWE candidates F1 F2 F3
# proposed bigrams 250 754 169
# correct MWEs 48 95 65
precision 19.20% 12.60% 38.46%
recall 3.38% 6.69% 4.57%
F 5.75% 8.74% 8.18%
# proposed trigrams 19 110 20
# correct MWEs 1 9 4
precision 5.26% 8.18% 20.00%
recall 0.14% 1.23% 0.55%
F 0.27% 2.14% 1.07%
# proposed bi/trigrams 269 864 189
# correct MWEs 49 104 69
precision 18.22% 12.04% 36,51%
recall 2.28% 4.83% 3.21%
F 4.05% 6.90% 5.90%
</table>
<tableCaption confidence="0.999901">
Table 4: Evaluation of MWE candidates
</tableCaption>
<bodyText confidence="0.999412754098361">
matic comparision we considered the final lists of
MWEs candidates generated by each filter in table
3. The number of matching entries and the values
for precision, recall and F-measure are showed in
table 4.
The different values of extracted MWEs (in ta-
ble 3) and evaluated ones (in table 4) are due to
the restriction of considering only bigrams and tri-
grams in the Pediatrics Glossary. Then, longer
MWEs — such as doenc¸a arterial coronariana
prematura (premature coronary artery disease)
and pequenos para idade gestacional (small for
gestational age) — extracted by the alignment-
based method are not being considered at the mo-
ment.
After the automatic comparison using the Pedi-
atrics Glossary, an analysis by human experts was
performed on one of the derived lists — that with
the best precision values so far (from filter F3).
The human analysis was necessary since, as stated
in (Caseli et al., 2009), the coverage of reference
lists may be low, and it is likely that a lot of MWE
candidates that were not found in the Pediatrics
Glossary are nonetheless true MWEs. In this pa-
per only the pt MWE candidates extracted using
filter F3 (as described in section 4.2) were manu-
ally evaluated.
From the 191 pt MWE candidates extracted af-
ter F3, 69 candidates (36.1% of the total amount)
were found in the bigrams or trigrams in the
Glossary (see table 4). Then, the remaining 122
candidates (63.9%) were analysed by two native-
speakers human judges, who classified each of the
122 candidates as true, if it is a multiword expres-
sion, or false, otherwise independently of being
a Pediatrics term. For the judges, a sequence of
words was considered a MWE mainly if it was:
(1) a proper name or (2) a sequence of words for
which the meaning cannot be obtained by com-
pounding the meanings of its component words.
The judgments of both judges were compared
and a disagreement of approximately 12% on mul-
tiwords was verified. This disagreement was also
measured by the kappa (K) measure (Carletta,
1996), with k = 0.73, which does not prevent
conclusions to be drawn. According to Carletta
(1996), among other authors, a value of k between
0.67 and 0.8 indicates a good agreement.
In order to calculate the percentage of true can-
didates among the 122, two approaches can be
followed, depending on what criteria one wants
to emphasize: precision or coverage (not recall
because we are not calculating regarding a refer-
ence list). To emphasize the precision, one should
consider as genuine MWEs only those candidates
classified as true by both judges, on the other hand,
to emphasize the coverage, one should consider
also those candidates classified as true by just one
of them. So, from 191 MWE candidates, 126
(65.97%) were classified as true by both judges
and 145 (75.92%) by at least one of them.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999960777777778">
MWEs are a complex and heterogeneous set of
phenomena that defy attempts to capture them
fully, but due to their role in communication they
need to be properly accounted for in NLP applica-
tions and tasks.
In this paper we investigated the identifica-
tion of MWEs from technical domain, test-
ing statistically-driven and alignment-based ap-
proaches for identifying MWEs from a Pediatrics
parallel corpus. The alignment-based method gen-
erates a targeted precision-oriented list of MWE
candidates, while the statistical methods produce
recall-oriented results at the expense of precision.
Therefore, the combination of these methods can
produce a set of MWE candidates that is both more
precise than the latter and has more coverage than
the former. This can significantly speed up lex-
icographic work. Moreover, the results obtained
</bodyText>
<page confidence="0.996172">
6
</page>
<bodyText confidence="0.999959423076923">
show that in comparison with the manual extrac-
tion of MWEs, this approach can provide also a
general set of MWE candidates in addition to the
manually selected technical terms.
Using the alignment-based extraction method
we notice that it is possible to extract MWEs that
are Pediatrics terms with a precision of 38% for
bigrams and 20% for trigrams, but with very low
recall since only the MWEs in the Pediatrics Glos-
sary were considered correct. However, after a
manual analysis carried out by two native speakers
of Portuguese we found that the percentage of true
MWEs considered by both or at least one of them
were, respectively, 65.97% and 75.92%. This was
a significative improvement but it is important to
say that, in this manual analysis, the human ex-
perts classified the MWEs as true independently of
them being Pediatrics terms. So, as future work we
intend to carry out a more carefull analysis with
experts in Pediatrics to evaluate how many MWEs
candidates are also Pediatrics terms.
In addition, we plan to investigate a weighted
combination of these methods, favouring those
that have better precision. Finally, we also in-
tend to apply the results obtained in to the semi-
automatic construction of ontologies.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974">
We would like to thank the TEXTQUIM/UFRGS
group for making the Corpus of Pediatrics and Pe-
diatrics Glossary available to us. We also thank the
financial support of FAPESP in bulding the paral-
lel corpus. This research has been partly funded
by the FINEP project COMUNICA.
</bodyText>
<sectionHeader confidence="0.999015" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999780911764706">
Carme Armentano-Oller, Rafael C. Carrasco, Anto-
nio M. Corb´ı-Bellot, Mikel L. Forcada, Mireia
Ginest´ı-Rosell, Sergio Ortiz-Rojas, Juan Anto-
nio P´erez-Ortiz, Gema Ram´ırez-S´anchez, Felipe
S´anchez-Mart´ınez, and Miriam A. Scalco. 2006.
Open-source Portuguese-Spanish machine transla-
tion. In R. Vieira, P. Quaresma, M.G.V. Nunes, N.J.
Mamede, C. Oliveira, and M.C. Dias, editors, Pro-
ceedings of the 7th International Workshop on Com-
putational Processing of Written and Spoken Por-
tuguese, (PROPOR 2006), volume 3960 of Lecture
Notes in Computer Science, pages 50–59. Springer-
Verlag, May.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the Unextractable: A Case Study on Verb-
particles. In Proceedings of the 6th Conference on
Natural Language Learning (CoNLL-2002), pages
98–104, Taipei, Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British Na-
tional Corpus. In Proceedings of the Fourth In-
ternational Conference on Language Resources and
Evaluation (LREC 2004), pages 2047–2050, Lisbon,
Portugal.
Timothy Baldwin. 2005. The deep lexical acquisition
of English verb-particles. Computer Speech and
Language, Special Issue on Multiword Expressions,
19(4):398–414.
Satanjeev Banerjee and Ted Pedersen. 2003. The De-
sign, Implementation and Use of the Ngram Statis-
tics Package. In In Proceedings of the Fourth Inter-
national Conference on Intelligent Text Processing
and Computational Linguistics, pages 370–381.
Douglas Biber, Stig Johansson, Geoffrey Leech, Susan
Conrad, and Edward Finegan. 1999. Grammar of
Spoken and Written English. Longman, Harlow.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistics. Computational
Linguistics, 22(2):249–254.
Carlos Ramisch and Aline Villavicencio and Leonardo
Moura and Marco Idiart. 2008. Picking them
up and Figuring them out: Verb-Particle Construc-
tions, Noise and Idiomaticity. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL 2008), pages 49–56.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In Bran Boguraev and Ted Briscoe, editors,
Computational Lexicography for Natural Language
Processing, pages 117–134. Longman, Harlow, UK.
Helena M. Caseli, Carlos Ramisch, Maria G. V. Nunes,
and Aline Villavicencio. 2009. Alignment-based
extraction of multiword expressions. Language Re-
sources and Evaluation, to appear.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Computer Speech and
Language, 19(4):450–466.
Afsaneh Fazly and Suzanne Stevenson. 2007. Distin-
guishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 9–16, Prague,
June.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15–28.
</reference>
<page confidence="0.990129">
7
</page>
<reference confidence="0.999922304347826">
Knut Hofland. 1996. A program for aligning English
and Norwegian sentences. In S. Hockey, N. Ide,
and G. Perissinotto, editors, Research in Humanities
Computing, pages 165–178, Oxford. Oxford Univer-
sity Press.
Ray Jackendoff. 1997. Twistin’ the night away. Lan-
guage, 73:534–59.
Frank Keller and Mirella Lapata. 2003. Using the Web
to Obtain Frequencies for Unseen Bigrams. Compu-
tational Linguistics, 29(3):459–484.
I. Dan Melamed. 1997. Automatic Discovery of
Non-Compositional Compounds in Parallel Data. In
eprint arXiv:cmp-lg/9706027, pages 6027–+, June.
Bego na Villada Moir´on and Jorg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the Workshop
on Multi-word-expressions in a Multilingual Context
(EACL-2006), pages 33–40, Trento, Italy.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the ACL, pages 440–447,
Hong Kong, China, October.
Darren Pearce. 2002. A Comparative Evaluation of
Collocation Extraction Techniques. In Proceedings
of the Third International Conference on Language
Resources and Evaluation, pages 1–7, Las Palmas,
Canary Islands, Spain.
Scott S. L. Piao, Guangfan Sun, Paul Rayson, and
Qi Yuan. 2006. Automatic Extraction of Chi-
nese Multiword Expressions with a Statistical Tool.
In Proceedings of the Workshop on Multi-word-
expressions in a Multilingual Context (EACL-2006),
pages 17–24, Trento, Italy, April.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing. Sec-
ond edition. Cambridge University Press.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP. In Pro-
ceedings of the Third International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing-2002), volume 2276 of (Lecture
Notes in Computer Science), pages 1–15, London,
UK. Springer-Verlag.
Tim Van de Cruys and Bego na Villada Moir´on. 2007.
Semantics-based Multiword Expression Extraction.
In Proceedings of the Workshop on A Broader Pre-
spective on Multiword Expressions, pages 25–32,
Prague, June.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1034–1043, Prague, June.
Aline Villavicencio. 2005. The Availability of Verb-
Particle Constructions in Lexical Resources: How
Much is Enough? Journal of Computer Speech and
Language Processing, 19(4):415–432.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated Multiword Ex-
pression Prediction for Grammar Engineering. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 36–44, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
</reference>
<page confidence="0.998495">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.335247">
<title confidence="0.997734">Statistically-Driven Alignment-Based Multiword Identification for Technical Domains</title>
<author confidence="0.997124">de_Medeiros Aline Andr´e Maria Jos´e</author>
<affiliation confidence="0.833837">of Computer Science, Federal University of Carlos of Informatics, Federal University of Rio Grande do Sul of Computer Sciences, Bath University</affiliation>
<address confidence="0.564755">of Language and Linguistics, Federal University of Rio Grande do Sul</address>
<email confidence="0.983223">ammachado@inf.ufrgs.br,mfinatto@terra.com.br</email>
<abstract confidence="0.998963214285714">Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems. Particularly, the lack of coverage of MWEs in resources can impact negatively on the performance of tasks and applications, and can lead to loss of information or communication errors. This is especially problematic in technical domains, where a significant portion of the vocabulary is composed of MWEs. This paper investigates the use of a statisticallydriven alignment-based approach to the identification of MWEs in technical corpora. We look at the use of several sources of data, including parallel corpora, using English and Portuguese data from a corpus of Pediatrics, and examining how a second language can provide relevant cues for this tasks. We report results obtained by a combination of statistical measures and linguistic information, and compare these to the reported in the literature. Such an approach to the (semi-)automatic identification of MWEs can considerably speed up lexicographic work, providing a more targeted list of MWE candidates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Carme Armentano-Oller</author>
<author>Rafael C Carrasco</author>
<author>Antonio M Corb´ı-Bellot</author>
<author>Mikel L Forcada</author>
</authors>
<title>Mireia Ginest´ı-Rosell, Sergio Ortiz-Rojas, Juan Antonio P´erez-Ortiz, Gema Ram´ırez-S´anchez, Felipe S´anchez-Mart´ınez, and Miriam</title>
<date>2006</date>
<booktitle>Proceedings of the 7th International Workshop on Computational Processing of Written and Spoken Portuguese, (PROPOR</booktitle>
<volume>3960</volume>
<pages>50--59</pages>
<editor>R. Vieira, P. Quaresma, M.G.V. Nunes, N.J. Mamede, C. Oliveira, and M.C. Dias, editors,</editor>
<publisher>SpringerVerlag,</publisher>
<marker>Armentano-Oller, Carrasco, Corb´ı-Bellot, Forcada, 2006</marker>
<rawString>Carme Armentano-Oller, Rafael C. Carrasco, Antonio M. Corb´ı-Bellot, Mikel L. Forcada, Mireia Ginest´ı-Rosell, Sergio Ortiz-Rojas, Juan Antonio P´erez-Ortiz, Gema Ram´ırez-S´anchez, Felipe S´anchez-Mart´ınez, and Miriam A. Scalco. 2006. Open-source Portuguese-Spanish machine translation. In R. Vieira, P. Quaresma, M.G.V. Nunes, N.J. Mamede, C. Oliveira, and M.C. Dias, editors, Proceedings of the 7th International Workshop on Computational Processing of Written and Spoken Portuguese, (PROPOR 2006), volume 3960 of Lecture Notes in Computer Science, pages 50–59. SpringerVerlag, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Aline Villavicencio</author>
</authors>
<title>Extracting the Unextractable: A Case Study on Verbparticles.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>98--104</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7953" citStr="Baldwin and Villavicencio, 2002" startWordPosition="1259" endWordPosition="1263">rom asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge for the task (Villavicencio, 2005), while others employ statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio, 2002; Van de Cruys and na Villada Moir´on, 2007) or automatic word alignment (na Villada Moir´on and Tiedemann, 2006). Statistical measures of association have been commonly used for this task, as they can be democratically applied to any language and MWE type. However, there is no consensus about which measure is best suited for identifying MWEs in general. Villavicencio et al. (2007) compared some of these measures (mutual information, permutation entropy and x2) for the type-independent detection of MWEs and found that Mutual Information seemed to differentiate MWEs from non-MWEs, but the same </context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>Timothy Baldwin and Aline Villavicencio. 2002. Extracting the Unextractable: A Case Study on Verbparticles. In Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002), pages 98–104, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Ara Kim</author>
<author>Stephan Oepen</author>
</authors>
<title>Road-testing the English Resource Grammar over the British National Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<pages>2047--2050</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="3009" citStr="Baldwin et al. (2004)" startWordPosition="462" endWordPosition="465"> for language from a specific domain the specialized vocabulary is going to consist largely of MWEs (global warming, protein sequencing) and new MWEs are constantly appearing (weapons of mass destruction, axis of evil). Multiword expressions play an important role in Natural Language Processing (NLP) applications, which should not only identify the MWEs but also be able to deal with them when they are found (Fazly and Stevenson, 2007). Failing to identify MWEs may cause serious problems for many NLP tasks, especially those envolving some kind of semantic processing. For parsing, for instance, Baldwin et al. (2004), found that for a random sample of 20,000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English (Flickinger, 2000) missing MWEs accounted for 8% of total parsing errors. Therefore, there is an enormous need for robust (semi-)automated ways of acquiring lexical information for MWEs (Villavicencio et al., 2007) that can significantly extend the coverage of resources. For example, one can more than double the number of verb-particle constructions (VPCs) entries in a dictionary, such as the Alvey Natural Language Tools (Carroll and Grover, 1989), just extra</context>
</contexts>
<marker>Baldwin, Bender, Flickinger, Kim, Oepen, 2004</marker>
<rawString>Timothy Baldwin, Emily M. Bender, Dan Flickinger, Ara Kim, and Stephan Oepen. 2004. Road-testing the English Resource Grammar over the British National Corpus. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004), pages 2047–2050, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>The deep lexical acquisition of English verb-particles.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="3662" citStr="Baldwin, 2005" startWordPosition="570" endWordPosition="572">000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English (Flickinger, 2000) missing MWEs accounted for 8% of total parsing errors. Therefore, there is an enormous need for robust (semi-)automated ways of acquiring lexical information for MWEs (Villavicencio et al., 2007) that can significantly extend the coverage of resources. For example, one can more than double the number of verb-particle constructions (VPCs) entries in a dictionary, such as the Alvey Natural Language Tools (Carroll and Grover, 1989), just extracting VPCs from a corpus like the BNC (Baldwin, 2005). Furthermore, as MWEs are language dependent and culturally motivated, identifying the adequate translation of MWE occurrences is an important challenge for machine translation methods. In this paper, we investigate experimentally the use of an alignment-based approach for the identification of MWEs in technical corpora. We look at the use of several sources of data, including par1 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 1–8, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP allel corpora, using English and Portuguese data from a corpus of Pediat</context>
<context position="7039" citStr="Baldwin, 2005" startWordPosition="1112" endWordPosition="1113">ing appropriate methods for the automatic identification and treatment of these phenomena is a real challenge for NLP systems. A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ l</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Timothy Baldwin. 2005. The deep lexical acquisition of English verb-particles. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):398–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>The Design, Implementation and Use of the Ngram Statistics Package. In</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>370--381</pages>
<contexts>
<context position="13594" citStr="Banerjee and Pedersen, 2003" startWordPosition="2177" endWordPosition="2180">tion (PMI), Mutual Information (MI), x2, log-likelihood (Press et al., 1992) and others have been employed for this task, and some of them seem to provide more accurate predictions of MWEness than others. In fact, in a comparison of some measures for the type-independent detection of MWEs, MI seemed 1Available in the TEXTQUIM/UFRGS website: http: //www.ufrgs.br/textquim 3 to differentiate MWEs from non-MWEs, but the same was not true of x2 (Villavicencio et al., 2007). In this work we use two commonly employed measures for this task: PMI and MI, as implemented in the Ngram Statistics Package (Banerjee and Pedersen, 2003). From the Portuguese portion of the Corpus of Pediatrics, 196,105 bigram and 362,663 trigram MWE candidates were generated, after filtering ngrams containing punctuation and numbers. In order to evaluate how these methods perform without any linguistic filtering, the only threshold employed was a frequency cut-off of 2 occurrences, resulting in 64,839 bigrams and 54,548 trigrams. Each of the four measures were then calculated for these ngrams, and we ranked each n-gram according to each of these measures. The average of all the rankings is used as the combined measure of the MWE candidates. 4</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. The Design, Implementation and Use of the Ngram Statistics Package. In In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 370–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
<author>Stig Johansson</author>
<author>Geoffrey Leech</author>
<author>Susan Conrad</author>
<author>Edward Finegan</author>
</authors>
<date>1999</date>
<booktitle>Grammar of Spoken and Written English.</booktitle>
<location>Longman, Harlow.</location>
<contexts>
<context position="2093" citStr="Biber et al. (1999)" startWordPosition="310" endWordPosition="313"> these to the reported in the literature. Such an approach to the (semi-)automatic identification of MWEs can considerably speed up lexicographic work, providing a more targeted list of MWE candidates. 1 Introduction A multiword expression (MWE) can be defined as any word combination for which the syntactic or semantic properties of the whole expression cannot be obtained from its parts (Sag et al., 2002). Examples of MWEs are phrasal verbs (break down, rely on), compounds (police car, coffee machine), idioms (rock the boat, let the cat out of the bag). They are very numerous in languages, as Biber et al. (1999) note, accouting for between 30% and 45% of spoken English and 21% of academic prose, and for Jackendoff (1997) the number of MWEs in a speaker’s lexicon is of the same order of magnitude as the number of single words. However, these estimates are likely to be underestimates if we consider that for language from a specific domain the specialized vocabulary is going to consist largely of MWEs (global warming, protein sequencing) and new MWEs are constantly appearing (weapons of mass destruction, axis of evil). Multiword expressions play an important role in Natural Language Processing (NLP) app</context>
</contexts>
<marker>Biber, Johansson, Leech, Conrad, Finegan, 1999</marker>
<rawString>Douglas Biber, Stig Johansson, Geoffrey Leech, Susan Conrad, and Edward Finegan. 1999. Grammar of Spoken and Written English. Longman, Harlow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistics.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="24733" citStr="Carletta, 1996" startWordPosition="4077" endWordPosition="4078">andidates (63.9%) were analysed by two nativespeakers human judges, who classified each of the 122 candidates as true, if it is a multiword expression, or false, otherwise independently of being a Pediatrics term. For the judges, a sequence of words was considered a MWE mainly if it was: (1) a proper name or (2) a sequence of words for which the meaning cannot be obtained by compounding the meanings of its component words. The judgments of both judges were compared and a disagreement of approximately 12% on multiwords was verified. This disagreement was also measured by the kappa (K) measure (Carletta, 1996), with k = 0.73, which does not prevent conclusions to be drawn. According to Carletta (1996), among other authors, a value of k between 0.67 and 0.8 indicates a good agreement. In order to calculate the percentage of true candidates among the 122, two approaches can be followed, depending on what criteria one wants to emphasize: precision or coverage (not recall because we are not calculating regarding a reference list). To emphasize the precision, one should consider as genuine MWEs only those candidates classified as true by both judges, on the other hand, to emphasize the coverage, one sho</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistics. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Leonardo Moura</author>
<author>Marco Idiart</author>
</authors>
<title>Picking them up and Figuring them out: Verb-Particle Constructions, Noise and Idiomaticity.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<pages>49--56</pages>
<marker>Ramisch, Villavicencio, Moura, Idiart, 2008</marker>
<rawString>Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart. 2008. Picking them up and Figuring them out: Verb-Particle Constructions, Noise and Idiomaticity. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL 2008), pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Claire Grover</author>
</authors>
<title>The derivation of a large computational lexicon of English from LDOCE.</title>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing,</booktitle>
<pages>117--134</pages>
<editor>In Bran Boguraev and Ted Briscoe, editors,</editor>
<publisher>Longman, Harlow, UK.</publisher>
<contexts>
<context position="3597" citStr="Carroll and Grover, 1989" startWordPosition="556" endWordPosition="559">, for instance, Baldwin et al. (2004), found that for a random sample of 20,000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English (Flickinger, 2000) missing MWEs accounted for 8% of total parsing errors. Therefore, there is an enormous need for robust (semi-)automated ways of acquiring lexical information for MWEs (Villavicencio et al., 2007) that can significantly extend the coverage of resources. For example, one can more than double the number of verb-particle constructions (VPCs) entries in a dictionary, such as the Alvey Natural Language Tools (Carroll and Grover, 1989), just extracting VPCs from a corpus like the BNC (Baldwin, 2005). Furthermore, as MWEs are language dependent and culturally motivated, identifying the adequate translation of MWE occurrences is an important challenge for machine translation methods. In this paper, we investigate experimentally the use of an alignment-based approach for the identification of MWEs in technical corpora. We look at the use of several sources of data, including par1 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 1–8, Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP allel c</context>
</contexts>
<marker>Carroll, Grover, 1989</marker>
<rawString>John Carroll and Claire Grover. 1989. The derivation of a large computational lexicon of English from LDOCE. In Bran Boguraev and Ted Briscoe, editors, Computational Lexicography for Natural Language Processing, pages 117–134. Longman, Harlow, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helena M Caseli</author>
<author>Carlos Ramisch</author>
<author>Maria G V Nunes</author>
<author>Aline Villavicencio</author>
</authors>
<title>Alignment-based extraction of multiword expressions. Language Resources and Evaluation,</title>
<date>2009</date>
<note>to appear.</note>
<contexts>
<context position="7491" citStr="Caseli et al., 2009" startWordPosition="1182" endWordPosition="1185">erogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge for the task (Villavicencio, 2005), while others employ statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio, 2002; Van de Cruys and na Villada Moir´on, 2007) or automatic word alignment (na Villada Moir´on and Tiedemann, 2006). Statistical measures of</context>
<context position="10068" citStr="Caseli et al. (2009)" startWordPosition="1608" endWordPosition="1611">with statistics to extract MWE, the one proposed by na Villada Moir´on and Tiedemann (2006) seems to be the most similar to our approach. The main difference between them is the way in which word alignment is used in the MWE extraction process. In this paper, the word alignment is the basis for the MWE extraction process while Villada Moir´on and Tiedemann’s method uses the alignment just for ranking the MWE candidates which were extracted on the basis of association measures (log-likelihood and salience) and head dependence heuristic (in parsed data). Our approach, as described in details by Caseli et al. (2009), also follows to some extent that of Zhang et al. (2006), as missing lexical entries for MWEs and related constructions are detected via error mining methods, and this paper focuses on the extraction of generic MWEs as a byproduct of an automatic word alignment. Another related work is the automatic detection of noncompositional compounds (NCC) by Melamed (1997) in which NCCs are identified by analyzing statistical translation models trained in a huge corpus by a time-demanding process. Given this context, our approach proposes the use of alignment techniques for identifying MWEs, looking at </context>
<context position="18310" citStr="Caseli et al., 2009" startWordPosition="2981" endWordPosition="2984">m3 (Armentano-Oller et al., 2006). 2GIZA++ is a well-known statistical word aligner that can be found at: http://www.fjoch.com/GIZA++.html 3Apertium is an open-source machine translation engine and toolbox available at: http://www.apertium. org. 4 From the preprocessed corpus, the MWE candidates are extracted as those in which two or more words have the same alignment, that is, they are linked to the same target unit. This initial list of MWE candidates is, then, filtered to remove those candidates that: (a) match some sequences of POS tags or words (patterns) defined in previous experiments (Caseli et al., 2009) or (b) whose frequency is below a certain threshold. The remaining units in the candidate list are considered to be MWEs. Several filtering patterns and minimum frequency thresholds were tested and three of them are presented in details here. The first one (F1) is the same used during the manual building of the reference lists of MWEs: (a) patterns beginning with Article + Noun and beginning or finishing with verbs and (b) with a minimum frequency threshold of 5. The second one (F2) is the same used in the (Caseli et al., 2009), mainly: (a) patterns beginning with determiner, auxiliary verb, </context>
<context position="23643" citStr="Caseli et al., 2009" startWordPosition="3883" endWordPosition="3886">le 4) are due to the restriction of considering only bigrams and trigrams in the Pediatrics Glossary. Then, longer MWEs — such as doenc¸a arterial coronariana prematura (premature coronary artery disease) and pequenos para idade gestacional (small for gestational age) — extracted by the alignmentbased method are not being considered at the moment. After the automatic comparison using the Pediatrics Glossary, an analysis by human experts was performed on one of the derived lists — that with the best precision values so far (from filter F3). The human analysis was necessary since, as stated in (Caseli et al., 2009), the coverage of reference lists may be low, and it is likely that a lot of MWE candidates that were not found in the Pediatrics Glossary are nonetheless true MWEs. In this paper only the pt MWE candidates extracted using filter F3 (as described in section 4.2) were manually evaluated. From the 191 pt MWE candidates extracted after F3, 69 candidates (36.1% of the total amount) were found in the bigrams or trigrams in the Glossary (see table 4). Then, the remaining 122 candidates (63.9%) were analysed by two nativespeakers human judges, who classified each of the 122 candidates as true, if it </context>
</contexts>
<marker>Caseli, Ramisch, Nunes, Villavicencio, 2009</marker>
<rawString>Helena M. Caseli, Carlos Ramisch, Maria G. V. Nunes, and Aline Villavicencio. 2009. Alignment-based extraction of multiword expressions. Language Resources and Evaluation, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
<author>Brigitte Krenn</author>
</authors>
<title>Using small random samples for the manual evaluation of statistical association measures. Computer Speech and Language,</title>
<date>2005</date>
<contexts>
<context position="7771" citStr="Evert and Krenn, 2005" startWordPosition="1232" endWordPosition="1235">of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge for the task (Villavicencio, 2005), while others employ statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio, 2002; Van de Cruys and na Villada Moir´on, 2007) or automatic word alignment (na Villada Moir´on and Tiedemann, 2006). Statistical measures of association have been commonly used for this task, as they can be democratically applied to any language and MWE type. However, there is no consensus about which measure is best suited for identifying MWEs in general. Villavicencio et al. (2007) compared some of these measures (</context>
</contexts>
<marker>Evert, Krenn, 2005</marker>
<rawString>Stefan Evert and Brigitte Krenn. 2005. Using small random samples for the manual evaluation of statistical association measures. Computer Speech and Language, 19(4):450–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Distinguishing Subtypes of Multiword Expressions Using Linguistically-Motivated Statistical Measures.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on A Broader Perspective on Multiword Expressions,</booktitle>
<pages>9--16</pages>
<location>Prague,</location>
<contexts>
<context position="2826" citStr="Fazly and Stevenson, 2007" startWordPosition="433" endWordPosition="436">997) the number of MWEs in a speaker’s lexicon is of the same order of magnitude as the number of single words. However, these estimates are likely to be underestimates if we consider that for language from a specific domain the specialized vocabulary is going to consist largely of MWEs (global warming, protein sequencing) and new MWEs are constantly appearing (weapons of mass destruction, axis of evil). Multiword expressions play an important role in Natural Language Processing (NLP) applications, which should not only identify the MWEs but also be able to deal with them when they are found (Fazly and Stevenson, 2007). Failing to identify MWEs may cause serious problems for many NLP tasks, especially those envolving some kind of semantic processing. For parsing, for instance, Baldwin et al. (2004), found that for a random sample of 20,000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English (Flickinger, 2000) missing MWEs accounted for 8% of total parsing errors. Therefore, there is an enormous need for robust (semi-)automated ways of acquiring lexical information for MWEs (Villavicencio et al., 2007) that can significantly extend the coverage of resources. For exam</context>
</contexts>
<marker>Fazly, Stevenson, 2007</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2007. Distinguishing Subtypes of Multiword Expressions Using Linguistically-Motivated Statistical Measures. In Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 9–16, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="3164" citStr="Flickinger, 2000" startWordPosition="490" endWordPosition="491">antly appearing (weapons of mass destruction, axis of evil). Multiword expressions play an important role in Natural Language Processing (NLP) applications, which should not only identify the MWEs but also be able to deal with them when they are found (Fazly and Stevenson, 2007). Failing to identify MWEs may cause serious problems for many NLP tasks, especially those envolving some kind of semantic processing. For parsing, for instance, Baldwin et al. (2004), found that for a random sample of 20,000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English (Flickinger, 2000) missing MWEs accounted for 8% of total parsing errors. Therefore, there is an enormous need for robust (semi-)automated ways of acquiring lexical information for MWEs (Villavicencio et al., 2007) that can significantly extend the coverage of resources. For example, one can more than double the number of verb-particle constructions (VPCs) entries in a dictionary, such as the Alvey Natural Language Tools (Carroll and Grover, 1989), just extracting VPCs from a corpus like the BNC (Baldwin, 2005). Furthermore, as MWEs are language dependent and culturally motivated, identifying the adequate trans</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6(1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knut Hofland</author>
</authors>
<title>A program for aligning English and Norwegian sentences.</title>
<date>1996</date>
<booktitle>Research in Humanities Computing,</booktitle>
<pages>165--178</pages>
<editor>In S. Hockey, N. Ide, and G. Perissinotto, editors,</editor>
<publisher>Oxford University Press.</publisher>
<location>Oxford.</location>
<contexts>
<context position="17572" citStr="Hofland, 1996" startWordPosition="2867" endWordPosition="2868">s. In this paper, we investigate experimentally whether MWEs can be identified as a by-product of the automatic word alignment of parallel texts. We focus on Portuguese MWEs from the Corpus of Pediatrics and the evaluation is performed using the bigrams and trigrams from the Pediatrics Glossary as gold standard. To perform the extraction of MWE candidates following the alignment-based approach, first, the original corpus had to be sentence and word aligned and Part-of-Speech (POS) tagged. For these preprocessing steps were used, respectively: a version of the Translation Corpus Aligner (TCA) (Hofland, 1996), the statistical word aligner GIZA++ (Och and Ney, 2000) and the morphological analysers and POS taggers from Apertium3 (Armentano-Oller et al., 2006). 2GIZA++ is a well-known statistical word aligner that can be found at: http://www.fjoch.com/GIZA++.html 3Apertium is an open-source machine translation engine and toolbox available at: http://www.apertium. org. 4 From the preprocessed corpus, the MWE candidates are extracted as those in which two or more words have the same alignment, that is, they are linked to the same target unit. This initial list of MWE candidates is, then, filtered to re</context>
</contexts>
<marker>Hofland, 1996</marker>
<rawString>Knut Hofland. 1996. A program for aligning English and Norwegian sentences. In S. Hockey, N. Ide, and G. Perissinotto, editors, Research in Humanities Computing, pages 165–178, Oxford. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Twistin’ the night away.</title>
<date>1997</date>
<journal>Language,</journal>
<pages>73--534</pages>
<contexts>
<context position="2204" citStr="Jackendoff (1997)" startWordPosition="331" endWordPosition="332">siderably speed up lexicographic work, providing a more targeted list of MWE candidates. 1 Introduction A multiword expression (MWE) can be defined as any word combination for which the syntactic or semantic properties of the whole expression cannot be obtained from its parts (Sag et al., 2002). Examples of MWEs are phrasal verbs (break down, rely on), compounds (police car, coffee machine), idioms (rock the boat, let the cat out of the bag). They are very numerous in languages, as Biber et al. (1999) note, accouting for between 30% and 45% of spoken English and 21% of academic prose, and for Jackendoff (1997) the number of MWEs in a speaker’s lexicon is of the same order of magnitude as the number of single words. However, these estimates are likely to be underestimates if we consider that for language from a specific domain the specialized vocabulary is going to consist largely of MWEs (global warming, protein sequencing) and new MWEs are constantly appearing (weapons of mass destruction, axis of evil). Multiword expressions play an important role in Natural Language Processing (NLP) applications, which should not only identify the MWEs but also be able to deal with them when they are found (Fazl</context>
</contexts>
<marker>Jackendoff, 1997</marker>
<rawString>Ray Jackendoff. 1997. Twistin’ the night away. Language, 73:534–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the Web to Obtain Frequencies for Unseen Bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="7015" citStr="Keller and Lapata, 2003" startWordPosition="1106" endWordPosition="1109">ve sense to eat). Therefore, providing appropriate methods for the automatic identification and treatment of these phenomena is a real challenge for NLP systems. A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the Web to Obtain Frequencies for Unseen Bigrams. Computational Linguistics, 29(3):459–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic Discovery of Non-Compositional Compounds in Parallel Data.</title>
<date>1997</date>
<booktitle>In eprint arXiv:cmp-lg/9706027,</booktitle>
<pages>6027</pages>
<contexts>
<context position="10433" citStr="Melamed (1997)" startWordPosition="1673" endWordPosition="1674">s the alignment just for ranking the MWE candidates which were extracted on the basis of association measures (log-likelihood and salience) and head dependence heuristic (in parsed data). Our approach, as described in details by Caseli et al. (2009), also follows to some extent that of Zhang et al. (2006), as missing lexical entries for MWEs and related constructions are detected via error mining methods, and this paper focuses on the extraction of generic MWEs as a byproduct of an automatic word alignment. Another related work is the automatic detection of noncompositional compounds (NCC) by Melamed (1997) in which NCCs are identified by analyzing statistical translation models trained in a huge corpus by a time-demanding process. Given this context, our approach proposes the use of alignment techniques for identifying MWEs, looking at sequences detected by the aligner as containing more than one word, which form the MWE candidates. As a result, sequences of two or more consecutive source words are treated as MWE candidates regardless of whether they are translated as one or more target words. 3 The Corpus and Reference Lists The Corpus of Pediatrics used in these experiments contains 283 texts</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. Automatic Discovery of Non-Compositional Compounds in Parallel Data. In eprint arXiv:cmp-lg/9706027, pages 6027–+, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bego na Villada Moir´on</author>
<author>Jorg Tiedemann</author>
</authors>
<title>Identifying idiomatic expressions using automatic word-alignment.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multi-word-expressions in a Multilingual Context (EACL-2006),</booktitle>
<pages>33--40</pages>
<location>Trento, Italy.</location>
<marker>Moir´on, Tiedemann, 2006</marker>
<rawString>Bego na Villada Moir´on and Jorg Tiedemann. 2006. Identifying idiomatic expressions using automatic word-alignment. In Proceedings of the Workshop on Multi-word-expressions in a Multilingual Context (EACL-2006), pages 33–40, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="16336" citStr="Och and Ney, 2000" startWordPosition="2668" endWordPosition="2671"> that a source sequence of words be aligned with a single target word. The method looks for the sequences of source words that are frequently joined together during the alignment despite the number of target words involved. These features indicate that the method priorizes precision in spite of recall. It is also important to say that although the sequences of source and target words resemble the phrases used in the phrase-based statistical machine translation (SMT), they are indeed a refinement of them. More specifically, although both approaches rely on word alignments performed by GIZA++2 (Och and Ney, 2000), in the alignment-based approach not all sequences of words are considered as phrases (and MWE candidates) but just those with an alignment n : m (n &gt;= 2) with a target sequence. To confirm this assumption a phrase-based SMT system was trained with the same corpus used in our experiments and the number of phrases extracted following both approaches were compared. While the SMT extracted 819,208 source phrases, our alignment-based approach (without applying any part-of-speech or frequency filter) extracted only 34,277. These results show that the alignmentbased approach refines in some way the</context>
<context position="17629" citStr="Och and Ney, 2000" startWordPosition="2875" endWordPosition="2878">er MWEs can be identified as a by-product of the automatic word alignment of parallel texts. We focus on Portuguese MWEs from the Corpus of Pediatrics and the evaluation is performed using the bigrams and trigrams from the Pediatrics Glossary as gold standard. To perform the extraction of MWE candidates following the alignment-based approach, first, the original corpus had to be sentence and word aligned and Part-of-Speech (POS) tagged. For these preprocessing steps were used, respectively: a version of the Translation Corpus Aligner (TCA) (Hofland, 1996), the statistical word aligner GIZA++ (Och and Ney, 2000) and the morphological analysers and POS taggers from Apertium3 (Armentano-Oller et al., 2006). 2GIZA++ is a well-known statistical word aligner that can be found at: http://www.fjoch.com/GIZA++.html 3Apertium is an open-source machine translation engine and toolbox available at: http://www.apertium. org. 4 From the preprocessed corpus, the MWE candidates are extracted as those in which two or more words have the same alignment, that is, they are linked to the same target unit. This initial list of MWE candidates is, then, filtered to remove those candidates that: (a) match some sequences of P</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the ACL, pages 440–447, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Pearce</author>
</authors>
<title>A Comparative Evaluation of Collocation Extraction Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation,</booktitle>
<pages>1--7</pages>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="6978" citStr="Pearce, 2002" startWordPosition="1103" endWordPosition="1104">article up adds a completive sense to eat). Therefore, providing appropriate methods for the automatic identification and treatment of these phenomena is a real challenge for NLP systems. A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MW</context>
</contexts>
<marker>Pearce, 2002</marker>
<rawString>Darren Pearce. 2002. A Comparative Evaluation of Collocation Extraction Techniques. In Proceedings of the Third International Conference on Language Resources and Evaluation, pages 1–7, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott S L Piao</author>
<author>Guangfan Sun</author>
<author>Paul Rayson</author>
<author>Qi Yuan</author>
</authors>
<title>Automatic Extraction of Chinese Multiword Expressions with a Statistical Tool.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multi-wordexpressions in a Multilingual Context (EACL-2006),</booktitle>
<pages>17--24</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="7272" citStr="Piao et al., 2006" startWordPosition="1144" endWordPosition="1147">terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge for the task (Villavicencio, 2005), while others employ statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic infor</context>
</contexts>
<marker>Piao, Sun, Rayson, Yuan, 2006</marker>
<rawString>Scott S. L. Piao, Guangfan Sun, Paul Rayson, and Qi Yuan. 2006. Automatic Extraction of Chinese Multiword Expressions with a Statistical Tool. In Proceedings of the Workshop on Multi-wordexpressions in a Multilingual Context (EACL-2006), pages 17–24, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C: The Art of Scientific Computing. Second edition.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="13042" citStr="Press et al., 1992" startWordPosition="2086" endWordPosition="2089">he idea behind their use is that they are an inexpensive language and type independent means of detecting recurrent patterns. As Firth famously said a word is characterized by the company it keeps and since we expect the component words of an MWE to occur frequently together, then these measures can give an indication of MWEness. In this way, if a group of words co-occurs with significantly high frequency when compared to the frequencies of the individual words, then they may form an MWE. Indeed, measures such as Pointwise Mutual Information (PMI), Mutual Information (MI), x2, log-likelihood (Press et al., 1992) and others have been employed for this task, and some of them seem to provide more accurate predictions of MWEness than others. In fact, in a comparison of some measures for the type-independent detection of MWEs, MI seemed 1Available in the TEXTQUIM/UFRGS website: http: //www.ufrgs.br/textquim 3 to differentiate MWEs from non-MWEs, but the same was not true of x2 (Villavicencio et al., 2007). In this work we use two commonly employed measures for this task: PMI and MI, as implemented in the Ngram Statistics Package (Banerjee and Pedersen, 2003). From the Portuguese portion of the Corpus of P</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C: The Art of Scientific Computing. Second edition. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword Expressions: A Pain in the Neck for NLP.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing (CICLing-2002),</booktitle>
<volume>2276</volume>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="1882" citStr="Sag et al., 2002" startWordPosition="271" endWordPosition="274">a corpus of Pediatrics, and examining how a second language can provide relevant cues for this tasks. We report results obtained by a combination of statistical measures and linguistic information, and compare these to the reported in the literature. Such an approach to the (semi-)automatic identification of MWEs can considerably speed up lexicographic work, providing a more targeted list of MWE candidates. 1 Introduction A multiword expression (MWE) can be defined as any word combination for which the syntactic or semantic properties of the whole expression cannot be obtained from its parts (Sag et al., 2002). Examples of MWEs are phrasal verbs (break down, rely on), compounds (police car, coffee machine), idioms (rock the boat, let the cat out of the bag). They are very numerous in languages, as Biber et al. (1999) note, accouting for between 30% and 45% of spoken English and 21% of academic prose, and for Jackendoff (1997) the number of MWEs in a speaker’s lexicon is of the same order of magnitude as the number of single words. However, these estimates are likely to be underestimates if we consider that for language from a specific domain the specialized vocabulary is going to consist largely of</context>
<context position="5561" citStr="Sag et al., 2002" startWordPosition="865" endWordPosition="868">ction 3 presents the resources used while section 4 describes the methods proposed to extract MWEs as a statistically-driven byproduct of an automatic word alignment process. Section 5 presents the evaluation methodology and analyses the results and section 6 finishes this paper with some conclusions and proposals for future work. 2 Related Work The term Multiword Expression has been used to describe a large number of distinct but related phenomena, such as phrasal verbs (e.g. come along), nominal compounds (e.g. frying pan), institutionalised phrases (e.g. bread and butter), and many others (Sag et al., 2002). They are very frequent in everyday language and this is reflected in several existing grammars and lexical resources, where almost half of the entries are Multiword Expressions. However, due to their heterogeneous characteristics, MWEs present a tough challenge for both linguistic and computational work (Sag et al., 2002). Some MWEs are fixed, and do not present internal variation, such as ad hoc, while others allow different degrees of internal variability and modification, such as touch a nerve (touch/find a nerve) and spill beans (spill several/musical/mountains of beans). In terms of sem</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword Expressions: A Pain in the Neck for NLP. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing (CICLing-2002), volume 2276 of (Lecture Notes in Computer Science), pages 1–15, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Bego na Villada Moir´on</author>
</authors>
<title>Semantics-based Multiword Expression Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on A Broader Prespective on Multiword Expressions,</booktitle>
<pages>25--32</pages>
<location>Prague,</location>
<marker>Van de Cruys, Moir´on, 2007</marker>
<rawString>Tim Van de Cruys and Bego na Villada Moir´on. 2007. Semantics-based Multiword Expression Extraction. In Proceedings of the Workshop on A Broader Prespective on Multiword Expressions, pages 25–32, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
<author>Valia Kordoni</author>
<author>Yi Zhang</author>
<author>Marco Idiart</author>
<author>Carlos Ramisch</author>
</authors>
<title>Validation and Evaluation of Automatically Acquired Multiword Expressions for Grammar Engineering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1034--1043</pages>
<location>Prague,</location>
<contexts>
<context position="3360" citStr="Villavicencio et al., 2007" startWordPosition="518" endWordPosition="521">y the MWEs but also be able to deal with them when they are found (Fazly and Stevenson, 2007). Failing to identify MWEs may cause serious problems for many NLP tasks, especially those envolving some kind of semantic processing. For parsing, for instance, Baldwin et al. (2004), found that for a random sample of 20,000 strings from the British National Corpus (BNC) even with a broad-coverage grammar for English (Flickinger, 2000) missing MWEs accounted for 8% of total parsing errors. Therefore, there is an enormous need for robust (semi-)automated ways of acquiring lexical information for MWEs (Villavicencio et al., 2007) that can significantly extend the coverage of resources. For example, one can more than double the number of verb-particle constructions (VPCs) entries in a dictionary, such as the Alvey Natural Language Tools (Carroll and Grover, 1989), just extracting VPCs from a corpus like the BNC (Baldwin, 2005). Furthermore, as MWEs are language dependent and culturally motivated, identifying the adequate translation of MWE occurrences is an important challenge for machine translation methods. In this paper, we investigate experimentally the use of an alignment-based approach for the identification of M</context>
<context position="6855" citStr="Villavicencio et al., 2007" startWordPosition="1079" endWordPosition="1082">e bucket as to die), while others have more transparent meanings that can be inferred from the words in the MWE (e.g. eat up, where the particle up adds a completive sense to eat). Therefore, providing appropriate methods for the automatic identification and treatment of these phenomena is a real challenge for NLP systems. A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and T</context>
<context position="8337" citStr="Villavicencio et al. (2007)" startWordPosition="1325" endWordPosition="1328">ploy statistical methods (Pearce, 2002; Evert and Krenn, 2005; Zhang et al., 2006; Villavicencio et al., 2007) or combine them with some kinds of linguistic information such as syntactic and semantic properties (Baldwin and Villavicencio, 2002; Van de Cruys and na Villada Moir´on, 2007) or automatic word alignment (na Villada Moir´on and Tiedemann, 2006). Statistical measures of association have been commonly used for this task, as they can be democratically applied to any language and MWE type. However, there is no consensus about which measure is best suited for identifying MWEs in general. Villavicencio et al. (2007) compared some of these measures (mutual information, permutation entropy and x2) for the type-independent detection of MWEs and found that Mutual Information seemed to differentiate MWEs from non-MWEs, but the same was not true of x2. In addition, Evert and Krenn (2005) found that for MWE identification the efficacy of a given measure depends on factors like the type of MWEs being targeted for identification, the domain and size of the cor2 pora used, and the amount of low-frequency data excluded by adopting a threshold. Nonetheless, Villavicencio et al. (2007), discussing the influence of th</context>
<context position="13438" citStr="Villavicencio et al., 2007" startWordPosition="2150" endWordPosition="2153">antly high frequency when compared to the frequencies of the individual words, then they may form an MWE. Indeed, measures such as Pointwise Mutual Information (PMI), Mutual Information (MI), x2, log-likelihood (Press et al., 1992) and others have been employed for this task, and some of them seem to provide more accurate predictions of MWEness than others. In fact, in a comparison of some measures for the type-independent detection of MWEs, MI seemed 1Available in the TEXTQUIM/UFRGS website: http: //www.ufrgs.br/textquim 3 to differentiate MWEs from non-MWEs, but the same was not true of x2 (Villavicencio et al., 2007). In this work we use two commonly employed measures for this task: PMI and MI, as implemented in the Ngram Statistics Package (Banerjee and Pedersen, 2003). From the Portuguese portion of the Corpus of Pediatrics, 196,105 bigram and 362,663 trigram MWE candidates were generated, after filtering ngrams containing punctuation and numbers. In order to evaluate how these methods perform without any linguistic filtering, the only threshold employed was a frequency cut-off of 2 occurrences, resulting in 64,839 bigrams and 54,548 trigrams. Each of the four measures were then calculated for these ngr</context>
</contexts>
<marker>Villavicencio, Kordoni, Zhang, Idiart, Ramisch, 2007</marker>
<rawString>Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco Idiart, and Carlos Ramisch. 2007. Validation and Evaluation of Automatically Acquired Multiword Expressions for Grammar Engineering. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1034–1043, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>The Availability of VerbParticle Constructions in Lexical Resources: How Much is Enough?</title>
<date>2005</date>
<journal>Journal of Computer Speech and Language Processing,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="7060" citStr="Villavicencio, 2005" startWordPosition="1114" endWordPosition="1115"> methods for the automatic identification and treatment of these phenomena is a real challenge for NLP systems. A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e.g. (na Villada Moir´on and Tiedemann, 2006; Caseli et al., 2009)). As basis for helping to determine whether a given sequence of words is in fact an MWE (e.g. ad hoc vs the small boy) some of these works employ linguistic knowledge f</context>
</contexts>
<marker>Villavicencio, 2005</marker>
<rawString>Aline Villavicencio. 2005. The Availability of VerbParticle Constructions in Lexical Resources: How Much is Enough? Journal of Computer Speech and Language Processing, 19(4):415–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Valia Kordoni</author>
<author>Aline Villavicencio</author>
<author>Marco Idiart</author>
</authors>
<title>Automated Multiword Expression Prediction for Grammar Engineering.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="6826" citStr="Zhang et al., 2006" startWordPosition="1075" endWordPosition="1078">ing (e.g. to kick the bucket as to die), while others have more transparent meanings that can be inferred from the words in the MWE (e.g. eat up, where the particle up adds a completive sense to eat). Therefore, providing appropriate methods for the automatic identification and treatment of these phenomena is a real challenge for NLP systems. A variety of approaches has been proposed for automatically identifying MWEs, differing basically in terms of the type of MWE and language to which they apply, and the sources of information they use. Although some work on MWEs is type independent (e.g. (Zhang et al., 2006; Villavicencio et al., 2007)), given the heterogeneity of MWEs much of the work looks instead at specific types of MWE like collocations (Pearce, 2002), compounds (Keller and Lapata, 2003) and VPCs (Baldwin, 2005; Villavicencio, 2005; Carlos Ramisch and Aline Villavicencio and Leonardo Moura and Marco Idiart, 2008). Some of these works concentrate on particular languages (e.g. (Pearce, 2002; Baldwin, 2005) for English and (Piao et al., 2006) for Chinese), but some work has also benefitted from asymmetries in languages, using information from one language to help deal with MWEs in the other (e</context>
<context position="10125" citStr="Zhang et al. (2006)" startWordPosition="1619" endWordPosition="1622">lada Moir´on and Tiedemann (2006) seems to be the most similar to our approach. The main difference between them is the way in which word alignment is used in the MWE extraction process. In this paper, the word alignment is the basis for the MWE extraction process while Villada Moir´on and Tiedemann’s method uses the alignment just for ranking the MWE candidates which were extracted on the basis of association measures (log-likelihood and salience) and head dependence heuristic (in parsed data). Our approach, as described in details by Caseli et al. (2009), also follows to some extent that of Zhang et al. (2006), as missing lexical entries for MWEs and related constructions are detected via error mining methods, and this paper focuses on the extraction of generic MWEs as a byproduct of an automatic word alignment. Another related work is the automatic detection of noncompositional compounds (NCC) by Melamed (1997) in which NCCs are identified by analyzing statistical translation models trained in a huge corpus by a time-demanding process. Given this context, our approach proposes the use of alignment techniques for identifying MWEs, looking at sequences detected by the aligner as containing more than</context>
</contexts>
<marker>Zhang, Kordoni, Villavicencio, Idiart, 2006</marker>
<rawString>Yi Zhang, Valia Kordoni, Aline Villavicencio, and Marco Idiart. 2006. Automated Multiword Expression Prediction for Grammar Engineering. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 36–44, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>