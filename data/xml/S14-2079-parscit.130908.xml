<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011804">
<title confidence="0.923336">
OPI: Semeval-2014 Task 3 System Description
</title>
<author confidence="0.974813">
Marek Kozlowski
</author>
<affiliation confidence="0.947333">
National Information Processing Institute
</affiliation>
<email confidence="0.976883">
mkozlowski@opi.org.pl
</email>
<sectionHeader confidence="0.997187" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946090909091">
In this paper, we describe the OPI system
participating in the Semeval-2014 task 3
Cross-Level Semantic Similarity. Our ap-
proach is knowledge-poor, there is no ex-
ploitation of any structured knowledge re-
sources as Wikipedia, WordNet or Babel-
Net. The method is also fully unsuper-
vised, the training set is only used in order
to tune the system. System measures the
semantic similarity of texts using corpus-
based measures of termsets similarity.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999665">
The task Cross-Level Semantic Similarity of
SemEval-2014 aims at an evaluation for seman-
tic similarity across different sizes of text (lexi-
cal levels). Unlike prior SemEval tasks on textual
similarity that have focused on comparing similar-
sized texts, the mentioned task evaluates the case
where larger text must be compared to smaller
text, namely there are covered four semantic sim-
ilarity comparisons: paragraph to sentence, sen-
tence to phrase, phrase to word and word to sense.
We present the method for measuring the se-
mantic similarity of texts using a corpus-based
measure of termsets (set of words) similarity. We
start from preprocessing texts, identifying bound-
ary values, computing termsets similarities and de-
rive from them the final score, which is normal-
ized.
The input of the task consists of two text seg-
ments of different level. We want to determine
a score indicating their semantic similarity of the
smaller item to the larger item. Similarity is scored
from 0 to 4, when 0 means no semantic intersec-
</bodyText>
<footnote confidence="0.909124">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.919153">
tion, 4 means that two items have very similar
meanings.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999311307692308">
There are lots of papers about measuring the
similarity between documents and single words.
Document-level similarity works are based on
Vector Space Models (Salton and Lesk, 1971;
Salton and McGill, 1983). A significant effort has
also been put into measuring similarity at the word
level, namely by approaches that use distributional
semantics (Turney and Pantel, 2010).
Related work can be classified into four ma-
jor categories: vector-based document mod-
els methods, corpus-based methods, knowledge-
based methods and hybrid methods (Islam and
Inkpen, 2008).
Vector-based document models represent docu-
ment as a vector of words and the similarity eval-
uation is based on the number of words that oc-
cur in both texts. Lexical similarity methods have
problems with different words sharing common
sense. Next approaches, such as corpus-based and
knowledge-based methods, overcome the above
issues.
Corpus based methods apply scores provided by
Pointwise Mutual Information (PMI) and Latent
Semantic Analysis (LSA).
The Pointwise Mutual Information (PMI) (Tur-
ney, 2001) between two words wi and wj is:
</bodyText>
<equation confidence="0.9943535">
PMI(wi,wj) = log2 p(wi, wj)
p(wi)p(wj)
</equation>
<bodyText confidence="0.999501625">
The Latent Semantic Analysis (LSA) (Landauer
and Dumais, 1997; Landauer et al., 2007) is a
mathematical method for modelling of the mean-
ing of words and contexts by analysis of represen-
tative corpora. It models the meaning of words and
contexts by projecting them into a vector space of
reduced dimensionality, which is built up by ap-
plying singular value decomposition (SVD).
</bodyText>
<page confidence="0.988307">
454
</page>
<note confidence="0.7311375">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999849">
Knowledge based methods apply information
from semantic networks as WordNet. They ex-
ploit the structure of WordNet to compare con-
cepts. Leacock and Chodorow (1998) proposed
metric based on the length of the shortest path be-
tween two concepts. Lesk (1986) defined sim-
ilarity between concepts as the intersection be-
tween the corresponding glosses. Budanitsky and
Hirst (2006) conducted the research on various
WordNet-based measures. Standard thesaurus-
based measures of word pair similarity are based
only on a single path between concepts. By con-
trast Hughes and Ramage (2009) used a seman-
tic representation of texts from random walks on
WordNet.
Hybrid methods use both corpus-based mea-
sures and knowledge-based measures of word se-
mantic similarity to determine the text similarity
(Islam and Inkpen, 2008). Mihalcea and Corley
(2006) suggested a combined method by exploit-
ing corpus based measures and knowledge-based
measures of words semantic similarity. Another
hybrid method was proposed by Li et al. (2006)
that combines semantic and syntactic information.
The methods presented above are working at
fixed level of textual granularity (documents,
phrases, or words). Pilehvar et al. (2013) proposed
a unified approach to semantic similarity that oper-
ates at multiple levels. The method builds a com-
mon probabilistic representation over word senses
in order to compare different types of linguistic
data. Any lexical item is represented as a distri-
bution over a set of word senses (obtained from
WordNet), named as item’s semantic signature.
</bodyText>
<sectionHeader confidence="0.974096" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999990611111111">
Our system is fully unsupervised and knowledge-
poor. It exploits Wikipedia as a raw corpus for
words co-occurrence estimation. The proposed
method is not using any kind of textual alignment
(e.g. exploiting PoS tagging or WordNet con-
cepts).
The method consists of four steps: prepro-
cessing, identifying boundary values, termset-to-
termset similarity computation, text-to-text sim-
ilarity phase, results normalization. The results
from the text-to-text similarity phase are very of-
ten beyond the range 0-4, therefore we must nor-
malize them. We evaluated two normalization
approaches: linear normalization and non-linear
one. The non-linear normalization is based on
built clusters (referring to integer values from 0 to
4), which are created using training data set. This
step will be described in details in the section 3.5.
</bodyText>
<subsectionHeader confidence="0.999581">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999993333333333">
In the first step the compared texts are retrieved,
and then processed into the contexts. Context is
the preprocessed original text represented as a bag
of words. Texts are processed using a dictionary
of proper names, name entities recognizers, PoS-
taggers, providing as a result the required contexts.
Contexts contain nouns, adjectives, adverbs and
proper names. The output of this stage is a pair
of contexts passed to the next phase.
</bodyText>
<subsectionHeader confidence="0.999887">
3.2 Identifying Boundary Values
</subsectionHeader>
<bodyText confidence="0.999992782608696">
This phase is introduced in order to fast detect
texts, which are unrelated (0 score) or very sim-
ilar (4 score). Unrelated ones are identified bas-
ing on the lack of any co-occurrences between
words from compared texts. It means that any
pair of words from compared contexts do not ap-
pear together in any Wikipedia paragraph. The
very similar texts are identified in two steps. At
first we check if all words from the shorter texts
are contained in the longer one. If the first check
is not fulfilled we compute: (c1,2) as the num-
ber of Wikipedia paragraphs that contain all of
words from both contexts in the nearest neigh-
borhood (20-words window), (c1) and (c2) as the
numbers of Wikipedia paragraphs that contain
contexts within 20-words window. If the ratio
c1,2/max(c1, c2) is higher than 50% then the ana-
lyzed pair of texts refers to the same concept (very
similar ones). Having two texts represented by
contexts we use the proximity Lucene1 query in
order to estimate the number of Wikipedia para-
graphs, which contain the words from contexts
within the 20-words window.
</bodyText>
<subsectionHeader confidence="0.994094">
3.3 Termset-to-termset Similarity
</subsectionHeader>
<bodyText confidence="0.999381">
Termset-to-termset similarity (t2t5im) is defined
by measure similar to PMI. Given a dictionary D
and two termsets (set of words) Wi ⊆ D and
Wj ⊆ D then the measure is expressed by the for-
mula:
</bodyText>
<equation confidence="0.977954">
c(Wi, Wj)
t2t5im(Wi,Wj) =
min(c(Wi), c(Wj))
</equation>
<bodyText confidence="0.982234">
Here, c(X1, .., Xn) is a number of Wikipedia para-
graphs that contain all terms covered by termsets
</bodyText>
<footnote confidence="0.991723">
1http://lucene.apache.org/core/
</footnote>
<page confidence="0.997498">
455
</page>
<bodyText confidence="0.999524538461538">
X1, .., Xn. Two input termsets are semantically
close if the similarity measure t2tSim is higher
than the user-defined threshold (e.g. 10%). Com-
paring to the previous step we use the minimum
operator in the formula’s denominator in order to
take into account even one directed relevant asso-
ciation. It was proved experimentally that the pro-
posed measure leads to better results than the PMI
measure using NEAR query (co-occurrence within
a 10-words window). Specifically, the following
formula is used to collect the PMI value between
termsets using the Wikipedia as a background cor-
pus:
</bodyText>
<equation confidence="0.995504666666667">
c(Wi,Wj) * WikiSize
PMI(Wi,Wj) = log2
c(Wi) * c(Wj)
</equation>
<bodyText confidence="0.999938333333333">
In the performed experiments we approximated
the value of WikiSize to 30 millions (number of
paragraphs of English articles in Wikipedia). In
table 1 we present results of Spearman correla-
tion reported by the System using different mea-
sures PMI and t2tSim. The second measure is
slightly better therefore it was chosen as the final
one. These correlations were computed after lin-
ear normalization of the output measures.
</bodyText>
<table confidence="0.9993366">
Level Measure Spearman
correlation
word2sense PMI 19
word2sense t2tSim 19
phrase2word PMI 29
phrase2word t2tSim 29
sentence2phrase PMI 45
sentence2phrase t2tSim 47
paragraph2sentence PMI 48
paragraph2sentence t2tSim 49
</table>
<tableCaption confidence="0.778117666666667">
Table 1: Comparison of PMI and t2tSim mea-
sures in the semantic similarity task using Spear-
man correlation (percentages).
</tableCaption>
<subsectionHeader confidence="0.93798">
3.4 Text-to-text Similarity
</subsectionHeader>
<bodyText confidence="0.999819666666667">
Given two input texts we compute the termset-to-
termset similarities in order to derive the final se-
mantic score. We attempt to model the semantic
similarity of texts as a function of the semantic
similarities of the component termsets. We do this
by combining metrics of termset-to-termset simi-
larities and weights into a formula that is a poten-
tially good indicator of semantic similarity of the
two input texts. Weights (wm1 &gt; wm2 &gt; wm3)
are experimentally set with linear scalable values
wm1 = 4, wm2 = 2, wm3 = 1 respectively. The
pseudo-code of this phase is in Algorithm 1.
</bodyText>
<equation confidence="0.86116175">
Algorithm 1 Text-to-text similarity
Input: cs, cl are contexts representing shorter and
longer texts respectively; wm1, wm2, wm3 as
weights for different scopes of similarity com-
parison;
Output: m as a similarity measure
m = 0
m = m + t2tSim(cs, cl) * wm1
for term ti E cl do
m = m + t2tSim(cs, {ti}) * wm2
end for
for term tj E cs do
m = m + t2tSim(cl, {tj}) * wm2
end for
for term ti E cs do
for term tj E cl do
m = m + t2tSim({ti}, {tj}) * wm3
end for
end for
return m
</equation>
<sectionHeader confidence="0.820258" genericHeader="method">
3.5 Results Normalization
</sectionHeader>
<bodyText confidence="0.999966904761905">
The crucial part of the method is a process of nor-
malization obtained measures into the range (0,4).
The values 0 and 4 are covered by the step de-
scribed in the section 3.2. We need to normalize
values from the text-to-text similarity phase. This
step can be done in two ways: linear normaliza-
tion and non-linear one. The first one is a ca-
sual transformation defined as dividing elements
by theirs maximum and scaling to 4. The sec-
ond one is based on clustering training set. In
other words, using training set we induce rules
how reported text-to-text similarity values should
be transformed into the range (0,4). We imple-
mented hierarchical agglomerative clustering al-
gorithm (with average linkage)2 in order to clus-
ter similarity measures into five distinct groups.
Sorted centroids of the above created groups are
labeled with values 0 to 4 respectively. For each
new similarity measure (obtained in the testing
phase) we measure the distance to the closest clus-
ter’s centroids. The final value is derived linearly
</bodyText>
<footnote confidence="0.9973632">
2Hierarchical Agglomerative Clustering treats initially
each instance as a singleton cluster and then successively
agglomerate pairs of clusters using the average distance be-
tween cluster’s elements until the user defined number of
clusters persist.
</footnote>
<page confidence="0.998299">
456
</page>
<bodyText confidence="0.999935555555556">
from the distance to the centroids (i.e. if the value
is in the middle between centroids referring to 1
and 2, we assign as a final value 1.5). In the test-
ing step we use the non-linear normalization, the
evaluations on training set show that clustering
based approach provides marginal improvement
against linear normalization (about 1% according
to Spearman rank, 4-8% according to Pearson cor-
relation).
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999718375">
In Task 3, systems were evaluated both within
one of four comparison types and also across all
comparison types. The system outputs and gold
standard ratings are compared in two ways, us-
ing Pearson correlation and Spearman’s rank cor-
relation (rho). Pearson correlation tests the degree
of similarity between the system’s similarity rat-
ings and the gold standard ratings. Spearman’s
rho tests the degree of similarity between the rank-
ings of the items according to similarity. Ranks
were computed by summing the correlation val-
ues across all four levels of comparisons. The sum
of the Pearson correlations is used for the official
rank of Task 3. However, the organizers provide
a second ranking using the sum of the Spearman
correlations.
</bodyText>
<table confidence="0.998107636363636">
Level System Pearson/
Spearman
word2sense OPI 15.2/13.1
word2sense SimCompass 35.6/34.4
word2sense Baseline 10.9/13.0
phrase2word OPI 21.3/18.8
phrase2word SimCompass 41.5/42.4
phrase2word Baseline 16.5/16.2
sentence2phrase OPI 43.3/42.4
sentence2phrase SimCompass 74.2/72.8
sentence2phrase Baseline 56.2/62.6
</table>
<tableCaption confidence="0.62103775">
Table 2: Results for Pearson and Spearman corre-
lation (percentages) scored by OPI System, Sim-
Compass (the best performing one) and the Base-
line one.
</tableCaption>
<bodyText confidence="0.999916382352941">
We submitted only one run in three compari-
son types. We avoided the paragraph-to-sentence
comparison. Evaluations on training set show that
our method reports values below the baseline in
both types: paragraph-to-sentence and sentence-
to-phrase. In the testing phase we decided to per-
form only sentence-to-phrase comparison because
it reports better values than paragraph-to-sentence
according to Pearson correlation, which is used for
the official rank.
The best results our algorithm scores in the cat-
egory phrase-to-word. In this comparison type
it was ranked at 12th position among 21 partic-
ipating systems. In the word-to-sense it was at
14th position among 20 systems. The word-to-
sense comparison is converted into the task sim-
ilar to phrase-to-word by using glosses of target
senses. Each key of WordNet sense is replaced
with its gloss. It is the only situation when we
use the external knowledge resources, but it is
not a part of the algorithm. The last comparison
(sentence-to-phrase) was our worst, because we
did not beat the baseline, as we did in the previous
categories. In the sentence-to-phrase comparison
word alignment or syntax parsing seems to be very
important, in our case none of them was applied.
The main conclusion is that comparison of larger
text units can not be based on bag of words ap-
proaches, where order of words is not important.
Let us recall that our method is knowledge-poor,
what leads to difficulties in evaluating it against
knowledge-rich ones (using sense inventories e.g.
WordNet). Generally, we scored better results us-
ing Pearson correlation than Spearman’s one.
</bodyText>
<sectionHeader confidence="0.999652" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999972666666667">
We presents our cross-level semantic similarity
method, which is knowledge-poor (not using any
kind of structured information from resources like
machine-readable dictionaries, thesaurus, or on-
tologies) and fully unsupervised (there is no learn-
ing phase leading to models enable to catego-
rize compared texts). The method exploits only
Wikipedia as a raw corpora in order to estimate
frequencies of co-occurrences. We were aimed
to verify how good results can be achieved us-
ing only corpus-based approach and not includ-
ing algorithms that have embedded deep language
knowledge. The system scores best in the phrase-
to-word (12th rank) and word-to-sense (14th rank)
types of comparison with regard to Pearson cor-
relation, while performing a little worse with the
Spearman’s correlation. The worst results were
reported in the sentence-to-phrase category, which
brings us the conclusion that larger text units de-
mand word alignment, syntax parsing and more
sophisticated text-to-text similarity models.
</bodyText>
<page confidence="0.99767">
457
</page>
<sectionHeader confidence="0.996267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856762711865">
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of Lexical Seman-
tic Relatedness. Computational Linguistics, 32(1):
13–47.
Thomas Hughes and Daniel Ramage. 2007. Lexical
semantic relatedness with random graph walk. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 581–
589.
Aminul Islam and Diana Inkpen. 2008. Semantic Text
Similarity using Corpus-Based Word Similarity and
String Similarity. ACM Transactions on Knowledge
Discovery from Data, 2(2): 1–25.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Platos problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104: 211–
240.
Thomas Landauer, Danielle McNamara, Simon Den-
nis and Walter Kintsch. 2007. Handbook of Latent
Semantic Analysis. Psychology Press.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet sense similarity
for word sense identification. In WordNet, An Elec-
tronic Lexical Database, pages 265–283.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the SIGDOC Conference, pages 24–26.
Yuhua Li, David McLean, Zuhair Bandar, James
O’Shea and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
IEEE Transactions on Knowledge and Data Engi-
neering, 18(8): 1138-1149.
Rada Mihalcea, Courtney Corley and Carlo Strappa-
rava. 2006. Corpus-based and Knowledge-based
Measures of Text Semantic Similarity. In Proceed-
ings of the American Association for Artificial Intel-
ligence, pages 775–780.
Mohammad Pilehvar, David Jurgens and Roberto Nav-
igli. 2013. Align, Disambiguate and Walk: A Uni-
fied Approach for Measuring Semantic Similarity.
In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages
1341–1351.
Gerard Salton and Michael Lesk. 1971. Computer
evaluation of indexing and text processing. Prentice-
Hall, Englewood Cliffs, New Jersey.
Gerard Salton and Michael McGill. 1983. Alterna-
tion. Introduction to modern information retrieval.
McGraw-Hill.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Articial Intelligence Research, 37:
141–188.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning, pages 491–502.
</reference>
<page confidence="0.997736">
458
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.932624">
<title confidence="0.996578">OPI: Semeval-2014 Task 3 System Description</title>
<author confidence="0.99994">Marek Kozlowski</author>
<affiliation confidence="0.99835">National Information Processing</affiliation>
<email confidence="0.946913">mkozlowski@opi.org.pl</email>
<abstract confidence="0.999106166666667">In this paper, we describe the OPI system participating in the Semeval-2014 task 3 Cross-Level Semantic Similarity. Our approach is knowledge-poor, there is no exploitation of any structured knowledge resources as Wikipedia, WordNet or Babel- Net. The method is also fully unsupervised, the training set is only used in order to tune the system. System measures the semantic similarity of texts using corpusbased measures of termsets similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>13--47</pages>
<contexts>
<context position="3924" citStr="Budanitsky and Hirst (2006)" startWordPosition="601" endWordPosition="604">into a vector space of reduced dimensionality, which is built up by applying singular value decomposition (SVD). 454 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458, Dublin, Ireland, August 23-24, 2014. Knowledge based methods apply information from semantic networks as WordNet. They exploit the structure of WordNet to compare concepts. Leacock and Chodorow (1998) proposed metric based on the length of the shortest path between two concepts. Lesk (1986) defined similarity between concepts as the intersection between the corresponding glosses. Budanitsky and Hirst (2006) conducted the research on various WordNet-based measures. Standard thesaurusbased measures of word pair similarity are based only on a single path between concepts. By contrast Hughes and Ramage (2009) used a semantic representation of texts from random walks on WordNet. Hybrid methods use both corpus-based measures and knowledge-based measures of word semantic similarity to determine the text similarity (Islam and Inkpen, 2008). Mihalcea and Corley (2006) suggested a combined method by exploiting corpus based measures and knowledge-based measures of words semantic similarity. Another hybrid </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1): 13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walk.</title>
<date>2007</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>581--589</pages>
<marker>Hughes, Ramage, 2007</marker>
<rawString>Thomas Hughes and Daniel Ramage. 2007. Lexical semantic relatedness with random graph walk. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 581– 589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic Text Similarity using Corpus-Based Word Similarity and String Similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>2</volume>
<issue>2</issue>
<pages>1--25</pages>
<contexts>
<context position="2454" citStr="Islam and Inkpen, 2008" startWordPosition="373" endWordPosition="376"> means that two items have very similar meanings. 2 Related Work There are lots of papers about measuring the similarity between documents and single words. Document-level similarity works are based on Vector Space Models (Salton and Lesk, 1971; Salton and McGill, 1983). A significant effort has also been put into measuring similarity at the word level, namely by approaches that use distributional semantics (Turney and Pantel, 2010). Related work can be classified into four major categories: vector-based document models methods, corpus-based methods, knowledgebased methods and hybrid methods (Islam and Inkpen, 2008). Vector-based document models represent document as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words sharing common sense. Next approaches, such as corpus-based and knowledge-based methods, overcome the above issues. Corpus based methods apply scores provided by Pointwise Mutual Information (PMI) and Latent Semantic Analysis (LSA). The Pointwise Mutual Information (PMI) (Turney, 2001) between two words wi and wj is: PMI(wi,wj) = log2 p(wi, wj) p(wi)p(wj) The Latent Semantic A</context>
<context position="4357" citStr="Islam and Inkpen, 2008" startWordPosition="668" endWordPosition="671">ed on the length of the shortest path between two concepts. Lesk (1986) defined similarity between concepts as the intersection between the corresponding glosses. Budanitsky and Hirst (2006) conducted the research on various WordNet-based measures. Standard thesaurusbased measures of word pair similarity are based only on a single path between concepts. By contrast Hughes and Ramage (2009) used a semantic representation of texts from random walks on WordNet. Hybrid methods use both corpus-based measures and knowledge-based measures of word semantic similarity to determine the text similarity (Islam and Inkpen, 2008). Mihalcea and Corley (2006) suggested a combined method by exploiting corpus based measures and knowledge-based measures of words semantic similarity. Another hybrid method was proposed by Li et al. (2006) that combines semantic and syntactic information. The methods presented above are working at fixed level of textual granularity (documents, phrases, or words). Pilehvar et al. (2013) proposed a unified approach to semantic similarity that operates at multiple levels. The method builds a common probabilistic representation over word senses in order to compare different types of linguistic da</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic Text Similarity using Corpus-Based Word Similarity and String Similarity. ACM Transactions on Knowledge Discovery from Data, 2(2): 1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<pages>211--240</pages>
<contexts>
<context position="3094" citStr="Landauer and Dumais, 1997" startWordPosition="471" endWordPosition="474">document models represent document as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words sharing common sense. Next approaches, such as corpus-based and knowledge-based methods, overcome the above issues. Corpus based methods apply scores provided by Pointwise Mutual Information (PMI) and Latent Semantic Analysis (LSA). The Pointwise Mutual Information (PMI) (Turney, 2001) between two words wi and wj is: PMI(wi,wj) = log2 p(wi, wj) p(wi)p(wj) The Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997; Landauer et al., 2007) is a mathematical method for modelling of the meaning of words and contexts by analysis of representative corpora. It models the meaning of words and contexts by projecting them into a vector space of reduced dimensionality, which is built up by applying singular value decomposition (SVD). 454 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458, Dublin, Ireland, August 23-24, 2014. Knowledge based methods apply information from semantic networks as WordNet. They exploit the structure of WordNet to compare concepts. Leacock</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104: 211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Danielle McNamara</author>
<author>Simon Dennis</author>
<author>Walter Kintsch</author>
</authors>
<title>Handbook of Latent Semantic Analysis.</title>
<date>2007</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="3118" citStr="Landauer et al., 2007" startWordPosition="475" endWordPosition="478">ocument as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words sharing common sense. Next approaches, such as corpus-based and knowledge-based methods, overcome the above issues. Corpus based methods apply scores provided by Pointwise Mutual Information (PMI) and Latent Semantic Analysis (LSA). The Pointwise Mutual Information (PMI) (Turney, 2001) between two words wi and wj is: PMI(wi,wj) = log2 p(wi, wj) p(wi)p(wj) The Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997; Landauer et al., 2007) is a mathematical method for modelling of the meaning of words and contexts by analysis of representative corpora. It models the meaning of words and contexts by projecting them into a vector space of reduced dimensionality, which is built up by applying singular value decomposition (SVD). 454 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458, Dublin, Ireland, August 23-24, 2014. Knowledge based methods apply information from semantic networks as WordNet. They exploit the structure of WordNet to compare concepts. Leacock and Chodorow (1998) pro</context>
</contexts>
<marker>Landauer, McNamara, Dennis, Kintsch, 2007</marker>
<rawString>Thomas Landauer, Danielle McNamara, Simon Dennis and Walter Kintsch. 2007. Handbook of Latent Semantic Analysis. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database,</title>
<date>1998</date>
<pages>265--283</pages>
<contexts>
<context position="3714" citStr="Leacock and Chodorow (1998)" startWordPosition="568" endWordPosition="571">s, 1997; Landauer et al., 2007) is a mathematical method for modelling of the meaning of words and contexts by analysis of representative corpora. It models the meaning of words and contexts by projecting them into a vector space of reduced dimensionality, which is built up by applying singular value decomposition (SVD). 454 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458, Dublin, Ireland, August 23-24, 2014. Knowledge based methods apply information from semantic networks as WordNet. They exploit the structure of WordNet to compare concepts. Leacock and Chodorow (1998) proposed metric based on the length of the shortest path between two concepts. Lesk (1986) defined similarity between concepts as the intersection between the corresponding glosses. Budanitsky and Hirst (2006) conducted the research on various WordNet-based measures. Standard thesaurusbased measures of word pair similarity are based only on a single path between concepts. By contrast Hughes and Ramage (2009) used a semantic representation of texts from random walks on WordNet. Hybrid methods use both corpus-based measures and knowledge-based measures of word semantic similarity to determine t</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database, pages 265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference,</booktitle>
<pages>24--26</pages>
<contexts>
<context position="3805" citStr="Lesk (1986)" startWordPosition="586" endWordPosition="587"> by analysis of representative corpora. It models the meaning of words and contexts by projecting them into a vector space of reduced dimensionality, which is built up by applying singular value decomposition (SVD). 454 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458, Dublin, Ireland, August 23-24, 2014. Knowledge based methods apply information from semantic networks as WordNet. They exploit the structure of WordNet to compare concepts. Leacock and Chodorow (1998) proposed metric based on the length of the shortest path between two concepts. Lesk (1986) defined similarity between concepts as the intersection between the corresponding glosses. Budanitsky and Hirst (2006) conducted the research on various WordNet-based measures. Standard thesaurusbased measures of word pair similarity are based only on a single path between concepts. By contrast Hughes and Ramage (2009) used a semantic representation of texts from random walks on WordNet. Hybrid methods use both corpus-based measures and knowledge-based measures of word semantic similarity to determine the text similarity (Islam and Inkpen, 2008). Mihalcea and Corley (2006) suggested a combine</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In Proceedings of the SIGDOC Conference, pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair Bandar</author>
<author>James O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<pages>1138--1149</pages>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair Bandar, James O’Shea and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8): 1138-1149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the American Association for Artificial Intelligence, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1341--1351</pages>
<contexts>
<context position="4746" citStr="Pilehvar et al. (2013)" startWordPosition="725" endWordPosition="728">9) used a semantic representation of texts from random walks on WordNet. Hybrid methods use both corpus-based measures and knowledge-based measures of word semantic similarity to determine the text similarity (Islam and Inkpen, 2008). Mihalcea and Corley (2006) suggested a combined method by exploiting corpus based measures and knowledge-based measures of words semantic similarity. Another hybrid method was proposed by Li et al. (2006) that combines semantic and syntactic information. The methods presented above are working at fixed level of textual granularity (documents, phrases, or words). Pilehvar et al. (2013) proposed a unified approach to semantic similarity that operates at multiple levels. The method builds a common probabilistic representation over word senses in order to compare different types of linguistic data. Any lexical item is represented as a distribution over a set of word senses (obtained from WordNet), named as item’s semantic signature. 3 Our Approach Our system is fully unsupervised and knowledgepoor. It exploits Wikipedia as a raw corpus for words co-occurrence estimation. The proposed method is not using any kind of textual alignment (e.g. exploiting PoS tagging or WordNet conc</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Pilehvar, David Jurgens and Roberto Navigli. 2013. Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1341–1351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael Lesk</author>
</authors>
<title>Computer evaluation of indexing and text processing. PrenticeHall, Englewood Cliffs,</title>
<date>1971</date>
<location>New Jersey.</location>
<contexts>
<context position="2075" citStr="Salton and Lesk, 1971" startWordPosition="317" endWordPosition="320">mine a score indicating their semantic similarity of the smaller item to the larger item. Similarity is scored from 0 to 4, when 0 means no semantic intersecThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ tion, 4 means that two items have very similar meanings. 2 Related Work There are lots of papers about measuring the similarity between documents and single words. Document-level similarity works are based on Vector Space Models (Salton and Lesk, 1971; Salton and McGill, 1983). A significant effort has also been put into measuring similarity at the word level, namely by approaches that use distributional semantics (Turney and Pantel, 2010). Related work can be classified into four major categories: vector-based document models methods, corpus-based methods, knowledgebased methods and hybrid methods (Islam and Inkpen, 2008). Vector-based document models represent document as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words </context>
</contexts>
<marker>Salton, Lesk, 1971</marker>
<rawString>Gerard Salton and Michael Lesk. 1971. Computer evaluation of indexing and text processing. PrenticeHall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael McGill</author>
</authors>
<title>Alternation. Introduction to modern information retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="2101" citStr="Salton and McGill, 1983" startWordPosition="321" endWordPosition="324"> their semantic similarity of the smaller item to the larger item. Similarity is scored from 0 to 4, when 0 means no semantic intersecThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ tion, 4 means that two items have very similar meanings. 2 Related Work There are lots of papers about measuring the similarity between documents and single words. Document-level similarity works are based on Vector Space Models (Salton and Lesk, 1971; Salton and McGill, 1983). A significant effort has also been put into measuring similarity at the word level, namely by approaches that use distributional semantics (Turney and Pantel, 2010). Related work can be classified into four major categories: vector-based document models methods, corpus-based methods, knowledgebased methods and hybrid methods (Islam and Inkpen, 2008). Vector-based document models represent document as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words sharing common sense. Next</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael McGill. 1983. Alternation. Introduction to modern information retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Articial Intelligence Research,</journal>
<volume>37</volume>
<pages>141--188</pages>
<contexts>
<context position="2267" citStr="Turney and Pantel, 2010" startWordPosition="346" endWordPosition="349">eative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ tion, 4 means that two items have very similar meanings. 2 Related Work There are lots of papers about measuring the similarity between documents and single words. Document-level similarity works are based on Vector Space Models (Salton and Lesk, 1971; Salton and McGill, 1983). A significant effort has also been put into measuring similarity at the word level, namely by approaches that use distributional semantics (Turney and Pantel, 2010). Related work can be classified into four major categories: vector-based document models methods, corpus-based methods, knowledgebased methods and hybrid methods (Islam and Inkpen, 2008). Vector-based document models represent document as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words sharing common sense. Next approaches, such as corpus-based and knowledge-based methods, overcome the above issues. Corpus based methods apply scores provided by Pointwise Mutual Information (</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Articial Intelligence Research, 37: 141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<contexts>
<context position="2961" citStr="Turney, 2001" startWordPosition="450" endWordPosition="452">t models methods, corpus-based methods, knowledgebased methods and hybrid methods (Islam and Inkpen, 2008). Vector-based document models represent document as a vector of words and the similarity evaluation is based on the number of words that occur in both texts. Lexical similarity methods have problems with different words sharing common sense. Next approaches, such as corpus-based and knowledge-based methods, overcome the above issues. Corpus based methods apply scores provided by Pointwise Mutual Information (PMI) and Latent Semantic Analysis (LSA). The Pointwise Mutual Information (PMI) (Turney, 2001) between two words wi and wj is: PMI(wi,wj) = log2 p(wi, wj) p(wi)p(wj) The Latent Semantic Analysis (LSA) (Landauer and Dumais, 1997; Landauer et al., 2007) is a mathematical method for modelling of the meaning of words and contexts by analysis of representative corpora. It models the meaning of words and contexts by projecting them into a vector space of reduced dimensionality, which is built up by applying singular value decomposition (SVD). 454 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 454–458, Dublin, Ireland, August 23-24, 2014. Knowledge </context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning, pages 491–502.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>