<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001314">
<title confidence="0.9963825">
Semantic Analysis and Helpfulness Prediction of Text
for Online Product Reviews
</title>
<author confidence="0.960122">
Yinfei Yang
</author>
<affiliation confidence="0.77971">
Amazon Inc.
</affiliation>
<address confidence="0.693128">
Seattle, WA 98121
</address>
<email confidence="0.466069">
yangyin7@ gmail.com
</email>
<author confidence="0.91638">
Minghui Qiu
</author>
<affiliation confidence="0.888872">
Alibaba Group
</affiliation>
<address confidence="0.643233">
Hangzhou, China 311121
</address>
<email confidence="0.898335">
minghuiqiu@ gmail.com
</email>
<sectionHeader confidence="0.988518" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931307692308">
Predicting the helpfulness of product re-
views is a key component of many e-
commerce tasks such as review ranking
and recommendation. However, previous
work mixed review helpfulness prediction
with those outer layer tasks. Using non-
text features, it leads to less transferable
models. This paper solves the problem
from a new angle by hypothesizing that
helpfulness is an internal property of text.
Purely using review text, we isolate re-
view helpfulness prediction from its outer
layer tasks, employ two interpretable se-
mantic features, and use human scoring
of helpfulness as ground truth. Experi-
mental results show that the two seman-
tic features can accurately predict helpful-
ness scores and greatly improve the per-
formance compared with using features
previously used. Cross-category test fur-
ther shows the models trained with seman-
tic features are easier to be generalized
to reviews of different product categories.
The models we built are also highly inter-
pretable and align well with human anno-
tations.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9980219">
Product reviews have influential impact to online
shopping as consumers tend to read product re-
views when finalizing purchase decisions (Duan et
al., 2008). However, a popular product usually has
too many reviews for a consumer to read. There-
fore, reviews need to be ranked and recommended
to consumers. In particular, review helpfulness
plays a critical role in review ranking and recom-
mendation (Ghose and Ipeirotis, 2011; Mudambi
and Schuff, 2010; Danescu-Niculescu-Mizil et al.,
</bodyText>
<note confidence="0.715067">
Yaowei Yan
Dept. of Electrical &amp; Computer Engineering
University of Akron
Akron, OH 44325-3904
yy28@ uakron.edu
</note>
<author confidence="0.983652">
Forrest Sheng Bao
</author>
<affiliation confidence="0.998628">
Dept. of Electrical &amp; Computer Engineering
University of Akron
</affiliation>
<address confidence="0.410412">
Akron, OH 44325-3904
</address>
<email confidence="0.581682">
forrest.bao@ gmail.com
</email>
<bodyText confidence="0.999155">
2009). The simple question “Was this review help-
ful to you?” increases an estimated $2.7B revenue
to Amazon.com annually1.
However, existing literature solves helpfulness
prediction together with its outer layer task, the
review ranking (Kim et al., 2006; O’Mahony and
Smyth, 2010; Liu et al., 2008; Martin and Pu,
2014). Those studies use features not contribut-
ing to helpfulness, such as date (Liu et al., 2008),
or features making the model less transferable,
such as product type (Mudambi and Schuff, 2010).
Models built in these ways are also difficult to in-
terpret from linguistic perspective.
Therefore, it is necessary to isolate review help-
fulness prediction from its outer layer tasks and
formulate it as a new problem. In this way, mod-
els can be more robust and generalizable. Beyond
predicting whether a review is helpful, we can also
understand why it is helpful. In our approach, the
results can also facilitate many other tasks, such as
review summarization (Xiong and Litman, 2014)
and sentiment extraction (Hu and Liu, 2004).
Recent NLP studies reveal the connection be-
tween text style and its properties, include read-
ability (Agichtein et al., 2008), informative-
ness (Yang and Nenkova, 2014) and trustworthi-
ness (Pasternack and Roth, 2011) of text. Hence,
we hypothesize that helpfulness is also an under-
lying property of text.
To understand the essence of review text, we
leverage existing linguistic and psychological dic-
tionaries and represent reviews in semantic dimen-
sions. Two semantic features that are new to solv-
ing this problem, LIWC (Pennebaker et al., 2007)
and INQUIRER (Stone et al., 1962), are employed
in this work. The intuition behind is that people
usually embed semantic meanings, such as emo-
tion and reasoning, into text. For example, the re-
</bodyText>
<footnote confidence="0.9984445">
1http://www.uie.com/articles/
magicbehindamazon/
</footnote>
<page confidence="0.957467">
38
</page>
<bodyText confidence="0.964968925">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 38–44,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
view “With the incredible brightness of the main LED, this
light is visible from a distance on a sunny day at noon. is
more helpful than the review “I ordered an iPad, I
received an iPad. I got exactly what I ordered which makes
me satisfied. Thanks!” because the former mentions
user experience and functionality of the product
while the latter has emotional statements only.
Previous work approximates the ground truth of
helpfulness from users’ votes using “X of Y ap-
proach”: if X of Y users think a review is help-
ful, then the helpfulness score of the review is
the ratio X/Y . However, not many reviews have
statistically abundant votes, i.e., a very small Y .
Fewer than 20% of the reviews in Amazon Review
Dataset (McAuley and Leskovec, 2013) have at
least 5 votes (Table 1) while only 0.44% have 100+
votes. In addition, the review voting itself may be
biased (Danescu-Niculescu-Mizil et al., 2009; Cao
et al., 2011). Therefore, we proactively recruited
human annotators and let them score the helpful-
ness of reviews in our dataset.
We model the problem of predicting review
helpfulness score as a regression problem. Ex-
perimental results show that it is feasible to use
text-only features to accurately predict helpful-
ness scores. The two semantic features signifi-
cantly outperform baseline features used in previ-
ous work. In cross-category test, the two semantic
features show good transferability. To interpret the
models, we analyze the semantic features and find
that Psychological Process plays an important role
in review text helpfulness. Words reflecting think-
ing and understanding are more related to helpful
reviews while emotional words are not. Lastly, we
validate the models trained on “X of Y approach”
data on human annotated data and achieve highly
correlated prediction.
</bodyText>
<sectionHeader confidence="0.993728" genericHeader="introduction">
2 Dataset
</sectionHeader>
<bodyText confidence="0.9991458">
Two subsets of reviews are constructed from Ama-
zon Review Dataset (McAuley and Leskovec,
2013), which includes nearly 35 million reviews
from Amazon.com between 1995 and 2013. A
subset of 696,696 reviews from 4 categories:
Books, Home (home and kitchen), Outdoors and
Electronics, are chosen in this research. For each
category, we select the top 100 products with the
most reviews and then include all reviews related
to the selected products for analysis. Each review
comes with users’ helpfulness votes and hence
helpfulness score can be approximated using “X
of Y approach.” Finally, 115,880 reviews, each of
which has at least 5 votes, form the automatic la-
beled dataset (Table 1).
</bodyText>
<tableCaption confidence="0.99895">
Table 1: Number of Reviews for Each Category
</tableCaption>
<table confidence="0.997937125">
Category Total number Number of reviews
of reviews with at least 5 votes, se-
lected for experiments
Books 391,666 81,014 (20.7%)
Home 116,194 13,331 (11.5%)
Outdoors 52,838 6,158 (11.7%)
Electronics 135,998 15,377 (11.3%)
Overall 696,696 115,880 (16.6%)
</table>
<bodyText confidence="0.999964333333333">
In addition, we also create the human labeled
dataset. As mentioned earlier, the X of Y ap-
proach may not be a good approximation to help-
fulness. A better option is human scoring. We
randomly select 400 reviews outside of the au-
tomatic labeled dataset, 100 from each category.
Eight students annotated these reviews in a fash-
ion similar to that in (Bard et al., 1996) by as-
signing real-value scores (∈ [0,100]) to each re-
view. Review text was the only information given
to them. The average helpfulness score of all
valid annotations is used as the ground truth for
each review. We have released the human annota-
tion data at https://sites.google.com/
site/forrestbao/acl_data.tar.bz2.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999974">
Driven by the hypothesis that helpfulness is an un-
derlying feature of text itself, we consider text-
based features only. Features used in previous re-
lated work, namely Structure (STR) (Kim et al.,
2006; Xiong and Litman, 2011), Unigram (Kim et
al., 2006; Xiong and Litman, 2011; Agarwal et al.,
2011) and GALC emotion (Martin and Pu, 2014),
are considered as baselines.
We then introduce two semantic features LIWC
and General Inquirer (INQUIRER) for easy map-
ping from text to human sense, including emo-
tions, writing styles, etc. Our rationale for the
two semantic features is that a helpful review in-
cludes opinions, analyses, emotions and personal
experiences, etc. These two features have been
proven effective in other semantic analysis tasks
and hence we are here giving them a try for study-
ing review helpfulness. We leave the study of us-
ing more sophisticated features like syntactic and
discourse representations to future work. All fea-
tures except UGR are independent of training data.
STR Following the (Xiong and Litman, 2011),
we use the following structural features: total
number of tokens, total number of sentences, av-
erage length of sentences, number of exclamation
marks, and the percentage of question sentences.
</bodyText>
<page confidence="0.996577">
39
</page>
<bodyText confidence="0.992468382352941">
UGR Unigram feature has been demonstrated
as a very reliable feature for review helpfulness
prediction in previous work. We build a vocab-
ulary with all stopwords and non-frequent words
(df &lt; 3) removed. Each review is represented by
the vocabulary with tf − idf weighting for each
appeared term.
GALC (Geneva Affect Label Coder) (Scherer,
2005) proposes to recognize 36 effective states
commonly distinguished by words. Similar to
(Martin and Pu, 2014), we construct a feature
vector with the number of occurrences of each
emotion plus one additional dimension for non-
emotional words.
LIWC (Linguistic Inquiry and Word Count)
(Pennebaker et al., 2007) is a dictionary which
helps users to determine the degree that any text
uses positive or negative emotions, self-references
and other language dimensions. Each word in
LIWC is assigned 1 or 0 for each language dimen-
sion. For each review, we sum up the values of all
words for each dimension. Eventually each review
is represented by a histogram of language dimen-
sions. We employ the LIWC2007 English dictio-
nary which contains 4,553 words with 64 dimen-
sions in our experiments.
INQUIRER General Inquirer (Stone et al.,
1962) is a dictionary in which words are grouped
in categories. It is basically a mapping tool which
maps each word to some semantic tags, e.g., ab-
surd is mapped to tags NEG and VICE. The dic-
tionary contains 182 categories and a total of 7,444
words. Like for LIWC representation, we compute
the histogram of categories for each review.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.996002">
Up to this point, we are very interested in first
whether a prediction model learned for one cat-
egory can be generalized to a new category, and
second what elements make a review helpful. In
other words, we want to know the robustness of
our approach and the underlying reasons.
In this section we will evaluate the effectiveness
of each of the features as well as the combination
of them. For convenience, we use FusionSemantic
to denote the combination of GALC, LIWC and
INQUIRER, and FusionAll to denote the combi-
nation of all features. Because STR and UGR are
widely used in previous work, we use them as two
baselines. GALC has been introduced for this task
as an emotion feature before, so we use it as the
third baseline. STR, URG and GALC are used as
3 baselines. For predicting helpfulness scores, we
use SVM regressor with RBF kernel provided by
LibSVM (Chang and Lin, 2011).
Two kinds of labels are used: automatic labels
obtained in “X of Y approach” from votes, and
human labels made by human annotators. Per-
formance is evaluated by Root Mean Square Er-
ror (RMSE) and Pearson’s correlation coefficients.
Ten-fold cross-validation is performed for all ex-
periments.
</bodyText>
<subsectionHeader confidence="0.906361">
4.1 Results using Automatic Labels
</subsectionHeader>
<bodyText confidence="0.999921333333333">
Before studying the transferability of models, we
first need to make sure that models work well on
reviews of products of the same category.
</bodyText>
<sectionHeader confidence="0.717874" genericHeader="evaluation">
4.1.1 RMSE
</sectionHeader>
<bodyText confidence="0.995017571428571">
RMSE and correlation coefficient using automatic
labels are given in Table 2 and Table 3 respec-
tively. Each row corresponds to the model trained
by a feature or a combination of features, while
each column corresponds to one product category.
The lowest RMSE achieved using every single fea-
ture in each category is marked in bold.
The two newly employed semantic features,
LIWC and INQUIRER, have 8% lower RMSE
on average than UGR, the best baseline feature.
FusionAll has the best overall RMSE, ranging
from 0.200 to 0.265. FusionSemantic has the sec-
ond best performance on average. It achieves the
lowest RMSE in Books category.
</bodyText>
<tableCaption confidence="0.988122">
Table 2: RMSE (the lower the better) using auto-
matic labels
</tableCaption>
<table confidence="0.999872375">
Books Home Outdoors Electro. Average
STR 0.239 0.289 0.314 0.307 0.287
UGR 0.242 0.260 0.284 0.286 0.268
GALC 0.266 0.290 0.310 0.308 0.365
LIWC 0.188 0.256 0.279 0.278 0.250
INQUIRER 0.193 0.248 0.274 0.273 0.247
FusionSemantic 0.187 0.248 0.272 0.268 0.244
FusionAll 0.200 0.247 0.261 0.265 0.243
</table>
<tableCaption confidence="0.8887615">
Table 3: Correlation coefficients (the higher the
better) using automatic labels. All correlations are
</tableCaption>
<table confidence="0.984345">
highly significant, with p &lt; 0.001.
Books Home Outdoors Electronics
STR 0.500 0.280 0.333 0.351
UGR 0.507 0.467 0.458 0.471
GALC 0.239 0.216 0.255 0.274
LIWC 0.742 0.439 0.424 0.475
INQUIRER 0.720 0.487 0.455 0.498
FusionSemantic 0.744 0.490 0.467 0.527
FusionAll 0.682 0.525 0.535 0.539
</table>
<subsubsectionHeader confidence="0.918391">
4.1.2 Correlation Coefficient
</subsubsectionHeader>
<bodyText confidence="0.991504">
In line with RMSE measurements, the seman-
tic feature based models outperform the baseline
</bodyText>
<page confidence="0.996333">
40
</page>
<bodyText confidence="0.999930125">
features in terms of correlation coefficient (Ta-
ble 3). In each category, the highest correla-
tion coefficient is achieved by using LIWC or
INQUIRER, with only one exception (Outdoors).
The two fusion models further improve the re-
sults. FusionSemantzc has the highest coefficients
in Books category while FusionAll has the highest
coefficients in other 3 categories.
</bodyText>
<subsectionHeader confidence="0.99615">
4.2 Cross Category Test
</subsectionHeader>
<bodyText confidence="0.999926625">
One motivation of introducing semantic features
is that, unlike UGR which is category-dependent,
they can be more transferable. To validate the
transferability of semantic features, we perform
cross category test by using the model trained from
one category to predict the helpfulness scores of
reviews in other categories. GALC is excluded in
this analysis due to its poor performance earlier.
</bodyText>
<figureCaption confidence="0.9329305">
Figure 1: Normalized cross-category correlation
coefficients
</figureCaption>
<bodyText confidence="0.9976927">
training category, and thus the model is fully trans-
ferable.
Results on transferrability are visualized in Fig-
ure 1 with same-category correlation coefficients
ignored as they are always 1. Correlation coef-
ficients of 4 features are clustered for each pair
of training and testing categories and are color-
coded.
It is shown that INQUIRER and STR are two
best features in cross category test, leading in most
of the category pairs. LIWC follows, achieving at
least 70% of the same-category correlation coeffi-
cients in most cases. The UGR feature, however,
performs poorly in this test. In most cases, the cor-
relation coefficients have been halved, compared
with same-category results.
According to the results, we can conclude that
semantic features are accurate and transferable,
UGR is accurate but is not transferable, and STR
is transferable but not accurate enough (Figure 2).
</bodyText>
<figure confidence="0.808457">
Accurate Transferable
</figure>
<figureCaption confidence="0.9433445">
Figure 2: Classification of features based on ex-
perimental results
</figureCaption>
<figure confidence="0.991422523809524">
INQUIRER
LIWC
STR
UGR
1.0
0.8
0.6
Electronics
0.4
Outdoor
0.2
0.0
Home
Books
Home
Books
Outdoor
Electronics
LIWC
UGR and STR
INQUIRER
</figure>
<bodyText confidence="0.99923285">
Model transferability from Category A to Cate-
gory B cannot be measured simply by the perfor-
mance when using A as the training set and B as
the test set. Instead, it should be compared rela-
tively with the performance when using A as both
the training and test sets. There are 4 categories
in our dataset, and the performances on the 4 cate-
gories vary (Tables 2 and 3). In order to provide a
fair comparison, we normalize cross-category cor-
relation coefficients by the corresponding same-
category ones, i.e., cross-category correlation co-
efficient / correlation coefficient on training cate-
gory. For example, the 3 cross-category correla-
tion coefficients of using Books category as train-
ing set are all normalized by the correlation coef-
ficient when using Books as both training and test
sets earlier. A normalized correlation coefficient
of 0 means the prediction on the test category is
random, and thus the model has no transferabil-
ity, while 1 means as accurate as predicting on the
</bodyText>
<subsectionHeader confidence="0.997863">
4.3 What Makes a Review Helpful: A
Semantic Interpretation
</subsectionHeader>
<bodyText confidence="0.998710882352941">
LIWC and INQUIRER not only have better per-
formances than previously used features but also
provide us a good semantic interpretation to what
makes a review helpful. We analyze the correla-
tion coefficients between helpfulness and each lan-
guage dimension in the two dictionaries. The top 5
language dimensions that are mostly correlated to
helpfulness from LIWC and INQUIRER are given
in Figure 3.
The top 5 dimensions from LIWC are: Rel-
ativ (Relativity), Time, Incl (Inclusive), Posemo
(Positive Emotion), and Cogmech (Cognitive Pro-
cesses). All of them belong to Psychological Pro-
cesses categories in LIWC, indicating that people
are more thoughtful when writing a helpful review.
The top 5 dimensions from INQUIRER are:
Vary, Begin, Exert, Vice and Undrst. Words with
</bodyText>
<page confidence="0.99869">
41
</page>
<bodyText confidence="0.9991856">
Vary, Begin or Exert tags belong to process or
change words, such as start, happen and break.
Vice tag contains words indicating an assess-
ment of moral disapproval or misfortune.Undrst
(Understated) tag contains words indicating de-
emphasis and caution in these realms, which often
reflects the lack of emotional expressiveness. Ac-
cordingly, we can infer that consumers perfer crit-
ical reviews with personal experience and a lack
of emotion.
</bodyText>
<figureCaption confidence="0.640526666666667">
Figure 3: Language dimensions with highest cor-
relation coefficients. Top: LIWC’s; Bottom: IN-
QUIRER’s.
</figureCaption>
<bodyText confidence="0.99747725">
The discovery that helpful reviews are less emo-
tional is consistent with the weak performance of
GALC (Tables 2, 3 and 4), which is emotion fo-
cused. However, we notice that one of the top
5 dimensions in LIWC, PosEmo, is an emotional
feature. This is partially because some words ap-
pear in both emotional and rational expressions,
such as LIWC PosEmo words: love, nice, sweet.
For example, the sentence “I used to love linksys, but
my experience with several of their products makes me se-
riously think that their quality is suspect” is a rational
statement. But the word “love” appears in it.
</bodyText>
<subsectionHeader confidence="0.999565">
4.4 Prediction Results on Human Labels
</subsectionHeader>
<bodyText confidence="0.999879333333333">
A better ground truth for helpfulness is human rat-
ing. We further evaluate the prediction models on
human annotated data to evaluate whether the pre-
dictions indeed align with human perceptions of
review helpfulness by reading text only.
The model we built indeed aligns with human
perceptions of review helpfulness when text is the
only data. Table 4 shows the correlation coef-
ficients between the predicted scores and human
annotated scores. INQUIRER is the best feature,
leading in 3 of 4 categories. It is followed by UGR
and LIWC, which show comparable results.
</bodyText>
<tableCaption confidence="0.995911">
Table 4: Correlation coefficients between pre-
dicted scores and human annotation, *: p &lt; 0.001.
</tableCaption>
<table confidence="0.99948125">
Books Home Outdoors Electronics
STR 0.539* 0.522* 0.471* 0.635*
UGR 0.607* 0.560* 0.579* 0.626*
GALC 0.214 0.405* 0.156 0.418*
LIWC 0.524* 0.553* 0.517* 0.702*
INQUIRER 0.620* 0.662* 0.620* 0.676*
FusionSemantic 0.556* 0.680* 0.569* 0.603*
FusionAll 0.610* 0.801* 0.698* 0.768*
</table>
<bodyText confidence="0.991917642857143">
For FusionAll models, correlation coefficients
are about or over 0.7 in 3 of 4 categories, indi-
cating the successful prediction. The only excep-
tion is on Books category. We notice that reviews
in Books are more subjective. Therefore, in Books
reviews, consumers are more influenced by factors
outside of the text, e.g., personal preference on the
book. In this case, the approximate scores used in
training may not reflect the real text helpfulness.
This observation echoes with our speculation that
the “X of Y approach” may not always be a good
approximation for helpfulness due to the subjec-
tivity. We will leave the analysis to this as a future
work.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999934529411765">
In this paper, we formulate a new problem which
is an important component of many tasks about
online product reviews: predicting the helpfulness
of review text. We hypothesize that helpfulness
is an underlying property of text and isolate help-
fulness prediction from its outer layer problems,
such as review ranking. Introducing two seman-
tic features, which have been shown effective in
other NLP tasks, we achieve more accurate and
transferable prediction than using features used in
existing related work. The ground truth is pro-
vided by votes on massive Amazon product re-
views. We further explore a semantic interpreta-
tion to reviews’ helpfulness that helpful reviews
exhibit more reasoning and experience and less
emotion. The results are further validated on hu-
man scoring to helpfulness.
</bodyText>
<page confidence="0.998747">
42
</page>
<sectionHeader confidence="0.989709" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999063372727273">
[Agarwal et al.2011] Deepak Agarwal, Bee-Chung
Chen, and Bo Pang. 2011. Personalized recom-
mendation of user comments via factor models. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
pages 571–582, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Agichtein et al.2008] Eugene Agichtein, Carlos
Castillo, Debora Donato, Aristides Gionis, and
Gilad Mishne. 2008. Finding high-quality content
in social media. In Proceedings of the 2008 Interna-
tional Conference on Web Search and Data Mining,
WSDM ’08, pages 183–194. ACM.
[Bard et al.1996] Ellen Gurman Bard, Dan Robertson,
and Antonella Sorace. 1996. Magnitude estimation
of linguistic acceptability. Language, 72(1):pp. 32–
68.
[Cao et al.2011] Qing Cao, Wenjing Duan, and Qiwei
Gan. 2011. Exploring determinants of voting for
the ”helpfulness” of online user reviews: A text min-
ing approach. Decis. Support Syst., 50(2):511–521,
January.
[Chang and Lin2011] Chih-Chung Chang and Chih-
Jen Lin. 2011. LIBSVM: A library for sup-
port vector machines. ACM Transactions on In-
telligent Systems and Technology, 2:27:1–27:27.
Software available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
[Danescu-Niculescu-Mizil et al.2009] Cristian
Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How
opinions are received by online communities: A
case study on amazon.com helpfulness votes. In
Proceedings of the 18th International Conference
on World Wide Web, WWW ’09, pages 141–150,
New York, NY, USA. ACM.
[Duan et al.2008] Wenjing Duan, Bin Gu, and An-
drew B. Whinston. 2008. The dynamics of on-
line word-of-mouth and product sales-an empirical
investigation of the movie industry. Journal of Re-
tailing, 84:233242.
[Ghose and Ipeirotis2011] A. Ghose and P.G. Ipeirotis.
2011. Estimating the helpfulness and economic im-
pact of product reviews: Mining text and reviewer
characteristics. volume 23, pages 1498–1512, Oct.
[Hu and Liu2004] Minqing Hu and Bing Liu. 2004.
Mining opinion features in customer reviews. In
Proceedings of the 19th National Conference on
Artifical Intelligence, AAAI’04, pages 755–760.
AAAI Press.
[Kim et al.2006] Soo-Min Kim, Patrick Pantel, Tim
Chklovski, and Marco Pennacchiotti. 2006. Au-
tomatically assessing review helpfulness. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’06,
pages 423–430, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Liu et al.2008] Yang Liu, Xiangji Huang, Aijun An,
and Xiaohui Yu. 2008. Modeling and predicting
the helpfulness of online reviews. In Proceedings of
the 2008 Eighth IEEE International Conference on
Data Mining, ICDM ’08, pages 443–452, Washing-
ton, DC, USA. IEEE Computer Society.
[Martin and Pu2014] Lionel Martin and Pearl Pu. 2014.
Prediction of helpful reviews using emotions extrac-
tion. In Twenty-Eighth AAAI Conference on Artifi-
cial Intelligence, AAAI ’14.
[McAuley and Leskovec2013] Julian McAuley and
Jure Leskovec. 2013. Hidden factors and hidden
topics: Understanding rating dimensions with
review text. In Proceedings of the 7th ACM Confer-
ence on Recommender Systems, RecSys ’13, pages
165–172, New York, NY, USA. ACM.
[Mudambi and Schuff2010] Susan M. Mudambi and
David Schuff. 2010. What makes a helpful on-
line review? a study of customer reviews on ama-
zon.com. MIS Quarterly, pages 185–200.
[O’Mahony and Smyth2010] Michael P. O’Mahony
and Barry Smyth. 2010. Using readability tests
to predict helpful product reviews. In Adaptivity,
Personalization and Fusion of Heterogeneous Infor-
mation, RIAO ’10, pages 164–167, Paris, France,
France. LE CENTRE DE HAUTES ETUDES
INTERNATIONALES D’INFORMATIQUE DOC-
UMENTAIRE.
[Pasternack and Roth2011] Jeff Pasternack and Dan
Roth. 2011. Making better informed trust deci-
sions with generalized fact-finding. In Proceedings
of the Twenty-Second International Joint Conference
on Artificial Intelligence - Volume Three, IJCAI’11,
pages 2324–2329. AAAI Press.
[Pennebaker et al.2007] J. W. Pennebaker, Roger J.
Booth, and M. E. Francis. 2007. Linguistic inquiry
and word count: Liwc.
[Scherer2005] Klaus R. Scherer. 2005. What are emo-
tions? and how can they be measured? Social Sci-
ence Information, 44(4):695–729.
[Stone et al.1962] P. J. Stone, R. F. Bales, J. Z. Namen-
wirth, and D. M. Ogilvie. 1962. The general in-
quirer: a computer system for content analysis and
retrieval based on the sentence as a unit of informa-
tion. In Behavioral Science, pages 484–498.
[Xiong and Litman2011] Wenting Xiong and Diane
Litman. 2011. Automatically predicting peer-
review helpfulness. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies:
Short Papers - Volume 2, HLT ’11, pages 502–507,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.995587">
43
</page>
<reference confidence="0.994899909090909">
[Xiong and Litman2014] Wenting Xiong and Diane
Litman. 2014. Empirical analysis of exploiting
review helpfulness for extractive summarization of
online reviews. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 1985–1995.
[Yang and Nenkova2014] Yinfei Yang and Ani
Nenkova. 2014. Detecting information-dense
texts in multiple news domains. In Proceedings
of Twenty-Eighth AAAI Conference on Artificial
Intelligence.
</reference>
<page confidence="0.99927">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.426108">
<title confidence="0.99802">Semantic Analysis and Helpfulness Prediction of for Online Product Reviews</title>
<author confidence="0.862604">Yinfei</author>
<affiliation confidence="0.79291">Amazon</affiliation>
<address confidence="0.991091">Seattle, WA</address>
<email confidence="0.985172">yangyin7@gmail.com</email>
<author confidence="0.775629">Minghui</author>
<affiliation confidence="0.759383">Alibaba</affiliation>
<address confidence="0.95071">Hangzhou, China</address>
<email confidence="0.998302">minghuiqiu@gmail.com</email>
<abstract confidence="0.996411185185185">Predicting the helpfulness of product reviews is a key component of many ecommerce tasks such as review ranking and recommendation. However, previous work mixed review helpfulness prediction with those outer layer tasks. Using nontext features, it leads to less transferable models. This paper solves the problem from a new angle by hypothesizing that helpfulness is an internal property of text. Purely using review text, we isolate review helpfulness prediction from its outer layer tasks, employ two interpretable semantic features, and use human scoring of helpfulness as ground truth. Experimental results show that the two semantic features can accurately predict helpfulness scores and greatly improve the performance compared with using features previously used. Cross-category test further shows the models trained with semantic features are easier to be generalized to reviews of different product categories. The models we built are also highly interpretable and align well with human annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Deepak Agarwal</author>
<author>Bee-Chung Chen</author>
<author>Bo Pang</author>
</authors>
<title>Personalized recommendation of user comments via factor models.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>571--582</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Agarwal et al.2011]</marker>
<rawString>Deepak Agarwal, Bee-Chung Chen, and Bo Pang. 2011. Personalized recommendation of user comments via factor models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 571–582, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Gilad Mishne</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08,</booktitle>
<pages>183--194</pages>
<publisher>ACM.</publisher>
<marker>[Agichtein et al.2008]</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding high-quality content in social media. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08, pages 183–194. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Gurman Bard</author>
<author>Dan Robertson</author>
<author>Antonella Sorace</author>
</authors>
<title>Magnitude estimation of linguistic acceptability.</title>
<date>1996</date>
<journal>Language,</journal>
<volume>72</volume>
<issue>1</issue>
<pages>32--68</pages>
<marker>[Bard et al.1996]</marker>
<rawString>Ellen Gurman Bard, Dan Robertson, and Antonella Sorace. 1996. Magnitude estimation of linguistic acceptability. Language, 72(1):pp. 32– 68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Cao</author>
<author>Wenjing Duan</author>
<author>Qiwei Gan</author>
</authors>
<title>Exploring determinants of voting for the ”helpfulness” of online user reviews: A text mining approach.</title>
<date>2011</date>
<journal>Decis. Support Syst.,</journal>
<volume>50</volume>
<issue>2</issue>
<marker>[Cao et al.2011]</marker>
<rawString>Qing Cao, Wenjing Duan, and Qiwei Gan. 2011. Exploring determinants of voting for the ”helpfulness” of online user reviews: A text mining approach. Decis. Support Syst., 50(2):511–521, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>ChihJen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</note>
<marker>[Chang and Lin2011]</marker>
<rawString>Chih-Chung Chang and ChihJen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Danescu-Niculescu-Mizil</author>
<author>Gueorgi Kossinets</author>
<author>Jon Kleinberg</author>
<author>Lillian Lee</author>
</authors>
<title>How opinions are received by online communities: A case study on amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th International Conference on World Wide Web, WWW ’09,</booktitle>
<pages>141--150</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>[Danescu-Niculescu-Mizil et al.2009]</marker>
<rawString>Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinberg, and Lillian Lee. 2009. How opinions are received by online communities: A case study on amazon.com helpfulness votes. In Proceedings of the 18th International Conference on World Wide Web, WWW ’09, pages 141–150, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenjing Duan</author>
<author>Bin Gu</author>
<author>Andrew B Whinston</author>
</authors>
<title>The dynamics of online word-of-mouth and product sales-an empirical investigation of the movie industry.</title>
<date>2008</date>
<journal>Journal of Retailing,</journal>
<pages>84--233242</pages>
<marker>[Duan et al.2008]</marker>
<rawString>Wenjing Duan, Bin Gu, and Andrew B. Whinston. 2008. The dynamics of online word-of-mouth and product sales-an empirical investigation of the movie industry. Journal of Retailing, 84:233242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghose</author>
<author>P G Ipeirotis</author>
</authors>
<title>Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics.</title>
<date>2011</date>
<volume>23</volume>
<pages>1498--1512</pages>
<marker>[Ghose and Ipeirotis2011]</marker>
<rawString>A. Ghose and P.G. Ipeirotis. 2011. Estimating the helpfulness and economic impact of product reviews: Mining text and reviewer characteristics. volume 23, pages 1498–1512, Oct.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 19th National Conference on Artifical Intelligence, AAAI’04,</booktitle>
<pages>755--760</pages>
<publisher>AAAI Press.</publisher>
<marker>[Hu and Liu2004]</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of the 19th National Conference on Artifical Intelligence, AAAI’04, pages 755–760. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Patrick Pantel</author>
<author>Tim Chklovski</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Kim et al.2006]</marker>
<rawString>Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. 2006. Automatically assessing review helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 423–430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Xiangji Huang</author>
<author>Aijun An</author>
<author>Xiaohui Yu</author>
</authors>
<title>Modeling and predicting the helpfulness of online reviews.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM ’08,</booktitle>
<pages>443--452</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker>[Liu et al.2008]</marker>
<rawString>Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online reviews. In Proceedings of the 2008 Eighth IEEE International Conference on Data Mining, ICDM ’08, pages 443–452, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lionel Martin</author>
<author>Pearl Pu</author>
</authors>
<title>Prediction of helpful reviews using emotions extraction.</title>
<date>2014</date>
<booktitle>In Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI ’14.</booktitle>
<marker>[Martin and Pu2014]</marker>
<rawString>Lionel Martin and Pearl Pu. 2014. Prediction of helpful reviews using emotions extraction. In Twenty-Eighth AAAI Conference on Artificial Intelligence, AAAI ’14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian McAuley</author>
<author>Jure Leskovec</author>
</authors>
<title>Hidden factors and hidden topics: Understanding rating dimensions with review text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13,</booktitle>
<pages>165--172</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>[McAuley and Leskovec2013]</marker>
<rawString>Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics: Understanding rating dimensions with review text. In Proceedings of the 7th ACM Conference on Recommender Systems, RecSys ’13, pages 165–172, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan M Mudambi</author>
<author>David Schuff</author>
</authors>
<title>What makes a helpful online review? a study of customer reviews on amazon.com.</title>
<date>2010</date>
<journal>MIS Quarterly,</journal>
<pages>185--200</pages>
<marker>[Mudambi and Schuff2010]</marker>
<rawString>Susan M. Mudambi and David Schuff. 2010. What makes a helpful online review? a study of customer reviews on amazon.com. MIS Quarterly, pages 185–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P O’Mahony</author>
<author>Barry Smyth</author>
</authors>
<title>Using readability tests to predict helpful product reviews. In Adaptivity, Personalization and Fusion of Heterogeneous Information,</title>
<date>2010</date>
<journal>RIAO</journal>
<booktitle>LE CENTRE DE HAUTES ETUDES INTERNATIONALES D’INFORMATIQUE DOCUMENTAIRE.</booktitle>
<volume>10</volume>
<pages>164--167</pages>
<location>Paris, France, France.</location>
<marker>[O’Mahony and Smyth2010]</marker>
<rawString>Michael P. O’Mahony and Barry Smyth. 2010. Using readability tests to predict helpful product reviews. In Adaptivity, Personalization and Fusion of Heterogeneous Information, RIAO ’10, pages 164–167, Paris, France, France. LE CENTRE DE HAUTES ETUDES INTERNATIONALES D’INFORMATIQUE DOCUMENTAIRE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Pasternack</author>
<author>Dan Roth</author>
</authors>
<title>Making better informed trust decisions with generalized fact-finding.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Three, IJCAI’11,</booktitle>
<pages>2324--2329</pages>
<publisher>AAAI Press.</publisher>
<marker>[Pasternack and Roth2011]</marker>
<rawString>Jeff Pasternack and Dan Roth. 2011. Making better informed trust decisions with generalized fact-finding. In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Three, IJCAI’11, pages 2324–2329. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>Roger J Booth</author>
<author>M E Francis</author>
</authors>
<title>Linguistic inquiry and word count: Liwc.</title>
<date>2007</date>
<marker>[Pennebaker et al.2007]</marker>
<rawString>J. W. Pennebaker, Roger J. Booth, and M. E. Francis. 2007. Linguistic inquiry and word count: Liwc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus R Scherer</author>
</authors>
<title>What are emotions? and how can they be measured?</title>
<date>2005</date>
<journal>Social Science Information,</journal>
<volume>44</volume>
<issue>4</issue>
<marker>[Scherer2005]</marker>
<rawString>Klaus R. Scherer. 2005. What are emotions? and how can they be measured? Social Science Information, 44(4):695–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>R F Bales</author>
<author>J Z Namenwirth</author>
<author>D M Ogilvie</author>
</authors>
<title>The general inquirer: a computer system for content analysis and retrieval based on the sentence as a unit of information.</title>
<date>1962</date>
<journal>In Behavioral Science,</journal>
<pages>484--498</pages>
<marker>[Stone et al.1962]</marker>
<rawString>P. J. Stone, R. F. Bales, J. Z. Namenwirth, and D. M. Ogilvie. 1962. The general inquirer: a computer system for content analysis and retrieval based on the sentence as a unit of information. In Behavioral Science, pages 484–498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane Litman</author>
</authors>
<title>Automatically predicting peerreview helpfulness.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>502--507</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>[Xiong and Litman2011]</marker>
<rawString>Wenting Xiong and Diane Litman. 2011. Automatically predicting peerreview helpfulness. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 502–507, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenting Xiong</author>
<author>Diane Litman</author>
</authors>
<title>Empirical analysis of exploiting review helpfulness for extractive summarization of online reviews.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1985--1995</pages>
<marker>[Xiong and Litman2014]</marker>
<rawString>Wenting Xiong and Diane Litman. 2014. Empirical analysis of exploiting review helpfulness for extractive summarization of online reviews. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1985–1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yinfei Yang</author>
<author>Ani Nenkova</author>
</authors>
<title>Detecting information-dense texts in multiple news domains.</title>
<date>2014</date>
<booktitle>In Proceedings of Twenty-Eighth AAAI Conference on Artificial Intelligence.</booktitle>
<marker>[Yang and Nenkova2014]</marker>
<rawString>Yinfei Yang and Ani Nenkova. 2014. Detecting information-dense texts in multiple news domains. In Proceedings of Twenty-Eighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>