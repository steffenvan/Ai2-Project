<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000098">
<title confidence="0.993846">
I’m a Belieber:
Social Roles via Self-identification and Conceptual Attributes
</title>
<author confidence="0.9970635">
Charley Beller, Rebecca Knowles, Craig Harman
Shane Bergsma†, Margaret Mitchell$, Benjamin Van Durme
</author>
<affiliation confidence="0.9508715">
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD USA
†University of Saskatchewan, Saskatoon, Saskatchewan Canada
$Microsoft Research, Redmond, Washington USA
</affiliation>
<email confidence="0.97586">
charleybeller@jhu.edu, rknowles@jhu.edu, craig@craigharman.net,
shane.a.bergsma@gmail.com, memitc@microsoft.com, vandurme@cs.jhu.edu
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999147333333333">
Motivated by work predicting coarse-
grained author categories in social me-
dia, such as gender or political preference,
we explore whether Twitter contains infor-
mation to support the prediction of fine-
grained categories, or social roles. We
find that the simple self-identification pat-
tern “I am a ” supports significantly
richer classification than previously ex-
plored, successfully retrieving a variety of
fine-grained roles. For a given role (e.g.,
writer), we can further identify character-
istic attributes using a simple possessive
construction (e.g., writer’s ). Tweets
that incorporate the attribute terms in first
person possessives (my ) are confirmed
to be an indicator that the author holds the
associated social role.
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995701754386">
With the rise of social media, researchers have
sought to induce models for predicting latent au-
thor attributes such as gender, age, and politi-
cal preferences (Garera and Yarowsky, 2009; Rao
et al., 2010; Burger et al., 2011; Van Durme,
2012b; Zamal et al., 2012). Such models are
clearly in line with the goals of both computa-
tional advertising (Wortman, 2008) and the grow-
ing area of computational social science (Conover
et al., 2011; Nguyen et al., 2011; Paul and Dredze,
2011; Pennacchiotti and Popescu, 2011; Moham-
mad et al., 2013) where big data and computa-
tion supplement methods based on, e.g., direct hu-
man surveys. For example, Eisenstein et al. (2010)
demonstrated a model that predicted where an au-
thor was located in order to analyze regional dis-
tinctions in communication. While some users ex-
plicitly share their GPS coordinates through their
Twitter clients, having a larger collection of au-
tomatically identified users within a region was
preferable even though the predictions for any
given user were uncertain.
We show that media such as Twitter can sup-
port classification that is more fine-grained than
gender or general location. Predicting social roles
such as doctor, teacher, vegetarian, christian,
may open the door to large-scale passive surveys
of public discourse that dwarf what has been pre-
viously available to social scientists. For exam-
ple, work on tracking the spread of flu infections
across Twitter (Lamb et al., 2013) might be en-
hanced with a factor based on aggregate predic-
tions of author occupation.
We present two studies showing that first-
person social content (tweets) contains intuitive
signals for such fine-grained roles. We argue that
non-trivial classifiers may be constructed based
purely on leveraging simple linguistic patterns.
These baselines suggest a wide range of author
categories to be explored further in future work.
Study 1 In the first study, we seek to determine
whether such a signal exists in self-identification:
we rely on variants of a single pattern, “I am a ”,
to bootstrap data for training balanced-class binary
classifiers using unigrams observed in tweet con-
tent. As compared to prior research that required
actively polling users for ground truth in order to
construct predictive models for demographic in-
formation (Kosinski et al., 2013), we demonstrate
that some users specify such properties publicly
through direct natural language.
Many of the resultant models show intuitive
strongly-weighted features, such as a writer be-
ing likely to tweet about a story, or an ath-
lete discussing a game. This demonstrates self-
identification as a viable signal in building predic-
tive models of social roles.
</bodyText>
<page confidence="0.978444">
181
</page>
<bodyText confidence="0.6580715">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181–186,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.840002">
Role Tweet
</subsectionHeader>
<bodyText confidence="0.7100655">
artist I’m an Artist the last of a dying breed
belieber @justinbieber I will support you in ev-
erything you do because I am a belieber
please follow me I love you 30
</bodyText>
<note confidence="0.560722">
vegetarian So glad I’m a vegetarian.
</note>
<tableCaption confidence="0.999018">
Table 1: Examples of self-identifying tweets.
</tableCaption>
<table confidence="0.999834">
# Role # Role # Role
29,924 little 5,694 man 564 champion
21,822 big ... ... 559 teacher
18,957 good 4,007 belieber 556 writer
13,069 huge 3,997 celebrity 556 awful
13,020 bit 3,737 virgin ... ...
12,816 fan 3,682 pretty 100 cashier
10,832 bad ... ... 100 bro
10,604 girl 2,915 woman ... ...
9,981 very 2,851 beast 10 linguist
... ... ... ... ... ...
</table>
<tableCaption confidence="0.985527">
Table 2: Number of self-identifying users per “role”. While
rich in interesting labels, cases such as very highlight the pur-
poseful simplicity of the current approach.
</tableCaption>
<bodyText confidence="0.9999808">
Study 2 In the second study we exploit a com-
plementary signal based on characteristic con-
ceptual attributes of a social role, or concept
class (Schubert, 2002; Almuhareb and Poesio,
2004; Pas¸ca and Van Durme, 2008). We identify
typical attributes of a given social role by collect-
ing terms in the Google n-gram corpus that occur
frequently in a possessive construction with that
role. For example, with the role doctor we extract
terms matching the simple pattern “doctor’s ”.
</bodyText>
<sectionHeader confidence="0.742337" genericHeader="introduction">
2 Self-identification
</sectionHeader>
<bodyText confidence="0.999910176470588">
All role-representative users were drawn from
the free public 1% sample of the Twitter Fire-
hose, over the period 2011-2013, from the sub-
set that selected English as their native language
(85,387,204 unique users). To identify users of
a particular role, we performed a case-agnostic
search of variants of a single pattern: I am a(n)
, and I’m a(n) , where all single tokens filling
the slot were taken as evidence of the author self-
reporting for the given “role”. Example tweets can
be seen in Table 1, examples of frequency per role
in Table 2. This resulted in 63,858 unique roles
identified, of which 44,260 appeared only once.1
We manually selected a set of roles for fur-
ther exploration, aiming for a diverse sample
across: occupation (e.g., doctor, teacher), family
(mother), disposition (pessimist), religion (chris-
</bodyText>
<footnote confidence="0.9757935">
1Future work should consider identifying multi-word role
labels (e.g., Doctor Who fan, or dog walker).
</footnote>
<figure confidence="0.434489">
Role
</figure>
<figureCaption confidence="0.99919525">
Figure 1: Success rate for querying a user. Random.0,1,2
are background draws from the population, with the mean of
those three samples drawn horizontally. Tails capture 95%
confidence intervals.
</figureCaption>
<bodyText confidence="0.9989164">
tian), and “followers” (belieber, directioner).2
We filtered users via language ID (Bergsma et al.,
2012) to better ensure English content.3
For each selected role, we randomly sampled up
to 500 unique self-reporting users and then queried
Twitter for up to 200 of their recent publicly
posted tweets.4 These tweets served as represen-
tative content for that role, with any tweet match-
ing the self-reporting patterns filtered. Three sets
of background populations were extracted based
on randomly sampling users that self-reported En-
glish (post-filtered via LID).
Twitter users are empowered to at any time
delete, rename or make private their accounts.
Any given user taken to be representative based on
a previously posted tweet may no longer be avail-
able to query on. As a hint of the sort of user stud-
ies one might explore given access to social role
prediction, we see in Figure 1 a correlation be-
tween self-reported role and the chance of an ac-
count still being publicly visible, with roles such
as belieber and directioner on the one hand, and
doctor and teacher on the other.
The authors examined the self-identifying tweet
of 20 random users per role. The accuracy of the
self-identification pattern varied across roles and
is attributable to various factors including quotes,
e.g. @StarTrek Jim, I’m a DOCTOR not a down-
load!. While these samples are small (and thus
estimates of quality come with wide variance), it
</bodyText>
<footnote confidence="0.484917428571428">
2Those that follow the music/life of the singer Justin
Bieber and the band One Direction, respectively.
3This removes users that selected English as their primary
language, used a self-identification phrase, e.g. I am a be-
lieber, but otherwise tended to communicate in non-English.
4Roughly half of the classes had less than 500 self-
reporting users in total, in those cases we used all matches.
</footnote>
<figure confidence="0.997127">
0.80
0.75
0.70
0.65
0.60
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Chance of Success
182
0 5 10 15
</figure>
<figureCaption confidence="0.999804">
Figure 2: Valid self-identifying tweets from sample of 20.
</figureCaption>
<bodyText confidence="0.998911305555555">
is noteworthy that a non-trivial number for each
were judged as actually self-identifying.
Indicative Language Most work in user clas-
sification relies on featurizing language use,
most simply through binary indicators recording
whether a user did or did not use a particular word
in a history of n tweets. To explore whether lan-
guage provides signal for future work in fine-grain
social role prediction, we constructed a set of ex-
periments, one per role, where training and test
sets were balanced between users from a random
background sample and self-reported users. Base-
line accuracy in these experiments was thus 50%.
Each training set had a target of 600 users (300
background, 300 self-identified); for those roles
with less than 300 users self-identifying, all users
were used, with an equal number background. We
used the Jerboa (Van Durme, 2012a) platform
to convert data to binary feature vectors over a un-
igram vocabulary filtered such that the minimum
frequency was 5 (across unique users). Training
and testing was done with a log-linear model via
LibLinear (Fan et al., 2008). We used the pos-
itively annotated data to form test sets, balanced
with data from the background set. Each test set
had a theoretical maximum size of 40, but for sev-
eral classes it was in the single digits (see Fig-
ure 2). Despite the varied noisiness of our simple
pattern-bootstrapped training data, and the small
size of our annotated test set, we see in Figure 3
that we are able to successfully achieve statisti-
cally significant predictions of social role for the
majority of our selected examples.
Table 3 highlights examples of language indica-
tive of role, as determined by the most positively
weighted unigrams in the classification experi-
</bodyText>
<figure confidence="0.779048">
Role
</figure>
<figureCaption confidence="0.997498">
Figure 3: Accuracy in classifying social roles.
</figureCaption>
<table confidence="0.2167646875">
Role:: Feature ( Rank)
artist morning, summer, life, most, amp, studio
atheist fuck, fucking, shit, makes, dead,..., religion19
athlete lol, game, probably, life, into,..., team9
belieber justin, justinbeiber, believe, beliebers, bieber
cheerleader cheer, best, excited, hate, mom, ..., prom16
christian lol, ..., god12, pray13, ..., bless17, ..., jesus20
dancer dance, since, hey, never, been
directioner harry, d, follow, direction, never, liam, niall
doctor sweet, oh, or, life, nothing
engineer (, then, since, may, ), test9, -17, =18
freshman summer, homework, na, ..., party19, school20
geek trying, oh, different, dead, been
grandma morning, baby, around, night, excited
hipster fucking, actually, thing, fuck, song
lawyer did, never, his, may, pretty, law, even, office
man man, away, ai, young, since
mother morning, take, fuck, fucking, trying
nurse lol, been, morning,..., night10, nursing11, shift13
optimist morning, enough, those, everything, never
poet feel, song, even, say, yo
rapper fuck, morning, lol, ..., mixtape8, songs15
singer sing, song, music, lol, never
smoker fuck, shit, fucking, since, ass, smoke, weed20
solider ai, beautiful, lol, wan, trying
sophmore summer, &gt;, ..., school11, homework12
student anything, summer, morning, since, actually
teacher teacher, morning, teach, ..., students7, ..., school20
vegetarian actually, dead, summer, oh, morning
waitress man, try, goes, hate, fat
woman lol, into, woman, morning, never
writer write, story, sweet, very, working
</table>
<tableCaption confidence="0.9911925">
Table 3: Most-positively weighted features per role, along
with select features within the top 20. Surprising mother
features come from ambigious self-identification, as seen in
tweets such as: I’m a motherf!cking starrrrr.
</tableCaption>
<bodyText confidence="0.999833833333333">
ment. These results qualitatively suggest many
roles under consideration may be teased out from a
background population by focussing on language
that follows expected use patterns. For example
the use of the term game by athletes, studio by
artists, mixtape by rappers, or jesus by Christians.
</bodyText>
<sectionHeader confidence="0.976889" genericHeader="method">
3 Characteristic Attributes
</sectionHeader>
<bodyText confidence="0.54076">
Bergsma and Van Durme (2013) showed that the
</bodyText>
<figure confidence="0.999050555555555">
writer
woman
waitress
vegetarian
teacher
student
sophomore soldier
smoker
singer
rapper
poet
pessimist
optimist
nurse
mother
man
lawyer
hipster
grandma
geek
freshman
engineer
doctor
directioner
dancer
christian
cheerleader
belieber
athlete
atheist
artist
actor
Accuracy
0.8
0.6
0.4
0.2
1.0
●
●
●
●
● ● ● ●
●
●
● ●
● ●
● ● ● ● ●
● ● ● ●
●
●
● ●
●
●
</figure>
<page confidence="0.9974">
183
</page>
<bodyText confidence="0.999938933333333">
task of mining attributes for conceptual classes can
relate straightforwardly to author attribute predic-
tion. If one views a role, in their case gender, as
two conceptual classes, male and female, then ex-
isting attribute extraction methods for third-person
content (e.g., news articles) can be cheaply used to
create a set of bootstrapping features for building
classifiers over first-person content (e.g., tweets).
For example, if we learn from news corpora that:
a man may have a wife, then a tweet saying: ...my
wife... can be taken as potential evidence of mem-
bership in the male conceptual class.
In our second study, we test whether this idea
extends to our wider set of fine-grained roles. For
example, we aimed to discover that a doctor may
have a patient, while a hairdresser may have a
salon; these properties can be expressed in first-
person content as possessives like my patient or my
salon. We approached this task by selecting target
roles from the first experiment and ranking charac-
teristic attributes for each using pointwise mutual
information (PMI) (Church and Hanks, 1990).
First, we counted all terms matching a target
social role’s possessive pattern (e.g., doctor’s )
in the web-scale n-gram corpus Google V2 (Lin
et al., 2010)5. We ranked the collected terms
by computing PMI between classes and attribute
terms. Probabilities were estimated from counts of
the class-attribute pairs along with counts match-
ing the generic possessive patterns his and
her which serve as general background cate-
gories. Following suggestions by Bergsma and
Van Durme, we manually filtered the ranked list.6
We removed attributes that were either (a) not
nominal, or (b) not indicative of the social role.
This left fewer than 30 attribute terms per role,
with many roles having fewer than 10.
We next performed a precision test to identify
potentially useful attributes in these lists. We ex-
amined tweets with a first person possessive pat-
tern for each attribute term from a small corpus
of tweets collected over a single month in 2013,
discarding those attribute terms with no positive
matches. This precision test is useful regardless
of how attribute lists are generated. The attribute
</bodyText>
<footnote confidence="0.939248125">
5In this corpus, follower-type roles like belieber and di-
rectioner are not at all prevalent. We therefore focused on
occupational and habitual roles (e.g., doctor, smoker).
6Evidence from cognitive work on memory-dependent
tasks suggests that such relevance based filtering (recogni-
tion) involves less cognitive effort than generating relevant
attributes (recall) see (Jacoby et al., 1979). Indeed, this filter-
ing step generally took less than a minute per class.
</footnote>
<bodyText confidence="0.999872428571429">
term chart, for example, had high PMI with doc-
tor; but a precision test on the phrase my chart
yielded a single tweet which referred not to a med-
ical chart but to a top ten list (prompting removal
of this attribute). Using this smaller high-precision
set of attribute terms, we collected tweets from the
Twitter Firehose over the period 2011-2013.
</bodyText>
<sectionHeader confidence="0.997013" genericHeader="method">
4 Attribute-based Classification
</sectionHeader>
<bodyText confidence="0.999961073170732">
Attribute terms are less indicative overall than
self-ID, e.g., the phrase I’m a barber is a clearer
signal than my scissors. We therefore include a
role verification step in curating a collection of
positively identified users. We use the crowd-
sourcing platform Mechanical Turk7 to judge
whether the person tweeting held a given role
Tweets were judged 5-way redundantly. Me-
chanical Turk judges (“Turkers”) were presented
with a tweet and the prompt: Based on this
tweet, would you think this person is a BAR-
BER/HAIRDRESSER? along with four response
options: Yes, Maybe, Hard to tell, and No.
We piloted this labeling task on 10 tweets per
attribute term over a variety of classes. Each an-
swer was associated with a score (Yes = 1, Maybe
= .5, Hard to tell = No = 0) and aggregated across
the five judges. We found in development that an
aggregate score of 4.0 (out of 5.0) led to an ac-
ceptable agreement rate between the Turkers and
the experimenters, when the tweets were randomly
sampled and judged internally. We found that
making conceptual class assignments based on a
single tweet was often a subtle task. The results of
this labeling study are shown in Figure 4, which
gives the percent of tweets per attribute that were
4.0 or above. Attribute terms shown in red were
manually discarded as being inaccurate (low on
the y-axis) or non-prevalent (small shape).
From the remaining attribute terms, we identi-
fied users with tweets scoring 4.0 or better as posi-
tive examples of the associated roles. Tweets from
those users were scraped via the Twitter API to
construct corpora for each role. These were split
intro train and test, balanced with data from the
same background set used in the self-ID study.
Test sets were usually of size 40 (20 positive, 20
background), with a few classes being sparse (the
smallest had only 16 instances). Results are shown
in Figure 5. Several classes in this balanced setup
can be predicted with accuracies in the 70-90%
</bodyText>
<footnote confidence="0.976344">
7https://www.mturk.com/mturk/
</footnote>
<page confidence="0.994366">
184
</page>
<figure confidence="0.799426">
Keyword
</figure>
<figureCaption confidence="0.976325333333333">
Figure 4: Turker judged quality of attributes selected as
candidate features for bootstrapping positive instances of the
given social role.
</figureCaption>
<figure confidence="0.9979985">
0.8
0.7
0.6
0.5
</figure>
<figureCaption confidence="0.987574666666667">
Figure 5: Classifier accuracy on balanced set contrasting
agreed upon Twitter users of a given role against users pulled
at random from the 1% stream.
</figureCaption>
<bodyText confidence="0.999742571428571">
range, supporting our claim that there is discrimi-
nating content for a variety of these social roles.
Conditional Classification How accurately we
can predict membership in a given class when a
Twitter user sends a tweet matching one of the tar-
geted attributes? For example, if one sends a tweet
saying my coach, then how likely is it that author
</bodyText>
<figureCaption confidence="0.908094666666667">
Figure 6: Results of positive vs negative by attribute term.
Given that a user tweets ... my lines ... we are nearly 80%
accurate in identifying whether or not the user is an actor.
</figureCaption>
<bodyText confidence="0.997346777777778">
is an athlete?
Using the same collection as the previous ex-
periment, we trained classifiers conditioned on a
given attribute term. Positive instances were taken
to be those with a score of 4.0 or higher, with neg-
ative instances taken to be those with scores of 1.0
or lower (strong agreement by judges that the orig-
inal tweet did not provide evidence of the given
role). Classification results are shown in Figure 6.
</bodyText>
<sectionHeader confidence="0.99453" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9993653">
We have shown that Twitter contains sufficiently
robust signal to support more fine-grained au-
thor attribute prediction tasks than have previously
been attempted. Our results are based on simple,
intuitive search patterns with minimal additional
filtering: this establishes the feasibility of the task,
but leaves wide room for future work, both in the
sophistication in methodology as well as the diver-
sity of roles to be targeted. We exploited two com-
plementary types of indicators: self-identification
and self-possession of conceptual class (role) at-
tributes. Those interested in identifying latent de-
mographics can extend and improve these indica-
tors in developing ways to identify groups of inter-
est within the general population of Twitter users.
Acknowledgements This material is partially
based on research sponsored by the NSF un-
der grants DGE-123285 and IIS-1249516 and by
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program).
</bodyText>
<figure confidence="0.99980341509434">
0.8
0.6
0.4
0.2
0.0
0.8
0.6
0.4
0.2
0.0
Christian College Student Dancer Doctor/Nurse Drummer Hunter
● ●
● ● ● ●
●
●
● ●
●
●
●
● ● ● ●
●
Swimmer Tattoo Artist Waiter/Waitress Writer
log10(Count)
●1●2●3●4
Keep
● FALSE TRUE
●
●
●
Jew Mom Musician Photographer Professor Rapper/Songwriter
Reporter Sailor Skier Smoker Soldier Student
0.8
0.6
0.4
0.2
0.0
Actor/Actress Athlete Barber/Hairdresser Bartender Blogger Cheerleader
● ● ●
●
● ●
● ●●● ● ●
0.8
0.6
0.4
0.2
0.0
Above Threshold
0.8
0.6
0.4
0.2
0.0
Accuracy
</figure>
<page confidence="0.991604">
185
</page>
<sectionHeader confidence="0.987721" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975140625">
Abdulrahman Almuhareb and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an eval-
uation. In Proceedings of EMNLP.
Shane Bergsma and Benjamin Van Durme. 2013. Us-
ing Conceptual Class Attributes to Characterize So-
cial Media Users. In Proceedings of ACL.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clay Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the NAACL Workshop
on Language and Social Media.
John D. Burger, John Henderson, George Kim, and
Guido Zarrella. 2011. Discriminating gender on
twitter. In Proceedings of EMNLP.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational linguistics, 16(1):22–29.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc¸alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Jacob Eisenstein, Brendan O’Connor, Noah Smith, and
Eric P. Xing. 2010. A latent variable model of
geographical lexical variation. In Proceedings of
EMNLP.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, (9).
Nikesh Garera and David Yarowsky. 2009. Modeling
latent biographic attributes in conversational genres.
In Proceedings of ACL.
Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979.
Effects of decision difficulty on recognition and re-
call. Journal of Verbal Learning and Verbal Behav-
ior, 18(5):585–600.
Michal Kosinski, David Stillwell, and Thore Graepel.
2013. Private traits and attributes are predictable
from digital records of human behavior. Proceed-
ings of the National Academy of Sciences.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In Proceedings of NAACL.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil,
Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil
Dalwani, and Sushant Narsale. 2010. New tools for
web-scale n-grams. In Proc. LREC, pages 2221–
2227.
Saif M. Mohammad, Svetlana Kiritchenko, and Joel
Martin. 2013. Identifying purpose behind elec-
toral tweets. In Proceedings of the Second Interna-
tional Workshop on Issues of Sentiment Discovery
and Opinion Mining, WISDOM ’13, pages 1–9.
Dong Nguyen, Noah A Smith, and Carolyn P Ros´e.
2011. Author age prediction from text using lin-
ear regression. In Proceedings of the 5th ACL-
HLT Workshop on Language Technology for Cul-
tural Heritage, Social Sciences, and Humanities,
pages 115–123. Association for Computational Lin-
guistics.
Marius Pas¸ca and Benjamin Van Durme. 2008.
Weakly-Supervised Acquisition of Open-Domain
Classes and Class Attributes from Web Documents
and Query Logs. In Proceedings of ACL.
Michael J Paul and Mark Dredze. 2011. You are what
you tweet: Analyzing twitter for public health. In
ICWSM.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data mining, pages 430–
438. ACM.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the Work-
shop on Search and Mining User-generated Con-
tents (SMUC).
Lenhart K. Schubert. 2002. Can we derive general
world knowledge from texts? In Proceedings of
HLT.
Benjamin Van Durme. 2012a. Jerboa: A toolkit for
randomized and streaming algorithms. Technical
Report 7, Human Language Technology Center of
Excellence, Johns Hopkins University.
Benjamin Van Durme. 2012b. Streaming analysis of
discourse participants. In Proceedings of EMNLP.
Jennifer Wortman. 2008. Viral marketing and the
diffusion of trends on social networks. Technical
Report MS-CIS-08-19, University of Pennsylvania,
May.
Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of Twitter users from neighbors. In
Proceedings of ICWSM.
</reference>
<page confidence="0.998792">
186
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.165823">
<title confidence="0.989551">I’m a Belieber: Social Roles via Self-identification and Conceptual Attributes</title>
<author confidence="0.9987985">Charley Beller</author>
<author confidence="0.9987985">Rebecca Knowles</author>
<author confidence="0.9987985">Craig Margaret Benjamin Van</author>
<affiliation confidence="0.504875">Human Language Technology Center of</affiliation>
<address confidence="0.523497333333333">Johns Hopkins University, Baltimore, MD of Saskatchewan, Saskatoon, Saskatchewan Research, Redmond, Washington</address>
<email confidence="0.9911035">charleybeller@jhu.edu,rknowles@jhu.edu,shane.a.bergsma@gmail.com,memitc@microsoft.com,vandurme@cs.jhu.edu</email>
<abstract confidence="0.998370947368421">Motivated by work predicting coarsegrained author categories in social media, such as gender or political preference, we explore whether Twitter contains inforto support the prediction of fineor We find that the simple self-identification patam a supports significantly richer classification than previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., we can further identify charactera simple possessive (e.g., that incorporate the attribute terms in first possessives are confirmed to be an indicator that the author holds the associated social role.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abdulrahman Almuhareb</author>
<author>Massimo Poesio</author>
</authors>
<title>Attribute-based and value-based clustering: an evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="5113" citStr="Almuhareb and Poesio, 2004" startWordPosition="788" endWordPosition="791">9 teacher 18,957 good 4,007 belieber 556 writer 13,069 huge 3,997 celebrity 556 awful 13,020 bit 3,737 virgin ... ... 12,816 fan 3,682 pretty 100 cashier 10,832 bad ... ... 100 bro 10,604 girl 2,915 woman ... ... 9,981 very 2,851 beast 10 linguist ... ... ... ... ... ... Table 2: Number of self-identifying users per “role”. While rich in interesting labels, cases such as very highlight the purposeful simplicity of the current approach. Study 2 In the second study we exploit a complementary signal based on characteristic conceptual attributes of a social role, or concept class (Schubert, 2002; Almuhareb and Poesio, 2004; Pas¸ca and Van Durme, 2008). We identify typical attributes of a given social role by collecting terms in the Google n-gram corpus that occur frequently in a possessive construction with that role. For example, with the role doctor we extract terms matching the simple pattern “doctor’s ”. 2 Self-identification All role-representative users were drawn from the free public 1% sample of the Twitter Firehose, over the period 2011-2013, from the subset that selected English as their native language (85,387,204 unique users). To identify users of a particular role, we performed a case-agnostic sea</context>
</contexts>
<marker>Almuhareb, Poesio, 2004</marker>
<rawString>Abdulrahman Almuhareb and Massimo Poesio. 2004. Attribute-based and value-based clustering: an evaluation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Using Conceptual Class Attributes to Characterize Social Media Users.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Bergsma, Van Durme, 2013</marker>
<rawString>Shane Bergsma and Benjamin Van Durme. 2013. Using Conceptual Class Attributes to Characterize Social Media Users. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clay Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL Workshop on Language and Social Media.</booktitle>
<contexts>
<context position="6658" citStr="Bergsma et al., 2012" startWordPosition="1034" endWordPosition="1037"> appeared only once.1 We manually selected a set of roles for further exploration, aiming for a diverse sample across: occupation (e.g., doctor, teacher), family (mother), disposition (pessimist), religion (chris1Future work should consider identifying multi-word role labels (e.g., Doctor Who fan, or dog walker). Role Figure 1: Success rate for querying a user. Random.0,1,2 are background draws from the population, with the mean of those three samples drawn horizontally. Tails capture 95% confidence intervals. tian), and “followers” (belieber, directioner).2 We filtered users via language ID (Bergsma et al., 2012) to better ensure English content.3 For each selected role, we randomly sampled up to 500 unique self-reporting users and then queried Twitter for up to 200 of their recent publicly posted tweets.4 These tweets served as representative content for that role, with any tweet matching the self-reporting patterns filtered. Three sets of background populations were extracted based on randomly sampling users that self-reported English (post-filtered via LID). Twitter users are empowered to at any time delete, rename or make private their accounts. Any given user taken to be representative based on a</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clay Fink, and Theresa Wilson. 2012. Language identification for creating language-specific twitter collections. In Proceedings of the NAACL Workshop on Language and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John Henderson</author>
<author>George Kim</author>
<author>Guido Zarrella</author>
</authors>
<title>Discriminating gender on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1485" citStr="Burger et al., 2011" startWordPosition="199" endWordPosition="202">previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share th</context>
</contexts>
<marker>Burger, Henderson, Kim, Zarrella, 2011</marker>
<rawString>John D. Burger, John Henderson, George Kim, and Guido Zarrella. 2011. Discriminating gender on twitter. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="13883" citStr="Church and Hanks, 1990" startWordPosition="2221" endWordPosition="2224">n may have a wife, then a tweet saying: ...my wife... can be taken as potential evidence of membership in the male conceptual class. In our second study, we test whether this idea extends to our wider set of fine-grained roles. For example, we aimed to discover that a doctor may have a patient, while a hairdresser may have a salon; these properties can be expressed in firstperson content as possessives like my patient or my salon. We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using pointwise mutual information (PMI) (Church and Hanks, 1990). First, we counted all terms matching a target social role’s possessive pattern (e.g., doctor’s ) in the web-scale n-gram corpus Google V2 (Lin et al., 2010)5. We ranked the collected terms by computing PMI between classes and attribute terms. Probabilities were estimated from counts of the class-attribute pairs along with counts matching the generic possessive patterns his and her which serve as general background categories. Following suggestions by Bergsma and Van Durme, we manually filtered the ranked list.6 We removed attributes that were either (a) not nominal, or (b) not indicative of </context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Conover</author>
<author>Jacob Ratkiewicz</author>
<author>Matthew Francisco</author>
<author>Bruno Gonc¸alves</author>
<author>Filippo Menczer</author>
<author>Alessandro Flammini</author>
</authors>
<title>Political polarization on twitter.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<marker>Conover, Ratkiewicz, Francisco, Gonc¸alves, Menczer, Flammini, 2011</marker>
<rawString>Michael Conover, Jacob Ratkiewicz, Matthew Francisco, Bruno Gonc¸alves, Filippo Menczer, and Alessandro Flammini. 2011. Political polarization on twitter. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Brendan O’Connor</author>
<author>Noah Smith</author>
<author>Eric P Xing</author>
</authors>
<title>A latent variable model of geographical lexical variation.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Eisenstein, O’Connor, Smith, Xing, 2010</marker>
<rawString>Jacob Eisenstein, Brendan O’Connor, Noah Smith, and Eric P. Xing. 2010. A latent variable model of geographical lexical variation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsief</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="9650" citStr="Fan et al., 2008" startWordPosition="1553" endWordPosition="1556">d test sets were balanced between users from a random background sample and self-reported users. Baseline accuracy in these experiments was thus 50%. Each training set had a target of 600 users (300 background, 300 self-identified); for those roles with less than 300 users self-identifying, all users were used, with an equal number background. We used the Jerboa (Van Durme, 2012a) platform to convert data to binary feature vectors over a unigram vocabulary filtered such that the minimum frequency was 5 (across unique users). Training and testing was done with a log-linear model via LibLinear (Fan et al., 2008). We used the positively annotated data to form test sets, balanced with data from the background set. Each test set had a theoretical maximum size of 40, but for several classes it was in the single digits (see Figure 2). Despite the varied noisiness of our simple pattern-bootstrapped training data, and the small size of our annotated test set, we see in Figure 3 that we are able to successfully achieve statistically significant predictions of social role for the majority of our selected examples. Table 3 highlights examples of language indicative of role, as determined by the most positively</context>
</contexts>
<marker>Fan, Chang, Hsief, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsief, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, (9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>Modeling latent biographic attributes in conversational genres.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1446" citStr="Garera and Yarowsky, 2009" startWordPosition="191" endWordPosition="194">rts significantly richer classification than previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communicatio</context>
</contexts>
<marker>Garera, Yarowsky, 2009</marker>
<rawString>Nikesh Garera and David Yarowsky. 2009. Modeling latent biographic attributes in conversational genres. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry L Jacoby</author>
<author>Fergus IM Craik</author>
<author>Ian Begg</author>
</authors>
<title>Effects of decision difficulty on recognition and recall.</title>
<date>1979</date>
<journal>Journal of Verbal Learning and Verbal Behavior,</journal>
<volume>18</volume>
<issue>5</issue>
<contexts>
<context position="15373" citStr="Jacoby et al., 1979" startWordPosition="2453" endWordPosition="2456">bute term from a small corpus of tweets collected over a single month in 2013, discarding those attribute terms with no positive matches. This precision test is useful regardless of how attribute lists are generated. The attribute 5In this corpus, follower-type roles like belieber and directioner are not at all prevalent. We therefore focused on occupational and habitual roles (e.g., doctor, smoker). 6Evidence from cognitive work on memory-dependent tasks suggests that such relevance based filtering (recognition) involves less cognitive effort than generating relevant attributes (recall) see (Jacoby et al., 1979). Indeed, this filtering step generally took less than a minute per class. term chart, for example, had high PMI with doctor; but a precision test on the phrase my chart yielded a single tweet which referred not to a medical chart but to a top ten list (prompting removal of this attribute). Using this smaller high-precision set of attribute terms, we collected tweets from the Twitter Firehose over the period 2011-2013. 4 Attribute-based Classification Attribute terms are less indicative overall than self-ID, e.g., the phrase I’m a barber is a clearer signal than my scissors. We therefore inclu</context>
</contexts>
<marker>Jacoby, Craik, Begg, 1979</marker>
<rawString>Larry L Jacoby, Fergus IM Craik, and Ian Begg. 1979. Effects of decision difficulty on recognition and recall. Journal of Verbal Learning and Verbal Behavior, 18(5):585–600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michal Kosinski</author>
<author>David Stillwell</author>
<author>Thore Graepel</author>
</authors>
<title>Private traits and attributes are predictable from digital records of human behavior.</title>
<date>2013</date>
<booktitle>Proceedings of the National Academy of Sciences.</booktitle>
<contexts>
<context position="3573" citStr="Kosinski et al., 2013" startWordPosition="533" endWordPosition="536">al classifiers may be constructed based purely on leveraging simple linguistic patterns. These baselines suggest a wide range of author categories to be explored further in future work. Study 1 In the first study, we seek to determine whether such a signal exists in self-identification: we rely on variants of a single pattern, “I am a ”, to bootstrap data for training balanced-class binary classifiers using unigrams observed in tweet content. As compared to prior research that required actively polling users for ground truth in order to construct predictive models for demographic information (Kosinski et al., 2013), we demonstrate that some users specify such properties publicly through direct natural language. Many of the resultant models show intuitive strongly-weighted features, such as a writer being likely to tweet about a story, or an athlete discussing a game. This demonstrates selfidentification as a viable signal in building predictive models of social roles. 181 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 181–186, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Role Tweet artist I’m an</context>
</contexts>
<marker>Kosinski, Stillwell, Graepel, 2013</marker>
<rawString>Michal Kosinski, David Stillwell, and Thore Graepel. 2013. Private traits and attributes are predictable from digital records of human behavior. Proceedings of the National Academy of Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Lamb</author>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>Separating fact from fear: Tracking flu infections on twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="2714" citStr="Lamb et al., 2013" startWordPosition="398" endWordPosition="401">rdinates through their Twitter clients, having a larger collection of automatically identified users within a region was preferable even though the predictions for any given user were uncertain. We show that media such as Twitter can support classification that is more fine-grained than gender or general location. Predicting social roles such as doctor, teacher, vegetarian, christian, may open the door to large-scale passive surveys of public discourse that dwarf what has been previously available to social scientists. For example, work on tracking the spread of flu infections across Twitter (Lamb et al., 2013) might be enhanced with a factor based on aggregate predictions of author occupation. We present two studies showing that firstperson social content (tweets) contains intuitive signals for such fine-grained roles. We argue that non-trivial classifiers may be constructed based purely on leveraging simple linguistic patterns. These baselines suggest a wide range of author categories to be explored further in future work. Study 1 In the first study, we seek to determine whether such a signal exists in self-identification: we rely on variants of a single pattern, “I am a ”, to bootstrap data for t</context>
</contexts>
<marker>Lamb, Paul, Dredze, 2013</marker>
<rawString>Alex Lamb, Michael J. Paul, and Mark Dredze. 2013. Separating fact from fear: Tracking flu infections on twitter. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for web-scale n-grams.</title>
<date>2010</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>2221--2227</pages>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale.</institution>
<contexts>
<context position="14041" citStr="Lin et al., 2010" startWordPosition="2247" endWordPosition="2250">ether this idea extends to our wider set of fine-grained roles. For example, we aimed to discover that a doctor may have a patient, while a hairdresser may have a salon; these properties can be expressed in firstperson content as possessives like my patient or my salon. We approached this task by selecting target roles from the first experiment and ranking characteristic attributes for each using pointwise mutual information (PMI) (Church and Hanks, 1990). First, we counted all terms matching a target social role’s possessive pattern (e.g., doctor’s ) in the web-scale n-gram corpus Google V2 (Lin et al., 2010)5. We ranked the collected terms by computing PMI between classes and attribute terms. Probabilities were estimated from counts of the class-attribute pairs along with counts matching the generic possessive patterns his and her which serve as general background categories. Following suggestions by Bergsma and Van Durme, we manually filtered the ranked list.6 We removed attributes that were either (a) not nominal, or (b) not indicative of the social role. This left fewer than 30 attribute terms per role, with many roles having fewer than 10. We next performed a precision test to identify potent</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for web-scale n-grams. In Proc. LREC, pages 2221– 2227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Joel Martin</author>
</authors>
<title>Identifying purpose behind electoral tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM ’13,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1798" citStr="Mohammad et al., 2013" startWordPosition="251" endWordPosition="255"> to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share their GPS coordinates through their Twitter clients, having a larger collection of automatically identified users within a region was preferable even though the predictions for any given user were uncertain. We show that media such as Twitter can support classification that is more fine-grained than gender or gene</context>
</contexts>
<marker>Mohammad, Kiritchenko, Martin, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Joel Martin. 2013. Identifying purpose behind electoral tweets. In Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, WISDOM ’13, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Nguyen</author>
<author>Noah A Smith</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Author age prediction from text using linear regression.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,</booktitle>
<pages>115--123</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Nguyen, Smith, Ros´e, 2011</marker>
<rawString>Dong Nguyen, Noah A Smith, and Carolyn P Ros´e. 2011. Author age prediction from text using linear regression. In Proceedings of the 5th ACLHLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115–123. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Weakly-Supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Pas¸ca, Van Durme, 2008</marker>
<rawString>Marius Pas¸ca and Benjamin Van Durme. 2008. Weakly-Supervised Acquisition of Open-Domain Classes and Class Attributes from Web Documents and Query Logs. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>You are what you tweet: Analyzing twitter for public health.</title>
<date>2011</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="1741" citStr="Paul and Dredze, 2011" startWordPosition="243" endWordPosition="246">te terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share their GPS coordinates through their Twitter clients, having a larger collection of automatically identified users within a region was preferable even though the predictions for any given user were uncertain. We show that media such as Twitter can support cla</context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>Michael J Paul and Mark Dredze. 2011. You are what you tweet: Analyzing twitter for public health. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Ana-Maria Popescu</author>
</authors>
<title>Democrats, Republicans and Starbucks afficionados: User classification in Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data mining,</booktitle>
<pages>430--438</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1774" citStr="Pennacchiotti and Popescu, 2011" startWordPosition="247" endWordPosition="250">n possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share their GPS coordinates through their Twitter clients, having a larger collection of automatically identified users within a region was preferable even though the predictions for any given user were uncertain. We show that media such as Twitter can support classification that is more fine-gra</context>
</contexts>
<marker>Pennacchiotti, Popescu, 2011</marker>
<rawString>Marco Pennacchiotti and Ana-Maria Popescu. 2011. Democrats, Republicans and Starbucks afficionados: User classification in Twitter. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data mining, pages 430– 438. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>David Yarowsky</author>
<author>Abhishek Shreevats</author>
<author>Manaswi Gupta</author>
</authors>
<title>Classifying latent user attributes in twitter.</title>
<date>2010</date>
<booktitle>In Proceedings of the Workshop on Search and Mining User-generated Contents (SMUC).</booktitle>
<contexts>
<context position="1464" citStr="Rao et al., 2010" startWordPosition="195" endWordPosition="198">assification than previously explored, successfully retrieving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some user</context>
</contexts>
<marker>Rao, Yarowsky, Shreevats, Gupta, 2010</marker>
<rawString>Delip Rao, David Yarowsky, Abhishek Shreevats, and Manaswi Gupta. 2010. Classifying latent user attributes in twitter. In Proceedings of the Workshop on Search and Mining User-generated Contents (SMUC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lenhart K Schubert</author>
</authors>
<title>Can we derive general world knowledge from texts?</title>
<date>2002</date>
<booktitle>In Proceedings of HLT.</booktitle>
<contexts>
<context position="5085" citStr="Schubert, 2002" startWordPosition="786" endWordPosition="787">2 big ... ... 559 teacher 18,957 good 4,007 belieber 556 writer 13,069 huge 3,997 celebrity 556 awful 13,020 bit 3,737 virgin ... ... 12,816 fan 3,682 pretty 100 cashier 10,832 bad ... ... 100 bro 10,604 girl 2,915 woman ... ... 9,981 very 2,851 beast 10 linguist ... ... ... ... ... ... Table 2: Number of self-identifying users per “role”. While rich in interesting labels, cases such as very highlight the purposeful simplicity of the current approach. Study 2 In the second study we exploit a complementary signal based on characteristic conceptual attributes of a social role, or concept class (Schubert, 2002; Almuhareb and Poesio, 2004; Pas¸ca and Van Durme, 2008). We identify typical attributes of a given social role by collecting terms in the Google n-gram corpus that occur frequently in a possessive construction with that role. For example, with the role doctor we extract terms matching the simple pattern “doctor’s ”. 2 Self-identification All role-representative users were drawn from the free public 1% sample of the Twitter Firehose, over the period 2011-2013, from the subset that selected English as their native language (85,387,204 unique users). To identify users of a particular role, we p</context>
</contexts>
<marker>Schubert, 2002</marker>
<rawString>Lenhart K. Schubert. 2002. Can we derive general world knowledge from texts? In Proceedings of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Jerboa: A toolkit for randomized and streaming algorithms.</title>
<date>2012</date>
<tech>Technical Report 7,</tech>
<institution>Human Language Technology Center of Excellence, Johns Hopkins University.</institution>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012a. Jerboa: A toolkit for randomized and streaming algorithms. Technical Report 7, Human Language Technology Center of Excellence, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Van Durme</author>
</authors>
<title>Streaming analysis of discourse participants.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Van Durme, 2012</marker>
<rawString>Benjamin Van Durme. 2012b. Streaming analysis of discourse participants. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Wortman</author>
</authors>
<title>Viral marketing and the diffusion of trends on social networks.</title>
<date>2008</date>
<tech>Technical Report MS-CIS-08-19,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="1622" citStr="Wortman, 2008" startWordPosition="224" endWordPosition="225">eristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share their GPS coordinates through their Twitter clients, having a larger collection of automatically identified users within a region was prefe</context>
</contexts>
<marker>Wortman, 2008</marker>
<rawString>Jennifer Wortman. 2008. Viral marketing and the diffusion of trends on social networks. Technical Report MS-CIS-08-19, University of Pennsylvania, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faiyaz Al Zamal</author>
<author>Wendy Liu</author>
<author>Derek Ruths</author>
</authors>
<title>Homophily and latent attribute inference: Inferring latent attributes of Twitter users from neighbors.</title>
<date>2012</date>
<booktitle>In Proceedings of ICWSM.</booktitle>
<contexts>
<context position="1524" citStr="Zamal et al., 2012" startWordPosition="206" endWordPosition="209">eving a variety of fine-grained roles. For a given role (e.g., writer), we can further identify characteristic attributes using a simple possessive construction (e.g., writer’s ). Tweets that incorporate the attribute terms in first person possessives (my ) are confirmed to be an indicator that the author holds the associated social role. 1 Introduction With the rise of social media, researchers have sought to induce models for predicting latent author attributes such as gender, age, and political preferences (Garera and Yarowsky, 2009; Rao et al., 2010; Burger et al., 2011; Van Durme, 2012b; Zamal et al., 2012). Such models are clearly in line with the goals of both computational advertising (Wortman, 2008) and the growing area of computational social science (Conover et al., 2011; Nguyen et al., 2011; Paul and Dredze, 2011; Pennacchiotti and Popescu, 2011; Mohammad et al., 2013) where big data and computation supplement methods based on, e.g., direct human surveys. For example, Eisenstein et al. (2010) demonstrated a model that predicted where an author was located in order to analyze regional distinctions in communication. While some users explicitly share their GPS coordinates through their Twitt</context>
</contexts>
<marker>Zamal, Liu, Ruths, 2012</marker>
<rawString>Faiyaz Al Zamal, Wendy Liu, and Derek Ruths. 2012. Homophily and latent attribute inference: Inferring latent attributes of Twitter users from neighbors. In Proceedings of ICWSM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>