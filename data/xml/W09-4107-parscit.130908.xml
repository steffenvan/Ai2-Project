<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.289987">
<title confidence="0.943124">
Enabling Adaptation of Lexicalised Grammars to New
Domains
</title>
<author confidence="0.964277">
Valia Kordoni &amp; Yi Zhang
</author>
<affiliation confidence="0.6579">
DFKI GmbH &amp; Dept. of Computational Linguistics, Saarland University
</affiliation>
<address confidence="0.887311">
PO Box 15 11 50, 66041 Saarbr¨ucken, GERMANY
</address>
<email confidence="0.996123">
kordoni,yzhang@coli.uni-sb.de
</email>
<sectionHeader confidence="0.987593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997887285714286">
This extended abstract focuses on the main
points we will be touching upon during our talk,
the aim of which is to present in a concise man-
ner our group’s work on enhancing robustness
of lexicalised grammars for real-life applications
and thus also on enabling their adaptation to
new domains in its entirety.
</bodyText>
<sectionHeader confidence="0.996879" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990578947368">
At present, various wide-coverage symbolic parsing
systems for different languages exist and have been in-
tegrated into real-world NLP applications, such as IE,
QA, grammar checking, MT and intelligent IR. This
integration, though, has reminded us of the shortcom-
ings of symbolic systems, in particular lack of cov-
erage, one consequence of which relates to enormous
and sometimes insurmountable difficulties with port-
ing and re-using such systems to new domains. When
the hand-crafted grammars which usually lie at the
heart of symbolic parsing systems are applied to natu-
rally occurring text, we often find that they are under-
performing. Typical sources of coverage deficiency in-
clude unknown words, words for which the dictionary
did not contain the relevant category, Multiword Ex-
pressions (MWEs), but also more general grammatical
knowledge, such as grammar rules and word ordering
constraints. Currently, grammars and their accompa-
nying lexica often need to be extended manually.
In this talk, we offer the overview to a range of ma-
chine learning-based methods which enable us to de-
rive linguistic knowledge from corpora, for instance, in
order to solve problems of coverage and efficiency de-
ficiency of large-scale lexicalised grammars, ensuring
this way their portability and re-usability and aiming
at domain-independent linguistic processing. In par-
ticular, we illustrate and underline the importance of
making detailed linguistic information a central part
of the process of automatic acquisition of large-scale
lexicons as a means for enhancing robustness and en-
suring maintainability and re-usability of lexicalised
grammars.
To this effect, we focus on enhancing robustness and
ensuring maintainability and re-usability for two large-
scale “deep” grammars, one of English (ERG; [3]) and
one of German (GG; [4]), developed in the framework
of Head-driven Phrase Structure Grammar (HPSG).
Specifically, we show that the incorporation of detailed
</bodyText>
<page confidence="0.979546">
48
</page>
<bodyText confidence="0.999159166666667">
linguistic information into the process of automatic ex-
tension of the lexicon of such language resources en-
hances their performance and provides linguistically
sound and more informative predictions which bring
a bigger benefit for the grammars when employed in
practical real-life applications.
</bodyText>
<sectionHeader confidence="0.892802" genericHeader="introduction">
2 Main Focus Points
</sectionHeader>
<bodyText confidence="0.999985146341463">
In recent years, various techniques and resources have
been developed in order to improve robustness of deep
grammars for real-life applications in various domains.
Nevertheless, low coverage of such grammars remains
the main hindrance to their employment in open do-
main natural language processing. [2], as well as [6]
and [7] have clearly shown that the majority of pars-
ing failures with large-scale deep grammars are caused
by missing or wrong entries in the lexica accompa-
nying grammars like the aforementioned ones. Based
on these findings, it has become clear that it is cru-
cial to explore and come up with efficient methods for
automated (Deep) Lexical Acquisition (henceforward
(D)LA), the process of automatically recovering miss-
ing entries in the lexicons of deep grammars.
Recently, various high-quality DLA approaches have
been proposed. [1], as well as [7] and [5] describe effi-
cient methods towards the task of lexicon acquisition
for large-scale deep grammars for English and Dutch.
They treat DLA as a classification task and make use
of various robust and efficient machine learning tech-
niques to perform the acquisition process.
We use the ERG and GG grammars for the work we
present in this talk, for the ERG is one of the biggest
deep linguistic resources to date which has been being
used in many (industrial) applications, and GG is to
our knowledge one of the most thorough deep gram-
mars of German, a language with rich morphology and
free word order, which exhibits a range of interest-
ing linguistic phenomena. Thus, the aforementioned
grammars are valuable linguistic resources since they
provide linguistically sound and detailed analyses of
phenomena in English and German. Apart from the
interesting syntactic structures, though, the lexical en-
tries in the lexicons of the aforementioned grammars
also exhibit a rich and complicated structure and con-
tain various important linguistic constraints. Based on
our claim above, in this talk we show how the infor-
mation these constraints provide can be captured and
used in linguistically-motivated DLA methods which
we have developed. It has been shown that, comparing
</bodyText>
<subsectionHeader confidence="0.586173">
Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 48–49
</subsectionHeader>
<bodyText confidence="0.999980222222222">
to statistically treebank-based parsers, parsers based
on these hand-written linguistic grammars have more
consistent performance over changing of domains [8].
In this we prove our assumption that the linguistic
information we incorporate into our DLA methods is
vital for the good performance of the acquisition pro-
cess and for the maintainability and re-usability of the
grammars domain-independently, as well for their suc-
cessful practical application.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987064516129">
[1] T. Baldwin. Bootstrapping deep lexical resources: Resources for
courses. In Proceedings of the ACL-SIGLEX 2005 Workshop
on Deep Lexical Acquisition, pages 67–76, Ann Arbor, USA,
2005.
[2] T. Baldwin, E. M. Bender, D. Flickinger, A. Kim, and S. Oepen.
Road-testing the English Resource Grammar over the British
National Corpus. In Proceedings of the Fourth Internation
Conference on Language Resources and Evaluation (LREC
2004), Lisbon, Portugal, 2004.
[3] A. Copestake and D. Flickinger. An open-sourse grammar devel-
opment environment and broad-coverage English grammar using
HPSG. In Proceedings of the Second conference on Language
Resources and Evaluation (LREC 2000), Athens, Greece, 2000.
[4] B. Crysmann. On the efficient implementation of German verb
placement in HPSG. In Proceedings of RANLP 2003, pages
112–116, Borovets, Bulgaria, 2003.
[5] T. V. de Cruys. Automatically extending the lexicon for parsing.
In Proceedings of the Student Session of the European Summer
School in Logic, Language and Information (ESSLLI), pages
180–191, Malaga, Spain, 2006.
[6] G. van Noord. Error mining for wide coverage grammar engi-
neering. In Proceedings of the 42nd Meeting of the Assiciation
for Computational Linguistics (ACL’04), Main Volume, pages
446–453, Barcelona, Spain, 2004.
[7] Y. Zhang and V. Kordoni. Automated deep lexical acquisition
for robust open text processing. In Proceedings of the Fifth
International Conference on Language Resourses and Evalu-
ation (LREC 2006), Genoa, Italy, 2006.
[8] Y. Zhang and R. Wang. Cross-domain dependency parsing using
a deep linguistic grammar. In Proceedings of ACL-IJCNLP
2009, Singapore, 2009.
</reference>
<page confidence="0.999545">
49
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.632488">
<title confidence="0.998797">Enabling Adaptation of Lexicalised Grammars to New Domains</title>
<author confidence="0.993766">Valia Kordoni</author>
<author confidence="0.993766">Yi</author>
<affiliation confidence="0.813399">DFKI GmbH &amp; Dept. of Computational Linguistics, Saarland</affiliation>
<author confidence="0.816736">PO Box</author>
<abstract confidence="0.99785775">This extended abstract focuses on the main points we will be touching upon during our talk, the aim of which is to present in a concise manner our group’s work on enhancing robustness of lexicalised grammars for real-life applications and thus also on enabling their adaptation to new domains in its entirety.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>Bootstrapping deep lexical resources: Resources for courses.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition,</booktitle>
<pages>67--76</pages>
<location>Ann Arbor, USA,</location>
<contexts>
<context position="3688" citStr="[1]" startWordPosition="560" endWordPosition="560">yment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English and Dutch. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. We use the ERG and GG grammars for the work we present in this talk, for the ERG is one of the biggest deep linguistic resources to date which has been being used in many (industrial) applications, and GG is to our knowledge one of the most thorough deep grammars of German, a language with rich morphology</context>
</contexts>
<marker>[1]</marker>
<rawString>T. Baldwin. Bootstrapping deep lexical resources: Resources for courses. In Proceedings of the ACL-SIGLEX 2005 Workshop on Deep Lexical Acquisition, pages 67–76, Ann Arbor, USA, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>A Kim</author>
<author>S Oepen</author>
</authors>
<title>Road-testing the English Resource Grammar over the British National Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth Internation Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="3138" citStr="[2]" startWordPosition="472" endWordPosition="472">mation into the process of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammars when employed in practical real-life applications. 2 Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient method</context>
</contexts>
<marker>[2]</marker>
<rawString>T. Baldwin, E. M. Bender, D. Flickinger, A. Kim, and S. Oepen. Road-testing the English Resource Grammar over the British National Corpus. In Proceedings of the Fourth Internation Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>An open-sourse grammar development environment and broad-coverage English grammar using HPSG.</title>
<date>2000</date>
<booktitle>In Proceedings of the Second conference on Language Resources and Evaluation (LREC 2000),</booktitle>
<location>Athens, Greece,</location>
<contexts>
<context position="2353" citStr="[3]" startWordPosition="356" endWordPosition="356">ency deficiency of large-scale lexicalised grammars, ensuring this way their portability and re-usability and aiming at domain-independent linguistic processing. In particular, we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and ensuring maintainability and re-usability of lexicalised grammars. To this effect, we focus on enhancing robustness and ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed 48 linguistic information into the process of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammars when employed in practical real-life applications. 2 Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars </context>
</contexts>
<marker>[3]</marker>
<rawString>A. Copestake and D. Flickinger. An open-sourse grammar development environment and broad-coverage English grammar using HPSG. In Proceedings of the Second conference on Language Resources and Evaluation (LREC 2000), Athens, Greece, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crysmann</author>
</authors>
<title>On the efficient implementation of German verb placement in HPSG.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP 2003,</booktitle>
<pages>112--116</pages>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="2381" citStr="[4]" startWordPosition="362" endWordPosition="362">le lexicalised grammars, ensuring this way their portability and re-usability and aiming at domain-independent linguistic processing. In particular, we illustrate and underline the importance of making detailed linguistic information a central part of the process of automatic acquisition of large-scale lexicons as a means for enhancing robustness and ensuring maintainability and re-usability of lexicalised grammars. To this effect, we focus on enhancing robustness and ensuring maintainability and re-usability for two largescale “deep” grammars, one of English (ERG; [3]) and one of German (GG; [4]), developed in the framework of Head-driven Phrase Structure Grammar (HPSG). Specifically, we show that the incorporation of detailed 48 linguistic information into the process of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammars when employed in practical real-life applications. 2 Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications i</context>
</contexts>
<marker>[4]</marker>
<rawString>B. Crysmann. On the efficient implementation of German verb placement in HPSG. In Proceedings of RANLP 2003, pages 112–116, Borovets, Bulgaria, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T V de Cruys</author>
</authors>
<title>Automatically extending the lexicon for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Student Session of the European Summer School in Logic, Language and Information (ESSLLI),</booktitle>
<pages>180--191</pages>
<location>Malaga,</location>
<contexts>
<context position="3712" citStr="[5]" startWordPosition="566" endWordPosition="566">ural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of lexicon acquisition for large-scale deep grammars for English and Dutch. They treat DLA as a classification task and make use of various robust and efficient machine learning techniques to perform the acquisition process. We use the ERG and GG grammars for the work we present in this talk, for the ERG is one of the biggest deep linguistic resources to date which has been being used in many (industrial) applications, and GG is to our knowledge one of the most thorough deep grammars of German, a language with rich morphology and free word order, wh</context>
</contexts>
<marker>[5]</marker>
<rawString>T. V. de Cruys. Automatically extending the lexicon for parsing. In Proceedings of the Student Session of the European Summer School in Logic, Language and Information (ESSLLI), pages 180–191, Malaga, Spain, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>Error mining for wide coverage grammar engineering.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Assiciation for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>446--453</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3154" citStr="[6]" startWordPosition="476" endWordPosition="476">process of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammars when employed in practical real-life applications. 2 Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the ta</context>
</contexts>
<marker>[6]</marker>
<rawString>G. van Noord. Error mining for wide coverage grammar engineering. In Proceedings of the 42nd Meeting of the Assiciation for Computational Linguistics (ACL’04), Main Volume, pages 446–453, Barcelona, Spain, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>V Kordoni</author>
</authors>
<title>Automated deep lexical acquisition for robust open text processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resourses and Evaluation (LREC</booktitle>
<location>Genoa, Italy,</location>
<contexts>
<context position="3162" citStr="[7]" startWordPosition="478" endWordPosition="478">of automatic extension of the lexicon of such language resources enhances their performance and provides linguistically sound and more informative predictions which bring a bigger benefit for the grammars when employed in practical real-life applications. 2 Main Focus Points In recent years, various techniques and resources have been developed in order to improve robustness of deep grammars for real-life applications in various domains. Nevertheless, low coverage of such grammars remains the main hindrance to their employment in open domain natural language processing. [2], as well as [6] and [7] have clearly shown that the majority of parsing failures with large-scale deep grammars are caused by missing or wrong entries in the lexica accompanying grammars like the aforementioned ones. Based on these findings, it has become clear that it is crucial to explore and come up with efficient methods for automated (Deep) Lexical Acquisition (henceforward (D)LA), the process of automatically recovering missing entries in the lexicons of deep grammars. Recently, various high-quality DLA approaches have been proposed. [1], as well as [7] and [5] describe efficient methods towards the task of le</context>
</contexts>
<marker>[7]</marker>
<rawString>Y. Zhang and V. Kordoni. Automated deep lexical acquisition for robust open text processing. In Proceedings of the Fifth International Conference on Language Resourses and Evaluation (LREC 2006), Genoa, Italy, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>R Wang</author>
</authors>
<title>Cross-domain dependency parsing using a deep linguistic grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP</booktitle>
<marker>[8]</marker>
<rawString>Y. Zhang and R. Wang. Cross-domain dependency parsing using a deep linguistic grammar. In Proceedings of ACL-IJCNLP 2009, Singapore, 2009.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>