<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005905">
<title confidence="0.99799">
Using Tweets to Help Sentence Compression for News Highlights
Generation
</title>
<author confidence="0.999878">
Zhongyu Wei1, Yang Liu1, Chen Li1, Wei Gao2
</author>
<affiliation confidence="0.970692333333333">
1Computer Science Department, The University of Texas at Dallas
Richardson, Texas 75080, USA
2Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar
</affiliation>
<email confidence="0.9646505">
{zywei,yangl,chenli}@hlt.utdallas.edu1
wgao@qf.org.qa2
</email>
<sectionHeader confidence="0.997263" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999823125">
We explore using relevant tweets of a
given news article to help sentence com-
pression for generating compressive news
highlights. We extend an unsupervised
dependency-tree based sentence compres-
sion approach by incorporating tweet in-
formation to weight the tree edge in terms
of informativeness and syntactic impor-
tance. The experimental results on a pub-
lic corpus that contains both news arti-
cles and relevant tweets show that our pro-
posed tweets guided sentence compres-
sion method can improve the summariza-
tion performance significantly compared
to the baseline generic sentence compres-
sion method.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882903225807">
“Story highlights” of news articles are provided
by only a few news websites such as CNN.com.
The highlights typically consist of three or four
succinct itemized sentences for readers to quickly
capture the gist of the document, and can dramat-
ically reduce reader’s information load. A high-
light sentence is usually much shorter than its orig-
inal corresponding news sentence; therefore ap-
plying extractive summarization methods directly
to sentences in a news article is not enough to gen-
erate high quality highlights.
Sentence compression aims to retain the most
important information of an original sentence in a
shorter form while being grammatical at the same
time. Previous research has shown the effective-
ness of sentence compression for automatic doc-
ument summarization (Knight and Marcu, 2000;
Lin, 2003; Galanis and Androutsopoulos, 2010;
Chali and Hasan, 2012; Wang et al., 2013; Li et
al., 2013; Qian and Liu, 2013; Li et al., 2014). The
compressed summaries can be generated through
a pipeline approach that combines a generic sen-
tence compression model with a summary sen-
tence pre-selection or post-selection step. Prior
studies have mostly used the generic sentence
compression approaches, however, a generic com-
pression system may not be the best fit for the
summarization purpose because it does not take
into account the summarization task in the com-
pression module. Li et al. (2013) thus proposed a
summary guided compression method to address
this problem and showed the effectiveness of their
method. But this approach relied heavily on the
training data, thus has the limitation of domain
generalization.
Instead of using a manually generated corpus,
we investigate using existing external sources to
guide sentence compression for the purpose of
compressive news highlights generation. Nowa-
days it becomes more and more common that
users share interesting news content via Twitter to-
gether with their comments. The availability of
cross-media information provides new opportuni-
ties for traditional tasks of Natural Language Pro-
cessing (Zhao et al., 2011; Subaˇsi´c and Berendt,
2011; Gao et al., 2012; Kothari et al., 2013;
ˇStajner et al., 2013). In this paper, we propose to
use relevant tweets of a news article to guide the
sentence compression process in a pipeline frame-
work for generating compressive news highlights.
This is a pioneer study for using such parallel data
to guide sentence compression for document sum-
marization.
Our work shares some similar ideas with (Wei
and Gao, 2014; Wei and Gao, 2015). They also
attempted to use tweets to help news highlights
generation. Wei and Gao (2014) derived external
features based on the relevant tweet collection to
assist the ranking of the original sentences for ex-
tractive summarization in a fashion of supervised
machine learning. Wei and Gao (2015) proposed a
graph-based approach to simultaneously rank the
</bodyText>
<page confidence="0.901067">
50
</page>
<bodyText confidence="0.932583526315789">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
original news sentences and relevant tweets in an
unsupervised way. Both of them focused on using
tweets to help sentence extraction while we lever-
age tweet information to guide sentence compres-
sion for compressive summary generation.
We extend an unsupervised dependency-tree
based sentence compression approach to incorpo-
rate tweet information from the aspects of both in-
formativeness and syntactic importance to weight
the tree edge. We evaluate our method on a public
corpus that contains both news articles and rele-
vant tweets. The result shows that generic com-
pression hurts the performance of highlights gen-
eration, while sentence compression guided by
relevant tweets of the news article can improve the
performance.
</bodyText>
<sectionHeader confidence="0.996959" genericHeader="introduction">
2 Framework
</sectionHeader>
<bodyText confidence="0.9994396">
We adopt a pipeline approach for compressive
news highlights generation. The framework in-
tegrates a sentence extraction component and a
post-sentence compression component. Each is
described below.
</bodyText>
<subsectionHeader confidence="0.990854">
2.1 Tweets Involved Sentence Extraction
</subsectionHeader>
<bodyText confidence="0.9945332">
We use LexRank (Erkan and Radev, 2004) as the
baseline to select the salient sentences in a news
article. This baseline is an unsupervised extractive
summarization approach and has been proved to
be effective for the summarization task.
Besides LexRank, we also use Heterogeneous
Graph Random Walk (HGRW) (Wei and Gao,
2015) to incorporate relevant tweet information
to extract news sentences. In this model, an
undirected similarity graph is created, similar to
LexRank. However, the graph is heterogeneous,
with two types of nodes for the news sentences and
tweets respectively.
Suppose we have a sentence set S and a tweet
set T. By considering the similarity between the
same type of nodes and cross types, the score of a
news sentence s is computed as follows:
where N and M are the size of S and T, respec-
tively, d is a damping factor, sim(x, y) is the simi-
larity function, and the parameter E is used to con-
trol the contribution of relevant tweets. For a tweet
node t, its score can be computed similarly. Both
d and sim(x, y) are computed following the setup
of LexRank, where sim(x, y) is computed as co-
sine similarity:
</bodyText>
<equation confidence="0.99474775">
Pw∈x,y tfw,xtfw,y(idfw)2
sim(x, y) =qP qP
wi∈x (tfwi,xidfwi)2 × wi∈y (tfwi,yidfwi)2
(2)
</equation>
<bodyText confidence="0.999958666666667">
where tfw,x is the number of occurrences of word
w in instance x, idfw is the inverse document fre-
quency of word w in the dataset. In our task, each
sentence or tweet is treated as a document to com-
pute the IDF value.
Although both types of nodes can be ranked in
this framework, we only output the top news sen-
tences as the highlights, and the input to the sub-
sequent compression component.
</bodyText>
<subsectionHeader confidence="0.7448335">
2.2 Dependency Tree Based Sentence
Compression
</subsectionHeader>
<bodyText confidence="0.999993882352941">
We use an unsupervised dependency tree based
compression framework (Filippova and Strube,
2008) as our baseline. This method achieved a
higher F-score (Riezler et al., 2003) than other sys-
tems on the Edinburgh corpus (Clarke and Lap-
ata, 2006). We will introduce the baseline in this
part and describe our extended model that lever-
ages tweet information in the next subsection.
The sentence compression task can be defined
as follows: given a sentence s, consisting of words
w1, w2,..., wm, identify a subset of the words of
s, such that it is grammatical and preserves es-
sential information of s. In the baseline frame-
work, a dependency graph for an original sentence
is first generated and then the compression is done
by deleting edges of the dependency graph. The
goal is to find a subtree with the highest score:
</bodyText>
<equation confidence="0.9762185">
f(X) = � xe × winfo(e) × wsyn(e) (3)
e∈E
</equation>
<bodyText confidence="0.999958777777778">
where xe is a binary variable, indicating whether
a directed dependency edge a is kept (xe is 1) or
removed (xe is 0), and E is the set of edges in the
dependency graph. The weighting of edge e con-
siders both its syntactic importance (wsyn(e)) as
well as the informativeness (winfo(e)). Suppose
edge a is pointed from head h to node n with de-
pendency label l, both weights can be computed
from a background news corpus as:
</bodyText>
<equation confidence="0.9773475">
Psummary(n)
winfo(e) = (4)
Particle(n)
⎡ ⎤
X sim(s, n)
+(1 − d) ⎣(1 − �) Pv∈S\{s} sim(s, v) p(n) ⎦
n∈S\{s}
⎡ ⎤
d sim(s, m)
(1)
51
wsyn(e) = P(l|h) (5)
</equation>
<bodyText confidence="0.999938375">
where Psummary(n) and Particle(n) are the uni-
gram probabilities of word n in the two language
models trained on human generated summaries
and the original articles respectively. P(l|h) is
the conditional probability of label l given head
h. Note that here we use the formula in (Filip-
pova and Altun, 2013) for winfo(e), which was
shown to be more effective for sentence compres-
sion than the original formula in (Filippova and
Strube, 2008).
The optimization problem can be solved under
the tree structure and length constraints by integer
linear programming1. Given that L is the maxi-
mum number of words permitted for the compres-
sion, the length constraint is simply represented
as:
</bodyText>
<equation confidence="0.658017">
� xe G L (6)
e∈E
</equation>
<bodyText confidence="0.999959833333333">
The surface realizatdion is standard: the words
in the compression subtree are put in the same or-
der they are found in the source sentence. Due
to space limit, we refer readers to (Filippova and
Strube, 2008) for a detailed description of the
baseline method.
</bodyText>
<subsectionHeader confidence="0.998197">
2.3 Leverage Tweets for Edge Weighting
</subsectionHeader>
<bodyText confidence="0.999988555555555">
We then extend the dependency-tree based com-
pression framework by incorporating tweet infor-
mation for dependency edge weighting. We in-
troduce two new factors, wT info(e) and wT syn(e),
for informativeness and syntactic importance re-
spectively, computed from relevant tweets of the
news. These are combined with the weights ob-
tained from the background news corpus defined
in Section 2.2, as shown below:
</bodyText>
<equation confidence="0.9999975">
winfo(e) = (1−α)&apos;wN info(e)+α&apos;wT info(e) (7)
wsyn(e) = (1 − β) &apos; wNsyn(e) + β &apos; wTsyn(e) (8)
</equation>
<bodyText confidence="0.9996018">
where α and β are used to balance the contribution
of the two sources, and wNinfo(e) and wNsyn(e) are
based on Equation 4 and 5.
The new informative weight wTinfo(e) is calcu-
lated as:
</bodyText>
<footnote confidence="0.8039">
1In our implementation we use GNU Linear Pro-
gramming Kit (GULP) (https://www.gnu.org/
software/glpk/)
</footnote>
<bodyText confidence="0.9397708">
PrelevantT (n) and PbackgroundT (n) are the uni-
gram probabilities of word n in two language mod-
els trained on the relevant tweet dataset and a
background tweet dataset respectively.
The new syntactic importance score is:
</bodyText>
<equation confidence="0.969414">
wTsyn(e) = NT(h,n) (10)
NT
</equation>
<bodyText confidence="0.982117555555555">
NT (h, n) is the number of tweets where n and
head h appear together within a window frame of
K, and NT is the total number of tweets in the
relevant tweet collection. Since tweets are always
noisy and informal, traditional parsers are not reli-
able to extract dependency trees. Therefore, we
use co-occurrence as pseudo syntactic informa-
tion here. Note wNinfo(e), wTinfo(e), wNsyn(e) and
wTsyn(e) are normalized before combination.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.959734">
3.1 Setup
</subsectionHeader>
<bodyText confidence="0.999971892857143">
We evaluate our pipeline news highlights gen-
eration framework on a public corpus based on
CNN/USAToday news (Wei and Gao, 2014). This
corpus was constructed via an event-oriented strat-
egy following four steps: 1) 17 salient news events
taking place in 2013 and 2014 were manually
identified. 2) For each event, relevant tweets were
retrieved via Topsy2 search API using a set of
manually generated core queries. 3) News arti-
cles explicitly linked by URLs embedded in the
tweets were collected. 4) News articles from
CNN/USAToday that have more than 100 explic-
itly linked tweets were kept. The resulting cor-
pus contains 121 documents, 455 highlights and
78,419 linking tweets.
We used tweets explicitly linked to a news ar-
ticle to help extract salience sentences in HGRW
and to generate the language model for computing
wTinfo(e). The co-occurrence information com-
puted from the set of explicitly linked tweets is
very sparse because the size of the tweet set is
small. Therefore, we used all the tweets re-
trieved for the event related to the target news arti-
cle to compute the co-occurrence information for
wTsyn(e). Tweets retrieved for events were not pub-
lished in (Wei and Gao, 2014). We make it avail-
able here3. The statistics of the dataset can be
found in Table. 1.
</bodyText>
<footnote confidence="0.971312333333333">
2http://topsy.com
3http://www.hlt.utdallas.edu/˜zywei/
data/CNNUSATodayEvent.zip
</footnote>
<equation confidence="0.9961625">
wT info(e) = PrelevantT (n)
PbackgroundT(n) (9)
</equation>
<page confidence="0.984625">
52
</page>
<table confidence="0.999777545454545">
Event Doc # HLight # Linked Retrieved Event Doc # HLight # Linked Retrieved
Tweet # Tweet # Tweet # Tweet #
Aurora shooting 14 54 12,463 588,140 African runner murder 8 29 9,461 303,535
Boston bombing 38 147 21,683 1,650,650 Syria chemical weapons use 1 4 331 11,850
Connecticut shooting 13 47 3,021 213,864 US military in Syria 2 7 719 619,22
Edward Snowden 5 17 1,955 379,349 DPRK Nuclear Test 2 8 3,329 103,964
Egypt balloon crash 3 12 836 36,261 Asiana Airlines Flight 214 11 42 8,353 351,412
Hurricane Sandy 4 15 607 189,082 Moore Tornado 5 19 1,259 1,154,656
Russian meteor 3 11 6,841 239,281 Chinese Computer Attacks 2 8 507 28,988
US Flu Season 7 23 6,304 1,042,169 Williams Olefins Explosion 1 4 268 14,196
Super Bowl blackout 2 8 482 214,775 Total 121 455 78,419 6,890,987
</table>
<tableCaption confidence="0.977416">
Table 1: Distribution of documents, highlights and tweets with respect to different events
</tableCaption>
<table confidence="0.9999535625">
Method ROUGE-1 Compr.
Rate(%)
F(%) P(%) R(%)
LexRank 26.1 19.9 39.1 100
LexRank + SC 25.2 22.4 29.6 63.0
LexRank + SC+wTinfo 25.7 22.8 30.1 62.0
LexRank + SC+wT 26.2 23.5 30.4 63.7
syn
LexRank + SC+both 27.5 25.0 31.4 61.5
HGRW 28.1 22.6 39.5 100
HGRW + SC 26.4 24.9 29.5 66.1
HGRW + SC+wT 27.5 25.7 30.8 65.4
info
HGRW + SC+wT 27.0 25.3 30.2 66.7
syn
HGRW + SC+both 28.4 26.9 31.2 64.8
</table>
<tableCaption confidence="0.984544">
Table 2: Overall Performance. Bold: the best
</tableCaption>
<bodyText confidence="0.984750842105263">
value in each group in terms of different metrics.
Following (Wei and Gao, 2014), we output 4
sentences for each news article as the highlights
and report the ROUGE-1 scores (Lin, 2004) using
human-generated highlights as the reference.
The sentence compression rates are set to 0.8 for
short sentences containing fewer than 9 words, and
0.5 for long sentences with more than 9 words, fol-
lowing (Filippova and Strube, 2008). We empiri-
cally use 0.8 for α, β and c such that tweets have
more impact for both sentence selection and com-
pression. We leveraged The New York Times An-
notated Corpus (LDC Catalog No: LDC2008T19)
as the background news corpus. It has both the
original news articles and human generated sum-
maries. The Stanford Parser4 is used to obtain de-
pendency trees. The background tweet corpus is
collected from Twitter public timeline via Twitter
API, and contains more than 50 million tweets.
</bodyText>
<subsectionHeader confidence="0.858051">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.969535333333333">
Table 2 shows the overall performance5. For sum-
maries generated by both LexRank and HGRW,
“+SC” means generic sentence compression base-
</bodyText>
<footnote confidence="0.965460285714286">
4http://nlp.stanford.edu/software/
lex-parser.shtml
5The performance of HGRW reported here is different
from (Wei and Gao, 2015) because the setup is different. We
use all the explicitly linked tweets in the ranking process here
without considering redundancy while a redundancy filtering
process was applied in (Wei and Gao, 2015) .
</footnote>
<bodyText confidence="0.996678666666667">
line (Section. 2.2) is used, “+wT info”and “+wT syn”
indicate tweets are used to help edge weighting
for sentence compression in terms of informative-
ness and syntactic importance respectively, and
“+both” means both factors are used. We have
several findings.
</bodyText>
<listItem confidence="0.973334583333333">
• The tweets involved sentence extraction model
HGRW can improve LexRank by 8.8% rela-
tively in terms of ROUGE-1 F score, showing
the effectiveness of relevant tweets for sentence
selection.
• With generic sentence compression, the
ROUGE-1 F scores for both LexRank and
HGRW drop, mainly because of a much lower
recall score. This indicates that generic sen-
tence compression without certain guidance
removes salient content of the original sentence
that may be important for summarization and
thus hurts the performance. This is consistent
with the finding of (Chali and Hasan, 2012).
• By adding either wTinfo or wTsyn, the perfor-
mance of summarization increases, showing
that relevant tweets can be used to help the
scores of both informativeness and syntactic im-
portance.
• +SC+both improves the summarization perfor-
mance significantly6 compared to the corre-
sponding compressive summarization baseline
+SC, and outperforms the corresponding origi-
nal baseline, LexRank and HGRW.
</listItem>
<bodyText confidence="0.881050777777778">
• The improvement obtained by
LexRank+SC+both compared to LexRank
is more promising than that obtained by
HGRW+SC+both compared to HGRW. This
may be because HGRW has used tweet in-
formation already, and leaves limited room
for improvement for the sentence compres-
sion model when using the same source of
information.
</bodyText>
<footnote confidence="0.995622">
6Significance throughout the paper is computed by two
tailed t-test and reported when P &lt; 0.05.
</footnote>
<page confidence="0.998156">
53
</page>
<figureCaption confidence="0.959379">
Figure 1: The influence of α and β. Solid lines are used for approaches based on LexRank; Dotted lines
are used for HGRW based approaches.
</figureCaption>
<table confidence="0.998422153846154">
Method Example 1 Example 2
LexRank Boston bombing suspect Tamerlan Tsarnaev, Three people were hospitalized in critical condition,
killed in a shootout with police days after the according to information provided by hospitals
blast, has been buried at an undisclosed who reported receiving patients from the blast.
location, police in Worcester, Mass., said.
LexRank+SC suspect Tamerlan Tsarnaev, killed in a Three people were hospitalized,
shootout after the blast, has been buried at an according to information provided by hospitals
location, police in Worcester Mass. said. who reported receiving from the blast.
LexRank+SC+both Boston bombing suspect Tamerlan Tsarnaev, Three people were hospitalized in critical condition,
killed in a shootout after the blast, has been according to information provided by hospitals.
buried at an location police said.
Ground Truth Boston bombing suspect Tamerlan Tsarnaev Hospitals report three people in critical condition
has been buried at an undisclosed location
</table>
<tableCaption confidence="0.999919">
Table 3: Example highlight sentences from different systems
</tableCaption>
<figure confidence="0.99867221875">
0.30
0.30
ROUGE−1 F score
0.29
0.28
0.27
● LexRank
LexRank+SC
LexRank+SC+both
HGRW
HGRW+SC
HGRW+SC+both
ROUGE−1 F score
0.29
0.28
0.27
● LexRank
LexRank+SC
LexRank+SC+both
HGRW
HGRW+SC
HGRW+SC+both
0.26
0.25
0.0 0.2 0.4 0.6 0.8 1.0
α
(a) Impact of α
0.26
0.25
0.0 0.2 0.4 0.6 0.8 1.0
β
(b) Impact of β
</figure>
<listItem confidence="0.9007235">
• By incorporating tweet information for both
sentence selection and compression, the per-
formance of HGRW+SC+both outperforms
LexRank significantly.
</listItem>
<bodyText confidence="0.9991482">
Table 3 shows some examples. As we can see
in Example 1, with the help of tweet informa-
tion, our compression model keeps the valuable
part “Boston bombing” for summarization while
the generic one abandons it.
We also investigate the influence of α and β. To
study the impact of α, we fix β to 0.8, and vice
versa. As shown in Figure 1, it is clear that larger
α or β, i.e., giving higher weights to tweets related
information, is generally helpful.
</bodyText>
<sectionHeader confidence="0.993542" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999964809523809">
In this paper, we showed that the relevant tweet
collection of a news article can guide the process
of sentence compression to generate better story
highlights. We extended a dependency-tree based
sentence compression model to incorporate tweet
information. The experiment results on a public
corpus that contains both news articles and rele-
vant tweets showed the effectiveness of our ap-
proach. With the popularity of Twitter and increas-
ing interaction between social media and news
media, such parallel data containing news and re-
lated tweets is easily available, making our ap-
proach feasible to be used in a real system.
There are some interesting future directions.
For example, we can explore more effective ways
to incorporate tweets for sentence compression;
we can study joint models to combine both sen-
tence extraction and compression with the help of
relevant tweets; it will also be interesting to use the
parallel dataset of the news articles and the tweets
for timeline generation for a specific event.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99991825">
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts
of this paper. The work is partially supported
by NSF award IIS-0845484 and DARPA Contract
No. FA8750-13-2-0041. Any opinions, findings,
and conclusions or recommendations expressed
are those of the authors and do not necessarily re-
flect the views of the funding agencies.
</bodyText>
<page confidence="0.997731">
54
</page>
<sectionHeader confidence="0.996197" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999584982300885">
YLlias Chali and Sadid A Hasan. 2012. On the effec-
tiveness of using sentence compression models for
query-focused multi-document summarization. In
Proceedings of the 25th International Conference on
Computational Linguistics, pages 457–474.
James Clarke and Mirella Lapata. 2006. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computa-
tional Linguistics, pages 377–384. Association for
Computational Linguistics.
G¨unes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of Artificial Intelligence
Research, 22:457–479.
Katja Filippova and Yasemin Altun. 2013. Overcom-
ing the lack of parallel data in sentence compression.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1481–1491. Association for Computational Linguis-
tics.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, pages 25–32. Association
for Computational Linguistics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 885–893. Association for Computa-
tional Linguistics.
Wei Gao, Peng Li, and Kareem Darwish. 2012. Joint
topic modeling for event summarization across news
and social media streams. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, pages 1173–1182.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of The 7th National Confer-
ence on Artificial Intelligence, pages 703–710.
Alok Kothari, Walid Magdy, Ahmed Mourad Ka-
reem Darwish, and Ahmed Taei. 2013. Detecting
comments on news articles in microblogs. In Pro-
ceedings of The 7th International AAAI Conference
on Weblogs and Social Media, pages 293–302.
Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013.
Document summarization via guided sentence com-
pression. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 490–500. Association for Computational
Linguistics.
Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang
Weng. 2014. Improving multi-documents summa-
rization by sentence compression based on expanded
constituent parse trees. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 691–701. Association for
Computational Linguistics.
Chin-Yew Lin. 2003. Improving summarization per-
formance by sentence compression: a pilot study. In
Proceedings of the sixth international workshop on
Information retrieval with Asian languages-Volume
11, pages 1–8. Association for Computational Lin-
guistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1492–1502.
Association for Computational Linguistics.
Stefan Riezler, Tracy H King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
pages 118–125. Association for Computational Lin-
guistics.
Tadej ˇStajner, Bart Thomee, Ana-Maria Popescu,
Marco Pennacchiotti, and Alejandro Jaimes. 2013.
Automatic selection of social media responses to
news. In Proceedings of the 19th ACM International
Conference on Knowledge Discovery and Data Min-
ing, pages 50–58. ACM.
Ilija Subaˇsi´c and Bettina Berendt. 2011. Peddling or
creating? investigating the role of twitter in news
reporting. In Advances in Information Retrieval,
pages 207–213. Springer.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1384–1394. Association
for Computational Linguistics.
Zhongyu Wei and Wei Gao. 2014. Utilizing microblog
for automatic news highlights extraction. In Pro-
ceedings of the 25th International Conference on
Computational Linguistics, pages 872–883.
Zhongyu Wei and Wei Gao. 2015. Gibberish, assis-
tant, or master? using tweets linking to news for
extractive single-document summarization. In Pro-
ceedings of the 38th International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval.
</reference>
<page confidence="0.978263">
55
</page>
<reference confidence="0.998917">
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing
He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.
2011. Comparing twitter and traditional media us-
ing topic models. In Advances in Information Re-
trieval, pages 338–349. Springer.
</reference>
<page confidence="0.998423">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.723821">
<title confidence="0.9982805">Using Tweets to Help Sentence Compression for News Highlights Generation</title>
<author confidence="0.999975">Yang Chen Wei</author>
<affiliation confidence="0.988004">Science Department, The University of Texas at</affiliation>
<address confidence="0.808209">Richardson, Texas 75080,</address>
<affiliation confidence="0.962932">Computing Research Institute, Hamad Bin Khalifa University, Doha,</affiliation>
<abstract confidence="0.996763294117647">We explore using relevant tweets of a given news article to help sentence compression for generating compressive news highlights. We extend an unsupervised dependency-tree based sentence compression approach by incorporating tweet information to weight the tree edge in terms of informativeness and syntactic importance. The experimental results on a public corpus that contains both news articles and relevant tweets show that our proposed tweets guided sentence compression method can improve the summarization performance significantly compared to the baseline generic sentence compression method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>YLlias Chali</author>
<author>Sadid A Hasan</author>
</authors>
<title>On the effectiveness of using sentence compression models for query-focused multi-document summarization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics,</booktitle>
<pages>457--474</pages>
<contexts>
<context position="1838" citStr="Chali and Hasan, 2012" startWordPosition="269" endWordPosition="272">y reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus proposed a summary guided compression method to address this pr</context>
<context position="15626" citStr="Chali and Hasan, 2012" startWordPosition="2557" endWordPosition="2560">ors are used. We have several findings. • The tweets involved sentence extraction model HGRW can improve LexRank by 8.8% relatively in terms of ROUGE-1 F score, showing the effectiveness of relevant tweets for sentence selection. • With generic sentence compression, the ROUGE-1 F scores for both LexRank and HGRW drop, mainly because of a much lower recall score. This indicates that generic sentence compression without certain guidance removes salient content of the original sentence that may be important for summarization and thus hurts the performance. This is consistent with the finding of (Chali and Hasan, 2012). • By adding either wTinfo or wTsyn, the performance of summarization increases, showing that relevant tweets can be used to help the scores of both informativeness and syntactic importance. • +SC+both improves the summarization performance significantly6 compared to the corresponding compressive summarization baseline +SC, and outperforms the corresponding original baseline, LexRank and HGRW. • The improvement obtained by LexRank+SC+both compared to LexRank is more promising than that obtained by HGRW+SC+both compared to HGRW. This may be because HGRW has used tweet information already, and </context>
</contexts>
<marker>Chali, Hasan, 2012</marker>
<rawString>YLlias Chali and Sadid A Hasan. 2012. On the effectiveness of using sentence compression models for query-focused multi-document summarization. In Proceedings of the 25th International Conference on Computational Linguistics, pages 457–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for sentence compression: A comparison across domains, training requirements and evaluation measures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>377--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7011" citStr="Clarke and Lapata, 2006" startWordPosition="1101" endWordPosition="1105">tance x, idfw is the inverse document frequency of word w in the dataset. In our task, each sentence or tweet is treated as a document to compute the IDF value. Although both types of nodes can be ranked in this framework, we only output the top news sentences as the highlights, and the input to the subsequent compression component. 2.2 Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1, w2,..., wm, identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The goal is to find a subtree with the highest score: f(X) = � xe × winfo(e) × </context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Models for sentence compression: A comparison across domains, training requirements and evaluation measures. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 377–384. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="5151" citStr="Erkan and Radev, 2004" startWordPosition="780" endWordPosition="783">syntactic importance to weight the tree edge. We evaluate our method on a public corpus that contains both news articles and relevant tweets. The result shows that generic compression hurts the performance of highlights generation, while sentence compression guided by relevant tweets of the news article can improve the performance. 2 Framework We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 Tweets Involved Sentence Extraction We use LexRank (Erkan and Radev, 2004) as the baseline to select the salient sentences in a news article. This baseline is an unsupervised extractive summarization approach and has been proved to be effective for the summarization task. Besides LexRank, we also use Heterogeneous Graph Random Walk (HGRW) (Wei and Gao, 2015) to incorporate relevant tweet information to extract news sentences. In this model, an undirected similarity graph is created, similar to LexRank. However, the graph is heterogeneous, with two types of nodes for the news sentences and tweets respectively. Suppose we have a sentence set S and a tweet set T. By co</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of Artificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Yasemin Altun</author>
</authors>
<title>Overcoming the lack of parallel data in sentence compression.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1481--1491</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8506" citStr="Filippova and Altun, 2013" startWordPosition="1374" endWordPosition="1378">s the informativeness (winfo(e)). Suppose edge a is pointed from head h to node n with dependency label l, both weights can be computed from a background news corpus as: Psummary(n) winfo(e) = (4) Particle(n) ⎡ ⎤ X sim(s, n) +(1 − d) ⎣(1 − �) Pv∈S\{s} sim(s, v) p(n) ⎦ n∈S\{s} ⎡ ⎤ d sim(s, m) (1) 51 wsyn(e) = P(l|h) (5) where Psummary(n) and Particle(n) are the unigram probabilities of word n in the two language models trained on human generated summaries and the original articles respectively. P(l|h) is the conditional probability of label l given head h. Note that here we use the formula in (Filippova and Altun, 2013) for winfo(e), which was shown to be more effective for sentence compression than the original formula in (Filippova and Strube, 2008). The optimization problem can be solved under the tree structure and length constraints by integer linear programming1. Given that L is the maximum number of words permitted for the compression, the length constraint is simply represented as: � xe G L (6) e∈E The surface realizatdion is standard: the words in the compression subtree are put in the same order they are found in the source sentence. Due to space limit, we refer readers to (Filippova and Strube, 20</context>
</contexts>
<marker>Filippova, Altun, 2013</marker>
<rawString>Katja Filippova and Yasemin Altun. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1481–1491. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Dependency tree based sentence compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6864" citStr="Filippova and Strube, 2008" startWordPosition="1076" endWordPosition="1079">ity: Pw∈x,y tfw,xtfw,y(idfw)2 sim(x, y) =qP qP wi∈x (tfwi,xidfwi)2 × wi∈y (tfwi,yidfwi)2 (2) where tfw,x is the number of occurrences of word w in instance x, idfw is the inverse document frequency of word w in the dataset. In our task, each sentence or tweet is treated as a document to compute the IDF value. Although both types of nodes can be ranked in this framework, we only output the top news sentences as the highlights, and the input to the subsequent compression component. 2.2 Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1, w2,..., wm, identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then</context>
<context position="8640" citStr="Filippova and Strube, 2008" startWordPosition="1397" endWordPosition="1400">ed from a background news corpus as: Psummary(n) winfo(e) = (4) Particle(n) ⎡ ⎤ X sim(s, n) +(1 − d) ⎣(1 − �) Pv∈S\{s} sim(s, v) p(n) ⎦ n∈S\{s} ⎡ ⎤ d sim(s, m) (1) 51 wsyn(e) = P(l|h) (5) where Psummary(n) and Particle(n) are the unigram probabilities of word n in the two language models trained on human generated summaries and the original articles respectively. P(l|h) is the conditional probability of label l given head h. Note that here we use the formula in (Filippova and Altun, 2013) for winfo(e), which was shown to be more effective for sentence compression than the original formula in (Filippova and Strube, 2008). The optimization problem can be solved under the tree structure and length constraints by integer linear programming1. Given that L is the maximum number of words permitted for the compression, the length constraint is simply represented as: � xe G L (6) e∈E The surface realizatdion is standard: the words in the compression subtree are put in the same order they are found in the source sentence. Due to space limit, we refer readers to (Filippova and Strube, 2008) for a detailed description of the baseline method. 2.3 Leverage Tweets for Edge Weighting We then extend the dependency-tree based</context>
<context position="13819" citStr="Filippova and Strube, 2008" startWordPosition="2271" endWordPosition="2274">28.1 22.6 39.5 100 HGRW + SC 26.4 24.9 29.5 66.1 HGRW + SC+wT 27.5 25.7 30.8 65.4 info HGRW + SC+wT 27.0 25.3 30.2 66.7 syn HGRW + SC+both 28.4 26.9 31.2 64.8 Table 2: Overall Performance. Bold: the best value in each group in terms of different metrics. Following (Wei and Gao, 2014), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores (Lin, 2004) using human-generated highlights as the reference. The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, following (Filippova and Strube, 2008). We empirically use 0.8 for α, β and c such that tweets have more impact for both sentence selection and compression. We leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated summaries. The Stanford Parser4 is used to obtain dependency trees. The background tweet corpus is collected from Twitter public timeline via Twitter API, and contains more than 50 million tweets. 3.2 Results Table 2 shows the overall performance5. For summaries generated by both LexRank and HGRW, “+SC” means ge</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Dependency tree based sentence compression. In Proceedings of the Fifth International Natural Language Generation Conference, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitrios Galanis</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>An extractive supervised two-stage method for sentence compression. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>885--893</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1815" citStr="Galanis and Androutsopoulos, 2010" startWordPosition="265" endWordPosition="268">f the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus proposed a summary guided compression me</context>
</contexts>
<marker>Galanis, Androutsopoulos, 2010</marker>
<rawString>Dimitrios Galanis and Ion Androutsopoulos. 2010. An extractive supervised two-stage method for sentence compression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 885–893. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Gao</author>
<author>Peng Li</author>
<author>Kareem Darwish</author>
</authors>
<title>Joint topic modeling for event summarization across news and social media streams.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>1173--1182</pages>
<contexts>
<context position="3091" citStr="Gao et al., 2012" startWordPosition="466" endWordPosition="469"> their method. But this approach relied heavily on the training data, thus has the limitation of domain generalization. Instead of using a manually generated corpus, we investigate using existing external sources to guide sentence compression for the purpose of compressive news highlights generation. Nowadays it becomes more and more common that users share interesting news content via Twitter together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇStajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the o</context>
</contexts>
<marker>Gao, Li, Darwish, 2012</marker>
<rawString>Wei Gao, Peng Li, and Kareem Darwish. 2012. Joint topic modeling for event summarization across news and social media streams. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, pages 1173–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of The 7th National Conference on Artificial Intelligence,</booktitle>
<pages>703--710</pages>
<contexts>
<context position="1769" citStr="Knight and Marcu, 2000" startWordPosition="259" endWordPosition="262">aders to quickly capture the gist of the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013)</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization-step one: Sentence compression. In Proceedings of The 7th National Conference on Artificial Intelligence, pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alok Kothari</author>
<author>Walid Magdy</author>
<author>Ahmed Mourad Kareem Darwish</author>
<author>Ahmed Taei</author>
</authors>
<title>Detecting comments on news articles in microblogs.</title>
<date>2013</date>
<booktitle>In Proceedings of The 7th International AAAI Conference on Weblogs and Social Media,</booktitle>
<pages>293--302</pages>
<contexts>
<context position="3113" citStr="Kothari et al., 2013" startWordPosition="470" endWordPosition="473"> this approach relied heavily on the training data, thus has the limitation of domain generalization. Instead of using a manually generated corpus, we investigate using existing external sources to guide sentence compression for the purpose of compressive news highlights generation. Nowadays it becomes more and more common that users share interesting news content via Twitter together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇStajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the original sentences for </context>
</contexts>
<marker>Kothari, Magdy, Darwish, Taei, 2013</marker>
<rawString>Alok Kothari, Walid Magdy, Ahmed Mourad Kareem Darwish, and Ahmed Taei. 2013. Detecting comments on news articles in microblogs. In Proceedings of The 7th International AAAI Conference on Weblogs and Social Media, pages 293–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Yang Liu</author>
</authors>
<title>Document summarization via guided sentence compression.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>490--500</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1874" citStr="Li et al., 2013" startWordPosition="277" endWordPosition="280">light sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus proposed a summary guided compression method to address this problem and showed the effectiveness o</context>
</contexts>
<marker>Li, Liu, Weng, Liu, 2013</marker>
<rawString>Chen Li, Fei Liu, Fuliang Weng, and Yang Liu. 2013. Document summarization via guided sentence compression. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 490–500. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Li</author>
<author>Yang Liu</author>
<author>Fei Liu</author>
<author>Lin Zhao</author>
<author>Fuliang Weng</author>
</authors>
<title>Improving multi-documents summarization by sentence compression based on expanded constituent parse trees.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>691--701</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1912" citStr="Li et al., 2014" startWordPosition="285" endWordPosition="288">r than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus proposed a summary guided compression method to address this problem and showed the effectiveness of their method. But this approach reli</context>
</contexts>
<marker>Li, Liu, Liu, Zhao, Weng, 2014</marker>
<rawString>Chen Li, Yang Liu, Fei Liu, Lin Zhao, and Fuliang Weng. 2014. Improving multi-documents summarization by sentence compression based on expanded constituent parse trees. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 691–701. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression: a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume 11,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1780" citStr="Lin, 2003" startWordPosition="263" endWordPosition="264"> the gist of the document, and can dramatically reduce reader’s information load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus propo</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Chin-Yew Lin. 2003. Improving summarization performance by sentence compression: a pilot study. In Proceedings of the sixth international workshop on Information retrieval with Asian languages-Volume 11, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="13581" citStr="Lin, 2004" startWordPosition="2235" endWordPosition="2236">ROUGE-1 Compr. Rate(%) F(%) P(%) R(%) LexRank 26.1 19.9 39.1 100 LexRank + SC 25.2 22.4 29.6 63.0 LexRank + SC+wTinfo 25.7 22.8 30.1 62.0 LexRank + SC+wT 26.2 23.5 30.4 63.7 syn LexRank + SC+both 27.5 25.0 31.4 61.5 HGRW 28.1 22.6 39.5 100 HGRW + SC 26.4 24.9 29.5 66.1 HGRW + SC+wT 27.5 25.7 30.8 65.4 info HGRW + SC+wT 27.0 25.3 30.2 66.7 syn HGRW + SC+both 28.4 26.9 31.2 64.8 Table 2: Overall Performance. Bold: the best value in each group in terms of different metrics. Following (Wei and Gao, 2014), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores (Lin, 2004) using human-generated highlights as the reference. The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, following (Filippova and Strube, 2008). We empirically use 0.8 for α, β and c such that tweets have more impact for both sentence selection and compression. We leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated summaries. The Stanford Parser4 is used to obtain dependency trees. The bac</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Fast joint compression and summarization via graph cuts.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1492--1502</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1894" citStr="Qian and Liu, 2013" startWordPosition="281" endWordPosition="284"> usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus proposed a summary guided compression method to address this problem and showed the effectiveness of their method. But </context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Fast joint compression and summarization via graph cuts. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1492–1502. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Annie Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>118--125</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6942" citStr="Riezler et al., 2003" startWordPosition="1089" endWordPosition="1092">wi)2 (2) where tfw,x is the number of occurrences of word w in instance x, idfw is the inverse document frequency of word w in the dataset. In our task, each sentence or tweet is treated as a document to compute the IDF value. Although both types of nodes can be ranked in this framework, we only output the top news sentences as the highlights, and the input to the subsequent compression component. 2.2 Dependency Tree Based Sentence Compression We use an unsupervised dependency tree based compression framework (Filippova and Strube, 2008) as our baseline. This method achieved a higher F-score (Riezler et al., 2003) than other systems on the Edinburgh corpus (Clarke and Lapata, 2006). We will introduce the baseline in this part and describe our extended model that leverages tweet information in the next subsection. The sentence compression task can be defined as follows: given a sentence s, consisting of words w1, w2,..., wm, identify a subset of the words of s, such that it is grammatical and preserves essential information of s. In the baseline framework, a dependency graph for an original sentence is first generated and then the compression is done by deleting edges of the dependency graph. The goal i</context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Stefan Riezler, Tracy H King, Richard Crouch, and Annie Zaenen. 2003. Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 118–125. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tadej ˇStajner</author>
<author>Bart Thomee</author>
<author>Ana-Maria Popescu</author>
<author>Marco Pennacchiotti</author>
<author>Alejandro Jaimes</author>
</authors>
<title>Automatic selection of social media responses to news.</title>
<date>2013</date>
<booktitle>In Proceedings of the 19th ACM International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>50--58</pages>
<publisher>ACM.</publisher>
<marker>ˇStajner, Thomee, Popescu, Pennacchiotti, Jaimes, 2013</marker>
<rawString>Tadej ˇStajner, Bart Thomee, Ana-Maria Popescu, Marco Pennacchiotti, and Alejandro Jaimes. 2013. Automatic selection of social media responses to news. In Proceedings of the 19th ACM International Conference on Knowledge Discovery and Data Mining, pages 50–58. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilija Subaˇsi´c</author>
<author>Bettina Berendt</author>
</authors>
<title>Peddling or creating? investigating the role of twitter in news reporting.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>207--213</pages>
<publisher>Springer.</publisher>
<marker>Subaˇsi´c, Berendt, 2011</marker>
<rawString>Ilija Subaˇsi´c and Bettina Berendt. 2011. Peddling or creating? investigating the role of twitter in news reporting. In Advances in Information Retrieval, pages 207–213. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lu Wang</author>
<author>Hema Raghavan</author>
<author>Vittorio Castelli</author>
<author>Radu Florian</author>
<author>Claire Cardie</author>
</authors>
<title>A sentence compression based framework to query-focused multidocument summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1384--1394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1857" citStr="Wang et al., 2013" startWordPosition="273" endWordPosition="276">mation load. A highlight sentence is usually much shorter than its original corresponding news sentence; therefore applying extractive summarization methods directly to sentences in a news article is not enough to generate high quality highlights. Sentence compression aims to retain the most important information of an original sentence in a shorter form while being grammatical at the same time. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Lin, 2003; Galanis and Androutsopoulos, 2010; Chali and Hasan, 2012; Wang et al., 2013; Li et al., 2013; Qian and Liu, 2013; Li et al., 2014). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. Prior studies have mostly used the generic sentence compression approaches, however, a generic compression system may not be the best fit for the summarization purpose because it does not take into account the summarization task in the compression module. Li et al. (2013) thus proposed a summary guided compression method to address this problem and showed th</context>
</contexts>
<marker>Wang, Raghavan, Castelli, Florian, Cardie, 2013</marker>
<rawString>Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Florian, and Claire Cardie. 2013. A sentence compression based framework to query-focused multidocument summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1384–1394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongyu Wei</author>
<author>Wei Gao</author>
</authors>
<title>Utilizing microblog for automatic news highlights extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics,</booktitle>
<pages>872--883</pages>
<contexts>
<context position="3485" citStr="Wei and Gao, 2014" startWordPosition="532" endWordPosition="535">ter together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇStajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the original sentences for extractive summarization in a fashion of supervised machine learning. Wei and Gao (2015) proposed a graph-based approach to simultaneously rank the 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, Beijing, China, July 26-31, 20</context>
<context position="10813" citStr="Wei and Gao, 2014" startWordPosition="1758" endWordPosition="1761">core is: wTsyn(e) = NT(h,n) (10) NT NT (h, n) is the number of tweets where n and head h appear together within a window frame of K, and NT is the total number of tweets in the relevant tweet collection. Since tweets are always noisy and informal, traditional parsers are not reliable to extract dependency trees. Therefore, we use co-occurrence as pseudo syntactic information here. Note wNinfo(e), wTinfo(e), wNsyn(e) and wTsyn(e) are normalized before combination. 3 Experiment 3.1 Setup We evaluate our pipeline news highlights generation framework on a public corpus based on CNN/USAToday news (Wei and Gao, 2014). This corpus was constructed via an event-oriented strategy following four steps: 1) 17 salient news events taking place in 2013 and 2014 were manually identified. 2) For each event, relevant tweets were retrieved via Topsy2 search API using a set of manually generated core queries. 3) News articles explicitly linked by URLs embedded in the tweets were collected. 4) News articles from CNN/USAToday that have more than 100 explicitly linked tweets were kept. The resulting corpus contains 121 documents, 455 highlights and 78,419 linking tweets. We used tweets explicitly linked to a news article </context>
<context position="13476" citStr="Wei and Gao, 2014" startWordPosition="2215" endWordPosition="2218">,419 6,890,987 Table 1: Distribution of documents, highlights and tweets with respect to different events Method ROUGE-1 Compr. Rate(%) F(%) P(%) R(%) LexRank 26.1 19.9 39.1 100 LexRank + SC 25.2 22.4 29.6 63.0 LexRank + SC+wTinfo 25.7 22.8 30.1 62.0 LexRank + SC+wT 26.2 23.5 30.4 63.7 syn LexRank + SC+both 27.5 25.0 31.4 61.5 HGRW 28.1 22.6 39.5 100 HGRW + SC 26.4 24.9 29.5 66.1 HGRW + SC+wT 27.5 25.7 30.8 65.4 info HGRW + SC+wT 27.0 25.3 30.2 66.7 syn HGRW + SC+both 28.4 26.9 31.2 64.8 Table 2: Overall Performance. Bold: the best value in each group in terms of different metrics. Following (Wei and Gao, 2014), we output 4 sentences for each news article as the highlights and report the ROUGE-1 scores (Lin, 2004) using human-generated highlights as the reference. The sentence compression rates are set to 0.8 for short sentences containing fewer than 9 words, and 0.5 for long sentences with more than 9 words, following (Filippova and Strube, 2008). We empirically use 0.8 for α, β and c such that tweets have more impact for both sentence selection and compression. We leveraged The New York Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news</context>
</contexts>
<marker>Wei, Gao, 2014</marker>
<rawString>Zhongyu Wei and Wei Gao. 2014. Utilizing microblog for automatic news highlights extraction. In Proceedings of the 25th International Conference on Computational Linguistics, pages 872–883.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongyu Wei</author>
<author>Wei Gao</author>
</authors>
<title>Gibberish, assistant, or master? using tweets linking to news for extractive single-document summarization.</title>
<date>2015</date>
<booktitle>In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="3505" citStr="Wei and Gao, 2015" startWordPosition="536" endWordPosition="539">heir comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇStajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant tweet collection to assist the ranking of the original sentences for extractive summarization in a fashion of supervised machine learning. Wei and Gao (2015) proposed a graph-based approach to simultaneously rank the 50 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 50–56, Beijing, China, July 26-31, 2015. c�2015 Associati</context>
<context position="5437" citStr="Wei and Gao, 2015" startWordPosition="825" endWordPosition="828">e news article can improve the performance. 2 Framework We adopt a pipeline approach for compressive news highlights generation. The framework integrates a sentence extraction component and a post-sentence compression component. Each is described below. 2.1 Tweets Involved Sentence Extraction We use LexRank (Erkan and Radev, 2004) as the baseline to select the salient sentences in a news article. This baseline is an unsupervised extractive summarization approach and has been proved to be effective for the summarization task. Besides LexRank, we also use Heterogeneous Graph Random Walk (HGRW) (Wei and Gao, 2015) to incorporate relevant tweet information to extract news sentences. In this model, an undirected similarity graph is created, similar to LexRank. However, the graph is heterogeneous, with two types of nodes for the news sentences and tweets respectively. Suppose we have a sentence set S and a tweet set T. By considering the similarity between the same type of nodes and cross types, the score of a news sentence s is computed as follows: where N and M are the size of S and T, respectively, d is a damping factor, sim(x, y) is the similarity function, and the parameter E is used to control the c</context>
<context position="14578" citStr="Wei and Gao, 2015" startWordPosition="2392" endWordPosition="2395">ork Times Annotated Corpus (LDC Catalog No: LDC2008T19) as the background news corpus. It has both the original news articles and human generated summaries. The Stanford Parser4 is used to obtain dependency trees. The background tweet corpus is collected from Twitter public timeline via Twitter API, and contains more than 50 million tweets. 3.2 Results Table 2 shows the overall performance5. For summaries generated by both LexRank and HGRW, “+SC” means generic sentence compression base4http://nlp.stanford.edu/software/ lex-parser.shtml 5The performance of HGRW reported here is different from (Wei and Gao, 2015) because the setup is different. We use all the explicitly linked tweets in the ranking process here without considering redundancy while a redundancy filtering process was applied in (Wei and Gao, 2015) . line (Section. 2.2) is used, “+wT info”and “+wT syn” indicate tweets are used to help edge weighting for sentence compression in terms of informativeness and syntactic importance respectively, and “+both” means both factors are used. We have several findings. • The tweets involved sentence extraction model HGRW can improve LexRank by 8.8% relatively in terms of ROUGE-1 F score, showing the e</context>
</contexts>
<marker>Wei, Gao, 2015</marker>
<rawString>Zhongyu Wei and Wei Gao. 2015. Gibberish, assistant, or master? using tweets linking to news for extractive single-document summarization. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Jianshu Weng</author>
<author>Jing He</author>
<author>Ee-Peng Lim</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Comparing twitter and traditional media using topic models.</title>
<date>2011</date>
<booktitle>In Advances in Information Retrieval,</booktitle>
<pages>338--349</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3044" citStr="Zhao et al., 2011" startWordPosition="458" endWordPosition="461">ess this problem and showed the effectiveness of their method. But this approach relied heavily on the training data, thus has the limitation of domain generalization. Instead of using a manually generated corpus, we investigate using existing external sources to guide sentence compression for the purpose of compressive news highlights generation. Nowadays it becomes more and more common that users share interesting news content via Twitter together with their comments. The availability of cross-media information provides new opportunities for traditional tasks of Natural Language Processing (Zhao et al., 2011; Subaˇsi´c and Berendt, 2011; Gao et al., 2012; Kothari et al., 2013; ˇStajner et al., 2013). In this paper, we propose to use relevant tweets of a news article to guide the sentence compression process in a pipeline framework for generating compressive news highlights. This is a pioneer study for using such parallel data to guide sentence compression for document summarization. Our work shares some similar ideas with (Wei and Gao, 2014; Wei and Gao, 2015). They also attempted to use tweets to help news highlights generation. Wei and Gao (2014) derived external features based on the relevant </context>
</contexts>
<marker>Zhao, Jiang, Weng, He, Lim, Yan, Li, 2011</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011. Comparing twitter and traditional media using topic models. In Advances in Information Retrieval, pages 338–349. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>