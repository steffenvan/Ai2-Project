<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030747">
<note confidence="0.808388666666667">
Proceedings of the Workshop on Speech-to-Speech Translation:
Algorithms and Systems, Philadelphia, July 2002, pp. 31-38.
Association for Computational Linguistics.
</note>
<title confidence="0.999196">
Speech Translation Performance of Statistical Dependency Transduction
and Semantic Similarity Transduction
</title>
<author confidence="0.74259">
Hiyan Alshawi and Shona Douglas
</author>
<affiliation confidence="0.528354">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.574404">
Florham Park, NJ 07932, USA
</address>
<email confidence="0.995146">
hiyan,shona @research.att.com
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999602117647059">
In this paper we compare the performance
of two methods for speech translation.
One is a statistical dependency transduc-
tion model using head transducers, the
other a case-based transduction model in-
volving a lexical similarity measure. Ex-
amples of translated utterance transcrip-
tions are used in training both models,
though the case-based model also uses se-
mantic labels classifying the source utter-
ances. The main conclusion is that while
the two methods provide similar transla-
tion accuracy under the experimental con-
ditions and accuracy metric used, the sta-
tistical dependency transduction method
is significantly faster at computing trans-
lations.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999558551020408">
Machine translation, natural language processing,
and more generally other computational problems
that are not amenable to closed form solutions,
have typically been tackled by one of three broad
approaches: rule-based systems, statistical mod-
els (including generative models), and case-based
systems. Hybrid solutions combining these ap-
proaches have also been used in language pro-
cessing generally (Klavans and Resnik, 1996) and
more specifically in machine translation (for exam-
ple Frederking et al. (1994)).
In this paper we compare the performance of two
methods for speech translation. One is the statistical
dependency transduction model (Alshawi and Dou-
glas, 2000; Alshawi et al., 2000b), a trainable gener-
ative statistical translation model using head trans-
ducers (Alshawi, 1996). The other is a case-based
transduction model which makes use of a semantic
similarity measure between words. Both models are
trained automatically using examples of translated
utterances (the transcription of a spoken utterance
and a translation of that transcription). The case-
based model makes use of additional information in
the form of labels associated with source language
utterances, typically one or two labels per utterance.
This additional information, which was originally
provided for a separate monolingual task, is used to
construct the lexical similarity measure.
In training these translation methods, as well as
their runtime application, no pre-existing bilingual
lexicon is needed. Instead, in both cases, the initial
phase of training from the translation data is a sta-
tistical hierarchical alignment search applied to the
set of bilingual examples. This training phase pro-
duces a bilingual lexicon, used by both methods, as
well as synchronized hierarchical alignments used to
build the dependency transduction model.
In the experiments comparing the performance
of the models we look at accuracy as well as the
time taken to translate sentences from English to
Japanese. The source language inputs used in these
experiments are naturally spoken utterances from
large numbers of real customers calling telephone
operator services.
In section 2 we describe the hierarchical align-
ment algorithm followed by descriptions of the
translation methods in sections 3 and 4. We present
the experiments in section 5 and provide concluding
remarks in section 6.
</bodyText>
<figureCaption confidence="0.9202625">
Figure 1: Alignment mapping , source head-map
, and target head-map
</figureCaption>
<sectionHeader confidence="0.978233" genericHeader="method">
2 Hierarchical alignments
</sectionHeader>
<bodyText confidence="0.9994645">
Both the translation systems described in this pa-
per make use of automatically created hierarchical
alignments of the source and target strings of the
training corpus bitexts. As will be described in sec-
tion 3, we estimate the parameters of a dependency
transduction model from such alignments. In the
case-based method described in section 4, the align-
ments are the basis for the translation lexicon used
to compute substitutions and word-for-word transla-
tions.
A hierarchical alignment consists of four func-
tions. The first two functions are an alignment
mapping from source words to target words
(which may be the empty word), and an in-
verse alignment mapping from target words to
source words . (The inverse mapping is needed
to handle mapping of target words to; it coincides
with for pairs without.) The other two functions
are a source head-map mapping source dependent
words to their heads in the source string,
and a target head-map mapping target dependent
words to their head words in the target string.
An example hierarchical alignment is shown in Fig-
ure 1.
A hierarchical alignment is synchronized (i.e.
corresponds to synchronized dependency trees) if,
roughly speaking, induces an isomorphism be-
tween the dependency functions and (see
Alshawi and Douglas (2000) for a more formal def-
inition). The hierarchical alignment in Figure 1 is
synchronized.
In some previous work (Alshawi et al., 1998; Al-
shawi et al., 2000a; Alshawi et al., 2000b) the train-
ing method constructs synchronized alignments in
which each head word has at most two dependent
phrases. Here we use the technique described by
Alshawi and Douglas (2000) where the models have
greater freedom to vary the granularity of phrase lo-
cality.
Constructing synchronized hierarchical align-
ments for a corpus has two stages: (a) computing
co-occurrence statistics from the training data; (b)
searching for an optimal synchronized hierarchical
alignment for each bitext.
</bodyText>
<subsectionHeader confidence="0.987532">
2.1 Word correlation statistics
</subsectionHeader>
<bodyText confidence="0.999733970588235">
For each source word in the dataset, a translation
pairing cost is assigned for all possible
translations in the context of a bitext. Here and
are usually words, but may also be the empty word
or compounds formed from contiguous words; here
we restrict compounds to a maximum length of two
words.
The assignment of these lexical translation pair-
ing costs may be done using various statistical mea-
sures. The main component of is the so-called
correlation measure (see Gale and Church (1991))
normalized to the range with indicating per-
fect correlation. In the experiments described in this
paper, the cost function relating a source word (or
compound) in a bitext with a target word (or com-
pound) is
where is a length-normalized measure of
the apparent distortion in the positions of and in
the source and target strings of. For example, if
appears at the middle of the source string and ap-
pears at the middle of the target string, then the dis-
tortion is . We have found that, at least for our data,
this pairing cost leads to better performance than the
use of log probabilities of target words given source
words (cf. Brown et al. (1993)).
The value used for is first computed from
counts of the number of bitexts in the training set in
which and co-occur, in which only appears, in
which only appears, and in which neither of them
appear. In other words, we first treat any word in
the target string to be a possible translation of any
word in the source string. This value is then refined
by re-estimation during the alignment optimization
process.
</bodyText>
<subsectionHeader confidence="0.999081">
2.2 Optimal hierarchical alignments
</subsectionHeader>
<bodyText confidence="0.999904131578947">
We wish to find a hierarchical alignment that re-
spects the co-occurrence statistics of bitexts as well
as the phrasal structure implicit in the source and tar-
get strings. For this purpose we define the cost of a
hierarchical subalignment to be the sum of the costs
of each pairing , where is the
(sub)alignment mapping function.
The complete hierarchical alignment which min-
imizes this cost function is computed using a dy-
namic programming procedure. This procedure
works bottom-up, starting with all possible sub-
alignments with at most one source word (or com-
pound) and one target word (or compound). Adja-
cent source substrings are then combined to deter-
mine the lowest cost subalignments for successively
larger substrings of the bitext satisfying the con-
straints for synchronized alignments stated above.
The successively larger substrings eventually span
the entire source string, yielding the optimal hierar-
chical alignment for the bitext.
At each combination step in the optimization pro-
cedure, one of the two source subphrases is added
as a dependent of the head of the other subphrase.
Since the alignment we are constructing is synchro-
nized, this choice will force the selection of a target
dependent phrase. Our current (admittedly crude)
strategy for selecting the dependent subphrase is to
choose the one with the highest subalignment cost,
i.e. the head of the subphrase with the better sub-
alignment becomes the head of the enlarged phrase.
Recall that the initial estimates for are computed
from co-occurence counts for in bitexts. In the
second and subsequent rounds of this procedure, the
values are computed from co-occurence counts for
in pairings in the alignments produced by the
previous round. The improvement in the models re-
sulting from this re-estimation seems to stabilize af-
ter approximately five to ten rounds.
</bodyText>
<sectionHeader confidence="0.97793" genericHeader="method">
3 Statistical Dependency Transduction
</sectionHeader>
<bodyText confidence="0.999942545454546">
The dependency transduction model is an automati-
cally trainable translation method that models cross-
lingual lexical mapping, hierarchical phrase struc-
ture, and monolingual lexical dependency. It is a
generative statistical model for synchronized pairs
of dependency trees in which each local tree is pro-
duced by a weighted head transducer. Since this
model has been presented at length elsewhere (Al-
shawi, 1996; Alshawi et al., 2000a; Alshawi and
Douglas, 2000), the description in this paper will be
relatively compact.
</bodyText>
<subsectionHeader confidence="0.993573">
3.1 Weighted finite state head transducers
</subsectionHeader>
<bodyText confidence="0.947358447368421">
A weighted finite state head transducer is a finite
state machine that differs from ‘standard’ finite state
transducers in that, instead of consuming the input
string left to right, it consumes it ‘middle out’ from
a symbol in the string. Similarly, the output of a
head transducer is built up middle-out at positions
relative to a symbol in the output string.
Formally, a weighted head transducer is a 5-tuple:
an alphabet of input symbols; an alphabet of
output symbols; a finite set of states ; a
set of final states ; and a finite set of state
transitions. A transition from state to state has
the form
where is a member of or is the empty string;
is a member of or; the integer is the input
position; the integer is the output position; and
the real number is the weight of the transition. The
roles of , , , and in transitions are similar to
the roles they have in left-to-right transducers, i.e. in
transitioning from state to state , the transducer
‘reads’ input symbol and ‘writes’ output symbol
, and as usual if (or ) is then no read (respec-
tively write) takes place for the transition.
To define the role of transition positions and
, we consider notional input (source) and output
(target) tapes divided into squares. On such a tape,
one square is numbered , and the other squares are
numbered rightwards from square , and
leftwards from square . A transition
with input position and output position is in-
terpreted as reading from square on the input
tape and writing to square of the output tape; if
square is already occupied then is written to the
next empty square to the left of if , or to the
right of if , and similarly if input was al-
ready read from position , is taken from the next
unread square to the left of if or to the right
of if .
</bodyText>
<subsectionHeader confidence="0.995274">
3.2 Dependency transduction models
</subsectionHeader>
<bodyText confidence="0.999890164179104">
Dependency transduction models are generative
statistical models which derive synchronized pairs
of dependency trees, a source language dependency
tree and a target dependency tree. A dependency
tree, in the sense of dependency grammar (for exam-
ple Hays (1964), Hudson (1984)), is a tree in which
the words of a sentence appear as nodes; the parent
of a node is its head and the child of a node is the
node’s dependent.
In a dependency transduction model, each syn-
chronized local subtree corresponds to a head trans-
ducer derivation: the head transducer is used to con-
vert a sequence consisting of a head word and its
immediate left and right dependent words to a se-
quence consisting of a target word and immedi-
ate left and right dependent words. (Since the empty
string may appear in a transition in place of a source
or target symbol, the number of source and target de-
pendents can be different.) When applying a depen-
dency transduction model to translation, we choose
the target string obtained by flattening the target tree
of the lowest cost recursive dependency derivation
that also yields the source string.
For a dependency transduction model to be a sta-
tistical model for generating pairs of strings, we as-
sign transition weights that are derived from condi-
tional probabilities. Several probabilistic parameter-
izations can be used for this purpose including the
following for a transition with head words and
and dependent words and :
Here and are the from-state and to-state for the
transition and and are the source and target posi-
tions, as before. We also need parameters
for the probability of choosing an initial head trans-
ducer state given a pair of words heading
a synchronized pair of subtrees. To start the deriva-
tion, we need parameters for the prob-
ability of choosing ,as the root nodes of the
two trees.
These model parameters can be used to generate
pairs of synchronized dependency trees starting with
the topmost nodes of the two trees and proceeding
recursively to the leaves. The probability of such a
derivation can be expressed as:
where is the probability of a subderivation
headed by and , that is
for a derivation in which the dependents of and
are generated by transitions.
The parameters of this probabilistic synchronized
tree derivation model are estimated from the results
of running the hierarchical alignment algorithm de-
scribed in section 2 on the sentence pairs in the train-
ing corpus. For this purpose, each synchronized tree
resulting from the alignment process is assumed to
be derived from a dependency transduction model,
so transition counts for the model are tallied from
the set of synchronized trees. (For further details,
see Alshawi and Douglas (2000).)
To carry out translation with a dependency trans-
duction model, we apply a “middle-out” dynamic
programming search to find the optimal derivation.
This algorithm can take as input either word strings
or word lattices produced by a speech recognizer.
The algorithm is similar to those for context free
parsing such as chart parsing (Earley, 1970) and the
CKY algorithm (Younger, 1967). It is described in
Alshawi et al. (2000b).
</bodyText>
<sectionHeader confidence="0.980482" genericHeader="method">
4 Similarity Cased-Based Transduction
</sectionHeader>
<subsectionHeader confidence="0.987013">
4.1 Training the transduction parameters
</subsectionHeader>
<bodyText confidence="0.999175275362319">
Our semantic similarity transduction method is a
case-based (or example-based) method for transduc-
ing source strings to target strings that makes use of
two different kinds of training data:
A set of source-string, target-string pairs that
are instances of the transduction mapping.
Specifically, transcriptions of spoken utter-
ances in the source language and their transla-
tion into the target language. This is the same
data used for training the dependency trans-
duction model. It is used in this transduction
method to construct a probabilistic bilingual
lexicon, while the source side is used as the set
of examples for matching.
A mapping between the source strings and sub-
sets of a (relatively small) set of classes, or la-
bels. The idea is that the labels give a broad
classification of the meaning of the source
strings, so we will refer to them informally as
“semantic” labels. In our experiments, these
classes correspond to 15 call routing destina-
tions associated with the transcribed utterances.
For the purposes of the case-based method, this
data is used to construct a similarity measure
between words of the source language.
As noted earlier, the alignment algorithm de-
scribed in section 2 is applied to the translation pairs
to yield a set of synchronized dependency trees. Us-
ing the resulting trees, the probabilities of a bilingual
lexicon, i.e.
where is a source language word, and is a tar-
get language word, are estimated from the counts of
synchronized lexical nodes. (Since the synchronized
trees are dependency trees, both paired fringe nodes
and interior nodes are included in the counts.) In this
probabilistic lexicon, may be, the empty symbol,
so source words may have different probabilities of
being deleted. However, for insertion probabilities,
we assume that , to avoid problems with
spurious insertions of target words.
The labels associated with the source strings were
originally assigned by manual annotation for the
purposes of a different research project, specifically
for training an automatic call routing system, us-
ing the methods described by Gorin et al. (1997).
(Many of the training sentences are assigned mul-
tiple labels.)
For the translation task, the labels are used to
compute a similarity measure as a diver-
gence between a probability distribution conditional
on source word and a corresponding distribution
conditional on another source word . The distri-
butions involved, and , are those
for the probability that a source string which
includes word has been assigned label. The sim-
ilarity measure is computed from the rel-
ative entropy (Kullback Leibler distance (Kull-
back and Leibler, 1951)) between these distribu-
tions. To make the similarity measure symmetrical,
i.e. , we take the average
of two relative entropy quantities:
Of course, this is one of many different possible
similarity measures which could have been used (cf
Pereira et al. (1993)), including ones that do not de-
pend on additional labels. However, since seman-
tic labels had already been assigned to our train-
ing data, the distributions seemed like a convenient
rough proxy for the semantic similarity of words in
this limited domain.
</bodyText>
<subsectionHeader confidence="0.991517">
4.2 Case-based transduction procedure
</subsectionHeader>
<bodyText confidence="0.998857472727273">
Basically, the transduction procedure (i) finds an
instance of the translation training pairs for
which the example source string provides the
“best” match to the input source string , and (ii)
produces, as the translation output, a modified ver-
sion of the example target string, where the modifi-
cations reflect mismatches between and the input.
For the first step, the similarity measure between
words computed in terms of the relative entropy for
label distributions is used to compute a distance
between two source strings and. The (seman-
tically influenced) string distance , is a weighted
edit distance (Wagner and Fischer, 1974) between
the two strings in which the cost of substituting one
source word for another is provided by the
“semantic” similarity measure . A stan-
dard quadratic dynamic programming search algo-
rithm is used to find the weighted edit distance be-
tween two strings. This algorithm finds a sequence
of edit operations (insertions, deletions, and substi-
tutions) that yield from so that , the
sum of the costs of the edit operations, is minimal
over all such edit sequences.
The weighted edit distance search is applied to
and each example source string to identify the
example translation pair for which is
minimal over all example source strings. The cor-
responding sequence of edits for this minimal dis-
tance is used to compute a modified version from
. For this purpose, the source language edits are
“translated” into corresponding target language edits
using the probabilistic bilingual lexicon estimated
from aligning the training data. Specifically, for
each substitution in the edits resulting
from the weighted edit distance search, a substitu-
tion is applied to. Here is chosen so
that is maximal. The translated edits are
applied sequentially to to give .
The modified example target string is used as
the output of this translation method unless the min-
imal edit distance between and the closest example
exceeds a threshold determined experimentally.
(For this purpose, the edit distance is normalized by
utterance length.) If the threshold is exceeded, so
that no “sufficiently close” examples are available,
then a word-for-word translation is used as the out-
put by simply applying the probabilistic lexicon to
each word of the input. It is perhaps worth men-
tioning that the statistical dependency transduction
method does not need a such a fall-back to word-
for-word translation: the middle-out (island parsing)
search algorithm used with head transducers grace-
fully degrades into word-for-word translation when
the training data is too sparse to cover the input
string.
</bodyText>
<sectionHeader confidence="0.980187" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.997453">
5.1 Data set
</subsectionHeader>
<bodyText confidence="0.999974476190476">
The corpora for the experiments reported here con-
sist of spoken English utterances, paired with their
translations into Japanese. The English utterances
were the customer side of actual AT&amp;T customer-
operator conversations. There were 12,226 training
bitexts and an additional 3,253 bitexts for testing. In
the text experiments, the English side of the bitext is
the human transcriptions of the recorded speech; in
the speech experiments, it is the output of speech
recognition. The case-based model makes use of
additional information in the form of labels associ-
ated with source language utterances, classifying the
source utterances into 15 task related classes such as
“collect-call”, “directory-assistance”, etc.
The translations were carried out by a commer-
cial translation company. Since Japanese text has no
word boundaries, we asked the translators to insert
spaces between Japanese characters whenever they
‘arose from different English words in the source’.
This imposed an English-centric view of Japanese
text segmentation.
</bodyText>
<subsectionHeader confidence="0.998473">
5.2 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999907962962963">
We use two simple string edit-distance evaluation
metrics that can be calculated automatically. These
metrics, simple accuracy and translation accuracy,
are used to compare the target string produced by the
system against the reference human translation from
held-out data. Simple accuracy (the ‘word accu-
racy’ of speech recognition research) is computed by
first finding a transformation of one string into an-
other that minimizes the total number of insertions,
deletions and substitutions. Translation accuracy in-
cludes transpositions (i.e. movement) of words as
well as insertions, deletions, and substitutions. We
regard the latter measure as more appropriate for
evaluation of translation systems because the simple
metric would count a transposition as two errors: an
insertion plus a deletion. If we write for the num-
ber of insertions, for deletions, for substitutions,
for transpositions, and for number of words in
the reference translation string, we can express the
metrics as follows:
simple accuracy
translation accuracy
Since a transposition corresponds to an insertion
and a deletion, the values of and will be different
in the expressions for computing the two accuracy
metrics. The units for string operations in the evalu-
ation metrics are Japanese characters.
</bodyText>
<subsectionHeader confidence="0.997725">
5.3 Experimental conditions and results
</subsectionHeader>
<bodyText confidence="0.9426576">
The following experimental systems are evaluated
here:
Word-Word A simple word for word baseline
method in which each source word is replaced with
the most highly correlated target word in the training
corpus.
Stat-Dep The statistical dependency transduction
method as described in section 3.
Simple Translation
accuracy accuracy
</bodyText>
<table confidence="0.997226666666667">
Word-Word 37.2 42.8
Stat-Dep 69.3 72.9
Sim-Case 70.6 71.5
</table>
<tableCaption confidence="0.848745">
Table 1: Accuracy for text (%)
Simple Translation
accuracy accuracy
</tableCaption>
<table confidence="0.998951666666667">
Word-Word 29.2 33.7
Stat-Dep 57.4 59.7
Sim-Case 59.4 60.2
</table>
<tableCaption confidence="0.983472">
Table 2: Accuracy for speech (%)
</tableCaption>
<bodyText confidence="0.997964538461539">
Sim-Case The semantic similarity case-based
method described in section 4.
Table 1 shows the results on human transcriptions
of the set of test utterances.
Table 2 shows the test set results of translating
automatic speech recognition output. The speech
recognizer used a speaker-independent telephony
acoustic model and a statisical trigram language
model.
Table 3 shows the speed of loading (once per test
set) and the average run time per utterance transla-
tion for the dependency transduction and case-based
systems.
</bodyText>
<sectionHeader confidence="0.985927" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999188411764706">
In this paper we have compared the accuracy and
speed of two translation methods, statistical depen-
dency transduction and semantic similarity cased-
based transduction. The statistical transduction
model is trainable from unannotated examples of
sentence translations, while the case-based method
additionally makes use of a modest amount of an-
notation to learn a lexical semantic similarity func-
tion, a factor in favor of the dependency transduction
method.
In the experiments we presented, the transduc-
tion methods were applied to translating automatic
speech recognition output for English utterances
into Japanese in a limited domain. The evaluation
metric used to compare translation accuracy was an
automatic string comparison function applied to the
output produced by both methods. The basic result
</bodyText>
<table confidence="0.999726">
Load time Run time/
translation
text
Stat-Dep 7176 53
Sim-Case 3856 2220
speech
Stat-Dep 7447 66
Sim-Case 5925 2333
</table>
<tableCaption confidence="0.99986">
Table 3: Translation time (ms)
</tableCaption>
<bodyText confidence="0.999955761904762">
was that translation accuracy was very similar for
both models, while the statistical dependency trans-
duction method was significantly faster at produc-
ing translations at run time. Since training time for
both methods is dominated by the alignment training
phase they share, training time issues do not favor
one method over the other.
These results need to be interpreted in the rather
narrow experimental setting used here: the amount
of training data used, the specific language pair (En-
glish to Japanese), the evaluation metric, and the
uncertainty in the input strings (speech recognition
output) to which the methods were applied. Fur-
ther research varying these experimental conditions
is needed to provide a fuller comparison of the rela-
tive performance of the methods. However, it should
be possible to develop algorithmic improvements to
increase the computational efficiency of similarity
cased-based transduction to make it more compet-
itive with statistical dependency transduction at run-
time.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997434375">
H. Alshawi and S. Douglas. 2000. Learning depen-
dency transduction models from unannotated exam-
ples. Philosophical Transactions of the Royal Soci-
ety (Series A: Mathematical, Physical and Engineer-
ing Sciences), 358:1357–1372, April.
H. Alshawi, S. Bangalore, and S. Douglas. 1998.
Learning Phrase-based Head Transduction Models for
Translation of Spoken Utterances. In Proceedings
of the International Conference on Spoken Language
Processing, pages 2767–2770, Sydney, Australia.
H. Alshawi, S. Bangalore, and S. Douglas. 2000a. Head
transducer models for speech translation and their au-
tomatic acquisition from bilingual data. Machine
Translation, 15(1/2):105–124.
H. Alshawi, S. Bangalore, and S. Douglas. 2000b.
Learning dependency translation models as collections
of finite state head transducers. Computational Lin-
guistics, 26(1), January.
H. Alshawi. 1996. Head automata for speech transla-
tion. In International Conference on Spoken Language
Processing, pages 2360–2364, Philadelphia, Pennsyl-
vania.
P.J. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer. 1993. The Mathematics of Machine Trans-
lation: Parameter Estimation. Computational Linguis-
tics, 16(2):263–312.
J. Earley. 1970. An Efficient Context-Free Parsing Algo-
rithm. Communications of the ACM, 13(2):94–102.
R. Frederking, S. Nirenburg, D. Farwell, S. Helmreich,
E. Hovy, K. Knight, S. Beale, C. Domashnev, D. At-
tardo, D. Grannes, and R. Brown. 1994. Integrating
translations from multiple sources within the pangloss
mark iii machine translation. In Proceedings of the
first conference of the Association for Machine Trans-
lation in the Americas (AMTA-94), Maryland.
W.A. Gale and K.W. Church. 1991. Identifying word
correspondences in parallel texts. In Proceedings of
the Fourth DARPA Speech and Natural Language Pro-
cessing Workshop, pages 152–157, Pacific Grove, Cal-
ifornia.
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997. How
may I help you? Speech Communication, 23(1-
2):113–127.
D. G. Hays. 1964. Dependency theory: a formalism and
some observations. Language, 40:511–525.
R.A. Hudson. 1984. Word Grammar. Blackwell, Ox-
ford.
Judith L. Klavans and Philip Resnik, editors. 1996. The
Balancing Act: combining Symbolic and Statistical
Approaches to Language. The MIT Press.
S. Kullback and R. A. Leibler. 1951. On information and
sufficiency. Annals ofMathematical Statistics, 22:76–
86.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of English words. In Proceedings of the
31st meeting ofthe Associationfor Computational Lin-
guistics, pages 183–190.
Robert A. Wagner and Michael J. Fischer. 1974. The
string-to-string correction problem. Journal of the As-
sociation for Computing Machinery, 21(1):168–173,
January.
D. Younger. 1967. Recognition and Parsing of Context-
Free Languages in Time . Information and Control,
10:189–208.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.546413">
<note confidence="0.95684">Proceedings of the Workshop on Speech-to-Speech Translation: Algorithms and Systems, Philadelphia, July 2002, pp. 31-38. Association for Computational Linguistics.</note>
<title confidence="0.892841">Speech Translation Performance of Statistical Dependency and Semantic Similarity Transduction</title>
<author confidence="0.908034">Alshawi</author>
<affiliation confidence="0.943091">AT&amp;T Labs -</affiliation>
<address confidence="0.995845">Florham Park, NJ 07932,</address>
<email confidence="0.994588">hiyan,shona@research.att.com</email>
<abstract confidence="0.9942355">In this paper we compare the performance of two methods for speech translation. One is a statistical dependency transduction model using head transducers, the other a case-based transduction model involving a lexical similarity measure. Examples of translated utterance transcriptions are used in training both models, though the case-based model also uses semantic labels classifying the source utterances. The main conclusion is that while the two methods provide similar translation accuracy under the experimental conditions and accuracy metric used, the statistical dependency transduction method is significantly faster at computing translations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency transduction models from unannotated examples.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society (Series A: Mathematical, Physical and Engineering Sciences),</journal>
<pages>358--1357</pages>
<contexts>
<context position="1729" citStr="Alshawi and Douglas, 2000" startWordPosition="240" endWordPosition="244">ing, and more generally other computational problems that are not amenable to closed form solutions, have typically been tackled by one of three broad approaches: rule-based systems, statistical models (including generative models), and case-based systems. Hybrid solutions combining these approaches have also been used in language processing generally (Klavans and Resnik, 1996) and more specifically in machine translation (for example Frederking et al. (1994)). In this paper we compare the performance of two methods for speech translation. One is the statistical dependency transduction model (Alshawi and Douglas, 2000; Alshawi et al., 2000b), a trainable generative statistical translation model using head transducers (Alshawi, 1996). The other is a case-based transduction model which makes use of a semantic similarity measure between words. Both models are trained automatically using examples of translated utterances (the transcription of a spoken utterance and a translation of that transcription). The casebased model makes use of additional information in the form of labels associated with source language utterances, typically one or two labels per utterance. This additional information, which was origina</context>
<context position="4792" citStr="Alshawi and Douglas (2000)" startWordPosition="715" endWordPosition="718">ent mapping from target words to source words . (The inverse mapping is needed to handle mapping of target words to; it coincides with for pairs without.) The other two functions are a source head-map mapping source dependent words to their heads in the source string, and a target head-map mapping target dependent words to their head words in the target string. An example hierarchical alignment is shown in Figure 1. A hierarchical alignment is synchronized (i.e. corresponds to synchronized dependency trees) if, roughly speaking, induces an isomorphism between the dependency functions and (see Alshawi and Douglas (2000) for a more formal definition). The hierarchical alignment in Figure 1 is synchronized. In some previous work (Alshawi et al., 1998; Alshawi et al., 2000a; Alshawi et al., 2000b) the training method constructs synchronized alignments in which each head word has at most two dependent phrases. Here we use the technique described by Alshawi and Douglas (2000) where the models have greater freedom to vary the granularity of phrase locality. Constructing synchronized hierarchical alignments for a corpus has two stages: (a) computing co-occurrence statistics from the training data; (b) searching for</context>
<context position="9415" citStr="Alshawi and Douglas, 2000" startWordPosition="1469" endWordPosition="1472">nd. The improvement in the models resulting from this re-estimation seems to stabilize after approximately five to ten rounds. 3 Statistical Dependency Transduction The dependency transduction model is an automatically trainable translation method that models crosslingual lexical mapping, hierarchical phrase structure, and monolingual lexical dependency. It is a generative statistical model for synchronized pairs of dependency trees in which each local tree is produced by a weighted head transducer. Since this model has been presented at length elsewhere (Alshawi, 1996; Alshawi et al., 2000a; Alshawi and Douglas, 2000), the description in this paper will be relatively compact. 3.1 Weighted finite state head transducers A weighted finite state head transducer is a finite state machine that differs from ‘standard’ finite state transducers in that, instead of consuming the input string left to right, it consumes it ‘middle out’ from a symbol in the string. Similarly, the output of a head transducer is built up middle-out at positions relative to a symbol in the output string. Formally, a weighted head transducer is a 5-tuple: an alphabet of input symbols; an alphabet of output symbols; a finite set of states ;</context>
<context position="14015" citStr="Alshawi and Douglas (2000)" startWordPosition="2267" endWordPosition="2270">bability of a subderivation headed by and , that is for a derivation in which the dependents of and are generated by transitions. The parameters of this probabilistic synchronized tree derivation model are estimated from the results of running the hierarchical alignment algorithm described in section 2 on the sentence pairs in the training corpus. For this purpose, each synchronized tree resulting from the alignment process is assumed to be derived from a dependency transduction model, so transition counts for the model are tallied from the set of synchronized trees. (For further details, see Alshawi and Douglas (2000).) To carry out translation with a dependency transduction model, we apply a “middle-out” dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings or word lattices produced by a speech recognizer. The algorithm is similar to those for context free parsing such as chart parsing (Earley, 1970) and the CKY algorithm (Younger, 1967). It is described in Alshawi et al. (2000b). 4 Similarity Cased-Based Transduction 4.1 Training the transduction parameters Our semantic similarity transduction method is a case-based (or example-based) method for t</context>
</contexts>
<marker>Alshawi, Douglas, 2000</marker>
<rawString>H. Alshawi and S. Douglas. 2000. Learning dependency transduction models from unannotated examples. Philosophical Transactions of the Royal Society (Series A: Mathematical, Physical and Engineering Sciences), 358:1357–1372, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning Phrase-based Head Transduction Models for Translation of Spoken Utterances.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>2767--2770</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="4923" citStr="Alshawi et al., 1998" startWordPosition="737" endWordPosition="740">r pairs without.) The other two functions are a source head-map mapping source dependent words to their heads in the source string, and a target head-map mapping target dependent words to their head words in the target string. An example hierarchical alignment is shown in Figure 1. A hierarchical alignment is synchronized (i.e. corresponds to synchronized dependency trees) if, roughly speaking, induces an isomorphism between the dependency functions and (see Alshawi and Douglas (2000) for a more formal definition). The hierarchical alignment in Figure 1 is synchronized. In some previous work (Alshawi et al., 1998; Alshawi et al., 2000a; Alshawi et al., 2000b) the training method constructs synchronized alignments in which each head word has at most two dependent phrases. Here we use the technique described by Alshawi and Douglas (2000) where the models have greater freedom to vary the granularity of phrase locality. Constructing synchronized hierarchical alignments for a corpus has two stages: (a) computing co-occurrence statistics from the training data; (b) searching for an optimal synchronized hierarchical alignment for each bitext. 2.1 Word correlation statistics For each source word in the datase</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 1998. Learning Phrase-based Head Transduction Models for Translation of Spoken Utterances. In Proceedings of the International Conference on Spoken Language Processing, pages 2767–2770, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Head transducer models for speech translation and their automatic acquisition from bilingual data. Machine Translation,</title>
<date>2000</date>
<pages>15--1</pages>
<contexts>
<context position="1751" citStr="Alshawi et al., 2000" startWordPosition="245" endWordPosition="248">er computational problems that are not amenable to closed form solutions, have typically been tackled by one of three broad approaches: rule-based systems, statistical models (including generative models), and case-based systems. Hybrid solutions combining these approaches have also been used in language processing generally (Klavans and Resnik, 1996) and more specifically in machine translation (for example Frederking et al. (1994)). In this paper we compare the performance of two methods for speech translation. One is the statistical dependency transduction model (Alshawi and Douglas, 2000; Alshawi et al., 2000b), a trainable generative statistical translation model using head transducers (Alshawi, 1996). The other is a case-based transduction model which makes use of a semantic similarity measure between words. Both models are trained automatically using examples of translated utterances (the transcription of a spoken utterance and a translation of that transcription). The casebased model makes use of additional information in the form of labels associated with source language utterances, typically one or two labels per utterance. This additional information, which was originally provided for a sep</context>
<context position="4945" citStr="Alshawi et al., 2000" startWordPosition="741" endWordPosition="745">other two functions are a source head-map mapping source dependent words to their heads in the source string, and a target head-map mapping target dependent words to their head words in the target string. An example hierarchical alignment is shown in Figure 1. A hierarchical alignment is synchronized (i.e. corresponds to synchronized dependency trees) if, roughly speaking, induces an isomorphism between the dependency functions and (see Alshawi and Douglas (2000) for a more formal definition). The hierarchical alignment in Figure 1 is synchronized. In some previous work (Alshawi et al., 1998; Alshawi et al., 2000a; Alshawi et al., 2000b) the training method constructs synchronized alignments in which each head word has at most two dependent phrases. Here we use the technique described by Alshawi and Douglas (2000) where the models have greater freedom to vary the granularity of phrase locality. Constructing synchronized hierarchical alignments for a corpus has two stages: (a) computing co-occurrence statistics from the training data; (b) searching for an optimal synchronized hierarchical alignment for each bitext. 2.1 Word correlation statistics For each source word in the dataset, a translation pairi</context>
<context position="9386" citStr="Alshawi et al., 2000" startWordPosition="1465" endWordPosition="1468">ced by the previous round. The improvement in the models resulting from this re-estimation seems to stabilize after approximately five to ten rounds. 3 Statistical Dependency Transduction The dependency transduction model is an automatically trainable translation method that models crosslingual lexical mapping, hierarchical phrase structure, and monolingual lexical dependency. It is a generative statistical model for synchronized pairs of dependency trees in which each local tree is produced by a weighted head transducer. Since this model has been presented at length elsewhere (Alshawi, 1996; Alshawi et al., 2000a; Alshawi and Douglas, 2000), the description in this paper will be relatively compact. 3.1 Weighted finite state head transducers A weighted finite state head transducer is a finite state machine that differs from ‘standard’ finite state transducers in that, instead of consuming the input string left to right, it consumes it ‘middle out’ from a symbol in the string. Similarly, the output of a head transducer is built up middle-out at positions relative to a symbol in the output string. Formally, a weighted head transducer is a 5-tuple: an alphabet of input symbols; an alphabet of output symb</context>
<context position="14441" citStr="Alshawi et al. (2000" startWordPosition="2336" endWordPosition="2339">ssumed to be derived from a dependency transduction model, so transition counts for the model are tallied from the set of synchronized trees. (For further details, see Alshawi and Douglas (2000).) To carry out translation with a dependency transduction model, we apply a “middle-out” dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings or word lattices produced by a speech recognizer. The algorithm is similar to those for context free parsing such as chart parsing (Earley, 1970) and the CKY algorithm (Younger, 1967). It is described in Alshawi et al. (2000b). 4 Similarity Cased-Based Transduction 4.1 Training the transduction parameters Our semantic similarity transduction method is a case-based (or example-based) method for transducing source strings to target strings that makes use of two different kinds of training data: A set of source-string, target-string pairs that are instances of the transduction mapping. Specifically, transcriptions of spoken utterances in the source language and their translation into the target language. This is the same data used for training the dependency transduction model. It is used in this transduction method</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 2000a. Head transducer models for speech translation and their automatic acquisition from bilingual data. Machine Translation, 15(1/2):105–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
<author>S Bangalore</author>
<author>S Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="1751" citStr="Alshawi et al., 2000" startWordPosition="245" endWordPosition="248">er computational problems that are not amenable to closed form solutions, have typically been tackled by one of three broad approaches: rule-based systems, statistical models (including generative models), and case-based systems. Hybrid solutions combining these approaches have also been used in language processing generally (Klavans and Resnik, 1996) and more specifically in machine translation (for example Frederking et al. (1994)). In this paper we compare the performance of two methods for speech translation. One is the statistical dependency transduction model (Alshawi and Douglas, 2000; Alshawi et al., 2000b), a trainable generative statistical translation model using head transducers (Alshawi, 1996). The other is a case-based transduction model which makes use of a semantic similarity measure between words. Both models are trained automatically using examples of translated utterances (the transcription of a spoken utterance and a translation of that transcription). The casebased model makes use of additional information in the form of labels associated with source language utterances, typically one or two labels per utterance. This additional information, which was originally provided for a sep</context>
<context position="4945" citStr="Alshawi et al., 2000" startWordPosition="741" endWordPosition="745">other two functions are a source head-map mapping source dependent words to their heads in the source string, and a target head-map mapping target dependent words to their head words in the target string. An example hierarchical alignment is shown in Figure 1. A hierarchical alignment is synchronized (i.e. corresponds to synchronized dependency trees) if, roughly speaking, induces an isomorphism between the dependency functions and (see Alshawi and Douglas (2000) for a more formal definition). The hierarchical alignment in Figure 1 is synchronized. In some previous work (Alshawi et al., 1998; Alshawi et al., 2000a; Alshawi et al., 2000b) the training method constructs synchronized alignments in which each head word has at most two dependent phrases. Here we use the technique described by Alshawi and Douglas (2000) where the models have greater freedom to vary the granularity of phrase locality. Constructing synchronized hierarchical alignments for a corpus has two stages: (a) computing co-occurrence statistics from the training data; (b) searching for an optimal synchronized hierarchical alignment for each bitext. 2.1 Word correlation statistics For each source word in the dataset, a translation pairi</context>
<context position="9386" citStr="Alshawi et al., 2000" startWordPosition="1465" endWordPosition="1468">ced by the previous round. The improvement in the models resulting from this re-estimation seems to stabilize after approximately five to ten rounds. 3 Statistical Dependency Transduction The dependency transduction model is an automatically trainable translation method that models crosslingual lexical mapping, hierarchical phrase structure, and monolingual lexical dependency. It is a generative statistical model for synchronized pairs of dependency trees in which each local tree is produced by a weighted head transducer. Since this model has been presented at length elsewhere (Alshawi, 1996; Alshawi et al., 2000a; Alshawi and Douglas, 2000), the description in this paper will be relatively compact. 3.1 Weighted finite state head transducers A weighted finite state head transducer is a finite state machine that differs from ‘standard’ finite state transducers in that, instead of consuming the input string left to right, it consumes it ‘middle out’ from a symbol in the string. Similarly, the output of a head transducer is built up middle-out at positions relative to a symbol in the output string. Formally, a weighted head transducer is a 5-tuple: an alphabet of input symbols; an alphabet of output symb</context>
<context position="14441" citStr="Alshawi et al. (2000" startWordPosition="2336" endWordPosition="2339">ssumed to be derived from a dependency transduction model, so transition counts for the model are tallied from the set of synchronized trees. (For further details, see Alshawi and Douglas (2000).) To carry out translation with a dependency transduction model, we apply a “middle-out” dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings or word lattices produced by a speech recognizer. The algorithm is similar to those for context free parsing such as chart parsing (Earley, 1970) and the CKY algorithm (Younger, 1967). It is described in Alshawi et al. (2000b). 4 Similarity Cased-Based Transduction 4.1 Training the transduction parameters Our semantic similarity transduction method is a case-based (or example-based) method for transducing source strings to target strings that makes use of two different kinds of training data: A set of source-string, target-string pairs that are instances of the transduction mapping. Specifically, transcriptions of spoken utterances in the source language and their translation into the target language. This is the same data used for training the dependency transduction model. It is used in this transduction method</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>H. Alshawi, S. Bangalore, and S. Douglas. 2000b. Learning dependency translation models as collections of finite state head transducers. Computational Linguistics, 26(1), January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata for speech translation.</title>
<date>1996</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<pages>2360--2364</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1846" citStr="Alshawi, 1996" startWordPosition="260" endWordPosition="261"> by one of three broad approaches: rule-based systems, statistical models (including generative models), and case-based systems. Hybrid solutions combining these approaches have also been used in language processing generally (Klavans and Resnik, 1996) and more specifically in machine translation (for example Frederking et al. (1994)). In this paper we compare the performance of two methods for speech translation. One is the statistical dependency transduction model (Alshawi and Douglas, 2000; Alshawi et al., 2000b), a trainable generative statistical translation model using head transducers (Alshawi, 1996). The other is a case-based transduction model which makes use of a semantic similarity measure between words. Both models are trained automatically using examples of translated utterances (the transcription of a spoken utterance and a translation of that transcription). The casebased model makes use of additional information in the form of labels associated with source language utterances, typically one or two labels per utterance. This additional information, which was originally provided for a separate monolingual task, is used to construct the lexical similarity measure. In training these </context>
<context position="9364" citStr="Alshawi, 1996" startWordPosition="1462" endWordPosition="1464">lignments produced by the previous round. The improvement in the models resulting from this re-estimation seems to stabilize after approximately five to ten rounds. 3 Statistical Dependency Transduction The dependency transduction model is an automatically trainable translation method that models crosslingual lexical mapping, hierarchical phrase structure, and monolingual lexical dependency. It is a generative statistical model for synchronized pairs of dependency trees in which each local tree is produced by a weighted head transducer. Since this model has been presented at length elsewhere (Alshawi, 1996; Alshawi et al., 2000a; Alshawi and Douglas, 2000), the description in this paper will be relatively compact. 3.1 Weighted finite state head transducers A weighted finite state head transducer is a finite state machine that differs from ‘standard’ finite state transducers in that, instead of consuming the input string left to right, it consumes it ‘middle out’ from a symbol in the string. Similarly, the output of a head transducer is built up middle-out at positions relative to a symbol in the output string. Formally, a weighted head transducer is a 5-tuple: an alphabet of input symbols; an a</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata for speech translation. In International Conference on Spoken Language Processing, pages 2360–2364, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<booktitle>The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics,</booktitle>
<pages>16--2</pages>
<contexts>
<context position="6627" citStr="Brown et al. (1993)" startWordPosition="1023" endWordPosition="1026"> indicating perfect correlation. In the experiments described in this paper, the cost function relating a source word (or compound) in a bitext with a target word (or compound) is where is a length-normalized measure of the apparent distortion in the positions of and in the source and target strings of. For example, if appears at the middle of the source string and appears at the middle of the target string, then the distortion is . We have found that, at least for our data, this pairing cost leads to better performance than the use of log probabilities of target words given source words (cf. Brown et al. (1993)). The value used for is first computed from counts of the number of bitexts in the training set in which and co-occur, in which only appears, in which only appears, and in which neither of them appear. In other words, we first treat any word in the target string to be a possible translation of any word in the source string. This value is then refined by re-estimation during the alignment optimization process. 2.2 Optimal hierarchical alignments We wish to find a hierarchical alignment that respects the co-occurrence statistics of bitexts as well as the phrasal structure implicit in the source</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P.J. Brown, S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 16(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="14362" citStr="Earley, 1970" startWordPosition="2324" endWordPosition="2325">urpose, each synchronized tree resulting from the alignment process is assumed to be derived from a dependency transduction model, so transition counts for the model are tallied from the set of synchronized trees. (For further details, see Alshawi and Douglas (2000).) To carry out translation with a dependency transduction model, we apply a “middle-out” dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings or word lattices produced by a speech recognizer. The algorithm is similar to those for context free parsing such as chart parsing (Earley, 1970) and the CKY algorithm (Younger, 1967). It is described in Alshawi et al. (2000b). 4 Similarity Cased-Based Transduction 4.1 Training the transduction parameters Our semantic similarity transduction method is a case-based (or example-based) method for transducing source strings to target strings that makes use of two different kinds of training data: A set of source-string, target-string pairs that are instances of the transduction mapping. Specifically, transcriptions of spoken utterances in the source language and their translation into the target language. This is the same data used for tra</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An Efficient Context-Free Parsing Algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frederking</author>
<author>S Nirenburg</author>
<author>D Farwell</author>
<author>S Helmreich</author>
<author>E Hovy</author>
<author>K Knight</author>
<author>S Beale</author>
<author>C Domashnev</author>
<author>D Attardo</author>
<author>D Grannes</author>
<author>R Brown</author>
</authors>
<title>Integrating translations from multiple sources within the pangloss mark iii machine translation.</title>
<date>1994</date>
<booktitle>In Proceedings of the first conference of the Association for Machine Translation in the Americas (AMTA-94),</booktitle>
<location>Maryland.</location>
<contexts>
<context position="1567" citStr="Frederking et al. (1994)" startWordPosition="216" endWordPosition="219">d, the statistical dependency transduction method is significantly faster at computing translations. 1 Introduction Machine translation, natural language processing, and more generally other computational problems that are not amenable to closed form solutions, have typically been tackled by one of three broad approaches: rule-based systems, statistical models (including generative models), and case-based systems. Hybrid solutions combining these approaches have also been used in language processing generally (Klavans and Resnik, 1996) and more specifically in machine translation (for example Frederking et al. (1994)). In this paper we compare the performance of two methods for speech translation. One is the statistical dependency transduction model (Alshawi and Douglas, 2000; Alshawi et al., 2000b), a trainable generative statistical translation model using head transducers (Alshawi, 1996). The other is a case-based transduction model which makes use of a semantic similarity measure between words. Both models are trained automatically using examples of translated utterances (the transcription of a spoken utterance and a translation of that transcription). The casebased model makes use of additional infor</context>
</contexts>
<marker>Frederking, Nirenburg, Farwell, Helmreich, Hovy, Knight, Beale, Domashnev, Attardo, Grannes, Brown, 1994</marker>
<rawString>R. Frederking, S. Nirenburg, D. Farwell, S. Helmreich, E. Hovy, K. Knight, S. Beale, C. Domashnev, D. Attardo, D. Grannes, and R. Brown. 1994. Integrating translations from multiple sources within the pangloss mark iii machine translation. In Proceedings of the first conference of the Association for Machine Translation in the Americas (AMTA-94), Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In Proceedings of the Fourth DARPA Speech and Natural Language Processing Workshop,</booktitle>
<pages>152--157</pages>
<location>Pacific Grove, California.</location>
<contexts>
<context position="5978" citStr="Gale and Church (1991)" startWordPosition="906" endWordPosition="909">the training data; (b) searching for an optimal synchronized hierarchical alignment for each bitext. 2.1 Word correlation statistics For each source word in the dataset, a translation pairing cost is assigned for all possible translations in the context of a bitext. Here and are usually words, but may also be the empty word or compounds formed from contiguous words; here we restrict compounds to a maximum length of two words. The assignment of these lexical translation pairing costs may be done using various statistical measures. The main component of is the so-called correlation measure (see Gale and Church (1991)) normalized to the range with indicating perfect correlation. In the experiments described in this paper, the cost function relating a source word (or compound) in a bitext with a target word (or compound) is where is a length-normalized measure of the apparent distortion in the positions of and in the source and target strings of. For example, if appears at the middle of the source string and appears at the middle of the target string, then the distortion is . We have found that, at least for our data, this pairing cost leads to better performance than the use of log probabilities of target </context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W.A. Gale and K.W. Church. 1991. Identifying word correspondences in parallel texts. In Proceedings of the Fourth DARPA Speech and Natural Language Processing Workshop, pages 152–157, Pacific Grove, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How may I help you? Speech Communication,</title>
<date>1997</date>
<pages>23--1</pages>
<contexts>
<context position="16628" citStr="Gorin et al. (1997)" startWordPosition="2682" endWordPosition="2685">s. (Since the synchronized trees are dependency trees, both paired fringe nodes and interior nodes are included in the counts.) In this probabilistic lexicon, may be, the empty symbol, so source words may have different probabilities of being deleted. However, for insertion probabilities, we assume that , to avoid problems with spurious insertions of target words. The labels associated with the source strings were originally assigned by manual annotation for the purposes of a different research project, specifically for training an automatic call routing system, using the methods described by Gorin et al. (1997). (Many of the training sentences are assigned multiple labels.) For the translation task, the labels are used to compute a similarity measure as a divergence between a probability distribution conditional on source word and a corresponding distribution conditional on another source word . The distributions involved, and , are those for the probability that a source string which includes word has been assigned label. The similarity measure is computed from the relative entropy (Kullback Leibler distance (Kullback and Leibler, 1951)) between these distributions. To make the similarity measure s</context>
</contexts>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A.L. Gorin, G. Riccardi, and J.H. Wright. 1997. How may I help you? Speech Communication, 23(1-2):113–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Dependency theory: a formalism and some observations.</title>
<date>1964</date>
<journal>Language,</journal>
<pages>40--511</pages>
<contexts>
<context position="11574" citStr="Hays (1964)" startWordPosition="1855" endWordPosition="1856">quare on the input tape and writing to square of the output tape; if square is already occupied then is written to the next empty square to the left of if , or to the right of if , and similarly if input was already read from position , is taken from the next unread square to the left of if or to the right of if . 3.2 Dependency transduction models Dependency transduction models are generative statistical models which derive synchronized pairs of dependency trees, a source language dependency tree and a target dependency tree. A dependency tree, in the sense of dependency grammar (for example Hays (1964), Hudson (1984)), is a tree in which the words of a sentence appear as nodes; the parent of a node is its head and the child of a node is the node’s dependent. In a dependency transduction model, each synchronized local subtree corresponds to a head transducer derivation: the head transducer is used to convert a sequence consisting of a head word and its immediate left and right dependent words to a sequence consisting of a target word and immediate left and right dependent words. (Since the empty string may appear in a transition in place of a source or target symbol, the number of source and</context>
</contexts>
<marker>Hays, 1964</marker>
<rawString>D. G. Hays. 1964. Dependency theory: a formalism and some observations. Language, 40:511–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Hudson</author>
</authors>
<title>Word Grammar.</title>
<date>1984</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="11589" citStr="Hudson (1984)" startWordPosition="1857" endWordPosition="1858">input tape and writing to square of the output tape; if square is already occupied then is written to the next empty square to the left of if , or to the right of if , and similarly if input was already read from position , is taken from the next unread square to the left of if or to the right of if . 3.2 Dependency transduction models Dependency transduction models are generative statistical models which derive synchronized pairs of dependency trees, a source language dependency tree and a target dependency tree. A dependency tree, in the sense of dependency grammar (for example Hays (1964), Hudson (1984)), is a tree in which the words of a sentence appear as nodes; the parent of a node is its head and the child of a node is the node’s dependent. In a dependency transduction model, each synchronized local subtree corresponds to a head transducer derivation: the head transducer is used to convert a sequence consisting of a head word and its immediate left and right dependent words to a sequence consisting of a target word and immediate left and right dependent words. (Since the empty string may appear in a transition in place of a source or target symbol, the number of source and target depende</context>
</contexts>
<marker>Hudson, 1984</marker>
<rawString>R.A. Hudson. 1984. Word Grammar. Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Judith</author>
</authors>
<title>The Balancing Act: combining Symbolic and Statistical Approaches to Language.</title>
<date>1996</date>
<editor>Klavans and Philip Resnik, editors.</editor>
<publisher>The MIT Press.</publisher>
<marker>Judith, 1996</marker>
<rawString>Judith L. Klavans and Philip Resnik, editors. 1996. The Balancing Act: combining Symbolic and Statistical Approaches to Language. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kullback</author>
<author>R A Leibler</author>
</authors>
<title>On information and sufficiency. Annals ofMathematical Statistics,</title>
<date>1951</date>
<pages>22--76</pages>
<contexts>
<context position="17165" citStr="Kullback and Leibler, 1951" startWordPosition="2767" endWordPosition="2771">ining an automatic call routing system, using the methods described by Gorin et al. (1997). (Many of the training sentences are assigned multiple labels.) For the translation task, the labels are used to compute a similarity measure as a divergence between a probability distribution conditional on source word and a corresponding distribution conditional on another source word . The distributions involved, and , are those for the probability that a source string which includes word has been assigned label. The similarity measure is computed from the relative entropy (Kullback Leibler distance (Kullback and Leibler, 1951)) between these distributions. To make the similarity measure symmetrical, i.e. , we take the average of two relative entropy quantities: Of course, this is one of many different possible similarity measures which could have been used (cf Pereira et al. (1993)), including ones that do not depend on additional labels. However, since semantic labels had already been assigned to our training data, the distributions seemed like a convenient rough proxy for the semantic similarity of words in this limited domain. 4.2 Case-based transduction procedure Basically, the transduction procedure (i) finds </context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>S. Kullback and R. A. Leibler. 1951. On information and sufficiency. Annals ofMathematical Statistics, 22:76– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st meeting ofthe Associationfor Computational Linguistics,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="17425" citStr="Pereira et al. (1993)" startWordPosition="2810" endWordPosition="2813">lity distribution conditional on source word and a corresponding distribution conditional on another source word . The distributions involved, and , are those for the probability that a source string which includes word has been assigned label. The similarity measure is computed from the relative entropy (Kullback Leibler distance (Kullback and Leibler, 1951)) between these distributions. To make the similarity measure symmetrical, i.e. , we take the average of two relative entropy quantities: Of course, this is one of many different possible similarity measures which could have been used (cf Pereira et al. (1993)), including ones that do not depend on additional labels. However, since semantic labels had already been assigned to our training data, the distributions seemed like a convenient rough proxy for the semantic similarity of words in this limited domain. 4.2 Case-based transduction procedure Basically, the transduction procedure (i) finds an instance of the translation training pairs for which the example source string provides the “best” match to the input source string , and (ii) produces, as the translation output, a modified version of the example target string, where the modifications refl</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st meeting ofthe Associationfor Computational Linguistics, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The string-to-string correction problem.</title>
<date>1974</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="18350" citStr="Wagner and Fischer, 1974" startWordPosition="2957" endWordPosition="2960">y, the transduction procedure (i) finds an instance of the translation training pairs for which the example source string provides the “best” match to the input source string , and (ii) produces, as the translation output, a modified version of the example target string, where the modifications reflect mismatches between and the input. For the first step, the similarity measure between words computed in terms of the relative entropy for label distributions is used to compute a distance between two source strings and. The (semantically influenced) string distance , is a weighted edit distance (Wagner and Fischer, 1974) between the two strings in which the cost of substituting one source word for another is provided by the “semantic” similarity measure . A standard quadratic dynamic programming search algorithm is used to find the weighted edit distance between two strings. This algorithm finds a sequence of edit operations (insertions, deletions, and substitutions) that yield from so that , the sum of the costs of the edit operations, is minimal over all such edit sequences. The weighted edit distance search is applied to and each example source string to identify the example translation pair for which is m</context>
</contexts>
<marker>Wagner, Fischer, 1974</marker>
<rawString>Robert A. Wagner and Michael J. Fischer. 1974. The string-to-string correction problem. Journal of the Association for Computing Machinery, 21(1):168–173, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Younger</author>
</authors>
<title>Recognition and Parsing of ContextFree Languages</title>
<date>1967</date>
<booktitle>in Time . Information and Control,</booktitle>
<pages>10--189</pages>
<contexts>
<context position="14400" citStr="Younger, 1967" startWordPosition="2330" endWordPosition="2331">ing from the alignment process is assumed to be derived from a dependency transduction model, so transition counts for the model are tallied from the set of synchronized trees. (For further details, see Alshawi and Douglas (2000).) To carry out translation with a dependency transduction model, we apply a “middle-out” dynamic programming search to find the optimal derivation. This algorithm can take as input either word strings or word lattices produced by a speech recognizer. The algorithm is similar to those for context free parsing such as chart parsing (Earley, 1970) and the CKY algorithm (Younger, 1967). It is described in Alshawi et al. (2000b). 4 Similarity Cased-Based Transduction 4.1 Training the transduction parameters Our semantic similarity transduction method is a case-based (or example-based) method for transducing source strings to target strings that makes use of two different kinds of training data: A set of source-string, target-string pairs that are instances of the transduction mapping. Specifically, transcriptions of spoken utterances in the source language and their translation into the target language. This is the same data used for training the dependency transduction mode</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. Younger. 1967. Recognition and Parsing of ContextFree Languages in Time . Information and Control, 10:189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>