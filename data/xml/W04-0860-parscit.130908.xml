<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004970">
<note confidence="0.873878">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<title confidence="0.6031335">
Association for Computational Linguistics
The R2D2 Team at SENSEVAL-3∗
</title>
<author confidence="0.805718333333333">
Manuel Garcia, M. Teresa Martin †
Sonia V´azquez, Rafael Romero
Armando Su´arez and Andr´es Montoyo M.
</author>
<affiliation confidence="0.9038835">
Dpto. de Lenguajes y Sistemas. Inform´aticos
Universidad de Alicante, Spain
</affiliation>
<email confidence="0.980119">
{svazquez,romero}@dlsi.ua.es
{armando,montoyo}@dlsi.ua.es
</email>
<note confidence="0.454246666666667">
´Angel Garcia and L. Alfonso Ure˜na
Dpto. de Inform´atica
Universidad de Ja´en, Spain
</note>
<email confidence="0.9163285">
{mgarcia,maite}@ujaen.es
{magc,laurena}@ujaen.es
</email>
<author confidence="0.993479">
Davide Buscaldi, Paolo Rosso ‡
Antonio Molina, Ferr´an Pl´a and Encarna Segarra
</author>
<affiliation confidence="0.8013155">
Dpto. de Sistemas Inform´aticos y Computaci´on
Univ. Polit. de Valencia, Spain
</affiliation>
<email confidence="0.939387">
{dbuscaldi,prosso}@dsic.upv.es
{amolina,fpla,esegarra}@dsic.upv.es
</email>
<sectionHeader confidence="0.995439" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991115">
The R2D2 systems for the English All-Words and
Lexical Sample tasks at SENSEVAL-3 are based on
several supervised and unsupervised methods com-
bined by means of a voting procedure. Main goal
was to take advantage of training data when avail-
able, and getting maximum coverage with the help
of methods that not need such learning examples.
The results reported in this paper show that super-
vised and unsupervised methods working in par-
allel, and a simple sequence of preferences when
comparing the answers of such methods, is a feasi-
ble method...
The whole system is, in fact, a cascade of deci-
sions of what label to assign to a concrete instance
based on the agreement of pairs of systems, when
it is possible, or selecting the available answer from
one of them. In this way, supervised are preferred to
unsupervised methods, but these last ones are able
to tag such words that not have available training
data.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.962058">
Designing a system for Natural Language Process-
ing (NLP) requires a large knowledge on language
structure, morphology, syntax, semantics and prag-
matic nuances. All of these different linguistic
knowledge forms, however, have a common asso-
ciated problem, their many ambiguities, which are
difficult to resolve.
In this paper we concentrate on the resolution
of the lexical ambiguity that appears when a given
word in a context has several different meanings.
∗ This paper has been partially supported by the Spanish
Government (CICyT) under project number TIC-2003-7180
and the Valencia Government (OCyT) under project number
CTIDIB-2002-151
This specific task is commonly referred as Word
Sense Disambiguation (WSD). This is a difficult
problem that is receiving a great deal of attention
from the research community because its resolu-
tion can help other NLP applications as Machine
Translation (MT), Information Retrieval (IR), Text
Processing, Grammatical Analysis, Information Ex-
traction (IE), hypertext navigation and so on.
The R2D2 Team has participated in two tasks:
English all-words and lexical sample. We use sev-
eral different systems both supervised and unsuper-
vised. The supervised methods are based on Max-
imum Entropy (ME) (Lau et al., 1993; Berger et
al., 1996; Ratnaparkhi, 1998), neural network using
the Learning Vector Quantization algorithm (Koho-
nen, 1995) and Specialized Hidden Markov Mod-
els (Pla, 2000). The unsupervised methods are Rel-
evant Domains (RD) (Montoyo et al., 2003) and
the CIAOSENSO WSD system which is based on
Conceptual Density (Agirre and Rigau, 1995), fre-
quency of WordNet (Miller et al., 1993a) senses and
WordNet Domains (Magnini and Cavaglia, 2000).
In the following section we will show a more
complete description of the systems. Next, how
such methods were combined in two voting sys-
tems, and the results obtained in SENSEVAL-3. Fi-
nally, some conclusions will be presented.
</bodyText>
<sectionHeader confidence="0.924979" genericHeader="method">
2 Systems description
</sectionHeader>
<bodyText confidence="0.9474205">
In this section the systems that have participated at
SENSEVAL-3 will be described.
</bodyText>
<subsectionHeader confidence="0.989342">
2.1 Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.9997569375">
ME modeling provides a framework for integrating
information for classification from many heteroge-
neous information sources (Manning and Sch¨utze,
1999). ME probability models have been success-
fully applied to some NLP tasks, such as POS tag-
ging or sentence boundary detection (Ratnaparkhi,
1998). ME have been also applied to WSD (van
Halteren et al., 2001; Montoyo and Su´arez, 2001;
Su´arez and Palomar, 2002), and as meta-learner in
(Ilhan et al., 2001).
Our ME-based system has been shown competi-
tive (M`arquez et al., 2003) when compared to other
supervised systems such as Decision Lists, Support
Vector Machines, and AdaBoost. The features that
were defined to train the system are those described
in Figure 1.
</bodyText>
<listItem confidence="0.999317117647059">
• the target word itself
• lemmas of content-words at positions f1, f2, f3
• words at positions f1, f2,
• words at positions f1, f2, f3
• content-words at positions f1, f2, f3
• POS-tags of words at positions f1, f2, f3
• lemmas of collocations at positions (−2, −1),
(−1,+1), (+1,+2)
• collocations at positions (−2,−1), (−1,+1),
(+1,+2)
• lemmas of nouns at any position in context, occur-
ring at least m% times with a sense
• grammatical relation of the target word
• the word that the target word depends on
• the verb that the target word depends on
• the target word belongs to a multi-word, as identi-
fied by the parser
</listItem>
<figureCaption confidence="0.8519245">
Figure 1: Features Used for the Supervised Learn-
ing of the ME system
</figureCaption>
<bodyText confidence="0.999792416666667">
Because the ME system needs annotated data
for the training, Semcor (Miller et al., 1993b) was
used for the English All-Words task, the system
was trained using Semcor (Miller et al., 1993b), and
parsed by Minipar (Lin, 1998). Only those words
that have 10 examples or more in Semcor were pro-
cessed in order to obtain a ME classifier.
For the Spanish Lexical Sample task, the train-
ing data from SENSEVAL-3 was the source of la-
beled examples. We did not use any parser, just the
lemmatization and POS-tagging information sup-
plied into the training data itself.
</bodyText>
<subsectionHeader confidence="0.995937">
2.2 UPV-SBMM-AW
</subsectionHeader>
<bodyText confidence="0.999970393939394">
The upv-shmm-aw WSD system is a supervised ap-
proach based on Specialized Hidden Markov Mod-
els (SHMM).
Basically, a SHMM consists of changing the
topology of a Hidden Markov Model in order to get
a more accurate model which includes more infor-
mation. This is done by means of an initial step
previous to the learning process. It consists of the
redefinition of the input vocabulary and the output
tags. This redefinition is done by means of two pro-
cesses which transform the training set: the selec-
tion process chooses which input features (words,
lemmas, part-of-speech tags, ...) are relevant to the
task, and the specialization process redefines the
output tags by adding information from the input.
This specialization produces some changes in the
model topology, in order to allow the model to bet-
ter capture some contextual restrictions and to get a
more accurate model.
We used as training data the part of the Sem-
Cor corpus that is semantically annotated and su-
pervised for nouns, verbs, adjectives and adverbs,
and the test data set provided by SENSEVAL-2.
We used 10% of the training corpus as a develop-
ment data set in order to determine the best selection
and specialization criteria.
In the experiments, we used WordNet1.6 (Miller
et al., 1993a) as a dictionary that supplies all the
possible semantic senses for a given word. Our sys-
tem disambiguated all the polysemic lemmas, that
is, the coverage of our system was 100%. For un-
known words (words that did not appear in the train-
ing data set), we assigned the first sense in WordNet.
</bodyText>
<subsectionHeader confidence="0.996219">
2.3 Relevant Domains
</subsectionHeader>
<bodyText confidence="0.99996216">
This is an unsupervised WSD method based on the
WordNet Domains lexical resource (Magnini and
Cavaglia, 2000). The underlaying working hypoth-
esis is that domain labels, such as ARCHITEC-
TURE, SPORT and MEDICINE provide a natural
way to establish semantic relations between word
senses, that can be used during the disambiguation
process. This resource has already been used on
Word Sense Disambiguation (Magnini and Strappa-
rava, 2000), but it has not made use of glosses infor-
mation. So our approach make use of a new lexical
resource obtained from glosses information named
Relevant Domains.
First step is to obtain the Relevant Domains re-
source from WordNet glosses. For this task is nec-
essary a previous part-of-speech tagging of Word-
Net glosses (each gloss has associated a domain la-
bel). So we extract all nouns, verbs, adjectives and
adverbs from glosses and assign them their associ-
ated domain label. With this information and using
the Association Ratio formula(w=word,D=domain
label), in (1), we obtain the Relevant Domains re-
source.
The final result is for each word, a set of domain
labels sorted by Association Ratio, for example,
</bodyText>
<equation confidence="0.998879">
P r(wJD)
AR(w, D) = P r(wJD)log2
Pr(w) (1)
</equation>
<bodyText confidence="0.999798375">
for word plant” its Relevant Domains are: genetics
0.177515, ecology 0.050065, botany 0.038544 ....
Once obtained Relevant Domains the disam-
biguation process is carried out. We obtain from
the text source the context words that co-occur with
the word to be disambiguated (context could be
a sentence or a window of words). We obtain a
context vector from Relevant Domains and context
words (in case of repeated domain labels, they are
weighted). Furthermore we need a sense vector ob-
tained in the same way as context vector from words
of glosses of each word sense. We select the cor-
rect sense using the cosine measure between con-
text vector and sense vectors. So the selected sense
is that for which the cosine with the context vector
is closer to one.
</bodyText>
<subsectionHeader confidence="0.927736">
2.4 LVQ-JA´EN-ELS
</subsectionHeader>
<bodyText confidence="0.999419409090909">
The LVQ-JA´EN-ELS system (Garcia-Vega et al.,
2003) is based on a supervised learning algorithm
for WSD. The method trains a neural network using
the Learning Vector Quantization (LVQ) algorithm
(Kohonen, 1995), integrating Semcor and several
semantic relations of WordNet.
The Vector Space Model (VSM) is used as an in-
formation representation model. Each sense of a
word is represented as a vector in an n-dimensional
space where n is the number of words in all its con-
texts.
We use the LVQ algorithm to adjust the word
weights. The input vector weights are calculated
as shown by (Salton and McGill, 1983) with the
standard (tf · idf). They are presented to the LVQ
network and, after training, the output vectors are
obtained, containing the adjusted weights for all
senses of each word.
Any word to disambiguate is represented with a
vector in the same way. This representation must be
compared with all the trained sense vectors of the
word by applying the cosine similarity rule:
</bodyText>
<equation confidence="0.819911">
sim(wk, xi) =  |wk ·wk |· |ixi  |(2)
</equation>
<bodyText confidence="0.987158">
The sense corresponding to the vector of highest
similarity is selected as the disambiguated sense.
To train the neural network we have inte-
grated semantic information from two linguistic re-
sources: SemCor1.6 corpus and WordNet1.7.1 lex-
ical database. From Semcor1.6 we used the para-
graph as a contextual semantic unit and each con-
text was included in the training vector set. From
WordNet1.7.1 some semantic relations were consid-
ered, specifically, synonymy, antonymy, hyponymy,
homonymy, hyperonymy, meronymy, and coordi-
nate terms. This information was introduced to the
training set through the creation of artificial para-
graphs with the words of each relation. So, for a
word with 7 senses, 7 artificial paragraphs with the
synonyms of the 7 senses were added, 7 more with
all its hyponyms, and so on.
The learning algorithm is very simple. First, the
learning rate and the codebook vectors are initial-
ized. Then, the following procedure is repeated for
all the training input vectors until a stopping crite-
rion is satisfied:
- Select a training input pattern, x, with class d,
and present it to the network
- Calculate the Euclidean distance between the in-
put vector and each codebook vector  ||x − wk ||
- Select the codebook vector, wc, that is closest to
the input vector, x, like the winner sense.
- The winner neuron updates its weights accord-
ing the learning equation:
</bodyText>
<equation confidence="0.999233">
wc(t + 1) = wc(t) + s · α(t) · [x(t) − wc(t)] (3)
</equation>
<bodyText confidence="0.999718">
where s = 0, if k =6 c; s = 1, if x(t) and wc(t)
belong to the same class (c = d); and s = −1, if
they do not (c =6 d). α(t) is the learning rate, and
0 &lt; α(t) &lt; 1 is a monotically decreasing func-
tion of time. It is recommended that α(t) should
already initially be rather small, say, smaller than
0.1 (Kohonen, 1995) and α(t) continues decreasing
to a given threshold, u, very close to 0.
</bodyText>
<subsectionHeader confidence="0.76992">
2.5 CIAOSENSO
</subsectionHeader>
<bodyText confidence="0.999983822222222">
The CIAOSENSO WSD system is an unsupervised
system based on Conceptual Density, the frequency
of WordNet sense, and WordNet Domains. Concep-
tual Density is a measure of the correlation among
the sense of a given word and its context. The
noun sense disambiguation is performed by means
of a formula combining the Conceptual Density
with WordNet sense frequency (Rosso et al., 2003).
The context window used in both the English all-
words and lexical sample tasks is of 4 nouns. Ad-
ditional weights are assigned to those senses hav-
ing the same domain as the context nouns’ senses.
Each weight is proportional to the frequency of such
senses, and is calculated as MDW (f, i) = 1/f ·1/i
where f is an integer representing the frequency
of the sense of the word to be disambiguated and
i gives the same information for the context word.
Example: If the word to be disambiguated is doc-
tor, the domains for senses 1 and 4 are, respec-
tively, Medicine and School. Therefore, if one of
the context words is university, the resulting weight
for doctor(4) and university(3) is 1/4 ∗ 1/3.
The sense disambiguation of an adjective is per-
formed only on the basis of the above weights.
Given one of its senses, we extract the synsets ob-
tained by the similar to, pertainym and attribute
relationships. For each of them, we calculate the
MDW with respect to the senses of the context
noun. The weight assigned to the adjective sense
is the average between these MDWs. The se-
lected sense is the one having the maximum average
weight.
The sense disambiguation of a verb is done nearly
in the same way, but taking into consideration only
the MDWs with the context words. In the all-words
task the context words are the noun before and af-
ter the verb, whereas in the lexical sample task the
context words are four (two before and two after the
verb), without regard to their morphological cate-
gory. This has been done in order to improve the
recall in the latter task, for which the test corpus is
made up mostly by verbs.
The sense disambiguation of adverbs (in both
tasks) is carried out in the same way of the disam-
biguation of verbs for the lexical sample task.
</bodyText>
<sectionHeader confidence="0.980158" genericHeader="method">
3 Tasks Processing
</sectionHeader>
<bodyText confidence="0.99669575">
We have selected several combinations of such sys-
tems described before for two voting systems, one
for the Lexical-Sample task and the other for the
All-Words task.
</bodyText>
<subsectionHeader confidence="0.999296">
3.1 English Lexical Sample Task
</subsectionHeader>
<bodyText confidence="0.999855818181818">
At the English Lexical Sample task we combined
the answers of four systems: Relevant Domains,
CIAOSENSO, LVQ-JA´EN-ELS and Maximum En-
tropy.
The four methods worked in parallel and their
sets of answers were the input of a majority voting
procedure. This procedure selected those answers
with more systems agreements. In case of tie we
gave priority to supervised systems.
With this voting system we obtained around a
63% precision and a 52% recall.
</bodyText>
<subsectionHeader confidence="0.997548">
3.2 English All Words Task
</subsectionHeader>
<bodyText confidence="0.99979341025641">
For this task we used a voting system combining
the results of Relevant Domains, Maximum En-
tropy, CIAOSENSO and UPV-SHMM-AW. So we
obtained the final results after 10 steps.
Step 1, we selected those answers with agree-
ment between ME and UPV-SHMM-AW (super-
vised systems).
Step 2, from no agreement in step 1 we selected
those answers with agreement between ME and Rel-
evant Domains.
Step 3, from no agreement in step 2 we selected
those answers with agreement between ME and
CIAOSENSO.
Step 4, from no agreement in step 3 we se-
lected those answers with agreement between
CIAOSENSO and UPV-SHMM-AW.
Step 5, from no agreement in step 4 we se-
lected those answers with agreement between UPV-
SHMM-AW and Relevant Domains.
Step 6, from no agreement in step 5 we selected
those answers with agreement between Relevant
Domains and CIAOSENSO.
Step 7, from no agreement in step 6 we selected
Maximum Entropy answers.
Step 8, from the remaining unlabeled instances
we selected UPV-SHMM-AW answers.
Step 9, from the remaining unlabeled instances
we selected Relevant Domains answers.
Step 10, from the remaining unlabeled instances
we selected CIAOSENSO answers.
Last step was labeling with the most frequent
sense in WordNet those instances that had been not
tagged by any system, but in view of the final results
only two instances had not answer and we didn’t
find them in WordNet.
With this voting system preference was given to
supervised systems over unsupervised systems.
We obtained around a 63% precision and a 63%
recall.
</bodyText>
<sectionHeader confidence="0.999685" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999961625">
This paper presents the main characteristics of
the Maximum Entropy, LVQ-JAEN-ELS, UPV-
SHMM-AW, Relevant Domains and CIAOSENSO
systems within the framework of SENSEVAL-3 En-
glish Lexical Sample and All Words tasks. These
systems are combined with a voting technique ob-
taining a promising results for English All Words
and English Lexical Sample tasks.
</bodyText>
<sectionHeader confidence="0.997141" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.96504791509434">
Eneko Agirre and German Rigau. 1995. A pro-
posal for word sense disambiguation using Con-
ceptual Distance. In Proceedings of the Interna-
tional Conference ”Recent Advances in Natural
Language Processing” (RANLP95).
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1):39–71.
Manuel Garcia-Vega, Maria Teresa Martin-
Valdivia, and Luis Alfonso Ure˜na. 2003.
Aprendizaje competitivo lvq para la desam-
biguaci´on l´exica. Revista de la Sociedad
Espaola para el Procesamiento del Lenguaje
Natural, 31:125–132.
H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein,
Christopher D. Manning, and Kristina Toutanova.
2001. Combining Heterogeneous Classifiers for
Word-Sense Disambiguation. In Judita Preiss
and David Yarowsky, editors, Proceedings of the
2nd International Workshop on Evaluating Word
Sense Disambiguation Systems (SENSEVAL-2),
pages 87–90, Toulouse, France, July. ACL-
SIGLEX.
T. Kohonen. 1995. Self-organization and associa-
tive memory. 2nd Ed. Springer Verlag, Berlin.
R. Lau, R. Rosenfeld, and S. Roukos. 1993.
Adaptative statistical language modeling using
the maximum entropy principle. In Proceedings
of the Human Language Technology Workshop,
ARPA.
Dekang Lin. 1998. Dependency-based evaluation
of minipar. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, First Inter-
national Conference on Language Resources and
Evaluation, Granada, Spain.
Bernardo Magnini and Gabriela Cavaglia. 2000.
Integrating Subject Field Codes into WordNet. In
M. Gavrilidou, G. Crayannis, S. Markantonatu,
S. Piperidis, and G. Stainhaouer, editors, Pro-
ceedings of LREC-2000, Second International
Conference on Language Resources and Evalu-
ation, pages 1413–1418, Athens, Greece.
Bernardo Magnini and C. Strapparava. 2000. Ex-
periments in Word Domain Disambiguation for
Parallel Texts. In Proceedings of the ACL Work-
shop on Word Senses and Multilinguality, Hong
Kong, China.
Christopher D. Manning and Hinrich Sch¨utze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
Massachusetts.
Lluis M`arquez, Fco. Javier Raya, John Car-
roll, Diana McCarthy, Eneko Agirre, David
Martinez, Carlo Strapparava, and Alfio
Gliozzo. 2003. Experiment A: several all-words
WSD systems for English. Technical Report
WP6.2, MEANING project (IST-2001-34460),
http://www.lsi.upc.es/∼nlp/meaning/meaning.html.
George A. Miller, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1993a. Five Papers on WordNet. Special Issue of
the International journal of lexicography, 3(4).
George A. Miller, C. Leacock, R. Tengi, and
T. Bunker. 1993b. A Semantic Concordance. In
Proceedings ofARPA Workshop on Human Lan-
guage Technology, pages 303–308, Plainsboro,
New Jersey.
Andr´es Montoyo and Armando Su´arez. 2001.
The University of Alicante word sense disam-
biguation system. In Judita Preiss and David
Yarowsky, editors, Proceedings of the 2nd In-
ternational Workshop on Evaluating Word Sense
Disambiguation Systems (SENSEVAL-2), pages
131–134, Toulouse, France, July. ACL-SIGLEX.
Andr´es Montoyo, Sonia V´azquez, and German
Rigau. 2003. M´etodo de desambiguaci´on l´exica
basada en el recurso l´exico Dominios Rele-
vantes. Procesamiento del Lenguaje Natural, 30,
september.
F. Pla. 2000. Etiquetado L´exico y An´alisis
Sint´actico Superficial basado en Modelos Es-
tadisticos. Tesis doctoral, Departamento de Sis-
temas Inform´aticos y Computaci´on. Universidad
de Polit´ecnica de Valencia, Septiembre.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and
A. Molina. 2003. Automatic noun disambigua-
tion. LNCS, Springer Verlag, 2588:273–276.
G. Salton and M.J. McGill. 1983. Introduction
to modern information retrieval. McGraw-Hill,
New York.
Armando Su´arez and Manuel Palomar. 2002.
A maximum entropy-based word sense disam-
biguation system. In Hsin-Hsi Chen and Chin-
Yew Lin, editors, Proceedings of the 19th In-
ternational Conference on Computational Lin-
guistics, pages 960–966, Taipei, Taiwan, August.
COLING 2002.
H. van Halteren, J. Zavrel, and W. Daelemans.
2001. Improving accuracy in wordclass tag-
ging through combination of machine learning
systems. Computational Linguistics, 27(2):199–
230.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.067327">
<note confidence="0.88649325">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics R2D2 Team at</note>
<author confidence="0.873928666666667">M Teresa Martin Sonia V´azquez Garcia</author>
<author confidence="0.873928666666667">Rafael Romero Armando Su´arez</author>
<author confidence="0.873928666666667">Andr´es Montoyo M</author>
<affiliation confidence="0.9207312">Dpto. de Lenguajes y Sistemas. Inform´aticos Universidad de Alicante, Spain Garcia and L. Alfonso Dpto. de Inform´atica Universidad de Ja´en, Spain</affiliation>
<address confidence="0.579549">Buscaldi, Paolo Rosso Antonio Molina, Ferr´an Pl´a and Encarna</address>
<affiliation confidence="0.616797">Dpto. de Sistemas Inform´aticos y Univ. Polit. de Valencia,</affiliation>
<abstract confidence="0.998410476190476">The R2D2 systems for the English All-Words and Sample tasks at are based on several supervised and unsupervised methods combined by means of a voting procedure. Main goal was to take advantage of training data when available, and getting maximum coverage with the help of methods that not need such learning examples. The results reported in this paper show that supervised and unsupervised methods working in parallel, and a simple sequence of preferences when comparing the answers of such methods, is a feasible method... The whole system is, in fact, a cascade of decisions of what label to assign to a concrete instance based on the agreement of pairs of systems, when it is possible, or selecting the available answer from one of them. In this way, supervised are preferred to unsupervised methods, but these last ones are able to tag such words that not have available training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>German Rigau</author>
</authors>
<title>A proposal for word sense disambiguation using Conceptual Distance.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Conference ”Recent Advances in Natural Language Processing” (RANLP95).</booktitle>
<contexts>
<context position="3326" citStr="Agirre and Rigau, 1995" startWordPosition="494" endWordPosition="497"> Information Extraction (IE), hypertext navigation and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some conclusions will be presented. 2 Systems description In this section the systems that have participated at SENSEVAL-3 will be described. 2.1 Maximum Entropy ME modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning and Sch¨utz</context>
</contexts>
<marker>Agirre, Rigau, 1995</marker>
<rawString>Eneko Agirre and German Rigau. 1995. A proposal for word sense disambiguation using Conceptual Distance. In Proceedings of the International Conference ”Recent Advances in Natural Language Processing” (RANLP95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3010" citStr="Berger et al., 1996" startWordPosition="446" endWordPosition="449">task is commonly referred as Word Sense Disambiguation (WSD). This is a difficult problem that is receiving a great deal of attention from the research community because its resolution can help other NLP applications as Machine Translation (MT), Information Retrieval (IR), Text Processing, Grammatical Analysis, Information Extraction (IE), hypertext navigation and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3.</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Garcia-Vega</author>
<author>Maria Teresa MartinValdivia</author>
<author>Luis Alfonso Ure˜na</author>
</authors>
<title>Aprendizaje competitivo lvq para la desambiguaci´on l´exica.</title>
<date>2003</date>
<booktitle>Revista de la Sociedad Espaola para el Procesamiento del Lenguaje Natural,</booktitle>
<pages>31--125</pages>
<marker>Garcia-Vega, MartinValdivia, Ure˜na, 2003</marker>
<rawString>Manuel Garcia-Vega, Maria Teresa MartinValdivia, and Luis Alfonso Ure˜na. 2003. Aprendizaje competitivo lvq para la desambiguaci´on l´exica. Revista de la Sociedad Espaola para el Procesamiento del Lenguaje Natural, 31:125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tolga Ilhan</author>
<author>Sepandar D Kamvar</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Kristina Toutanova</author>
</authors>
<title>Combining Heterogeneous Classifiers for Word-Sense Disambiguation.</title>
<date>2001</date>
<booktitle>In Judita Preiss and David Yarowsky, editors, Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>87--90</pages>
<publisher>ACLSIGLEX.</publisher>
<location>Toulouse, France,</location>
<contexts>
<context position="4239" citStr="Ilhan et al., 2001" startWordPosition="638" endWordPosition="641">some conclusions will be presented. 2 Systems description In this section the systems that have participated at SENSEVAL-3 will be described. 2.1 Maximum Entropy ME modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning and Sch¨utze, 1999). ME probability models have been successfully applied to some NLP tasks, such as POS tagging or sentence boundary detection (Ratnaparkhi, 1998). ME have been also applied to WSD (van Halteren et al., 2001; Montoyo and Su´arez, 2001; Su´arez and Palomar, 2002), and as meta-learner in (Ilhan et al., 2001). Our ME-based system has been shown competitive (M`arquez et al., 2003) when compared to other supervised systems such as Decision Lists, Support Vector Machines, and AdaBoost. The features that were defined to train the system are those described in Figure 1. • the target word itself • lemmas of content-words at positions f1, f2, f3 • words at positions f1, f2, • words at positions f1, f2, f3 • content-words at positions f1, f2, f3 • POS-tags of words at positions f1, f2, f3 • lemmas of collocations at positions (−2, −1), (−1,+1), (+1,+2) • collocations at positions (−2,−1), (−1,+1), (+1,+2)</context>
</contexts>
<marker>Ilhan, Kamvar, Klein, Manning, Toutanova, 2001</marker>
<rawString>H. Tolga Ilhan, Sepandar D. Kamvar, Dan Klein, Christopher D. Manning, and Kristina Toutanova. 2001. Combining Heterogeneous Classifiers for Word-Sense Disambiguation. In Judita Preiss and David Yarowsky, editors, Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 87–90, Toulouse, France, July. ACLSIGLEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Self-organization and associative memory. 2nd Ed.</title>
<date>1995</date>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="3111" citStr="Kohonen, 1995" startWordPosition="460" endWordPosition="462">g a great deal of attention from the research community because its resolution can help other NLP applications as Machine Translation (MT), Information Retrieval (IR), Text Processing, Grammatical Analysis, Information Extraction (IE), hypertext navigation and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some conclusions will be presented. 2 Systems description In this section the systems that </context>
<context position="9507" citStr="Kohonen, 1995" startWordPosition="1536" endWordPosition="1537">ins and context words (in case of repeated domain labels, they are weighted). Furthermore we need a sense vector obtained in the same way as context vector from words of glosses of each word sense. We select the correct sense using the cosine measure between context vector and sense vectors. So the selected sense is that for which the cosine with the context vector is closer to one. 2.4 LVQ-JA´EN-ELS The LVQ-JA´EN-ELS system (Garcia-Vega et al., 2003) is based on a supervised learning algorithm for WSD. The method trains a neural network using the Learning Vector Quantization (LVQ) algorithm (Kohonen, 1995), integrating Semcor and several semantic relations of WordNet. The Vector Space Model (VSM) is used as an information representation model. Each sense of a word is represented as a vector in an n-dimensional space where n is the number of words in all its contexts. We use the LVQ algorithm to adjust the word weights. The input vector weights are calculated as shown by (Salton and McGill, 1983) with the standard (tf · idf). They are presented to the LVQ network and, after training, the output vectors are obtained, containing the adjusted weights for all senses of each word. Any word to disambi</context>
<context position="12063" citStr="Kohonen, 1995" startWordPosition="1991" endWordPosition="1992">idean distance between the input vector and each codebook vector ||x − wk || - Select the codebook vector, wc, that is closest to the input vector, x, like the winner sense. - The winner neuron updates its weights according the learning equation: wc(t + 1) = wc(t) + s · α(t) · [x(t) − wc(t)] (3) where s = 0, if k =6 c; s = 1, if x(t) and wc(t) belong to the same class (c = d); and s = −1, if they do not (c =6 d). α(t) is the learning rate, and 0 &lt; α(t) &lt; 1 is a monotically decreasing function of time. It is recommended that α(t) should already initially be rather small, say, smaller than 0.1 (Kohonen, 1995) and α(t) continues decreasing to a given threshold, u, very close to 0. 2.5 CIAOSENSO The CIAOSENSO WSD system is an unsupervised system based on Conceptual Density, the frequency of WordNet sense, and WordNet Domains. Conceptual Density is a measure of the correlation among the sense of a given word and its context. The noun sense disambiguation is performed by means of a formula combining the Conceptual Density with WordNet sense frequency (Rosso et al., 2003). The context window used in both the English allwords and lexical sample tasks is of 4 nouns. Additional weights are assigned to tho</context>
</contexts>
<marker>Kohonen, 1995</marker>
<rawString>T. Kohonen. 1995. Self-organization and associative memory. 2nd Ed. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lau</author>
<author>R Rosenfeld</author>
<author>S Roukos</author>
</authors>
<title>Adaptative statistical language modeling using the maximum entropy principle.</title>
<date>1993</date>
<booktitle>In Proceedings of the Human Language Technology Workshop,</booktitle>
<location>ARPA.</location>
<contexts>
<context position="2989" citStr="Lau et al., 1993" startWordPosition="442" endWordPosition="445">151 This specific task is commonly referred as Word Sense Disambiguation (WSD). This is a difficult problem that is receiving a great deal of attention from the research community because its resolution can help other NLP applications as Machine Translation (MT), Information Retrieval (IR), Text Processing, Grammatical Analysis, Information Extraction (IE), hypertext navigation and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results ob</context>
</contexts>
<marker>Lau, Rosenfeld, Roukos, 1993</marker>
<rawString>R. Lau, R. Rosenfeld, and S. Roukos. 1993. Adaptative statistical language modeling using the maximum entropy principle. In Proceedings of the Human Language Technology Workshop, ARPA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of minipar.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation,</booktitle>
<location>Granada,</location>
<contexts>
<context position="5420" citStr="Lin, 1998" startWordPosition="850" endWordPosition="851">ons (−2,−1), (−1,+1), (+1,+2) • lemmas of nouns at any position in context, occurring at least m% times with a sense • grammatical relation of the target word • the word that the target word depends on • the verb that the target word depends on • the target word belongs to a multi-word, as identified by the parser Figure 1: Features Used for the Supervised Learning of the ME system Because the ME system needs annotated data for the training, Semcor (Miller et al., 1993b) was used for the English All-Words task, the system was trained using Semcor (Miller et al., 1993b), and parsed by Minipar (Lin, 1998). Only those words that have 10 examples or more in Semcor were processed in order to obtain a ME classifier. For the Spanish Lexical Sample task, the training data from SENSEVAL-3 was the source of labeled examples. We did not use any parser, just the lemmatization and POS-tagging information supplied into the training data itself. 2.2 UPV-SBMM-AW The upv-shmm-aw WSD system is a supervised approach based on Specialized Hidden Markov Models (SHMM). Basically, a SHMM consists of changing the topology of a Hidden Markov Model in order to get a more accurate model which includes more information.</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of minipar. In Proceedings of the Workshop on the Evaluation of Parsing Systems, First International Conference on Language Resources and Evaluation, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavaglia</author>
</authors>
<title>Integrating Subject Field Codes into WordNet.</title>
<date>2000</date>
<booktitle>Proceedings of LREC-2000, Second International Conference on Language Resources and Evaluation,</booktitle>
<pages>1413--1418</pages>
<editor>In M. Gavrilidou, G. Crayannis, S. Markantonatu, S. Piperidis, and G. Stainhaouer, editors,</editor>
<location>Athens, Greece.</location>
<contexts>
<context position="3427" citStr="Magnini and Cavaglia, 2000" startWordPosition="510" endWordPosition="513">wo tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some conclusions will be presented. 2 Systems description In this section the systems that have participated at SENSEVAL-3 will be described. 2.1 Maximum Entropy ME modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning and Sch¨utze, 1999). ME probability models have been successfully applied to some NLP tasks, such as POS tagging</context>
<context position="7449" citStr="Magnini and Cavaglia, 2000" startWordPosition="1194" endWordPosition="1197">y SENSEVAL-2. We used 10% of the training corpus as a development data set in order to determine the best selection and specialization criteria. In the experiments, we used WordNet1.6 (Miller et al., 1993a) as a dictionary that supplies all the possible semantic senses for a given word. Our system disambiguated all the polysemic lemmas, that is, the coverage of our system was 100%. For unknown words (words that did not appear in the training data set), we assigned the first sense in WordNet. 2.3 Relevant Domains This is an unsupervised WSD method based on the WordNet Domains lexical resource (Magnini and Cavaglia, 2000). The underlaying working hypothesis is that domain labels, such as ARCHITECTURE, SPORT and MEDICINE provide a natural way to establish semantic relations between word senses, that can be used during the disambiguation process. This resource has already been used on Word Sense Disambiguation (Magnini and Strapparava, 2000), but it has not made use of glosses information. So our approach make use of a new lexical resource obtained from glosses information named Relevant Domains. First step is to obtain the Relevant Domains resource from WordNet glosses. For this task is necessary a previous par</context>
</contexts>
<marker>Magnini, Cavaglia, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavaglia. 2000. Integrating Subject Field Codes into WordNet. In M. Gavrilidou, G. Crayannis, S. Markantonatu, S. Piperidis, and G. Stainhaouer, editors, Proceedings of LREC-2000, Second International Conference on Language Resources and Evaluation, pages 1413–1418, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>C Strapparava</author>
</authors>
<title>Experiments in Word Domain Disambiguation for Parallel Texts.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL Workshop on Word Senses and Multilinguality,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="7773" citStr="Magnini and Strapparava, 2000" startWordPosition="1243" endWordPosition="1247">polysemic lemmas, that is, the coverage of our system was 100%. For unknown words (words that did not appear in the training data set), we assigned the first sense in WordNet. 2.3 Relevant Domains This is an unsupervised WSD method based on the WordNet Domains lexical resource (Magnini and Cavaglia, 2000). The underlaying working hypothesis is that domain labels, such as ARCHITECTURE, SPORT and MEDICINE provide a natural way to establish semantic relations between word senses, that can be used during the disambiguation process. This resource has already been used on Word Sense Disambiguation (Magnini and Strapparava, 2000), but it has not made use of glosses information. So our approach make use of a new lexical resource obtained from glosses information named Relevant Domains. First step is to obtain the Relevant Domains resource from WordNet glosses. For this task is necessary a previous part-of-speech tagging of WordNet glosses (each gloss has associated a domain label). So we extract all nouns, verbs, adjectives and adverbs from glosses and assign them their associated domain label. With this information and using the Association Ratio formula(w=word,D=domain label), in (1), we obtain the Relevant Domains r</context>
</contexts>
<marker>Magnini, Strapparava, 2000</marker>
<rawString>Bernardo Magnini and C. Strapparava. 2000. Experiments in Word Domain Disambiguation for Parallel Texts. In Proceedings of the ACL Workshop on Word Senses and Multilinguality, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Javier Raya</author>
</authors>
<location>John Carroll, Diana McCarthy, Eneko Agirre, David</location>
<marker>Raya, </marker>
<rawString>Lluis M`arquez, Fco. Javier Raya, John Carroll, Diana McCarthy, Eneko Agirre, David</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlo Strapparava Martinez</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Experiment A: several all-words WSD systems for English.</title>
<date>2003</date>
<tech>Technical Report WP6.2, MEANING project (IST-2001-34460),</tech>
<note>http://www.lsi.upc.es/∼nlp/meaning/meaning.html.</note>
<marker>Martinez, Gliozzo, 2003</marker>
<rawString>Martinez, Carlo Strapparava, and Alfio Gliozzo. 2003. Experiment A: several all-words WSD systems for English. Technical Report WP6.2, MEANING project (IST-2001-34460), http://www.lsi.upc.es/∼nlp/meaning/meaning.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine J Miller</author>
</authors>
<date>1993</date>
<booktitle>Five Papers on WordNet. Special Issue of the International journal of lexicography,</booktitle>
<pages>3--4</pages>
<contexts>
<context position="3369" citStr="Miller et al., 1993" startWordPosition="502" endWordPosition="505">ion and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some conclusions will be presented. 2 Systems description In this section the systems that have participated at SENSEVAL-3 will be described. 2.1 Maximum Entropy ME modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning and Sch¨utze, 1999). ME probability models have been s</context>
<context position="5283" citStr="Miller et al., 1993" startWordPosition="825" endWordPosition="828">s f1, f2, f3 • POS-tags of words at positions f1, f2, f3 • lemmas of collocations at positions (−2, −1), (−1,+1), (+1,+2) • collocations at positions (−2,−1), (−1,+1), (+1,+2) • lemmas of nouns at any position in context, occurring at least m% times with a sense • grammatical relation of the target word • the word that the target word depends on • the verb that the target word depends on • the target word belongs to a multi-word, as identified by the parser Figure 1: Features Used for the Supervised Learning of the ME system Because the ME system needs annotated data for the training, Semcor (Miller et al., 1993b) was used for the English All-Words task, the system was trained using Semcor (Miller et al., 1993b), and parsed by Minipar (Lin, 1998). Only those words that have 10 examples or more in Semcor were processed in order to obtain a ME classifier. For the Spanish Lexical Sample task, the training data from SENSEVAL-3 was the source of labeled examples. We did not use any parser, just the lemmatization and POS-tagging information supplied into the training data itself. 2.2 UPV-SBMM-AW The upv-shmm-aw WSD system is a supervised approach based on Specialized Hidden Markov Models (SHMM). Basically,</context>
<context position="7026" citStr="Miller et al., 1993" startWordPosition="1121" endWordPosition="1124">s redefines the output tags by adding information from the input. This specialization produces some changes in the model topology, in order to allow the model to better capture some contextual restrictions and to get a more accurate model. We used as training data the part of the SemCor corpus that is semantically annotated and supervised for nouns, verbs, adjectives and adverbs, and the test data set provided by SENSEVAL-2. We used 10% of the training corpus as a development data set in order to determine the best selection and specialization criteria. In the experiments, we used WordNet1.6 (Miller et al., 1993a) as a dictionary that supplies all the possible semantic senses for a given word. Our system disambiguated all the polysemic lemmas, that is, the coverage of our system was 100%. For unknown words (words that did not appear in the training data set), we assigned the first sense in WordNet. 2.3 Relevant Domains This is an unsupervised WSD method based on the WordNet Domains lexical resource (Magnini and Cavaglia, 2000). The underlaying working hypothesis is that domain labels, such as ARCHITECTURE, SPORT and MEDICINE provide a natural way to establish semantic relations between word senses, t</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1993</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J. Miller. 1993a. Five Papers on WordNet. Special Issue of the International journal of lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>T Bunker</author>
</authors>
<title>A Semantic Concordance.</title>
<date>1993</date>
<booktitle>In Proceedings ofARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, New Jersey.</location>
<contexts>
<context position="3369" citStr="Miller et al., 1993" startWordPosition="502" endWordPosition="505">ion and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some conclusions will be presented. 2 Systems description In this section the systems that have participated at SENSEVAL-3 will be described. 2.1 Maximum Entropy ME modeling provides a framework for integrating information for classification from many heterogeneous information sources (Manning and Sch¨utze, 1999). ME probability models have been s</context>
<context position="5283" citStr="Miller et al., 1993" startWordPosition="825" endWordPosition="828">s f1, f2, f3 • POS-tags of words at positions f1, f2, f3 • lemmas of collocations at positions (−2, −1), (−1,+1), (+1,+2) • collocations at positions (−2,−1), (−1,+1), (+1,+2) • lemmas of nouns at any position in context, occurring at least m% times with a sense • grammatical relation of the target word • the word that the target word depends on • the verb that the target word depends on • the target word belongs to a multi-word, as identified by the parser Figure 1: Features Used for the Supervised Learning of the ME system Because the ME system needs annotated data for the training, Semcor (Miller et al., 1993b) was used for the English All-Words task, the system was trained using Semcor (Miller et al., 1993b), and parsed by Minipar (Lin, 1998). Only those words that have 10 examples or more in Semcor were processed in order to obtain a ME classifier. For the Spanish Lexical Sample task, the training data from SENSEVAL-3 was the source of labeled examples. We did not use any parser, just the lemmatization and POS-tagging information supplied into the training data itself. 2.2 UPV-SBMM-AW The upv-shmm-aw WSD system is a supervised approach based on Specialized Hidden Markov Models (SHMM). Basically,</context>
<context position="7026" citStr="Miller et al., 1993" startWordPosition="1121" endWordPosition="1124">s redefines the output tags by adding information from the input. This specialization produces some changes in the model topology, in order to allow the model to better capture some contextual restrictions and to get a more accurate model. We used as training data the part of the SemCor corpus that is semantically annotated and supervised for nouns, verbs, adjectives and adverbs, and the test data set provided by SENSEVAL-2. We used 10% of the training corpus as a development data set in order to determine the best selection and specialization criteria. In the experiments, we used WordNet1.6 (Miller et al., 1993a) as a dictionary that supplies all the possible semantic senses for a given word. Our system disambiguated all the polysemic lemmas, that is, the coverage of our system was 100%. For unknown words (words that did not appear in the training data set), we assigned the first sense in WordNet. 2.3 Relevant Domains This is an unsupervised WSD method based on the WordNet Domains lexical resource (Magnini and Cavaglia, 2000). The underlaying working hypothesis is that domain labels, such as ARCHITECTURE, SPORT and MEDICINE provide a natural way to establish semantic relations between word senses, t</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, C. Leacock, R. Tengi, and T. Bunker. 1993b. A Semantic Concordance. In Proceedings ofARPA Workshop on Human Language Technology, pages 303–308, Plainsboro, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´es Montoyo</author>
<author>Armando Su´arez</author>
</authors>
<title>The University of Alicante word sense disambiguation system.</title>
<date>2001</date>
<booktitle>In Judita Preiss and David Yarowsky, editors, Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2),</booktitle>
<pages>131--134</pages>
<publisher>ACL-SIGLEX.</publisher>
<location>Toulouse, France,</location>
<marker>Montoyo, Su´arez, 2001</marker>
<rawString>Andr´es Montoyo and Armando Su´arez. 2001. The University of Alicante word sense disambiguation system. In Judita Preiss and David Yarowsky, editors, Proceedings of the 2nd International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 131–134, Toulouse, France, July. ACL-SIGLEX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´es Montoyo</author>
<author>Sonia V´azquez</author>
<author>German Rigau</author>
</authors>
<title>M´etodo de desambiguaci´on l´exica basada en el recurso l´exico Dominios Relevantes. Procesamiento del</title>
<date>2003</date>
<journal>Lenguaje Natural,</journal>
<volume>30</volume>
<marker>Montoyo, V´azquez, Rigau, 2003</marker>
<rawString>Andr´es Montoyo, Sonia V´azquez, and German Rigau. 2003. M´etodo de desambiguaci´on l´exica basada en el recurso l´exico Dominios Relevantes. Procesamiento del Lenguaje Natural, 30, september.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pla</author>
</authors>
<title>Etiquetado L´exico y An´alisis Sint´actico Superficial basado en Modelos Estadisticos. Tesis doctoral, Departamento de Sistemas Inform´aticos y Computaci´on. Universidad de Polit´ecnica de</title>
<date>2000</date>
<location>Valencia, Septiembre.</location>
<contexts>
<context position="3160" citStr="Pla, 2000" startWordPosition="469" endWordPosition="470">ty because its resolution can help other NLP applications as Machine Translation (MT), Information Retrieval (IR), Text Processing, Grammatical Analysis, Information Extraction (IE), hypertext navigation and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some conclusions will be presented. 2 Systems description In this section the systems that have participated at SENSEVAL-3 will be described</context>
</contexts>
<marker>Pla, 2000</marker>
<rawString>F. Pla. 2000. Etiquetado L´exico y An´alisis Sint´actico Superficial basado en Modelos Estadisticos. Tesis doctoral, Departamento de Sistemas Inform´aticos y Computaci´on. Universidad de Polit´ecnica de Valencia, Septiembre.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3030" citStr="Ratnaparkhi, 1998" startWordPosition="450" endWordPosition="451">rred as Word Sense Disambiguation (WSD). This is a difficult problem that is receiving a great deal of attention from the research community because its resolution can help other NLP applications as Machine Translation (MT), Information Retrieval (IR), Text Processing, Grammatical Analysis, Information Extraction (IE), hypertext navigation and so on. The R2D2 Team has participated in two tasks: English all-words and lexical sample. We use several different systems both supervised and unsupervised. The supervised methods are based on Maximum Entropy (ME) (Lau et al., 1993; Berger et al., 1996; Ratnaparkhi, 1998), neural network using the Learning Vector Quantization algorithm (Kohonen, 1995) and Specialized Hidden Markov Models (Pla, 2000). The unsupervised methods are Relevant Domains (RD) (Montoyo et al., 2003) and the CIAOSENSO WSD system which is based on Conceptual Density (Agirre and Rigau, 1995), frequency of WordNet (Miller et al., 1993a) senses and WordNet Domains (Magnini and Cavaglia, 2000). In the following section we will show a more complete description of the systems. Next, how such methods were combined in two voting systems, and the results obtained in SENSEVAL-3. Finally, some concl</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rosso</author>
<author>F Masulli</author>
<author>D Buscaldi</author>
<author>F Pla</author>
<author>A Molina</author>
</authors>
<title>Automatic noun disambiguation.</title>
<date>2003</date>
<pages>2588--273</pages>
<publisher>LNCS, Springer Verlag,</publisher>
<contexts>
<context position="12530" citStr="Rosso et al., 2003" startWordPosition="2066" endWordPosition="2069">s a monotically decreasing function of time. It is recommended that α(t) should already initially be rather small, say, smaller than 0.1 (Kohonen, 1995) and α(t) continues decreasing to a given threshold, u, very close to 0. 2.5 CIAOSENSO The CIAOSENSO WSD system is an unsupervised system based on Conceptual Density, the frequency of WordNet sense, and WordNet Domains. Conceptual Density is a measure of the correlation among the sense of a given word and its context. The noun sense disambiguation is performed by means of a formula combining the Conceptual Density with WordNet sense frequency (Rosso et al., 2003). The context window used in both the English allwords and lexical sample tasks is of 4 nouns. Additional weights are assigned to those senses having the same domain as the context nouns’ senses. Each weight is proportional to the frequency of such senses, and is calculated as MDW (f, i) = 1/f ·1/i where f is an integer representing the frequency of the sense of the word to be disambiguated and i gives the same information for the context word. Example: If the word to be disambiguated is doctor, the domains for senses 1 and 4 are, respectively, Medicine and School. Therefore, if one of the con</context>
</contexts>
<marker>Rosso, Masulli, Buscaldi, Pla, Molina, 2003</marker>
<rawString>P. Rosso, F. Masulli, D. Buscaldi, F. Pla, and A. Molina. 2003. Automatic noun disambiguation. LNCS, Springer Verlag, 2588:273–276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to modern information retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="9904" citStr="Salton and McGill, 1983" startWordPosition="1604" endWordPosition="1607">.4 LVQ-JA´EN-ELS The LVQ-JA´EN-ELS system (Garcia-Vega et al., 2003) is based on a supervised learning algorithm for WSD. The method trains a neural network using the Learning Vector Quantization (LVQ) algorithm (Kohonen, 1995), integrating Semcor and several semantic relations of WordNet. The Vector Space Model (VSM) is used as an information representation model. Each sense of a word is represented as a vector in an n-dimensional space where n is the number of words in all its contexts. We use the LVQ algorithm to adjust the word weights. The input vector weights are calculated as shown by (Salton and McGill, 1983) with the standard (tf · idf). They are presented to the LVQ network and, after training, the output vectors are obtained, containing the adjusted weights for all senses of each word. Any word to disambiguate is represented with a vector in the same way. This representation must be compared with all the trained sense vectors of the word by applying the cosine similarity rule: sim(wk, xi) = |wk ·wk |· |ixi |(2) The sense corresponding to the vector of highest similarity is selected as the disambiguated sense. To train the neural network we have integrated semantic information from two linguisti</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to modern information retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Armando Su´arez</author>
<author>Manuel Palomar</author>
</authors>
<title>A maximum entropy-based word sense disambiguation system.</title>
<date>2002</date>
<booktitle>In Hsin-Hsi Chen and ChinYew Lin, editors, Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>960--966</pages>
<location>Taipei, Taiwan, August. COLING</location>
<marker>Su´arez, Palomar, 2002</marker>
<rawString>Armando Su´arez and Manuel Palomar. 2002. A maximum entropy-based word sense disambiguation system. In Hsin-Hsi Chen and ChinYew Lin, editors, Proceedings of the 19th International Conference on Computational Linguistics, pages 960–966, Taipei, Taiwan, August. COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving accuracy in wordclass tagging through combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<pages>230</pages>
<marker>van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 2001. Improving accuracy in wordclass tagging through combination of machine learning systems. Computational Linguistics, 27(2):199– 230.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>