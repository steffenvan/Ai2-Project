<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022278">
<title confidence="0.9730595">
Coordination and context-dependence in the generation of embodied
conversation
</title>
<author confidence="0.971077">
Justine Cassell*
</author>
<affiliation confidence="0.947928">
*Media Laboratory
</affiliation>
<address confidence="0.7556685">
MIT
E15-315
20 Ames, Cambridge MA
1j ustine , yanhaol@media .mit
</address>
<author confidence="0.915837">
Matthew Stone t Hao Yan*
</author>
<affiliation confidence="0.986885333333333">
tDepartrnent of Computer Science &amp;
Center for Cognitive Science
Rutgers University
</affiliation>
<address confidence="0.965281">
110 Frelinghuysen, Piscataway NJ 08854-8019
</address>
<email confidence="0.994928">
edu mdstone@cs.rutgers.edu
</email>
<sectionHeader confidence="0.969473" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974363636364">
We describe the generation of communicative ac-
tions in an implemented embodied conversational
agent. Our agent plans each utterance so that mul-
tiple communicative goals may be realized oppor-
tunistically by a composite action including not only
speech but also coverbal gesture that fits the con-
text and the ongoing speech in ways representative
of natural human conversation. We accomplish this
by reasoning from a grammar which describes ges-
ture declaratively in terms of its discourse function,
semantics and synchrony with speech.
</bodyText>
<sectionHeader confidence="0.990224" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996423375">
When we are face-to-face with another human, no
matter what our language, cultural background, or
age, we virtually all use our faces and hands as an in-
tegral part of our dialogue with others. Research on
embodied conversational agents aims to imbue in-
teractive dialogue systems with the same nonverbal
skills and behaviors (Cassell, 2000a).
There is good reason to think that nonverbal be-
havior will play an important role in evoking from
users the kinds of communicative dialogue behav-
iors they use with other humans, and thus allow
them to use the computer with the same kind of ef-
ficiency and smoothness that characterizes their di-
alogues with other people. For example, (Cassell
and Thorisson, 1999) show that humans are more
likely to consider computers lifelike, and to rate their
language skills more highly, when those computers
display not only speech but appropriate nonverbal
communicative behavior_ This argument takes on
particular importance given that users repeat them-
selves needlessly, mistake when it is their turn to
speak, and so forth when interacting with voice di-
alogue systems (Oviatt, 1995): In life, noisy situa-
tions like these provoke the non-verbal modalities to
come into play (Rogers, 1978).
In this paper, we describe the generation of com-
municative actions in an implemented embodied
conversational agent. Our generation framework
adopts a goal-directed view of generation and casts
knowledge about communicative action in the form
of a grammar that specifies how forms combine,
what interpretive effects they impart and in what
contexts they are appropriate (Appelt, 1985; Moore,
1994; Dale, 1992; Stone and Doran, 1997). We ex-
pand this framework to take into account findings,
by ourselves and others, on the relationship between
spontaneous coverbal hand gestures and speech. In
particular, our agent plans each utterance so that
multiple communicative goals may be realized op-
portunistically by a composite action including not
only speech but also coverbal gesture. By describing
gesture declaratively in terms of its discourse func-
tion, semantics and synchrony with speech, we en-
sure that coverbal gesture fits the context and the on-
going speech in ways representative of natural hu-
man conversation. The result is a streamlined imple-
mentation that instantiates important theoretical in-
sights into the relationship between speech and ges-
ture in human-human conversation.
2 Exploring the relationship between
-speech and gesture
To generate embodied communicative action re-
quires an architecture for embodied conversation;
ours is provided by the agent REA (&amp;quot;Real Estate
Agent&amp;quot;), a computer-generated humanoid that has
an articulated graphical body, can sense the user
passively through cameras and audio input, and
supports communicative actions realized in speech
with intonation, facial display, and animated ges-
ture. REA currently offers the reasoning and dis-
play capabilities to act as a real estate agent showing
&apos;userg Thee-feature s&apos;of v ieus motel of houses. that -
appear on-screen behind her. We use existing fea-
tures of REA here as a research platform for imple-
</bodyText>
<page confidence="0.997035">
171
</page>
<bodyText confidence="0.993298802083333">
menting models of the relationship between speech
and spontaneous hand gestures during conversation.
For more details about the functionality of REA see
(Cassell, 2000a).
Evidence from many sources suggests that this re-
lationship is a close one. About three-quarters of all
clauses in narrative discourse are accompanied by
gestures of one kind or another (McNeill, 1992), and
within those clauses, the most effortful part of ges-
tures tends to co-occur with or just before the phono-
logically most prominent syllable of the accompany-
ing speech (Kendon, 1974).
Of course, communication is still possible with-
out gesture. But it has been shown that when speech
is ambiguous (Thompson and Massaro, 1986) or in
a speech situation with some noise (Rogers, 1978),
listeners do rely on gestural cues (and, the higher the
noise-to-signal ratio, the more facilitation by ges-
ture). Similarly, Cassell et al. (1999) established that
listeners rely on information conveyed only in ges-
ture as they try to comprehend a story.
Most interesting in terms of building interactive
dialogue systems is the semantic and pragmatic rela-
tionship between gesture and speech. The two chan-
nels do not always manifest the same information,
but what they convey is virtually always compati-
ble. Semantically, speech and gesture give a con-
sistent view of an overall situation. For example,
gesture may depict the way in which an action was
carried out when this aspect of meaning is not de-
picted in speech. Pragmatically, speech and ges-
ture mark information about this meaning as advanc-
ing the purposes of the conversation in a consistent
way. Indeed, gesture often emphasizes information
that is also focused pragmatically by mechanisms
like prosody in speech (Cassell, 2000b). The seman-
tic and pragmatic compatibility seen in the gesture-
speech relationship recalls the interaction of words
and graphics in multimodal presentations (Feiner
and McKeown, 1991; Green et al., 1998; Wahlster
et al., 1991). In fact, some suggest (McNeill, 1992),
that gesture and speech arise together from an under-
lying representation that has both visual and linguis-
tic aspects, and so the relationship between gesture
and speech is essential to the production of meaning
and to its comprehension.
This theoretical perspective on speech and gesture
involves two key claims with computational import:
that gesture and speech .reflect-a- common concep-
tual source; and that the content and form of a ges-
ture is tuned to the communicative context and the
actor&apos;s communicative intentions. We believe that
these characteristics of the use of gesture are uni-
versal, and see the key contribution of this work as
providing a general framework for building dialogue
systems in accord with them. However, a concrete
implementation requires more than just generalities
behind its operation; we&apos;also need an understanding
of the precise ways gesture and speech are used to-
gether in a particular task and setting.
To this end, we collected a sample of real-estate
descriptions in line with what REA might be asked
to provide. To elicit each description, we asked one
subject to study a video and floor plan of a partic-
ular house, and then to describe the house to a sec-
ond subject (who did not know the house and had not
seen the video). During the conversation, the video
and floor plan were not available to either subject;
the listener was free to interrupt and ask questions.
The collected conversations were transcribed,
yielding 328 utterances and 134 referential gestures,
and coded to describe the general communicative
goals of the speaker and the kinds of semantic fea-
tures realized in speech and gesture.
Analysis of the data revealed that for roughly
50% of the gesture-accompanied utterances, gestu-
ral content was redundant with speech; for the other
50% gesture contributed content that was different,
but complementary, to that contributed by speech.
In addition, the relationship between content of ges-
ture, content of speech and general communicative
functions in house descriptions could be captured by
a small number or rules; these rules are informed by
and accord with our two key claims about speech
and gesture. For example, one rule describes di-
alogue contributions whose general function was
what we call presentation, to advance the descrip-
tion of the house by introducing a single new ob-
ject.. These contributions tended to be made up of
a sentence that asserted the existence of an object
of some type, accompanied by a non-redundant ges-
ture that elaborated the•shape or location of the ob-
ject. Our approach casts this extended description of
a new entity, mediated by two compatible modali-
ties, as the speaker&apos;s expression of one overall func-
tion of presentation.
</bodyText>
<equation confidence="0.6535525">
( I ) is a representative example.
(1) It has [a nice garden]. (right hand, held flat,
</equation>
<bodyText confidence="0.971601333333333">
traces a circle, indicating location of the
garden surrounding the house)
Six rules account for 60% of the gestures in the
</bodyText>
<page confidence="0.902637">
172
</page>
<bodyText confidence="0.998788805755396">
the state of the dialogue, and to regulate the over-
all process of conversation (interactional function)?
Within this focus, REA&apos;s talk is firmly delimited.
REA&apos;s utterances take a question-answer format, in
which the user asks about (and REA describes) a
single house .at _a time. REA&apos;s .sentences are short;
generally, they contribute just a few new semantic
features about particular rooms or features of the
house (in. speech and gesture), and flesh this contri-
bution out with a handful of meaningful elements (in
speech and gesture) that ground the contribution in
shared context of the conversation.
Despite the apparent simplicity, the dialogue
manager must contribute a wealth of information
about the domain and the conversation to represent
the communicative context. This detail is needed for
REA to achieve a theoretically-motivated realization
of the common patterns of speech and gesture we ob-
served in human conversation. For example, a vari-
ety of changing features determine whether marked
forms in speech and gesture are appropriate in the
context. REA&apos;s dialogue manager tracks the chang-
ing status of such features as:
O Attentional prominence, represented (as usual
in natural language generation) by setting up a
context set for each entity (Dale, 1992). Our
model of prominence is a simple local one sim-
ilar to (Strube, 1998).
▪ Cognitive status, including whether an entity is
hearer-old or hearer-new (Prince, 1992), and
whether an entity is in-focus or not (Gundel
et al., 1993). We can assume that houses and
their rooms are hearer-new until REA describes
them; and that just those entities mentioned in
the prior sentence are in-focus.
a Information structure, including the open
propositions or, following (Steedman, 1991),
themes, which describe the salient questions
currently at issue in the discourse (Prince,
1986). In REA&apos;s dialogue, open questions are
always general questions about some entity
raised by a recent turn; although in principle
such an open question ought to be formalized
as theme(A.P.Pe), REA can use the simpler
therne(e).
In fact, both speech and gesture depend on the same
kinds of featurestand.acces-s-them in the&apos; same way;-
this specification of the dialogue state crosscuts dis-
tinctions of communicative modality.
Figure l: Interacting with REA
transcriptions (recall) and apply with an accuracy of
96% (precision). These patterns provide a concrete
specification for the main communicative strategies
and communicative resources required for REA. A
full discussion of the experimental methods and
analysis, and the resulting rules, can be found in
(Yan, 2000).
3 Framing the generation problem
In REA, requests for the generation of speech and
gesture are formulated within the dialogue manage-
ment module. REA&apos;s utterances reflect a coordina-
tion of multiple kinds of processing in the dialogue
manager—the system recognizes that it has the floor,
derives the appropriate communicative context for
a response and an appropriate set of communicative
goals, triggers the generation process, and realizes
the resulting speech and gesture. The dialogue man-
ager is only one component in a multithreaded ar-
chitecture that carries out hardwired reactions to in-
put as well as deliberative processing. The diver-
sity is required in order to exhibit appropriate inter-
actional and propositional conversational behaviors
at a range of time scales, from tracking the user&apos;s
movements with gaze and providing nods and other
feedback as the user speaks, to participating in rou-
tine exchanges and generating principled responses
to user&apos;s queries. See (Cassell, 2000a) for descrip-
tion and motivation of the architecture, as well as the
conversational functions and behaviors it supports.
REA&apos;S design and capabilities reflect our research
focus on allying conversational content with conver-
sation management, and allying nonverbal modali-
ties with speech: how can an embodied-agent aseall
its communicative modalities to contribute new con-
tent when needed (propositional function), to signal
173
Another component of context is provided by a
domain knowledge base, consisting of facts explic-
itly labeled with the kind of information they repre-
sent. This defines the common ground in the con-
versation in terms of sources of information that
speaker and hearer share. Modeling the discourse, as.
a shared source of informaiion *means that new se-
mantic features REA imparts are added to the com-
mon ground as the dialogue proceeds. Following re-
sults from (Kelly et al., 1999) which show that infor-
mation from both speech and gesture is used to pro-
vide context for ongoing talk, our common ground
may be updated by both speech and gesture.
The structured domain knowledge also provides
a resource for specifying communicative strategies.
Recall that REAk&apos;s communicative strategies are for-
mulated in terms of functions which are common
in naturally-occurring dialogues (such as &amp;quot;presenta-
tion&amp;quot;) and which lead to distinctive bundles of con-
tent in gesture and speech. The knowledge base&apos;s
kinds of information provide a mechanism for spec-
ifying and reasoning about such functions. The
knowledge base is structured to describe the rela-
tionship between the system&apos;s private information
and the questions of interest that that information
can be used to settle. Once the user&apos;s words have
been interpreted, a layer of production rules con-
structs obligations for response (Traum and Allen,
1994); then, a second layer plans to meet these obli-
gations by deciding to present a specified kind of
information about a specified object. This deter-
mines some concrete communicative goals—facts
of this kind that a contribution to dialogue could
make. Both speech and gesture can access the
whole structured database in realizing these concrete
communicative goals. For example, a variety of
facts that bear on where a residence is—which city.
which neighborhood or, if appropriate, where in a
building—all provide the same kind of information,
and would therefore fit the obligation to specify the
location of a residence. Or, to implement the rule-
for presentation described in connection with (1), we
can associate an obligation of presentation with a
cluster of facts describing an object&apos;s type, its loca-
tion in a house, and its size, shape or quality.
The communicative context and concrete com-
municative goals provide a common source for gen-
erating speech and gesture in REA. The utterance
generation problem in REA,,then, is to &apos;construct a-
complex communicative action, made up of speech
and coverbal gesture, that achieves a given constel-
lation of goals and tightly fits the context specified
by the dialogue manager.
</bodyText>
<sectionHeader confidence="0.962652" genericHeader="introduction">
4 Generation and linguistic representation
</sectionHeader>
<bodyText confidence="0.99995285106383">
We model REA&apos;s communicative actions as corn-
-posed--of a veileetiorretatornic-demen ts n clu din g
both lexical items in speech and clusters of seman-
tic features expressed as gestures; since we assume
that any such item usually conveys a specific piece
of content, we refer to these elements generally as
lexicalized descriptors. The generation task in REA
thus involves selecting a number of such lexical-
ized descriptors and organizing them into a gram-
matical whole that manifests the right semantic and
pragmatic coordination between speech and gesture.
The information conveyed must be enough that the
hearer can identify the entity in each domain ref-
erence from among its context set. Moreover, the
descriptors must provide a source which allows the
hearer to recover any needed new domain proposi-
tion, either explicitly or by inference.
We use the SPUD generator (&amp;quot;Sentence Planning
Using Description&amp;quot;) introduced in (Stone and Do-
ran, 1997) to carry out this task for REA. SPUD
builds the utterance element-by-element; at each
stage of construction, SPUD&apos;S representation of the
current, incomplete utterance specifies its syntax,
semantics, interpretation and fit to context. This rep-
resentation both allows SPUD to determine which
lexicalized descriptors are available at each stage to
extend the utterance, and to assess the progress to-
wards its communicative goals which each exten-
sion would bring about. At each stage, then, SPUD
selects the available option that offers the best im-
mediate advance toward completing the utterance
successfully. (We have developed a suite of guide-
lines for the design of syntactic structures, seman-
tic and pragmatic representations, and the interface
between them so that SPUD&apos;S greedy search, which
is necessary for real-time performance, succeeds in
finding concise and effective Utterances -described
by the grammar (Stone et al., 2000).)
As part of the development Of REA, we have con-
structed a new inventory of lexicalized descriptors.
REA &apos;s descriptors consist of entries that contribute
to coverbal gestures, as well as revised entries for
spoken words that allow for their coordination with
gesture under appropriate discourse conditions. The
-- -organization ofthese entries assures‘that=tising the
same mechanism as with speech—REA&apos;S gestures
draw on the single available conceptual representa-
</bodyText>
<page confidence="0.968535">
174
</page>
<bodyText confidence="0.984698040404041">
tion and that both REA&apos;S gesture and the relation- in a single tree conveniently allows modules down.-
ship between gesture and speech-vary as a function stream to describe embodied communicative actions
of pragmatic context in the same way as natural ges- as marked-up text.)
tures and speech do. More abstractly, these entries The syntactic description of the gesture itself in-
enable SPUD to realize the concrete goals tied to dicates the choices the generator must make to pro-
common communicative functions with same dis- duce a gesture, but does not analyze a gesture lit-
tribution of speech and Observed in natural - eralIy as a hierarchy of separate movements. In-
conversations. stead, these choices specify independent semantic
To explain how these entries work, we need to features which we can associate with aspects of a
consider SPUD&apos;S representation of lexicalized de- gesture (such as handshape and trajectory through
scriptors in more detail. Each entry is specified space). Our current grammar does not undertake the
in three parts. The first part—the syntax of the final step of associating semantic features to choice
element—sets out what words or other actions the of particular handshapes and movements, or gesture
element contributes to its utterance. The syn- morphology; we reserve this problem for later in
tax is a hierarchical structure, formalized using the research program. We allow gesture to accom-
Feature-Based Lexicalized Tree Adjoining Gram- pany alternative constituents by introducing alterna-
mar (LTAG) (Joshi et al., 1975; Schabes, 1990). tive syntactic entries; these entries take on different
Syntactic structures are also associated with referen- pragmatic requirements (as described below) to cap-
tial indices that specify the entities in the discourse ture their respective discourse functions.
that the entry refers to. For the entry to apply at a So much for syntax. The second part—the seman-
particular stage, its syntactic structure must combine tics of the element—is a formula that specifies the
by LTAG operations with the syntax of the ongoing content that the element carries. Before the entry
utterance. can be used, SPUD must establish that the semantics
REA&apos;S syntactic entries combine typical phrase- holds of the entities the entry describes. If the se-
structure analyses of linguistic constructions with mantics already follows from the common ground,
annotations that describe the occurrence of gestures SPUD assumes that the hearer can use it to help iden-
in coordination with linguistic phrases. Our device tify the entities described. If the semantics is merely
for this is a construction SYNC which pairs a descrip- part of the system&apos;s private knowledge, SPUD treats
tion of a gesture G with the syntactic structure of a it as new information for the hearer.
spoken constituent C: Finally, the third part—the pragmatics of the
SYNC element—is also a formula that SPUD looks to prove
(2) before using the entry. Unlike the semantics, how-
A A ever, the pragmatics does not achieve specific com-
The temporal interpretation of (2) mirrors the rules municative goals like identifying referents. Instead,
for surface synchrony between speech and gesture the pragmatics establishes a general fit between the
presented in (Cassell et aI., 1994). That is, the entry and the context.
preparatory phase of gesture G is set to begin before The entry schematized in (3) illustrates these three
the time constituent c begins; the stroke of gesture components; the entry also suggests how these com-
G (the most effortful part) co-occurs with the most ponents can define coordinated actions of speech
phonologically prominent syllable in c; and, except and gesture that respond coherently to the context.
in cases of coarticulation between successive ges- (3) a syntax:
tures, by the time the constituent c is complete, the NP VP
speaker must be relaxing and bringing the hands out NP:o SYNC
of gesture space (while the generator specifies syn- ihavei G:x NP:A
chrony as described, in practice the synchronization b semantics: have(o.x)
of synthesized speech with graphics is an ongoing c &apos;pragmatics: hearer-new(x) theMe(b).
challenge in the REA--projeet):- insurn; the produc- (3) describes the use of have to introduce a new fea-
tion of gesture G IS synchronized with the produc-
tion of speech c. (Our representation of synchrony
175
ture of (a house) o. The feature, indicated through- tion of the entity x that the gesture describes). We
out the entry by the variable x,.-is realized as the ob- pair (4a) with the semantics in (4c), and thereby
ject NP of the verb have, but x can also form the ba- model that the gesture indicates that one object, x,
sis of a gesture G coordinated with the noun phrase surrounds another, p. Since p cannot be further de-
(as indicated by the SYNC constituent). The entry as- scribed, p must be identified by an additional pre-
serts that o has x. supposition of the gesture which.pieks up_a reference
(3) is a presentational conStruction; in other frame from the shared context.
words, it coordinates non-redundant paired speech Similarly, (4b) describes how we could modify
and gesture in the same way as demonstrated by our the VP introduced by (3) (using the LTAG operation
house description data. To represent this constraint of adjunction), to produce an utterance such as It
on its use, the entry carries two pragmatic require- has a garden surrounding it. By pairing (4b) with
ments: first, x must be new to the hearer; moreover, the same semantics (4c), we ensure that SPUD will
o must link up with the open question in the dis- treat the communicative contribution of the alterna-
course that the sentence responds to. tive constructions of (4) in a parallel fashion. Both
The pragmatic conditions of (3) help support are triggered by accessing background knowledge
our theory of the discourse function of gesture and and both are recognized as directly communicating
speech. A similar kind of sentence could be used specified facts.
to address other open questions in the discourse— 5 Solving the generation problem
for example, to answer which house has a garden? We now sketch how entries such as these combine
This would not be a presentational function, and together to account for REA &apos;s utterances. Our exam-
(3) would be infelicitous here. In that case, gesture ple is the dialogue in (5):
would naturally coordinate with and elaborate on the (5) a User: Tell me more about the house.
answering information—in this case the house. So b REA: It has [a nice garden]. (right hand, held
the different information structure would activate a flat, traces a circle)
different entry, where the gesture would coordinate REA &apos;s response indicates both that the house has a
with the subject and describe o. nice garden and that it surrounds the house.
Meanwhile, alternative entries like (4a) and As we have seen, (5b) represents a common pat-
(4b)—two entries that both convey (4c) and that tern of description; this particular example is moti-
both could combine with (3) by LTAG operations— vated by an exchange two human subjects had in our
underlie our claim that our implementation allows study, cf. (I). (56) represents a solution to a gen-
gesture and speech to draw on a single conceptual eration problem that arises as follows within REA&apos;S
source and fulfill similar communicative intentions. overall architecture. The user&apos;s directive is inter-
(4) a syntax: Ox preted and classified as a directive requiring a delib-
erative response. The dialogue manager recognizes
an obligation to respond to the directive, and con-
cludes that to fulfill the function of presenting the
garden would discharge this obligation. The presen-
tational function grounds out in the communicative
goal to convey a collection of facts about the garden
(type, quality, location relative to the house). Along
with these goals, the dialogue manager supplies its
communicative context, which represents the cen-
trality of the house in attentional prominence, cog-
nitive status and information structure.
En producing (5b) in response to this NW prob-
lem, SPUD both calculates the applicability of and
determines. a .preference for thegexiealized descrip-
tors involved. Initially, (3) is applicable; the system
knows the house has the garden, and represents the
</bodyText>
<table confidence="0.977810166666667">
circular-trajectory S:x
b syntax: NP
NP.:x VP
N P:p I
surrounding
C semantics: surround(x.
</table>
<bodyText confidence="0.926154391304348">
(4a) provides a structure that could substitute for the
node in (3) to produce semantically and pragmat-
ically coordinated speech and gesture. (4a) speci-
fies a right hand gesture 411,whieh-the nand traces -
out a circular trajectory; a further decision must de-
termine the correct handshape (node RS, as a lune-
176
garden as new and the house as questioned. The en-
try can be selected over potential alternatives based
on its interpretation—it achieves a communicative
goal, refers to a prominent entity, and makes a rel-
atively specific connection to facts in the context.
Similarly, in the second. stage, SPUD evaluates and.
selects (4a) because it communicates a needed fact
in a way that helps flesh • out a concise, balanced
communicative act by supplying a gesture that by
using (3) SPUD has already realized belongs here.
Choices of remaining elements—the words garden
and nice, the semantic features to represent the gar-
den in the gesture—proceed similarly. Thus SPUD
arrives at the response in (5b) just by reasoning from
the declarative specification of the meaning and con-
text of communicative actions.
</bodyText>
<sectionHeader confidence="0.999406" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999907605263158">
The interpretation of speech and gesture has been
investigated since the pioneering work of (Bolt,
1980) on deictic gesture; recent work includes
(Koons et al., 1993; Bolt and Heffanz, 1992). Sys-
tems have also attempted generation of gesture in
conjunction with speech. Lester et al. (1998) gener-
ate deictic gestures and choose referring expressions
as a function of the potential ambiguity of objects re-
ferred to, and their proximity to the animated agent.
Rickel and Johnson (1999)&apos;s pedagogical agent pro-
duces a deictic gesture at the beginning of explana-
tions about objects in the virtual world. Andre et
al. (1999) generate pointing gestures as a sub-action
of the rhetorical action of labeling, in turn a sub-
action of elaborating.
Missing from these prior systems, however, is a
representation of communicative action that treats
the different modalities on a par. Such representa-
tions have been explored in research on combining
linguistic and graphical interaction. For example,
multimodal managers have been described to allo-
cate an underlying content representation for gen-
eration of text and graphics (Wahlster et al., 1991.,
Green et al., 1998). Meanwhile, (Johnston et al.,
1997; Johnston, 1998) describe a formalism for
Lightly-coupled interpretation which uses a gram-
mar and semantic constraints to analyze input from
speech and pen. While many insights from these
formalisms are relevant in embodied conversation,
spontaneous gesture requires a distinct analysis with
different emphasis: For example;--we need some no-
tion of discourse pragmatics that would allow us to
predict where gesture occurs with respect to speech,
and what its role might be. Likewise, we need a
model of the communicative effects of spontaneous
coverbal gesture—one that allows us to reason nat-
urally about the multiple goals speakers have in pro-
ducing each utterance.
</bodyText>
<sectionHeader confidence="0.996651" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999962666666667">
Research on the robustness of human conversation
suggests that a dialogue agent capable of acting
as a conversational partner would provide for effi-
cient and natural collaborative dialogue. But human
conversational partners display gestures that derive
from the same underlying conceptual source as their
speech, and which relate appropriately to their corn-
-municative intent. In this paper, we have summa-
rized the evidence for this view of human conver-
sation, and shown how it informs the generation
of communicative action in our artificial embodied
conversational agent, REA. REA has a working im-
plementation, which includes the modules described
in this paper, and can engage in a variety of interac-
tions including that in (5). Experiments are under-
way to investigate the extent to which REA &apos;s conver-
sational capacities share the strengths of the human
capacities they are modeled on.
</bodyText>
<sectionHeader confidence="0.995988" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999515875">
The research reported here was supported by NSF (award
HS-9618939), Deutsche Telekom, AT&amp;T, and the other
generous sponsors of the MIT Media Lab, and a postdoc-
toral fellowship from RUCCS. Hannes Vilhjalmsson as-
sisted with the implementation of REA&apos;S discourse man-
ager_ We thank Nancy Green. James Lester, Jeff Rickel,
Candy Sidner, and anonymous reviewers for comments
on this and earlier drafts.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996773823529412">
Elisabeth Andre:, Thomas Rist, and Jochen Muller. 1999.
Employing Al. methods to control the behavior of ani-
mated interface agents. Applied Artificial intelligence.
13:415-448.
Douglas Appelt. 1985. Planning English Sentences.
Cambridge University Press, Cambridge England.
R. A. Bolt and E. Herranz. 1992. Two-handed gesture in
multi-modal natural dialog. In UIST 92: Fifth Annual
Symposium on User Interface Software and Technol-
ogy.
R. A. Bolt. 1980. Put-that-there: voice and gesture at the
graphics interface. Computer Graphics. I 4(3):262-
270.
J. Cassell and K. Tharissom. 1999. .The power of a nod
and a glance: Envelope vs. emotional feedback in an-
imateci conversational agents. Applied Artificial Intel-
ligence, 13(3).
</reference>
<page confidence="0.989791">
177
</page>
<reference confidence="0.998648901785714">
Justine Cassell, Catherine Pelachaud, Norm Badler,
Mark Steedman, Brett Achorn, Tripp Becket, Brett
Douville. Scott Prevost, and Matthew Stone. 1994.
Animated conversation: Rule-based generation of fa-
cial expression, gesture and spoken intonation for mul-
tiple conversational agents. In SIGGRAPH, pages
413-420. _
J. Cassell, D. McNeill, and K. E. McCullough. 1999.
Speech-gesture mismatches: evidence for one under-
lying representation of linguistic and nonlinguistic in-
formation. Pragmatics and Cognition, 6(2).
Justine Cassell. 2000a. Embodied conversational inter-
face agents. Communications of the ACM, 43(4):70-
78.
Justine Cassell. 2000b. Nudge nudge wink wink: Ele-
ments of face-to-face conversation for embodied con-
versational agents. In J. Cassell, J. Sullivan, S. Pre-
vost, and E. Churchill, editors, Embodied Conversa-
tional Agents, pages 1-28. MIT Press, Cambridge,
MA.
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge MA.
S. Feiner and K. McKeown. 1991. Automating the gen-
eration of coordinated multimedia explanations. IEEE
Computer, 24(10):33-41.
Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev,
Steven Roth, and Johanna Moore. 1998. A media-
independent content language for integrated text and
graphics generation. In CVIR &apos;98 Workshop on Con-
tent Visualization and Intermedia Representations.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharsk.i.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69(2):274-307.
M. Johnston, P. R. Cohen, D. McGee, J. Pittman, S. L.
Oviatt, and I. Smith. 1997. Unification-based multi-
modal integration. In ACLIEACL 97: Proceedings of
the Annual Meeting of the Assocation for Computa-
tional Linguistics.
Michael Johnston. 1998, Unification-based multiniodal
parsing. In COLINGIACL.
Aravind K. Joshi, L. Levy, and M. Takahashi. 1975_ Tree
adjunct grammars. Journal of the Computer and Sys-
tem Sciences.10:136-163.
S. D. Kelly. J. D. Barr, R. B. Church, and K. Lynch. 1999.
Offering a hand to pragmatic understanding: The role
of speech and gesture in comprehension and memory.
Journal of Memory and Language, 40:577-592.
A. Kendon. 1974. Movement coordination in social in-
teraction: somem examples described. in S. Weitz, ed-
itor. Nonverbal Communication. Oxford, New York.
D. B. Koons. C. J. Sparrell, and K. R. Tharisson. 1993.
Integrating simultaneous input from speech. gaze and
hand gestures. In M. T. Maybury,&apos;editor.-Intelkgent
Multi-media Interfilces. MIT Press, Cambridge.
James Lester. Stuart Towns. Charles Calloway, and
Patrick FitzGerald. 1998. Deictic and emotive coni
munication in animated pedagogical agents. In WorA
shop on Embodied Conversational Characters.
David McNeill. 1992. Hand and Mind: What Gesture
Reveal about Thought. University of Chicago Pres
Chicago.
4ohanna,Moore. 49942. Participating in Explanatory Di
alogues. MIT Press, Cambridge MA.
S. L. Oviatt. 1995. Predicting spoken language disflu
encies during human-computer interaction. Compute
Speech and Language, 9( l ):19-35.
Ellen Prince. 1986. On the syntactic marking of pre
supposed open propositions. In Proceedings of th,
22nd Annual Meeting of the Chicago Linguistic Soci
ety, pages 208-222, Chicago. CLS.
Ellen F. Prince. 1992. The ZPG letter: Subjects. definite
ness and information status. In William C. Mann an
Sandra A. Thompson, editors, Discourse Description.
Diverse Analyses of a Fund-raising Text, pages 295-
325. John Benjamins, Philadelphia.
Jeff Rickel and W. Lewis Johnson. 1999. AnimateC
agents for procedural training in virtual reality: Per.
ception, cognition and motor control. Applied Artifi-
cial Intelligence, 13:343-382,
W. T. Rogers. 1978. The contribution of kinesic illus.
trators towards the comprehension of verbal behaviol
within utterances, Human Conzmuniccition Research,
5:54-62.
Yves Schabes. 1990. Mathematical and Computationa,
Aspects of Lexicalized Granunars. Ph.D. thesis, Com-
puter Science Department, University of Pennsylva-
nia.
Mark Steedman. 1991. Structure and intonation. Lan-
guage, 67:260-296.
Matthew Stone and Christine Doran. 1997. Sentence
planning as description using tree-adjoining grammar.
In Proceedings of ACL, pages 198-205.
Matthew Stone, Tonia Bleam, Christine Doran, and
Martha Palmer. 2000. Lexicalized grammar and the
description of motion events. In TAG+: Workshop on
Tree-Adjoining Grammar and Related Formalisms.
Michael Strube. 1998. Never look back: An alternative
to centering. In Proceedings of COLING-ACL.
L. A. Thompson and D. W. Massaro. 1986. Evaluation
and integration of speech and pointing gestures dur-
ing referential understanding. Journal of Experimen-
tal Child Psychology, 42:144-168.
David R. Traum and James F. Allen. 1994. Discourse
obligations in dialogue processing. In ACL, pages 1-
8.
W. Wahlster, E. Andre, W. Graf, and T. Rist. 1991.
Designing illustrated texts. In Proceedings EACL,
pages 8-14.
Hao Yan., 2000. Paired speech,and gesture generation in
embodied conversational agents. Master&apos;s thesis, Me-
dia Lab, MIT.
</reference>
<page confidence="0.997145">
178
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.193820">
<title confidence="0.975292">Coordination and context-dependence in the generation of embodied conversation</title>
<author confidence="0.980137">Justine Cassell</author>
<affiliation confidence="0.9992955">Media Laboratory MIT</affiliation>
<address confidence="0.93809">E15-315 20 Ames, Cambridge MA</address>
<email confidence="0.404747">1justine,yanhaol@media.mit</email>
<affiliation confidence="0.851265">Stone t Yan* tDepartrnent of Computer Science Center for Cognitive Rutgers</affiliation>
<address confidence="0.995239">110 Frelinghuysen, Piscataway NJ 08854-8019</address>
<email confidence="0.99973">edumdstone@cs.rutgers.edu</email>
<abstract confidence="0.993740916666667">We describe the generation of communicative actions in an implemented embodied conversational agent. Our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture that fits the context and the ongoing speech in ways representative of natural human conversation. We accomplish this by reasoning from a grammar which describes gesture declaratively in terms of its discourse function, semantics and synchrony with speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas Rist</author>
<author>Jochen Muller</author>
</authors>
<title>Employing Al. methods to control the behavior of animated interface agents.</title>
<date>1999</date>
<journal>Applied Artificial</journal>
<volume>intelligence.</volume>
<pages>13--415</pages>
<marker>Rist, Muller, 1999</marker>
<rawString>Elisabeth Andre:, Thomas Rist, and Jochen Muller. 1999. Employing Al. methods to control the behavior of animated interface agents. Applied Artificial intelligence. 13:415-448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Appelt</author>
</authors>
<title>Planning English Sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge England.</location>
<contexts>
<context position="2521" citStr="Appelt, 1985" startWordPosition="384" endWordPosition="385">eedlessly, mistake when it is their turn to speak, and so forth when interacting with voice dialogue systems (Oviatt, 1995): In life, noisy situations like these provoke the non-verbal modalities to come into play (Rogers, 1978). In this paper, we describe the generation of communicative actions in an implemented embodied conversational agent. Our generation framework adopts a goal-directed view of generation and casts knowledge about communicative action in the form of a grammar that specifies how forms combine, what interpretive effects they impart and in what contexts they are appropriate (Appelt, 1985; Moore, 1994; Dale, 1992; Stone and Doran, 1997). We expand this framework to take into account findings, by ourselves and others, on the relationship between spontaneous coverbal hand gestures and speech. In particular, our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture. By describing gesture declaratively in terms of its discourse function, semantics and synchrony with speech, we ensure that coverbal gesture fits the context and the ongoing speech in ways representati</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Douglas Appelt. 1985. Planning English Sentences. Cambridge University Press, Cambridge England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Bolt</author>
<author>E Herranz</author>
</authors>
<title>Two-handed gesture in multi-modal natural dialog.</title>
<date>1992</date>
<booktitle>In UIST 92: Fifth Annual Symposium on User Interface Software and Technology.</booktitle>
<marker>Bolt, Herranz, 1992</marker>
<rawString>R. A. Bolt and E. Herranz. 1992. Two-handed gesture in multi-modal natural dialog. In UIST 92: Fifth Annual Symposium on User Interface Software and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Bolt</author>
</authors>
<title>Put-that-there: voice and gesture at the graphics interface. Computer Graphics.</title>
<date>1980</date>
<journal>I</journal>
<pages>4--3</pages>
<contexts>
<context position="27633" citStr="Bolt, 1980" startWordPosition="4378" endWordPosition="4379">and. selects (4a) because it communicates a needed fact in a way that helps flesh • out a concise, balanced communicative act by supplying a gesture that by using (3) SPUD has already realized belongs here. Choices of remaining elements—the words garden and nice, the semantic features to represent the garden in the gesture—proceed similarly. Thus SPUD arrives at the response in (5b) just by reasoning from the declarative specification of the meaning and context of communicative actions. 6 Related Work The interpretation of speech and gesture has been investigated since the pioneering work of (Bolt, 1980) on deictic gesture; recent work includes (Koons et al., 1993; Bolt and Heffanz, 1992). Systems have also attempted generation of gesture in conjunction with speech. Lester et al. (1998) generate deictic gestures and choose referring expressions as a function of the potential ambiguity of objects referred to, and their proximity to the animated agent. Rickel and Johnson (1999)&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects in the virtual world. Andre et al. (1999) generate pointing gestures as a sub-action of the rhetorical action of labeling, in </context>
</contexts>
<marker>Bolt, 1980</marker>
<rawString>R. A. Bolt. 1980. Put-that-there: voice and gesture at the graphics interface. Computer Graphics. I 4(3):262-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>K Tharissom</author>
</authors>
<title>The power of a nod and a glance: Envelope vs. emotional feedback in animateci conversational agents.</title>
<date>1999</date>
<journal>Applied Artificial Intelligence,</journal>
<volume>13</volume>
<issue>3</issue>
<marker>Cassell, Tharissom, 1999</marker>
<rawString>J. Cassell and K. Tharissom. 1999. .The power of a nod and a glance: Envelope vs. emotional feedback in animateci conversational agents. Applied Artificial Intelligence, 13(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Prevost</author>
<author>Matthew Stone</author>
</authors>
<title>Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents.</title>
<date>1994</date>
<booktitle>In SIGGRAPH,</booktitle>
<pages>413--420</pages>
<marker>Prevost, Stone, 1994</marker>
<rawString>Justine Cassell, Catherine Pelachaud, Norm Badler, Mark Steedman, Brett Achorn, Tripp Becket, Brett Douville. Scott Prevost, and Matthew Stone. 1994. Animated conversation: Rule-based generation of facial expression, gesture and spoken intonation for multiple conversational agents. In SIGGRAPH, pages 413-420. _</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cassell</author>
<author>D McNeill</author>
<author>K E McCullough</author>
</authors>
<title>Speech-gesture mismatches: evidence for one underlying representation of linguistic and nonlinguistic information.</title>
<date>1999</date>
<journal>Pragmatics and Cognition,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="4933" citStr="Cassell et al. (1999)" startWordPosition="758" endWordPosition="761">in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners do rely on gestural cues (and, the higher the noise-to-signal ratio, the more facilitation by gesture). Similarly, Cassell et al. (1999) established that listeners rely on information conveyed only in gesture as they try to comprehend a story. Most interesting in terms of building interactive dialogue systems is the semantic and pragmatic relationship between gesture and speech. The two channels do not always manifest the same information, but what they convey is virtually always compatible. Semantically, speech and gesture give a consistent view of an overall situation. For example, gesture may depict the way in which an action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and ge</context>
</contexts>
<marker>Cassell, McNeill, McCullough, 1999</marker>
<rawString>J. Cassell, D. McNeill, and K. E. McCullough. 1999. Speech-gesture mismatches: evidence for one underlying representation of linguistic and nonlinguistic information. Pragmatics and Cognition, 6(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
</authors>
<title>Embodied conversational interface agents.</title>
<date>2000</date>
<journal>Communications of the ACM,</journal>
<pages>43--4</pages>
<contexts>
<context position="1254" citStr="Cassell, 2000" startWordPosition="185" endWordPosition="186">re that fits the context and the ongoing speech in ways representative of natural human conversation. We accomplish this by reasoning from a grammar which describes gesture declaratively in terms of its discourse function, semantics and synchrony with speech. 1 Introduction When we are face-to-face with another human, no matter what our language, cultural background, or age, we virtually all use our faces and hands as an integral part of our dialogue with others. Research on embodied conversational agents aims to imbue interactive dialogue systems with the same nonverbal skills and behaviors (Cassell, 2000a). There is good reason to think that nonverbal behavior will play an important role in evoking from users the kinds of communicative dialogue behaviors they use with other humans, and thus allow them to use the computer with the same kind of efficiency and smoothness that characterizes their dialogues with other people. For example, (Cassell and Thorisson, 1999) show that humans are more likely to consider computers lifelike, and to rate their language skills more highly, when those computers display not only speech but appropriate nonverbal communicative behavior_ This argument takes on par</context>
<context position="4197" citStr="Cassell, 2000" startWordPosition="641" endWordPosition="642">ed graphical body, can sense the user passively through cameras and audio input, and supports communicative actions realized in speech with intonation, facial display, and animated gesture. REA currently offers the reasoning and display capabilities to act as a real estate agent showing &apos;userg Thee-feature s&apos;of v ieus motel of houses. that - appear on-screen behind her. We use existing features of REA here as a research platform for imple171 menting models of the relationship between speech and spontaneous hand gestures during conversation. For more details about the functionality of REA see (Cassell, 2000a). Evidence from many sources suggests that this relationship is a close one. About three-quarters of all clauses in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners </context>
<context position="5773" citStr="Cassell, 2000" startWordPosition="896" endWordPosition="897">esture and speech. The two channels do not always manifest the same information, but what they convey is virtually always compatible. Semantically, speech and gesture give a consistent view of an overall situation. For example, gesture may depict the way in which an action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and gesture mark information about this meaning as advancing the purposes of the conversation in a consistent way. Indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (Cassell, 2000b). The semantic and pragmatic compatibility seen in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations (Feiner and McKeown, 1991; Green et al., 1998; Wahlster et al., 1991). In fact, some suggest (McNeill, 1992), that gesture and speech arise together from an underlying representation that has both visual and linguistic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension. This theoretical perspective on speech and gesture involves two key claims with computational i</context>
<context position="12579" citStr="Cassell, 2000" startWordPosition="1976" endWordPosition="1977">ommunicative goals, triggers the generation process, and realizes the resulting speech and gesture. The dialogue manager is only one component in a multithreaded architecture that carries out hardwired reactions to input as well as deliberative processing. The diversity is required in order to exhibit appropriate interactional and propositional conversational behaviors at a range of time scales, from tracking the user&apos;s movements with gaze and providing nods and other feedback as the user speaks, to participating in routine exchanges and generating principled responses to user&apos;s queries. See (Cassell, 2000a) for description and motivation of the architecture, as well as the conversational functions and behaviors it supports. REA&apos;S design and capabilities reflect our research focus on allying conversational content with conversation management, and allying nonverbal modalities with speech: how can an embodied-agent aseall its communicative modalities to contribute new content when needed (propositional function), to signal 173 Another component of context is provided by a domain knowledge base, consisting of facts explicitly labeled with the kind of information they represent. This defines the c</context>
</contexts>
<marker>Cassell, 2000</marker>
<rawString>Justine Cassell. 2000a. Embodied conversational interface agents. Communications of the ACM, 43(4):70-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justine Cassell</author>
</authors>
<title>Nudge nudge wink wink: Elements of face-to-face conversation for embodied conversational agents. In</title>
<date>2000</date>
<booktitle>Embodied Conversational Agents,</booktitle>
<pages>1--28</pages>
<editor>J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1254" citStr="Cassell, 2000" startWordPosition="185" endWordPosition="186">re that fits the context and the ongoing speech in ways representative of natural human conversation. We accomplish this by reasoning from a grammar which describes gesture declaratively in terms of its discourse function, semantics and synchrony with speech. 1 Introduction When we are face-to-face with another human, no matter what our language, cultural background, or age, we virtually all use our faces and hands as an integral part of our dialogue with others. Research on embodied conversational agents aims to imbue interactive dialogue systems with the same nonverbal skills and behaviors (Cassell, 2000a). There is good reason to think that nonverbal behavior will play an important role in evoking from users the kinds of communicative dialogue behaviors they use with other humans, and thus allow them to use the computer with the same kind of efficiency and smoothness that characterizes their dialogues with other people. For example, (Cassell and Thorisson, 1999) show that humans are more likely to consider computers lifelike, and to rate their language skills more highly, when those computers display not only speech but appropriate nonverbal communicative behavior_ This argument takes on par</context>
<context position="4197" citStr="Cassell, 2000" startWordPosition="641" endWordPosition="642">ed graphical body, can sense the user passively through cameras and audio input, and supports communicative actions realized in speech with intonation, facial display, and animated gesture. REA currently offers the reasoning and display capabilities to act as a real estate agent showing &apos;userg Thee-feature s&apos;of v ieus motel of houses. that - appear on-screen behind her. We use existing features of REA here as a research platform for imple171 menting models of the relationship between speech and spontaneous hand gestures during conversation. For more details about the functionality of REA see (Cassell, 2000a). Evidence from many sources suggests that this relationship is a close one. About three-quarters of all clauses in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners </context>
<context position="5773" citStr="Cassell, 2000" startWordPosition="896" endWordPosition="897">esture and speech. The two channels do not always manifest the same information, but what they convey is virtually always compatible. Semantically, speech and gesture give a consistent view of an overall situation. For example, gesture may depict the way in which an action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and gesture mark information about this meaning as advancing the purposes of the conversation in a consistent way. Indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (Cassell, 2000b). The semantic and pragmatic compatibility seen in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations (Feiner and McKeown, 1991; Green et al., 1998; Wahlster et al., 1991). In fact, some suggest (McNeill, 1992), that gesture and speech arise together from an underlying representation that has both visual and linguistic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension. This theoretical perspective on speech and gesture involves two key claims with computational i</context>
<context position="12579" citStr="Cassell, 2000" startWordPosition="1976" endWordPosition="1977">ommunicative goals, triggers the generation process, and realizes the resulting speech and gesture. The dialogue manager is only one component in a multithreaded architecture that carries out hardwired reactions to input as well as deliberative processing. The diversity is required in order to exhibit appropriate interactional and propositional conversational behaviors at a range of time scales, from tracking the user&apos;s movements with gaze and providing nods and other feedback as the user speaks, to participating in routine exchanges and generating principled responses to user&apos;s queries. See (Cassell, 2000a) for description and motivation of the architecture, as well as the conversational functions and behaviors it supports. REA&apos;S design and capabilities reflect our research focus on allying conversational content with conversation management, and allying nonverbal modalities with speech: how can an embodied-agent aseall its communicative modalities to contribute new content when needed (propositional function), to signal 173 Another component of context is provided by a domain knowledge base, consisting of facts explicitly labeled with the kind of information they represent. This defines the c</context>
</contexts>
<marker>Cassell, 2000</marker>
<rawString>Justine Cassell. 2000b. Nudge nudge wink wink: Elements of face-to-face conversation for embodied conversational agents. In J. Cassell, J. Sullivan, S. Prevost, and E. Churchill, editors, Embodied Conversational Agents, pages 1-28. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="2546" citStr="Dale, 1992" startWordPosition="388" endWordPosition="389">is their turn to speak, and so forth when interacting with voice dialogue systems (Oviatt, 1995): In life, noisy situations like these provoke the non-verbal modalities to come into play (Rogers, 1978). In this paper, we describe the generation of communicative actions in an implemented embodied conversational agent. Our generation framework adopts a goal-directed view of generation and casts knowledge about communicative action in the form of a grammar that specifies how forms combine, what interpretive effects they impart and in what contexts they are appropriate (Appelt, 1985; Moore, 1994; Dale, 1992; Stone and Doran, 1997). We expand this framework to take into account findings, by ourselves and others, on the relationship between spontaneous coverbal hand gestures and speech. In particular, our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture. By describing gesture declaratively in terms of its discourse function, semantics and synchrony with speech, we ensure that coverbal gesture fits the context and the ongoing speech in ways representative of natural human conve</context>
<context position="10246" citStr="Dale, 1992" startWordPosition="1618" endWordPosition="1619">st contribute a wealth of information about the domain and the conversation to represent the communicative context. This detail is needed for REA to achieve a theoretically-motivated realization of the common patterns of speech and gesture we observed in human conversation. For example, a variety of changing features determine whether marked forms in speech and gesture are appropriate in the context. REA&apos;s dialogue manager tracks the changing status of such features as: O Attentional prominence, represented (as usual in natural language generation) by setting up a context set for each entity (Dale, 1992). Our model of prominence is a simple local one similar to (Strube, 1998). ▪ Cognitive status, including whether an entity is hearer-old or hearer-new (Prince, 1992), and whether an entity is in-focus or not (Gundel et al., 1993). We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus. a Information structure, including the open propositions or, following (Steedman, 1991), themes, which describe the salient questions currently at issue in the discourse (Prince, 1986). In REA&apos;s dialogue, open q</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Robert Dale. 1992. Generating Referring Expressions: Constructing Descriptions in a Domain of Objects and Processes. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Feiner</author>
<author>K McKeown</author>
</authors>
<title>Automating the generation of coordinated multimedia explanations.</title>
<date>1991</date>
<journal>IEEE Computer,</journal>
<pages>24--10</pages>
<contexts>
<context position="5956" citStr="Feiner and McKeown, 1991" startWordPosition="920" endWordPosition="923">a consistent view of an overall situation. For example, gesture may depict the way in which an action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and gesture mark information about this meaning as advancing the purposes of the conversation in a consistent way. Indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (Cassell, 2000b). The semantic and pragmatic compatibility seen in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations (Feiner and McKeown, 1991; Green et al., 1998; Wahlster et al., 1991). In fact, some suggest (McNeill, 1992), that gesture and speech arise together from an underlying representation that has both visual and linguistic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension. This theoretical perspective on speech and gesture involves two key claims with computational import: that gesture and speech .reflect-a- common conceptual source; and that the content and form of a gesture is tuned to the communicative context and the actor&apos;s communicative int</context>
</contexts>
<marker>Feiner, McKeown, 1991</marker>
<rawString>S. Feiner and K. McKeown. 1991. Automating the generation of coordinated multimedia explanations. IEEE Computer, 24(10):33-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Green</author>
<author>Giuseppe Carenini</author>
<author>Stephan Kerpedjiev</author>
<author>Steven Roth</author>
<author>Johanna Moore</author>
</authors>
<title>A mediaindependent content language for integrated text and graphics generation.</title>
<date>1998</date>
<booktitle>In CVIR &apos;98 Workshop on Content Visualization and Intermedia Representations.</booktitle>
<contexts>
<context position="5976" citStr="Green et al., 1998" startWordPosition="924" endWordPosition="927">erall situation. For example, gesture may depict the way in which an action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and gesture mark information about this meaning as advancing the purposes of the conversation in a consistent way. Indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (Cassell, 2000b). The semantic and pragmatic compatibility seen in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations (Feiner and McKeown, 1991; Green et al., 1998; Wahlster et al., 1991). In fact, some suggest (McNeill, 1992), that gesture and speech arise together from an underlying representation that has both visual and linguistic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension. This theoretical perspective on speech and gesture involves two key claims with computational import: that gesture and speech .reflect-a- common conceptual source; and that the content and form of a gesture is tuned to the communicative context and the actor&apos;s communicative intentions. We believe </context>
<context position="28685" citStr="Green et al., 1998" startWordPosition="4542" endWordPosition="4545"> beginning of explanations about objects in the virtual world. Andre et al. (1999) generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a subaction of elaborating. Missing from these prior systems, however, is a representation of communicative action that treats the different modalities on a par. Such representations have been explored in research on combining linguistic and graphical interaction. For example, multimodal managers have been described to allocate an underlying content representation for generation of text and graphics (Wahlster et al., 1991., Green et al., 1998). Meanwhile, (Johnston et al., 1997; Johnston, 1998) describe a formalism for Lightly-coupled interpretation which uses a grammar and semantic constraints to analyze input from speech and pen. While many insights from these formalisms are relevant in embodied conversation, spontaneous gesture requires a distinct analysis with different emphasis: For example;--we need some notion of discourse pragmatics that would allow us to predict where gesture occurs with respect to speech, and what its role might be. Likewise, we need a model of the communicative effects of spontaneous coverbal gesture—one</context>
</contexts>
<marker>Green, Carenini, Kerpedjiev, Roth, Moore, 1998</marker>
<rawString>Nancy Green, Giuseppe Carenini, Stephan Kerpedjiev, Steven Roth, and Johanna Moore. 1998. A mediaindependent content language for integrated text and graphics generation. In CVIR &apos;98 Workshop on Content Visualization and Intermedia Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeanette K Gundel</author>
<author>Nancy Hedberg</author>
<author>Ron Zacharsk i</author>
</authors>
<title>Cognitive status and the form of referring expressions in discourse.</title>
<date>1993</date>
<journal>Language,</journal>
<pages>69--2</pages>
<contexts>
<context position="10475" citStr="Gundel et al., 1993" startWordPosition="1655" endWordPosition="1658"> speech and gesture we observed in human conversation. For example, a variety of changing features determine whether marked forms in speech and gesture are appropriate in the context. REA&apos;s dialogue manager tracks the changing status of such features as: O Attentional prominence, represented (as usual in natural language generation) by setting up a context set for each entity (Dale, 1992). Our model of prominence is a simple local one similar to (Strube, 1998). ▪ Cognitive status, including whether an entity is hearer-old or hearer-new (Prince, 1992), and whether an entity is in-focus or not (Gundel et al., 1993). We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus. a Information structure, including the open propositions or, following (Steedman, 1991), themes, which describe the salient questions currently at issue in the discourse (Prince, 1986). In REA&apos;s dialogue, open questions are always general questions about some entity raised by a recent turn; although in principle such an open question ought to be formalized as theme(A.P.Pe), REA can use the simpler therne(e). In fact, both speech and ges</context>
</contexts>
<marker>Gundel, Hedberg, i, 1993</marker>
<rawString>Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharsk.i. 1993. Cognitive status and the form of referring expressions in discourse. Language, 69(2):274-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>P R Cohen</author>
<author>D McGee</author>
<author>J Pittman</author>
<author>S L Oviatt</author>
<author>I Smith</author>
</authors>
<title>Unification-based multimodal integration.</title>
<date>1997</date>
<booktitle>In ACLIEACL 97: Proceedings of the Annual Meeting of the Assocation for Computational Linguistics.</booktitle>
<contexts>
<context position="28720" citStr="Johnston et al., 1997" startWordPosition="4547" endWordPosition="4550">objects in the virtual world. Andre et al. (1999) generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a subaction of elaborating. Missing from these prior systems, however, is a representation of communicative action that treats the different modalities on a par. Such representations have been explored in research on combining linguistic and graphical interaction. For example, multimodal managers have been described to allocate an underlying content representation for generation of text and graphics (Wahlster et al., 1991., Green et al., 1998). Meanwhile, (Johnston et al., 1997; Johnston, 1998) describe a formalism for Lightly-coupled interpretation which uses a grammar and semantic constraints to analyze input from speech and pen. While many insights from these formalisms are relevant in embodied conversation, spontaneous gesture requires a distinct analysis with different emphasis: For example;--we need some notion of discourse pragmatics that would allow us to predict where gesture occurs with respect to speech, and what its role might be. Likewise, we need a model of the communicative effects of spontaneous coverbal gesture—one that allows us to reason naturally</context>
</contexts>
<marker>Johnston, Cohen, McGee, Pittman, Oviatt, Smith, 1997</marker>
<rawString>M. Johnston, P. R. Cohen, D. McGee, J. Pittman, S. L. Oviatt, and I. Smith. 1997. Unification-based multimodal integration. In ACLIEACL 97: Proceedings of the Annual Meeting of the Assocation for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Johnston</author>
</authors>
<title>Unification-based multiniodal parsing.</title>
<date>1998</date>
<booktitle>In COLINGIACL.</booktitle>
<contexts>
<context position="28737" citStr="Johnston, 1998" startWordPosition="4551" endWordPosition="4552">world. Andre et al. (1999) generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a subaction of elaborating. Missing from these prior systems, however, is a representation of communicative action that treats the different modalities on a par. Such representations have been explored in research on combining linguistic and graphical interaction. For example, multimodal managers have been described to allocate an underlying content representation for generation of text and graphics (Wahlster et al., 1991., Green et al., 1998). Meanwhile, (Johnston et al., 1997; Johnston, 1998) describe a formalism for Lightly-coupled interpretation which uses a grammar and semantic constraints to analyze input from speech and pen. While many insights from these formalisms are relevant in embodied conversation, spontaneous gesture requires a distinct analysis with different emphasis: For example;--we need some notion of discourse pragmatics that would allow us to predict where gesture occurs with respect to speech, and what its role might be. Likewise, we need a model of the communicative effects of spontaneous coverbal gesture—one that allows us to reason naturally about the multip</context>
</contexts>
<marker>Johnston, 1998</marker>
<rawString>Michael Johnston. 1998, Unification-based multiniodal parsing. In COLINGIACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>L Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of the Computer and System</journal>
<pages>10--136</pages>
<contexts>
<context position="19567" citStr="Joshi et al., 1975" startWordPosition="3062" endWordPosition="3065">s in more detail. Each entry is specified space). Our current grammar does not undertake the in three parts. The first part—the syntax of the final step of associating semantic features to choice element—sets out what words or other actions the of particular handshapes and movements, or gesture element contributes to its utterance. The syn- morphology; we reserve this problem for later in tax is a hierarchical structure, formalized using the research program. We allow gesture to accomFeature-Based Lexicalized Tree Adjoining Gram- pany alternative constituents by introducing alternamar (LTAG) (Joshi et al., 1975; Schabes, 1990). tive syntactic entries; these entries take on different Syntactic structures are also associated with referen- pragmatic requirements (as described below) to captial indices that specify the entities in the discourse ture their respective discourse functions. that the entry refers to. For the entry to apply at a So much for syntax. The second part—the semanparticular stage, its syntactic structure must combine tics of the element—is a formula that specifies the by LTAG operations with the syntax of the ongoing content that the element carries. Before the entry utterance. can </context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, L. Levy, and M. Takahashi. 1975_ Tree adjunct grammars. Journal of the Computer and System Sciences.10:136-163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Barr</author>
<author>R B Church</author>
<author>K Lynch</author>
</authors>
<title>Offering a hand to pragmatic understanding: The role of speech and gesture in comprehension and memory.</title>
<date>1999</date>
<journal>Journal of Memory and Language,</journal>
<pages>40--577</pages>
<marker>Barr, Church, Lynch, 1999</marker>
<rawString>S. D. Kelly. J. D. Barr, R. B. Church, and K. Lynch. 1999. Offering a hand to pragmatic understanding: The role of speech and gesture in comprehension and memory. Journal of Memory and Language, 40:577-592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Movement coordination in social interaction: somem examples described.</title>
<date>1974</date>
<booktitle>Nonverbal Communication.</booktitle>
<editor>in S. Weitz, editor.</editor>
<location>Oxford, New York.</location>
<contexts>
<context position="4587" citStr="Kendon, 1974" startWordPosition="704" endWordPosition="705">eatures of REA here as a research platform for imple171 menting models of the relationship between speech and spontaneous hand gestures during conversation. For more details about the functionality of REA see (Cassell, 2000a). Evidence from many sources suggests that this relationship is a close one. About three-quarters of all clauses in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners do rely on gestural cues (and, the higher the noise-to-signal ratio, the more facilitation by gesture). Similarly, Cassell et al. (1999) established that listeners rely on information conveyed only in gesture as they try to comprehend a story. Most interesting in terms of building interactive dialogue systems is the semantic and pragmatic relationship between gesture and speech. The two </context>
</contexts>
<marker>Kendon, 1974</marker>
<rawString>A. Kendon. 1974. Movement coordination in social interaction: somem examples described. in S. Weitz, editor. Nonverbal Communication. Oxford, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Sparrell</author>
<author>K R Tharisson</author>
</authors>
<title>Integrating simultaneous input from speech. gaze and hand gestures.</title>
<date>1993</date>
<editor>In M. T. Maybury,&apos;editor.-Intelkgent</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Sparrell, Tharisson, 1993</marker>
<rawString>D. B. Koons. C. J. Sparrell, and K. R. Tharisson. 1993. Integrating simultaneous input from speech. gaze and hand gestures. In M. T. Maybury,&apos;editor.-Intelkgent Multi-media Interfilces. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Towns Charles Calloway</author>
<author>Patrick FitzGerald</author>
</authors>
<title>Deictic and emotive coni munication in animated pedagogical agents.</title>
<date>1998</date>
<booktitle>In WorA shop on Embodied Conversational Characters.</booktitle>
<marker>Calloway, FitzGerald, 1998</marker>
<rawString>James Lester. Stuart Towns. Charles Calloway, and Patrick FitzGerald. 1998. Deictic and emotive coni munication in animated pedagogical agents. In WorA shop on Embodied Conversational Characters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McNeill</author>
</authors>
<title>Hand and Mind: What Gesture Reveal about Thought.</title>
<date>1992</date>
<institution>University of Chicago Pres Chicago.</institution>
<contexts>
<context position="4401" citStr="McNeill, 1992" startWordPosition="673" endWordPosition="674"> offers the reasoning and display capabilities to act as a real estate agent showing &apos;userg Thee-feature s&apos;of v ieus motel of houses. that - appear on-screen behind her. We use existing features of REA here as a research platform for imple171 menting models of the relationship between speech and spontaneous hand gestures during conversation. For more details about the functionality of REA see (Cassell, 2000a). Evidence from many sources suggests that this relationship is a close one. About three-quarters of all clauses in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners do rely on gestural cues (and, the higher the noise-to-signal ratio, the more facilitation by gesture). Similarly, Cassell et al. (1999) established that listeners rely on information conveyed only in ges</context>
<context position="6039" citStr="McNeill, 1992" startWordPosition="936" endWordPosition="937">n action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and gesture mark information about this meaning as advancing the purposes of the conversation in a consistent way. Indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (Cassell, 2000b). The semantic and pragmatic compatibility seen in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations (Feiner and McKeown, 1991; Green et al., 1998; Wahlster et al., 1991). In fact, some suggest (McNeill, 1992), that gesture and speech arise together from an underlying representation that has both visual and linguistic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension. This theoretical perspective on speech and gesture involves two key claims with computational import: that gesture and speech .reflect-a- common conceptual source; and that the content and form of a gesture is tuned to the communicative context and the actor&apos;s communicative intentions. We believe that these characteristics of the use of gesture are universal,</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>David McNeill. 1992. Hand and Mind: What Gesture Reveal about Thought. University of Chicago Pres Chicago.</rawString>
</citation>
<citation valid="false">
<title>Participating in Explanatory Di alogues.</title>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<marker></marker>
<rawString>4ohanna,Moore. 49942. Participating in Explanatory Di alogues. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Predicting spoken language disflu encies during human-computer interaction. Compute Speech and Language, 9( l</title>
<date>1995</date>
<contexts>
<context position="2032" citStr="Oviatt, 1995" startWordPosition="310" endWordPosition="311">th other humans, and thus allow them to use the computer with the same kind of efficiency and smoothness that characterizes their dialogues with other people. For example, (Cassell and Thorisson, 1999) show that humans are more likely to consider computers lifelike, and to rate their language skills more highly, when those computers display not only speech but appropriate nonverbal communicative behavior_ This argument takes on particular importance given that users repeat themselves needlessly, mistake when it is their turn to speak, and so forth when interacting with voice dialogue systems (Oviatt, 1995): In life, noisy situations like these provoke the non-verbal modalities to come into play (Rogers, 1978). In this paper, we describe the generation of communicative actions in an implemented embodied conversational agent. Our generation framework adopts a goal-directed view of generation and casts knowledge about communicative action in the form of a grammar that specifies how forms combine, what interpretive effects they impart and in what contexts they are appropriate (Appelt, 1985; Moore, 1994; Dale, 1992; Stone and Doran, 1997). We expand this framework to take into account findings, by o</context>
</contexts>
<marker>Oviatt, 1995</marker>
<rawString>S. L. Oviatt. 1995. Predicting spoken language disflu encies during human-computer interaction. Compute Speech and Language, 9( l ):19-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Prince</author>
</authors>
<title>On the syntactic marking of pre supposed open propositions.</title>
<date>1986</date>
<booktitle>In Proceedings of th, 22nd Annual Meeting of the Chicago Linguistic Soci ety,</booktitle>
<pages>208--222</pages>
<location>Chicago. CLS.</location>
<contexts>
<context position="10819" citStr="Prince, 1986" startWordPosition="1708" endWordPosition="1709">context set for each entity (Dale, 1992). Our model of prominence is a simple local one similar to (Strube, 1998). ▪ Cognitive status, including whether an entity is hearer-old or hearer-new (Prince, 1992), and whether an entity is in-focus or not (Gundel et al., 1993). We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus. a Information structure, including the open propositions or, following (Steedman, 1991), themes, which describe the salient questions currently at issue in the discourse (Prince, 1986). In REA&apos;s dialogue, open questions are always general questions about some entity raised by a recent turn; although in principle such an open question ought to be formalized as theme(A.P.Pe), REA can use the simpler therne(e). In fact, both speech and gesture depend on the same kinds of featurestand.acces-s-them in the&apos; same way;- this specification of the dialogue state crosscuts distinctions of communicative modality. Figure l: Interacting with REA transcriptions (recall) and apply with an accuracy of 96% (precision). These patterns provide a concrete specification for the main communicativ</context>
</contexts>
<marker>Prince, 1986</marker>
<rawString>Ellen Prince. 1986. On the syntactic marking of pre supposed open propositions. In Proceedings of th, 22nd Annual Meeting of the Chicago Linguistic Soci ety, pages 208-222, Chicago. CLS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen F Prince</author>
</authors>
<title>The ZPG letter: Subjects. definite ness and information status. In</title>
<date>1992</date>
<booktitle>Discourse Description. Diverse Analyses of a Fund-raising Text,</booktitle>
<pages>295--325</pages>
<editor>William C. Mann an Sandra A. Thompson, editors,</editor>
<publisher>John Benjamins,</publisher>
<location>Philadelphia.</location>
<contexts>
<context position="10411" citStr="Prince, 1992" startWordPosition="1645" endWordPosition="1646">retically-motivated realization of the common patterns of speech and gesture we observed in human conversation. For example, a variety of changing features determine whether marked forms in speech and gesture are appropriate in the context. REA&apos;s dialogue manager tracks the changing status of such features as: O Attentional prominence, represented (as usual in natural language generation) by setting up a context set for each entity (Dale, 1992). Our model of prominence is a simple local one similar to (Strube, 1998). ▪ Cognitive status, including whether an entity is hearer-old or hearer-new (Prince, 1992), and whether an entity is in-focus or not (Gundel et al., 1993). We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus. a Information structure, including the open propositions or, following (Steedman, 1991), themes, which describe the salient questions currently at issue in the discourse (Prince, 1986). In REA&apos;s dialogue, open questions are always general questions about some entity raised by a recent turn; although in principle such an open question ought to be formalized as theme(A.P.Pe),</context>
</contexts>
<marker>Prince, 1992</marker>
<rawString>Ellen F. Prince. 1992. The ZPG letter: Subjects. definite ness and information status. In William C. Mann an Sandra A. Thompson, editors, Discourse Description. Diverse Analyses of a Fund-raising Text, pages 295-325. John Benjamins, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Rickel</author>
<author>W Lewis Johnson</author>
</authors>
<title>AnimateC agents for procedural training in virtual reality: Per. ception, cognition and motor control.</title>
<date>1999</date>
<journal>Applied Artificial Intelligence,</journal>
<pages>13--343</pages>
<contexts>
<context position="28012" citStr="Rickel and Johnson (1999)" startWordPosition="4437" endWordPosition="4440">the response in (5b) just by reasoning from the declarative specification of the meaning and context of communicative actions. 6 Related Work The interpretation of speech and gesture has been investigated since the pioneering work of (Bolt, 1980) on deictic gesture; recent work includes (Koons et al., 1993; Bolt and Heffanz, 1992). Systems have also attempted generation of gesture in conjunction with speech. Lester et al. (1998) generate deictic gestures and choose referring expressions as a function of the potential ambiguity of objects referred to, and their proximity to the animated agent. Rickel and Johnson (1999)&apos;s pedagogical agent produces a deictic gesture at the beginning of explanations about objects in the virtual world. Andre et al. (1999) generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a subaction of elaborating. Missing from these prior systems, however, is a representation of communicative action that treats the different modalities on a par. Such representations have been explored in research on combining linguistic and graphical interaction. For example, multimodal managers have been described to allocate an underlying content representation for gen</context>
</contexts>
<marker>Rickel, Johnson, 1999</marker>
<rawString>Jeff Rickel and W. Lewis Johnson. 1999. AnimateC agents for procedural training in virtual reality: Per. ception, cognition and motor control. Applied Artificial Intelligence, 13:343-382,</rawString>
</citation>
<citation valid="true">
<authors>
<author>W T Rogers</author>
</authors>
<title>The contribution of kinesic illus. trators towards the comprehension of verbal behaviol within utterances, Human Conzmuniccition Research,</title>
<date>1978</date>
<pages>5--54</pages>
<contexts>
<context position="2137" citStr="Rogers, 1978" startWordPosition="327" endWordPosition="328">that characterizes their dialogues with other people. For example, (Cassell and Thorisson, 1999) show that humans are more likely to consider computers lifelike, and to rate their language skills more highly, when those computers display not only speech but appropriate nonverbal communicative behavior_ This argument takes on particular importance given that users repeat themselves needlessly, mistake when it is their turn to speak, and so forth when interacting with voice dialogue systems (Oviatt, 1995): In life, noisy situations like these provoke the non-verbal modalities to come into play (Rogers, 1978). In this paper, we describe the generation of communicative actions in an implemented embodied conversational agent. Our generation framework adopts a goal-directed view of generation and casts knowledge about communicative action in the form of a grammar that specifies how forms combine, what interpretive effects they impart and in what contexts they are appropriate (Appelt, 1985; Moore, 1994; Dale, 1992; Stone and Doran, 1997). We expand this framework to take into account findings, by ourselves and others, on the relationship between spontaneous coverbal hand gestures and speech. In partic</context>
<context position="4785" citStr="Rogers, 1978" startWordPosition="737" endWordPosition="738">of REA see (Cassell, 2000a). Evidence from many sources suggests that this relationship is a close one. About three-quarters of all clauses in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners do rely on gestural cues (and, the higher the noise-to-signal ratio, the more facilitation by gesture). Similarly, Cassell et al. (1999) established that listeners rely on information conveyed only in gesture as they try to comprehend a story. Most interesting in terms of building interactive dialogue systems is the semantic and pragmatic relationship between gesture and speech. The two channels do not always manifest the same information, but what they convey is virtually always compatible. Semantically, speech and gesture give a consistent view of an overall situation. For exampl</context>
</contexts>
<marker>Rogers, 1978</marker>
<rawString>W. T. Rogers. 1978. The contribution of kinesic illus. trators towards the comprehension of verbal behaviol within utterances, Human Conzmuniccition Research, 5:54-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computationa, Aspects of Lexicalized Granunars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, University of Pennsylvania.</institution>
<contexts>
<context position="19583" citStr="Schabes, 1990" startWordPosition="3066" endWordPosition="3067">ch entry is specified space). Our current grammar does not undertake the in three parts. The first part—the syntax of the final step of associating semantic features to choice element—sets out what words or other actions the of particular handshapes and movements, or gesture element contributes to its utterance. The syn- morphology; we reserve this problem for later in tax is a hierarchical structure, formalized using the research program. We allow gesture to accomFeature-Based Lexicalized Tree Adjoining Gram- pany alternative constituents by introducing alternamar (LTAG) (Joshi et al., 1975; Schabes, 1990). tive syntactic entries; these entries take on different Syntactic structures are also associated with referen- pragmatic requirements (as described below) to captial indices that specify the entities in the discourse ture their respective discourse functions. that the entry refers to. For the entry to apply at a So much for syntax. The second part—the semanparticular stage, its syntactic structure must combine tics of the element—is a formula that specifies the by LTAG operations with the syntax of the ongoing content that the element carries. Before the entry utterance. can be used, SPUD mu</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and Computationa, Aspects of Lexicalized Granunars. Ph.D. thesis, Computer Science Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<date>1991</date>
<booktitle>Structure and intonation. Language,</booktitle>
<pages>67--260</pages>
<contexts>
<context position="10722" citStr="Steedman, 1991" startWordPosition="1694" endWordPosition="1695">s: O Attentional prominence, represented (as usual in natural language generation) by setting up a context set for each entity (Dale, 1992). Our model of prominence is a simple local one similar to (Strube, 1998). ▪ Cognitive status, including whether an entity is hearer-old or hearer-new (Prince, 1992), and whether an entity is in-focus or not (Gundel et al., 1993). We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus. a Information structure, including the open propositions or, following (Steedman, 1991), themes, which describe the salient questions currently at issue in the discourse (Prince, 1986). In REA&apos;s dialogue, open questions are always general questions about some entity raised by a recent turn; although in principle such an open question ought to be formalized as theme(A.P.Pe), REA can use the simpler therne(e). In fact, both speech and gesture depend on the same kinds of featurestand.acces-s-them in the&apos; same way;- this specification of the dialogue state crosscuts distinctions of communicative modality. Figure l: Interacting with REA transcriptions (recall) and apply with an accur</context>
</contexts>
<marker>Steedman, 1991</marker>
<rawString>Mark Steedman. 1991. Structure and intonation. Language, 67:260-296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
</authors>
<title>Sentence planning as description using tree-adjoining grammar.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>198--205</pages>
<contexts>
<context position="2570" citStr="Stone and Doran, 1997" startWordPosition="390" endWordPosition="393">n to speak, and so forth when interacting with voice dialogue systems (Oviatt, 1995): In life, noisy situations like these provoke the non-verbal modalities to come into play (Rogers, 1978). In this paper, we describe the generation of communicative actions in an implemented embodied conversational agent. Our generation framework adopts a goal-directed view of generation and casts knowledge about communicative action in the form of a grammar that specifies how forms combine, what interpretive effects they impart and in what contexts they are appropriate (Appelt, 1985; Moore, 1994; Dale, 1992; Stone and Doran, 1997). We expand this framework to take into account findings, by ourselves and others, on the relationship between spontaneous coverbal hand gestures and speech. In particular, our agent plans each utterance so that multiple communicative goals may be realized opportunistically by a composite action including not only speech but also coverbal gesture. By describing gesture declaratively in terms of its discourse function, semantics and synchrony with speech, we ensure that coverbal gesture fits the context and the ongoing speech in ways representative of natural human conversation. The result is a</context>
<context position="16607" citStr="Stone and Doran, 1997" startWordPosition="2609" endWordPosition="2613">ptors. The generation task in REA thus involves selecting a number of such lexicalized descriptors and organizing them into a grammatical whole that manifests the right semantic and pragmatic coordination between speech and gesture. The information conveyed must be enough that the hearer can identify the entity in each domain reference from among its context set. Moreover, the descriptors must provide a source which allows the hearer to recover any needed new domain proposition, either explicitly or by inference. We use the SPUD generator (&amp;quot;Sentence Planning Using Description&amp;quot;) introduced in (Stone and Doran, 1997) to carry out this task for REA. SPUD builds the utterance element-by-element; at each stage of construction, SPUD&apos;S representation of the current, incomplete utterance specifies its syntax, semantics, interpretation and fit to context. This representation both allows SPUD to determine which lexicalized descriptors are available at each stage to extend the utterance, and to assess the progress towards its communicative goals which each extension would bring about. At each stage, then, SPUD selects the available option that offers the best immediate advance toward completing the utterance succe</context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>Matthew Stone and Christine Doran. 1997. Sentence planning as description using tree-adjoining grammar. In Proceedings of ACL, pages 198-205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Tonia Bleam</author>
<author>Christine Doran</author>
<author>Martha Palmer</author>
</authors>
<title>Lexicalized grammar and the description of motion events.</title>
<date>2000</date>
<booktitle>In TAG+: Workshop on Tree-Adjoining Grammar and Related Formalisms.</booktitle>
<contexts>
<context position="17543" citStr="Stone et al., 2000" startWordPosition="2751" endWordPosition="2754">are available at each stage to extend the utterance, and to assess the progress towards its communicative goals which each extension would bring about. At each stage, then, SPUD selects the available option that offers the best immediate advance toward completing the utterance successfully. (We have developed a suite of guidelines for the design of syntactic structures, semantic and pragmatic representations, and the interface between them so that SPUD&apos;S greedy search, which is necessary for real-time performance, succeeds in finding concise and effective Utterances -described by the grammar (Stone et al., 2000).) As part of the development Of REA, we have constructed a new inventory of lexicalized descriptors. REA &apos;s descriptors consist of entries that contribute to coverbal gestures, as well as revised entries for spoken words that allow for their coordination with gesture under appropriate discourse conditions. The -- -organization ofthese entries assures‘that=tising the same mechanism as with speech—REA&apos;S gestures draw on the single available conceptual representa174 tion and that both REA&apos;S gesture and the relation- in a single tree conveniently allows modules down.- ship between gesture and spe</context>
</contexts>
<marker>Stone, Bleam, Doran, Palmer, 2000</marker>
<rawString>Matthew Stone, Tonia Bleam, Christine Doran, and Martha Palmer. 2000. Lexicalized grammar and the description of motion events. In TAG+: Workshop on Tree-Adjoining Grammar and Related Formalisms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Strube</author>
</authors>
<title>Never look back: An alternative to centering.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="10319" citStr="Strube, 1998" startWordPosition="1632" endWordPosition="1633">tion to represent the communicative context. This detail is needed for REA to achieve a theoretically-motivated realization of the common patterns of speech and gesture we observed in human conversation. For example, a variety of changing features determine whether marked forms in speech and gesture are appropriate in the context. REA&apos;s dialogue manager tracks the changing status of such features as: O Attentional prominence, represented (as usual in natural language generation) by setting up a context set for each entity (Dale, 1992). Our model of prominence is a simple local one similar to (Strube, 1998). ▪ Cognitive status, including whether an entity is hearer-old or hearer-new (Prince, 1992), and whether an entity is in-focus or not (Gundel et al., 1993). We can assume that houses and their rooms are hearer-new until REA describes them; and that just those entities mentioned in the prior sentence are in-focus. a Information structure, including the open propositions or, following (Steedman, 1991), themes, which describe the salient questions currently at issue in the discourse (Prince, 1986). In REA&apos;s dialogue, open questions are always general questions about some entity raised by a recen</context>
</contexts>
<marker>Strube, 1998</marker>
<rawString>Michael Strube. 1998. Never look back: An alternative to centering. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Thompson</author>
<author>D W Massaro</author>
</authors>
<title>Evaluation and integration of speech and pointing gestures during referential understanding.</title>
<date>1986</date>
<journal>Journal of Experimental Child Psychology,</journal>
<pages>42--144</pages>
<contexts>
<context position="4729" citStr="Thompson and Massaro, 1986" startWordPosition="725" endWordPosition="728">estures during conversation. For more details about the functionality of REA see (Cassell, 2000a). Evidence from many sources suggests that this relationship is a close one. About three-quarters of all clauses in narrative discourse are accompanied by gestures of one kind or another (McNeill, 1992), and within those clauses, the most effortful part of gestures tends to co-occur with or just before the phonologically most prominent syllable of the accompanying speech (Kendon, 1974). Of course, communication is still possible without gesture. But it has been shown that when speech is ambiguous (Thompson and Massaro, 1986) or in a speech situation with some noise (Rogers, 1978), listeners do rely on gestural cues (and, the higher the noise-to-signal ratio, the more facilitation by gesture). Similarly, Cassell et al. (1999) established that listeners rely on information conveyed only in gesture as they try to comprehend a story. Most interesting in terms of building interactive dialogue systems is the semantic and pragmatic relationship between gesture and speech. The two channels do not always manifest the same information, but what they convey is virtually always compatible. Semantically, speech and gesture gi</context>
</contexts>
<marker>Thompson, Massaro, 1986</marker>
<rawString>L. A. Thompson and D. W. Massaro. 1986. Evaluation and integration of speech and pointing gestures during referential understanding. Journal of Experimental Child Psychology, 42:144-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>James F Allen</author>
</authors>
<title>Discourse obligations in dialogue processing.</title>
<date>1994</date>
<booktitle>In ACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="14396" citStr="Traum and Allen, 1994" startWordPosition="2259" endWordPosition="2262">ive strategies are formulated in terms of functions which are common in naturally-occurring dialogues (such as &amp;quot;presentation&amp;quot;) and which lead to distinctive bundles of content in gesture and speech. The knowledge base&apos;s kinds of information provide a mechanism for specifying and reasoning about such functions. The knowledge base is structured to describe the relationship between the system&apos;s private information and the questions of interest that that information can be used to settle. Once the user&apos;s words have been interpreted, a layer of production rules constructs obligations for response (Traum and Allen, 1994); then, a second layer plans to meet these obligations by deciding to present a specified kind of information about a specified object. This determines some concrete communicative goals—facts of this kind that a contribution to dialogue could make. Both speech and gesture can access the whole structured database in realizing these concrete communicative goals. For example, a variety of facts that bear on where a residence is—which city. which neighborhood or, if appropriate, where in a building—all provide the same kind of information, and would therefore fit the obligation to specify the loca</context>
</contexts>
<marker>Traum, Allen, 1994</marker>
<rawString>David R. Traum and James F. Allen. 1994. Discourse obligations in dialogue processing. In ACL, pages 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wahlster</author>
<author>E Andre</author>
<author>W Graf</author>
<author>T Rist</author>
</authors>
<title>Designing illustrated texts.</title>
<date>1991</date>
<booktitle>In Proceedings EACL,</booktitle>
<pages>8--14</pages>
<contexts>
<context position="6000" citStr="Wahlster et al., 1991" startWordPosition="928" endWordPosition="931"> example, gesture may depict the way in which an action was carried out when this aspect of meaning is not depicted in speech. Pragmatically, speech and gesture mark information about this meaning as advancing the purposes of the conversation in a consistent way. Indeed, gesture often emphasizes information that is also focused pragmatically by mechanisms like prosody in speech (Cassell, 2000b). The semantic and pragmatic compatibility seen in the gesturespeech relationship recalls the interaction of words and graphics in multimodal presentations (Feiner and McKeown, 1991; Green et al., 1998; Wahlster et al., 1991). In fact, some suggest (McNeill, 1992), that gesture and speech arise together from an underlying representation that has both visual and linguistic aspects, and so the relationship between gesture and speech is essential to the production of meaning and to its comprehension. This theoretical perspective on speech and gesture involves two key claims with computational import: that gesture and speech .reflect-a- common conceptual source; and that the content and form of a gesture is tuned to the communicative context and the actor&apos;s communicative intentions. We believe that these characteristi</context>
<context position="28663" citStr="Wahlster et al., 1991" startWordPosition="4538" endWordPosition="4541">a deictic gesture at the beginning of explanations about objects in the virtual world. Andre et al. (1999) generate pointing gestures as a sub-action of the rhetorical action of labeling, in turn a subaction of elaborating. Missing from these prior systems, however, is a representation of communicative action that treats the different modalities on a par. Such representations have been explored in research on combining linguistic and graphical interaction. For example, multimodal managers have been described to allocate an underlying content representation for generation of text and graphics (Wahlster et al., 1991., Green et al., 1998). Meanwhile, (Johnston et al., 1997; Johnston, 1998) describe a formalism for Lightly-coupled interpretation which uses a grammar and semantic constraints to analyze input from speech and pen. While many insights from these formalisms are relevant in embodied conversation, spontaneous gesture requires a distinct analysis with different emphasis: For example;--we need some notion of discourse pragmatics that would allow us to predict where gesture occurs with respect to speech, and what its role might be. Likewise, we need a model of the communicative effects of spontaneou</context>
</contexts>
<marker>Wahlster, Andre, Graf, Rist, 1991</marker>
<rawString>W. Wahlster, E. Andre, W. Graf, and T. Rist. 1991. Designing illustrated texts. In Proceedings EACL, pages 8-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Yan</author>
</authors>
<title>Paired speech,and gesture generation in embodied conversational agents. Master&apos;s thesis,</title>
<date>2000</date>
<location>Media Lab, MIT.</location>
<contexts>
<context position="11590" citStr="Yan, 2000" startWordPosition="1824" endWordPosition="1825">be formalized as theme(A.P.Pe), REA can use the simpler therne(e). In fact, both speech and gesture depend on the same kinds of featurestand.acces-s-them in the&apos; same way;- this specification of the dialogue state crosscuts distinctions of communicative modality. Figure l: Interacting with REA transcriptions (recall) and apply with an accuracy of 96% (precision). These patterns provide a concrete specification for the main communicative strategies and communicative resources required for REA. A full discussion of the experimental methods and analysis, and the resulting rules, can be found in (Yan, 2000). 3 Framing the generation problem In REA, requests for the generation of speech and gesture are formulated within the dialogue management module. REA&apos;s utterances reflect a coordination of multiple kinds of processing in the dialogue manager—the system recognizes that it has the floor, derives the appropriate communicative context for a response and an appropriate set of communicative goals, triggers the generation process, and realizes the resulting speech and gesture. The dialogue manager is only one component in a multithreaded architecture that carries out hardwired reactions to input as </context>
</contexts>
<marker>Yan, 2000</marker>
<rawString>Hao Yan., 2000. Paired speech,and gesture generation in embodied conversational agents. Master&apos;s thesis, Media Lab, MIT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>