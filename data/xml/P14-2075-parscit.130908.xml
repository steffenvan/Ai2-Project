<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000588">
<title confidence="0.9717">
Learning a Lexical Simplifier Using Wikipedia
</title>
<author confidence="0.994652">
Colby Horn, Cathryn Manduca and David Kauchak
</author>
<affiliation confidence="0.9735385">
Computer Science Department
Middlebury College
</affiliation>
<email confidence="0.997598">
{chorn,cmanduca,dkauchak}@middlebury.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99902">
In this paper we introduce a new lexical
simplification approach. We extract over
30K candidate lexical simplifications by
identifying aligned words in a sentence-
aligned corpus of English Wikipedia with
Simple English Wikipedia. To apply these
rules, we learn a feature-based ranker us-
ing SVMrank trained on a set of labeled
simplifications collected using Amazon’s
Mechanical Turk. Using human simplifi-
cations for evaluation, we achieve a preci-
sion of 76% with changes in 86% of the
examples.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977192982456">
Text simplification is aimed at reducing the read-
ing and grammatical complexity of text while re-
taining the meaning (Chandrasekar and Srinivas,
1997). Text simplification techniques have a broad
range of applications centered around increasing
data availability to both targeted audiences, such
as children, language learners, and people with
cognitive disabilities, as well as to general readers
in technical domains such as health and medicine
(Feng, 2008).
Simplifying a text can require a wide range
of transformation operations including lexical
changes, syntactic changes, sentence splitting,
deletion and elaboration (Coster and Kauchak,
2011; Zhu et al., 2010). In this paper, we ex-
amine a restricted version of the text simplifica-
tion problem, lexical simplification, where text is
simplified by substituting words or phrases with
simpler variants. Even with this restriction, lexi-
cal simplification techniques have been shown to
positively impact the simplicity of text and to im-
prove reader understanding and information reten-
tion (Leroy et al., 2013). Additionally, restrict-
ing the set of transformation operations allows for
more straightforward evaluation than the general
simplification problem (Specia et al., 2012).
Most lexical simplification techniques rely on
transformation rules that change a word or phrase
into a simpler variant with similar meaning (Bi-
ran et al., 2011; Specia et al., 2012; Yatskar et
al., 2010). Two main challenges exist for this type
of approach. First, the lexical focus of the trans-
formation rules makes generalization difficult; a
large number of transformation rules is required to
achieve reasonable coverage and impact. Second,
rules do not apply in all contexts and care must be
taken when performing lexical transformations to
ensure local cohesion, grammaticality and, most
importantly, the preservation of the original mean-
ing.
In this paper, we address both of these issues.
We leverage a data set of 137K aligned sentence
pairs between English Wikipedia and Simple En-
glish Wikipedia to learn simplification rules. Pre-
vious approaches have used unaligned versions of
Simple English Wikipedia to learn rules (Biran et
al., 2011; Yatskar et al., 2010), however, by using
the aligned version we are able to learn a much
larger rule set.
To apply lexical simplification rules to a new
sentence, a decision must be made about which, if
any, transformations should be applied. Previous
approaches have used similarity measures (Biran
et al., 2011) and feature-based approaches (Specia
et al., 2012) to make this decision. We take the lat-
ter approach and train a supervised model to rank
candidate transformations.
</bodyText>
<sectionHeader confidence="0.978572" genericHeader="method">
2 Problem Setup
</sectionHeader>
<bodyText confidence="0.999207333333333">
We learn lexical simplification rules that consist
of a word to be simplified and a list of candidate
simplifications:
</bodyText>
<equation confidence="0.918711">
W → C1, C2, ..., Cm
</equation>
<bodyText confidence="0.915358">
Consider the two aligned sentence pairs in Table
</bodyText>
<page confidence="0.977046">
458
</page>
<bodyText confidence="0.919513463414634">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 458–463,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
The first school was established in 1857.
The first school was started in 1857.
The district was established in 1993 by merging
the former districts of Bernau and Eberswalde.
The district was made in 1993 by joining the
old districts of Bernau and Eberswalde.
Table 1: Two aligned sentence pairs. The bottom
sentence is a human simplified version of the top
sentence. Bold words are candidate lexical simpli-
fications.
1. The bottom sentence of each pair is a simpli-
fied variant of the top sentence. By identifying
aligned words within the aligned sentences, can-
didate lexical simplifications can be learned. The
bold words show two such examples, though other
candidates exist in the bottom pair. By examining
aligned sentence pairs we can learn a simplifica-
tion rule. For example, we might learn:
established → began, made, settled, started
Given a sentence s1, s2, ..., sn, a simplification
rule applies if the left hand side of the rule can be
found in the sentence (si = w, for some i). If a
rule applies, then a decision must be made about
which, if any, of the candidate simplifications
should be substituted for the word w to simplify
the sentence. For example, if we were attempting
to simplify the sentence
The ACL was established in 1962.
using the simplification rule above, some of the
simplification options would not apply because
of grammatical constraints, e.g. began, while
others would not apply for semantic reasons, e.g.
settled. This does not mean that these are not
good simplifications for established since in other
contexts, they might be appropriate. For example,
in the sentence
The researcher established a new paper
writing routine.
began is a reasonable option.
</bodyText>
<sectionHeader confidence="0.818459" genericHeader="method">
3 Learning a Lexical Simplifier
</sectionHeader>
<bodyText confidence="0.9999906">
We break the learning problem into two steps: 1)
learn a set of simplification rules and 2) learn a
ranking function for determining the best simpli-
fication candidate when a rule applies. Each of
these steps are outlined below.
</bodyText>
<subsectionHeader confidence="0.999437">
3.1 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.999869578947368">
To extract the set of simplification rules, we use
a sentence-aligned data set of English Wikipedia
sentences (referred to as normal) aligned to Sim-
ple English Wikipedia sentences (referred to as
simple) (Coster and Kauchak, 2011). The data set
contains 137K such aligned sentence pairs.
Given a normal sentence and the corresponding
aligned simple sentence, candidate simplifications
are extracted by identifying a word in the simple
sentence that corresponds to a different word in the
normal sentence. To identify such pairs, we au-
tomatically induce a word alignment between the
normal and simple sentence pairs using GIZA++
(Och and Ney, 2000). Words that are aligned are
considered as possible candidates for extraction.
Due to errors in the sentence and word alignment
processes, not all words that are aligned are actu-
ally equivalent lexical variants. We apply the fol-
lowing filters to reduce such spurious alignments:
</bodyText>
<listItem confidence="0.898949769230769">
• We remove any pairs where the normal word
occurs in a stoplist. Stoplist words tend to be
simple already and stoplist words that are be-
ing changed are likely either bad alignments
or are not simplifications.
• We require that the part of speeches (POS)
of the two words be the same. The parts of
speech were calculated based on a full parse
of the sentences using the Berkeley parser
(Petrov and Klein, 2007).
• We remove any candidates where the POS
is labeled as a proper noun. In most cases,
proper nouns should not be simplified.
</listItem>
<bodyText confidence="0.970560133333333">
All other aligned word pairs are extracted. To
generate the simplification rules, we collect all
candidate simplifications (simple words) that are
aligned to the same normal word.
As mentioned before, one of the biggest chal-
lenges for lexical simplification systems is gen-
eralizability. To improve the generalizability of
the extracted rules, we add morphological variants
of the words in the rules. For nouns, we include
both singular and plural variants. For verbs, we
expand to all inflection variants. The morpholog-
ical changes are generated using MorphAdorner
(Burns, 2013) and are applied symmetrically: any
change to the normal word is also applied to the
corresponding simplification candidates.
</bodyText>
<page confidence="0.995337">
459
</page>
<subsectionHeader confidence="0.982032">
3.2 Lexical Simplification as a Ranking
Problem
</subsectionHeader>
<bodyText confidence="0.999981689655172">
A lexical simplification example consists of three
parts: 1) a sentence, s1, s2,..., sn, 2), a word in
that sentence, si, and 3) a list of candidate sim-
plifications for si, c1, c2,..., cm. A labeled exam-
ple is an example where the rank of the candidate
simplifications has been specified. Given a set of
labeled examples, the goal is to learn a ranking
function that, given an unlabeled example (exam-
ple without the candidate simplifications ranked),
specifies a ranking of the candidates.
To learn this function, features are extracted
from a set of labeled lexical simplification exam-
ples. These labeled examples are then used to train
a ranking function. We use SVMrank (Joachims,
2006), which uses a linear support vector machine.
Besides deciding which of the candidates is
most applicable in the context of the sentence,
even if a rule applies, we must also decide if
any simplification should occur. For example,
there may be an instance where none of the can-
didate simplifications are appropriate in this con-
text. Rather than viewing this as a separate prob-
lem, we incorporate this decision into the ranking
problem by adding w as a candidate simplifica-
tion. For each rule, w → c1, c2, ..., cm we add one
additional candidate simplification which does not
change the sentence, w → c1, c2, ..., cm, w. If w is
ranked as the most likely candidate by the ranking
algorithm, then the word is not simplified.
</bodyText>
<sectionHeader confidence="0.730317" genericHeader="method">
3.2.1 Features
</sectionHeader>
<bodyText confidence="0.99999076923077">
The role of the features is to capture information
about the applicability of the word in the context
of the sentence as well as the simplicity of the
word. Many features have been suggested previ-
ously for use in determining the simplicity of a
word (Specia et al., 2012) and for determining if
a word is contextually relevant (Biran et al., 2011;
McCarthy and Navigli, 2007). Our goal for this
paper is not feature exploration, but to examine
the usefulness of a general framework for feature-
based ranking for lexical simplification. The fea-
tures below represent a first pass at candidate fea-
tures, but many others could be explored.
</bodyText>
<subsectionHeader confidence="0.844221">
Candidate Probability
</subsectionHeader>
<bodyText confidence="0.976464615384615">
p(ci|w): in the sentence-aligned Wikipedia data,
when w is aligned to some candidate simplifica-
tion, what proportion of the time is that candidate
ci.
Frequency
The frequency of a word has been shown to cor-
relate with the word’s simplicity and with peo-
ple’s knowledge of that word (Leroy and Kauchak,
2013). We measured a candidate simplification’s
frequency in two corpora: 1) Simple English
Wikipedia and 2) the web, as measured by the un-
igram frequency from the Google n-gram corpus
(Brants and Franz, 2006).
</bodyText>
<subsectionHeader confidence="0.9488">
Language Models
</subsectionHeader>
<bodyText confidence="0.991939538461538">
n-gram language models capture how likely a par-
ticular sequence is and can help identify candidate
simplifications that are not appropriate in the con-
text of the sentence. We included features from
four different language models trained on four dif-
ferent corpora: 1) Simple English Wikipedia, 2)
English Wikipedia, 3) Google n-gram corpus and
4) a linearly interpolated model between 1) and
2) with A = 0.5, i.e. an even blending. We
used the SRI language modeling toolkit (Stolcke,
2002) with Kneser-Kney smoothing. All models
were trigram language models except the Google
n-gram model, which was a 5-gram model.
</bodyText>
<subsectionHeader confidence="0.722791">
Context Frequency
</subsectionHeader>
<bodyText confidence="0.9995224">
As another measure of the applicability of a can-
didate in the context of the sentence, we also cal-
culate the frequency in the Google n-grams of the
candidate simplification in the context of the sen-
tence with context windows of one and two words.
If the word to be substituted is at position i in the
sentence (w = si), then the one word window
frequency for simplification cj is the trigram fre-
quency of si−1 cj si+1 and the two word window
the 5-gram frequency of si−2 si−1 cj si+1 si+2.
</bodyText>
<sectionHeader confidence="0.998551" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999889538461539">
For training and evaluation of the models, we col-
lected human labelings of 500 lexical simplifica-
tion examples using Amazon’s Mechanical Turk
(MTurk)1. MTurk has been used extensively for
annotating and evaluating NLP tasks and has been
shown to provide data that is as reliable as other
forms of human annotation (Callison-Burch and
Dredze, 2010; Zaidan and Callison-Burch, 2011).
Figure 1 shows an example of the task we asked
annotators to do. Given a sentence and a word
to be simplified, the task is to suggest a simpler
variant of that word that is appropriate in the con-
text of the sentence. Candidate sentences were se-
</bodyText>
<footnote confidence="0.988656">
1https://www.mturk.com/
</footnote>
<page confidence="0.999054">
460
</page>
<bodyText confidence="0.778364">
Enter a simpler word that could be substituted for the red, bold word in the sentence. A simpler
word is one that would be understood by more people or people with a lower reading level (e.g.
children).
Food is procured with its suckers and then crushed using its tough “beak” of chitin.
</bodyText>
<figureCaption confidence="0.998897">
Figure 1: Example task setup on MTurk soliciting lexical simplifications from annotators.
</figureCaption>
<bodyText confidence="0.999768535714286">
lected from the sentence-aligned Wikipedia cor-
pus where a word in the normal sentence is be-
ing simplified to a different word in the simple
sentence, as identified by the automatically in-
duced word alignment. The normal sentence and
the aligned word were then selected for annota-
tion. These examples represent words that other
people (those that wrote/edited the Simple En-
glish Wikipedia page) decided were difficult and
required simplification.
We randomly selected 500 such sentences and
collected candidate simplifications from 50 people
per sentence, for a total of 25,000 annotations. To
participate in the annotation process, we required
that the MTurk workers live in the U.S. (for En-
glish proficiency) and had at least a 95% accep-
tance rate on previous tasks.
The simplifications suggested by the annotators
were then tallied and the resulting list of simpli-
fications with frequencies provides a ranking for
training the candidate ranker. Table 2 shows the
ranked list of annotations collected for the exam-
ple in Figure 1. This data set is available online.2
Since these examples were selected from En-
glish Wikipedia they, and the corresponding
aligned Simple English Wikipedia sentences, were
removed from all resources used during both the
rule extraction and the training of the ranker.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997457">
5.1 Other Approaches
</subsectionHeader>
<bodyText confidence="0.9994109">
We compared our lexical simplification approach
(rank-simplify) to two other approaches. To un-
derstand the benefit of the feature-based ranking
algorithm, we compared against a simplifier that
uses the same rule set, but ranks the candidates
only based on their frequency in Simple English
Wikipedia (frequency). This is similar to base-
lines used in previous work (Biran et al., 2011).
To understand how our extracted rules com-
pared to the rules extracted by Biran et al., we
</bodyText>
<footnote confidence="0.849245">
2http://www.cs.middlebury.edu/˜dkauchak/simplification/
</footnote>
<bodyText confidence="0.999062166666667">
used their rules with our ranking approach (rank-
Biran). Their approach also extracts rules from
a corpus of English Wikipedia and Simple En-
glish Wikipedia, however, they do not utilize a
sentence-aligned version and instead rely on con-
text similarity measures to extract their rules.
</bodyText>
<subsectionHeader confidence="0.992661">
5.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99985876">
We used the 500 ranked simplification examples to
train and evaluate our approach. We employed 10-
fold cross validation for all experiments, training
on 450 examples and testing on 50.
We evaluated the models with four different
metrics:
precision: Of the words that the system changed,
what percentage were found in any of the human
annotations.
precision@k: Of the words that the system
changed, what percentage were found in the top
k human annotations, where the annotations were
ranked by response frequency. For example, if we
were calculating the precision@1 for the example
in Table 2, only “obtained” would be considered
correct.
accuracy: The percentage of the test examples
where the system made a change to one of the
annotations suggested by the human annotators.
Note that unlike precision, if the system does not
suggest a change to a word that was simplified it
still gets penalized.
changed: The percentage of the test examples
where the system suggested some change (even if
it wasn’t a “correct” change).
</bodyText>
<subsectionHeader confidence="0.803373">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999966166666667">
Table 3 shows the precision, accuracy and percent
changed for the three systems. Based on all three
metrics, our system achieves the best results. Al-
though the rules generated by Biran et al. have rea-
sonable precision, they suffer from a lack of cov-
erage, only making changes on about 5% of the
</bodyText>
<page confidence="0.998149">
461
</page>
<table confidence="0.996278666666667">
word frequency word frequency word frequency
obtained 17 made 2 secured 1
gathered 9 created 1 found 1
gotten 8 processed 1 attained 1
grabbed 4 received 1 procured 1
acquired 2 collected 1 aquired 1
</table>
<tableCaption confidence="0.916613">
Table 2: Candidate simplifications generated using MTurk for the examples in Figure 1. The frequency
is the number of annotators that suggested that simplification.
</tableCaption>
<table confidence="0.9998415">
precision accuracy changed
frequency 53.9% 46.1% 84.9%
rank-Biran 71.4% 3.4% 5.2%
rank-simplify 76.1% 66.3% 86.3%
</table>
<tableCaption confidence="0.998724">
Table 3: Precision, accuracy and percent changed
</tableCaption>
<bodyText confidence="0.986673115384615">
for the three systems, averaged over the 10 folds.
examples. For our approach, the extracted rules
had very good coverage, applying in over 85% of
the examples.
This difference in coverage can be partially at-
tributed to the number of rules learned. We learned
simplifications for 14,478 words with an average
of 2.25 candidate simplifications per word. In con-
trast, the rules from Biran et al. only had simpli-
fications for 3,598 words with an average of 1.18
simplifications per word.
The precision of both of the approaches that
utilized the SVM candidate ranking were sig-
nificantly better than the frequency-based ap-
proach. To better understand the types of sug-
gestions made by the systems, Figure 2 shows the
precision@k for increasing k. On average, over
the 500 examples we collected, people suggested
12 different simplifications, though this varied de-
pending on the word in question and the sentence.
As such, at around k=12, the precision@k of most
of the systems has almost reached the final preci-
sion. However, even at k = 5, which only counts
correct an answer in the top 5 human suggested
results, our system still achieved a precision of
around 67%.
</bodyText>
<sectionHeader confidence="0.999877" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.998196166666667">
In this paper we have introduced a new rule ex-
traction algorithm and a new feature-based rank-
ing approach for applying these rules in the con-
text of different sentences. The number of rules
learned is an order of magnitude larger than any
previous lexical simplification approach and the
</bodyText>
<figure confidence="0.941063">
0 5 10 15 20 25 30 35 40 45 50
k
</figure>
<figureCaption confidence="0.994933">
Figure 2: Precision@k for varying k for the three
different approaches averaged over the 10 folds.
</figureCaption>
<bodyText confidence="0.999751409090909">
quality of the resulting simplifications after apply-
ing these rules is better than previous approaches.
Many avenues exist for improvement and for
better understanding how well the current ap-
proach works. First, we have only explored a
small set of possible features in the ranking algo-
rithm. Additional improvements could be seen by
incorporating a broader feature set. Second, more
analysis needs to be done to understand the quality
of the produced simplifications and their impact on
the simplicity of the resulting sentences. Third, the
experiments above assume that the word to be sim-
plified has already been identified in the sentence.
This identification step also needs to be explored
to implement a sentence-level simplifier using our
approach. Fourth, the ranking algorithm can be
applied to most simplification rules (e.g. we ap-
plied the ranking approach to the rules obtained
by Biran et al. (2011)). We hope to explore other
approaches for increasing the rule set by incorpo-
rating other rule sources and other rule extraction
techniques.
</bodyText>
<figure confidence="0.9980272">
precision
0.75
0.65
0.55
0.45
0.35
0.8
0.7
0.6
0.5
0.4
0.3
rank-simplify
rank-Biran
frequency
</figure>
<page confidence="0.996086">
462
</page>
<bodyText confidence="0.83857575">
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simplifi-
cations from Wikipedia. In NAACL/HLT.
</bodyText>
<sectionHeader confidence="0.751663" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985361107142857">
Or Biran, Samuel Brody, and Noe ´mie Elhadad. 2011.
Putting it simply: A context-aware approach to lexi-
cal simplification. In Proceedings of ACL.
Thorsten Brants and Alex Franz. 2006. Web 1T
5-gram version 1. Linguistic Data Consortium,
Philadelphia.
Philip R. Burns. 2013. Morphadorner v2: A Java li-
brary for the morphological adornment of english
language texts.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon’s Me-
chanical Turk. In Proceedings of NAACL-HLT 2010
Workshop on Creating Speech and Language Data
with Amazon’s Mechanical Turk.
Raman Chandrasekar and Bangalore Srinivas. 1997.
Automatic induction of rules for text simplification.
In Knowledge Based Systems.
William Coster and David Kauchak. 2011. Simple En-
glish Wikipedia: A new text simplification task. In
Proceedings of ACL.
Lijun Feng. 2008. Text simplification: A survey.
CUNY Technical Report.
Thorsten Joachims. 2006. Training linear svms in lin-
ear time. In Proceedings of KDD.
Gondy Leroy and David Kauchak. 2013. The effect
of word familiarity on actual and perceived text dif-
ficulty. Journal of American Medical Informatics
Association.
Gondy Leroy, James E. Endicott, David Kauchak,
Obay Mouradi, and Melissa Just. 2013. User evalu-
ation of the effects of a text simplification algorithm
using term familiarity on perception, understanding,
learning, and information retention. Journal of Med-
ical Internet Research (JMIR).
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task. In
Proceedings of SEMEVAL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HTL-
NAACL.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In Joint Conference on Lexical and
Computerational Semantics (*SEM).
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Statistical Language Pro-
cessing.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of ACL.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of ICCL.
</reference>
<page confidence="0.999714">
463
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.555930">
<title confidence="0.999667">Learning a Lexical Simplifier Using Wikipedia</title>
<author confidence="0.985412">Cathryn Manduca Horn</author>
<affiliation confidence="0.789934">Computer Science Middlebury</affiliation>
<abstract confidence="0.997822571428571">In this paper we introduce a new lexical simplification approach. We extract over 30K candidate lexical simplifications by identifying aligned words in a sentencealigned corpus of English Wikipedia with Simple English Wikipedia. To apply these rules, we learn a feature-based ranker ustrained on a set of labeled simplifications collected using Amazon’s Mechanical Turk. Using human simplifications for evaluation, we achieve a precision of 76% with changes in 86% of the examples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Samuel Brody</author>
<author>Noe ´mie Elhadad</author>
</authors>
<title>Putting it simply: A context-aware approach to lexical simplification.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2091" citStr="Biran et al., 2011" startWordPosition="299" endWordPosition="303">, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexical simplification techniques have been shown to positively impact the simplicity of text and to improve reader understanding and information retention (Leroy et al., 2013). Additionally, restricting the set of transformation operations allows for more straightforward evaluation than the general simplification problem (Specia et al., 2012). Most lexical simplification techniques rely on transformation rules that change a word or phrase into a simpler variant with similar meaning (Biran et al., 2011; Specia et al., 2012; Yatskar et al., 2010). Two main challenges exist for this type of approach. First, the lexical focus of the transformation rules makes generalization difficult; a large number of transformation rules is required to achieve reasonable coverage and impact. Second, rules do not apply in all contexts and care must be taken when performing lexical transformations to ensure local cohesion, grammaticality and, most importantly, the preservation of the original meaning. In this paper, we address both of these issues. We leverage a data set of 137K aligned sentence pairs between </context>
<context position="9744" citStr="Biran et al., 2011" startWordPosition="1558" endWordPosition="1561">lification. For each rule, w → c1, c2, ..., cm we add one additional candidate simplification which does not change the sentence, w → c1, c2, ..., cm, w. If w is ranked as the most likely candidate by the ranking algorithm, then the word is not simplified. 3.2.1 Features The role of the features is to capture information about the applicability of the word in the context of the sentence as well as the simplicity of the word. Many features have been suggested previously for use in determining the simplicity of a word (Specia et al., 2012) and for determining if a word is contextually relevant (Biran et al., 2011; McCarthy and Navigli, 2007). Our goal for this paper is not feature exploration, but to examine the usefulness of a general framework for featurebased ranking for lexical simplification. The features below represent a first pass at candidate features, but many others could be explored. Candidate Probability p(ci|w): in the sentence-aligned Wikipedia data, when w is aligned to some candidate simplification, what proportion of the time is that candidate ci. Frequency The frequency of a word has been shown to correlate with the word’s simplicity and with people’s knowledge of that word (Leroy a</context>
<context position="14462" citStr="Biran et al., 2011" startWordPosition="2333" endWordPosition="2336">ted from English Wikipedia they, and the corresponding aligned Simple English Wikipedia sentences, were removed from all resources used during both the rule extraction and the training of the ranker. 5 Experiments 5.1 Other Approaches We compared our lexical simplification approach (rank-simplify) to two other approaches. To understand the benefit of the feature-based ranking algorithm, we compared against a simplifier that uses the same rule set, but ranks the candidates only based on their frequency in Simple English Wikipedia (frequency). This is similar to baselines used in previous work (Biran et al., 2011). To understand how our extracted rules compared to the rules extracted by Biran et al., we 2http://www.cs.middlebury.edu/˜dkauchak/simplification/ used their rules with our ranking approach (rankBiran). Their approach also extracts rules from a corpus of English Wikipedia and Simple English Wikipedia, however, they do not utilize a sentence-aligned version and instead rely on context similarity measures to extract their rules. 5.2 Evaluation We used the 500 ranked simplification examples to train and evaluate our approach. We employed 10- fold cross validation for all experiments, training on</context>
</contexts>
<marker>Biran, Brody, Elhadad, 2011</marker>
<rawString>Or Biran, Samuel Brody, and Noe ´mie Elhadad. 2011. Putting it simply: A context-aware approach to lexical simplification. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram version 1. Linguistic Data Consortium,</title>
<date>2006</date>
<location>Philadelphia.</location>
<contexts>
<context position="10565" citStr="Brants and Franz, 2006" startWordPosition="1692" endWordPosition="1695">res below represent a first pass at candidate features, but many others could be explored. Candidate Probability p(ci|w): in the sentence-aligned Wikipedia data, when w is aligned to some candidate simplification, what proportion of the time is that candidate ci. Frequency The frequency of a word has been shown to correlate with the word’s simplicity and with people’s knowledge of that word (Leroy and Kauchak, 2013). We measured a candidate simplification’s frequency in two corpora: 1) Simple English Wikipedia and 2) the web, as measured by the unigram frequency from the Google n-gram corpus (Brants and Franz, 2006). Language Models n-gram language models capture how likely a particular sequence is and can help identify candidate simplifications that are not appropriate in the context of the sentence. We included features from four different language models trained on four different corpora: 1) Simple English Wikipedia, 2) English Wikipedia, 3) Google n-gram corpus and 4) a linearly interpolated model between 1) and 2) with A = 0.5, i.e. an even blending. We used the SRI language modeling toolkit (Stolcke, 2002) with Kneser-Kney smoothing. All models were trigram language models except the Google n-gram </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram version 1. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Burns</author>
</authors>
<title>Morphadorner v2: A Java library for the morphological adornment of english language texts.</title>
<date>2013</date>
<contexts>
<context position="7793" citStr="Burns, 2013" startWordPosition="1227" endWordPosition="1228">r nouns should not be simplified. All other aligned word pairs are extracted. To generate the simplification rules, we collect all candidate simplifications (simple words) that are aligned to the same normal word. As mentioned before, one of the biggest challenges for lexical simplification systems is generalizability. To improve the generalizability of the extracted rules, we add morphological variants of the words in the rules. For nouns, we include both singular and plural variants. For verbs, we expand to all inflection variants. The morphological changes are generated using MorphAdorner (Burns, 2013) and are applied symmetrically: any change to the normal word is also applied to the corresponding simplification candidates. 459 3.2 Lexical Simplification as a Ranking Problem A lexical simplification example consists of three parts: 1) a sentence, s1, s2,..., sn, 2), a word in that sentence, si, and 3) a list of candidate simplifications for si, c1, c2,..., cm. A labeled example is an example where the rank of the candidate simplifications has been specified. Given a set of labeled examples, the goal is to learn a ranking function that, given an unlabeled example (example without the candid</context>
</contexts>
<marker>Burns, 2013</marker>
<rawString>Philip R. Burns. 2013. Morphadorner v2: A Java library for the morphological adornment of english language texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</booktitle>
<contexts>
<context position="12059" citStr="Callison-Burch and Dredze, 2010" startWordPosition="1945" endWordPosition="1948">text windows of one and two words. If the word to be substituted is at position i in the sentence (w = si), then the one word window frequency for simplification cj is the trigram frequency of si−1 cj si+1 and the two word window the 5-gram frequency of si−2 si−1 cj si+1 si+2. 4 Data For training and evaluation of the models, we collected human labelings of 500 lexical simplification examples using Amazon’s Mechanical Turk (MTurk)1. MTurk has been used extensively for annotating and evaluating NLP tasks and has been shown to provide data that is as reliable as other forms of human annotation (Callison-Burch and Dredze, 2010; Zaidan and Callison-Burch, 2011). Figure 1 shows an example of the task we asked annotators to do. Given a sentence and a word to be simplified, the task is to suggest a simpler variant of that word that is appropriate in the context of the sentence. Candidate sentences were se1https://www.mturk.com/ 460 Enter a simpler word that could be substituted for the red, bold word in the sentence. A simpler word is one that would be understood by more people or people with a lower reading level (e.g. children). Food is procured with its suckers and then crushed using its tough “beak” of chitin. Figu</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raman Chandrasekar</author>
<author>Bangalore Srinivas</author>
</authors>
<title>Automatic induction of rules for text simplification. In Knowledge Based Systems.</title>
<date>1997</date>
<contexts>
<context position="847" citStr="Chandrasekar and Srinivas, 1997" startWordPosition="118" endWordPosition="121"> a new lexical simplification approach. We extract over 30K candidate lexical simplifications by identifying aligned words in a sentencealigned corpus of English Wikipedia with Simple English Wikipedia. To apply these rules, we learn a feature-based ranker using SVMrank trained on a set of labeled simplifications collected using Amazon’s Mechanical Turk. Using human simplifications for evaluation, we achieve a precision of 76% with changes in 86% of the examples. 1 Introduction Text simplification is aimed at reducing the reading and grammatical complexity of text while retaining the meaning (Chandrasekar and Srinivas, 1997). Text simplification techniques have a broad range of applications centered around increasing data availability to both targeted audiences, such as children, language learners, and people with cognitive disabilities, as well as to general readers in technical domains such as health and medicine (Feng, 2008). Simplifying a text can require a wide range of transformation operations including lexical changes, syntactic changes, sentence splitting, deletion and elaboration (Coster and Kauchak, 2011; Zhu et al., 2010). In this paper, we examine a restricted version of the text simplification probl</context>
</contexts>
<marker>Chandrasekar, Srinivas, 1997</marker>
<rawString>Raman Chandrasekar and Bangalore Srinivas. 1997. Automatic induction of rules for text simplification. In Knowledge Based Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Simple English Wikipedia: A new text simplification task.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1347" citStr="Coster and Kauchak, 2011" startWordPosition="187" endWordPosition="190">med at reducing the reading and grammatical complexity of text while retaining the meaning (Chandrasekar and Srinivas, 1997). Text simplification techniques have a broad range of applications centered around increasing data availability to both targeted audiences, such as children, language learners, and people with cognitive disabilities, as well as to general readers in technical domains such as health and medicine (Feng, 2008). Simplifying a text can require a wide range of transformation operations including lexical changes, syntactic changes, sentence splitting, deletion and elaboration (Coster and Kauchak, 2011; Zhu et al., 2010). In this paper, we examine a restricted version of the text simplification problem, lexical simplification, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexical simplification techniques have been shown to positively impact the simplicity of text and to improve reader understanding and information retention (Leroy et al., 2013). Additionally, restricting the set of transformation operations allows for more straightforward evaluation than the general simplification problem (Specia et al., 2012). Most lexical sim</context>
<context position="5983" citStr="Coster and Kauchak, 2011" startWordPosition="930" endWordPosition="933">For example, in the sentence The researcher established a new paper writing routine. began is a reasonable option. 3 Learning a Lexical Simplifier We break the learning problem into two steps: 1) learn a set of simplification rules and 2) learn a ranking function for determining the best simplification candidate when a rule applies. Each of these steps are outlined below. 3.1 Rule Extraction To extract the set of simplification rules, we use a sentence-aligned data set of English Wikipedia sentences (referred to as normal) aligned to Simple English Wikipedia sentences (referred to as simple) (Coster and Kauchak, 2011). The data set contains 137K such aligned sentence pairs. Given a normal sentence and the corresponding aligned simple sentence, candidate simplifications are extracted by identifying a word in the simple sentence that corresponds to a different word in the normal sentence. To identify such pairs, we automatically induce a word alignment between the normal and simple sentence pairs using GIZA++ (Och and Ney, 2000). Words that are aligned are considered as possible candidates for extraction. Due to errors in the sentence and word alignment processes, not all words that are aligned are actually </context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011. Simple English Wikipedia: A new text simplification task. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
</authors>
<title>Text simplification: A survey. CUNY</title>
<date>2008</date>
<tech>Technical Report.</tech>
<contexts>
<context position="1156" citStr="Feng, 2008" startWordPosition="164" endWordPosition="165"> Amazon’s Mechanical Turk. Using human simplifications for evaluation, we achieve a precision of 76% with changes in 86% of the examples. 1 Introduction Text simplification is aimed at reducing the reading and grammatical complexity of text while retaining the meaning (Chandrasekar and Srinivas, 1997). Text simplification techniques have a broad range of applications centered around increasing data availability to both targeted audiences, such as children, language learners, and people with cognitive disabilities, as well as to general readers in technical domains such as health and medicine (Feng, 2008). Simplifying a text can require a wide range of transformation operations including lexical changes, syntactic changes, sentence splitting, deletion and elaboration (Coster and Kauchak, 2011; Zhu et al., 2010). In this paper, we examine a restricted version of the text simplification problem, lexical simplification, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexical simplification techniques have been shown to positively impact the simplicity of text and to improve reader understanding and information retention (Leroy et al., 2</context>
</contexts>
<marker>Feng, 2008</marker>
<rawString>Lijun Feng. 2008. Text simplification: A survey. CUNY Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Training linear svms in linear time.</title>
<date>2006</date>
<booktitle>In Proceedings of KDD.</booktitle>
<contexts>
<context position="8660" citStr="Joachims, 2006" startWordPosition="1368" endWordPosition="1369">1, s2,..., sn, 2), a word in that sentence, si, and 3) a list of candidate simplifications for si, c1, c2,..., cm. A labeled example is an example where the rank of the candidate simplifications has been specified. Given a set of labeled examples, the goal is to learn a ranking function that, given an unlabeled example (example without the candidate simplifications ranked), specifies a ranking of the candidates. To learn this function, features are extracted from a set of labeled lexical simplification examples. These labeled examples are then used to train a ranking function. We use SVMrank (Joachims, 2006), which uses a linear support vector machine. Besides deciding which of the candidates is most applicable in the context of the sentence, even if a rule applies, we must also decide if any simplification should occur. For example, there may be an instance where none of the candidate simplifications are appropriate in this context. Rather than viewing this as a separate problem, we incorporate this decision into the ranking problem by adding w as a candidate simplification. For each rule, w → c1, c2, ..., cm we add one additional candidate simplification which does not change the sentence, w → </context>
</contexts>
<marker>Joachims, 2006</marker>
<rawString>Thorsten Joachims. 2006. Training linear svms in linear time. In Proceedings of KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gondy Leroy</author>
<author>David Kauchak</author>
</authors>
<title>The effect of word familiarity on actual and perceived text difficulty.</title>
<date>2013</date>
<journal>Journal of American Medical Informatics Association.</journal>
<contexts>
<context position="10361" citStr="Leroy and Kauchak, 2013" startWordPosition="1659" endWordPosition="1662">., 2011; McCarthy and Navigli, 2007). Our goal for this paper is not feature exploration, but to examine the usefulness of a general framework for featurebased ranking for lexical simplification. The features below represent a first pass at candidate features, but many others could be explored. Candidate Probability p(ci|w): in the sentence-aligned Wikipedia data, when w is aligned to some candidate simplification, what proportion of the time is that candidate ci. Frequency The frequency of a word has been shown to correlate with the word’s simplicity and with people’s knowledge of that word (Leroy and Kauchak, 2013). We measured a candidate simplification’s frequency in two corpora: 1) Simple English Wikipedia and 2) the web, as measured by the unigram frequency from the Google n-gram corpus (Brants and Franz, 2006). Language Models n-gram language models capture how likely a particular sequence is and can help identify candidate simplifications that are not appropriate in the context of the sentence. We included features from four different language models trained on four different corpora: 1) Simple English Wikipedia, 2) English Wikipedia, 3) Google n-gram corpus and 4) a linearly interpolated model be</context>
</contexts>
<marker>Leroy, Kauchak, 2013</marker>
<rawString>Gondy Leroy and David Kauchak. 2013. The effect of word familiarity on actual and perceived text difficulty. Journal of American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gondy Leroy</author>
<author>James E Endicott</author>
<author>David Kauchak</author>
<author>Obay Mouradi</author>
<author>Melissa Just</author>
</authors>
<title>User evaluation of the effects of a text simplification algorithm using term familiarity on perception, understanding, learning, and information retention.</title>
<date>2013</date>
<journal>Journal of Medical Internet Research (JMIR).</journal>
<contexts>
<context position="1760" citStr="Leroy et al., 2013" startWordPosition="252" endWordPosition="255">ne (Feng, 2008). Simplifying a text can require a wide range of transformation operations including lexical changes, syntactic changes, sentence splitting, deletion and elaboration (Coster and Kauchak, 2011; Zhu et al., 2010). In this paper, we examine a restricted version of the text simplification problem, lexical simplification, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexical simplification techniques have been shown to positively impact the simplicity of text and to improve reader understanding and information retention (Leroy et al., 2013). Additionally, restricting the set of transformation operations allows for more straightforward evaluation than the general simplification problem (Specia et al., 2012). Most lexical simplification techniques rely on transformation rules that change a word or phrase into a simpler variant with similar meaning (Biran et al., 2011; Specia et al., 2012; Yatskar et al., 2010). Two main challenges exist for this type of approach. First, the lexical focus of the transformation rules makes generalization difficult; a large number of transformation rules is required to achieve reasonable coverage and</context>
</contexts>
<marker>Leroy, Endicott, Kauchak, Mouradi, Just, 2013</marker>
<rawString>Gondy Leroy, James E. Endicott, David Kauchak, Obay Mouradi, and Melissa Just. 2013. User evaluation of the effects of a text simplification algorithm using term familiarity on perception, understanding, learning, and information retention. Journal of Medical Internet Research (JMIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of SEMEVAL.</booktitle>
<contexts>
<context position="9773" citStr="McCarthy and Navigli, 2007" startWordPosition="1562" endWordPosition="1565"> rule, w → c1, c2, ..., cm we add one additional candidate simplification which does not change the sentence, w → c1, c2, ..., cm, w. If w is ranked as the most likely candidate by the ranking algorithm, then the word is not simplified. 3.2.1 Features The role of the features is to capture information about the applicability of the word in the context of the sentence as well as the simplicity of the word. Many features have been suggested previously for use in determining the simplicity of a word (Specia et al., 2012) and for determining if a word is contextually relevant (Biran et al., 2011; McCarthy and Navigli, 2007). Our goal for this paper is not feature exploration, but to examine the usefulness of a general framework for featurebased ranking for lexical simplification. The features below represent a first pass at candidate features, but many others could be explored. Candidate Probability p(ci|w): in the sentence-aligned Wikipedia data, when w is aligned to some candidate simplification, what proportion of the time is that candidate ci. Frequency The frequency of a word has been shown to correlate with the word’s simplicity and with people’s knowledge of that word (Leroy and Kauchak, 2013). We measure</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of SEMEVAL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6400" citStr="Och and Ney, 2000" startWordPosition="995" endWordPosition="998">ication rules, we use a sentence-aligned data set of English Wikipedia sentences (referred to as normal) aligned to Simple English Wikipedia sentences (referred to as simple) (Coster and Kauchak, 2011). The data set contains 137K such aligned sentence pairs. Given a normal sentence and the corresponding aligned simple sentence, candidate simplifications are extracted by identifying a word in the simple sentence that corresponds to a different word in the normal sentence. To identify such pairs, we automatically induce a word alignment between the normal and simple sentence pairs using GIZA++ (Och and Ney, 2000). Words that are aligned are considered as possible candidates for extraction. Due to errors in the sentence and word alignment processes, not all words that are aligned are actually equivalent lexical variants. We apply the following filters to reduce such spurious alignments: • We remove any pairs where the normal word occurs in a stoplist. Stoplist words tend to be simple already and stoplist words that are being changed are likely either bad alignments or are not simplifications. • We require that the part of speeches (POS) of the two words be the same. The parts of speech were calculated </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of HTLNAACL.</booktitle>
<contexts>
<context position="7089" citStr="Petrov and Klein, 2007" startWordPosition="1114" endWordPosition="1117"> extraction. Due to errors in the sentence and word alignment processes, not all words that are aligned are actually equivalent lexical variants. We apply the following filters to reduce such spurious alignments: • We remove any pairs where the normal word occurs in a stoplist. Stoplist words tend to be simple already and stoplist words that are being changed are likely either bad alignments or are not simplifications. • We require that the part of speeches (POS) of the two words be the same. The parts of speech were calculated based on a full parse of the sentences using the Berkeley parser (Petrov and Klein, 2007). • We remove any candidates where the POS is labeled as a proper noun. In most cases, proper nouns should not be simplified. All other aligned word pairs are extracted. To generate the simplification rules, we collect all candidate simplifications (simple words) that are aligned to the same normal word. As mentioned before, one of the biggest challenges for lexical simplification systems is generalizability. To improve the generalizability of the extracted rules, we add morphological variants of the words in the rules. For nouns, we include both singular and plural variants. For verbs, we exp</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HTLNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay Kumar Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<date>2012</date>
<booktitle>Semeval-2012 task 1: English lexical simplification. In Joint Conference on Lexical and Computerational Semantics (*SEM).</booktitle>
<contexts>
<context position="1929" citStr="Specia et al., 2012" startWordPosition="274" endWordPosition="277"> elaboration (Coster and Kauchak, 2011; Zhu et al., 2010). In this paper, we examine a restricted version of the text simplification problem, lexical simplification, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexical simplification techniques have been shown to positively impact the simplicity of text and to improve reader understanding and information retention (Leroy et al., 2013). Additionally, restricting the set of transformation operations allows for more straightforward evaluation than the general simplification problem (Specia et al., 2012). Most lexical simplification techniques rely on transformation rules that change a word or phrase into a simpler variant with similar meaning (Biran et al., 2011; Specia et al., 2012; Yatskar et al., 2010). Two main challenges exist for this type of approach. First, the lexical focus of the transformation rules makes generalization difficult; a large number of transformation rules is required to achieve reasonable coverage and impact. Second, rules do not apply in all contexts and care must be taken when performing lexical transformations to ensure local cohesion, grammaticality and, most imp</context>
<context position="3246" citStr="Specia et al., 2012" startWordPosition="483" endWordPosition="486">s. We leverage a data set of 137K aligned sentence pairs between English Wikipedia and Simple English Wikipedia to learn simplification rules. Previous approaches have used unaligned versions of Simple English Wikipedia to learn rules (Biran et al., 2011; Yatskar et al., 2010), however, by using the aligned version we are able to learn a much larger rule set. To apply lexical simplification rules to a new sentence, a decision must be made about which, if any, transformations should be applied. Previous approaches have used similarity measures (Biran et al., 2011) and feature-based approaches (Specia et al., 2012) to make this decision. We take the latter approach and train a supervised model to rank candidate transformations. 2 Problem Setup We learn lexical simplification rules that consist of a word to be simplified and a list of candidate simplifications: W → C1, C2, ..., Cm Consider the two aligned sentence pairs in Table 458 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 458–463, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics The first school was established in 1857. The first school was st</context>
<context position="9669" citStr="Specia et al., 2012" startWordPosition="1545" endWordPosition="1548">porate this decision into the ranking problem by adding w as a candidate simplification. For each rule, w → c1, c2, ..., cm we add one additional candidate simplification which does not change the sentence, w → c1, c2, ..., cm, w. If w is ranked as the most likely candidate by the ranking algorithm, then the word is not simplified. 3.2.1 Features The role of the features is to capture information about the applicability of the word in the context of the sentence as well as the simplicity of the word. Many features have been suggested previously for use in determining the simplicity of a word (Specia et al., 2012) and for determining if a word is contextually relevant (Biran et al., 2011; McCarthy and Navigli, 2007). Our goal for this paper is not feature exploration, but to examine the usefulness of a general framework for featurebased ranking for lexical simplification. The features below represent a first pass at candidate features, but many others could be explored. Candidate Probability p(ci|w): in the sentence-aligned Wikipedia data, when w is aligned to some candidate simplification, what proportion of the time is that candidate ci. Frequency The frequency of a word has been shown to correlate w</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In Joint Conference on Lexical and Computerational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Statistical Language Processing.</booktitle>
<contexts>
<context position="11071" citStr="Stolcke, 2002" startWordPosition="1776" endWordPosition="1777"> and 2) the web, as measured by the unigram frequency from the Google n-gram corpus (Brants and Franz, 2006). Language Models n-gram language models capture how likely a particular sequence is and can help identify candidate simplifications that are not appropriate in the context of the sentence. We included features from four different language models trained on four different corpora: 1) Simple English Wikipedia, 2) English Wikipedia, 3) Google n-gram corpus and 4) a linearly interpolated model between 1) and 2) with A = 0.5, i.e. an even blending. We used the SRI language modeling toolkit (Stolcke, 2002) with Kneser-Kney smoothing. All models were trigram language models except the Google n-gram model, which was a 5-gram model. Context Frequency As another measure of the applicability of a candidate in the context of the sentence, we also calculate the frequency in the Google n-grams of the candidate simplification in the context of the sentence with context windows of one and two words. If the word to be substituted is at position i in the sentence (w = si), then the one word window frequency for simplification cj is the trigram frequency of si−1 cj si+1 and the two word window the 5-gram fr</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Statistical Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from non-professionals.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="12093" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="1949" endWordPosition="1952">. If the word to be substituted is at position i in the sentence (w = si), then the one word window frequency for simplification cj is the trigram frequency of si−1 cj si+1 and the two word window the 5-gram frequency of si−2 si−1 cj si+1 si+2. 4 Data For training and evaluation of the models, we collected human labelings of 500 lexical simplification examples using Amazon’s Mechanical Turk (MTurk)1. MTurk has been used extensively for annotating and evaluating NLP tasks and has been shown to provide data that is as reliable as other forms of human annotation (Callison-Burch and Dredze, 2010; Zaidan and Callison-Burch, 2011). Figure 1 shows an example of the task we asked annotators to do. Given a sentence and a word to be simplified, the task is to suggest a simpler variant of that word that is appropriate in the context of the sentence. Candidate sentences were se1https://www.mturk.com/ 460 Enter a simpler word that could be substituted for the red, bold word in the sentence. A simpler word is one that would be understood by more people or people with a lower reading level (e.g. children). Food is procured with its suckers and then crushed using its tough “beak” of chitin. Figure 1: Example task setup on MTurk </context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011. Crowdsourcing translation: Professional quality from non-professionals. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhemin Zhu</author>
<author>Delphine Bernhard</author>
<author>Iryna Gurevych</author>
</authors>
<title>A monolingual tree-based translation model for sentence simplification.</title>
<date>2010</date>
<booktitle>In Proceedings of ICCL.</booktitle>
<contexts>
<context position="1366" citStr="Zhu et al., 2010" startWordPosition="191" endWordPosition="194">g and grammatical complexity of text while retaining the meaning (Chandrasekar and Srinivas, 1997). Text simplification techniques have a broad range of applications centered around increasing data availability to both targeted audiences, such as children, language learners, and people with cognitive disabilities, as well as to general readers in technical domains such as health and medicine (Feng, 2008). Simplifying a text can require a wide range of transformation operations including lexical changes, syntactic changes, sentence splitting, deletion and elaboration (Coster and Kauchak, 2011; Zhu et al., 2010). In this paper, we examine a restricted version of the text simplification problem, lexical simplification, where text is simplified by substituting words or phrases with simpler variants. Even with this restriction, lexical simplification techniques have been shown to positively impact the simplicity of text and to improve reader understanding and information retention (Leroy et al., 2013). Additionally, restricting the set of transformation operations allows for more straightforward evaluation than the general simplification problem (Specia et al., 2012). Most lexical simplification techniq</context>
</contexts>
<marker>Zhu, Bernhard, Gurevych, 2010</marker>
<rawString>Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych. 2010. A monolingual tree-based translation model for sentence simplification. In Proceedings of ICCL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>