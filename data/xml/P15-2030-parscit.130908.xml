<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002472">
<title confidence="0.989944">
Non-Linear Text Regression with a Deep Convolutional Neural Network
</title>
<author confidence="0.997013">
Zsolt Bitvai Trevor Cohn
</author>
<affiliation confidence="0.996079">
University of Sheffield, UK University of Melbourne, Australia
</affiliation>
<email confidence="0.989747">
z.bitvai@shef.ac.uk t.cohn@unimelb.edu.au
</email>
<sectionHeader confidence="0.997265" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997866">
Text regression has traditionally been
tackled using linear models. Here we
present a non-linear method based on a
deep convolutional neural network. We
show that despite having millions of pa-
rameters, this model can be trained on
only a thousand documents, resulting in a
40% relative improvement over sparse lin-
ear models, the previous state of the art.
Further, this method is flexible allowing
for easy incorporation of side information
such as document meta-data. Finally we
present a novel technique for interpreting
the effect of different text inputs on this
complex non-linear model.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901789473684">
Text regression involves predicting a real world
phenomenon from textual inputs, and has been
shown to be effective in domains including elec-
tion results (Lampos et al., 2013), financial risk
(Kogan et al., 2009) and public health (Lampos
and Cristianini, 2010). Almost universally, the
text regression problem has been framed as lin-
ear regression, with the modelling innovation fo-
cussed on effective regression, e.g., using Lasso
penalties to promote feature sparsity (Tibshirani,
1996).1 Despite their successes, linear models are
limiting: text regression problems will often in-
volve complex interactions between textual inputs,
thus requiring a non-linear approach to properly
capture such phenomena. For instance, in mod-
elling movie revenue conjunctions of features are
likely to be important, e.g., a movie described as
‘scary’ is likely to have different effects for chil-
dren’s versus adult movies. While these kinds of
</bodyText>
<footnote confidence="0.955957">
1Some preliminary work has shown strong results for non-
linear text regression using Gaussian Process models (Lam-
pos et al., 2014), however this approach has not been shown
to scale to high dimensional inputs.
</footnote>
<bodyText confidence="0.999414097560976">
features can be captured using explicit feature en-
gineering, this process is tedious, limited in scope
(e.g., to conjunctions) and – as we show here –
can be dramatically improved by representational
learning as part of a non-linear model.
In this paper, we propose an artificial neu-
ral network (ANN) for modelling text regression.
In language processing, ANNs were first pro-
posed for probabilistic language modelling (Ben-
gio et al., 2003), followed by models of sentences
(Kalchbrenner et al., 2014) and parsing (Socher
et al., 2013) inter alia. These approaches have
shown strong results through automatic learning
dense low-dimensional distributed representations
for words and other linguistic units, which have
been shown to encode important aspects of lan-
guage syntax and semantics. In this paper we
develop a convolutional neural network, inspired
by their breakthrough results in image process-
ing (Krizhevsky et al., 2012) and recent applica-
tions to language processing (Kalchbrenner et al.,
2014; Kim, 2014). These works have mainly fo-
cused on ‘big data’ problems with plentiful train-
ing examples. Given their large numbers of pa-
rameters, often in the millions, one would expect
that such models can only be effectively learned
on very large datasets. However we show here
that a complex deep convolution network can be
trained on about a thousand training examples, al-
though careful model design and regularisation is
paramount.
We consider the problem of predicting the fu-
ture box-office takings of movies based on reviews
by movie critics and movie attributes. Our ap-
proach is based on the method and dataset of Joshi
et al. (2010), who presented a linear regression
model over uni-, bi-, and tri-gram term frequency
counts extracted from reviews, as well as movie
and reviewer metadata. This problem is especially
interesting, as comparatively few instances are
available for training (see Table 1) while each in-
</bodyText>
<page confidence="0.829954">
180
</page>
<note confidence="0.295187333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 180–185,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.987246">
train dev test total
# movies 1147 317 254 1718
# reviews per movie 4.2 4.0 4.1 4.1
# sentences per movie 95 84 82 91
# words per movie 2879 2640 2605 2794
</table>
<tableCaption confidence="0.999036">
Table 1: Movie review dataset (Joshi et al., 2010).
</tableCaption>
<bodyText confidence="0.999845724137931">
stance (movie) includes a rich array of data includ-
ing the text of several critic reviews from various
review sites, as well as structured data (genre, rat-
ing, actors, etc.) Joshi et al. found that regression
purely from movie meta-data gave strong predic-
tive accuracy, while text had a weaker but comple-
mentary signal. Their best results were achieved
by domain adaptation whereby text features were
conjoined with a review site identifier. Inspired by
Joshi et al. (2010) our model also operates over n-
grams, 1 &lt; n &lt; 3, and movie metadata, albeit
using an ANN in place of their linear model. We
use word embeddings to represent words in a low
dimensional space, a convolutional network with
max-pooling to represent documents in terms of
n-grams, and several fully connected hidden lay-
ers to allow for learning of complex non-linear in-
teractions. We show that including non-linearities
in the model is crucial for accurate modelling, pro-
viding a relative error reduction of 40% (MAE)
over their best linear model. Our final contribu-
tion is a novel means of model interpretation. Al-
though it is notoriously difficult to interpret the pa-
rameters of an ANN, we show a simple method of
quantifying the effect of text n-grams on the pre-
diction output. This allows for identification of the
most important textual inputs, and investigation of
non-linear interactions between these words and
phrases in different data instances.
</bodyText>
<sectionHeader confidence="0.977064" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999679166666667">
The outline of the convolutional network is shown
in Figure 1. We have n training examples of the
form {bi, ri, yiJni=1, where bi is the meta data as-
sociated with movie i, yi is the target gross week-
end revenue, and ri is a collection of ui number of
reviews, ri = {xj, tjJui j=1 where each review has
review text xj and a site id tj. We concatenate all
the review texts di = (x1, x2, ..., xu) to form our
text input (see part I of Figure 1).
To acquire a distributed representation of the
text, we look up the input tokens in a pretrained
word embedding matrix E with size |V |xe, where
</bodyText>
<figureCaption confidence="0.999131">
Figure 1: Outline of the network architecture.
</figureCaption>
<bodyText confidence="0.9876334375">
|V  |is the size of our vocabulary and e is the em-
bedding dimensionality. This gives us a dense
document matrix Di,j = Edi,j with dimensions
m x e where m is the number of tokens in the
document.
Since the length of text documents vary, in part
II we apply convolutions with width one, two and
three over the document matrix to obtain a fixed
length representation of the text. The n-gram con-
volutions help identify local context and map that
to a new higher level feature space. For each fea-
ture map, the convolution takes adjacent word em-
beddings and performs a feed forward computa-
tion with shared weights over the convolution win-
dow. For a convolution with width 1 &lt; q &lt; m this
is
</bodyText>
<equation confidence="0.99985">
S(q) = (Di,·,Di+1,·, ..., Di+q−1,·)
i,·
C(q)
i,· = (S(q) i,· ,W(q))
</equation>
<bodyText confidence="0.815542">
where S(q)
i,· is q adjacent word embeddings con-
catenated, and C(q) is the convolution output ma-
trix with (m−q+1) rows after a linear transforma-
tion with weights W(q). To allow for a non-linear
</bodyText>
<figure confidence="0.991053417475728">
e
Embeddings, E
...
• • •
•
• • • •
• • • •
• • •
• • • •
•
• • •
•
|V|
e
m
u
Boston Globe, r1= {x1,t1}
This movie is the best
one of the century, the
actors are exceptional
...
New York Times ru
(I) This
movie
is
the
best
one
...
1
a
b
p(1) p(2) p(3)
fully connected
W1
g
h
ReLu
fully connected
W2
g
h
ReLu
fully connected
W3
g
h
ReLu
o1=a2
o3=a4
(V)
g
J
prediction $ Revenue
w4
fully connected
error
f y
o2=a3
(IV)
max
pool
m
uni-grams
ReLu
C(1)
g W(1)
m
h ReLu h ReLu h
This
movie
is
the
best
one
...
m-1
bi-grams
Site ids, t
Document matrix, D
...
• • •
• • • •
•
• • • •
• • •
• • • •
g W(2) g W(3)
C(2)
•
• • •
m-2
tri-grams
0 1 1 0 0
C(3)
•
convolution
genre,
rating,
famous
actors
...
Meta data
</figure>
<page confidence="0.986969">
181
</page>
<bodyText confidence="0.999963888888889">
transformation, we make use of rectified linear ac-
tivation units, H(q) = max(C(q), 0), which are
universal function approximators. Finally, to com-
press the representation of text to a fixed dimen-
sional vector while ensuring that important infor-
mation is preserved and propagated throughout the
network, we apply max pooling over time, i.e. the
sequence of words, for each dimension, as shown
in part III,
</bodyText>
<equation confidence="0.970169">
p�q) = max H· �)
</equation>
<bodyText confidence="0.999044368421053">
where p(q)
j is dimension j of the pooling layer for
convolution q, and p is the concatenation of all
pooling layers, p = (p(1), p(2), ..., p(q)).
Next, we perform a series of non-linear trans-
formations to the document vector in order to pro-
gressively acquire higher level representations of
the text and approximate a linear relationship in
the final output prediction layer. Applying multi-
ple hidden layers in succession can require expo-
nentially less data than mapping through a single
hidden layer (Bengio, 2009). Therefore, in part
IV, we apply densely connected neural net layers
of the form ok = h(g(ak, Wk)) where ak is the
input and ok = ak+1 is the output vector for layer
k, g is a linear transformation function (ak, Wk),
and h is the activation function, i.e. rectified linear
seen above. l = 3 hidden layers are applied be-
fore the final regression layer to produce the out-
put f = g(ol, wl+1) in part V.
The mean absolute error is measured between
the predictions f and the targets y, which is more
permissible to outliers than the squared error. The
cost J is defined as
|fv − yv|.
The network is trained with stochastic gradient de-
scent and the Ada Delta (Zeiler, 2012) update rule
using random restarts. Stochastic gradient descent
is noisier than batch training due to a local esti-
mation of the gradient, but it can start converging
much faster. Ada Delta keeps an exponentially de-
caying history of gradients and updates in order to
adapt the learning rate for each parameter, which
partially smooths out the training noise. Regulari-
sation and hyperparmeter selection are performed
by early stopping on the development set. The
size of the vocabulary is 90K words. Note that
10% of our lexicon is not found in the embeddings
</bodyText>
<table confidence="0.999740416666667">
Model Description MAE($M)
Baseline mean 11.7
Linear Text 8.0
Linear Text+Domain+POS 7.4
Linear Meta 6.0
Linear Text+Meta 5.9
Linear Text+Meta+Domain+Deps 5.7
ANN Text 6.3
ANN Text+Domain 6.0
ANN Meta 3.9
ANN Text+Meta 3.4
ANN Text+Meta+Domain 3.4
</table>
<tableCaption confidence="0.980954">
Table 2: Experiment results on test set. Linear
models by (Joshi et al., 2010).
</tableCaption>
<bodyText confidence="0.97040947368421">
pretrained on Google News. Those terms are ini-
tialised with random small weights. The model
has around 4 million weights plus 27 million tun-
able word embedding parameters.
Structured data Besides text, injecting meta
data and domain information into the model likely
provides additional predictive power. Combin-
ing text with structured data early in the network
fosters joint non-linear interaction in subsequent
hidden layers. Hence, if meta data b is present,
we concatenate that with the max pooling layer
a1 = (p, b) in part III. Domain specific infor-
mation t is appended to each n-gram convolution
input (S(q)
i,· ,t) in part II, where tz = 1z indicates
whether domain z has reviewed the movie.2 This
helps the network bias the convolutions, and thus
change which features get propagated in the pool-
ing layer.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.992057375">
The results in Table 2 show that the neural net-
work performs very well, with around 40% im-
provement over the previous best results (Joshi et
al., 2010). Our dataset splits are identical, and we
have accurately reproduced the results of their lin-
ear model. Non-linearities are clearly helpful as
evidenced by the ANN Text model beating the bag
of words Linear Text model with a mean absolute
test error of 6.0 vs 8.0. Moreover, simply using
structured data in the ANN Meta beats all the Lin-
ear models by a sizeable margin. Further improve-
ments are realised through the inclusion of text,
giving the lowest error of 3.4. Note that Joshi et al.
(2010) preprocessed the text by stemming, down-
2Alternatively, site information can be encoded with one-
hot categorical variables.
</bodyText>
<equation confidence="0.950251">
J = 1 n
n
v=1
</equation>
<page confidence="0.96331">
182
</page>
<figure confidence="0.988483785714286">
Model Description MAE($M)
fixed word2vec embeddings 3.4*
tuned word2vec embeddings 3.6
fixed random embeddings 3.6
tuned random embeddings 3.8
uni-grams 3.6
uni+bi-grams 3.5
uni+bi+tri-grams 3.4*
uni+bi+tri+four-grams 3.6
0 hidden layer 6.3
1 hidden layer 3.9
2 hidden layers 3.5
3 hidden layers 3.4*
4 hidden layers 3.6
</figure>
<tableCaption confidence="0.934564">
Table 3: Various alternative configurations, based
</tableCaption>
<bodyText confidence="0.988551813953489">
on the ANN Text+Meta model. The asterisk (∗)
denotes the settings in the ANN Text+Meta model.
casing, and discarding feature instances that oc-
curred in fewer than five reviews. In contrast, we
did not perform any processing of the text or fea-
ture engineering, apart from tokenization, instead
learning this automatically.3
We find that both text and meta data con-
tain complementary signals with some informa-
tion overlap between them. This confirms the find-
ing of Bitvai and Cohn (2015) on another text re-
gression problem. The meta features alone almost
achieve the best results whereas text alone per-
forms worse but still well above the baseline. For
the combined model, the performance improves
slightly. In Table 3 we can see that contrary to ex-
pectations, fine tuning the word embeddings does
not help significantly compared to keeping them
fixed. Moreover, randomly initialising the embed-
dings and fixing them performs quite well. Fine
tuning may be a challenging optimisation due to
the high dimensional embedding space and the
loss of monolingual information. This is further
exacerbated due to the limited supervision signal.
One of the main sources of improvement ap-
pears to come from the non-linearity applied to the
neural network activations. To test this, we try us-
ing linear activation units in parts II and IV of the
network. Composition of linear functions yields a
linear function, and therefore we recover the lin-
ear model results. This is much worse than the
model with non-linear activations. Changing the
network depth, we find that the model performs
much better with a single hidden layer than with-
3Although we do make use of pretrained word embed-
dings in our text features.
out any, while three hidden layers are optimal. For
the weight dimensions we find square 1058 dimen-
sional weights to perform the best. The ideal num-
ber of convolutions are three with uni, bi and tri-
grams, but unigrams alone perform only slightly
worse, while taking a larger n-gram window n &gt; 3
does not help. Average and sum pooling perform
comparatively well, while max pooling achieves
the best result. Note that sum pooling recovers a
non-linear bag-of-words model. With respect to
activation functions, both ReLU and sigmoid work
well.
Model extensions Multi task learning with task
identifiers, ANN Text+Domain, does improve the
ANN Text model. This suggests that the tendency
by certain sites to review specific movies is in it-
self indicative of the revenue. However this im-
provement is more difficult to discern with the
ANN Text+Meta+Domain model, possibly due to
redundancy with the meta data. An alternative ap-
proach for multi-task learning is to have a sepa-
rate convolutional weight matrix for each review
site, which can learn site specific characteristics
of the text. This can also be achieved with site
specific word embedding dimensions. However
neither of these methods resulted in performance
improvements. In addition, we experimented with
applying a hierarchical convolution over reviews
in two steps with k-max pooling (Kalchbrenner et
al., 2014), as well as parsing sentences recursively
(Socher et al., 2013), but did not observe any im-
provements.
For optimisation, both Ada Grad and Steepest
Gradient Descent had occasional problems with
local minima, which Ada Delta was able to es-
cape more often. In contrast to earlier work (Kim,
2014), applying dropout on the final layer did not
improve the validation error. The optimiser mostly
found good parameters after around 40 epochs
which took around 30 minutes on a NVidia Kepler
Tesla K40m GPU.
Model interpretation Next we perform anal-
ysis to determine which words and phrases in-
fluenced the output the most in the ANN Text
model. To do so, we set each phrase input to
zeros in turn and measure the prediction differ-
ence for each movie across the test set. We re-
port the min/max/average/count values in Table
4. We isolate the effect of each n-gram by mak-
ing sure the uni, bi and trigrams are independent,
</bodyText>
<page confidence="0.996633">
183
</page>
<figure confidence="0.807122">
$0
</figure>
<figureCaption confidence="0.98316775">
Figure 2: Projection of the last hidden layer of
test movies using t-SNE. Red means high and blue
means low revenue. The cross vs dot symbols in-
dicate a production budget above or below $15M.
</figureCaption>
<bodyText confidence="0.999953388888889">
i.e. we process “Hong Kong” without zeroing
“Hong” or “Kong”. About 95% of phrases re-
sult in no output change, including common sen-
timent words, which shows that text regression is
a different problem to sentiment analysis. We see
that words related to series “# 2”, effects, awards
“praise”, positive sentiment “intense”, locations,
references “Batman”, body parts “chest”, and oth-
ers such as “plot twist”, “evil”, and “cameo” re-
sult in increased revenue by up to $5 million.
On the other hand, words related to independent
films “v´erit´e”, documentaries “the period”, for-
eign film “English subtitles” and negative senti-
ment decrease revenue. Note that the model has
identified structured data in the unstructured text,
such as related to revenue of prequels “39 mil-
lion”, crew members, duration “15 minutes in”,
genre “[sci] fi”, ratings, sexuality, profanity, re-
lease periods “late 2008 release”, availability “In
selected theaters” and themes. Phrases can be
composed, such as “action unfolds” amplifies “ac-
tion”, and “cautioned” is amplified by “strongly
cautioned”. “functional” is neutral, but “func-
tional at best” is strongly negative. Some words
exhibit both positive and negative impacts depend-
ing on the context. This highlights the limitation
of a linear model which is unable to discover these
non-linear relationships. “13 - year [old]” is posi-
tive in New in Town, a romantic comedy and nega-
tive in Zombieland, a horror. The character strings
“k /” (mannerism of reviewer), “they’re” (unique
apostrophe), “&apos;” (encoding error) are high im-
pact and unique to specific review sites, showing
that the model indirectly uncovers domain infor-
mation. This can explain the limited gain that can
be achieved via multi task learning. Last, we have
</bodyText>
<table confidence="0.615338214285714">
min max avg #
20 4400 2300 28
0 3700 1600 22
1500 3600 2200 3
10 3400 1800 27
22 3400 1400 13
min max avg #
-3100 -3100 -3100 1
-2500 1 -570 75
-2400 -900 -1500 3
-2200 -2200 -2200 1
-2200 -2200 -2200 1
min max avg #
145 3000 1700 28
-7 1500 700 105
3 1200 560 42
3 1300 530 68
10 1600 500 17
8 950 440 72
-15 340 160 26
7 95 45 28
-440 40 -85 11
-780 1 -180 77
-850 6 -180 41
-790 3 -180 10
-750 -3 -220 19
-990 -2 -320 12
-1600 6 -520 13
</table>
<tableCaption confidence="0.930094">
Table 4: Selected phrase impacts on the predic-
</tableCaption>
<bodyText confidence="0.978159272727273">
tions in $ USD(K) in the test set, showing min,
max and avg change in prediction value and num-
ber of occurrences (denoted #). Periods denote ab-
breviations (language, accompanying).
plotted the last hidden layer of each test set movie
with t-SNE (Van der Maaten and Hinton, 2008).
This gives a high level representation of a movie.
In Figure 2 it is visible that the test set movies can
be discriminated into high and low revenue groups
and this also correlates closely with their produc-
tion budget.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999793083333333">
In this paper, we have shown that convolutional
neural networks with deep architectures greatly
outperform linear models even with very little su-
pervision, and they can identify key textual and
numerical characteristics of data with respect to
predicting a real world phenomenon. In addition,
we have demonstrated a way to intuitively inter-
pret the model. In the future, we will investi-
gate ways for automatically optimising the hyper-
parameters of the network (Snoek et al., 2012) and
various extensions to recursive or hierarchical con-
volutions.
</bodyText>
<figure confidence="0.998255348837209">
transformers2
inglouriousbasterds
fastandfurious4
mylifeinruins
pinkpanther2
greatbuckhoward
informers
bobfunk
grace
objective
budget &gt; $15M
budget &lt; $15M
$200M Top 5 positive phrases
sequel
$5.2M flick
k /
product
predecessor
Top 5 negative phrases
$3.6K
Revenue
$140K
Mildly raunchy lang.
( Under 17
Lars von
talk the language
. their English
Selected phrases
CGI
action
summer
they’re
1950s
hit
fi
Cage
Hong Kong
requires acc. parent
English
Sundance Film Festival
written and directed
independent
some strong language
</figure>
<page confidence="0.993818">
184
</page>
<sectionHeader confidence="0.998029" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860647887324">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1–127.
Zsolt Bitvai and Trevor Cohn. 2015. Predicting peer-
to-peer loan rates using Bayesian non-linear regres-
sion. In Proceedings of the 29th AAAI conference
on Artificial Intelligence, pages 2203–2210.
Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and
Noah A Smith. 2010. Movie reviews and revenues:
An experiment in text regression. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 293–296.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Shimon Kogan, Dimitry Levin, Bryan R Routledge,
Jacob S Sagi, and Noah A Smith. 2009. Pre-
dicting risk from financial reports with regression.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 272–280.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-
ton. 2012. ImageNet classification with deep con-
volutional neural networks. In Advances in Neural
Information Processing Systems, pages 1097–1105.
Vasileios Lampos and Nello Cristianini. 2010. Track-
ing the flu pandemic by monitoring the social web.
In Cognitive Information Processing, pages 411–
416.
Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from social media. In Proc 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 993–1003.
Vasileios Lampos, Nikolaos Aletras, D Preot¸iuc-Pietro,
and Trevor Cohn. 2014. Predicting and characteris-
ing user impact on twitter. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 405–
–413.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams.
2012. Practical Bayesian optimization of machine
learning algorithms. In Advances in Neural Infor-
mation Processing Systems, pages 2951–2959.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 1631–1642.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B (Methodological), pages 267–288.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-SNE. Journal of Machine
Learning Research, 9(2579-2605):85.
Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701.
</reference>
<page confidence="0.998845">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951883">
<title confidence="0.999526">Non-Linear Text Regression with a Deep Convolutional Neural Network</title>
<author confidence="0.999606">Zsolt Bitvai Trevor Cohn</author>
<affiliation confidence="0.983761">University of Sheffield, UK University of Melbourne, Australia</affiliation>
<email confidence="0.976631">z.bitvai@shef.ac.ukt.cohn@unimelb.edu.au</email>
<abstract confidence="0.9994594375">Text regression has traditionally been tackled using linear models. Here we present a non-linear method based on a deep convolutional neural network. We show that despite having millions of parameters, this model can be trained on only a thousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art. Further, this method is flexible allowing for easy incorporation of side information such as document meta-data. Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2390" citStr="Bengio et al., 2003" startWordPosition="358" endWordPosition="362">hown strong results for nonlinear text regression using Gaussian Process models (Lampos et al., 2014), however this approach has not been shown to scale to high dimensional inputs. features can be captured using explicit feature engineering, this process is tedious, limited in scope (e.g., to conjunctions) and – as we show here – can be dramatically improved by representational learning as part of a non-linear model. In this paper, we propose an artificial neural network (ANN) for modelling text regression. In language processing, ANNs were first proposed for probabilistic language modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have main</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for AI. Foundations and Trends</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="9022" citStr="Bengio, 2009" startWordPosition="1545" endWordPosition="1546">ver time, i.e. the sequence of words, for each dimension, as shown in part III, p�q) = max H· �) where p(q) j is dimension j of the pooling layer for convolution q, and p is the concatenation of all pooling layers, p = (p(1), p(2), ..., p(q)). Next, we perform a series of non-linear transformations to the document vector in order to progressively acquire higher level representations of the text and approximate a linear relationship in the final output prediction layer. Applying multiple hidden layers in succession can require exponentially less data than mapping through a single hidden layer (Bengio, 2009). Therefore, in part IV, we apply densely connected neural net layers of the form ok = h(g(ak, Wk)) where ak is the input and ok = ak+1 is the output vector for layer k, g is a linear transformation function (ak, Wk), and h is the activation function, i.e. rectified linear seen above. l = 3 hidden layers are applied before the final regression layer to produce the output f = g(ol, wl+1) in part V. The mean absolute error is measured between the predictions f and the targets y, which is more permissible to outliers than the squared error. The cost J is defined as |fv − yv|. The network is train</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zsolt Bitvai</author>
<author>Trevor Cohn</author>
</authors>
<title>Predicting peerto-peer loan rates using Bayesian non-linear regression.</title>
<date>2015</date>
<booktitle>In Proceedings of the 29th AAAI conference on Artificial Intelligence,</booktitle>
<pages>2203--2210</pages>
<contexts>
<context position="13046" citStr="Bitvai and Cohn (2015)" startWordPosition="2220" endWordPosition="2223">r 6.3 1 hidden layer 3.9 2 hidden layers 3.5 3 hidden layers 3.4* 4 hidden layers 3.6 Table 3: Various alternative configurations, based on the ANN Text+Meta model. The asterisk (∗) denotes the settings in the ANN Text+Meta model. casing, and discarding feature instances that occurred in fewer than five reviews. In contrast, we did not perform any processing of the text or feature engineering, apart from tokenization, instead learning this automatically.3 We find that both text and meta data contain complementary signals with some information overlap between them. This confirms the finding of Bitvai and Cohn (2015) on another text regression problem. The meta features alone almost achieve the best results whereas text alone performs worse but still well above the baseline. For the combined model, the performance improves slightly. In Table 3 we can see that contrary to expectations, fine tuning the word embeddings does not help significantly compared to keeping them fixed. Moreover, randomly initialising the embeddings and fixing them performs quite well. Fine tuning may be a challenging optimisation due to the high dimensional embedding space and the loss of monolingual information. This is further exa</context>
</contexts>
<marker>Bitvai, Cohn, 2015</marker>
<rawString>Zsolt Bitvai and Trevor Cohn. 2015. Predicting peerto-peer loan rates using Bayesian non-linear regression. In Proceedings of the 29th AAAI conference on Artificial Intelligence, pages 2203–2210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Dipanjan Das</author>
<author>Kevin Gimpel</author>
<author>Noah A Smith</author>
</authors>
<title>Movie reviews and revenues: An experiment in text regression.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>293--296</pages>
<contexts>
<context position="3594" citStr="Joshi et al. (2010)" startWordPosition="550" endWordPosition="553">works have mainly focused on ‘big data’ problems with plentiful training examples. Given their large numbers of parameters, often in the millions, one would expect that such models can only be effectively learned on very large datasets. However we show here that a complex deep convolution network can be trained on about a thousand training examples, although careful model design and regularisation is paramount. We consider the problem of predicting the future box-office takings of movies based on reviews by movie critics and movie attributes. Our approach is based on the method and dataset of Joshi et al. (2010), who presented a linear regression model over uni-, bi-, and tri-gram term frequency counts extracted from reviews, as well as movie and reviewer metadata. This problem is especially interesting, as comparatively few instances are available for training (see Table 1) while each in180 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 180–185, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics train dev test total # movies 1147 317</context>
<context position="4837" citStr="Joshi et al. (2010)" startWordPosition="751" endWordPosition="754">movie 4.2 4.0 4.1 4.1 # sentences per movie 95 84 82 91 # words per movie 2879 2640 2605 2794 Table 1: Movie review dataset (Joshi et al., 2010). stance (movie) includes a rich array of data including the text of several critic reviews from various review sites, as well as structured data (genre, rating, actors, etc.) Joshi et al. found that regression purely from movie meta-data gave strong predictive accuracy, while text had a weaker but complementary signal. Their best results were achieved by domain adaptation whereby text features were conjoined with a review site identifier. Inspired by Joshi et al. (2010) our model also operates over ngrams, 1 &lt; n &lt; 3, and movie metadata, albeit using an ANN in place of their linear model. We use word embeddings to represent words in a low dimensional space, a convolutional network with max-pooling to represent documents in terms of n-grams, and several fully connected hidden layers to allow for learning of complex non-linear interactions. We show that including non-linearities in the model is crucial for accurate modelling, providing a relative error reduction of 40% (MAE) over their best linear model. Our final contribution is a novel means of model interpre</context>
<context position="10573" citStr="Joshi et al., 2010" startWordPosition="1811" endWordPosition="1814">o adapt the learning rate for each parameter, which partially smooths out the training noise. Regularisation and hyperparmeter selection are performed by early stopping on the development set. The size of the vocabulary is 90K words. Note that 10% of our lexicon is not found in the embeddings Model Description MAE($M) Baseline mean 11.7 Linear Text 8.0 Linear Text+Domain+POS 7.4 Linear Meta 6.0 Linear Text+Meta 5.9 Linear Text+Meta+Domain+Deps 5.7 ANN Text 6.3 ANN Text+Domain 6.0 ANN Meta 3.9 ANN Text+Meta 3.4 ANN Text+Meta+Domain 3.4 Table 2: Experiment results on test set. Linear models by (Joshi et al., 2010). pretrained on Google News. Those terms are initialised with random small weights. The model has around 4 million weights plus 27 million tunable word embedding parameters. Structured data Besides text, injecting meta data and domain information into the model likely provides additional predictive power. Combining text with structured data early in the network fosters joint non-linear interaction in subsequent hidden layers. Hence, if meta data b is present, we concatenate that with the max pooling layer a1 = (p, b) in part III. Domain specific information t is appended to each n-gram convolu</context>
<context position="12048" citStr="Joshi et al. (2010)" startWordPosition="2063" endWordPosition="2066">hat the neural network performs very well, with around 40% improvement over the previous best results (Joshi et al., 2010). Our dataset splits are identical, and we have accurately reproduced the results of their linear model. Non-linearities are clearly helpful as evidenced by the ANN Text model beating the bag of words Linear Text model with a mean absolute test error of 6.0 vs 8.0. Moreover, simply using structured data in the ANN Meta beats all the Linear models by a sizeable margin. Further improvements are realised through the inclusion of text, giving the lowest error of 3.4. Note that Joshi et al. (2010) preprocessed the text by stemming, down2Alternatively, site information can be encoded with onehot categorical variables. J = 1 n n v=1 182 Model Description MAE($M) fixed word2vec embeddings 3.4* tuned word2vec embeddings 3.6 fixed random embeddings 3.6 tuned random embeddings 3.8 uni-grams 3.6 uni+bi-grams 3.5 uni+bi+tri-grams 3.4* uni+bi+tri+four-grams 3.6 0 hidden layer 6.3 1 hidden layer 3.9 2 hidden layers 3.5 3 hidden layers 3.4* 4 hidden layers 3.6 Table 3: Various alternative configurations, based on the ANN Text+Meta model. The asterisk (∗) denotes the settings in the ANN Text+Meta </context>
</contexts>
<marker>Joshi, Das, Gimpel, Smith, 2010</marker>
<rawString>Mahesh Joshi, Dipanjan Das, Kevin Gimpel, and Noah A Smith. 2010. Movie reviews and revenues: An experiment in text regression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 293–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2451" citStr="Kalchbrenner et al., 2014" startWordPosition="368" endWordPosition="371"> Gaussian Process models (Lampos et al., 2014), however this approach has not been shown to scale to high dimensional inputs. features can be captured using explicit feature engineering, this process is tedious, limited in scope (e.g., to conjunctions) and – as we show here – can be dramatically improved by representational learning as part of a non-linear model. In this paper, we propose an artificial neural network (ANN) for modelling text regression. In language processing, ANNs were first proposed for probabilistic language modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training exa</context>
<context position="15626" citStr="Kalchbrenner et al., 2014" startWordPosition="2638" endWordPosition="2641">icative of the revenue. However this improvement is more difficult to discern with the ANN Text+Meta+Domain model, possibly due to redundancy with the meta data. An alternative approach for multi-task learning is to have a separate convolutional weight matrix for each review site, which can learn site specific characteristics of the text. This can also be achieved with site specific word embedding dimensions. However neither of these methods resulted in performance improvements. In addition, we experimented with applying a hierarchical convolution over reviews in two steps with k-max pooling (Kalchbrenner et al., 2014), as well as parsing sentences recursively (Socher et al., 2013), but did not observe any improvements. For optimisation, both Ada Grad and Steepest Gradient Descent had occasional problems with local minima, which Ada Delta was able to escape more often. In contrast to earlier work (Kim, 2014), applying dropout on the final layer did not improve the validation error. The optimiser mostly found good parameters after around 40 epochs which took around 30 minutes on a NVidia Kepler Tesla K40m GPU. Model interpretation Next we perform analysis to determine which words and phrases influenced the o</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2967" citStr="Kim, 2014" startWordPosition="446" endWordPosition="447">e modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training examples. Given their large numbers of parameters, often in the millions, one would expect that such models can only be effectively learned on very large datasets. However we show here that a complex deep convolution network can be trained on about a thousand training examples, although careful model design and regularisation is paramount. We consider the problem of predicting the future box-office takings of movies based on reviews by movie critics and movie attributes. Our approach is based on the method and dat</context>
<context position="15921" citStr="Kim, 2014" startWordPosition="2689" endWordPosition="2690"> characteristics of the text. This can also be achieved with site specific word embedding dimensions. However neither of these methods resulted in performance improvements. In addition, we experimented with applying a hierarchical convolution over reviews in two steps with k-max pooling (Kalchbrenner et al., 2014), as well as parsing sentences recursively (Socher et al., 2013), but did not observe any improvements. For optimisation, both Ada Grad and Steepest Gradient Descent had occasional problems with local minima, which Ada Delta was able to escape more often. In contrast to earlier work (Kim, 2014), applying dropout on the final layer did not improve the validation error. The optimiser mostly found good parameters after around 40 epochs which took around 30 minutes on a NVidia Kepler Tesla K40m GPU. Model interpretation Next we perform analysis to determine which words and phrases influenced the output the most in the ANN Text model. To do so, we set each phrase input to zeros in turn and measure the prediction difference for each movie across the test set. We report the min/max/average/count values in Table 4. We isolate the effect of each n-gram by making sure the uni, bi and trigrams</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Kogan</author>
<author>Dimitry Levin</author>
<author>Bryan R Routledge</author>
<author>Jacob S Sagi</author>
<author>Noah A Smith</author>
</authors>
<title>Predicting risk from financial reports with regression.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>272--280</pages>
<contexts>
<context position="1027" citStr="Kogan et al., 2009" startWordPosition="149" endWordPosition="152"> this model can be trained on only a thousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art. Further, this method is flexible allowing for easy incorporation of side information such as document meta-data. Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model. 1 Introduction Text regression involves predicting a real world phenomenon from textual inputs, and has been shown to be effective in domains including election results (Lampos et al., 2013), financial risk (Kogan et al., 2009) and public health (Lampos and Cristianini, 2010). Almost universally, the text regression problem has been framed as linear regression, with the modelling innovation focussed on effective regression, e.g., using Lasso penalties to promote feature sparsity (Tibshirani, 1996).1 Despite their successes, linear models are limiting: text regression problems will often involve complex interactions between textual inputs, thus requiring a non-linear approach to properly capture such phenomena. For instance, in modelling movie revenue conjunctions of features are likely to be important, e.g., a movie</context>
</contexts>
<marker>Kogan, Levin, Routledge, Sagi, Smith, 2009</marker>
<rawString>Shimon Kogan, Dimitry Levin, Bryan R Routledge, Jacob S Sagi, and Noah A Smith. 2009. Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 272–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>ImageNet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1097--1105</pages>
<contexts>
<context position="2881" citStr="Krizhevsky et al., 2012" startWordPosition="431" endWordPosition="434">odelling text regression. In language processing, ANNs were first proposed for probabilistic language modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training examples. Given their large numbers of parameters, often in the millions, one would expect that such models can only be effectively learned on very large datasets. However we show here that a complex deep convolution network can be trained on about a thousand training examples, although careful model design and regularisation is paramount. We consider the problem of predicting the future box-office takings of movies based on revi</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pages 1097–1105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nello Cristianini</author>
</authors>
<title>Tracking the flu pandemic by monitoring the social web.</title>
<date>2010</date>
<booktitle>In Cognitive Information Processing,</booktitle>
<pages>411--416</pages>
<contexts>
<context position="1076" citStr="Lampos and Cristianini, 2010" startWordPosition="156" endWordPosition="159">ousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art. Further, this method is flexible allowing for easy incorporation of side information such as document meta-data. Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model. 1 Introduction Text regression involves predicting a real world phenomenon from textual inputs, and has been shown to be effective in domains including election results (Lampos et al., 2013), financial risk (Kogan et al., 2009) and public health (Lampos and Cristianini, 2010). Almost universally, the text regression problem has been framed as linear regression, with the modelling innovation focussed on effective regression, e.g., using Lasso penalties to promote feature sparsity (Tibshirani, 1996).1 Despite their successes, linear models are limiting: text regression problems will often involve complex interactions between textual inputs, thus requiring a non-linear approach to properly capture such phenomena. For instance, in modelling movie revenue conjunctions of features are likely to be important, e.g., a movie described as ‘scary’ is likely to have different</context>
</contexts>
<marker>Lampos, Cristianini, 2010</marker>
<rawString>Vasileios Lampos and Nello Cristianini. 2010. Tracking the flu pandemic by monitoring the social web. In Cognitive Information Processing, pages 411– 416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Daniel Preotiuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A user-centric model of voting intention from social media.</title>
<date>2013</date>
<booktitle>In Proc 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>993--1003</pages>
<contexts>
<context position="990" citStr="Lampos et al., 2013" startWordPosition="143" endWordPosition="146">despite having millions of parameters, this model can be trained on only a thousand documents, resulting in a 40% relative improvement over sparse linear models, the previous state of the art. Further, this method is flexible allowing for easy incorporation of side information such as document meta-data. Finally we present a novel technique for interpreting the effect of different text inputs on this complex non-linear model. 1 Introduction Text regression involves predicting a real world phenomenon from textual inputs, and has been shown to be effective in domains including election results (Lampos et al., 2013), financial risk (Kogan et al., 2009) and public health (Lampos and Cristianini, 2010). Almost universally, the text regression problem has been framed as linear regression, with the modelling innovation focussed on effective regression, e.g., using Lasso penalties to promote feature sparsity (Tibshirani, 1996).1 Despite their successes, linear models are limiting: text regression problems will often involve complex interactions between textual inputs, thus requiring a non-linear approach to properly capture such phenomena. For instance, in modelling movie revenue conjunctions of features are </context>
</contexts>
<marker>Lampos, Preotiuc-Pietro, Cohn, 2013</marker>
<rawString>Vasileios Lampos, Daniel Preotiuc-Pietro, and Trevor Cohn. 2013. A user-centric model of voting intention from social media. In Proc 51st Annual Meeting of the Association for Computational Linguistics, pages 993–1003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nikolaos Aletras</author>
<author>D Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>Predicting and characterising user impact on twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>405--413</pages>
<marker>Lampos, Aletras, Preot¸iuc-Pietro, Cohn, 2014</marker>
<rawString>Vasileios Lampos, Nikolaos Aletras, D Preot¸iuc-Pietro, and Trevor Cohn. 2014. Predicting and characterising user impact on twitter. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 405– –413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jasper Snoek</author>
<author>Hugo Larochelle</author>
<author>Ryan P Adams</author>
</authors>
<title>Practical Bayesian optimization of machine learning algorithms.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2951--2959</pages>
<contexts>
<context position="19968" citStr="Snoek et al., 2012" startWordPosition="3396" endWordPosition="3399"> movies can be discriminated into high and low revenue groups and this also correlates closely with their production budget. 4 Conclusions In this paper, we have shown that convolutional neural networks with deep architectures greatly outperform linear models even with very little supervision, and they can identify key textual and numerical characteristics of data with respect to predicting a real world phenomenon. In addition, we have demonstrated a way to intuitively interpret the model. In the future, we will investigate ways for automatically optimising the hyperparameters of the network (Snoek et al., 2012) and various extensions to recursive or hierarchical convolutions. transformers2 inglouriousbasterds fastandfurious4 mylifeinruins pinkpanther2 greatbuckhoward informers bobfunk grace objective budget &gt; $15M budget &lt; $15M $200M Top 5 positive phrases sequel $5.2M flick k / product predecessor Top 5 negative phrases $3.6K Revenue $140K Mildly raunchy lang. ( Under 17 Lars von talk the language . their English Selected phrases CGI action summer they’re 1950s hit fi Cage Hong Kong requires acc. parent English Sundance Film Festival written and directed independent some strong language 184 Referen</context>
</contexts>
<marker>Snoek, Larochelle, Adams, 2012</marker>
<rawString>Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical Bayesian optimization of machine learning algorithms. In Advances in Neural Information Processing Systems, pages 2951–2959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="2485" citStr="Socher et al., 2013" startWordPosition="374" endWordPosition="377"> 2014), however this approach has not been shown to scale to high dimensional inputs. features can be captured using explicit feature engineering, this process is tedious, limited in scope (e.g., to conjunctions) and – as we show here – can be dramatically improved by representational learning as part of a non-linear model. In this paper, we propose an artificial neural network (ANN) for modelling text regression. In language processing, ANNs were first proposed for probabilistic language modelling (Bengio et al., 2003), followed by models of sentences (Kalchbrenner et al., 2014) and parsing (Socher et al., 2013) inter alia. These approaches have shown strong results through automatic learning dense low-dimensional distributed representations for words and other linguistic units, which have been shown to encode important aspects of language syntax and semantics. In this paper we develop a convolutional neural network, inspired by their breakthrough results in image processing (Krizhevsky et al., 2012) and recent applications to language processing (Kalchbrenner et al., 2014; Kim, 2014). These works have mainly focused on ‘big data’ problems with plentiful training examples. Given their large numbers o</context>
<context position="15690" citStr="Socher et al., 2013" startWordPosition="2648" endWordPosition="2651">discern with the ANN Text+Meta+Domain model, possibly due to redundancy with the meta data. An alternative approach for multi-task learning is to have a separate convolutional weight matrix for each review site, which can learn site specific characteristics of the text. This can also be achieved with site specific word embedding dimensions. However neither of these methods resulted in performance improvements. In addition, we experimented with applying a hierarchical convolution over reviews in two steps with k-max pooling (Kalchbrenner et al., 2014), as well as parsing sentences recursively (Socher et al., 2013), but did not observe any improvements. For optimisation, both Ada Grad and Steepest Gradient Descent had occasional problems with local minima, which Ada Delta was able to escape more often. In contrast to earlier work (Kim, 2014), applying dropout on the final layer did not improve the validation error. The optimiser mostly found good parameters after around 40 epochs which took around 30 minutes on a NVidia Kepler Tesla K40m GPU. Model interpretation Next we perform analysis to determine which words and phrases influenced the output the most in the ANN Text model. To do so, we set each phra</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1996</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodological),</journal>
<pages>267--288</pages>
<contexts>
<context position="1302" citStr="Tibshirani, 1996" startWordPosition="190" endWordPosition="191">e present a novel technique for interpreting the effect of different text inputs on this complex non-linear model. 1 Introduction Text regression involves predicting a real world phenomenon from textual inputs, and has been shown to be effective in domains including election results (Lampos et al., 2013), financial risk (Kogan et al., 2009) and public health (Lampos and Cristianini, 2010). Almost universally, the text regression problem has been framed as linear regression, with the modelling innovation focussed on effective regression, e.g., using Lasso penalties to promote feature sparsity (Tibshirani, 1996).1 Despite their successes, linear models are limiting: text regression problems will often involve complex interactions between textual inputs, thus requiring a non-linear approach to properly capture such phenomena. For instance, in modelling movie revenue conjunctions of features are likely to be important, e.g., a movie described as ‘scary’ is likely to have different effects for children’s versus adult movies. While these kinds of 1Some preliminary work has shown strong results for nonlinear text regression using Gaussian Process models (Lampos et al., 2014), however this approach has not</context>
</contexts>
<marker>Tibshirani, 1996</marker>
<rawString>Robert Tibshirani. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), pages 267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-SNE.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--2579</pages>
<marker>Van der Maaten, Hinton, 2008</marker>
<rawString>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of Machine Learning Research, 9(2579-2605):85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</title>
<date>2012</date>
<contexts>
<context position="9690" citStr="Zeiler, 2012" startWordPosition="1671" endWordPosition="1672">ral net layers of the form ok = h(g(ak, Wk)) where ak is the input and ok = ak+1 is the output vector for layer k, g is a linear transformation function (ak, Wk), and h is the activation function, i.e. rectified linear seen above. l = 3 hidden layers are applied before the final regression layer to produce the output f = g(ol, wl+1) in part V. The mean absolute error is measured between the predictions f and the targets y, which is more permissible to outliers than the squared error. The cost J is defined as |fv − yv|. The network is trained with stochastic gradient descent and the Ada Delta (Zeiler, 2012) update rule using random restarts. Stochastic gradient descent is noisier than batch training due to a local estimation of the gradient, but it can start converging much faster. Ada Delta keeps an exponentially decaying history of gradients and updates in order to adapt the learning rate for each parameter, which partially smooths out the training noise. Regularisation and hyperparmeter selection are performed by early stopping on the development set. The size of the vocabulary is 90K words. Note that 10% of our lexicon is not found in the embeddings Model Description MAE($M) Baseline mean 11</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D Zeiler. 2012. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>