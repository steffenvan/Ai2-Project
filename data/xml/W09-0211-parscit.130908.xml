<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000442">
<title confidence="0.9996295">
A Non-negative Tensor Factorization Model for
Selectional Preference Induction
</title>
<author confidence="0.995438">
Tim Van de Cruys
</author>
<affiliation confidence="0.8227505">
University of Groningen
The Netherlands
</affiliation>
<email confidence="0.988849">
t.van.de.cruys@rug.nl
</email>
<sectionHeader confidence="0.997241" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998439736842105">
Distributional similarity methods have
proven to be a valuable tool for the in-
duction of semantic similarity. Up till
now, most algorithms use two-way co-
occurrence data to compute the mean-
ing of words. Co-occurrence frequencies,
however, need not be pairwise. One can
easily imagine situations where it is desir-
able to investigate co-occurrence frequen-
cies of three modes and beyond. This pa-
per will investigate a tensor factorization
method called non-negative tensor factor-
ization to build a model of three-way co-
occurrences. The approach is applied to
the problem of selectional preference in-
duction, and automatically evaluated in a
pseudo-disambiguation task. The results
show that non-negative tensor factoriza-
tion is a promising tool for NLP.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9983543">
Distributional similarity methods have proven to
be a valuable tool for the induction of semantic
similarity. The aggregate of a word’s contexts gen-
erally provides enough information to compute its
meaning, viz. its semantic similarity or related-
ness to other words.
Up till now, most algorithms use two-way co-
occurrence data to compute the meaning of words.
A word’s meaning might for example be computed
by looking at:
</bodyText>
<listItem confidence="0.988358166666667">
• the various documents that the word appears
in (words x documents);
• a bag of words context window around the
word (words x context words);
• the dependency relations that the word ap-
pears with (words x dependency relations).
</listItem>
<bodyText confidence="0.999859451612903">
The extracted data – representing the co-
occurrence frequencies of two different entities
– is encoded in a matrix. Co-occurrence fre-
quencies, however, need not be pairwise. One
can easily imagine situations where it is desirable
to investigate co-occurrence frequencies of three
modes and beyond. In an information retrieval
context, one such situation might be the investiga-
tion of words x documents x authors. In an NLP
context, one might want to investigate words x de-
pendency relations x bag of word context words,
or verbs x subjects x direct objects.
Note that it is not possible to investigate the
three-way co-occurrences in a matrix represen-
tation form. It is possible to capture the co-
occurrence frequencies of a verb with its sub-
jects and its direct objects, but one cannot cap-
ture the co-occurrence frequencies of the verb ap-
pearing with the subject and the direct object at
the same time. When the actual three-way co-
occurrence data is ‘matricized’, valuable informa-
tion is thrown-away. To be able to capture the mu-
tual dependencies among the three modes, we will
make use of a generalized tensor representation.
Two-way co-occurrence models (such as la-
tent semantic analysis) have often been augmented
with some form of dimensionality reduction in or-
der to counter noise and overcome data sparseness.
We will also make use of a dimensionality reduc-
tion algorithm appropriate for tensor representa-
tions.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<subsectionHeader confidence="0.8562225">
2.1 Selectional Preferences &amp; Verb
Clustering
</subsectionHeader>
<bodyText confidence="0.9997654">
Selectional preferences have been a popular re-
search subject in the NLP community. One of
the first to automatically induce selectional pref-
erences from corpora was Resnik (1996). Resnik
generalizes among nouns by using WordNet noun
</bodyText>
<note confidence="0.3613575">
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 83–90,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998966">
83
</page>
<bodyText confidence="0.999938294117647">
synsets as clusters. He then calculates the se-
lectional preference strength of a specific verb in
a particular relation by computing the Kullback-
Leibler divergence between the cluster distribu-
tion of the verb and the aggregate cluster distri-
bution. The selectional association is then the
contribution of the cluster to the verb’s prefer-
ence strength. The model’s generalization relies
entirely on WordNet; there is no generalization
among the verbs.
The research in this paper is related to previous
work on clustering. Pereira et al. (1993) use an
information-theoretic based clustering approach,
clustering nouns according to their distribution as
direct objects among verbs. Their model is a one-
sided clustering model: only the direct objects are
clustered, there is no clustering among the verbs.
Rooth et al. (1999) use an EM-based cluster-
ing technique to induce a clustering based on the
co-occurrence frequencies of verbs with their sub-
jects and direct objects. As opposed to the method
of Pereira et al. (1993), their model is two-sided:
the verbs as well as the subjects/direct objects are
clustered. We will use a similar model for evalua-
tion purposes.
Recent approaches using distributional similar-
ity methods for the induction of selectional pref-
erences are the ones by Erk (2007), Bhagat et al.
(2007) and Basili et al. (2007).
This research differs from the approaches men-
tioned above by its use of multi-way data: where
the approaches above limit themselves to two-way
co-occurrences, this research will focus on co-
occurrences for multi-way data.
</bodyText>
<subsectionHeader confidence="0.997421">
2.2 Factorization Algorithms
2.2.1 Two-way Factorizations
</subsectionHeader>
<bodyText confidence="0.999970965517241">
One of the best known factorization algorithms
is principal component analysis (PCA, Pearson
(1901)). PCA transforms the data into a new co-
ordinate system, yielding the best possible fit in a
least square sense given a limited number of di-
mensions. Singular value decomposition (SVD)
is the generalization of the eigenvalue decompo-
sition used in PCA (Wall et al., 2003).
In information retrieval, singular value decom-
position has been applied in latent semantic analy-
sis (LSA, Landauer and Dumais (1997), Landauer
et al. (1998)). In LSA, a term-document matrix
is created, containing the frequency of each word
in a specific document. This matrix is then de-
composed into three other matrices with SVD. The
most important dimensions that come out of the
SVD allegedly represent ‘latent semantic dimen-
sions’, according to which nouns and documents
can be represented more efficiently.
LSA has been criticized for a number of rea-
sons, one of them being the fact that the factor-
ization contains negative numbers. It is not clear
what negativity on a semantic scale should des-
ignate. Subsequent methods such as probabilistic
latent semantic analysis (PLSA, Hofmann (1999))
and non-negative matrix factorization (NMF, Lee
and Seung (2000)) remedy these problems, and
indeed get much more clear-cut semantic dimen-
sions.
</bodyText>
<subsubsectionHeader confidence="0.542403">
2.2.2 Three-way Factorizations
</subsubsectionHeader>
<bodyText confidence="0.999977866666667">
To be able to cope with three-way data, sev-
eral algorithms have been developed as multilin-
ear generalizations of the SVD. In statistics, three-
way component analysis has been extensively in-
vestigated (for an overview, see Kiers and van
Mechelen (2001)). The two most popular methods
are parallel factor analysis (PARAFAC, Harshman
(1970), Carroll and Chang (1970)) and three-mode
principal component analysis (3MPCA, Tucker
(1966)), also called higher order singular value
decomposition (HOSVD, De Lathauwer et al.
(2000)). Three-way factorizations have been ap-
plied in various domains, such as psychometry
and image recognition (Vasilescu and Terzopou-
los, 2002). In information retrieval, three-way fac-
torizations have been applied to the problem of
link analysis (Kolda and Bader, 2006).
One last important method dealing with multi-
way data is non-negative tensor factorization
(NTF, Shashua and Hazan (2005)). NTF is a gener-
alization of non-negative matrix factorization, and
can be considered an extension of the PARAFAC
model with the constraint of non-negativity (cfr.
infra).
One of the few papers that has investigated the
application of tensor factorization for NLP is Tur-
ney (2007), in which a three-mode tensor is used
to compute the semantic similarity of words. The
method achieves 83.75% accuracy on the TOEFL
synonym questions.
</bodyText>
<page confidence="0.996594">
84
</page>
<sectionHeader confidence="0.998463" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.997212">
3.1 Tensors
</subsectionHeader>
<bodyText confidence="0.998741727272727">
Distributional similarity methods usually repre-
sent co-occurrence data in the form of a matrix.
This form is perfectly suited to represent two-way
co-occurrence data, but for co-occurrence data be-
yond two modes, we need a more general repre-
sentation. The generalization of a matrix is called
a tensor. A tensor is able to encode co-occurrence
data of any n modes. Figure 1 shows a graphi-
cal comparison of a matrix and a tensor with three
modes – although a tensor can easily be general-
ized to more than three modes.
</bodyText>
<figureCaption confidence="0.9437845">
Figure 1: Matrix representation vs. tensor repre-
sentation
</figureCaption>
<subsectionHeader confidence="0.999807">
3.2 Non-negative Tensor Factorization
</subsectionHeader>
<bodyText confidence="0.999971352941176">
In order to create a succinct and generalized model
of the extracted data, a statistical dimensional-
ity reduction technique called non-negative tensor
factorization (NTF) is applied to the data. The NTF
model is similar to the PARAFAC analysis – popu-
lar in areas such as psychology and bio-chemistry
– with the constraint that all data needs to be non-
negative (i.e. ≥ 0).
Parallel factor analysis (PARAFAC) is a multi-
linear analogue of the singular value decomposi-
tion (SVD) used in latent semantic analysis. The
key idea is to minimize the sum of squares be-
tween the original tensor and the factorized model
of the tensor. For the three mode case of a tensor
T ∈ RD1×D2×D3 this gives equation 1, where k is
the number of dimensions in the factorized model
and ◦ denotes the outer product.
</bodyText>
<equation confidence="0.64579">
min k T −
xi∈RD1,yi∈RD2,zi∈RD3
</equation>
<bodyText confidence="0.999903">
With non-negative tensor factorization, the non-
negativity constraint is enforced, yielding a model
like the one in equation 2:
</bodyText>
<equation confidence="0.9793024">
min k T −
xi∈RD1
≥0,yi∈RD2
≥0,zi∈RD3
≥0
</equation>
<bodyText confidence="0.999255">
The algorithm results in three matrices, indicat-
ing the loadings of each mode on the factorized
dimensions. The model is represented graphically
in figure 2, visualizing the fact that the PARAFAC
decomposition consists of the summation over the
outer products of n (in this case three) vectors.
</bodyText>
<figureCaption confidence="0.571843">
Figure 2: Graphical representation of the NTF as
the sum of outer products
</figureCaption>
<bodyText confidence="0.999764833333333">
Computationally, the non-negative tensor fac-
torization model is fitted by applying an alternat-
ing least-squares algorithm. In each iteration, two
of the modes are fixed and the third one is fitted
in a least squares sense. This process is repeated
until convergence.1
</bodyText>
<subsectionHeader confidence="0.999386">
3.3 Applied to Language Data
</subsectionHeader>
<bodyText confidence="0.999922238095238">
The model can straightforwardly be applied to lan-
guage data. In this part, we describe the fac-
torization of verbs × subjects × direct objects
co-occurrences, but the example can easily be
substituted with other co-occurrence information.
Moreover, the model need not be restricted to 3
modes; it is very well possible to go to 4 modes
and beyond — as long as the computations remain
feasible.
The NTF decomposition for the verbs × sub-
jects × direct objects co-occurrences into the three
loadings matrices is represented graphically in fig-
ure 3. By applying the NTF model to three-way
(s,v,o) co-occurrences, we want to extract a gen-
eralized selectional preference model, and eventu-
ally even induce some kind of frame semantics (in
the broad sense of the word).
In the resulting factorization, each verb, subject
and direct object gets a loading value for each fac-
tor dimension in the corresponding loadings ma-
trix. The original value for a particular (s,v,o)
</bodyText>
<footnote confidence="0.960588">
1The algorithm has been implemented in MATLAB, using
the Tensor Toolbox for sparse tensor calculations (Bader and
Kolda, 2007).
</footnote>
<equation confidence="0.991687666666667">
k
� xi ◦yi ◦zi k2F (1)
i=1
k
� xi ◦yi ◦ zi k2 F (2)
i=1
</equation>
<page confidence="0.996226">
85
</page>
<figureCaption confidence="0.85998425">
Figure 3: Graphical representation of the NTF for
language data
triple xsvo can then be reconstructed with equa-
tion 3.
</figureCaption>
<equation confidence="0.868132">
ssivviooi (3)
</equation>
<bodyText confidence="0.999915428571429">
To reconstruct the selectional preference value
for the triple (man,bite,dog), for example, we
look up the subject vector for man, the verb vector
for bite and the direct object vector for dog. Then,
for each dimension i in the model, we multiply the
ith value of the three vectors. The sum of these
values is the final preference value.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.968653">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999973210526316">
The approach described in the previous section has
been applied to Dutch, using the Twente Nieuws
Corpus (Ordelman, 2002), a 500M words corpus
of Dutch newspaper texts. The corpus has been
parsed with the Dutch dependency parser Alpino
(van Noord, 2006), and three-way co-occurrences
of verbs with their respective subject and direct
object relations have been extracted. As dimen-
sion sizes, the 1K most frequent verbs were used,
together with the 10K most frequent subjects and
10K most frequent direct objects, yielding a ten-
sor of 1K x 10K x 10K. The resulting tensor is
very sparse, with only 0.0002% of the values be-
ing non-zero.
The tensor has been adapted with a straight-
forward extension of pointwise mutual informa-
tion (Church and Hanks, 1990) for three-way co-
occurrences, following equation 4. Negative val-
ues are set to zero.2
</bodyText>
<footnote confidence="0.4998624">
2This is not just an ad hoc conversion to enforce non-
negativity. Negative values indicate a smaller co-occurrence
probability than the expected number of co-occurrences. Set-
ting those values to zero proves beneficial for similarity cal-
culations (see e.g. Bullinaria and Levy (2007)).
</footnote>
<equation confidence="0.9978945">
MI3(x,y,z) =log p(x,y,z) (4)
p(x)p(y)p(z)
</equation>
<bodyText confidence="0.999940666666667">
The resulting matrix has been factorized into k
dimensions (varying between 50 and 300) with the
NTF algorithm described in section 3.2.
</bodyText>
<subsectionHeader confidence="0.969034">
4.2 Examples
</subsectionHeader>
<bodyText confidence="0.999550482758621">
Table 1, 2 and 3 show example dimensions that
have been found by the algorithm with k = 100.
Each example gives the top 10 subjects, verbs
and direct objects for a particular dimension, to-
gether with the score for that particular dimension.
Table 1 shows the induction of a ‘police action’
frame, with police authorities as subjects, police
actions as verbs and patients of the police actions
as direct objects.
In table 2, a legislation dimension is induced,
with legislative bodies as subjects3, legislative ac-
tions as verbs, and mostly law (proposals) as direct
objects. Note that some direct objects (e.g. ‘min-
ister’) also designate persons that can be the object
of a legislative act.
Table 3, finally, is clearly an exhibition dimen-
sion, with verbs describing actions of display and
trade that art institutions (subjects) can do with
works of art (objects).
These are not the only sensible dimensions that
have been found by the algorithm. A quick qual-
itative evaluation indicates that about 44 dimen-
sions contain similar, framelike semantics. In an-
other 43 dimensions, the semantics are less clear-
cut (single verbs account for one dimension, or
different senses of a verb get mixed up). 13 dimen-
sions are not so much based on semantic character-
istics, but rather on syntax (e.g. fixed expressions
and pronomina).
</bodyText>
<subsectionHeader confidence="0.996635">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999923454545455">
The results of the NTF model have been quantita-
tively evaluated in a pseudo-disambiguation task,
similar to the one used by Rooth et al. (1999). It is
used to evaluate the generalization capabilities of
the algorithm. The task is to judge which subject
(s or s&apos;) and direct object (o or o&apos;) is more likely
for a particular verb v, where (s,v,o) is a combi-
nation drawn from the corpus, and s&apos; and o&apos; are a
subject and direct object randomly drawn from the
corpus. A triple is considered correct if the algo-
rithm prefers both s and o over their counterparts
</bodyText>
<footnote confidence="0.654161">
3Note that VVD, D66, PvdA and CDA are Dutch political
parties.
</footnote>
<equation confidence="0.9953235">
k
�
i=1
xsvo =
</equation>
<page confidence="0.926592">
86
</page>
<bodyText confidence="0.971157454545455">
subjects sus verbs vs objects ob js
politie ‘police’ .99 houd aan ‘arrest’ .64 verdachte ‘suspect’ .16
agent ‘policeman’ .07 arresteer ‘arrest’ .63 man ‘man’ .16
autoriteit ‘authority’ .05 pak op ‘run in’ .41 betoger ‘demonstrator’ .14
Justitie ’Justice’ .05 schiet dood ‘shoot’ .08 relschopper ‘rioter’ .13
recherche ‘detective force’ .04 verdenk ‘suspect’ .07 raddraaiers ‘instigator’ .13
marechaussee ‘military police’ .04 tref aan ‘find’ .06 overvaller ‘raider’ .13
justitie ‘justice’ .04 achterhaal ‘overtake’ .05 Roemeen ‘Romanian’ .13
arrestatieteam ‘special squad’ .03 verwijder ‘remove’ .05 actievoerder ‘campaigner’ .13
leger ‘army’ .03 zoek ‘search’ .04 hooligan ‘hooligan’ .13
douane ‘customs’ .02 spoor op ‘track’ .03 Algerijn ‘Algerian’ .13
</bodyText>
<tableCaption confidence="0.999686">
Table 1: Top 10 subjects, verbs and direct objects for the ‘police action’ dimension
</tableCaption>
<table confidence="0.986135045454546">
subjects sus
meerderheid ‘majority’ .33
VVD .28
D66 .25
Kamermeerderheid ‘Chamber majority’ .25
fractie ‘party’ .24
PvdA .23
CDA .23
Tweede Kamer ‘Second Chamber’ .21
partij ‘party’ .20
Kamer ‘Chamber’ .20
verbs vs objects objs
steun ‘support’ .83 motie ‘motion’ .63
dien in ‘submit’ .44 voorstel ‘proposal’ .53
neem aan ‘pass’ .23 plan ‘plan’ .28
wijs af ‘reject’ .17 wetsvoorstel ‘bill’ .19
verwerp ‘reject’ .14 hem ‘him’ .18
vind ‘think’ .08 kabinet ‘cabinet’ .16
aanvaard ‘accepts’ .05 minister ‘minister’ .16
behandel ‘treat’ .05 beleid ‘policy’ .13
doe ‘do’ .04 kandidatuur ‘candidature’ .11
keur goed ‘pass’ .03 amendement ‘amendment’ .09
</table>
<tableCaption confidence="0.997642">
Table 2: Top 10 subjects, verbs and direct objects for the ‘legislation’ dimension
</tableCaption>
<bodyText confidence="0.8962295">
s0 and o0 (so the (s,v,o) triple – that appears in the
test corpus – is preferred over the triples (s0,v,o0),
(s0,v,o) and (s,v,o0)). Table 4 shows three exam-
ples from the pseudo-disambiguation task.
</bodyText>
<table confidence="0.920077142857143">
s v o s0 o0
jongere drink bier coalitie aandeel
‘youngster’ ‘drink’ ‘beer’ ‘coalition’ ‘share’
werkgever riskeer boete doel kopzorg
‘employer’ ‘risk’ ‘fine’ ‘goal’ ‘worry’
directeur zwaai scepter informateur vodka
‘manager’ ‘sway’ ‘sceptre’ ‘informer’ ‘wodka’
</table>
<tableCaption confidence="0.919363">
Table 4: Three examples from the pseudo-
</tableCaption>
<bodyText confidence="0.986074483870968">
disambiguation evaluation task’s test set
Four different models have been evaluated. The
first two models are tensor factorization models.
The first model is the NTF model, as described
in section 3.2. The second model is the original
PARAFAC model, without the non-negativity con-
straints.
The other two models are matrix factorization
models. The third model is the non-negative ma-
trix factorization (NMF) model, and the fourth
model is the singular value decomposition (SVD).
For these models, a matrix has been constructed
that contains the pairwise co-occurrence frequen-
cies of verbs by subjects as well as direct objects.
This gives a matrix of 1K verbs by 10K subjects
+ 10K direct objects (1K × 20K). The matrix has
been adapted with pointwise mutual information.
The models have been evaluated with 10-fold
cross-validation. The corpus contains 298,540 dif-
ferent (s,v,o) co-occurrences. Those have been
randomly divided into 10 equal parts. So in each
fold, 268,686 co-occurrences have been used for
training, and 29,854 have been used for testing.
The accuracy results of the evaluation are given in
table 5.
The results clearly indicate that the NTF model
outperforms all the other models. The model
achieves the best result with 300 dimensions, but
the differences between the different NTF models
are not very large – all attaining scores around
90%.
</bodyText>
<page confidence="0.99753">
87
</page>
<bodyText confidence="0.825422818181818">
subjects sus verbs vs objects ob js
tentoonstelling ‘exhibition’ .50 toon ‘display’ .72 schilderij ‘painting’ .47
expositie ‘exposition’ .49 omvat ‘cover’ .63 werk ‘work’ .46
galerie ‘gallery’ .36 bevat ‘contain’ .18 tekening ‘drawing’ .36
collectie ‘collection’ .29 presenteer ‘present’ .17 foto ‘picture’ .33
museum ‘museum’ .27 laat ‘let’ .07 sculptuur ‘sculpture’ .25
oeuvre ‘oeuvre’ .22 koop ‘buy’ .07 aquarel ‘aquarelle’ .20
Kunsthal .19 bezit ‘own’ .06 object ‘object’ .19
kunstenaar ‘artist’ .15 zie ‘see’ .05 beeld ‘statue’ .12
dat ‘that’ .12 koop aan ‘acquire’ .05 overzicht ‘overview’ .12
hij ‘he’ .10 in huis heb ‘own’ .04 portret ‘portrait’ .11
</bodyText>
<tableCaption confidence="0.9989">
Table 3: Top 10 subjects, verbs and direct objects for the ‘exhibition’ dimension
</tableCaption>
<table confidence="0.999543333333333">
dimensions
50 (%) 100 (%) 300 (%)
NTF 89.52 f 0.18 90.43 f 0.14 90.89 f 0.16
PARAFAC 85.57 f 0.25 83.58 f 0.59 80.12 f 0.76
NMF 81.79 f 0.15 78.83 f 0.40 75.74 f 0.63
SVD 69.60 f 0.41 62.84 f 1.30 45.22 f 1.01
</table>
<tableCaption confidence="0.991723">
Table 5: Results of the 10-fold cross-validation for
</tableCaption>
<bodyText confidence="0.983217">
the NTF, PARAFAC, NMF and SVD model for 50,
100 and 300 dimensions (averages and standard
deviation)
The PARAFAC results indicate the fitness of ten-
sor factorization for the induction of three-way se-
lectional preferences. Even without the constraint
of non-negativity, the model outperforms the ma-
trix factorization models, reaching a score of about
85%. The model deteriorates when more dimen-
sions are used.
Both matrix factorization models perform
worse than their tensor factorization counterparts.
The NMF still scores reasonably well, indicating
the positive effect of the non-negativity constraint.
The simple SVD model performs worst, reaching a
score of about 70% with 50 dimensions.
</bodyText>
<sectionHeader confidence="0.986992" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990744680851">
This paper has presented a novel method that
is able to investigate three-way co-occurrences.
Other distributional methods deal almost exclu-
sively with pairwise co-occurrences. The ability
to keep track of multi-way co-occurrences opens
up new possibilities and brings about interesting
results. The method uses a factorization model –
non-negative tensor factorization – that is suitable
for three way data. The model is able to generalize
among the data and overcome data sparseness.
The method has been applied to the problem
of selectional preference induction. The results
indicate that the algorithm is able to induce se-
lectional preferences, leading to a broad kind
of frame semantics. The quantitative evaluation
shows that use of three-way data is clearly benefi-
cial for the induction of three-way selectional pref-
erences. The tensor models outperform the sim-
ple matrix models in the pseudo-disambiguation
task. The results also indicate the positive ef-
fect of the non-negativity constraint: both mod-
els with non-negative constraints outperform their
non-constrained counterparts.
The results as well as the evaluation indicate
that the method presented here is a promising tool
for the investigation of NLP topics, although more
research and thorough evaluation are desirable.
There is quite some room for future work. First
of all, we want to further investigate the useful-
ness of the method for selectional preference in-
duction. This includes a deeper quantitative eval-
uation and a comparison to other methods for se-
lectional preference induction. We also want to
include other dependency relations in our model,
apart from subjects and direct objects.
Secondly, there is room for improvement and
further research with regard to the tensor factor-
ization model. The model presented here min-
imizes the sum of squared distance. This is,
however, not the only objective function possi-
ble. Another possibility is the minimization of the
Kullback-Leibler divergence. Minimizing the sum
of squared distance assumes normally distributed
data, and language phenomena are rarely normally
distributed. Other objective functions – such as the
minimization of the Kullback-Leibler divergence
– might be able to capture the language structures
</bodyText>
<page confidence="0.996113">
88
</page>
<bodyText confidence="0.9997808">
much more adequately. We specifically want to
stress this second line of future research as one of
the most promising and exciting ones.
Finally, the model presented here is not
only suitable for selectional preference induction.
There are many problems in NLP that involve
three-way co-occurrences. In future work, we
want to apply the NTF model presented here to
other problems in NLP, the most important one be-
ing word sense discrimination.
</bodyText>
<sectionHeader confidence="0.996401" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999926333333333">
Brett Bader kindly provided his implementation of
non-negative tensor factorization for sparse ma-
trices, from which this research has substantially
benefited. The three anonymous reviewers pro-
vided fruitful comments and remarks, which con-
siderably improved the quality of this paper.
</bodyText>
<sectionHeader confidence="0.999255" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993815698795181">
Brett W. Bader and Tamara G. Kolda. 2006. Efficient
MATLAB computations with sparse and factored
tensors. Technical Report SAND2006-7592, Sandia
National Laboratories, Albuquerque, NM and Liver-
more, CA, December.
Brett W. Bader and Tamara G. Kolda. 2007. Mat-
lab tensor toolbox version 2.2. http://csmr.ca.
sandia.gov/∼tgkolda/TensorToolbox/, Jan-
uary.
Roberto Basili, Diego De Cao, Paolo Marocco, and
Marco Pennacchiotti. 2007. Learning selectional
preferences for entailment or paraphrasing rules. In
Proceedings of RANLP 2007, Borovets, Bulgaria.
Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007.
Ledir: An unsupervised algorithm for learning di-
rectionality of inference rules. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-07), pages 161–170,
Prague, Czech Republic.
John A. Bullinaria and Joseph P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39:510–526.
J. D. Carroll and J.-J. Chang. 1970. Analysis of in-
dividual differences in multidimensional scaling via
an n-way generalization of ”eckart-young” decom-
position. Psychometrika, 35:283–319.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information &amp; lexicogra-
phy. Computational Linguistics, 16(1):22–29.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM Journal on Matrix Analysis and Appli-
cations, 21(4):1253–1278.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of ACL
2007, Prague, Czech Republic.
R.A. Harshman. 1970. Foundations of the parafac pro-
cedure: models and conditions for an ”explanatory”
multi-mode factor analysis. In UCLA Working Pa-
pers in Phonetics, volume 16, pages 1–84, Los An-
geles. University of California.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI’99, Stockholm.
H.A.L Kiers and I. van Mechelen. 2001. Three-way
component analysis: Principles and illustrative ap-
plication. Psychological Methods, 6:84–110.
Tamara Kolda and Brett Bader. 2006. The TOPHITS
model for higher-order web link analysis. In Work-
shop on Link Analysis, Counterterrorism and Secu-
rity.
Thomas Landauer and Susan Dumais. 1997. A so-
lution to Plato’s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and
representation of knowledge. Psychology Review,
104:211–240.
Thomas Landauer, Peter Foltz, and Darrell Laham.
1998. An Introduction to Latent Semantic Analysis.
Discourse Processes, 25:295–284.
Daniel D. Lee and H. Sebastian Seung. 2000. Al-
gorithms for non-negative matrix factorization. In
NIPS, pages 556–562.
R.J.F. Ordelman. 2002. Twente Nieuws Corpus
(TwNC), August. Parlevink Language Technology
Group. University of Twente.
K. Pearson. 1901. On lines and planes of closest fit to
systems of points in space. Philosophical Magazine,
2(6):559–572.
Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words. In
31st Annual Meeting of the ACL, pages 183–190.
Philip Resnik. 1996. Selectional Constraints: An
Information-Theoretic Model and its Computational
Realization. Cognition, 61:127–159, November.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn
Carroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via em-based clustering. In
37th Annual Meeting of the ACL.
Amnon Shashua and Tamir Hazan. 2005. Non-
negative tensor factorization with applications to
statistics and computer vision. In ICML ’05: Pro-
ceedings of the 22nd international conference on
</reference>
<page confidence="0.994743">
89
</page>
<reference confidence="0.999368285714286">
Machine learning, pages 792–799, New York, NY,
USA. ACM.
L.R. Tucker. 1966. Some mathematical notes on three-
mode factor analysis. Psychometrika, 31:279–311.
Peter D. Turney. 2007. Empirical evaluation of four
tensor decomposition algorithms. Technical Report
ERB-1152, National Research Council, Institute for
Information Technology.
Gertjan van Noord. 2006. At Last Parsing Is Now
Operational. In Piet Mertens, Cedrick Fairon, Anne
Dister, and Patrick Watrin, editors, TALN06. Verbum
Ex Machina. Actes de la 13e conference sur le traite-
ment automatique des langues naturelles, pages 20–
42, Leuven.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In ECCV, pages 447–460.
Michael E. Wall, Andreas Rechtsteiner, and Luis M.
Rocha, 2003. Singular Value Decomposition and
Principal Component Analysis, chapter 5, pages 91–
109. Hluwel, Norwell, MA, Mar.
</reference>
<page confidence="0.99864">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.336339">
<title confidence="0.9992395">A Non-negative Tensor Factorization Model Selectional Preference Induction</title>
<author confidence="0.999989">Tim Van_de</author>
<affiliation confidence="0.805682">University of The</affiliation>
<email confidence="0.948509">t.van.de.cruys@rug.nl</email>
<abstract confidence="0.999898736842105">Distributional similarity methods have proven to be a valuable tool for the induction of semantic similarity. Up till now, most algorithms use two-way cooccurrence data to compute the meaning of words. Co-occurrence frequencies, however, need not be pairwise. One can easily imagine situations where it is desirable to investigate co-occurrence frequencies of three modes and beyond. This paper will investigate a tensor factorization method called non-negative tensor factorization to build a model of three-way cooccurrences. The approach is applied to the problem of selectional preference induction, and automatically evaluated in a pseudo-disambiguation task. The results show that non-negative tensor factoriza-</abstract>
<intro confidence="0.553704">is a promising tool for</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Efficient MATLAB computations with sparse and factored tensors.</title>
<date>2006</date>
<tech>Technical Report SAND2006-7592,</tech>
<institution>Sandia National Laboratories,</institution>
<location>Albuquerque, NM and Livermore, CA,</location>
<marker>Bader, Kolda, 2006</marker>
<rawString>Brett W. Bader and Tamara G. Kolda. 2006. Efficient MATLAB computations with sparse and factored tensors. Technical Report SAND2006-7592, Sandia National Laboratories, Albuquerque, NM and Livermore, CA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett W Bader</author>
<author>Tamara G Kolda</author>
</authors>
<title>Matlab tensor toolbox version 2.2.</title>
<date>2007</date>
<note>http://csmr.ca. sandia.gov/∼tgkolda/TensorToolbox/,</note>
<contexts>
<context position="11174" citStr="Bader and Kolda, 2007" startWordPosition="1779" endWordPosition="1782">urrences into the three loadings matrices is represented graphically in figure 3. By applying the NTF model to three-way (s,v,o) co-occurrences, we want to extract a generalized selectional preference model, and eventually even induce some kind of frame semantics (in the broad sense of the word). In the resulting factorization, each verb, subject and direct object gets a loading value for each factor dimension in the corresponding loadings matrix. The original value for a particular (s,v,o) 1The algorithm has been implemented in MATLAB, using the Tensor Toolbox for sparse tensor calculations (Bader and Kolda, 2007). k � xi ◦yi ◦zi k2F (1) i=1 k � xi ◦yi ◦ zi k2 F (2) i=1 85 Figure 3: Graphical representation of the NTF for language data triple xsvo can then be reconstructed with equation 3. ssivviooi (3) To reconstruct the selectional preference value for the triple (man,bite,dog), for example, we look up the subject vector for man, the verb vector for bite and the direct object vector for dog. Then, for each dimension i in the model, we multiply the ith value of the three vectors. The sum of these values is the final preference value. 4 Results 4.1 Setup The approach described in the previous section h</context>
</contexts>
<marker>Bader, Kolda, 2007</marker>
<rawString>Brett W. Bader and Tamara G. Kolda. 2007. Matlab tensor toolbox version 2.2. http://csmr.ca. sandia.gov/∼tgkolda/TensorToolbox/, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Diego De Cao</author>
<author>Paolo Marocco</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Learning selectional preferences for entailment or paraphrasing rules.</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP</booktitle>
<location>Borovets, Bulgaria.</location>
<marker>Basili, De Cao, Marocco, Pennacchiotti, 2007</marker>
<rawString>Roberto Basili, Diego De Cao, Paolo Marocco, and Marco Pennacchiotti. 2007. Learning selectional preferences for entailment or paraphrasing rules. In Proceedings of RANLP 2007, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-07),</booktitle>
<pages>161--170</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4801" citStr="Bhagat et al. (2007)" startWordPosition="756" endWordPosition="759"> onesided clustering model: only the direct objects are clustered, there is no clustering among the verbs. Rooth et al. (1999) use an EM-based clustering technique to induce a clustering based on the co-occurrence frequencies of verbs with their subjects and direct objects. As opposed to the method of Pereira et al. (1993), their model is two-sided: the verbs as well as the subjects/direct objects are clustered. We will use a similar model for evaluation purposes. Recent approaches using distributional similarity methods for the induction of selectional preferences are the ones by Erk (2007), Bhagat et al. (2007) and Basili et al. (2007). This research differs from the approaches mentioned above by its use of multi-way data: where the approaches above limit themselves to two-way co-occurrences, this research will focus on cooccurrences for multi-way data. 2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901)). PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions. Singular value decomposition (SVD) is the gen</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, and Eduard Hovy. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-07), pages 161–170, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bullinaria</author>
<author>Joseph P Levy</author>
</authors>
<title>Extracting semantic representations from word cooccurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="12845" citStr="Bullinaria and Levy (2007)" startWordPosition="2065" endWordPosition="2068">ent direct objects, yielding a tensor of 1K x 10K x 10K. The resulting tensor is very sparse, with only 0.0002% of the values being non-zero. The tensor has been adapted with a straightforward extension of pointwise mutual information (Church and Hanks, 1990) for three-way cooccurrences, following equation 4. Negative values are set to zero.2 2This is not just an ad hoc conversion to enforce nonnegativity. Negative values indicate a smaller co-occurrence probability than the expected number of co-occurrences. Setting those values to zero proves beneficial for similarity calculations (see e.g. Bullinaria and Levy (2007)). MI3(x,y,z) =log p(x,y,z) (4) p(x)p(y)p(z) The resulting matrix has been factorized into k dimensions (varying between 50 and 300) with the NTF algorithm described in section 3.2. 4.2 Examples Table 1, 2 and 3 show example dimensions that have been found by the algorithm with k = 100. Each example gives the top 10 subjects, verbs and direct objects for a particular dimension, together with the score for that particular dimension. Table 1 shows the induction of a ‘police action’ frame, with police authorities as subjects, police actions as verbs and patients of the police actions as direct ob</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John A. Bullinaria and Joseph P. Levy. 2007. Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Carroll</author>
<author>J-J Chang</author>
</authors>
<title>Analysis of individual differences in multidimensional scaling via an n-way generalization of ”eckart-young” decomposition.</title>
<date>1970</date>
<tech>Psychometrika,</tech>
<pages>35--283</pages>
<contexts>
<context position="6813" citStr="Carroll and Chang (1970)" startWordPosition="1069" endWordPosition="1072">sequent methods such as probabilistic latent semantic analysis (PLSA, Hofmann (1999)) and non-negative matrix factorization (NMF, Lee and Seung (2000)) remedy these problems, and indeed get much more clear-cut semantic dimensions. 2.2.2 Three-way Factorizations To be able to cope with three-way data, several algorithms have been developed as multilinear generalizations of the SVD. In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization of non-negative matrix fac</context>
</contexts>
<marker>Carroll, Chang, 1970</marker>
<rawString>J. D. Carroll and J.-J. Chang. 1970. Analysis of individual differences in multidimensional scaling via an n-way generalization of ”eckart-young” decomposition. Psychometrika, 35:283–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information &amp; lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="12478" citStr="Church and Hanks, 1990" startWordPosition="2008" endWordPosition="2011"> words corpus of Dutch newspaper texts. The corpus has been parsed with the Dutch dependency parser Alpino (van Noord, 2006), and three-way co-occurrences of verbs with their respective subject and direct object relations have been extracted. As dimension sizes, the 1K most frequent verbs were used, together with the 10K most frequent subjects and 10K most frequent direct objects, yielding a tensor of 1K x 10K x 10K. The resulting tensor is very sparse, with only 0.0002% of the values being non-zero. The tensor has been adapted with a straightforward extension of pointwise mutual information (Church and Hanks, 1990) for three-way cooccurrences, following equation 4. Negative values are set to zero.2 2This is not just an ad hoc conversion to enforce nonnegativity. Negative values indicate a smaller co-occurrence probability than the expected number of co-occurrences. Setting those values to zero proves beneficial for similarity calculations (see e.g. Bullinaria and Levy (2007)). MI3(x,y,z) =log p(x,y,z) (4) p(x)p(y)p(z) The resulting matrix has been factorized into k dimensions (varying between 50 and 300) with the NTF algorithm described in section 3.2. 4.2 Examples Table 1, 2 and 3 show example dimensio</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information &amp; lexicography. Computational Linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lieven De Lathauwer</author>
<author>Bart De Moor</author>
<author>Joos Vandewalle</author>
</authors>
<title>A multilinear singular value decomposition.</title>
<date>2000</date>
<journal>SIAM Journal on Matrix Analysis and Applications,</journal>
<volume>21</volume>
<issue>4</issue>
<marker>De Lathauwer, De Moor, Vandewalle, 2000</marker>
<rawString>Lieven De Lathauwer, Bart De Moor, and Joos Vandewalle. 2000. A multilinear singular value decomposition. SIAM Journal on Matrix Analysis and Applications, 21(4):1253–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>A simple, similarity-based model for selectional preferences.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4779" citStr="Erk (2007)" startWordPosition="754" endWordPosition="755">r model is a onesided clustering model: only the direct objects are clustered, there is no clustering among the verbs. Rooth et al. (1999) use an EM-based clustering technique to induce a clustering based on the co-occurrence frequencies of verbs with their subjects and direct objects. As opposed to the method of Pereira et al. (1993), their model is two-sided: the verbs as well as the subjects/direct objects are clustered. We will use a similar model for evaluation purposes. Recent approaches using distributional similarity methods for the induction of selectional preferences are the ones by Erk (2007), Bhagat et al. (2007) and Basili et al. (2007). This research differs from the approaches mentioned above by its use of multi-way data: where the approaches above limit themselves to two-way co-occurrences, this research will focus on cooccurrences for multi-way data. 2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901)). PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions. Singular value decompos</context>
</contexts>
<marker>Erk, 2007</marker>
<rawString>Katrin Erk. 2007. A simple, similarity-based model for selectional preferences. In Proceedings of ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Harshman</author>
</authors>
<title>Foundations of the parafac procedure: models and conditions for an ”explanatory” multi-mode factor analysis.</title>
<date>1970</date>
<booktitle>In UCLA Working Papers in Phonetics,</booktitle>
<volume>16</volume>
<pages>1--84</pages>
<institution>Los Angeles. University of California.</institution>
<contexts>
<context position="6787" citStr="Harshman (1970)" startWordPosition="1067" endWordPosition="1068">ld designate. Subsequent methods such as probabilistic latent semantic analysis (PLSA, Hofmann (1999)) and non-negative matrix factorization (NMF, Lee and Seung (2000)) remedy these problems, and indeed get much more clear-cut semantic dimensions. 2.2.2 Three-way Factorizations To be able to cope with three-way data, several algorithms have been developed as multilinear generalizations of the SVD. In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization </context>
</contexts>
<marker>Harshman, 1970</marker>
<rawString>R.A. Harshman. 1970. Foundations of the parafac procedure: models and conditions for an ”explanatory” multi-mode factor analysis. In UCLA Working Papers in Phonetics, volume 16, pages 1–84, Los Angeles. University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proc. of Uncertainty in Artificial Intelligence, UAI’99,</booktitle>
<location>Stockholm.</location>
<contexts>
<context position="6273" citStr="Hofmann (1999)" startWordPosition="990" endWordPosition="991">t matrix is created, containing the frequency of each word in a specific document. This matrix is then decomposed into three other matrices with SVD. The most important dimensions that come out of the SVD allegedly represent ‘latent semantic dimensions’, according to which nouns and documents can be represented more efficiently. LSA has been criticized for a number of reasons, one of them being the fact that the factorization contains negative numbers. It is not clear what negativity on a semantic scale should designate. Subsequent methods such as probabilistic latent semantic analysis (PLSA, Hofmann (1999)) and non-negative matrix factorization (NMF, Lee and Seung (2000)) remedy these problems, and indeed get much more clear-cut semantic dimensions. 2.2.2 Three-way Factorizations To be able to cope with three-way data, several algorithms have been developed as multilinear generalizations of the SVD. In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proc. of Uncertainty in Artificial Intelligence, UAI’99, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H A L Kiers</author>
<author>I van Mechelen</author>
</authors>
<title>Three-way component analysis: Principles and illustrative application. Psychological Methods,</title>
<date>2001</date>
<pages>6--84</pages>
<marker>Kiers, van Mechelen, 2001</marker>
<rawString>H.A.L Kiers and I. van Mechelen. 2001. Three-way component analysis: Principles and illustrative application. Psychological Methods, 6:84–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara Kolda</author>
<author>Brett Bader</author>
</authors>
<title>The TOPHITS model for higher-order web link analysis.</title>
<date>2006</date>
<booktitle>In Workshop on Link Analysis, Counterterrorism and Security.</booktitle>
<contexts>
<context position="7238" citStr="Kolda and Bader, 2006" startWordPosition="1129" endWordPosition="1132">has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization of non-negative matrix factorization, and can be considered an extension of the PARAFAC model with the constraint of non-negativity (cfr. infra). One of the few papers that has investigated the application of tensor factorization for NLP is Turney (2007), in which a three-mode tensor is used to compute the semantic similarity of words. The method achieves 83.75% accuracy on the TOEFL synonym questions. 84 3 Methodology 3.1 Tensors Distributional s</context>
</contexts>
<marker>Kolda, Bader, 2006</marker>
<rawString>Tamara Kolda and Brett Bader. 2006. The TOPHITS model for higher-order web link analysis. In Workshop on Link Analysis, Counterterrorism and Security.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychology Review,</journal>
<pages>104--211</pages>
<contexts>
<context position="5610" citStr="Landauer and Dumais (1997)" startWordPosition="881" endWordPosition="884">s, this research will focus on cooccurrences for multi-way data. 2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901)). PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions. Singular value decomposition (SVD) is the generalization of the eigenvalue decomposition used in PCA (Wall et al., 2003). In information retrieval, singular value decomposition has been applied in latent semantic analysis (LSA, Landauer and Dumais (1997), Landauer et al. (1998)). In LSA, a term-document matrix is created, containing the frequency of each word in a specific document. This matrix is then decomposed into three other matrices with SVD. The most important dimensions that come out of the SVD allegedly represent ‘latent semantic dimensions’, according to which nouns and documents can be represented more efficiently. LSA has been criticized for a number of reasons, one of them being the fact that the factorization contains negative numbers. It is not clear what negativity on a semantic scale should designate. Subsequent methods such </context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychology Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Peter Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--295</pages>
<contexts>
<context position="5634" citStr="Landauer et al. (1998)" startWordPosition="885" endWordPosition="888">on cooccurrences for multi-way data. 2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901)). PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions. Singular value decomposition (SVD) is the generalization of the eigenvalue decomposition used in PCA (Wall et al., 2003). In information retrieval, singular value decomposition has been applied in latent semantic analysis (LSA, Landauer and Dumais (1997), Landauer et al. (1998)). In LSA, a term-document matrix is created, containing the frequency of each word in a specific document. This matrix is then decomposed into three other matrices with SVD. The most important dimensions that come out of the SVD allegedly represent ‘latent semantic dimensions’, according to which nouns and documents can be represented more efficiently. LSA has been criticized for a number of reasons, one of them being the fact that the factorization contains negative numbers. It is not clear what negativity on a semantic scale should designate. Subsequent methods such as probabilistic latent </context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas Landauer, Peter Foltz, and Darrell Laham. 1998. An Introduction to Latent Semantic Analysis. Discourse Processes, 25:295–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Lee</author>
<author>H Sebastian Seung</author>
</authors>
<title>Algorithms for non-negative matrix factorization.</title>
<date>2000</date>
<booktitle>In NIPS,</booktitle>
<pages>556--562</pages>
<contexts>
<context position="6339" citStr="Lee and Seung (2000)" startWordPosition="997" endWordPosition="1000">n a specific document. This matrix is then decomposed into three other matrices with SVD. The most important dimensions that come out of the SVD allegedly represent ‘latent semantic dimensions’, according to which nouns and documents can be represented more efficiently. LSA has been criticized for a number of reasons, one of them being the fact that the factorization contains negative numbers. It is not clear what negativity on a semantic scale should designate. Subsequent methods such as probabilistic latent semantic analysis (PLSA, Hofmann (1999)) and non-negative matrix factorization (NMF, Lee and Seung (2000)) remedy these problems, and indeed get much more clear-cut semantic dimensions. 2.2.2 Three-way Factorizations To be able to cope with three-way data, several algorithms have been developed as multilinear generalizations of the SVD. In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (H</context>
</contexts>
<marker>Lee, Seung, 2000</marker>
<rawString>Daniel D. Lee and H. Sebastian Seung. 2000. Algorithms for non-negative matrix factorization. In NIPS, pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J F Ordelman</author>
</authors>
<date>2002</date>
<institution>Twente Nieuws Corpus (TwNC), August. Parlevink Language Technology Group. University of Twente.</institution>
<contexts>
<context position="11847" citStr="Ordelman, 2002" startWordPosition="1904" endWordPosition="1905">85 Figure 3: Graphical representation of the NTF for language data triple xsvo can then be reconstructed with equation 3. ssivviooi (3) To reconstruct the selectional preference value for the triple (man,bite,dog), for example, we look up the subject vector for man, the verb vector for bite and the direct object vector for dog. Then, for each dimension i in the model, we multiply the ith value of the three vectors. The sum of these values is the final preference value. 4 Results 4.1 Setup The approach described in the previous section has been applied to Dutch, using the Twente Nieuws Corpus (Ordelman, 2002), a 500M words corpus of Dutch newspaper texts. The corpus has been parsed with the Dutch dependency parser Alpino (van Noord, 2006), and three-way co-occurrences of verbs with their respective subject and direct object relations have been extracted. As dimension sizes, the 1K most frequent verbs were used, together with the 10K most frequent subjects and 10K most frequent direct objects, yielding a tensor of 1K x 10K x 10K. The resulting tensor is very sparse, with only 0.0002% of the values being non-zero. The tensor has been adapted with a straightforward extension of pointwise mutual infor</context>
</contexts>
<marker>Ordelman, 2002</marker>
<rawString>R.J.F. Ordelman. 2002. Twente Nieuws Corpus (TwNC), August. Parlevink Language Technology Group. University of Twente.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Pearson</author>
</authors>
<title>On lines and planes of closest fit to systems of points in space.</title>
<date>1901</date>
<journal>Philosophical Magazine,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="5206" citStr="Pearson (1901)" startWordPosition="817" endWordPosition="818">red. We will use a similar model for evaluation purposes. Recent approaches using distributional similarity methods for the induction of selectional preferences are the ones by Erk (2007), Bhagat et al. (2007) and Basili et al. (2007). This research differs from the approaches mentioned above by its use of multi-way data: where the approaches above limit themselves to two-way co-occurrences, this research will focus on cooccurrences for multi-way data. 2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901)). PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions. Singular value decomposition (SVD) is the generalization of the eigenvalue decomposition used in PCA (Wall et al., 2003). In information retrieval, singular value decomposition has been applied in latent semantic analysis (LSA, Landauer and Dumais (1997), Landauer et al. (1998)). In LSA, a term-document matrix is created, containing the frequency of each word in a specific document. This matrix is then decomposed into three other matrices with SV</context>
</contexts>
<marker>Pearson, 1901</marker>
<rawString>K. Pearson. 1901. On lines and planes of closest fit to systems of points in space. Philosophical Magazine, 2(6):559–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In 31st Annual Meeting of the ACL,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="4028" citStr="Pereira et al. (1993)" startWordPosition="632" endWordPosition="635">ns, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 83 synsets as clusters. He then calculates the selectional preference strength of a specific verb in a particular relation by computing the KullbackLeibler divergence between the cluster distribution of the verb and the aggregate cluster distribution. The selectional association is then the contribution of the cluster to the verb’s preference strength. The model’s generalization relies entirely on WordNet; there is no generalization among the verbs. The research in this paper is related to previous work on clustering. Pereira et al. (1993) use an information-theoretic based clustering approach, clustering nouns according to their distribution as direct objects among verbs. Their model is a onesided clustering model: only the direct objects are clustered, there is no clustering among the verbs. Rooth et al. (1999) use an EM-based clustering technique to induce a clustering based on the co-occurrence frequencies of verbs with their subjects and direct objects. As opposed to the method of Pereira et al. (1993), their model is two-sided: the verbs as well as the subjects/direct objects are clustered. We will use a similar model for</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In 31st Annual Meeting of the ACL, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional Constraints: An Information-Theoretic Model and its Computational Realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--127</pages>
<contexts>
<context position="3239" citStr="Resnik (1996)" startWordPosition="514" endWordPosition="515">s among the three modes, we will make use of a generalized tensor representation. Two-way co-occurrence models (such as latent semantic analysis) have often been augmented with some form of dimensionality reduction in order to counter noise and overcome data sparseness. We will also make use of a dimensionality reduction algorithm appropriate for tensor representations. 2 Previous Work 2.1 Selectional Preferences &amp; Verb Clustering Selectional preferences have been a popular research subject in the NLP community. One of the first to automatically induce selectional preferences from corpora was Resnik (1996). Resnik generalizes among nouns by using WordNet noun Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 83–90, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 83 synsets as clusters. He then calculates the selectional preference strength of a specific verb in a particular relation by computing the KullbackLeibler divergence between the cluster distribution of the verb and the aggregate cluster distribution. The selectional association is then the contribution of the cluster to the verb’s preference strength. T</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional Constraints: An Information-Theoretic Model and its Computational Realization. Cognition, 61:127–159, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mats Rooth</author>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Glenn Carroll</author>
<author>Franz Beil</author>
</authors>
<title>Inducing a semantically annotated lexicon via em-based clustering.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="4307" citStr="Rooth et al. (1999)" startWordPosition="674" endWordPosition="677">e verb and the aggregate cluster distribution. The selectional association is then the contribution of the cluster to the verb’s preference strength. The model’s generalization relies entirely on WordNet; there is no generalization among the verbs. The research in this paper is related to previous work on clustering. Pereira et al. (1993) use an information-theoretic based clustering approach, clustering nouns according to their distribution as direct objects among verbs. Their model is a onesided clustering model: only the direct objects are clustered, there is no clustering among the verbs. Rooth et al. (1999) use an EM-based clustering technique to induce a clustering based on the co-occurrence frequencies of verbs with their subjects and direct objects. As opposed to the method of Pereira et al. (1993), their model is two-sided: the verbs as well as the subjects/direct objects are clustered. We will use a similar model for evaluation purposes. Recent approaches using distributional similarity methods for the induction of selectional preferences are the ones by Erk (2007), Bhagat et al. (2007) and Basili et al. (2007). This research differs from the approaches mentioned above by its use of multi-w</context>
<context position="14518" citStr="Rooth et al. (1999)" startWordPosition="2341" endWordPosition="2344">ts). These are not the only sensible dimensions that have been found by the algorithm. A quick qualitative evaluation indicates that about 44 dimensions contain similar, framelike semantics. In another 43 dimensions, the semantics are less clearcut (single verbs account for one dimension, or different senses of a verb get mixed up). 13 dimensions are not so much based on semantic characteristics, but rather on syntax (e.g. fixed expressions and pronomina). 4.3 Evaluation The results of the NTF model have been quantitatively evaluated in a pseudo-disambiguation task, similar to the one used by Rooth et al. (1999). It is used to evaluate the generalization capabilities of the algorithm. The task is to judge which subject (s or s&apos;) and direct object (o or o&apos;) is more likely for a particular verb v, where (s,v,o) is a combination drawn from the corpus, and s&apos; and o&apos; are a subject and direct object randomly drawn from the corpus. A triple is considered correct if the algorithm prefers both s and o over their counterparts 3Note that VVD, D66, PvdA and CDA are Dutch political parties. k � i=1 xsvo = 86 subjects sus verbs vs objects ob js politie ‘police’ .99 houd aan ‘arrest’ .64 verdachte ‘suspect’ .16 age</context>
</contexts>
<marker>Rooth, Riezler, Prescher, Carroll, Beil, 1999</marker>
<rawString>Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Carroll, and Franz Beil. 1999. Inducing a semantically annotated lexicon via em-based clustering. In 37th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amnon Shashua</author>
<author>Tamir Hazan</author>
</authors>
<title>Nonnegative tensor factorization with applications to statistics and computer vision.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>792--799</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7360" citStr="Shashua and Hazan (2005)" startWordPosition="1147" endWordPosition="1150"> parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization of non-negative matrix factorization, and can be considered an extension of the PARAFAC model with the constraint of non-negativity (cfr. infra). One of the few papers that has investigated the application of tensor factorization for NLP is Turney (2007), in which a three-mode tensor is used to compute the semantic similarity of words. The method achieves 83.75% accuracy on the TOEFL synonym questions. 84 3 Methodology 3.1 Tensors Distributional similarity methods usually represent co-occurrence data in the form of a matrix. This form is perfectly suited to represent</context>
</contexts>
<marker>Shashua, Hazan, 2005</marker>
<rawString>Amnon Shashua and Tamir Hazan. 2005. Nonnegative tensor factorization with applications to statistics and computer vision. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 792–799, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Tucker</author>
</authors>
<title>Some mathematical notes on threemode factor analysis.</title>
<date>1966</date>
<tech>Psychometrika,</tech>
<pages>31--279</pages>
<contexts>
<context position="6880" citStr="Tucker (1966)" startWordPosition="1079" endWordPosition="1080">(1999)) and non-negative matrix factorization (NMF, Lee and Seung (2000)) remedy these problems, and indeed get much more clear-cut semantic dimensions. 2.2.2 Three-way Factorizations To be able to cope with three-way data, several algorithms have been developed as multilinear generalizations of the SVD. In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization of non-negative matrix factorization, and can be considered an extension of the PARAFAC model</context>
</contexts>
<marker>Tucker, 1966</marker>
<rawString>L.R. Tucker. 1966. Some mathematical notes on threemode factor analysis. Psychometrika, 31:279–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Empirical evaluation of four tensor decomposition algorithms.</title>
<date>2007</date>
<tech>Technical Report ERB-1152,</tech>
<institution>National Research Council, Institute for Information Technology.</institution>
<contexts>
<context position="7641" citStr="Turney (2007)" startWordPosition="1193" endWordPosition="1195">domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization of non-negative matrix factorization, and can be considered an extension of the PARAFAC model with the constraint of non-negativity (cfr. infra). One of the few papers that has investigated the application of tensor factorization for NLP is Turney (2007), in which a three-mode tensor is used to compute the semantic similarity of words. The method achieves 83.75% accuracy on the TOEFL synonym questions. 84 3 Methodology 3.1 Tensors Distributional similarity methods usually represent co-occurrence data in the form of a matrix. This form is perfectly suited to represent two-way co-occurrence data, but for co-occurrence data beyond two modes, we need a more general representation. The generalization of a matrix is called a tensor. A tensor is able to encode co-occurrence data of any n modes. Figure 1 shows a graphical comparison of a matrix and a</context>
</contexts>
<marker>Turney, 2007</marker>
<rawString>Peter D. Turney. 2007. Empirical evaluation of four tensor decomposition algorithms. Technical Report ERB-1152, National Research Council, Institute for Information Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At Last Parsing Is Now Operational. In</title>
<date>2006</date>
<booktitle>TALN06. Verbum Ex Machina. Actes de la 13e conference sur le traitement automatique des langues naturelles,</booktitle>
<pages>20--42</pages>
<editor>Piet Mertens, Cedrick Fairon, Anne Dister, and Patrick Watrin, editors,</editor>
<location>Leuven.</location>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At Last Parsing Is Now Operational. In Piet Mertens, Cedrick Fairon, Anne Dister, and Patrick Watrin, editors, TALN06. Verbum Ex Machina. Actes de la 13e conference sur le traitement automatique des langues naturelles, pages 20– 42, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Alex O Vasilescu</author>
<author>Demetri Terzopoulos</author>
</authors>
<title>Multilinear analysis of image ensembles: Tensorfaces. In</title>
<date>2002</date>
<booktitle>ECCV,</booktitle>
<pages>447--460</pages>
<contexts>
<context position="7112" citStr="Vasilescu and Terzopoulos, 2002" startWordPosition="1109" endWordPosition="1113">-way data, several algorithms have been developed as multilinear generalizations of the SVD. In statistics, threeway component analysis has been extensively investigated (for an overview, see Kiers and van Mechelen (2001)). The two most popular methods are parallel factor analysis (PARAFAC, Harshman (1970), Carroll and Chang (1970)) and three-mode principal component analysis (3MPCA, Tucker (1966)), also called higher order singular value decomposition (HOSVD, De Lathauwer et al. (2000)). Three-way factorizations have been applied in various domains, such as psychometry and image recognition (Vasilescu and Terzopoulos, 2002). In information retrieval, three-way factorizations have been applied to the problem of link analysis (Kolda and Bader, 2006). One last important method dealing with multiway data is non-negative tensor factorization (NTF, Shashua and Hazan (2005)). NTF is a generalization of non-negative matrix factorization, and can be considered an extension of the PARAFAC model with the constraint of non-negativity (cfr. infra). One of the few papers that has investigated the application of tensor factorization for NLP is Turney (2007), in which a three-mode tensor is used to compute the semantic similari</context>
</contexts>
<marker>Vasilescu, Terzopoulos, 2002</marker>
<rawString>M. Alex O. Vasilescu and Demetri Terzopoulos. 2002. Multilinear analysis of image ensembles: Tensorfaces. In ECCV, pages 447–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Wall</author>
<author>Andreas Rechtsteiner</author>
<author>Luis M Rocha</author>
</authors>
<date>2003</date>
<booktitle>Singular Value Decomposition and Principal Component Analysis, chapter 5,</booktitle>
<pages>91--109</pages>
<publisher>Hluwel,</publisher>
<location>Norwell, MA,</location>
<contexts>
<context position="5476" citStr="Wall et al., 2003" startWordPosition="861" endWordPosition="864"> approaches mentioned above by its use of multi-way data: where the approaches above limit themselves to two-way co-occurrences, this research will focus on cooccurrences for multi-way data. 2.2 Factorization Algorithms 2.2.1 Two-way Factorizations One of the best known factorization algorithms is principal component analysis (PCA, Pearson (1901)). PCA transforms the data into a new coordinate system, yielding the best possible fit in a least square sense given a limited number of dimensions. Singular value decomposition (SVD) is the generalization of the eigenvalue decomposition used in PCA (Wall et al., 2003). In information retrieval, singular value decomposition has been applied in latent semantic analysis (LSA, Landauer and Dumais (1997), Landauer et al. (1998)). In LSA, a term-document matrix is created, containing the frequency of each word in a specific document. This matrix is then decomposed into three other matrices with SVD. The most important dimensions that come out of the SVD allegedly represent ‘latent semantic dimensions’, according to which nouns and documents can be represented more efficiently. LSA has been criticized for a number of reasons, one of them being the fact that the f</context>
</contexts>
<marker>Wall, Rechtsteiner, Rocha, 2003</marker>
<rawString>Michael E. Wall, Andreas Rechtsteiner, and Luis M. Rocha, 2003. Singular Value Decomposition and Principal Component Analysis, chapter 5, pages 91– 109. Hluwel, Norwell, MA, Mar.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>