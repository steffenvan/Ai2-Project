<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039214">
<figure confidence="0.995184111111111">
Cell input
multiplied by input
Inputs Outputs gate activation
1.0
Cell
Cell output
multiplied by output
gate activation
Input gate Output gate
</figure>
<bodyText confidence="0.999863222222222">
word, the chunk tags and POS tags con-
sisted of 25, 5 and 7 units respectively. The
word representations were derived using
lexical space encodings (Zavrel and Veen-
stra, 1996), the rest by handl. An input
unit is used to indicate which pass through
the current sentence the network is cur-
rently in (see below). There are thus a total
of 38 input units.
</bodyText>
<listItem confidence="0.950206793103448">
• The outputs were represented by 13 unit
vector representations. These were sup-
posed to indicate the number of clauses
starting or ending on the current word by
having 2 sets of units, N units of which were
set to the value &amp;quot;1&amp;quot; when N clauses started
or ended on the current word. However a
mistake in the encoding was only spotted
when all the training and testing on the de-
velopment data had been done. Note that
when presented as targets to the network,
the zeros are converted 0.1s and the ones
to 0.9s. This helps training, most probably
because it helps prevent the weights taking
on very high values that make the network
resistant to change.
• Each sentence is presented to the network,
one word/POS tag/chunk tag at a time, in
two passes. On the first pass, the network
is not presented with target outputs. On
the second pass the network has to output
the number of clauses starting and ending
on the current word.
• The purpose of the first pass is to enable
the network to collect information to use
in disambiguation during the second pass.
• A network with 12 memory blocks of 8 cells
was used in training.
• The network was trained for 1000 iterations
</listItem>
<bodyText confidence="0.990166660377359">
on the first 1000 sentences from the sen-
tences from sections 15 to 18 of the Wall
Street Journal corpus. The best set of
weights as indicated by the number of er-
roneous patterns was then used in testing.
When deciding if a pattern was erroneous
a tolerance of 0.39 was used. I.e. if all
output activations came within 0.39 of the
target value the pattern was deemed to be
&apos;Contact the author for details of the input/output
representations.
correct. The learning rate was 0.3 and no
momentum was used.
• The network was tested on the develop-
ment and test data for the shared task.
When evaluating the network&apos;s output,
the outputs are converted to brackets.
These are balanced by assuming that every
opening bracket the net outputs is valid,
and discarding any extra closing brackets.
Should closing brackets need to be added,
they are added after the final word in the
sentence (i.e. before the final punctuation).
The network used above was much larger
than anything used in the LSTM literature and
was intensive to train (hence the decision to
train only on the first 1000 sentences). Whilst
this may in part reflect the difficulty of the task,
it may also reflect the possibility that the ap-
proach to training the network for the task is
not optimal. E.g. correcting the mistake in the
encoding of the output tags could improve per-
formance.
During investigation to see whether training
of some LSTM networks could be improved for
another task (noun-phrase bracketing), it was
discovered that outputting all zeroes (as op-
posed to not using targets) during the first pass
improved performance, as did using orthogonal
representations for the POS and chunk tags.
Applying these changes (and correcting the mis-
take in the encoding of the output tags de-
scribed above) enabled a network with only 12
blocks of 4 cells to be trained and to reach a
similar level of performance on the same train-
ing set as the network above. This network had
90 inputs due to the use of orthogonal vectors
for the tags.
Finally, a third network was trained on the
first 2000 sentences of the training set, which
also employed 12 blocks of 4 cells and the same
vector representations of the words, tags and
output as with the second network above.
</bodyText>
<sectionHeader confidence="0.999824" genericHeader="abstract">
4 Results
</sectionHeader>
<bodyText confidence="0.999539833333333">
Table 1 gives the results of training the networks
mentioned above and Table 2 gives the results
of testing them. Note that the fscores are below
the baseline value given in the specification for
the shared task in the first 2 cases but the fscore
is just above baseline in the 3rd case. Note also
</bodyText>
<table confidence="0.82279625">
training Iters MSE Errs Weights
Net 1 998 0.028 1047 17883
Net 2 997 0.014 1014 13495
Net 3 983 0.016 2817 13495
</table>
<tableCaption confidence="0.964725666666667">
Table 1: Results of training 2 LSTM networks
on first 1000 sentences of the training set and a
3rd network (Net 3) on the first 2000 sentences.
</tableCaption>
<bodyText confidence="0.957226">
that the recall is above baseline in all 3 cases.
The column headings are as follows:
</bodyText>
<listItem confidence="0.988995785714286">
• Iters. The number of iterations performed
when the best error was reached.
• MSE. The mean squared error between the
target outputs and the actual outputs.
• Errs. The number of erroneous patterns.
• Weights. The number of weights in the net-
work (the most accurate measure of net-
work size).
• Precision. The percentage of clauses found
that were correct.
• Recall. The percentage of clauses defined
in the corpus that were found.
• Fscore. F — 2PR where P=precision and
P+R
</listItem>
<bodyText confidence="0.339541">
R= recall.
</bodyText>
<sectionHeader confidence="0.914156" genericHeader="categories and subject descriptors">
5 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.99981115">
As noted earlier this work was exploratory
in nature as part of an investigation into us-
ing LSTM networks for shallow parsing tasks.
Given that these preliminary attempts to do
this task and the networks were trained on only
a small part of the training set, which has a to-
tal of 8936 sentences, and that improved per-
formance compared to the first network was
achieved whilst using a smaller network, these
results are not as disappointing as they might
initially appear. Both the fscore and the recall
are above baseline in the best case, and recall is
above baseline in all cases. There is also scope
for improving on these results.
The differences in performance between Net
1 and Net 2 on the development data may sim-
ply be due to noise from different weight initial-
isations. A more thorough experiment would
employ averages from several training runs to
make allowances for this.
</bodyText>
<table confidence="0.998994625">
development Precision Recall Fscore
Net 1 55.19% 46.35% 50.38
Net 2 54.74% 45.37% 49.62
Net 3 59.85% 55.56% 57.62
test Precision Recall Fscore
Net 1 51.92% 40.59% 45.56
Net 2 52.92% 40.08% 45.61
Net 3 55.81% 45.99% 50.42
</table>
<tableCaption confidence="0.8383135">
Table 2: Results of testing the networks on the
development and testing data for Part 3 of the
</tableCaption>
<bodyText confidence="0.968371684210526">
shared task ONLY. The best fscores are high-
lighted in bold.
The scope for improving on these results
comes from the possibilities of training with
more data, training for more iterations and pos-
sible further improvements in training times
through different ways of presenting the task to
the networks.
However it is clear there is some way to go be-
fore these networks reach a good level of perfor-
mance. Future work will look at improving the
performance via increasing the amount of train-
ing data, using the difference between the cur-
rent word vector and the previous word vector
as an extra input to the network (which thus ex-
plicitly provides the transitions from one word
to the next) and possibly employing a different
processing strategy to the current 2-pass strat-
egy.
</bodyText>
<sectionHeader confidence="0.990179" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9960726">
The author would like to thank Ronan Reilly
and Fred Cummins for their comments and ad-
vice on this work and for Fred&apos;s LSTM code.
This work was funded by the EU TMR project
&amp;quot;Learning Computational Grammars&amp;quot;.
</bodyText>
<sectionHeader confidence="0.998213" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9916885">
S. Hochreiter and J. Schmidhuber. 1997. Long
Short-Term Memory. Neural Computation,
9:1735-1780.
J. Zavrel and J. Veenstra. 1996. The language envi-
ronment and syntactic word class acquisition. In
Koster C. and Wijnen F., editors, Proceedings of
the Groningen Assembly on Language Acquisition
(GALA &apos;95).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.017717">
<title confidence="0.563234">Cell input</title>
<abstract confidence="0.97980272">multiplied by input Inputs Outputs gate activation 1.0 Cell Cell output multiplied by output gate activation Input gate Output gate word, the chunk tags and POS tags consisted of 25, 5 and 7 units respectively. The word representations were derived using lexical space encodings (Zavrel and Veen- 1996), the rest by An input unit is used to indicate which pass through the current sentence the network is currently in (see below). There are thus a total of 38 input units. • The outputs were represented by 13 unit vector representations. These were supposed to indicate the number of clauses starting or ending on the current word by having 2 sets of units, N units of which were set to the value &amp;quot;1&amp;quot; when N clauses started or ended on the current word. However a mistake in the encoding was only spotted when all the training and testing on the development data had been done. Note that when presented as targets to the network, the zeros are converted 0.1s and the ones to 0.9s. This helps training, most probably because it helps prevent the weights taking on very high values that make the network resistant to change. • Each sentence is presented to the network, one word/POS tag/chunk tag at a time, in two passes. On the first pass, the network is not presented with target outputs. On the second pass the network has to output the number of clauses starting and ending on the current word. • The purpose of the first pass is to enable the network to collect information to use in disambiguation during the second pass. • A network with 12 memory blocks of 8 cells was used in training. • The network was trained for 1000 iterations on the first 1000 sentences from the sentences from sections 15 to 18 of the Wall Street Journal corpus. The best set of weights as indicated by the number of erroneous patterns was then used in testing. When deciding if a pattern was erroneous a tolerance of 0.39 was used. I.e. if all output activations came within 0.39 of the target value the pattern was deemed to be &apos;Contact the author for details of the input/output representations. correct. The learning rate was 0.3 and no momentum was used. • The network was tested on the development and test data for the shared task. When evaluating the network&apos;s output, the outputs are converted to brackets. These are balanced by assuming that every opening bracket the net outputs is valid, and discarding any extra closing brackets. Should closing brackets need to be added, they are added after the final word in the sentence (i.e. before the final punctuation). The network used above was much larger than anything used in the LSTM literature and was intensive to train (hence the decision to train only on the first 1000 sentences). Whilst this may in part reflect the difficulty of the task, it may also reflect the possibility that the approach to training the network for the task is not optimal. E.g. correcting the mistake in the encoding of the output tags could improve performance. During investigation to see whether training of some LSTM networks could be improved for another task (noun-phrase bracketing), it was discovered that outputting all zeroes (as opposed to not using targets) during the first pass improved performance, as did using orthogonal representations for the POS and chunk tags. Applying these changes (and correcting the mistake in the encoding of the output tags described above) enabled a network with only 12 blocks of 4 cells to be trained and to reach a similar level of performance on the same training set as the network above. This network had 90 inputs due to the use of orthogonal vectors for the tags. Finally, a third network was trained on the first 2000 sentences of the training set, which also employed 12 blocks of 4 cells and the same vector representations of the words, tags and output as with the second network above. 4 Results Table 1 gives the results of training the networks mentioned above and Table 2 gives the results of testing them. Note that the fscores are below the baseline value given in the specification for the shared task in the first 2 cases but the fscore is just above baseline in the 3rd case. Note also training Iters MSE Errs Weights Net 1 998 0.028 1047 17883 Net 2 997 0.014 1014 13495 Net 3 983 0.016 2817 13495 Table 1: Results of training 2 LSTM networks on first 1000 sentences of the training set and a 3rd network (Net 3) on the first 2000 sentences. that the recall is above baseline in all 3 cases. The column headings are as follows: • Iters. The number of iterations performed when the best error was reached. • MSE. The mean squared error between the target outputs and the actual outputs. • Errs. The number of erroneous patterns. • Weights. The number of weights in the network (the most accurate measure of network size). • Precision. The percentage of clauses found that were correct. • Recall. The percentage of clauses defined in the corpus that were found. Fscore. — 2PRwhere P=precision and P+R 5 Concluding Remarks As noted earlier this work was exploratory in nature as part of an investigation into using LSTM networks for shallow parsing tasks. Given that these preliminary attempts to do this task and the networks were trained on only a small part of the training set, which has a total of 8936 sentences, and that improved performance compared to the first network was achieved whilst using a smaller network, these results are not as disappointing as they might initially appear. Both the fscore and the recall are above baseline in the best case, and recall is above baseline in all cases. There is also scope for improving on these results. The differences in performance between Net 1 and Net 2 on the development data may simply be due to noise from different weight initialisations. A more thorough experiment would employ averages from several training runs to make allowances for this.</abstract>
<note confidence="0.785482111111111">development Precision Recall Fscore Net 1 55.19% 46.35% 50.38 Net 2 54.74% 45.37% 49.62 Net 3 59.85% 55.56% 57.62 test Precision Recall Fscore Net 1 51.92% 40.59% 45.56 Net 2 52.92% 40.08% 45.61 Net 3 55.81% 45.99% 50.42 Table 2: Results of testing the networks on the</note>
<abstract confidence="0.938634333333333">development and testing data for Part 3 of the task best fscores are highin The scope for improving on these results comes from the possibilities of training with more data, training for more iterations and possible further improvements in training times through different ways of presenting the task to the networks. However it is clear there is some way to go before these networks reach a good level of performance. Future work will look at improving the performance via increasing the amount of training data, using the difference between the current word vector and the previous word vector as an extra input to the network (which thus explicitly provides the transitions from one word to the next) and possibly employing a different processing strategy to the current 2-pass strategy. Acknowledgements The author would like to thank Ronan Reilly and Fred Cummins for their comments and advice on this work and for Fred&apos;s LSTM code.</abstract>
<note confidence="0.917151090909091">This work was funded by the EU TMR project &amp;quot;Learning Computational Grammars&amp;quot;. References S. Hochreiter and J. Schmidhuber. 1997. Long Memory. Computation, 9:1735-1780. J. Zavrel and J. Veenstra. 1996. The language environment and syntactic word class acquisition. In C. and Wijnen F., editors, of the Groningen Assembly on Language Acquisition (GALA &apos;95).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Hochreiter</author>
<author>J Schmidhuber</author>
</authors>
<title>Long Short-Term Memory.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<pages>9--1735</pages>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>S. Hochreiter and J. Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9:1735-1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zavrel</author>
<author>J Veenstra</author>
</authors>
<title>The language environment and syntactic word class acquisition. In</title>
<date>1996</date>
<booktitle>Proceedings of the Groningen Assembly on Language Acquisition (GALA &apos;95).</booktitle>
<editor>Koster C. and Wijnen F., editors,</editor>
<marker>Zavrel, Veenstra, 1996</marker>
<rawString>J. Zavrel and J. Veenstra. 1996. The language environment and syntactic word class acquisition. In Koster C. and Wijnen F., editors, Proceedings of the Groningen Assembly on Language Acquisition (GALA &apos;95).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>