<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002933">
<title confidence="0.9969105">
BioinformaticsUA: Concept Recognition in Clinical Narratives Using a
Modular and Highly Efficient Text Processing Framework
</title>
<author confidence="0.995579">
S´ergio Matos
</author>
<affiliation confidence="0.985262">
DETI/IEETA
University of Aveiro
</affiliation>
<address confidence="0.711006">
3810-193 Aveiro, Portugal
</address>
<email confidence="0.996374">
aleixomatos@ua.pt
</email>
<author confidence="0.953218">
Tiago Nunes
</author>
<affiliation confidence="0.95833">
DETI/IEETA
University of Aveiro
</affiliation>
<address confidence="0.701352">
3810-193 Aveiro, Portugal
</address>
<email confidence="0.996402">
tiago.nunes@ua.pt
</email>
<author confidence="0.808276">
Jos´e Luis Oliveira
</author>
<affiliation confidence="0.8537455">
DETI/IEETA
University of Aveiro
</affiliation>
<address confidence="0.659975">
3810-193 Aveiro, Portugal
</address>
<email confidence="0.996948">
jlo@ua.pt
</email>
<sectionHeader confidence="0.993837" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908217391304">
Clinical texts, such as discharge sum-
maries or test reports, contain a valuable
amount of information that, if efficiently
and effectively mined, could be used to
infer new knowledge, possibly leading to
better diagnosis and therapeutics. With
this in mind, the SemEval-2014 Analysis
of Clinical Text task aimed at assessing
and improving current methods for identi-
fication and normalization of concepts oc-
curring in clinical narrative. This paper
describes our approach in this task, which
was based on a fully modular architec-
ture for text mining. We followed a pure
dictionary-based approach, after perform-
ing error analysis to refine our dictionaries.
We obtained an F-measure of 69.4% in
the entity recognition task, achieving the
second best precision over all submitted
runs (81.3%), with above average recall
(60.5%). In the normalization task, we
achieved a strict accuracy of 53.1% and a
relaxed accuracy of 87.0%.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996976">
Named entity recognition (NER) is an information
extraction task where the aim is to identify men-
tions of specific types of entities in text. This task
has been one of the main focus in the biomedi-
cal text mining research field, specially when ap-
plied to the scientific literature. Such efforts have
led to the development of various tools for the
recognition of diverse entities, including species
names, genes and proteins, chemicals and drugs,
anatomical concepts and diseases. These tools use
</bodyText>
<footnote confidence="0.9856995">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999722724137931">
methods based on dictionaries, rules, and machine
learning, or a combination of those depending on
the specificities and requirements of each concept
type (Campos et al., 2013b). After identifying en-
tities occurring in texts, it is also relevant to dis-
ambiguate those entities and associate each occur-
rence to a specific concept, using an univocal iden-
tifier from a reference database such as Uniprot1
for proteins, or OMIM2 for genetic disorders. This
is usually performed by matching the identified
entities against a knowledge-base, possibly eval-
uating the textual context in which the entity oc-
curred to identify the best matching concept.
The SemEval-2014 Analysis of Clinical Text
task aimed at the identification and normalization
of concepts in clinical narrative. Two subtasks
were defined, where Task A was focused on the
recognition of entities belonging to the ‘disorders’
semantic group of the Unified Medical Language
System (UMLS), and Task B was focused on nor-
malization of these entities to a specific UMLS
Concept Unique Identifier (CUI). Specifically, the
task definition required that concepts should only
be normalized to CUIs that could be mapped to the
SNOMED CT3 terminology.
In this paper, we present a dictionary-based ap-
proach for the recognition of these concepts, sup-
ported by a modular text analysis and annotation
pipeline.
</bodyText>
<sectionHeader confidence="0.99075" genericHeader="introduction">
2 Methods
</sectionHeader>
<subsectionHeader confidence="0.938212">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.9998935">
The task made use of the ShARe corpus (Pradhan
et al., 2013), which contains manually annotated
clinical notes from the MIMIC II database4 (Saeed
et al., 2011). The corpus contains 298 documents,
</bodyText>
<footnote confidence="0.9999875">
1http://www.uniprot.org/
2http://www.omim.org/
3http://www.ihtsdo.org/snomed-ct/
4http://mimic.physionet.org/database.html
</footnote>
<page confidence="0.933008">
135
</page>
<note confidence="0.7405915">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 135–139,
Dublin, Ireland, August 23-24, 2014.
</note>
<figure confidence="0.999319526315789">
Documents Annotated
Documents
Reader
Sentence
Tagger
NLP
Processing pipeline
Dictionaries
Dictionary
Tagger
ML Tagger
Models
Abbreviation resolution
Post-processing
Disambiguator
Custom Module
Relation extractor
Indexer
Writer
</figure>
<figureCaption confidence="0.9647375">
Figure 1: Neji’s processing pipeline used for annotating the documents. Boxes with dotted lines indicate
optional processing modules. Machine-learning models were not used.
</figureCaption>
<bodyText confidence="0.999335125">
with a total of 11156 annotations of disorder men-
tions. These annotations include a UMLS concept
identifier when such normalization was possible
according to the annotation guidelines.
Besides this manually annotated corpus, a larger
unannotated data set was also made available to
task participants, in order to allow the application
of unsupervised methods.
</bodyText>
<subsectionHeader confidence="0.999871">
2.2 Processing Pipeline
</subsectionHeader>
<bodyText confidence="0.999951952380952">
We used Neji, an open source framework for
biomedical concept recognition based on an au-
tomated processing pipeline that supports the
combined application of machine learning and
dictionary-based approaches (Campos et al.,
2013a). Apart from offering a flexible frame-
work for developing different text mining sys-
tems, Neji includes various built-in methods, from
text loading and pre-processing, to natural lan-
guage parsing and entity tagging, all optimized
for processing biomedical text. Namely, it in-
cludes a sentence splitting module adapted from
the Lingpipe library5 and a customized version
of GDep (Sagae and Tsujii, 2007) for tokeniza-
tion, part-of-speech tagging, and other natural lan-
guage processing tasks. Figure 1 shows the com-
plete Neji text processing pipeline, illustrating its
module based architecture built on top of a com-
mon data structure. The dictionary module per-
forms exact, case-insensitive matching using De-
terministic Finite Automatons (DFAs), allowing
</bodyText>
<footnote confidence="0.592864">
5http://alias-i.com/lingpipe/index.html
</footnote>
<bodyText confidence="0.998588090909091">
very efficient processing of documents and match-
ing against dozens of dictionaries containing mil-
lions of terms.
Neji has been validated against different
biomedical literature corpora, using specifically
created machine learning models and dictionar-
ies. Regarding the recognition of disorder con-
cepts, Neji achieved an F-measure of 68% on ex-
act mathing and 83% on approximate matching
against the NCBI disease corpus, using a pure
dictionary-based approach (Do˘gan and Lu, 2012).
</bodyText>
<subsectionHeader confidence="0.972859">
2.3 Dictionaries
</subsectionHeader>
<bodyText confidence="0.985876">
Following the task description and the corpus an-
notation guidelines, we compiled dictionaries for
the following UMLS semantic types, using the
2012AB version of the UMLS Metathesaurus:
</bodyText>
<listItem confidence="0.9999901">
• Congenital Abnormality
• Acquired Abnormality
• Injury or Poisoning
• Pathologic Function
• Disease or Syndrome
• Mental or Behavioral Dysfunction
• Cell or Molecular Dysfunction
• Anatomical Abnormality
• Neoplastic Process
• Signs and Symptoms
</listItem>
<bodyText confidence="0.9998708">
Additionally, although the semantic type ‘Find-
ings’ was not considered as part of the ‘Disorders’
group, we created a customized dictionary includ-
ing only those concepts of this semantic type that
occurred as an annotation in the training data. If
</bodyText>
<page confidence="0.995119">
136
</page>
<bodyText confidence="0.989876114754098">
a synonym of a given concept was present in the
training data annotations, we added all the syn-
onyms of that concept to this dictionary. This
allowed including some concepts that occur very
frequently (e.g. ’fever’), while filtering out many
concepts of this semantic type that are not relevant
for this task. In total, these dictionaries contain
almost 1.5 million terms, of which 525 thousand
(36%) were distinct terms, for nearly 293 thousand
distinct concept identifiers.
Refining the dictionaries
In order to expand the dictionaries, we pre-
processed the UMLS terms to find certain patterns
indicating acronyms. For example, if a term such
as ‘Miocardial infarction (MI)’ or ‘Miocardial in-
farction - MI’ appeared as a synonym for a given
UMLS concept, we checked if the acronym (in this
example, ‘MI’) was also a synonym for that con-
cept, and added it to a separate dictionary if this
was not the case. This resulted in the addition of
10430 terms, for which only 1459 (14%) were dis-
tinct, for 2086 concepts. These numbers reflect the
expected ambiguity in the acronyms, which repre-
sents one of the main challenges in the annotation
of clinical texts.
Furthermore, in order to improve the baseline
results obtained with the initial dictionaries, we
performed error analysis to identify frequent er-
rors in the automatic annotations. Using the man-
ual annotations as reference, we counted the num-
ber of times a term was correctly annotated in the
documents (true positives) and compared it to the
number of times that same term caused an annota-
tion to be incorrectly added (a false positive). We
then defined an exclusion list containing 817 terms
for which the ratio of these two counts was 0.25 or
less.
Following the same approach, we created a sec-
ond exclusion list by comparing the number of
FNs to the number of FPs, and selecting those
terms for which this ratio was lower than 0.5. This
resulted in an exclusion list containing 623 terms.
We also processed the unannotated data set, in
order to identify frequently occurring terms that
could be removed from the dictionaries to avoid
large numbers of false positives. This dataset in-
cludes over 92 thousand documents, which were
processed in around 23 minutes (an average of
67 documents per second) and produced almost
4 million annotations. Examples of terms from
our dictionaries that occur very frequently in this
data set are: ‘sinus rhythm’, which occurred al-
most 35 thousand times across all documents, and
‘past medical history’, ‘allergies’ and ‘abnormal-
ities’, all occurring more than 15 thousand times.
In fact, most of the highly frequent terms belonged
to the ‘Findings’ semantic type. Although this
analysis gave some insights regarding the content
of the data, its results were not directly used to
refine the dictionaries, since the filtering steps de-
scribed above led to better overall results.
</bodyText>
<subsectionHeader confidence="0.991528">
2.4 Concept Normalization
</subsectionHeader>
<bodyText confidence="0.9999774375">
According to the task description, only those
UMLS concepts that could be mapped to a
SNOMED CT identifier should be considered in
the normalization step, while all other entities
should be added to the results without a concept
identifier. We followed a straightforward normal-
ization strategy, by assigning the corresponding
UMLS CUIs to each identified entity, during the
dictionary-matching phase. We then filtered out
any CUIs that did not have a SNOMED CT map-
ping in the UMLS data. In the cases when multi-
ple idenfiers were still left, we naively selected the
first one, according the dictionary ordering defined
above, followed in the end by the filtered ‘Find-
ings’ dictionary and the additional acronyms dic-
tionary.
</bodyText>
<sectionHeader confidence="0.999072" genericHeader="related work">
3 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.99542">
3.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.974587736842105">
The common evaluation metrics were used to
evaluate the entity recognition task, namely
Precision = TP/(TP + FP) and Recall =
TP/(TP+FN), where TP, FP and FN are respec-
tively the number of true positive, false positive,
and false negative annotations, and Fmeasure =
2 × Precision × Recall/(Precision + Recall),
the harmonic mean of precision and recall. Addi-
tionally, the performance was evaluated consider-
ing both strict and relaxed, or overlap, matching of
the gold standard annotations.
For the normalization task, the metric used to
evaluate performance was accuracy. Again, two
matching methods were considered: strict accu-
racy was defined as the ratio between the number
of correct identifiers assigned to the predicted en-
tities, and the total number of entities manually
annotated in the corpus; while relaxed accuracy
measured the ratio between the number of correct
</bodyText>
<page confidence="0.994831">
137
</page>
<table confidence="0.999559">
Task A Task B
Strict Relaxed Strict Relaxed
Run P R F P R F Acc Acc
Best 0,843 0,786 0,813 0,936 0,866 0,900 0,741 0,873
Average 0,648 0,574 0,599 0,842 0,731 0,770 0,461 0,753
0 0,813 0,605 0,694 0,929 0,693 0,794 0,527 0,870
1 0,600 0,621 0,610 0,698 0,723 0,710 0,531 0,855
2 0,753 0,538 0,628 0,865 0,621 0,723 0,463 0,861
</table>
<tableCaption confidence="0.998043">
Table 1: Official results on the test dataset. The best results for each task and matching strategy are
</tableCaption>
<bodyText confidence="0.571088">
identified in bold. The best run from all participating teams as well as the overall average are shown for
comparison.
identifiers and the number of entities correctly pre-
dicted by the system.
</bodyText>
<subsectionHeader confidence="0.999963">
3.2 Test Results
</subsectionHeader>
<bodyText confidence="0.99979">
We submitted three runs of annotations for the
documents in the test set, as described below:
</bodyText>
<listItem confidence="0.7335125">
• Run 0: Resulting annotations were filtered
using the first exclusion list (817 terms,
</listItem>
<bodyText confidence="0.9676002">
TP/FP ratio 0.25 or lower). The ex-
tra acronyms dictionary was not used, and
matches up to 3 characters long were filtered
out, except if they were 3 characters long and
appeared as uppercase in the original text.
</bodyText>
<listItem confidence="0.860948285714286">
• Run 1: The extra acronyms dictionary was
included. The same exclusion list as in Run
0 was used, but short annotations were not
removed.
• Run 2: The extra acronyms dictionary was
included. The second exclusion list was used,
and short annotations were not removed.
</listItem>
<bodyText confidence="0.999969833333333">
Table 1 shows the official results obtained on
the test set for each submitted run.
Overall, the best results were obtained with the
more stringent dictionaries and filtering, leading
to a precision of 81.3% and and F-measure of
69.4%. This results was achieved without the use
of the additional acronyms list, and also by re-
moving short annotations. This filtering does not
discard annotations with three characters if they
appeared in uppercase in the original text, as this
more clearly indicates the use of an acronym. Pre-
liminary evaluation on the training data showed
that this choice had a small, but positive contri-
bution to the overall results.
We achieved the second-best precision results
with this first run, considering both strict and re-
laxed matching. Although this level of precision
was not associated to a total loss in recall, we
were only able to identify 70% of the disorder
entities, even when considering relaxed match-
ing. To overcome this limitation, we will evalu-
ate the combined use of dictionaries and machine-
learning models, taking advantage of the Neji
framework. Another possible limitation has to
do with the recognition and disambiguation of
acronyms, which we will also evaluate further.
Regarding the normalization results (Task B),
we achieved the 12th and 10th best overall results,
considering strict and relaxed accuracies respec-
tively, corresponding to the 7th and 6th best team.
For relaxed matching, our results are 5,8% lower
than the best team, which is a positive result given
the naive approach taken. These performances
may be improved as a result of enhancements in
the entity recognition step, and by applying a bet-
ter normalization strategy.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999949928571429">
We present results for the recognition and normal-
ization of disorder mentions in clinical texts, us-
ing a dictionary-based approach. The dictionaries
were iteratively filtered following error-analysis,
in order to better tailor the dictionaries according
to the task annotation guidelines. In the end, a
precision of 81.3% was achieved, for a recall of
60.5% and a F-measure of 69.4%. The use of
a machine-learning based approach and a better
acronym resolution method are being studied with
the aim of improving the recall rate.
In the normalization task, using the refined dic-
tionaries directly, we achieved a strict accuracy of
53.1% and a relaxed accuracy of 87.0%. Strict
</bodyText>
<page confidence="0.994621">
138
</page>
<bodyText confidence="0.99992375">
normalization results, as given by the metric de-
fined for this task, are dependent on the entity
recognition recall rate, and are expected to follow
improvements that may be achieved in that step.
</bodyText>
<sectionHeader confidence="0.992147" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9990064">
This work was supported by National Funds
through FCT - Foundation for Science and Tech-
nology, in the context of the project PEst-
OE/EEI/UI0127/2014. S. Matos is funded by FCT
under the FCT Investigator programme.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878566666667">
David Campos, S´ergio Matos, and Jos´e Lu´ıs Oliveira.
2013a. A modular framework for biomedical con-
cept recognition. BMC Bioinformatics, 14:281.
David Campos, S´ergio Matos, and Jos´e Lu´ıs Oliveira,
2013b. Current Methodologies for Biomedical
Named Entity Recognition, pages 839–868. John
Wiley &amp; Sons, Inc., Hoboken, New Jersey.
Rezarta Islamaj Do˘gan and Zhiyong Lu. 2012. An
improved corpus of disease mentions in PubMed ci-
tations. In Proceedings of BioNLP’12, pages 91–99,
Stroudsburg, PA, USA, June.
Sameer Pradhan, Noemie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy Chapman, and Guergana Savova.
2013. Task 1: ShARe/CLEF eHealth Evaluation
Lab 2013. Online Working Notes of the CLEF 2013
Evaluation Labs and Workshop.
Mohammed Saeed, Mauricio Villarroel, Andrew Reis-
ner, Gari Clifford, Li-Wei Lehman, George Moody,
Thomas Heldt, Tin Kyaw, Benjamin Moody, and
Roger Mark. 2011. Multiparameter Intelligent
Monitoring in Intensive Care II (MIMIC-II): a
public-access intensive care unit database. Critical
Care Medicine, 39(5):952.
Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency
parsing and domain adaptation with LR models and
parser ensembles. In Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1044–1050, Prague, Czech Republic.
</reference>
<page confidence="0.998868">
139
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.498705">
<title confidence="0.9994925">BioinformaticsUA: Concept Recognition in Clinical Narratives Using Modular and Highly Efficient Text Processing Framework</title>
<author confidence="0.998647">S´ergio</author>
<affiliation confidence="0.999903">University of</affiliation>
<address confidence="0.989923">3810-193 Aveiro,</address>
<email confidence="0.994412">aleixomatos@ua.pt</email>
<author confidence="0.559419">Tiago</author>
<affiliation confidence="0.998993">University of</affiliation>
<address confidence="0.98893">3810-193 Aveiro,</address>
<email confidence="0.997395">tiago.nunes@ua.pt</email>
<author confidence="0.992269">Jos´e Luis</author>
<affiliation confidence="0.999897">University of</affiliation>
<address confidence="0.99237">3810-193 Aveiro,</address>
<email confidence="0.998294">jlo@ua.pt</email>
<abstract confidence="0.99719025">Clinical texts, such as discharge summaries or test reports, contain a valuable amount of information that, if efficiently and effectively mined, could be used to infer new knowledge, possibly leading to better diagnosis and therapeutics. With this in mind, the SemEval-2014 Analysis of Clinical Text task aimed at assessing and improving current methods for identification and normalization of concepts occurring in clinical narrative. This paper describes our approach in this task, which was based on a fully modular architecture for text mining. We followed a pure dictionary-based approach, after performing error analysis to refine our dictionaries. We obtained an F-measure of 69.4% in the entity recognition task, achieving the second best precision over all submitted runs (81.3%), with above average recall (60.5%). In the normalization task, we achieved a strict accuracy of 53.1% and a relaxed accuracy of 87.0%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Campos</author>
<author>S´ergio Matos</author>
<author>Jos´e Lu´ıs Oliveira</author>
</authors>
<title>A modular framework for biomedical concept recognition.</title>
<date>2013</date>
<journal>BMC Bioinformatics,</journal>
<pages>14--281</pages>
<contexts>
<context position="2224" citStr="Campos et al., 2013" startWordPosition="323" endWordPosition="326">fic literature. Such efforts have led to the development of various tools for the recognition of diverse entities, including species names, genes and proteins, chemicals and drugs, anatomical concepts and diseases. These tools use This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ methods based on dictionaries, rules, and machine learning, or a combination of those depending on the specificities and requirements of each concept type (Campos et al., 2013b). After identifying entities occurring in texts, it is also relevant to disambiguate those entities and associate each occurrence to a specific concept, using an univocal identifier from a reference database such as Uniprot1 for proteins, or OMIM2 for genetic disorders. This is usually performed by matching the identified entities against a knowledge-base, possibly evaluating the textual context in which the entity occurred to identify the best matching concept. The SemEval-2014 Analysis of Clinical Text task aimed at the identification and normalization of concepts in clinical narrative. Tw</context>
<context position="4894" citStr="Campos et al., 2013" startWordPosition="704" endWordPosition="707">t used. with a total of 11156 annotations of disorder mentions. These annotations include a UMLS concept identifier when such normalization was possible according to the annotation guidelines. Besides this manually annotated corpus, a larger unannotated data set was also made available to task participants, in order to allow the application of unsupervised methods. 2.2 Processing Pipeline We used Neji, an open source framework for biomedical concept recognition based on an automated processing pipeline that supports the combined application of machine learning and dictionary-based approaches (Campos et al., 2013a). Apart from offering a flexible framework for developing different text mining systems, Neji includes various built-in methods, from text loading and pre-processing, to natural language parsing and entity tagging, all optimized for processing biomedical text. Namely, it includes a sentence splitting module adapted from the Lingpipe library5 and a customized version of GDep (Sagae and Tsujii, 2007) for tokenization, part-of-speech tagging, and other natural language processing tasks. Figure 1 shows the complete Neji text processing pipeline, illustrating its module based architecture built o</context>
</contexts>
<marker>Campos, Matos, Oliveira, 2013</marker>
<rawString>David Campos, S´ergio Matos, and Jos´e Lu´ıs Oliveira. 2013a. A modular framework for biomedical concept recognition. BMC Bioinformatics, 14:281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Campos</author>
<author>S´ergio Matos</author>
<author>Jos´e Lu´ıs Oliveira</author>
</authors>
<title>Current Methodologies for Biomedical Named Entity Recognition,</title>
<date>2013</date>
<pages>839--868</pages>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>Hoboken, New Jersey.</location>
<contexts>
<context position="2224" citStr="Campos et al., 2013" startWordPosition="323" endWordPosition="326">fic literature. Such efforts have led to the development of various tools for the recognition of diverse entities, including species names, genes and proteins, chemicals and drugs, anatomical concepts and diseases. These tools use This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ methods based on dictionaries, rules, and machine learning, or a combination of those depending on the specificities and requirements of each concept type (Campos et al., 2013b). After identifying entities occurring in texts, it is also relevant to disambiguate those entities and associate each occurrence to a specific concept, using an univocal identifier from a reference database such as Uniprot1 for proteins, or OMIM2 for genetic disorders. This is usually performed by matching the identified entities against a knowledge-base, possibly evaluating the textual context in which the entity occurred to identify the best matching concept. The SemEval-2014 Analysis of Clinical Text task aimed at the identification and normalization of concepts in clinical narrative. Tw</context>
<context position="4894" citStr="Campos et al., 2013" startWordPosition="704" endWordPosition="707">t used. with a total of 11156 annotations of disorder mentions. These annotations include a UMLS concept identifier when such normalization was possible according to the annotation guidelines. Besides this manually annotated corpus, a larger unannotated data set was also made available to task participants, in order to allow the application of unsupervised methods. 2.2 Processing Pipeline We used Neji, an open source framework for biomedical concept recognition based on an automated processing pipeline that supports the combined application of machine learning and dictionary-based approaches (Campos et al., 2013a). Apart from offering a flexible framework for developing different text mining systems, Neji includes various built-in methods, from text loading and pre-processing, to natural language parsing and entity tagging, all optimized for processing biomedical text. Namely, it includes a sentence splitting module adapted from the Lingpipe library5 and a customized version of GDep (Sagae and Tsujii, 2007) for tokenization, part-of-speech tagging, and other natural language processing tasks. Figure 1 shows the complete Neji text processing pipeline, illustrating its module based architecture built o</context>
</contexts>
<marker>Campos, Matos, Oliveira, 2013</marker>
<rawString>David Campos, S´ergio Matos, and Jos´e Lu´ıs Oliveira, 2013b. Current Methodologies for Biomedical Named Entity Recognition, pages 839–868. John Wiley &amp; Sons, Inc., Hoboken, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rezarta Islamaj Do˘gan</author>
<author>Zhiyong Lu</author>
</authors>
<title>An improved corpus of disease mentions in PubMed citations.</title>
<date>2012</date>
<booktitle>In Proceedings of BioNLP’12,</booktitle>
<pages>91--99</pages>
<location>Stroudsburg, PA, USA,</location>
<marker>Do˘gan, Lu, 2012</marker>
<rawString>Rezarta Islamaj Do˘gan and Zhiyong Lu. 2012. An improved corpus of disease mentions in PubMed citations. In Proceedings of BioNLP’12, pages 91–99, Stroudsburg, PA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Noemie Elhadad</author>
<author>Brett South</author>
<author>David Martinez</author>
<author>Lee Christensen</author>
<author>Amy Vogel</author>
<author>Hanna Suominen</author>
<author>Wendy Chapman</author>
<author>Guergana Savova</author>
</authors>
<date>2013</date>
<booktitle>Task 1: ShARe/CLEF eHealth Evaluation Lab 2013. Online Working Notes of the CLEF 2013 Evaluation Labs and Workshop.</booktitle>
<contexts>
<context position="3483" citStr="Pradhan et al., 2013" startWordPosition="524" endWordPosition="527">was focused on the recognition of entities belonging to the ‘disorders’ semantic group of the Unified Medical Language System (UMLS), and Task B was focused on normalization of these entities to a specific UMLS Concept Unique Identifier (CUI). Specifically, the task definition required that concepts should only be normalized to CUIs that could be mapped to the SNOMED CT3 terminology. In this paper, we present a dictionary-based approach for the recognition of these concepts, supported by a modular text analysis and annotation pipeline. 2 Methods 2.1 Data The task made use of the ShARe corpus (Pradhan et al., 2013), which contains manually annotated clinical notes from the MIMIC II database4 (Saeed et al., 2011). The corpus contains 298 documents, 1http://www.uniprot.org/ 2http://www.omim.org/ 3http://www.ihtsdo.org/snomed-ct/ 4http://mimic.physionet.org/database.html 135 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 135–139, Dublin, Ireland, August 23-24, 2014. Documents Annotated Documents Reader Sentence Tagger NLP Processing pipeline Dictionaries Dictionary Tagger ML Tagger Models Abbreviation resolution Post-processing Disambiguator Custom Module Relatio</context>
</contexts>
<marker>Pradhan, Elhadad, South, Martinez, Christensen, Vogel, Suominen, Chapman, Savova, 2013</marker>
<rawString>Sameer Pradhan, Noemie Elhadad, Brett South, David Martinez, Lee Christensen, Amy Vogel, Hanna Suominen, Wendy Chapman, and Guergana Savova. 2013. Task 1: ShARe/CLEF eHealth Evaluation Lab 2013. Online Working Notes of the CLEF 2013 Evaluation Labs and Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Saeed</author>
<author>Mauricio Villarroel</author>
<author>Andrew Reisner</author>
<author>Gari Clifford</author>
<author>Li-Wei Lehman</author>
<author>George Moody</author>
<author>Thomas Heldt</author>
<author>Tin Kyaw</author>
<author>Benjamin Moody</author>
<author>Roger Mark</author>
</authors>
<title>Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): a public-access intensive care unit database.</title>
<date>2011</date>
<journal>Critical Care Medicine,</journal>
<volume>39</volume>
<issue>5</issue>
<contexts>
<context position="3582" citStr="Saeed et al., 2011" startWordPosition="539" endWordPosition="542"> Medical Language System (UMLS), and Task B was focused on normalization of these entities to a specific UMLS Concept Unique Identifier (CUI). Specifically, the task definition required that concepts should only be normalized to CUIs that could be mapped to the SNOMED CT3 terminology. In this paper, we present a dictionary-based approach for the recognition of these concepts, supported by a modular text analysis and annotation pipeline. 2 Methods 2.1 Data The task made use of the ShARe corpus (Pradhan et al., 2013), which contains manually annotated clinical notes from the MIMIC II database4 (Saeed et al., 2011). The corpus contains 298 documents, 1http://www.uniprot.org/ 2http://www.omim.org/ 3http://www.ihtsdo.org/snomed-ct/ 4http://mimic.physionet.org/database.html 135 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 135–139, Dublin, Ireland, August 23-24, 2014. Documents Annotated Documents Reader Sentence Tagger NLP Processing pipeline Dictionaries Dictionary Tagger ML Tagger Models Abbreviation resolution Post-processing Disambiguator Custom Module Relation extractor Indexer Writer Figure 1: Neji’s processing pipeline used for annotating the documents. </context>
</contexts>
<marker>Saeed, Villarroel, Reisner, Clifford, Lehman, Moody, Heldt, Kyaw, Moody, Mark, 2011</marker>
<rawString>Mohammed Saeed, Mauricio Villarroel, Andrew Reisner, Gari Clifford, Li-Wei Lehman, George Moody, Thomas Heldt, Tin Kyaw, Benjamin Moody, and Roger Mark. 2011. Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC-II): a public-access intensive care unit database. Critical Care Medicine, 39(5):952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1044--1050</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5297" citStr="Sagae and Tsujii, 2007" startWordPosition="765" endWordPosition="768"> Neji, an open source framework for biomedical concept recognition based on an automated processing pipeline that supports the combined application of machine learning and dictionary-based approaches (Campos et al., 2013a). Apart from offering a flexible framework for developing different text mining systems, Neji includes various built-in methods, from text loading and pre-processing, to natural language parsing and entity tagging, all optimized for processing biomedical text. Namely, it includes a sentence splitting module adapted from the Lingpipe library5 and a customized version of GDep (Sagae and Tsujii, 2007) for tokenization, part-of-speech tagging, and other natural language processing tasks. Figure 1 shows the complete Neji text processing pipeline, illustrating its module based architecture built on top of a common data structure. The dictionary module performs exact, case-insensitive matching using Deterministic Finite Automatons (DFAs), allowing 5http://alias-i.com/lingpipe/index.html very efficient processing of documents and matching against dozens of dictionaries containing millions of terms. Neji has been validated against different biomedical literature corpora, using specifically creat</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1044–1050, Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>