<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008731">
<title confidence="0.7740215">
RAISINS, SULTANAS, AND CURRANTS: LEXICAL
CLASSIFICATION AND ABSTRACTION VIA CONTEXT PRIMING
</title>
<author confidence="0.878952">
David J. Hutches
</author>
<affiliation confidence="0.9987645">
Department of Computer Science and Engineering, Mail Code 0114
University of California, San Diego
</affiliation>
<address confidence="0.903703">
La Jolla, CA 92093-0114
</address>
<email confidence="0.971035">
dhutches@ucsd.edu
</email>
<sectionHeader confidence="0.730373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999018831460674">
In this paper we discuss the results of experiments
which use a context, essentially an ordered set of
lexical items, as the seed from which to build a
network representing statistically important rela-
tionships among lexical items in some corpus. A
metric is then applied to the nodes in the network
in order to discover those pairs of items related by
high indices of similarity. The goal of this research
is to instantiate a class of items corresponding to
each item in the priming context. We believe that
this instantiation process is ultimately a special
case of abstraction over the entire network; in this
abstraction, similar nodes are collapsed into meta-
nodes which may then function as if they were sin-
gle lexical items.
I. Motivation and Background
With respect to the processing of language,
one of the tasks at which human beings seem rel-
atively adept is the ability to determine when it is
appropriate to make generalizations and when it is
appropriate to preserve distinctions. The process
of abstraction and knowing when it might reason-
ably be used is a necessary tool in reducing the
complexity of the task of processing natural lan-
guage. Part of our current research is an investi-
gation into how the process of abstraction might
be realized using relatively low-level statistical in-
formation extracted from large textual corpora.
Our experiments are an attempt to discover
a method by which class information about the
members of some sequence of lexical items may
be obtained using strictly statistical methods. For
our purposes, the class to which a lexical item be-
longs is defined by its instantiation. Given some
context such as he walked across the room, we
would like to be able to instantiate classes of items
corresponding to each item in the context (e.g., the
class associated with walked might include items
such as paced, stepped, or sauntered).
The corpora used in our experiments are the
Lancaster-Oslo-Bergen (LOB) corpus and a sub-
set of the ACL/DCI Wall Street Journal (WSJ)
corpus. The LOB corpus consists of a total
of 1,008,035 words, composed of 49,174 unique
words. The subset of the WSJ corpus that we
use has been pre-processed such that all letters
are folded to lower case, and numbers have been
collapsed to a single token; the subset consists of
18,188,548 total words and 159,713 unique words.
H. Context Priming
It is not an uncommon notion that a word
may be defined not rigourously as by the as-
signment of static syntactic and semantic classes,
but dynamically as a function of its usage (Firth
1957, 11). Such usage may be derived from co-
occurrence information over the course of a large
body of text. For each unique lexical item in a cor-
pus, there exists an &amp;quot;association neighbourhood&amp;quot;
in which that item lives; such a neighbourhood
is the probability distribution of the words with
which the item has co-occurred. If one posits that
similar lexical items will have similar neighbour-
hoods, one possible method of instantiating a class
of lexical items would be to examine all unique
items in a corpus and find those whose neighbour-
hoods are most similar to the neighbourhood of
the item whose class is being instantiated. How-
ever, the potential computational problems of such
an approach are clear. In the context of our ap-
proach to this problem, most lexical items in the
search space are not even remotely similar to the
item for which a class is being instantiated. Fur-
thermore, a substantial part of a lexical item&apos;s as-
sociation neighbourhood provides only superficial
information about that item. What is required
is a process whereby the search space is reduced
dramatically. One method of accomplishing this
pruning is via context priming.
In context priming, we view a context as the
seed upon which to build a network describing that
part of the corpus which is, in some sense, close
to the context. Thus, just as an individual lexical
item has associated with it a unique neighbour-
hood, so too does a context have such a neigh-
bourhood. The basic process of building a net-
work is straightforward. Each item in the priming
context has associated with it a unique neighbour-
hood defined in terms of those lexical items with
which it has co-occurred. Similarly, each of these
</bodyText>
<page confidence="0.981549">
292
</page>
<bodyText confidence="0.99417327027027">
latter items also has a unique association neigh-
bourhood. Generating a network based on some
context consists in simply expanding nodes (lexi-
cal items) further and further away from the con-
text until some threshold, called the depth of the
network, is reached.
Just as we prune the total set of unique lexical
items by context priming, we also prune the neigh-
bourhood of each node in the network by using a
statistical metric which provides some indication
of how important the relationship is between each
lexical item and the items in its neighbourhood.
In the results we describe here, we use mutual in-
formation (Fano 1961, 27-28; Church and Hanks
1990) as the metric for neighbourhood pruning,
pruning which occurs as the network is being gen-
erated. Yet, another parameter controlling the
topology of the network is the extent of the &amp;quot;win-
dow&amp;quot; which defines the neighbourhood of a lexi-
cal item (e.g., does the neighbourhood of a lexical
item consist of only those items which have co-
occurred at a distance of up to 3, 5, 10, or 1000
words from the item).
III. Operations on the Network
The network primed by a context consists
merely of those lexical items which are closely
reachable via co-occurrence from the priming con-
text. Nodes in the network are lexical items; arcs
represent co-occurrence relations and carry the
value of the statistical metric mentioned above
and the distance of co-occurrence. With such a
network we attempt to approximate the statisti-
cally relevant neighbourhood in which a particular
context might be found.
In the tests performed on the network thus
far we use the similarity metric
A n B12
</bodyText>
<equation confidence="0.8995805">
S(x, y) =
IA U BI
</equation>
<bodyText confidence="0.957740842105263">
where x and y are two nodes representing lexical
items, the neighbourhoods of which are expressed
as the sets of arcs A and B respectively. The met-
ric S is thus defined in terms of the cardinalities of
sets of arcs. Two arcs are said to be equal if they
reference (point to) the same lexical item at the
same offset distance. Our metric is a modification
of the Tanimoto coefficient (Bensch and Savitch
1992); the numerator is squared in order to assign
a higher index of similarity to those nodes which
have a higher percentage of arcs in common.
Our first set of tests concentrated directly on
items in the seed context. Using the metric above,
we attempted to instantiate classes of lexical items
for each item in the context. In those cases where
there were matches, the results were often encour-
aging. For example, in the LOB corpus, using the
seed context John walked across the room, a net-
work depth of 6, a mutual information threshold
of 6.0 for neighbourhood pruning, and a window
of 5, for the item John, we instantiated the class
{Edward, David, Charles, Thomas} . A similar test
on the WSJ corpus yielded the following class for
john
{
robert,william,james,charles,
richard,paul,thomas,edward,david,
donald,daniel,frank,michael,dennis,
joseph,j im,alan,dan,roger
Recall that the subset of the WSJ corpus we use
has had all items folded to lower case as part of
the pre-processing phase, thus all items in an in-
stantiated class will also be folded to lower case.
In other tests, the instantiated classes were
less satisfying, such as the following class gener-
ated for wife using the parameters above, the
LOB, and the context his wife walked across
the room
</bodyText>
<equation confidence="0.932513">
1 mouth,f ather,uncle,lordship,
f ingers,mother,husband,f ather&apos; s,
shoulder,mother &apos; s,brother
</equation>
<bodyText confidence="0.9563666">
In still other cases, a class could not be instan-
tiated at all, typically for items whose neigh-
bourhoods were too small to provide meaningful
matching information.
IV. Abstraction
It is clear that even the most perfectly derived
lexical classes will have members in common. The
different senses of bank are often given as the clas-
sic example of a lexically ambiguous word. From
our own data, we observed this problem because of
our preprocessing of the WSJ corpus; the instan-
tiation of the class associated with mark included
some proper names, but also included items such
as marks, currencies, yen, and dollar, a con-
founding of class information that would not have
occurred had not case folding taken place. Ide-
ally, it would be useful if a context could be made
to exert a more constraining influence during the
course of instantiating classes. For example, if it
is reasonably clear from a context, such as mark
loves mary, that the &amp;quot;mark&amp;quot; in question is the
human rather than the financial variety, how may
we ensure that the context provides the proper
constraining information if loves has never co-
occurred with mark in the original corpus?
</bodyText>
<page confidence="0.996678">
293
</page>
<bodyText confidence="0.999971923076923">
In the case of the ambiguous mark above,
while this item does not appear in the neighbour-
hood of loves, other lexical items do (e.g., every-
one, who, him, mr), items which may be members
of a class associated with mark. What is proposed,
then, is to construct incrementally classes of items
over the network, such that these classes may then
function as a single item for the purpose of deriv-
ing indices of similarity. In this way, we would
not be looking for a specific match between mark
and loves, but rather a match among items in
the same class as mark, items in the same class as
loves, and items in the same class as mary. With
this in mind, our second set of experiments con-
centrated not specifically on items in the priming
context, but on the entire network, searching for
candidate items to be collapsed into meta-nodes
representing classes of items.
Our initial experiments in the generation of
pairs of items which could be collapsed into meta-
nodes were more successful than the tests based
on items in the priming context. Using the LOB
corpus, the same parameters as before, and the
priming context John walked across the room,
the following set of pairs represents some of the
good matches over the generated network.
</bodyText>
<construct confidence="0.555363666666667">
(15,20),(dont &apos;t,didn&apos;t),(3,4),(her,his),
(minutes ,days),(three,f ive),(few, five),
(2,3),(f ig,table),(days,years),(40,50),
(me,him),(three,f ew),(4,5),(50,100),
(currants,sultanas),(sultanas,raisins),
(currants,raisins),...
</construct>
<bodyText confidence="0.989818">
Using the WSJ corpus, again the same parameters,
and the context john walked across the room,
part of the set of good matches generated was
(months,weeks),(rose,fell),(days,weeks),
(single-a-plus,triple-b-plus),
(single-a-minus,triple-b-plus),
(lawsuit,complaint),(analyst,economist)
( j ohn,robert),(next,past),(six,f ive),
(lower,higher),(goodyear,f irestone),
(prof it,loss),(billion,million),
(june,march),(concedes,acknowledges),
(days,weeks),(months,years),...
It should be noted that the sets given above repre-
sent the best good matches. Empirically, we found
that a value of S&gt; 1.0 tends to produce the most
meaningful pairings. At S &lt; 1.0, the amount of
&amp;quot;noisy&amp;quot; pairings increases dramatically. This is
not an absolute threshold, however, as apparently
unacceptable pairings do occur at S &gt; 1.0, such
as, for example, the pairs (catching,teamed),
(accumulating, rebuffed), and (f ather, mind).
</bodyText>
<sectionHeader confidence="0.447053" genericHeader="categories and subject descriptors">
V. Future Research
</sectionHeader>
<bodyText confidence="0.999562035714286">
The results of our initial experiments in gen-
erating classes of lexical items are encouraging,
though not conclusive. We believe that by in-
crementally collapsing pairs of very similar items
into meta-nodes, we may accomplish a kind of ab-
straction over the network which will ultimately
allow the more accurate instantiation of classes
for the priming context. The notion of incremen-
tally merging classes of lexical items is intuitively
satisfying and is explored in detail in (Brown,
et al. 1992). The approach taken in the cited
work is somewhat different than ours and while
our method is no less computationally complex
than that of Brown, et al., we believe that it is
somewhat more manageable because of the prun-
ing effect provided by context priming. On the
other hand, unlike the work described by Brown,
et al., we as yet have no clear criterion for stopping
the merging process, save an arbitrary threshold.
Finally, it should be noted that our goal is not,
strictly speaking, to generate classes over an entire
vocabulary, but only that portion of the vocabu-
lary relevant for a particular context. It is hoped
that, by priming with a context, we may be able to
effect some manner of word sense disambiguation
in those cases where the meaning of a potentially
ambiguous item may be resolved by hints in the
context.
</bodyText>
<reference confidence="0.974058047619048">
VI. References
Bensch, Peter A. and Walter J. Savitch. 1992.
&amp;quot;An Occurrence-Based Model of Word Cat-
egorization&amp;quot;. Third Meeting on Mathemat-
ics of Language. Austin, Texas: Association
for Computational Linguistics, Special Inter-
est Group on the Mathematics of Language.
Brown, Peter F., et al. 1992. &amp;quot;Class-Based n-
gram Models of Natural Language&amp;quot;. Compu-
tational Linguistics 18.4: 467-479.
Church, Kenneth Ward, and Patrick Hanks. 1990.
&amp;quot;Word Association Norms, Mutual Informa-
tion, and Lexicography&amp;quot;. Computational Lin-
guistics 16.1: 22-29.
Fano, Robert M. 1961. Transmission of Infor-
mation: A Statistical Theory of Communica-
tions. New York: MIT Press.
Firth, J[ohn] R[upert]. 1957. &amp;quot;A Synopsis of Lin-
guistic Theory, 1930-55.&amp;quot; Studies in Linguis-
tic Analysis. Philological Society, London.
Oxford, England: Basil Blackwell. 1-32.
</reference>
<page confidence="0.998399">
294
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002430">
<title confidence="0.996663">RAISINS, SULTANAS, AND CURRANTS: LEXICAL CLASSIFICATION AND ABSTRACTION VIA CONTEXT PRIMING</title>
<author confidence="0.999997">David J Hutches</author>
<affiliation confidence="0.9820465">Department of Computer Science and Engineering, Mail Code 0114 University of California, San Diego</affiliation>
<address confidence="0.998864">La Jolla, CA 92093-0114</address>
<email confidence="0.999806">dhutches@ucsd.edu</email>
<abstract confidence="0.999188419354838">In this paper we discuss the results of experiments which use a context, essentially an ordered set of lexical items, as the seed from which to build a network representing statistically important relationships among lexical items in some corpus. A metric is then applied to the nodes in the network in order to discover those pairs of items related by high indices of similarity. The goal of this research is to instantiate a class of items corresponding to each item in the priming context. We believe that this instantiation process is ultimately a special case of abstraction over the entire network; in this abstraction, similar nodes are collapsed into metanodes which may then function as if they were single lexical items. and Background With respect to the processing of language, one of the tasks at which human beings seem relatively adept is the ability to determine when it is appropriate to make generalizations and when it is appropriate to preserve distinctions. The process of abstraction and knowing when it might reasonably be used is a necessary tool in reducing the complexity of the task of processing natural language. Part of our current research is an investigation into how the process of abstraction might be realized using relatively low-level statistical information extracted from large textual corpora. Our experiments are an attempt to discover a method by which class information about the members of some sequence of lexical items may be obtained using strictly statistical methods. For our purposes, the class to which a lexical item belongs is defined by its instantiation. Given some such as walked across the we would like to be able to instantiate classes of items corresponding to each item in the context (e.g., the associated with include items as stepped, The corpora used in our experiments are the Lancaster-Oslo-Bergen (LOB) corpus and a subset of the ACL/DCI Wall Street Journal (WSJ) corpus. The LOB corpus consists of a total of 1,008,035 words, composed of 49,174 unique words. The subset of the WSJ corpus that we use has been pre-processed such that all letters are folded to lower case, and numbers have been collapsed to a single token; the subset consists of 18,188,548 total words and 159,713 unique words. Priming It is not an uncommon notion that a word may be defined not rigourously as by the assignment of static syntactic and semantic classes, but dynamically as a function of its usage (Firth 1957, 11). Such usage may be derived from cooccurrence information over the course of a large body of text. For each unique lexical item in a corpus, there exists an &amp;quot;association neighbourhood&amp;quot; in which that item lives; such a neighbourhood is the probability distribution of the words with which the item has co-occurred. If one posits that similar lexical items will have similar neighbourhoods, one possible method of instantiating a class of lexical items would be to examine all unique items in a corpus and find those whose neighbourhoods are most similar to the neighbourhood of the item whose class is being instantiated. However, the potential computational problems of such an approach are clear. In the context of our approach to this problem, most lexical items in the search space are not even remotely similar to the item for which a class is being instantiated. Furthermore, a substantial part of a lexical item&apos;s association neighbourhood provides only superficial information about that item. What is required is a process whereby the search space is reduced dramatically. One method of accomplishing this pruning is via context priming. In context priming, we view a context as the seed upon which to build a network describing that part of the corpus which is, in some sense, close to the context. Thus, just as an individual lexical item has associated with it a unique neighbourhood, so too does a context have such a neighbourhood. The basic process of building a network is straightforward. Each item in the priming context has associated with it a unique neighbourhood defined in terms of those lexical items with which it has co-occurred. Similarly, each of these 292 latter items also has a unique association neighbourhood. Generating a network based on some context consists in simply expanding nodes (lexical items) further and further away from the context until some threshold, called the depth of the network, is reached. Just as we prune the total set of unique lexical items by context priming, we also prune the neighbourhood of each node in the network by using a statistical metric which provides some indication of how important the relationship is between each lexical item and the items in its neighbourhood. In the results we describe here, we use mutual information (Fano 1961, 27-28; Church and Hanks 1990) as the metric for neighbourhood pruning, pruning which occurs as the network is being generated. Yet, another parameter controlling the topology of the network is the extent of the &amp;quot;window&amp;quot; which defines the neighbourhood of a lexical item (e.g., does the neighbourhood of a lexical item consist of only those items which have cooccurred at a distance of up to 3, 5, 10, or 1000 words from the item). III. Operations on the Network The network primed by a context consists merely of those lexical items which are closely reachable via co-occurrence from the priming context. Nodes in the network are lexical items; arcs represent co-occurrence relations and carry the value of the statistical metric mentioned above and the distance of co-occurrence. With such a network we attempt to approximate the statistically relevant neighbourhood in which a particular context might be found. In the tests performed on the network thus far we use the similarity metric An = IA U BI y are two nodes representing lexical items, the neighbourhoods of which are expressed the sets of arcs A and The metthus defined in terms of the cardinalities of sets of arcs. Two arcs are said to be equal if they reference (point to) the same lexical item at the same offset distance. Our metric is a modification of the Tanimoto coefficient (Bensch and Savitch 1992); the numerator is squared in order to assign a higher index of similarity to those nodes which have a higher percentage of arcs in common. Our first set of tests concentrated directly on items in the seed context. Using the metric above, we attempted to instantiate classes of lexical items for each item in the context. In those cases where there were matches, the results were often encouraging. For example, in the LOB corpus, using the context walked across the room, network depth of 6, a mutual information threshold of 6.0 for neighbourhood pruning, and a window 5, for the item instantiated the class David, Charles, Thomas} . A test on the WSJ corpus yielded the following class for john { robert,william,james,charles, richard,paul,thomas,edward,david, donald,daniel,frank,michael,dennis, joseph,j im,alan,dan,roger Recall that the subset of the WSJ corpus we use has had all items folded to lower case as part of the pre-processing phase, thus all items in an instantiated class will also be folded to lower case. In other tests, the instantiated classes were less satisfying, such as the following class generfor the parameters above, the and the context wife walked across the room 1 mouth,f ather,uncle,lordship, f ingers,mother,husband,f ather&apos; s, shoulder,mother &apos; s,brother other cases, a class could not be instantiated at all, typically for items whose neighbourhoods were too small to provide meaningful matching information. IV. Abstraction It is clear that even the most perfectly derived lexical classes will have members in common. The senses of often given as the classic example of a lexically ambiguous word. From our own data, we observed this problem because of our preprocessing of the WSJ corpus; the instanof the class associated with some proper names, but also included items such currencies, yen, and dollar, confounding of class information that would not have occurred had not case folding taken place. Ideally, it would be useful if a context could be made to exert a more constraining influence during the course of instantiating classes. For example, if it reasonably clear from a context, such as mary, that the &amp;quot;mark&amp;quot; question is the human rather than the financial variety, how may we ensure that the context provides the proper information if never cowith in original corpus? 293 In the case of the ambiguous mark above, while this item does not appear in the neighbourof lexical items do (e.g., everywho, him, mr), which may be members a class associated with is proposed, then, is to construct incrementally classes of items over the network, such that these classes may then function as a single item for the purpose of deriving indices of similarity. In this way, we would be looking for a specific match between rather a match among items in the same class as mark, items in the same class as items in the same class as mary. With this in mind, our second set of experiments concentrated not specifically on items in the priming context, but on the entire network, searching for candidate items to be collapsed into meta-nodes representing classes of items. Our initial experiments in the generation of pairs of items which could be collapsed into metanodes were more successful than the tests based on items in the priming context. Using the LOB corpus, the same parameters as before, and the context walked across the room, the following set of pairs represents some of the good matches over the generated network. (15,20),(dont &apos;t,didn&apos;t),(3,4),(her,his), (minutes ,days),(three,f ive),(few, five), (2,3),(f ig,table),(days,years),(40,50), (me,him),(three,f ew),(4,5),(50,100), (currants,sultanas),(sultanas,raisins), (currants,raisins),... Using the WSJ corpus, again the same parameters, the context walked across the room, part of the set of good matches generated was (months,weeks),(rose,fell),(days,weeks), (single-a-plus,triple-b-plus), (single-a-minus,triple-b-plus), (lawsuit,complaint),(analyst,economist) ( j ohn,robert),(next,past),(six,f ive), (lower,higher),(goodyear,f irestone), (prof it,loss),(billion,million), (june,march),(concedes,acknowledges), (days,weeks),(months,years),... It should be noted that the sets given above represent the best good matches. Empirically, we found a value of tends to produce the most pairings. At 1.0, the amount of &amp;quot;noisy&amp;quot; pairings increases dramatically. This is not an absolute threshold, however, as apparently pairings do occur at &gt; such for example, the pairs rebuffed), ather, mind). V. Future Research The results of our initial experiments in generating classes of lexical items are encouraging, though not conclusive. We believe that by incrementally collapsing pairs of very similar items into meta-nodes, we may accomplish a kind of abstraction over the network which will ultimately allow the more accurate instantiation of classes for the priming context. The notion of incrementally merging classes of lexical items is intuitively satisfying and is explored in detail in (Brown, et al. 1992). The approach taken in the cited work is somewhat different than ours and while our method is no less computationally complex than that of Brown, et al., we believe that it is somewhat more manageable because of the pruning effect provided by context priming. On the other hand, unlike the work described by Brown, et al., we as yet have no clear criterion for stopping the merging process, save an arbitrary threshold. Finally, it should be noted that our goal is not, strictly speaking, to generate classes over an entire vocabulary, but only that portion of the vocabulary relevant for a particular context. It is hoped that, by priming with a context, we may be able to effect some manner of word sense disambiguation in those cases where the meaning of a potentially ambiguous item may be resolved by hints in the context.</abstract>
<note confidence="0.843873">VI. References Bensch, Peter A. and Walter J. Savitch. 1992.</note>
<title confidence="0.63921425">amp;quot;An Occurrence-Based Model of Word Cat- Meeting on Mathematof Language. Texas: Association for Computational Linguistics, Special Inter-</title>
<affiliation confidence="0.765261">est Group on the Mathematics of Language.</affiliation>
<address confidence="0.469825">Brown, Peter F., et al. 1992. &amp;quot;Class-Based n-</address>
<note confidence="0.536693357142857">Models of Natural Language&amp;quot;. Compu- Linguistics 467-479. Church, Kenneth Ward, and Patrick Hanks. 1990. &amp;quot;Word Association Norms, Mutual Informaand Lexicography&amp;quot;. Lin- 22-29. Robert M. 1961. of Infor- Theory of Communica- York: MIT Press. Firth, J[ohn] R[upert]. 1957. &amp;quot;A Synopsis of Lin- Theory, 1930-55.&amp;quot; in Linguis- Analysis. Society, London. Oxford, England: Basil Blackwell. 1-32. 294</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<publisher>References</publisher>
<marker></marker>
<rawString>VI. References</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Bensch</author>
<author>Walter J Savitch</author>
</authors>
<title>An Occurrence-Based Model of Word Categorization&amp;quot;. Third Meeting on Mathematics of Language.</title>
<date>1992</date>
<journal>Computational Linguistics, Special Interest Group on the Mathematics of Language.</journal>
<publisher>Association for</publisher>
<location>Austin, Texas:</location>
<contexts>
<context position="6547" citStr="Bensch and Savitch 1992" startWordPosition="1108" endWordPosition="1111"> we attempt to approximate the statistically relevant neighbourhood in which a particular context might be found. In the tests performed on the network thus far we use the similarity metric A n B12 S(x, y) = IA U BI where x and y are two nodes representing lexical items, the neighbourhoods of which are expressed as the sets of arcs A and B respectively. The metric S is thus defined in terms of the cardinalities of sets of arcs. Two arcs are said to be equal if they reference (point to) the same lexical item at the same offset distance. Our metric is a modification of the Tanimoto coefficient (Bensch and Savitch 1992); the numerator is squared in order to assign a higher index of similarity to those nodes which have a higher percentage of arcs in common. Our first set of tests concentrated directly on items in the seed context. Using the metric above, we attempted to instantiate classes of lexical items for each item in the context. In those cases where there were matches, the results were often encouraging. For example, in the LOB corpus, using the seed context John walked across the room, a network depth of 6, a mutual information threshold of 6.0 for neighbourhood pruning, and a window of 5, for the ite</context>
</contexts>
<marker>Bensch, Savitch, 1992</marker>
<rawString>Bensch, Peter A. and Walter J. Savitch. 1992. &amp;quot;An Occurrence-Based Model of Word Categorization&amp;quot;. Third Meeting on Mathematics of Language. Austin, Texas: Association for Computational Linguistics, Special Interest Group on the Mathematics of Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
</authors>
<title>Class-Based ngram Models of Natural Language&amp;quot;.</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<pages>467--479</pages>
<marker>Brown, 1992</marker>
<rawString>Brown, Peter F., et al. 1992. &amp;quot;Class-Based ngram Models of Natural Language&amp;quot;. Computational Linguistics 18.4: 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography&amp;quot;.</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<pages>22--29</pages>
<contexts>
<context position="5152" citStr="Church and Hanks 1990" startWordPosition="862" endWordPosition="865">ociation neighbourhood. Generating a network based on some context consists in simply expanding nodes (lexical items) further and further away from the context until some threshold, called the depth of the network, is reached. Just as we prune the total set of unique lexical items by context priming, we also prune the neighbourhood of each node in the network by using a statistical metric which provides some indication of how important the relationship is between each lexical item and the items in its neighbourhood. In the results we describe here, we use mutual information (Fano 1961, 27-28; Church and Hanks 1990) as the metric for neighbourhood pruning, pruning which occurs as the network is being generated. Yet, another parameter controlling the topology of the network is the extent of the &amp;quot;window&amp;quot; which defines the neighbourhood of a lexical item (e.g., does the neighbourhood of a lexical item consist of only those items which have cooccurred at a distance of up to 3, 5, 10, or 1000 words from the item). III. Operations on the Network The network primed by a context consists merely of those lexical items which are closely reachable via co-occurrence from the priming context. Nodes in the network are</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, Kenneth Ward, and Patrick Hanks. 1990. &amp;quot;Word Association Norms, Mutual Information, and Lexicography&amp;quot;. Computational Linguistics 16.1: 22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Fano</author>
</authors>
<title>Transmission of Information: A Statistical Theory of Communications.</title>
<date>1961</date>
<publisher>MIT Press.</publisher>
<location>New York:</location>
<contexts>
<context position="5121" citStr="Fano 1961" startWordPosition="859" endWordPosition="860">o has a unique association neighbourhood. Generating a network based on some context consists in simply expanding nodes (lexical items) further and further away from the context until some threshold, called the depth of the network, is reached. Just as we prune the total set of unique lexical items by context priming, we also prune the neighbourhood of each node in the network by using a statistical metric which provides some indication of how important the relationship is between each lexical item and the items in its neighbourhood. In the results we describe here, we use mutual information (Fano 1961, 27-28; Church and Hanks 1990) as the metric for neighbourhood pruning, pruning which occurs as the network is being generated. Yet, another parameter controlling the topology of the network is the extent of the &amp;quot;window&amp;quot; which defines the neighbourhood of a lexical item (e.g., does the neighbourhood of a lexical item consist of only those items which have cooccurred at a distance of up to 3, 5, 10, or 1000 words from the item). III. Operations on the Network The network primed by a context consists merely of those lexical items which are closely reachable via co-occurrence from the priming co</context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, Robert M. 1961. Transmission of Information: A Statistical Theory of Communications. New York: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory, 1930-55.&amp;quot; Studies in Linguistic Analysis.</title>
<date>1957</date>
<pages>1--32</pages>
<publisher>Philological Society,</publisher>
<location>London. Oxford, England:</location>
<contexts>
<context position="2807" citStr="Firth 1957" startWordPosition="463" endWordPosition="464">gen (LOB) corpus and a subset of the ACL/DCI Wall Street Journal (WSJ) corpus. The LOB corpus consists of a total of 1,008,035 words, composed of 49,174 unique words. The subset of the WSJ corpus that we use has been pre-processed such that all letters are folded to lower case, and numbers have been collapsed to a single token; the subset consists of 18,188,548 total words and 159,713 unique words. H. Context Priming It is not an uncommon notion that a word may be defined not rigourously as by the assignment of static syntactic and semantic classes, but dynamically as a function of its usage (Firth 1957, 11). Such usage may be derived from cooccurrence information over the course of a large body of text. For each unique lexical item in a corpus, there exists an &amp;quot;association neighbourhood&amp;quot; in which that item lives; such a neighbourhood is the probability distribution of the words with which the item has co-occurred. If one posits that similar lexical items will have similar neighbourhoods, one possible method of instantiating a class of lexical items would be to examine all unique items in a corpus and find those whose neighbourhoods are most similar to the neighbourhood of the item whose cla</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>Firth, J[ohn] R[upert]. 1957. &amp;quot;A Synopsis of Linguistic Theory, 1930-55.&amp;quot; Studies in Linguistic Analysis. Philological Society, London. Oxford, England: Basil Blackwell. 1-32.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>