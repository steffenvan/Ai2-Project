<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002614">
<title confidence="0.9975645">
The Potential and Limitations of Automatic
Sentence Extraction for Summarization
</title>
<author confidence="0.985949">
Chin-Yew Lin and Eduard Hovy
</author>
<affiliation confidence="0.996525">
University of Southern California/Information Sciences Institute
</affiliation>
<address confidence="0.877238">
4676 Admiralty Way
Marina del Rey, CA 90292, USA
</address>
<email confidence="0.99965">
{cyl,hovy}@isi.edu
</email>
<sectionHeader confidence="0.99861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999604722222223">
In this paper we present an empirical study of
the potential and limitation of sentence extrac-
tion in text summarization. Our results show
that the single document generic summariza-
tion task as defined in DUC 2001 needs to be
carefully refocused as reflected in the low in-
ter-human agreement at 100-word 1 (0.40
score) and high upper bound at full text 2
(0.88) summaries. For 100-word summaries,
the performance upper bound, 0.65, achieved
oracle extracts3. Such oracle extracts show the
promise of sentence extraction algorithms;
however, we first need to raise inter-human
agreement to be able to achieve this perform-
ance level. We show that compression is a
promising direction and that the compression
ratio of summaries affects average human and
system performance.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99952225">
Most automatic text summarization systems existing
today are extraction systems that extract parts of origi-
nal documents and output the results as summaries.
Among them, sentence extraction is by far the most
</bodyText>
<footnote confidence="0.997951">
1 We compute unigram co-occurrence score of a pair of man-
ual summaries, one as candidate summary and the other as
reference.
2 We compute unigram co-occurrence scores of a full text and
its manual summaries of 100 words. These scores are the best
achievable using the unigram co-occurrence scoring metric
since all possible words are contained in the full text. Three
manual summaries are used.
3 Oracle extracts are the best scoring extracts generated by
exhaustive search of all possible sentence combinations of
100±5 words.
</footnote>
<bodyText confidence="0.999255888888889">
popular (Edmundson 1969, Luhn 1969, Kupiec et al.
1995, Goldstein et al. 1999, Hovy and Lin 1999). The
majority of systems participating in the past Document
Understanding Conference (DUC 2002), a large scale
summarization evaluation effort sponsored by the US
government, are extraction based. Although systems
based on information extraction (Radev and McKeown
1998, White et al. 2001, McKeown et al. 2002) and dis-
course analysis (Marcu 1999b, Strzalkowski et al. 1999)
also exist, we focus our study on the potential and limi-
tations of sentence extraction systems with the hope that
our results will further progress in most of the automatic
text summarization systems and evaluation setup.
The evaluation results of the single document summari-
zation task in DUC 2001 and 2002 (DUC 2002, Paul &amp;
Liggett 2002) indicate that most systems are as good as
the baseline lead-based system and that humans are sig-
nificantly better, though not by much. This leads to the
belief that lead-based summaries are as good as we can
get for single document summarization in the news
genre, implying that the research community should
invest future efforts in other areas. In fact, a very short
summary of about 10 words (headline-like) task has
replaced the single document 100-word summary task
in DUC 2003. The goal of this study is to renew interest
in sentence extraction-based summarization and its
evaluation by estimating the performance upper bound
using oracle extracts, and to highlight the importance of
taking into account the compression ratio when we
evaluate extracts or summaries.
Section 2 gives an overview of DUC relevant to this
study. Section 3 introduces a recall-based unigram co-
occurrence automatic evaluation metric. Section 4 pre-
sents the experimental design. Section 5 shows the em-
pirical results. Section 6 concludes this paper and
discusses future directions.
</bodyText>
<sectionHeader confidence="0.981815" genericHeader="method">
2 Document Understanding Conference
</sectionHeader>
<bodyText confidence="0.9999155">
Fully automatic single-document summarization was
one of two main tasks in the 2001 Document Under-
standing Conference. Participants were required to cre-
ate a generic 100-word summary. There were 30 test
sets in DUC 2001 and each test set contained about 10
documents. For each document, one summary was cre-
ated manually as the ëidealí model summary at ap-
proximately 100 words. We will refer to this manual
summary as H1. Two other manual summaries were
also created at about that length. We will refer to these
two additional human summaries as H2 and H3. In addi-
tion, baseline summaries were created automatically by
taking the first n sentences up to 100 words. We will
refer this baseline extract as B1.
</bodyText>
<sectionHeader confidence="0.993946" genericHeader="method">
3 Unigram Co-Occurrence Metric
</sectionHeader>
<bodyText confidence="0.998106153846154">
In a recent study (Lin and Hovy 2003), we showed that
the recall-based unigram co-occurrence automatic scor-
ing metric correlated highly with human evaluation and
has high recall and precision in predicting statistical
significance of results comparing with its human coun-
terpart. The idea is to measure the content similarity
between a system extract and a manual summary using
simple n-gram overlap. A similar idea called IBM
BLEU score has proved successful in automatic ma-
chine translation evaluation (Papineni et al. 2001, NIST
2002). For summarization, we can express the degree of
content overlap in terms of n-gram matches as the fol-
lowing equation:
</bodyText>
<equation confidence="0.971011818181818">
Count n gram
( − )
match
C Model Units
∈{ } n gram C
− ∈
∑ ∑ Count n gram
( − )
C Model Units
∈{ } n gram C
− ∈
</equation>
<bodyText confidence="0.999686304347826">
Model units are segments of manual summaries. They
are typically either sentences or elementary discourse
units as defined by Marcu (1999b). Countmatch(n-gram)
is the maximum number of n-grams co-occurring in a
system extract and a model unit. Count(n-gram) is the
number of n-grams in the model unit. Notice that the
average n-gram coverage score, Cn, as shown in equa-
tion 1, is a recall-based metric, since the denominator of
equation 1 is the sum total of the number of n-grams
occurring in the model summary instead of the system
summary and only one model summary is used for each
evaluation. In summary, the unigram co-occurrence
statistics we use in the following sections are based on
the following formula:
Where j ≥ i, i and j range from 1 to 4, and wn is 1/(j-
i+1). Ngram(1, 4) is a weighted variable length n-gram
match score similar to the IBM BLEU score; while
Ngram(k, k), i.e. i = j = k, is simply the average k-gram
co-occurrence score Ck. In this study, we set i = j = 1,
i.e. unigram co-occurrence score.
With a test collection available and an automatic scoring
metric defined, we describe the experimental setup in
the next section.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="method">
4 Experimental Designs
</sectionHeader>
<bodyText confidence="0.999989">
As stated in the introduction, we aim to find the per-
formance upper bound of a sentence extraction system
and the effect of compression ratio on its performance.
We present our experimental designs to address these
questions in the following sections.
</bodyText>
<subsectionHeader confidence="0.9992625">
4.1 Performance Upper Bound Estimation
Using Oracle Extract
</subsectionHeader>
<bodyText confidence="0.999529388888889">
In order to estimate the potential of sentence extraction
systems, it is important to know the upper bound that an
ideal sentence extraction method might achieve and
how far the state-of-the-art systems are away from the
bound. If the upper bound is close to state-of-the-art
systemsí performance then we need to look for other
summarization methods to improve performance. If the
upper bound is much higher than any current systems
can achieve, then it is reasonable to invest more effort in
sentence extraction methods. The question is how to
estimate the performance upper bound. Our solution is
to cast this estimation problem as an optimization prob-
lem. We exhaustively generate all possible sentence
combinations that satisfy given length constraints for a
summary, for example, all the sentence combinations
totaling 100±5 words. We then compute the unigram
co-occurrence score for each sentence combination,
against the ideal. The best combinations are the ones
with the highest unigram co-occurrence score. We call
this sentence combination the oracle extract. Figure 1
shows an oracle extract for document AP900424-0035.
One of its human summaries is shown in Figure 2. The
oracle extract covers almost all aspects of the human
summary except sentences 5 and 6 and part of sentence
4. However, if we allow the automatic extract to contain
more words, for example, 150 words shown in Figure 3,
the longer oracle extract then covers everything in the
human summary. This indicates that lower compression
can boost system performance. The ultimate effect of
compression can be computed using the full text as the
oracle extract, since the full text should contain every-
thing included in the human summary. That situation
provides the best achievable unigram co-occurrence
score. A near optimal score also confirms the validity of
using the unigram co-occurrence scoring method as an
automatic evaluation method.
</bodyText>
<equation confidence="0.966426222222222">
j
 
( , ) exp
i j = ∑=
 w n C n
log 
 n i 
Ngram
(
</equation>
<page confidence="0.959217">
2
</page>
<figure confidence="0.98869384375">
)
∑ ∑
Cn
(
1
)
&lt;DOC&gt;
&lt;DOCNO&gt;AP900424-0035&lt;/DOCNO&gt;
&lt;DATE&gt;04/24/90&lt;/DATE&gt;
&lt;HEADLINE&gt;
&lt;S HSNTNO=&amp;quot;1&amp;quot;&gt;Elizabeth Taylor in Intensive Care Unit&lt;/S&gt;
&lt;S HSNTNO=&amp;quot;2&amp;quot;&gt;By JEFF WILSON&lt;/S&gt;
&lt;S HSNTNO=&amp;quot;3&amp;quot;&gt;Associated Press Writer&lt;/S&gt;
&lt;S HSNTNO=&amp;quot;4&amp;quot;&gt;SANTA MONICA, Calif. (AP)&lt;/S&gt;
&lt;/HEADLINE&gt;
&lt;TEXT&gt;
&lt;S SNTNO=&amp;quot;1&amp;quot;&gt;A seriously ill Elizabeth Taylor battled pneumonia at her
hospital, her breathing assisted by a ventilator, doctors say.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;2&amp;quot;&gt;Hospital officials described her condition late Monday
as stabilizing after a lung biopsy to determine the cause of the pneumo-
nia.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;3&amp;quot;&gt;Analysis of the tissue sample was expected to take until
Thursday, said her spokeswoman, Chen Sam.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;9&amp;quot;&gt;Another spokewoman for the actress, Lisa Del Favaro,
said Miss Taylor&apos;s family was at her bedside.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;13&amp;quot;&gt;``It is serious, but they are really pleased with her
progress.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;22&amp;quot;&gt;During a nearly fatal bout with pneumonia in 1961,
Miss Taylor underwent a tracheotomy, an incision into her windpipe to
help her breathe.&lt;/S&gt;
&lt;/TEXT&gt;
&lt;/DOC&gt;
</figure>
<figureCaption confidence="0.991329">
Figure 1. A 100-word oracle extract for docu-
ment AP900424-0035.
</figureCaption>
<figure confidence="0.999335052631579">
&lt;DOC&gt;
&lt;TEXT&gt;
&lt;S SNTNO=&amp;quot;1&amp;quot;&gt;Elizabeth Taylor battled pneumonia at her hospital,
assisted by a ventilator, doctors say.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;2&amp;quot;&gt;Hospital officials described her condition late Monday
as stabilizing after a lung biopsy to determine the cause of the pneumo-
nia.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;3&amp;quot;&gt;Analysis of the tissue sample was expected to be com-
plete by Thursday.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;4&amp;quot;&gt;Ms. Sam, spokeswoman said &amp;quot;it is serious, but they are
really pleased with her progress.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;5&amp;quot;&gt;She&apos;s not well.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;6&amp;quot;&gt;She&apos;s not on her deathbed or anything.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;7&amp;quot;&gt;Another spokeswoman, Lisa Del Favaro, said Miss
Taylor&apos;s family was at her bedside.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;8&amp;quot;&gt;During a nearly fatal bout with pneumonia in 1961, Miss
Taylor underwent a tracheotomy to help her breathe.&lt;/S&gt;
&lt;/TEXT&gt;
&lt;/DOC&gt;
</figure>
<figureCaption confidence="0.974699">
Figure 2. A manual summary for document
AP900424-0035.
</figureCaption>
<subsectionHeader confidence="0.9853555">
4.2 Compression Ratio and Its Effect on System
Performance
</subsectionHeader>
<bodyText confidence="0.999956833333333">
One important factor that affects the average perform-
ance of sentence extraction system is the number of
sentences contained in the original documents. This
factor is often overlooked and has never been addressed
systematically. For example, if a document contains
only one sentence then this document will not be useful
in differentiating summarization system performance ñ
there is only one choice. However, for a document of
100 sentences and assuming each sentence is 20 words
long, there are C(100,5) = 75,287,520 different 100-
word extracts. This huge search space lowers the chance
of agreement between humans on what constitutes a
</bodyText>
<figure confidence="0.998677323529412">
&lt;DOC&gt;
&lt;DOCNO&gt;AP900424-0035&lt;/DOCNO&gt;
&lt;DATE&gt;04/24/90&lt;/DATE&gt;
&lt;HEADLINE&gt;
&lt;S HSNTNO=&amp;quot;1&amp;quot;&gt;Elizabeth Taylor in Intensive Care Unit&lt;/S&gt;
&lt;S HSNTNO=&amp;quot;2&amp;quot;&gt;By JEFF WILSON&lt;/S&gt;
&lt;S HSNTNO=&amp;quot;3&amp;quot;&gt;Associated Press Writer&lt;/S&gt;
&lt;S HSNTNO=&amp;quot;4&amp;quot;&gt;SANTA MONICA, Calif. (AP)&lt;/S&gt;
&lt;/HEADLINE&gt;
&lt;TEXT&gt;
&lt;S SNTNO=&amp;quot;1&amp;quot;&gt;A seriously ill Elizabeth Taylor battled pneumonia at her
hospital, her breathing assisted by a ventilator, doctors say.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;2&amp;quot;&gt;Hospital officials described her condition late Monday
as stabilizing after a lung biopsy to determine the cause of the pneumo-
nia.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;3&amp;quot;&gt;Analysis of the tissue sample was expected to take until
Thursday, said her spokeswoman, Chen Sam.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;4&amp;quot;&gt;The 58-year-old actress, who won best-actress Oscars
for ``Butterfield 8&amp;quot;and ``Who&apos;s Afraid of Virginia Woolf,&amp;quot;has been
hospitalized more than two weeks.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;8&amp;quot;&gt;Her condition is presently stabilizing and her physicians
are pleased with her progress.&amp;quot;&lt;/S&gt;
&lt;S SNTNO=&amp;quot;9&amp;quot;&gt;Another spokewoman for the actress, Lisa Del Favaro,
said Miss Taylor&apos;s family was at her bedside.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;13&amp;quot;&gt;``It is serious, but they are really pleased with her
progress.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;14&amp;quot;&gt;She&apos;s not well.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;15&amp;quot;&gt;She&apos;s not on her deathbed or anything,&amp;quot;Ms. Sam said
late Monday.&lt;/S&gt;
&lt;S SNTNO=&amp;quot;22&amp;quot;&gt;During a nearly fatal bout with pneumonia in 1961,
Miss Taylor underwent a tracheotomy, an incision into her windpipe to
help her breathe.&lt;/S&gt;
&lt;/TEXT&gt;
&lt;/DOC&gt;
</figure>
<figureCaption confidence="0.9987155">
Figure 3. A 150-word oracle extract for docu-
ment AP900424-0035.
</figureCaption>
<bodyText confidence="0.991859857142857">
good summary. It also makes system and human per-
formance approach average since it is more likely to
include some good sentences but not all of them. Em-
pirical results shown in Section 5 confirm this and that
leads us to the question of how to construct a corpus to
evaluate summarization systems. We discuss this issue
in the conclusion section.
</bodyText>
<subsectionHeader confidence="0.9996205">
4.3 Inter-Human Agreement and Its Effect on
System Performance
</subsectionHeader>
<bodyText confidence="0.9997852">
In this section we study how inter-human agreement
affects system performance. Lin and Hovy (2002) re-
ported that, compared to a manually created ideal, hu-
mans scored about 0.40 in average coverage score and
the best system scored about 0.35. According to these
numbers, we might assume that humans cannot agree to
each other on what is important and the best system is
almost as good as humans. If this is true then estimating
an upper bound using oracle extracts is meaningless. No
matter how high the estimated upper bounds may be, we
probably would never be able to achieve that perform-
ance due to lack of agreement between humans: the
oracle approximating one human would fail miserably
with another. Therefore we set up experiments to inves-
tigate the following:
</bodyText>
<listItem confidence="0.816236">
1. What is the distribution of inter-human agree-
ment?
</listItem>
<bodyText confidence="0.947270333333333">
2. How does a state-of-the-art system differ from
average human performance at different inter-
human agreement levels?
We present our results in the next section using 303
newspaper articles from the DUC 2001 single document
summarization task. Besides the original documents, we
also have three human summaries, one lead summary
(B1), and one automatic summary from one top per-
forming system (T) for each document.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.99992275">
In order to determine the empirical upper and lower
bounds of inter-human agreement, we first ran cross-
human evaluation using unigram co-occurrence scoring
through six human summary pairs, i.e. (H1,H2),
(H1,H3), (H2,H1), (H2,H3), (H3,H1), and (H3,H2). For
a summary pair (X,Y), we used X as the model sum-
mary and Y as the system summary. Figure 4 shows the
distributions of four different scenarios. The MaxH dis-
tribution picks the best inter-human agreement scores
for each document, the MinH distribution the minimum
one, the MedH distribution the median, and the AvgH
distribution the average. The average of the best inter-
</bodyText>
<sectionHeader confidence="0.489404" genericHeader="method">
AvgH MaxH MedH MinH
</sectionHeader>
<figureCaption confidence="0.98378275">
Figure 4. DUC 2001 single document inter-
human unigram co-occurrence score distribu-
tions for maximum, minimum, average, and
median.
</figureCaption>
<bodyText confidence="0.9997576">
human agreement and the average of average inter-
human agreement differ by about 10 percent in unigram
co-occurrence score and 18 percent between MaxH and
MinH. These big differences might come from two
sources. The first one is the limitation of the unigram
</bodyText>
<figure confidence="0.9974843">
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Unigram Co-occurrence Scores
# of Instances
80
70
60
50
40
30
20
10
0
Average MIN = 0.32
Average MED = 0.39
Average AVG = 0.40
Average MAX = 0.50
Unigram Co-occurrence Scores
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
1.00
Document IDs
MaxH B1 T E100 E150 FT Avg M axH Avg B1 Avg T Avg E100 Avg E150 Avg FT
</figure>
<figureCaption confidence="0.999923111111111">
Figure 5. DUC 2001 single document inter-human, baseline, system, 100-word, 150-word, and full text
oracle extracts unigram co-occurrence score distributions (# of sentences&lt;=30). Document IDs are sorted
by decreasing MaxH.
Figure 6. DUC 2001 single document inter-
human, baseline, system, and full text unigram
co-occurrence score distributions (Set A).
Figure 7. DUC 2001 single document inter-
human, baseline, system, and full text unigram
co-occurrence score distributions (Set B).
</figureCaption>
<bodyText confidence="0.99993355">
co-occurrence scoring applied to manual summaries that
it cannot recognize synonyms or paraphrases. The sec-
ond one is the true lack of agreement between humans.
We would like to conduct an in-depth study to address
this question, and would just assume the unigram co-
occurrence scoring is reliable.
In other experiments, we used the best inter-human
agreement results as the reference point for human per-
formance upper bound. This also implied that we used
the human summary achieving the best inter-human
agreement score as our reference summary.
Figure 5 shows the unigram co-occurrence scores of
human, baseline, system T, and three oracle extraction
systems at different extraction lengths. We generated all
possible sentence combinations that satisfied 100±5
words constraints. Due to computation-intensive nature
of this task, we only used documents with fewer than 30
sentences. We then computed the unigram co-
occurrence score for each combination, selected the best
one as the oracle extraction, and plotted the score in the
</bodyText>
<figureCaption confidence="0.999816">
Figure 8. DUC 2001 single document inter-
human, baseline, system, and full text unigram
co-occurrence score distributions (Set C).
</figureCaption>
<bodyText confidence="0.999933714285714">
figure. The curve for 100±5 words oracle extractions is
the upper bound that a sentence extraction system can
achieve within the given word limit. If an automatic
system is allowed to extract more words, we can expect
that longer extracts would boost system performance.
The question is how much better and what is the ulti-
mate limit? To address these questions, we also com-
puted unigram co-occurrence scores for oracle
extractions of 150±5 words and full text4. The perform-
ance of full text is the ultimate performance an extrac-
tion system can reach using the unigram co-occurrence
scoring method. We also computed the scores of the
lead baseline system (B1) and an automatic system (T).
The average unigram co-occurrence score for full text
(FT) was 0.833, 150±5 words (E150) was 0.796, 100±5
words (E100) was 0.650, the best inter-human agree-
ment (MaxH) was 0.546, system T was 0.465, and base-
line was 0.456. It is interesting to note that the state-of-
the-art system performed at the same level as the base-
line system but was still about 10% away from human.
The 10% difference between E100 and MaxH (0.650 vs.
0.546) implies we might need to constraint humans to
focus their summaries in certain aspects to boost inter-
human agreement to the level of E100; while the 15%
and 24% improvements from E100 to E150 and FT in-
dicate compression would help push overall system per-
formance to a much higher level, if a system is able to
compress longer summaries into a shorter without los-
ing important content.
To investigate relative performance of humans, sys-
tems, and oracle extracts at different inter-human
agreement levels, we created three separate document
sets based on their maximum inter-human agreement
(MaxH) scores. Set Set A had MaxH score greater than
or equal to 0.70, set B was between 0.70 and 0.60, and
</bodyText>
<footnote confidence="0.358137">
4 We used full text as extract and computed its unigram co-
occurrence score against a reference summary.
</footnote>
<figure confidence="0.986361868421053">
Unigram Co-occurrence Scores
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
1.00
MaxH B1 T AvgMaxH Avg B1 Avg T
Avg FT FT Avg E100 Avg E150
Document IDs
Avg T = 0.525
Avg E100 = 0.705
Avg E150 = 0.863
Avg B1 = 0.516
Avg FT = 0.924
Avg MaxH = 0.741
Unigram Co-occurrence Scores
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
1.00
MaxH B1 T FT AvgMaxH Avg B1
Avg T Avg FT Avg E100 Avg E150
Dcoument IDs
Avg T = 0.490
Avg E150 = 0840
Avg E100 = 0.698
Avg B1 = 0.509
Avg FT = 0.917
Avg MaxH = 0.645
1.00
0.90
0.80
Unigram Co-occurrence Scores
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
Avg B1 = 0.423
Avg E150 = 0.790
Avg E100 = 0.645
Avg FT = 0.897
Avg T = 0.435
Avg MaxH = 0.536
Document IDs
MaxH B1 T AvgMaxH Avg B1 Avg T
Avg FT FT Avg E100 Avg E150
Unigram Co-occurrence Scores
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
0.00
1.00
Document IDs
</figure>
<figureCaption confidence="0.9950175">
Figure 9. DUC 2001 single doc inter-human, baseline, and system unigram co-occurrence score versus
compression ratio. Document IDs are sorted by increasing compression ratio CMPR H1.
</figureCaption>
<figure confidence="0.5125055">
B1 MaxH T CMPR H1 CMPR H2 CMPR H3
FT Linear (B1) Linear (MaxH) Linear (T) Linear (FT)
</figure>
<bodyText confidence="0.999784000000001">
set C between 0.60 and 0.50. A had 22 documents, set B
37, and set C 100. Total was about 52% (=159/303) of
the test collection. The 100±5 and 150±5 words aver-
ages were computed over documents which contain at
most 30 sentences. The results are shown in Figures 6,
7, and 8. In the highest inter-human agreement set (A),
we found that average MaxH, 0.741, was higher than
average 100±5 words oracle extract, 0.705; while the
average automatic system performance was around
0.525. This is good news since the high inter-human
agreement and the big difference (0.18) between 100±5
words oracle and automatic system performance pre-
sents a research opportunity for improving sentence
extraction algorithms. The scores of MaxH (0.645 for
set B and 0.536 for set C) in the other two sets are both
lower than 100±5 words oracles (0.698 for set B, 5.3%
lower, and 0.645 for set C, 9.9% lower). This result
suggests that optimizing sentence extraction algorithms
at the Set C level might not be worthwhile since the
algorithms are likely to overfit the training data. The
reason is that the average run time performance of a
sentence extraction algorithm depends on the maximum
inter-human agreement. For example, given a training
reference summary TSUM1 and its full document TDOC1,
we optimize our sentence extraction algorithm to gener-
ate an oracle extract based on TSUM1 from TDOC1. In the
run time, we test on a reference summary RSUM1 and its
full document RDOC1. In the unlikely case that RDOC1 is
the same as TDOC1 and RSUM1 is the same as TSUM1, i.e.
TSUM1 and RSUM1 have unigram co-occurrence score of 1
(perfect inter-human agreement for two summaries of
one document), the optimized algorithm will generate a
perfect extract for RDOC1 and achieve the best perform-
ance since it is optimized on TSUM1. However, usually
TSUM1 and RSUM1 are different. Then the performance of
the algorithm will not exceed the maximum unigram co-
occurrence score between TSUM1 and RSUM1. Therefore it
is important to ensure high inter-human agreement to
allow researchers room to optimize sentence extraction
algorithms using oracle extracts.
Finally, we present the effect of compression ratio on
inter-human agreement (MaxH) and performance of
baseline (B1), automatic system T (T), and full text ora-
cle (FT) in Figure 9. Compression ratio is computed in
terms of words instead of sentences. For example, a 100
words summary of a 500 words document has a com-
pression ratio of 0.80 (=1ñ100/500). The figure shows
that three human summaries (H1, H2, and H3) had dif-
ferent compression ratios (CMPR H1, CMPR H2, and
CMPR H3) for different documents but did not differ
much. The unigram co-occurrence scores for B1, T, and
MaxH were noisy but had a general trend (Linear B1,
Linear T, and Linear MaxH) of drifting into lower
performance when compression ratio increased (i.e.
when summaries became shorter); while the per-
formance of FT did not exhibit a similar trend. This
confirms our earlier hypothesis that humans are less
likely to agree at high compression ratio and system
performance will also suffer at high compression ratio.
The constancy of FT across different compression ratios
is reasonable since FT scores should only depend on
how well the unigram co-occurrence scoring method
captures content overlap between a full text and its ref-
erence summaries and how likely humans use
vocabulary outside the original document.
</bodyText>
<sectionHeader confidence="0.999791" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.993265302325581">
In this paper we presented an empirical study of the
potential and limitations of sentence extraction as a
method of automatic text summarization. We showed
the following:
(1) How to use oracle extracts to estimate the per-
formance upper bound of sentence extraction
methods at different extract lengths. We under-
stand that summaries optimized using unigram
co-occurrence score do not guarantee good
quality in terms of coherence, cohesion, and
overall organization. However, we would argue
that a good summary does require good content
and we will leave how to make the content co-
hesive, coherent, and organized to future re-
search.
(2) Inter-human agreement varied a lot and the dif-
ference between maximum agreement (MaxH)
and minimum agreement (MinH) was about
18% on the DUC 2001 data. To minimize the
gap, we need to define the summarization task
better. This has been addressed by providing
guided summarization tasks in DUC 2003
(DUC 2002). We guesstimate the gap should
be smaller in DUC 2003 data.
(3) State-of-the-art systems performed at the same
level as the baseline system but were still about
10% away from the average human perform-
ance.
(4) The potential performance gains (15% from
E100 to E150 and 24% to FT) estimated by
oracle extracts of different sizes indicated that
sentence compression or sub-sentence extrac-
tion are promising future directions.
(5) The relative performance of humans and oracle
extracts at three inter-human agreement inter-
vals showed that it was only meaningful to op-
timize sentence extraction algorithms if inter-
human agreement was high. Although overall
high inter-human agreement was low but sub-
sets of high inter-human agreement did exist.
For example, about human achieved at least
60% agreement in 59 out of 303 (~19%)
documents of 30 sentences or less.
</bodyText>
<listItem confidence="0.472644">
(6) We also studied how compression ratio af-
</listItem>
<bodyText confidence="0.99998608">
fected inter-human agreement and system per-
formance, and the results supported our
hypothesis that humans tend to agree less at
high compression ratio, and similar between
humans and systems. How to take into account
this factor in future summarization evaluations
is an interesting topic to pursue further.
Using exhaustive search to identify oracle extraction has
been studied by other researchers but in different con-
texts. Marcu (1999a) suggested using exhaustive search
to create training extracts from abstracts. Donaway et al.
(2000) used exhaustive search to generate all three sen-
tences extracts to evaluate different evaluation metrics.
The main difference between their work and ours is that
we searched for extracts of a fixed number of words
while they looked for extracts of a fixed number of sen-
tences.
In the future, we would like to apply a similar method-
ology to different text units, for example, sub-sentence
units such as elementary discourse unit (Marcu 1999b).
We want to study how to constrain the summarization
task to achieve higher inter-human agreement, train
sentence extraction algorithms using oracle extracts at
different compression sizes, and explore compression
techniques to go beyond simple sentence extraction.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998532125">
Donaway, R.L., Drummey, K.W., and Mather, L.A.
2000. A Comparison of Rankings Produced by
Summarization Evaluation Measures. In Proceeding
of the Workshop on Automatic Summarization, post-
conference workshop of ANLP-NAACL-2000, Seat-
tle, WA, USA, 69ñ78.
DUC. 2002. The Document Understanding Conference.
http://duc.nist.gov.
Edmundson, H.P. 1969. New Methods in Automatic
Abstracting. Journal of the Association for Comput-
ing Machinery. 16(2).
Goldstein, J., M. Kantrowitz, V. Mittal, and J. Car-
bonell. 1999. Summarizing Text Documents: Sen-
tence Selection and Evaluation Metrics. In
Proceedings of the 22nd International ACM Confer-
ence on Research and Development in Information
Retrieval (SIGIR-99), Berkeley, CA, USA, 121ñ128.
Hovy, E. and C.-Y. Lin. 1999. Automatic Text Summa-
rization in SUMMARIST. In I. Mani and M.
Maybury (eds), Advances in Automatic Text Sum-
marization, 81fi94. MIT Press.
Kupiec, J., J. Pederson, and F. Chen. 1995. A Trainable
Document Summarizer. In Proceedings of the 18th
International ACM Conference on Research and De-
velopment in Information Retrieval (SIGIR-95), Se-
attle, WA, USA, 68fi73.
Lin, C.-Y. and E. Hovy. 2002. Manual and Automatic
Evaluations of Summaries. In Proceedings of the
Workshop on Automatic Summarization, post-
conference workshop of ACL-2002, pp. 45-51,
Philadelphia, PA, 2002.
Lin, C.-Y. and E.H. Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-occurrence Statis-
tics. In Proceedings of the 2003 Human Language
Technology Conference (HLT-NAACL 2003), Ed-
monton, Canada, May 27 fi June 1, 2003.
Luhn, H. P. 1969. The Automatic Creation of Literature
Abstracts. IBM Journal of Research and Develop-
ment. 2(2), 1969.
Marcu, D. 1999a. The automatic construction of large-
scale corpora for summarization research. Proceed-
ings of the 22nd International ACM Conference on
Research and Development in Information Retrieval
(SIGIR-99), Berkeley, CA, USA, 137fi144.
Marcu, D. 1999b. Discourse trees are good indicators of
importance in text. In I. Mani and M. Maybury (eds),
Advances in Automatic Text Summarization, 123fi
136. MIT Press.
McKeown, K., R. Barzilay, D. Evans, V. Hatzivassi-
loglou, J. L. Klavans, A. Nenkova, C. Sable, B.
Schiffman, S. Sigelman. 2002. Tracking and Summa-
rizing News on a Daily Basis with Columbiaís News-
blaster. In Proceedings of Human Language
Technology Conference 2002 (HLT 2002). San
Diego, CA, USA.
NIST. 2002. Automatic Evaluation of Machine Transla-
tion Quality using N-gram Co-Occurrence Statistics.
Over, P. and W. Liggett. 2002. Introduction to DUC-
2002: an Intrinsic Evaluation of Generic News Text
Summarization Systems. In Proceedings of Work-
shop on Automatic Summarization (DUC 2002),
Philadelphia, PA, USA.
http://www-nlpir.nist.gov/projects/duc/pubs/
2002slides/overview.02.pdf
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Machine
Translation. IBM Research Report RC22176
(W0109-022).
Radev, D.R. and K.R. McKeown. 1998. Generating
Natural Language Summaries from Multiple On-line
Sources. Computational Linguistics, 24(3):469fi500.
Strzalkowski, T, G. Stein, J. Wang, and B, Wise. A Ro-
bust Practical Text Summarizer. 1999. In I. Mani and
M. Maybury (eds), Advances in Automatic Text
Summarization, 137fi154. MIT Press.
White, M., T. Korelsky, C. Cardie, V. Ng, D. Pierce,
and K. Wagstaff. 2001. Multidocument Summariza-
tion via Information Extraction. In Proceedings of
Human Language Technology Conference 2001
(HLT 2001), San Diego, CA, USA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965215">
<title confidence="0.9990915">The Potential and Limitations of Sentence Extraction for Summarization</title>
<author confidence="0.987656">Chin-Yew Lin</author>
<author confidence="0.987656">Eduard</author>
<affiliation confidence="0.999785">University of Southern California/Information Sciences</affiliation>
<address confidence="0.993927">4676 Admiralty Marina del Rey, CA 90292, USA</address>
<email confidence="0.999787">cyl@isi.edu</email>
<email confidence="0.999787">hovy@isi.edu</email>
<abstract confidence="0.99953">In this paper we present an empirical study of the potential and limitation of sentence extraction in text summarization. Our results show that the single document generic summarization task as defined in DUC 2001 needs to be carefully refocused as reflected in the low inagreement at 100-word 1(0.40 and high upper bound at full text 2 (0.88) summaries. For 100-word summaries, the performance upper bound, 0.65, achieved Such oracle extracts show the promise of sentence extraction algorithms; however, we first need to raise inter-human agreement to be able to achieve this performance level. We show that compression is a promising direction and that the compression ratio of summaries affects average human and system performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R L Donaway</author>
<author>K W Drummey</author>
<author>L A Mather</author>
</authors>
<title>A Comparison of Rankings Produced by Summarization Evaluation Measures.</title>
<date>2000</date>
<booktitle>In Proceeding of the Workshop on Automatic Summarization, postconference workshop of ANLP-NAACL-2000,</booktitle>
<location>Seattle, WA, USA, 69ñ78.</location>
<contexts>
<context position="26427" citStr="Donaway et al. (2000)" startWordPosition="4282" endWordPosition="4285">) documents of 30 sentences or less. (6) We also studied how compression ratio affected inter-human agreement and system performance, and the results supported our hypothesis that humans tend to agree less at high compression ratio, and similar between humans and systems. How to take into account this factor in future summarization evaluations is an interesting topic to pursue further. Using exhaustive search to identify oracle extraction has been studied by other researchers but in different contexts. Marcu (1999a) suggested using exhaustive search to create training extracts from abstracts. Donaway et al. (2000) used exhaustive search to generate all three sentences extracts to evaluate different evaluation metrics. The main difference between their work and ours is that we searched for extracts of a fixed number of words while they looked for extracts of a fixed number of sentences. In the future, we would like to apply a similar methodology to different text units, for example, sub-sentence units such as elementary discourse unit (Marcu 1999b). We want to study how to constrain the summarization task to achieve higher inter-human agreement, train sentence extraction algorithms using oracle extracts</context>
</contexts>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>Donaway, R.L., Drummey, K.W., and Mather, L.A. 2000. A Comparison of Rankings Produced by Summarization Evaluation Measures. In Proceeding of the Workshop on Automatic Summarization, postconference workshop of ANLP-NAACL-2000, Seattle, WA, USA, 69ñ78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DUC</author>
</authors>
<title>The Document Understanding Conference.</title>
<date>2002</date>
<note>http://duc.nist.gov.</note>
<contexts>
<context position="1969" citStr="DUC 2002" startWordPosition="303" endWordPosition="304">s reference. 2 We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words. These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Pa</context>
<context position="24985" citStr="DUC 2002" startWordPosition="4057" endWordPosition="4058">timized using unigram co-occurrence score do not guarantee good quality in terms of coherence, cohesion, and overall organization. However, we would argue that a good summary does require good content and we will leave how to make the content cohesive, coherent, and organized to future research. (2) Inter-human agreement varied a lot and the difference between maximum agreement (MaxH) and minimum agreement (MinH) was about 18% on the DUC 2001 data. To minimize the gap, we need to define the summarization task better. This has been addressed by providing guided summarization tasks in DUC 2003 (DUC 2002). We guesstimate the gap should be smaller in DUC 2003 data. (3) State-of-the-art systems performed at the same level as the baseline system but were still about 10% away from the average human performance. (4) The potential performance gains (15% from E100 to E150 and 24% to FT) estimated by oracle extracts of different sizes indicated that sentence compression or sub-sentence extraction are promising future directions. (5) The relative performance of humans and oracle extracts at three inter-human agreement intervals showed that it was only meaningful to optimize sentence extraction algorith</context>
</contexts>
<marker>DUC, 2002</marker>
<rawString>DUC. 2002. The Document Understanding Conference. http://duc.nist.gov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New Methods in Automatic Abstracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery.</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1799" citStr="Edmundson 1969" startWordPosition="276" endWordPosition="277"> summaries. Among them, sentence extraction is by far the most 1 We compute unigram co-occurrence score of a pair of manual summaries, one as candidate summary and the other as reference. 2 We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words. These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in </context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H.P. 1969. New Methods in Automatic Abstracting. Journal of the Association for Computing Machinery. 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>M Kantrowitz</author>
<author>V Mittal</author>
<author>J Carbonell</author>
</authors>
<title>Summarizing Text Documents: Sentence Selection and Evaluation Metrics.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval (SIGIR-99),</booktitle>
<location>Berkeley, CA, USA,</location>
<contexts>
<context position="1853" citStr="Goldstein et al. 1999" startWordPosition="284" endWordPosition="287">by far the most 1 We compute unigram co-occurrence score of a pair of manual summaries, one as candidate summary and the other as reference. 2 We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words. These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and e</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Goldstein, J., M. Kantrowitz, V. Mittal, and J. Carbonell. 1999. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. In Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval (SIGIR-99), Berkeley, CA, USA, 121ñ128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C-Y Lin</author>
</authors>
<title>Automatic Text Summarization in SUMMARIST.</title>
<date>1999</date>
<booktitle>In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization, 81fi94.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1873" citStr="Hovy and Lin 1999" startWordPosition="288" endWordPosition="291">mpute unigram co-occurrence score of a pair of manual summaries, one as candidate summary and the other as reference. 2 We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words. These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The</context>
</contexts>
<marker>Hovy, Lin, 1999</marker>
<rawString>Hovy, E. and C.-Y. Lin. 1999. Automatic Text Summarization in SUMMARIST. In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization, 81fi94. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pederson</author>
<author>F Chen</author>
</authors>
<title>A Trainable Document Summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th International ACM Conference on Research and Development in Information Retrieval (SIGIR-95),</booktitle>
<location>Seattle, WA, USA, 68fi73.</location>
<contexts>
<context position="1830" citStr="Kupiec et al. 1995" startWordPosition="280" endWordPosition="283">tence extraction is by far the most 1 We compute unigram co-occurrence score of a pair of manual summaries, one as candidate summary and the other as reference. 2 We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words. These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summ</context>
</contexts>
<marker>Kupiec, Pederson, Chen, 1995</marker>
<rawString>Kupiec, J., J. Pederson, and F. Chen. 1995. A Trainable Document Summarizer. In Proceedings of the 18th International ACM Conference on Research and Development in Information Retrieval (SIGIR-95), Seattle, WA, USA, 68fi73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Manual and Automatic Evaluations of Summaries.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002,</booktitle>
<pages>45--51</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="13326" citStr="Lin and Hovy (2002)" startWordPosition="2092" endWordPosition="2095">ndpipe to help her breathe.&lt;/S&gt; &lt;/TEXT&gt; &lt;/DOC&gt; Figure 3. A 150-word oracle extract for document AP900424-0035. good summary. It also makes system and human performance approach average since it is more likely to include some good sentences but not all of them. Empirical results shown in Section 5 confirm this and that leads us to the question of how to construct a corpus to evaluate summarization systems. We discuss this issue in the conclusion section. 4.3 Inter-Human Agreement and Its Effect on System Performance In this section we study how inter-human agreement affects system performance. Lin and Hovy (2002) reported that, compared to a manually created ideal, humans scored about 0.40 in average coverage score and the best system scored about 0.35. According to these numbers, we might assume that humans cannot agree to each other on what is important and the best system is almost as good as humans. If this is true then estimating an upper bound using oracle extracts is meaningless. No matter how high the estimated upper bounds may be, we probably would never be able to achieve that performance due to lack of agreement between humans: the oracle approximating one human would fail miserably with an</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Lin, C.-Y. and E. Hovy. 2002. Manual and Automatic Evaluations of Summaries. In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002, pp. 45-51, Philadelphia, PA, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference (HLT-NAACL 2003),</booktitle>
<volume>27</volume>
<location>Edmonton, Canada,</location>
<contexts>
<context position="4463" citStr="Lin and Hovy 2003" startWordPosition="706" endWordPosition="709"> generic 100-word summary. There were 30 test sets in DUC 2001 and each test set contained about 10 documents. For each document, one summary was created manually as the ëidealí model summary at approximately 100 words. We will refer to this manual summary as H1. Two other manual summaries were also created at about that length. We will refer to these two additional human summaries as H2 and H3. In addition, baseline summaries were created automatically by taking the first n sentences up to 100 words. We will refer this baseline extract as B1. 3 Unigram Co-Occurrence Metric In a recent study (Lin and Hovy 2003), we showed that the recall-based unigram co-occurrence automatic scoring metric correlated highly with human evaluation and has high recall and precision in predicting statistical significance of results comparing with its human counterpart. The idea is to measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, C.-Y. and E.H. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of the 2003 Human Language Technology Conference (HLT-NAACL 2003), Edmonton, Canada, May 27 fi June 1, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of Literature Abstracts.</title>
<date>1969</date>
<journal>IBM Journal of Research and Development.</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="1810" citStr="Luhn 1969" startWordPosition="278" endWordPosition="279">g them, sentence extraction is by far the most 1 We compute unigram co-occurrence score of a pair of manual summaries, one as candidate summary and the other as reference. 2 We compute unigram co-occurrence scores of a full text and its manual summaries of 100 words. These scores are the best achievable using the unigram co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the</context>
</contexts>
<marker>Luhn, 1969</marker>
<rawString>Luhn, H. P. 1969. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development. 2(2), 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>The automatic construction of largescale corpora for summarization research.</title>
<date>1999</date>
<booktitle>Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval (SIGIR-99),</booktitle>
<location>Berkeley, CA, USA,</location>
<contexts>
<context position="2219" citStr="Marcu 1999" startWordPosition="341" endWordPosition="342"> Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul &amp; Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-based summaries are as good as we can get for single document</context>
<context position="5332" citStr="Marcu (1999" startWordPosition="860" endWordPosition="861">measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the following equation: Count n gram ( − ) match C Model Units ∈{ } n gram C − ∈ ∑ ∑ Count n gram ( − ) C Model Units ∈{ } n gram C − ∈ Model units are segments of manual summaries. They are typically either sentences or elementary discourse units as defined by Marcu (1999b). Countmatch(n-gram) is the maximum number of n-grams co-occurring in a system extract and a model unit. Count(n-gram) is the number of n-grams in the model unit. Notice that the average n-gram coverage score, Cn, as shown in equation 1, is a recall-based metric, since the denominator of equation 1 is the sum total of the number of n-grams occurring in the model summary instead of the system summary and only one model summary is used for each evaluation. In summary, the unigram co-occurrence statistics we use in the following sections are based on the following formula: Where j ≥ i, i and j </context>
<context position="26325" citStr="Marcu (1999" startWordPosition="4270" endWordPosition="4271">t did exist. For example, about human achieved at least 60% agreement in 59 out of 303 (~19%) documents of 30 sentences or less. (6) We also studied how compression ratio affected inter-human agreement and system performance, and the results supported our hypothesis that humans tend to agree less at high compression ratio, and similar between humans and systems. How to take into account this factor in future summarization evaluations is an interesting topic to pursue further. Using exhaustive search to identify oracle extraction has been studied by other researchers but in different contexts. Marcu (1999a) suggested using exhaustive search to create training extracts from abstracts. Donaway et al. (2000) used exhaustive search to generate all three sentences extracts to evaluate different evaluation metrics. The main difference between their work and ours is that we searched for extracts of a fixed number of words while they looked for extracts of a fixed number of sentences. In the future, we would like to apply a similar methodology to different text units, for example, sub-sentence units such as elementary discourse unit (Marcu 1999b). We want to study how to constrain the summarization ta</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, D. 1999a. The automatic construction of largescale corpora for summarization research. Proceedings of the 22nd International ACM Conference on Research and Development in Information Retrieval (SIGIR-99), Berkeley, CA, USA, 137fi144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>Discourse trees are good indicators of importance in text.</title>
<date>1999</date>
<booktitle>In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization, 123fi 136.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2219" citStr="Marcu 1999" startWordPosition="341" endWordPosition="342"> Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul &amp; Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-based summaries are as good as we can get for single document</context>
<context position="5332" citStr="Marcu (1999" startWordPosition="860" endWordPosition="861">measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the following equation: Count n gram ( − ) match C Model Units ∈{ } n gram C − ∈ ∑ ∑ Count n gram ( − ) C Model Units ∈{ } n gram C − ∈ Model units are segments of manual summaries. They are typically either sentences or elementary discourse units as defined by Marcu (1999b). Countmatch(n-gram) is the maximum number of n-grams co-occurring in a system extract and a model unit. Count(n-gram) is the number of n-grams in the model unit. Notice that the average n-gram coverage score, Cn, as shown in equation 1, is a recall-based metric, since the denominator of equation 1 is the sum total of the number of n-grams occurring in the model summary instead of the system summary and only one model summary is used for each evaluation. In summary, the unigram co-occurrence statistics we use in the following sections are based on the following formula: Where j ≥ i, i and j </context>
<context position="26325" citStr="Marcu (1999" startWordPosition="4270" endWordPosition="4271">t did exist. For example, about human achieved at least 60% agreement in 59 out of 303 (~19%) documents of 30 sentences or less. (6) We also studied how compression ratio affected inter-human agreement and system performance, and the results supported our hypothesis that humans tend to agree less at high compression ratio, and similar between humans and systems. How to take into account this factor in future summarization evaluations is an interesting topic to pursue further. Using exhaustive search to identify oracle extraction has been studied by other researchers but in different contexts. Marcu (1999a) suggested using exhaustive search to create training extracts from abstracts. Donaway et al. (2000) used exhaustive search to generate all three sentences extracts to evaluate different evaluation metrics. The main difference between their work and ours is that we searched for extracts of a fixed number of words while they looked for extracts of a fixed number of sentences. In the future, we would like to apply a similar methodology to different text units, for example, sub-sentence units such as elementary discourse unit (Marcu 1999b). We want to study how to constrain the summarization ta</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, D. 1999b. Discourse trees are good indicators of importance in text. In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization, 123fi 136. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>R Barzilay</author>
<author>D Evans</author>
<author>V Hatzivassiloglou</author>
<author>J L Klavans</author>
<author>A Nenkova</author>
<author>C Sable</author>
<author>B Schiffman</author>
<author>S Sigelman</author>
</authors>
<title>Tracking and Summarizing News on a Daily Basis with Columbiaís Newsblaster.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology Conference</booktitle>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="2184" citStr="McKeown et al. 2002" startWordPosition="333" endWordPosition="336">ossible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul &amp; Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-based summaries are as goo</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Klavans, Nenkova, Sable, Schiffman, Sigelman, 2002</marker>
<rawString>McKeown, K., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman, S. Sigelman. 2002. Tracking and Summarizing News on a Daily Basis with Columbiaís Newsblaster. In Proceedings of Human Language Technology Conference 2002 (HLT 2002). San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<contexts>
<context position="4963" citStr="NIST 2002" startWordPosition="784" endWordPosition="785">e will refer this baseline extract as B1. 3 Unigram Co-Occurrence Metric In a recent study (Lin and Hovy 2003), we showed that the recall-based unigram co-occurrence automatic scoring metric correlated highly with human evaluation and has high recall and precision in predicting statistical significance of results comparing with its human counterpart. The idea is to measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the following equation: Count n gram ( − ) match C Model Units ∈{ } n gram C − ∈ ∑ ∑ Count n gram ( − ) C Model Units ∈{ } n gram C − ∈ Model units are segments of manual summaries. They are typically either sentences or elementary discourse units as defined by Marcu (1999b). Countmatch(n-gram) is the maximum number of n-grams co-occurring in a system extract and a model unit. Count(n-gram) is the number of n-grams in the model unit. Notice that the average n-gram coverage score, Cn, as shown in equ</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co-Occurrence Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Over</author>
<author>W Liggett</author>
</authors>
<title>Introduction to DUC2002: an Intrinsic Evaluation of Generic News Text Summarization Systems.</title>
<date>2002</date>
<booktitle>In Proceedings of Workshop on Automatic Summarization (DUC 2002),</booktitle>
<location>Philadelphia, PA, USA.</location>
<note>http://www-nlpir.nist.gov/projects/duc/pubs/ 2002slides/overview.02.pdf</note>
<marker>Over, Liggett, 2002</marker>
<rawString>Over, P. and W. Liggett. 2002. Introduction to DUC2002: an Intrinsic Evaluation of Generic News Text Summarization Systems. In Proceedings of Workshop on Automatic Summarization (DUC 2002), Philadelphia, PA, USA. http://www-nlpir.nist.gov/projects/duc/pubs/ 2002slides/overview.02.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<journal>IBM Research Report</journal>
<volume>22176</volume>
<pages>0109--022</pages>
<contexts>
<context position="4951" citStr="Papineni et al. 2001" startWordPosition="780" endWordPosition="783">ces up to 100 words. We will refer this baseline extract as B1. 3 Unigram Co-Occurrence Metric In a recent study (Lin and Hovy 2003), we showed that the recall-based unigram co-occurrence automatic scoring metric correlated highly with human evaluation and has high recall and precision in predicting statistical significance of results comparing with its human counterpart. The idea is to measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the following equation: Count n gram ( − ) match C Model Units ∈{ } n gram C − ∈ ∑ ∑ Count n gram ( − ) C Model Units ∈{ } n gram C − ∈ Model units are segments of manual summaries. They are typically either sentences or elementary discourse units as defined by Marcu (1999b). Countmatch(n-gram) is the maximum number of n-grams co-occurring in a system extract and a model unit. Count(n-gram) is the number of n-grams in the model unit. Notice that the average n-gram coverage score, Cn, as </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. IBM Research Report RC22176 (W0109-022).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>K R McKeown</author>
</authors>
<title>Generating Natural Language Summaries from Multiple On-line Sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="2143" citStr="Radev and McKeown 1998" startWordPosition="325" endWordPosition="328">am co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul &amp; Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the be</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Radev, D.R. and K.R. McKeown. 1998. Generating Natural Language Summaries from Multiple On-line Sources. Computational Linguistics, 24(3):469fi500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Strzalkowski</author>
<author>G Stein</author>
<author>J Wang</author>
<author>B Wise</author>
</authors>
<title>A Robust Practical Text Summarizer.</title>
<date>1999</date>
<booktitle>In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization, 137fi154.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2247" citStr="Strzalkowski et al. 1999" startWordPosition="343" endWordPosition="346"> summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul &amp; Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-based summaries are as good as we can get for single document summarization in the news g</context>
</contexts>
<marker>Strzalkowski, Stein, Wang, Wise, 1999</marker>
<rawString>Strzalkowski, T, G. Stein, J. Wang, and B, Wise. A Robust Practical Text Summarizer. 1999. In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization, 137fi154. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M White</author>
<author>T Korelsky</author>
<author>C Cardie</author>
<author>V Ng</author>
<author>D Pierce</author>
<author>K Wagstaff</author>
</authors>
<title>Multidocument Summarization via Information Extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of Human Language Technology Conference</booktitle>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="2162" citStr="White et al. 2001" startWordPosition="329" endWordPosition="332"> metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul &amp; Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-base</context>
</contexts>
<marker>White, Korelsky, Cardie, Ng, Pierce, Wagstaff, 2001</marker>
<rawString>White, M., T. Korelsky, C. Cardie, V. Ng, D. Pierce, and K. Wagstaff. 2001. Multidocument Summarization via Information Extraction. In Proceedings of Human Language Technology Conference 2001 (HLT 2001), San Diego, CA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>