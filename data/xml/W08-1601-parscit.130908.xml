<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9671625">
Semantic Chunk Annotation for complex questions using Conditional
Random Field
</title>
<author confidence="0.99196">
Shixi Fan
</author>
<affiliation confidence="0.954911666666667">
Department of computer science
Harbin Institute of Technology
Shenzhen Graduate School,
</affiliation>
<address confidence="0.93137">
Shenzhen,518055, china
</address>
<email confidence="0.998547">
fanshixi@hit.edu.cn
</email>
<author confidence="0.987445">
Wing W. Y. Ng
</author>
<affiliation confidence="0.981994333333333">
Department of computer science
Harbin Institute of Technology
Shenzhen Graduate School,
</affiliation>
<address confidence="0.831014">
Shenzhen,518055, china
</address>
<email confidence="0.991636">
wing@hitsz.edu.cn
</email>
<author confidence="0.997256">
Xiaolong Wang
</author>
<affiliation confidence="0.944346">
Department of computer science
Harbin Institute of Technology
Shenzhen Graduate School,
</affiliation>
<address confidence="0.94163">
Shenzhen,518055, china
</address>
<email confidence="0.994796">
wangxl@insun.hit.edu.cn
</email>
<sectionHeader confidence="0.994065" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938266666667">
This paper presents a CRF (Conditional
Random Field) model for Semantic
Chunk Annotation in a Chinese Question
and Answering System (SCACQA). The
model was derived from a corpus of real
world questions, which are collected
from some discussion groups on the
Internet. The questions are supposed to
be answered by other people, so some of
the questions are very complex. Mutual
information was adopted for feature se-
lection. The training data collection con-
sists of 14000 sentences and the testing
data collection consists of 4000 sentences.
The result shows an F-score of 93.07%.
</bodyText>
<author confidence="0.83906">
Yaoyun Zhang
</author>
<affiliation confidence="0.971589333333333">
Department of computer science
Harbin Institute of Technology
Shenzhen Graduate School,
</affiliation>
<address confidence="0.909408">
Shenzhen,518055, china
</address>
<email confidence="0.986403">
Xiaoni5122@gmail.com
</email>
<author confidence="0.984757">
Xuan Wang
</author>
<affiliation confidence="0.990214">
Department of computer science
Harbin Institute of Technology
Shenzhen Graduate School,
</affiliation>
<address confidence="0.769873">
Shenzhen,518055, china
</address>
<email confidence="0.987447">
wangxuan@insun.hit.edu.cn
</email>
<sectionHeader confidence="0.999577" genericHeader="introduction">
1 Introduction
</sectionHeader>
<subsectionHeader confidence="0.998978">
1.1 Introduction of Q&amp;A System
</subsectionHeader>
<bodyText confidence="0.999818">
Automated question answering has been a hot
topic of research and development since the ear-
liest AI applications (A.M. Turing, 1950). Since
then there has been a continual interest in proc-
essing knowledge and retrieving it efficiently to
users automatically. The end of the 1980s saw a
boost in information retrieval technologies and
applications, with an unprecedented growth in
the amount of digital information available, an
explosion of growth in the use of computers for
communications, and the increasing number of
users that have access to all this information
(Diego Moll′and Jose′Luis Vicedo, 2007).
Search engines such as Google, Yahoo, Baidu
and etc have made a great success for people’s
information need.
Anyhow, search engines are keywords-based
which can only return links of relevant web
pages, failing to provide a friendly user-interface
with queries expressed in natural language sen-
tences or questions, or to return precise answers
to users. Especially from the end of the 1990s, as
</bodyText>
<footnote confidence="0.99067875">
© 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
</footnote>
<page confidence="0.498126">
1
</page>
<note confidence="0.856134">
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 1–8
Manchester, August 2008
</note>
<bodyText confidence="0.999614214285714">
information retrieval technologies and method-
ologies became mature and grew more slowly in
pace, automated question answering(Q&amp;A) sys-
tems which accept questions in free natural lan-
guage formations and return exactly the answer
or a short paragraph containing relevant informa-
tion has become an urgent necessity. Major in-
ternational evaluations such as TREC, CLEF and
NTCIR have attracted the participation of many
powerful systems.
The architecture of a Q&amp;A system generally in-
cludes three modules: question processing, can-
didate answer/document retrieval, and answer
extraction and re-ranking.
</bodyText>
<subsectionHeader confidence="0.956261">
1.2 Introduction of Question Analyzing
</subsectionHeader>
<bodyText confidence="0.999935906976745">
Question Analyzing, as the premise and founda-
tion of the latter two modules, is of paramount
importance to the integrated performance of a
Q&amp;A system. The reason is quite intuitive: a
question contains all the information to retrieve
the corresponding answer. Misinterpretation or
too much loss of information during the process-
ing will inevitably lead to poor precision of the
system.
The early research efforts and evaluations in
Q&amp;A were focused mainly on factoid questions
asking for named entities, such as time, numbers,
and locations and so on. The questions in the test
corpus of TREC and other organizations are also
in short and simple form. Complex hierarchy in
question types (Dragomir Radev et al, 2001),
question templates (Min-Yuh Day et al, 2005),
question parsing (Ulf Hermjakob, 2001) and
various machine learning methods (Dell Zhang
and Wee Sun Lee, 2003)are used for factoid
question analysis, aiming to find what named
entity is asked in the question. There are some
questions which are very complicated or even
need domain restricted knowledge and reasoning
technique. Automatic Q&amp;A system can not deal
with such questions with current technique.
In china, there is a new kind of web based Q&amp;A
system which is a special kind of discussion
group. Unlike common discussion group, in the
web based Q&amp;A system one user posts a ques-
tion, other users can give answers to it. It is
found that at least 50% percent questions
(Valentin Jijkoun and Maarten de Rijke,
2005)posted by users are non-factoid and surely
more complicated both in question pattern and
information need than those questions in the test
set of TREC and other FAQ. An example is as
follows:
This kind of Q&amp;A system can complement the
search engines effectively. As the best search
engines in china, Baidu open the Baidu Knowl-
edge2 Q&amp;A system from 2003, and now it has
more than 29 million question-answer pairs.
There are also many other systems of this kind
such as Google Groups, Yahoo Answers and
Sina Knowledge3. This kind of system is a big
question-answer pair database which can be
treated as a FAQ database. How to search from
the database and how to analyze the questions in
the database needs new methods and techniques.
More deeper and precise capture of the semantics
in those complex questions is required. This phe-
nomenon has also been noticed by some re-
searchers and organizations. The spotlight gradu-
ally shifted to the processing and semantic un-
derstanding of complex questions. From 2006,
TREC launched a new annually evaluation
CIQ&amp;A (complex, interactive Question Answer-
ing), aiming to promote the development of in-
teractive systems capable of addressing complex
information needs. The targets of national pro-
grams AQUAINT and QUETAL are all at new
interface and new enhancements to current state-
of-the-art Q&amp;A systems to handle more complex
inputs and situations.
A few researchers and institutions serve as pio-
neers in complex questions study. Different tech-
nologies, such as definitions of different sets of
question types, templates and sentence patterns
(Noriko Tomuro, 2003) (Hyo-Jung Oh et al,
2005) machine learning methods (Radu Soricut
and Eric Brill, 2004), language translation model
(Jiwoon Jeon, W et al, 2005), composition of
information needs of the complex question
(Sanda Harabagiu et al, 2006) and so on, have
been experimented on the processing of complex
question, gearing the acquired information to the
facility of other Q&amp;A modules.
Several major problems faced now by researcher
of complex questions are stated as follow:
First: Unlike factoid questions, it is very dif-
ficult to define a comprehensive type hierarchy
for complex questions. Different domains under
research may require definitions of different sets
of question types, as shown in (Hyo-Jung Oh et
al, 2005). Especially, the types of certain ques-
</bodyText>
<footnote confidence="0.9991665">
2 http://zhidao.baidu.com/
3 http://iask.sina.com.cn/
</footnote>
<page confidence="0.995951">
2
</page>
<bodyText confidence="0.999667151515152">
tions are ambiguous and hard to identify. For
example:
This question type can be treated as definition,
procedure or entity.
Second: Lack of recognition of different seman-
tic chunks and the relations between them.
FAQFinder (Radu Soricut and Eric Brill, 2004)
also used semantic measure to credit the similar-
ity between different questions. Nevertheless, the
question similarity is only a simple summation of
the semantic similarity between words from the
two question sentences. Question pattern are very
useful and easy to implement, as justified by pre-
vious work. However, just like the problem with
question types, question patterns have limitation
on the coverage of all the variations of complex
question formation. Currently, after the question
processing step in most systems, the semantic
meaning of large part of complex questions still
remain vague. Besides, confining user’s input
only within the selection of provided pattern may
lead to unfriendly and unwelcome user interface.
(Ingrid Zukerman and Eric Horvitz, 2001) used
decision tree to model and recognize the infor-
mation need, question and answer coverage,
topic, focus and restrictions of a question. Al-
though features employed in the experiments
were described in detail, no selection process of
those feature, or comparison between them was
mentioned.
This paper presents a general method for Chinese
question analyzing. Our goal is to annotate the
semantic chunks for the question automatically.
</bodyText>
<sectionHeader confidence="0.944557" genericHeader="method">
2 Semantic Chunk Annotation
</sectionHeader>
<bodyText confidence="0.999962533333333">
Chinese language differs a lot from English in
many aspects. Mature methodologies and fea-
tures well-justified in English Q&amp;A systems are
valuable sources of reference, but no direct copy
is possible.
The Ask-Answer system4 is a Chinese online
Q&amp;A system where people can ask and answer
questions like other web based Q&amp;A system. The
characteristic of this system is that it can give the
answer automatically by searching from the
asked question database when a new question is
presented by people. The architecture of the
automatically answer system is shown in figure 1.
The system contains a list of question-answer
pairs on particular subject. When users input a
</bodyText>
<footnote confidence="0.679217">
4 http://haitianyuan.com/qa
</footnote>
<bodyText confidence="0.998943777777778">
question from the web pages, the question is
submitted to the system and then question-
answer pair is returned by searching from the
questions asked before. The system includes four
main parts: question pre-processing, question
analyzing, searching and answer getting.
The question pre-processing part will segment
the input questions into words, label POS tags
for every word. Sometimes people ask two or
more questions at one time, the questions should
be made into simple forms by conjunctive struc-
ture detection. The question analyzing program
will find out the question type, topic, focus and
etc. The answer getting part will get the answer
by computing the similarity between the input
question and the questions asked before. The
question analyzing part annotates the semantic
chunks for the question. So that the question can
be mapped into semantic space and the question
similarity can be computed semantically. The
Semantic chunk annotation is the most important
part of the system.
Figure 1 the architecture of the automatically
answer system
Currently, no work has been reported yet on the
question semantic chunk annotation in Chinese.
The prosperity of major on-line discussion
groups provides an abundant ready corpus for
question answering research. Using questions
collected from on-line discussion groups; we
make a deep research on semantic meanings and
build a question semantic chunk annotation
model based on Conditional Random Field.
Five types of semantic chunks were defined:
Topic, Focus, Restriction, Rubbish information
and Interrogative information. The topic of a
</bodyText>
<figure confidence="0.930295">
Question Analyzing
Question Pre- processing
Answer getting
Search reference question-answer pairs form database
Get and extend key words
Segmentation and pos
tagging
Score the constituent
answers
Question pattern and knowledge base
Semantic chunk annotation
Detect conjunctive structure
Out put the top
five answers
</figure>
<page confidence="0.982008">
3
</page>
<bodyText confidence="0.999879692307692">
question which is the topic or subject asked is the
most important semantic chunk. The focus of a
question is the asking point of the question. The
restriction information can restrict the question’s
information need and the answers. The rubbish
information is those words in the question that
has no semantic meanings for the question. Inter-
rogative information is a semantic tag set which
corresponds to the question type. The interroga-
tive information includes interrogative words,
some special verbs and nouns words and all these
words together determine the question type. The
semantic chunk information is shown in table 1.
</bodyText>
<table confidence="0.999490888888889">
Semantic Abbreviation Meaning
chunk tag
Topic T The question subject
Focus F The additional information
of topic
Restrict Re Such as Time restriction and
location restriction
Rubbish Ru Words no meaning for the
information question
Other O other information without
semantic meaning
The following is interrogative information
Quantity Wqua
Description Wdes The answer need description
Yes/No Wyes The answer should be yes or
no
List Wlis The answer should be a list
of entity
Definition Wdef The answer is the definition
of topic
Location Wloc The answer is location
Reason Wrea The answer can explain the
question
Contrast Wcon The answer is the compari-
son of the items proposed in
the question
People Wwho The answer is about the
people’s information
Choice Wcho The answer is one of the
choice proposed in the ques-
tion
Time Wtim The answer is the data or
time length about the event
in the question
Entity Went The answer is the attribute
of the topic.
</table>
<tableCaption confidence="0.999637">
Table 1: Semantic chunks
</tableCaption>
<bodyText confidence="0.991096142857143">
An annotation example question is as follows:
This question can be annotated as follows:
This kind of annotation is not convenient for CRF
model, so the tags were transfer into the B I O
form. (Shown as follows)
Then the Semantic chunk annotation can be
treated as a sequence tag problem.
</bodyText>
<sectionHeader confidence="0.985404" genericHeader="method">
3 Semantic Chunk Annotation model
</sectionHeader>
<subsectionHeader confidence="0.999226">
3.1 Overview of the CRF model
</subsectionHeader>
<bodyText confidence="0.995481176470588">
The conditional random field (CRF) is a dis-
criminative probabilistic model proposed by John
Lafferty, et al (2001) to overcome the long-range
dependencies problems associated with genera-
tive models. CRF was originally designed to la-
bel and segment sequences of observations, but
can be used more generally. Let X, Y be random
variables over observed data sequences and cor-
responding label sequences, respectively. For
simplicity of descriptions, we assume that the
random variable sequences X and Y have the
same length, and use x = [x1, x2 xm ]
and y = [y1 , y2 ym ] to represent instances of
X and Y, respectively. CRF defines the condi-
tional probability distribution P(Y |X) of label
sequences given observation sequences as fol-
lows
</bodyText>
<equation confidence="0.949972363636364">
P (Y  |X) = 1 exp( ( ,
i f i X Y
λ ∑= λ
Z X
( ) i 1
y
∑
λ
y |x) =1 (2)
is a model parameter an
λi
</equation>
<bodyText confidence="0.803057733333333">
d
is a feature function (often binary-
tain feature in a certain position and
a
certain label, and becomes zero otherwise.
Unlike Maximum Entropy model which use sin-
gle normalization constant to yield a joint distri-
bution, CRFs use the observation-dependent
normalization
(X) for conditional distribu-
tions. So CRFs can
Y takes
Zλ
avoid the label biased prob-
lem. Given a set of training data
</bodyText>
<equation confidence="0.81656525">
T = { ( x k , y k ), k = 1,2 . . . . n }
~
Where
Zλ (X) is the normalizing factor that
</equation>
<bodyText confidence="0.8697302">
ensures equation 2.
In equation 2 the
valued) that becomes positive (one for binary-
valued feature function) when X contains a cer-
With an empirical distribution ( , ), CRF
</bodyText>
<equation confidence="0.989427714285714">
P X Y
n
λ
)) (1)
)
(X,Y
fi
</equation>
<page confidence="0.960735">
4
</page>
<bodyText confidence="0.9792325">
determines the model parameters A = {Ai } by
maximizing the log-likelihood of the training set
</bodyText>
<equation confidence="0.9971525">
PA (yk  |xk )
k= 1 (3)
</equation>
<subsectionHeader confidence="0.779311">
3.2 Features for the model
</subsectionHeader>
<bodyText confidence="0.996029">
The following features, which are used for train-
ing the CRF model, are selected according to the
empirical observation and some semantic mean-
ings. These features are listed in the following
table.
</bodyText>
<table confidence="0.9747525">
Feature type in- Feature type name
dex
1 Current word
2 Current POS tag
3 Pre-1 word POS tag
4 Pre-2 word POS tag
5 Post -1 word POS tag
6 Post -2 word POS tag
7 Question pattern
8 Question type
9 Is pattern key word
10 Pattern tag
</table>
<tableCaption confidence="0.990522">
Table 2: the Features for the model
</tableCaption>
<subsectionHeader confidence="0.467438">
Current word:
</subsectionHeader>
<bodyText confidence="0.9999794">
The current word should be considered when
adding semantic tag for it. But there are too
many words in Chinese language and only part
of them will contribute to the performance, a set
of words was selected. The word set includes
segment note and some key words such as time
key word and rubbish key word. When the cur-
rent word is in the word set the current word fea-
ture is the current word itself, and null on the
other hand.
</bodyText>
<subsectionHeader confidence="0.553875">
Current POS tag:
</subsectionHeader>
<bodyText confidence="0.9275055">
Current POS tag is the part of speech tag for the
current word.
Pre-1 word POS tag:
Pre- 1 word POS tag is the POS tag of the first
word before the labeling word in the sentence. If
the Pre-1 word does not exit (the current is the
first word in the sentence), the Pre- 1 word POS
tag is set to null.
Pre-2 word POS tag:
Pre- 2 word POS tag is the POS tag of the second
word before the labeling word in the sentence. If
the Pre-2 word does not exit, the Pre- 2 word
POS tag is set to null.
Post -1 word POS tag:
Post - 1 word POS tag is the POS tag of the first
word after the labeling word in the sentence. If
the Post -1 word does not exit (the current is the
first word in the sentence), the Post - 1 word POS
tag is set to null.
Post -2 word POS tag:
Post - 2 word POS tag is the POS tag of the sec-
ond word after the labeling word in the sentence.
If the Post-2 word does not exit, the Pre- 2 word
POS tag is set to null.
</bodyText>
<subsectionHeader confidence="0.924391">
Question pattern:
</subsectionHeader>
<bodyText confidence="0.982581473684211">
Question pattern which is associated with ques-
tion type, can locate question topic, question fo-
cus by surface string matching. For example,
(where is &lt;topic&gt;). The patterns are extracted
from the training data automatically. When a pat-
tern is matched, it is treated as a feature. There
are 1083 question patterns collected manually.
Question type:
Question type is an important feature for ques-
tion analyzing. The question patterns have the
ability of deciding the question type. If there is
no question pattern matching the question, the
question type is defined by a decision tree algo-
rithm.
Is pattern key word:
For each question pattern, there are some key
words. When the current word belongs to the
pattern key word this feature is set to “yes”, else
it is set to “no”.
</bodyText>
<subsectionHeader confidence="0.684761">
Pattern tag:
</subsectionHeader>
<bodyText confidence="0.999939">
When a pattern is matched, the topic, focus and
restriction can be identified by the pattern. We
can give out the tags for the question and the tags
are treated as features. If there is no pattern is
matched, the feature is set to null.
</bodyText>
<sectionHeader confidence="0.993189" genericHeader="method">
4 Feature Selection experiment
</sectionHeader>
<bodyText confidence="0.9939732">
Feature selection is important in classifying sys-
tems such as neural networks (NNs), Maximum
Entropy, Conditional Random Field and etc. The
problem of feature selection has been tackled by
many researchers. Principal component analysis
(PCA) method and Rough Set Method are often
used for feature selection. Recent years, mutual
information has received more attention for fea-
ture selection problem.
According to the information theory, the uncer-
tainty of a random variable X can be measured
by its entropy H(X) . For a classifying problem,
there are class label set represented by C and fea-
ture set represented by F. The conditional en-
tropy H(C  |F) measures the uncertainty about
</bodyText>
<equation confidence="0.929464055555556">
~
oc
(  |)
y x
P(x ,
E
y P
) log A
x,y
N
E
log
P A
IF ( )
5
C when F is known, and the Mutual information
I(C, F) is defined as:
I (C; F) = H(C) - H (C  |F) (4)
</equation>
<bodyText confidence="0.97330805">
The feature set is known; so that the objective of
training the model is to minimize the conditional
entropy H(C  |F) equally maximize the mutual
information I (C; F) . In the feature set F, some
features are irrelevant or redundant. So that the
goal of a feature selection problem is to find a
feature S ( S c F ), which achieve the higher
values ofI (C; F) . The set S is a subset of F and
its size should be as small as possible. There are
some algorithms for feature selection problem.
The ideal greedy selection algorithm using mu-
tual information is realized as follows (Nojun
Kwak and Chong-Ho Choi, 2002):
Input: S- an empty set
F- The selected feature set
Output: a small reduced feature set S which is
equivalent to F
Step 1: calculate the MI with the Class
setC , b&apos;f E F , compute I (C; fi )
Step 2: select the feature that maximizes I (C; fi) ,
</bodyText>
<listItem confidence="0.995665666666667">
1) Calculate the MI with the Class set C and S,
b&apos;f E F , compute I (C; S, fi )
2) Select the feature that maximizes I (C; S, fi ) ,
</listItem>
<equation confidence="0.736114">
F+-F\{fi} ,S+-{f
i
</equation>
<bodyText confidence="0.9987217">
Step 4: Output the set S that contains the se-
lected features
To calculate MI the PDFs (Probability Distribu-
tion Functions) are required. When features and
classing types are dispersing, the probability can
be calculated statistically. In our system, the
PDFs are got from the training corpus statistically.
The training corpus contains 14000 sentences.
The training corpus was divided into 10 parts,
with each part 1400 sentences. And each part is
divided into working set and checking set. The
working set, which contains 90% percent data,
was used to select feature by MI algorithm. The
checking set, which contains 10% percent data,
was used to test the performance of the selected
feature sequence. When the feature sequence was
selected by the MI algorithm, a sequence of CRF
models was trained by adding one feature at each
time. The checking data was used to test the per-
formance of these models.
</bodyText>
<figure confidence="0.9005355">
set
}
F+-F\{fi} ,S+-{f
i
</figure>
<figureCaption confidence="0.5350475">
Step 3: repeat until desired number of features
are selected.
</figureCaption>
<table confidence="0.999366304347826">
The open test result
Selected feature 1 2 3 4 5 6 7 8 9 10
sequence
7, 10, 3, 1, 5, 2, 0.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.9018
4, 6, 8,9
7, 10, 1, 3, 5, 2, 0.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.9007
4,6,8,9
7, 10, 1, 3, 5, 2, 0.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.8949
4, 6,8,9
7, 10, 1, 3, 5, 2, 0.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.9010
4, 6,9,8
7, 10, 1, 3, 5, 2, 0.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.9007
4, 6,8,9
7, 10, 3, 1, 5, 2, 0.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.9011
4,6,8,9
7, 10, 1, 3, 5, 2, 0.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.9009
4, 6, 8, 9
7, 10, 1, 3, 5, 2, 0.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.9023
4, 6,8,9
7, 10, 1, 3, 5, 2, 0.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.8986
4, 6,8,9
7, 10, 1, 3, 5, 2, 0.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067
4, 6, 8,9
</table>
<tableCaption confidence="0.999804">
Table 3: the feature selection result and the test result
</tableCaption>
<bodyText confidence="0.944411142857143">
set
}
In table 3, each row contains data corresponding
to one part of the training corpus so there are ten
rows with data in the table. The third row corre-
sponds to the first part and the last row corre-
sponds to the tenth part. There are eleven col-
umns in the table, the first columns is the fea-
tures sequence selected by the mutual informa-
tion algorithm for each part. The second column
is the open test result with the first feature in the
feature sequence. The third column is the open
test result with the first two features in the fea-
ture sequence and so on. From the table, it is
</bodyText>
<page confidence="0.996172">
6
</page>
<bodyText confidence="0.9999635">
clear that the feature 7(Question pattern) and
10(Pattern tag) are very important, while the fea-
ture 8(Question type) and 9(Is pattern key word)
are not necessary. The explanation about this
phenomenon is that the “pattern key word” and
“Question type” information can be covered by
the Question patterns. So feature 8 and 9 are not
used in the Conditional Random Field model.
</bodyText>
<sectionHeader confidence="0.98511" genericHeader="method">
5 Semantic Chunk Annotation Experi-
ment
</sectionHeader>
<bodyText confidence="0.999658">
The test and training data used in our system are
collected from the website (Baidu knowledge
and the Ask-Answer system), where people pro-
posed questions and answers. The training data
consists of 14000 and the test data consists of
4000 sentences. The data set consists of word
tokens, POS and semantic chunk tags. The POS
and semantic tags are assigned to each word to-
kens.
The performance is measured with three rates:
precision (Pre), recall (Rec) and F-score (F1).
</bodyText>
<equation confidence="0.999811666666667">
Pre = Match/Model (5)
Rec=Match/Manual (6)
F1=2*Pre*Rec/(Pre+Rec) (7)
</equation>
<bodyText confidence="0.998081666666667">
Match is the count of the tags that was predicted
right. Model is the count of the tags that was pre-
dicted by the model. Manual is the count of the
tags that was labeled manually.
Table 4 shows the performance of annotation of
different semantic chunk types. The first column
is the semantic chunk tag. The last three columns
are precision, recall and F1 value of the semantic
chunk performance, respectively.
</bodyText>
<table confidence="0.995824166666667">
Label Manual Model Match Pre.() Rec.() F1
B-T,I-T 17061,78462 16327,80488 14825,76461 90.80,95.00 86.89,97.45 88.80,96.21
B-F,I-F 5072,13029 5079,13583 4657,12259 91.69,90.25 91.82,94.09 91.75,92.13
B-Ru,I-Ru 775,30 11,0 2,0 18.18,0.00 0.26,0.00 0.51,0.00
O 8354 8459 6676 78.92 79.91 79.41
B-Wqua,I-Wqua 1363,934 1327,1028 1298,881 97.81,85.70 95.23,94.33 96.51,89.81
B-Wyes,I-Wyes 5669,1162 5702,1098 5550,1083 97.33,98.63 97.90,93.20 97.62,95.84
B-Wdes,I-Wdes 2907,278 2855,185 2779,184 97.34,99.46 95.60,66.19 96.46,79.48
B-Wlis,I-Wlis 603,257 563,248 560,248 99.47,100 92.87,96.50 96.05,98.22
B-Wdef,I-Wdef 1420,1813 1430,1878 1280,1695 89.51,90.26 90.14,93.49 89.82,91.85
B-Wloc,I-Wloc 683,431 665,395 661,392 99.40,99.24 96.78,90.95 98.07,94.92
B-Wrea,I-Wrea 902,159 873,83 843,82 96.56,98.80 93.46,51.57 94.99,67.77
B-Wcon,I-Wcon 552,317 515,344 503,291 97.67,84.59 91.12,91.80 94.28,88.05
B-Wwho,I-Wwho 420,364 357,350 348,336 97.48,96.00 82.86,92.31 89.58,94.12
B-Wcho,I-Wcho 857,85 738,0 686,0 92.95,0.00 80.05,0.00 86.02,0.00
B-Wtim,I-Wtim 408,427 401,419 355,380 88.53,90.69 87.01,88.99 87.76,89.83
B-Went,I-Went 284,150 95,81 93,80 97.89,98.77 32.75,53.33 49.08,69.26
Avg 145577 145577 135488 93.07 93.07 93.07
</table>
<tableCaption confidence="0.998744">
Table 4: the performance of different semantic chunk
</tableCaption>
<bodyText confidence="0.999933">
The semantic chunk type of “Topic” and “Focus”
can be annotated well. Topic and focus semantic
chunks have a large percentage in all the seman-
tic chunks and they are important for question
analyzing. So the result is really good for the
whole Q&amp;A system.
As for “Rubbish” semantic chunk, it only has
0.51 and 0.0 F1 measure for B-Ru and I-Ru. One
reason is lacking enough training examples, for
there are only 1031 occurrences in the training
data. Another reason is sometimes restriction is
complex.
</bodyText>
<sectionHeader confidence="0.993992" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999841875">
This paper present a new method for Chinese
question analyzing based on CRF. The features
are selected by using mutual information algo-
rithm. The selected features work effectively for
the CRF model. The experiments on the test data
set achieve 93.07% in F1 measure. In the future,
new features should be discovered and new
methods will be used.
</bodyText>
<sectionHeader confidence="0.953704" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.9832298">
This work is supported by Major Program of Na-
tional Natural Science Foundation of China
(No.60435020 and No. 90612005) and the High
Technology Research and Development Program
of China (2006AA01Z197).
</bodyText>
<sectionHeader confidence="0.998372" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994382">
A.M. Turing. 1950. Computing Machinery and
Intelligence. Mind, 236 (59): 433~460.
Diego Moll′, Jose′Luis Vicedo. 2007. Question
Answering in Restricted Domains: An Overview.
Computational Linguistics, 33(1),
</reference>
<page confidence="0.990261">
7
</page>
<reference confidence="0.998728148148148">
Dragomir Radev, WeiGuo Fan, Leila Kosseim. 2001.
The QUANTUM Question Answering System.
TREC.
Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU,
Chormg-Shyong Ong, Wen-Lian Hsu. 2005. An
Integrated Knowledge-based and Machine
Learning Approach for Chinese Question
Classification. Proceedings of the IEEE
International Conference on Natural Language
Processing and Knowledge Engineering, Wuhan,
China,:620~625.
Ulf Hermjakob. 2001. Parsing and Question
Classification for Question Answering.
Proceedings of the ACL Workshop on Open-
Domain Question Answering, Toulouse,:19~25.
Dell Zhang, Wee Sun Lee. 2003. Question
classification using support vector machines.
Proceedings of the 26th Annual International ACM
Conference on Research and Development in
Information Retrieval(SIGIR), Toronto, Canada,26
~ 32.
Valentin Jijkoun, Maarten de Rijke.2005. Retrieving
Answers from Frequently Asked Questions Pages
on the Web. CIKM’05, Bermen, Germany.
Noriko Tomuro. 2003. Interrogative Reformulation
Patterns and Acquisition of Question Paraphrases.
Proceeding of the Second International Workshop
on Paraphrasing, :33~40.
Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim,
Myung-Gil Jang. 2005. Descriptive Question
Answering in Encyclopedia. Proceedings of the
ACL Interactive Poster and Demonstration Sessions,
pages 21–24, Ann Arbor.
Radu Soricut, Eric Brill. 2004, Automatic Question
Answering: Beyond the Factoid. Proceedings of
HLT-NAACL ,:57~64.
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005.
Finding Similar Questions in Large Question and
Answer Archives. CIKM’05, Bremen, Germany.
Sanda Harabagiu, Finley Lacatusu and Andrew Hickl.
2006 . Answering Complex Questions with Random
Walk Models. SIGIR’06, Seattle, Washington,
USA.pp220-227.
Ingrid Zukerman, Eric Horvitz. 2001. Using Machine
Learning Techniques to Interpret WH-questions.
ACL.
John Lafferty, Andrew McCallum, Fernando Pereira.
2001. Conditional Random Fields: probabilistic
Models for Segmenting and Labeling Sequence
Data. Proceedings of the Eighteenth International
Conference on Machine Learning, p.282-289.
Nojun Kwak and Chong-Ho Choi. 2002. Input
feature selection for classification problems.
IEEE Trans on Neural Networks,,13(1):143-
</reference>
<page confidence="0.752028">
159
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.014112">
<title confidence="0.999659">Semantic Chunk Annotation for complex questions using</title>
<author confidence="0.8077835">Random Field Shixi Fan</author>
<affiliation confidence="0.906362">computer science of Technology Shenzhen Graduate School,</affiliation>
<address confidence="0.649564">china</address>
<author confidence="0.989164">Wing W Y Ng</author>
<affiliation confidence="0.958697">computer science of Technology Shenzhen Graduate School,</affiliation>
<address confidence="0.729794">china</address>
<author confidence="0.964518">Xiaolong Wang</author>
<affiliation confidence="0.940119666666667">computer science of Technology Shenzhen Graduate School,</affiliation>
<address confidence="0.686093">china</address>
<abstract confidence="0.858472125">This paper presents a CRF (Conditional Random Field) model for Semantic Chunk Annotation in a Chinese Question and Answering System (SCACQA). The model was derived from a corpus of real world questions, which are collected from some discussion groups on the Internet. The questions are supposed to be answered by other people, so some of the questions are very complex. Mutual information was adopted for feature selection. The training data collection consists of 14000 sentences and the testing data collection consists of 4000 sentences. The result shows an F-score of 93.07%.</abstract>
<author confidence="0.797403">Yaoyun Zhang</author>
<affiliation confidence="0.902302666666667">computer science of Technology Shenzhen Graduate School,</affiliation>
<address confidence="0.720065">china</address>
<author confidence="0.994605">Xuan Wang</author>
<affiliation confidence="0.878376666666667">computer science of Technology Shenzhen Graduate School,</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A M Turing</author>
</authors>
<date>1950</date>
<journal>Computing Machinery and Intelligence. Mind,</journal>
<volume>236</volume>
<issue>59</issue>
<pages>433--460</pages>
<contexts>
<context position="1571" citStr="Turing, 1950" startWordPosition="212" endWordPosition="213">data collection consists of 14000 sentences and the testing data collection consists of 4000 sentences. The result shows an F-score of 93.07%. Yaoyun Zhang Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china Xiaoni5122@gmail.com Xuan Wang Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china wangxuan@insun.hit.edu.cn 1 Introduction 1.1 Introduction of Q&amp;A System Automated question answering has been a hot topic of research and development since the earliest AI applications (A.M. Turing, 1950). Since then there has been a continual interest in processing knowledge and retrieving it efficiently to users automatically. The end of the 1980s saw a boost in information retrieval technologies and applications, with an unprecedented growth in the amount of digital information available, an explosion of growth in the use of computers for communications, and the increasing number of users that have access to all this information (Diego Moll′and Jose′Luis Vicedo, 2007). Search engines such as Google, Yahoo, Baidu and etc have made a great success for people’s information need. Anyhow, search</context>
</contexts>
<marker>Turing, 1950</marker>
<rawString>A.M. Turing. 1950. Computing Machinery and Intelligence. Mind, 236 (59): 433~460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego Moll′</author>
<author>Jose′Luis Vicedo</author>
</authors>
<title>Question Answering in Restricted Domains: An Overview.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<marker>Moll′, Vicedo, 2007</marker>
<rawString>Diego Moll′, Jose′Luis Vicedo. 2007. Question Answering in Restricted Domains: An Overview. Computational Linguistics, 33(1),</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>WeiGuo Fan</author>
<author>Leila Kosseim</author>
</authors>
<title>The QUANTUM Question Answering System.</title>
<date>2001</date>
<publisher>TREC.</publisher>
<contexts>
<context position="4099" citStr="Radev et al, 2001" startWordPosition="590" endWordPosition="593">tance to the integrated performance of a Q&amp;A system. The reason is quite intuitive: a question contains all the information to retrieve the corresponding answer. Misinterpretation or too much loss of information during the processing will inevitably lead to poor precision of the system. The early research efforts and evaluations in Q&amp;A were focused mainly on factoid questions asking for named entities, such as time, numbers, and locations and so on. The questions in the test corpus of TREC and other organizations are also in short and simple form. Complex hierarchy in question types (Dragomir Radev et al, 2001), question templates (Min-Yuh Day et al, 2005), question parsing (Ulf Hermjakob, 2001) and various machine learning methods (Dell Zhang and Wee Sun Lee, 2003)are used for factoid question analysis, aiming to find what named entity is asked in the question. There are some questions which are very complicated or even need domain restricted knowledge and reasoning technique. Automatic Q&amp;A system can not deal with such questions with current technique. In china, there is a new kind of web based Q&amp;A system which is a special kind of discussion group. Unlike common discussion group, in the web based</context>
</contexts>
<marker>Radev, Fan, Kosseim, 2001</marker>
<rawString>Dragomir Radev, WeiGuo Fan, Leila Kosseim. 2001. The QUANTUM Question Answering System. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yuh Day</author>
<author>Cheng-Wei Lee</author>
<author>Shih-Hung WU</author>
<author>Chormg-Shyong Ong</author>
<author>Wen-Lian Hsu</author>
</authors>
<title>An Integrated Knowledge-based and Machine Learning Approach for Chinese Question Classification.</title>
<date>2005</date>
<booktitle>Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<location>Wuhan, China,:620~625.</location>
<contexts>
<context position="4145" citStr="Day et al, 2005" startWordPosition="597" endWordPosition="600">tem. The reason is quite intuitive: a question contains all the information to retrieve the corresponding answer. Misinterpretation or too much loss of information during the processing will inevitably lead to poor precision of the system. The early research efforts and evaluations in Q&amp;A were focused mainly on factoid questions asking for named entities, such as time, numbers, and locations and so on. The questions in the test corpus of TREC and other organizations are also in short and simple form. Complex hierarchy in question types (Dragomir Radev et al, 2001), question templates (Min-Yuh Day et al, 2005), question parsing (Ulf Hermjakob, 2001) and various machine learning methods (Dell Zhang and Wee Sun Lee, 2003)are used for factoid question analysis, aiming to find what named entity is asked in the question. There are some questions which are very complicated or even need domain restricted knowledge and reasoning technique. Automatic Q&amp;A system can not deal with such questions with current technique. In china, there is a new kind of web based Q&amp;A system which is a special kind of discussion group. Unlike common discussion group, in the web based Q&amp;A system one user posts a question, other u</context>
</contexts>
<marker>Day, Lee, WU, Ong, Hsu, 2005</marker>
<rawString>Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU, Chormg-Shyong Ong, Wen-Lian Hsu. 2005. An Integrated Knowledge-based and Machine Learning Approach for Chinese Question Classification. Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering, Wuhan, China,:620~625.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
</authors>
<title>Parsing and Question Classification for Question Answering.</title>
<date>2001</date>
<booktitle>Proceedings of the ACL Workshop on OpenDomain Question Answering, Toulouse,:19~25.</booktitle>
<contexts>
<context position="4185" citStr="Hermjakob, 2001" startWordPosition="604" endWordPosition="605">estion contains all the information to retrieve the corresponding answer. Misinterpretation or too much loss of information during the processing will inevitably lead to poor precision of the system. The early research efforts and evaluations in Q&amp;A were focused mainly on factoid questions asking for named entities, such as time, numbers, and locations and so on. The questions in the test corpus of TREC and other organizations are also in short and simple form. Complex hierarchy in question types (Dragomir Radev et al, 2001), question templates (Min-Yuh Day et al, 2005), question parsing (Ulf Hermjakob, 2001) and various machine learning methods (Dell Zhang and Wee Sun Lee, 2003)are used for factoid question analysis, aiming to find what named entity is asked in the question. There are some questions which are very complicated or even need domain restricted knowledge and reasoning technique. Automatic Q&amp;A system can not deal with such questions with current technique. In china, there is a new kind of web based Q&amp;A system which is a special kind of discussion group. Unlike common discussion group, in the web based Q&amp;A system one user posts a question, other users can give answers to it. It is found</context>
</contexts>
<marker>Hermjakob, 2001</marker>
<rawString>Ulf Hermjakob. 2001. Parsing and Question Classification for Question Answering. Proceedings of the ACL Workshop on OpenDomain Question Answering, Toulouse,:19~25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>Proceedings of the 26th Annual International ACM Conference on Research and Development in Information Retrieval(SIGIR),</booktitle>
<pages>32</pages>
<location>Toronto, Canada,26 ~</location>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang, Wee Sun Lee. 2003. Question classification using support vector machines. Proceedings of the 26th Annual International ACM Conference on Research and Development in Information Retrieval(SIGIR), Toronto, Canada,26 ~ 32.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Valentin Jijkoun</author>
</authors>
<title>Maarten de Rijke.2005. Retrieving Answers from Frequently Asked Questions Pages on the Web. CIKM’05,</title>
<location>Bermen, Germany.</location>
<marker>Jijkoun, </marker>
<rawString>Valentin Jijkoun, Maarten de Rijke.2005. Retrieving Answers from Frequently Asked Questions Pages on the Web. CIKM’05, Bermen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
</authors>
<title>Interrogative Reformulation Patterns and Acquisition of Question Paraphrases.</title>
<date>2003</date>
<booktitle>Proceeding of the Second International Workshop on Paraphrasing,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="6468" citStr="Tomuro, 2003" startWordPosition="979" endWordPosition="980">estions. From 2006, TREC launched a new annually evaluation CIQ&amp;A (complex, interactive Question Answering), aiming to promote the development of interactive systems capable of addressing complex information needs. The targets of national programs AQUAINT and QUETAL are all at new interface and new enhancements to current stateof-the-art Q&amp;A systems to handle more complex inputs and situations. A few researchers and institutions serve as pioneers in complex questions study. Different technologies, such as definitions of different sets of question types, templates and sentence patterns (Noriko Tomuro, 2003) (Hyo-Jung Oh et al, 2005) machine learning methods (Radu Soricut and Eric Brill, 2004), language translation model (Jiwoon Jeon, W et al, 2005), composition of information needs of the complex question (Sanda Harabagiu et al, 2006) and so on, have been experimented on the processing of complex question, gearing the acquired information to the facility of other Q&amp;A modules. Several major problems faced now by researcher of complex questions are stated as follow: First: Unlike factoid questions, it is very difficult to define a comprehensive type hierarchy for complex questions. Different domai</context>
</contexts>
<marker>Tomuro, 2003</marker>
<rawString>Noriko Tomuro. 2003. Interrogative Reformulation Patterns and Acquisition of Question Paraphrases. Proceeding of the Second International Workshop on Paraphrasing, :33~40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyo-Jung Oh</author>
<author>Chung-Hee Lee</author>
<author>Hyeon-Jin Kim</author>
<author>Myung-Gil Jang</author>
</authors>
<title>Descriptive Question Answering in Encyclopedia.</title>
<date>2005</date>
<booktitle>Proceedings of the ACL Interactive Poster and Demonstration Sessions,</booktitle>
<pages>21--24</pages>
<location>Ann Arbor.</location>
<contexts>
<context position="6494" citStr="Oh et al, 2005" startWordPosition="982" endWordPosition="985"> launched a new annually evaluation CIQ&amp;A (complex, interactive Question Answering), aiming to promote the development of interactive systems capable of addressing complex information needs. The targets of national programs AQUAINT and QUETAL are all at new interface and new enhancements to current stateof-the-art Q&amp;A systems to handle more complex inputs and situations. A few researchers and institutions serve as pioneers in complex questions study. Different technologies, such as definitions of different sets of question types, templates and sentence patterns (Noriko Tomuro, 2003) (Hyo-Jung Oh et al, 2005) machine learning methods (Radu Soricut and Eric Brill, 2004), language translation model (Jiwoon Jeon, W et al, 2005), composition of information needs of the complex question (Sanda Harabagiu et al, 2006) and so on, have been experimented on the processing of complex question, gearing the acquired information to the facility of other Q&amp;A modules. Several major problems faced now by researcher of complex questions are stated as follow: First: Unlike factoid questions, it is very difficult to define a comprehensive type hierarchy for complex questions. Different domains under research may requ</context>
</contexts>
<marker>Oh, Lee, Kim, Jang, 2005</marker>
<rawString>Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, Myung-Gil Jang. 2005. Descriptive Question Answering in Encyclopedia. Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 21–24, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
</authors>
<title>Eric Brill.</title>
<date>2004</date>
<booktitle>Proceedings of HLT-NAACL</booktitle>
<pages>57--64</pages>
<marker>Soricut, 2004</marker>
<rawString>Radu Soricut, Eric Brill. 2004, Automatic Question Answering: Beyond the Factoid. Proceedings of HLT-NAACL ,:57~64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding Similar Questions in Large Question and Answer Archives.</title>
<date>2005</date>
<location>CIKM’05, Bremen, Germany.</location>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005. Finding Similar Questions in Large Question and Answer Archives. CIKM’05, Bremen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Finley Lacatusu</author>
<author>Andrew Hickl</author>
</authors>
<date>2006</date>
<booktitle>Answering Complex Questions with Random Walk Models. SIGIR’06,</booktitle>
<pages>220--227</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="6700" citStr="Harabagiu et al, 2006" startWordPosition="1013" endWordPosition="1016">ets of national programs AQUAINT and QUETAL are all at new interface and new enhancements to current stateof-the-art Q&amp;A systems to handle more complex inputs and situations. A few researchers and institutions serve as pioneers in complex questions study. Different technologies, such as definitions of different sets of question types, templates and sentence patterns (Noriko Tomuro, 2003) (Hyo-Jung Oh et al, 2005) machine learning methods (Radu Soricut and Eric Brill, 2004), language translation model (Jiwoon Jeon, W et al, 2005), composition of information needs of the complex question (Sanda Harabagiu et al, 2006) and so on, have been experimented on the processing of complex question, gearing the acquired information to the facility of other Q&amp;A modules. Several major problems faced now by researcher of complex questions are stated as follow: First: Unlike factoid questions, it is very difficult to define a comprehensive type hierarchy for complex questions. Different domains under research may require definitions of different sets of question types, as shown in (Hyo-Jung Oh et al, 2005). Especially, the types of certain ques2 http://zhidao.baidu.com/ 3 http://iask.sina.com.cn/ 2 tions are ambiguous a</context>
</contexts>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu, Finley Lacatusu and Andrew Hickl. 2006 . Answering Complex Questions with Random Walk Models. SIGIR’06, Seattle, Washington, USA.pp220-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
</authors>
<title>Eric Horvitz.</title>
<date>2001</date>
<publisher>ACL.</publisher>
<marker>Zukerman, 2001</marker>
<rawString>Ingrid Zukerman, Eric Horvitz. 2001. Using Machine Learning Techniques to Interpret WH-questions. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="13462" citStr="Lafferty, et al (2001)" startWordPosition="2070" endWordPosition="2073"> Wtim The answer is the data or time length about the event in the question Entity Went The answer is the attribute of the topic. Table 1: Semantic chunks An annotation example question is as follows: This question can be annotated as follows: This kind of annotation is not convenient for CRF model, so the tags were transfer into the B I O form. (Shown as follows) Then the Semantic chunk annotation can be treated as a sequence tag problem. 3 Semantic Chunk Annotation model 3.1 Overview of the CRF model The conditional random field (CRF) is a discriminative probabilistic model proposed by John Lafferty, et al (2001) to overcome the long-range dependencies problems associated with generative models. CRF was originally designed to label and segment sequences of observations, but can be used more generally. Let X, Y be random variables over observed data sequences and corresponding label sequences, respectively. For simplicity of descriptions, we assume that the random variable sequences X and Y have the same length, and use x = [x1, x2 xm ] and y = [y1 , y2 ym ] to represent instances of X and Y, respectively. CRF defines the conditional probability distribution P(Y |X) of label sequences given observation</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, Fernando Pereira. 2001. Conditional Random Fields: probabilistic Models for Segmenting and Labeling Sequence Data. Proceedings of the Eighteenth International Conference on Machine Learning, p.282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nojun Kwak</author>
<author>Chong-Ho Choi</author>
</authors>
<title>Input feature selection for classification problems.</title>
<date>2002</date>
<journal>IEEE Trans on Neural Networks,,13(1):143-</journal>
<marker>Kwak, Choi, 2002</marker>
<rawString>Nojun Kwak and Chong-Ho Choi. 2002. Input feature selection for classification problems. IEEE Trans on Neural Networks,,13(1):143-</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>