<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003669">
<title confidence="0.988209">
Question Answering Using Enhanced Lexical Semantic Models
</title>
<author confidence="0.981495">
Wen-tau Yih Ming-Wei Chang Christopher Meek Andrzej Pastusiak
</author>
<affiliation confidence="0.958607">
Microsoft Research
</affiliation>
<address confidence="0.918883">
Redmond, WA 98052, USA
</address>
<email confidence="0.99738">
{scottyih,minchang,meek,andrzejp}@microsoft.com
</email>
<sectionHeader confidence="0.997365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971210526316">
In this paper, we study the answer
sentence selection problem for ques-
tion answering. Unlike previous work,
which primarily leverages syntactic analy-
sis through dependency tree matching, we
focus on improving the performance us-
ing models of lexical semantic resources.
Experiments show that our systems can
be consistently and significantly improved
with rich lexical semantic information, re-
gardless of the choice of learning algo-
rithms. When evaluated on a bench-
mark dataset, the MAP and MRR scores
are increased by 8 to 10 points, com-
pared to one of our baseline systems using
only surface-form matching. Moreover,
our best system also outperforms pervious
work that makes use of the dependency
tree structure by a wide margin.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977294068965517">
Open-domain question answering (QA), which
fulfills a user’s information need by outputting di-
rect answers to natural language queries, is a chal-
lenging but important problem (Etzioni, 2011).
State-of-the-art QA systems often implement a
complicated pipeline architecture, consisting of
question analysis, document or passage retrieval,
answer selection and verification (Ferrucci, 2012;
Moldovan et al., 2003). In this paper, we focus
on one of the key subtasks – answer sentence se-
lection. Given a question and a set of candidate
sentences, the task is to choose the correct sen-
tence that contains the exact answer and can suf-
ficiently support the answer choice. For instance,
although both of the following sentences contain
the answer “Jack Lemmon” to the question “Who
won the best actor Oscar in 1973?” only the first
sentence is correct.
A1: Jack Lemmon won the Academy Award for
Best Actor for Save the Tiger (1973).
A2: Oscar winner Kevin Spacey said that Jack
Lemmon is remembered as always making
time for other people.
One of the benefits of answer sentence selec-
tion is that the output can be provided directly to
the user. Instead of outputting only the answer, re-
turning the whole sentence often adds more value
as the user can easily verify the correctness with-
out reading a lengthy document.
Answer sentence selection can be naturally re-
duced to a semantic text matching problem. Con-
ceptually, we would like to measure how close
the question and sentence can be matched seman-
tically. Due to the variety of word choices and
inherent ambiguities in natural languages, bag-of-
words approaches with simple surface-form word
matching tend to produce brittle results with poor
prediction accuracy (Bilotti et al., 2007). As a
result, researchers put more emphasis on exploit-
ing both the syntactic and semantic structure in
questions/sentences. Representative examples in-
clude methods based on deeper semantic anal-
ysis (Shen and Lapata, 2007; Moldovan et al.,
2007) and on tree edit-distance (Punyakanok et
al., 2004; Heilman and Smith, 2010) and quasi-
synchronous grammar (Wang et al., 2007) that
match the dependency parse trees of questions and
sentences. However, such approaches often re-
quire more computational resources. In addition
to applying a syntactic or semantic parser during
run-time, finding the best matching between struc-
tured representations of sentences is not trivial.
For example, the computational complexity of tree
matching is O(V 2L4), where V is the number of
nodes and L is the maximum depth (Tai, 1979).
Instead of focusing on the high-level seman-
tic representation, we turn our attention in this
work to improving the shallow semantic compo-
</bodyText>
<page confidence="0.941213">
1744
</page>
<note confidence="0.9128895">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744–1753,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999414205882353">
nent, lexical semantics. We formulate answer se-
lection as a semantic matching problem with a la-
tent word-alignment structure as in (Chang et al.,
2010) and conduct a series of experimental stud-
ies on leveraging recently proposed lexical seman-
tic models. Our main contributions in this work
are two key findings. First, by incorporating the
abundant information from a variety of lexical se-
mantic models, the answer selection system can
be enhanced substantially, regardless of the choice
of learning algorithms and settings. Compared to
the previous work, our latent alignment model im-
proves the result on a benchmark dataset by a wide
margin – the mean average precision (MAP) and
mean reciprocal rank (MRR) scores are increased
by 25.6% and 18.8%, respectively. Second, while
the latent alignment model performs better than
unstructured models, the difference diminishes af-
ter adding the enhanced lexical semantics infor-
mation. This may suggest that compared to in-
troducing complex structured constraints, incorpo-
rating shallow semantic information is both more
effective and computationally inexpensive in im-
proving the performance, at least for the specific
word alignment model tested in this work.
The rest of the paper is structured as follows.
We first survey the related work in Sec. 2. Sec. 3
defines the problem of answer sentence selection,
along with the high-level description of our solu-
tion. The enhanced lexical semantic models and
the learning frameworks we explore are presented
in Sec. 4 and Sec. 5, respectively. Our experimen-
tal results on a benchmark QA dataset is shown in
Sec. 6. Finally, Sec. 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99993252631579">
While the task of question answering has a long
history dated back to the dawn of artificial in-
telligence, early systems like STUDENT (Wino-
grad, 1977) and LUNAR (Woods, 1973) are typ-
ically designed to demonstrate natural language
understanding for a small and specific domain.
The Text REtrieval Conference (TREC) Question
Answering Track was arguably the first large-
scale evaluation of open-domain question answer-
ing (Voorhees and Tice, 2000). The task is de-
signed in an information retrieval oriented setting.
Given a factoid question along with a collection
of documents, a system is required to return the
exact answer, along with the document that sup-
ports the answer. In contrast, the Jeopardy! TV
quiz show provides another open-domain question
answering setting, in which IBM’s Watson system
famously beat the two highest ranked players (Fer-
rucci, 2012). Questions in this game are presented
in a statement form and the system needs to iden-
tify the true question and to give the exact answer.
A short sentence or paragraph to justify the answer
is not required in either TREC-QA or Jeopardy!
As any QA system can virtually be decomposed
into two major high-level components, retrieval
and selection (Echihabi and Marcu, 2003), the an-
swer selection problem is clearly critical. Limiting
the scope of an answer to a sentence is first high-
lighted by Wang et al. (2007), who argued that it
was more informative to present the whole sen-
tence instead of a short answer to users.
Observing the limitations of the bag-of-words
models, Wang et al. (2007) proposed a syntax-
driven approach, where each pair of question and
sentence are matched by their dependency trees.
The mapping is learned by a generative probabilis-
tic model based on a Quasi-synchronous Gram-
mar formulation (Smith and Eisner, 2006). This
approach was later improved by Wang and Man-
ning (2010) with a tree-edit CRF model that learns
the latent alignment structure. In contrast, gen-
eral tree matching methods based on tree-edit dis-
tance have been first proposed by Punyakanok et
al. (2004) for a similar answer selection task. Heil-
man and Smith (2010) proposed a discriminative
approach that first computes a tree kernel func-
tion between the dependency trees of the question
and candidate sentence, and then learns a classifier
based on the tree-edit features extracted.
Although lexical semantic information derived
from WordNet has been used in some of these
approaches, the research has mainly focused
on modeling the mapping between the syntac-
tic structures of questions and sentences, pro-
duced from syntactic analysis. The potential im-
provement from enhanced lexical semantic mod-
els seems to have been deliberately overlooked.1
</bodyText>
<sectionHeader confidence="0.9934" genericHeader="method">
3 Problem Definition
</sectionHeader>
<bodyText confidence="0.999519">
We consider the answer selection problem in a
supervised learning setting. For a set of ques-
tions {q1, · · · , qm}, each question qi is associated
with a list of labeled candidate answer sentences
</bodyText>
<footnote confidence="0.987497666666667">
1For example, Heilman and Smith (2010) emphasized that
“The tree edit model, which does not use lexical semantics
knowledge, produced the best result reported to date.”
</footnote>
<page confidence="0.991076">
1745
</page>
<figure confidence="0.667886">
What is the fastest car in the world?
The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet.
</figure>
<figureCaption confidence="0.973267">
Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan,
2001). Words connected by solid lines are clear synonyms or hyponym/hypernym; words with weaker
semantic association are linked by dashed lines.
</figureCaption>
<bodyText confidence="0.9949253">
f(yi1, si1), (yi1, si2), ··· , (yi , sin)I, where yi, =
1 indicates that sentence si, is a correct answer to
question qi, and 0 otherwise. Using this labeled
data, our goal is to learn a probabilistic classifier
to predict the label of a new, unseen pair of ques-
tion and sentence.
Fundamentally, what the classifier predicts is
whether the sentence “matches” the question se-
mantically. In other words, does s have the an-
swer that satisfies the semantic constraints pro-
vided in the question? Without representing the
question and sentence in logic or syntactic trees,
we take a word-alignment view for solving this
problem. We assume that there is an underly-
ing structure h that describes how q and s can
be associated through the relations of the words
in them. Figure 1 illustrates this setting using a
revised example from (Harabagiu and Moldovan,
2001). In this figure, words connected by solid
lines are clear synonyms or hyponym/hypernym;
words connected by dashed lines indicate that they
are weakly related. With this alignment structure,
features like the degree of mapping or whether all
the content words in the question can be mapped
to some words in the sentence can be extracted and
help improve the classifier. Notice that the struc-
ture representation in terms of word-alignment is
fairly general. For instance, if we assume a naive
complete bipartite matching, then effectively it re-
duces to the simple bag-of-words model.
Typically, the “ideal” alignment structure is not
available in the data, and previous work exploited
mostly syntactic analysis (e.g., dependency trees)
to reveal the latent mapping structure. In this
work, we focus our study on leveraging the low-
level semantic cues from recently proposed lexical
semantic models. As will be shown in our experi-
ments, such information not only improves a latent
structure learning method, but also makes a simple
bipartite matching approach extremely strong.2
</bodyText>
<sectionHeader confidence="0.993804" genericHeader="method">
4 Lexical Semantic Models
</sectionHeader>
<bodyText confidence="0.999675777777778">
In this section, we introduce the lexical seman-
tic models we adopt for solving the semantic
matching problem in answer selection. To go be-
yond the simple, limited surface-form matching,
we aim to pair words that are semantically re-
lated, specifically measured by models of word
relations including synonymy/antonymy, hyper-
nymy/hyponymy (the Is-A relation) and general se-
mantic word similarity.
</bodyText>
<subsectionHeader confidence="0.996234">
4.1 Synonymy and Antonymy
</subsectionHeader>
<bodyText confidence="0.999992157894737">
Among all the word relations, synonymy is per-
haps the most basic one and needs to be handled
reliably. Although sets of synonyms can be eas-
ily found in thesauri or WordNet synsets, such
resources typically cover only strict synonyms.
When comparing two words, it is more useful to
estimate the degree of synonymy as well. For in-
stance, ship and boat are not strict synonyms be-
cause a ship is usually viewed as a large boat.
Knowing that two words are somewhat synony-
mous could be valuable in determining whether
they should be mapped.
In order to estimate the degree of synonymy, we
leverage a recently proposed polarity-inducing la-
tent semantic analysis (PILSA) model (Yih et al.,
2012). Given a thesaurus, the model first con-
structs a signed d-by-n co-occurrence matrix W,
where d is the number of word groups and n is
the size of the vocabulary. Each row consists of a
</bodyText>
<footnote confidence="0.673938833333333">
2Proposed by an anonymous reviewer, one justification of
this word-alignment approach, where syntactic analysis plays
a less important role, is that there are often few sensible com-
binations of words. For instance, knowing only the set of
words {”car”, ”fastest”, ”world”}, one may still guess cor-
rectly the question “What is the fastest car in the world?”
</footnote>
<page confidence="0.973513">
1746
</page>
<bodyText confidence="0.9999885">
group of synonyms and antonyms of a particular
sense and each column represents a unique word.
Values of the elements in each row vector are the
TFIDF values of the corresponding words in this
group. The notion of polarity is then induced by
making the values of words in the antonym groups
negative, and the matrix is generalized by a low-
rank approximation derived by singular-value de-
composition (SVD) in the end. This design has an
intriguing property – if the cosine score of two col-
umn vectors are positive, then the two correspond-
ing words tend to be synonymous; if it’s negative,
then the two words are antonymous. The degree is
measured by the absolute value.
Following the setting described in (Yih et al.,
2012), we construct a PILSA model based on the
Encarta thesaurus and enhance it with a discrimi-
native projection matrix training method. The es-
timated degrees of both synonymy and antonymy
are used our experiments.3
</bodyText>
<subsectionHeader confidence="0.992597">
4.2 Hypernymy and Hyponymy
</subsectionHeader>
<bodyText confidence="0.998987438596491">
The Class-Inclusion or Is-A relation is commonly
observed between words in questions and answer
sentences. For example, to correctly answer the
question “What color is Saturn?”, it is crucial that
the selected sentence mentions a specific kind of
color, as in “Saturn is a giant gas planet with
brown and beige clouds.” Another example is
“Who wrote Moonlight Sonata?”, where compose
in “Ludwig van Beethoven composed the Moon-
light Sonata in 1801.” is one kind of write.
Traditionally, WordNet taxonomy is the linguis-
tic resource for identifying hypernyms and hy-
ponyms, applied broadly to many NLP problems.
However, WordNet has a number of well-known
limitations including its rather limited or skewed
concept distribution and the lack of the coverage
of the Is-A relation (Song et al., 2011). For in-
stance, when a word refers to a named entity, the
particular sense and meaning is often not encoded.
As a result, relations such as “Apple” is-a “com-
pany” and “Jaguar” is-a “car” cannot be found in
WordNet. Similar to the case in synonymy, the
Is-A relation defined in WordNet does not provide
a native, real-valued degree of the relation, which
can only be roughly approximated using the num-
ber of links on the taxonomy path connecting two
3Mapping two antonyms may be desired if one of them is
in the scope of negation (Morante and Blanco, 2012; Blanco
and Moldovan, 2011). However, we do not attempt to resolve
the negation scope in this work.
concepts (Resnik, 1995).
In order to remedy these issues, we aug-
ment WordNet with the Is-A relations found in
Probase (Wu et al., 2012). Probase is a knowledge
base that establishes connections between 2.7 mil-
lion concepts, discovered automatically by apply-
ing Hearst patterns (Hearst, 1992) to 1.68 billion
Web pages. Its abundant concept coverage dis-
tinguishes it from other knowledge bases, such as
Freebase (Bollacker et al., 2008) and WikiTaxon-
omy (Ponzetto and Strube, 2007). Based on the
frequency of term co-occurrences, each Is-A rela-
tion from Probase is associated with a probability
value, indicating the degree of the relation.
We verified the quality of Probase Is-A relations
using a recently proposed SemEval task of rela-
tional similarity (Jurgens et al., 2012) in a com-
panion paper (Zhila et al., 2013), where a subset
of the data is to measure the degree of two words
having a class-inclusion relation. Probase’s pre-
diction correlates well with the human annotations
and achieves a high Spearman’s rank correlation
coefficient score, p = 0.619. In comparison, the
previous best system (Rink and Harabagiu, 2012)
in the task only reaches p = 0.233. These appeal-
ing qualities make Probase a robust lexical seman-
tic model for hypernymy/hyponymy.
</bodyText>
<subsectionHeader confidence="0.998768">
4.3 Semantic Word Similarity
</subsectionHeader>
<bodyText confidence="0.99997655">
The third lexical semantic model we introduce tar-
gets a general notion of word similarity. Unlike
synonymy and hyponymy, word similarity is only
loosely defined when two words can be associated
by some implicit relation.4 The general word sim-
ilarity model can be viewed as a “back-off” so-
lution when the exact lexical relation (e.g., part-
whole and attribute) is not available or cannot be
accurately detected.
Among various word similarity models (Agirre
et al., 2009; Reisinger and Mooney, 2010;
Gabrilovich and Markovitch, 2007; Radinsky et
al., 2011), the vector space models (VSMs) based
on the idea of distributional similarity (Turney
and Pantel, 2010) are often used as the core com-
ponent. Inspired by (Yih and Qazvinian, 2012),
which argues the importance of incorporating het-
erogeneous vector space models for measuring
word similarity, we leverage three different VSMs
in this work: Wiki term-vectors, recurrent neural
</bodyText>
<footnote confidence="0.997445666666667">
4Instead of making the distinction, word similarity here
refers to the larger set of relations commonly covered by word
relatedness (Budanitsky and Hirst, 2006).
</footnote>
<page confidence="0.989727">
1747
</page>
<bodyText confidence="0.999972522727273">
network language model (RNNLM) and a concept
vector space model learned from click-through
data. Semantic word similarity is estimated using
the cosine score of the corresponding word vectors
in these VSMs.
Contextual term-vectors created using the
Wikipedia corpus have shown to perform well
on measuring word similarity (Reisinger and
Mooney, 2010). Following the setting suggested
by Yih and Qazvinian (2012), we create term-
vectors representing about 1 million words by ag-
gregating terms within a window of [−10,10] of
each occurrence of the target word. The vectors
are further refined by applying the same vocabu-
lary and feature pruning techniques.
A recurrent neural network language
model (Mikolov et al., 2010) aims to esti-
mate the probability of observing a word given its
preceding context. However, one by-product of
this model is the word embedding learned in its
hidden-layer, which can be viewed as capturing
the word meaning in some latent, conceptual
space. As a result, vectors of related words tend
to be close to each other. For this word similarity
model, we take a 640-dimensional version of
RNNLM vectors, which is trained using the
Broadcast News corpus of 320M words.5
The final word relatedness model is a projec-
tion model learned from the click-through data of
a commercial search engine (Gao et al., 2011).
Unlike the previous two models, which are cre-
ated or trained using a text corpus, the input for
this model is pairs of aggregated queries and ti-
tles of pages users click. This parallel data is
used to train a projection matrix for creating the
mapping between words in queries and documents
based on user feedback, using a Siamese neural
network (Yih et al., 2011). Each row vector of
this matrix is the dense vector representation of
the corresponding word in the vocabulary. Perhaps
due to its unique information source, we found this
particular word embedding seems to complement
the other two VSMs and tends to improve the word
similarity measure in general.
</bodyText>
<sectionHeader confidence="0.988197" genericHeader="method">
5 Learning QA Matching Models
</sectionHeader>
<bodyText confidence="0.991005">
In this section, we investigate the effectiveness of
various learning models for matching questions
and sentences, including the bag-of-words setting
</bodyText>
<footnote confidence="0.9895115">
5http://www.fit.vutbr.cz/˜imikolov/
rnnlm/
</footnote>
<bodyText confidence="0.566234">
and the framework of learning latent structures.
</bodyText>
<subsectionHeader confidence="0.635096">
5.1 Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.997624333333333">
The bag-of-words model treats each question and
sentence as an unstructured bag of words. When
comparing a question with a sentence, the model
first matches each word in the question to each
word in the sentence. It then aggregates features
extracted from each of these word pairs to rep-
resent the whole question/sentence pair. A bi-
nary classifier can be trained easily using any ma-
chine learning algorithm in this standard super-
vised learning setting.
Formally, let x = (q, s) be a pair of question q
and sentence s. Let Vq = {wq1, wq2, · · · , wqmI
and Vs = {ws1, ws2, · · · , wsnI be the sets of
words in q and s, respectively. Given a word pair
(wq, ws), where wq E Vq and ws E Vs, feature
functions 01, · · · , 0d map it to a d-dimensional
real-valued feature vector.
We consider two aggregate functions for defin-
ing the feature vectors of the whole ques-
tion/answer pair: average and max.
Together, each question/sentence pair is repre-
sented by a 2d-dimensional feature vector.
We tested two learning algorithms in this set-
ting: logistic regression and boosted decision
trees (Friedman, 2001). The former is the log-
linear model widely used in the NLP community
and the latter is a robust non-linear learning algo-
rithm that has shown great empirical performance.
The bag-of-words model does not require an ad-
ditional inference stage as in structured learning,
which may be computationally expensive. Nev-
ertheless, its lack of structure information could
limit the expressiveness of the model and make it
difficult to capture more sophisticated semantics
in the sentences. To address this concern, we in-
vestigate models of learning latent structures next.
</bodyText>
<subsectionHeader confidence="0.999603">
5.2 Learning Latent Structures
</subsectionHeader>
<bodyText confidence="0.99993675">
One obvious issue of the bag-of-words model is
that words in the unrelated part of the sentence
may still be paired with words in the question,
which introduces noise to the final feature vector.
</bodyText>
<equation confidence="0.7838834">
Φavgj(q, s) = 1 mn wqEVq 0j(wq, ws) (1)
wsEVs
Φmaxj(q, s) = max 0j(wq, ws) (2)
wqEVq
wsEVs
</equation>
<page confidence="0.932711">
1748
</page>
<bodyText confidence="0.99744448">
This is observed in many question/sentence pairs,
such as the one below.
alignment structure, but found that they generally
performed the same. Therefore, we chose the
many-to-one alignment6, where inference can be
solved exactly using a simple greedy algorithm.
Q: Which was the first movie that James Dean
was in?
A: James Dean, who began as an actor on TV
dramas, didn’t make his screen debut until
1951’s “Fixed Bayonet.”
While this sentence correctly answers the ques-
tion, the fact that James Dean began as a TV
actor is unrelated to the question. As a result,
an “ideal” word alignment structure should not
link words in this clause to those in the ques-
tion. In order to leverage the latent structured in-
formation, we adapt a recently proposed frame-
work of learning constrained latent representa-
tions (LCLR) (Chang et al., 2010). LCLR can be
viewed as a variant of Latent-SVM (Felzenszwalb
et al., 2009) with different learning formulations
and a general inference framework. The idea of
LCLR is to replace the decision function of a stan-
dard linear model BTO(x) with
</bodyText>
<equation confidence="0.9256905">
arg max BT O(x, h), (3)
h
</equation>
<bodyText confidence="0.9945816">
where B represents the weight vector and h repre-
sents the latent variables.
In this answer selection task, x = (q, s) rep-
resents a pair of question q and candidate sen-
tence s. As described in Sec. 3, h refers to the
latent alignment between q and s. The intuition
behinds Eq. (3) is: candidate sentence s correctly
answers question q if and only if the decision can
be supported by the best alignment h.
The objective function of LCLR is defined as:
</bodyText>
<equation confidence="0.922971">
minθ 2||B||2 + C
1 �
i ξ2i
s.t. ξi &gt; 1 − yi max BT O(x, h)
h
</equation>
<bodyText confidence="0.999927727272727">
Note that the alignment is latent, so LCLR uses
the binary labels in the training data as feedback
to find the alignment for each example.
The computational difficulty of the inference
problem (Eq. (3)) largely depends on the con-
straints we enforce in the alignment. Complicated
constraints may result in a difficult inference prob-
lem, which can be solved by integer linear pro-
gramming (Roth and Yih, 2007). In this work,
we considered several sets of constraints for the
alignment task, including a two-layer phrase/word
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999635">
We present our experimental results in this sec-
tion by first introducing the data and evaluation
metrics, followed by the results of existing sys-
tems and some baseline methods. We then show
the positive impact of adding information of word
relations from various lexical semantics models,
with some discussion on the limitation of the
word-matching approach.
</bodyText>
<subsectionHeader confidence="0.9899">
6.1 Data &amp; Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999963083333333">
The answer selection dataset we used was orig-
inally created by Wang et al. (2007) based on
the QA track of past Text REtrieval Confer-
ences (TREC-QA). Questions in this dataset are
short factoid questions, such as “What is Crips’
gang color?” In average, each question is associ-
ated with approximately 33 answer candidate sen-
tences. A pair of question and sentence is judged
positive if the sentence contains the exact answer
key and can provide sufficient context as support-
ing evidence.
The training set of the data contains manu-
ally labeled 5,919 question/sentence pairs from
TREC 8-12. The development and testing sets
are both from TREC 13, which contain 1,374
and 1,866 pairs, respectively. The task is treated as
a sentence ranking problem for each question and
thus evaluated in Mean Average Precision (MAP)
and Mean Reciprocal Rank (MRR), using the offi-
cial TREC evaluation program. Following (Wang
et al., 2007), candidate sentences with more than
40 words are removed from evaluation, as well as
questions with only positive or negative candidate
sentences.
</bodyText>
<subsectionHeader confidence="0.900415">
6.2 Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999914285714286">
Several systems have been proposed and tested
using this dataset. Wang et al. (2007) pre-
sented a generative probabilistic model based on
a Quasi-synchronous Grammar formulation and
was later improved by Wang and Manning (2010)
with a tree-edit CRF model that learns the la-
tent alignment structure. In contrast, Heilman and
</bodyText>
<footnote confidence="0.978594666666667">
6Each word in the question needs to be linked to a word
in the sentence. Each word in the sentence can be linked to
zero or multiple words in the question.
</footnote>
<page confidence="0.977186">
1749
</page>
<table confidence="0.71405575">
System MAP MRR
Wang et al. (2007) 0.6029 0.6852
Heilman and Smith (2010) 0.6091 0.6917
Wang and Manning (2010) 0.5951 0.6951
</table>
<tableCaption confidence="0.888494">
Table 1: Test set results of existing methods, taken
from Table 3 of (Wang and Manning, 2010).
</tableCaption>
<table confidence="0.9998346">
Dev Test
Baseline MAP MRR MAP MRR
Random 0.5243 0.5816 0.4708 0.5286
Word Cnt 0.6516 0.7216 0.6263 0.6822
Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071
</table>
<tableCaption confidence="0.999782">
Table 2: Results of three baseline methods.
</tableCaption>
<bodyText confidence="0.999935518518518">
Smith (2010) proposed a discriminative approach
that first computes a tree kernel function between
the dependency trees of the question and candidate
sentence, and then learns a classifier based on the
tree-edit features extracted. Table 1 summarizes
their results on the test set. All these systems in-
corporated lexical semantics features derived from
WordNet and named entity features.
In order to further estimate the difficulty of
this task and dataset, we tested three simple base-
lines. The first is random scoring, which sim-
ply assigns a random score to each candidate sen-
tence. The second one, word count, is to count
how many words in the question that also occur in
the answer sentence, after removing stopwords7,
and lowering the case. Finally, the last base-
line method, weighted word count, is basically the
same as identical word matching, but the count is
re-weighted using the IDF value of the question
word. This is similar to the BM25 ranking func-
tion (Robertson et al., 1995). The results of these
three methods are shown in Table 1.
Somewhat surprisingly, we find that word count
is fairly strong and performs comparably to previ-
ous systems.8 In addition, weighting the question
words with their IDF values further improves the
results.
</bodyText>
<subsectionHeader confidence="0.999085">
6.3 Incorporating Rich Lexical Semantics
</subsectionHeader>
<bodyText confidence="0.999693">
We test the effectiveness of adding rich lexical
semantics information by creating examples of
different feature sets. As described in Sec. 5,
</bodyText>
<footnote confidence="0.94652875">
7We used a list of 101 stopwords, including articles, pro-
nouns and punctuation.
8The finding has been confirmed by the lead author
of (Wang et al., 2007).
</footnote>
<bodyText confidence="0.999800411764706">
all the features are based on the properties of
the pair of a word from the question and a
word from the candidate sentence. Stopwords
are first removed from both questions and sen-
tences and all words are lower-cased. Features
used in the experiments can be categorized into
six types: identical word matching (I), lemma
matching (L), WordNet (WN), enhanced Lexi-
cal Semantics (LS), Named Entity matching (NE)
and Answer type checking (Ans). Inspired by
the weighted word count baseline, all features ex-
cept (Ans) are weighted by the IDF value of the
question word. In other words, the IDF values help
decide the importance of word pairs to the model.
Staring from the our baseline model, weighted
word count, the identical word matching (I) fea-
ture checks whether the pair of words are the
same. Instead of checking the surface form of
the word, lemma matching (L) verifies whether
the two words have the same lemma form. Ar-
guably the most common source of word rela-
tions, WordNet (WN) provides the primitive fea-
tures of whether two words could belong to the
same synset in WordNet, could be antonyms and
whether one is a hypernym of the other. Alter-
natively, the enhanced lexical semantics (LS) fea-
tures apply the models described in Sec. 4 to the
word pair and use their estimated degree of syn-
onymy, antonymy, hyponymy and semantic relat-
edness as features. Named entity matching (NE)
checks whether two words are individually part
of some named entities with the same type. Fi-
nally, when the question word is the WH-word, we
check if the paired word belongs to some phrase
that has the correct answer type using simple rules,
such as “Who should link to a word that is part of
a named entity of type Person.” We created exam-
ples in each round of experiments by augmenting
these features in the same order, and observed how
adding different information helped improve the
model performance.
Three models are included in our study. For
the unstructured, bag-of-words setting, we tested
logistic regression (LR) and boosted decision
trees (BDT). As mentioned in Sec. 5, the features
for the whole question/sentence pair are the aver-
age and max of features of all the word pairs. For
the structured-output setting, we used the frame-
work of learning constrained latent representa-
tion (LCLR) and required that each question word
needed to be mapped to a word in the sentence.
</bodyText>
<page confidence="0.956522">
1750
</page>
<table confidence="0.999794857142857">
LR BDT LCLR
Feature set MAP MRR MAP MRR MAP MRR
1: I 0.6531 0.7071 0.6323 0.6898 0.6629 0.7279
2: I+L 0.6744 0.7223 0.6496 0.6923 0.6815 0.7270
3:I+L+WN 0.7039 0.7705 0.6798 0.7450 0.7316 0.7921
4:I+L+WN+LS 0.7339 0.8107 0.7523 0.8455 0.7626 0.8231
5: All 0.7374 0.8171 0.7495 0.8450 0.7648 0.8255
</table>
<tableCaption confidence="0.996604">
Table 3: Test results of various models and feature groups. Logistic regression (LR) and boosted decision
</tableCaption>
<bodyText confidence="0.986402034482759">
trees (BDT) are the two unstructured models. LCLR is the algorithm for learning latent structures.
Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhanced
Lexical Semantics (LS). All includes these four plus Named Entity matching (NE) and Answer type
checking (Ans).
Hyper-parameters are selected using the ones that
achieve the best MAP score on the development
set. Results of these models and feature sets are
presented in Table 3.
We make two observations from the results.
First, while incorporating more information of the
word pairs in general helps, it is clear that map-
ping words beyond surface-form matching with
the help of WordNet (Line #3 vs. #2) is impor-
tant. Moreover, when richer information from
other lexical semantic models is available, the per-
formance can be further improved (Line #4 vs.
#3). Overall, by simply incorporating more in-
formation on word relations, we gain approxi-
mately 10 points in both MAP and MRR com-
pared to surface-form matching (Line #4 vs. #2),
consistently across all three models. However,
adding more information like named entity match-
ing and answer type verification does not seem to
help much (Line #5 vs. #4). Second, while the
structured-output model usually performs better
than both unstructured models (LCLR vs. LR &amp;
BDT), the performance gain diminishes after more
information of word pairs is available (e.g., Lines
#4 and #5).
</bodyText>
<subsectionHeader confidence="0.999879">
6.4 Limitation of Word Matching Models
</subsectionHeader>
<bodyText confidence="0.99987472">
Although we have demonstrated the benefits of
leveraging various lexical semantic models to help
find the association between words, the problem of
question answering is nevertheless far from solved
using the word-based approach. Examining the
output of the LCLR model with all features on the
development set, we found that there were three
main sources of errors, including uncovered or in-
accurate entity relations, the lack of robust ques-
tion analysis and the need of high-level semantic
representation and inference. While the first two
can be improved by, say, using a better named en-
tity tagger, incorporating other knowledge bases
and building a question classifier, how to solve the
third problem is tricky. Below is an example:
Q: In what film is Gordon Gekko the main char-
acter?
A: He received a best actor Oscar in 1987 for his
role as Gordon Gekko in “Wall Street”.
This is a correct answer sentence because “win-
ning a best actor Oscar” implies that the role Gor-
don Gekko is the main character. It is hard to be-
lieve that a pure word-matching model would be
able to solve this type of “inferential question an-
swering” problem.
</bodyText>
<sectionHeader confidence="0.999459" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999737944444445">
In this paper, we present an experimental study
on solving the answer selection problem using en-
hanced lexical semantic models. Following the
word-alignment paradigm, we find that the rich
lexical semantic information improves the models
consistently in the unstructured bag-of-words set-
ting and also in the framework of learning latent
structures. Another interesting finding we have
is that while the latent structured model, LCLR,
performs better than the other two unstructured
models, the difference diminishes after more in-
formation, including the enhanced lexical seman-
tic knowledge and answer type verification, has
been incorporated. This may suggest that adding
shallow semantic information is more effective
than introducing complex structured constraints,
at least for the specific word alignment model we
experimented with in this work.
</bodyText>
<page confidence="0.967695">
1751
</page>
<bodyText confidence="0.999985592592593">
In the future, we plan to explore several di-
rections. First, although we focus on improv-
ing TREC-style open-domain question answering
in this work, we would like to apply the pro-
posed technology to other QA scenarios, such
as community-based QA (CQA). For instance,
the sentence matching technique can help map a
given question to some questions in an existing
CQA database (e.g., Yahoo! Answers). More-
over, the answer sentence selection scheme could
also be useful in extracting the most related sen-
tences from the answer text to form a summary
answer. Second, because the task of answer sen-
tence selection is very similar to paraphrase de-
tection (Dolan et al., 2004) and recognizing tex-
tual entailment (Dagan et al., 2006), we would like
to investigate whether systems for these tasks can
be improved by incorporating enhanced lexical se-
mantic knowledge as well. Finally, we would like
to improve our system for the answer sentence se-
lection task and for question answering in general.
In addition to following the directions suggested
by the error analysis presented in Sec. 6.4, we plan
to use logic-like semantic representations of ques-
tions and sentences, and explore the role of lexical
semantics for handling questions that require in-
ference.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999786">
We are grateful to Mengqiu Wang for providing
the dataset and helping clarify some issues in the
experiments. We also thank Chris Burges and Hoi-
fung Poon for valuable discussion and the anony-
mous reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999635838235294">
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas¸ca and A. Soroa. 2009. A study on similarity
and relatedness using distributional and WordNet-
based approaches. In Proceedings of NAACL, pages
19–27.
M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007.
Structured retrieval for question answering. In Pro-
ceedings of SIGIR, pages 351–358.
E. Blanco and D. Moldovan. 2011. Semantic repre-
sentation of negation using focus detection. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT 2011).
K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and
J. Taylor. 2008. Freebase: a collaboratively cre-
ated graph database for structuring human knowl-
edge. In ACM Conference on Management of Data
(SIGMOD), pages 1247–1250.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic re-
latedness. Computational Linguistics, 32:13–47,
March.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Discriminative learning over constrained la-
tent representations. In Proceedings of NAACL.
I. Dagan, O. Glickman, and B. Magnini, editors. 2006.
The PASCAL Recognising Textual Entailment Chal-
lenge, volume 3944. Springer-Verlag, Berlin.
W. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of COLING.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 16–23.
Oren Etzioni. 2011. Search needs a shake-up. Nature,
476(7358):25–26.
P. Felzenszwalb, R. Girshick, D. McAllester, and
D. Ramanan. 2009. Object detection with discrim-
inatively trained part based models. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
99(1).
D. Ferrucci. 2012. Introduction to “This is Wat-
son”. IBM Journal of Research and Development,
56(3.4):1–1.
J. Friedman. 2001. Greedy function approximation:
a gradient boosting machine. Annals of Statistics,
29(5):1189–1232.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In AAAI Conference on Artificial
Intelligence (AAAI).
J. Gao, K. Toutanova, and W. Yih. 2011.
Clickthrough-based latent semantic models for web
search. In Proceedings of SIGIR, pages 675–684.
S. Harabagiu and D. Moldovan. 2001. Open-domain
textual question answering. Tutorial of NAACL-
2001.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539–545.
M. Heilman and N. Smith. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and
answers to questions. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 1011–1019.
</reference>
<page confidence="0.856307">
1752
</page>
<reference confidence="0.999912141509434">
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation (Se-
mEval 2012), pages 356–364.
T. Mikolov, M. Karafi´at, L. Burget, J. Cernock´y, and
S. Khudanpur. 2010. Recurrent neural network
based language model. In Annual Conference of
the International Speech Communication Associa-
tion (INTERSPEECH), pages 1045–1048.
D. Moldovan, M. Pas¸ca, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analy-
sis in an open-domain question answering system.
ACM Transactions on Information Systems (TOIS),
21(2):133–154.
D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges.
2007. COGEX: A semantically and contextually en-
riched logic prover for question answering. Journal
of Applied Logic, 5(1):49–69.
R. Morante and E. Blanco. 2012. *SEM 2012 shared
task: Resolving the scope and focus of negation. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantics, pages 265–274.
S. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In AAAI Confer-
ence on Artificial Intelligence (AAAI).
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping
dependencies trees: An application to question an-
swering. In International Symposium on Artificial
Intelligence and Mathematics (AI &amp; Math).
K. Radinsky, E. Agichtein, E. Gabrilovich, and
S. Markovitch. 2011. A word at a time: computing
word relatedness using temporal semantic analysis.
In WWW ’11, pages 337–346.
J. Reisinger and R. Mooney. 2010. Multi-prototype
vector-space models of word meaning. In Proceed-
ings of NAACL.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In International
Joint Conference on Artificial Intelligence (IJCAI).
B. Rink and S. Harabagiu. 2012. UTD: Determining
relational similarity using lexical patterns. In Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012), pages 413–418.
S. Robertson, S. Walker, S. Jones, M. Hancock-
Beaulieu, and M. Gatford. 1995. Okapi at TREC-3.
In Text REtrieval Conference (TREC), pages 109–
109.
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
D. Shen and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP-CoNLL, pages 12–21.
D. Smith and J. Eisner. 2006. Quasi-synchronous
grammars: Alignment by soft projection of syntactic
dependencies. In Proceedings of the HLT-NAACL
Workshop on Statistical Machine Translation, pages
23–30.
Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. 2011.
Short text conceptualization using a probabilistic
knowledgebase. In International Joint Conference
on Artificial Intelligence (IJCAI), pages 2330–2336.
K. Tai. 1979. The tree-to-tree correction problem. J.
ACM, 26(3):422–433, July.
P. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37(1):141–
188.
E. Voorhees and D. Tice. 2000. Building a question
answering test collection. In Proceedings of SIGIR,
pages 200–207.
M. Wang and C. Manning. 2010. Probabilistic tree-
edit models with structured latent variables for tex-
tual entailment and question answering. In Proceed-
ings of COLING.
M. Wang, N. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proceedings of EMNLP-CoNLL.
T. Winograd. 1977. Five lectures on artificial intelli-
gence. In A. Zampolli, editor, Linguistic Structures
Processing, pages 399–520. North Holland.
W. Woods. 1973. Progress in natural language under-
standing: An application to lunar geology. In Pro-
ceedings of the National Computer Conference and
Exposition (AFIPS), pages 441–450.
W. Wu, H. Li, H. Wang, and K. Zhu. 2012. Probase:
a probabilistic taxonomy for text understanding. In
ACM Conference on Management of Data (SIG-
MOD), pages 481–492.
W. Yih and V. Qazvinian. 2012. Measuring word relat-
edness using heterogeneous vector space models. In
Proceedings of NAACL-HLT 2012, pages 616–620.
W. Yih, K. Toutanova, J. Platt, and C. Meek. 2011.
Learning discriminative projections for text similar-
ity measures. In ACL Conference on Natural Lan-
guage Learning (CoNLL), pages 247–256.
W. Yih, G. Zweig, and J. Platt. 2012. Polarity inducing
latent semantic analysis. In Proceedings of EMNLP-
CoNLL, pages 1212–1222.
A. Zhila, W. Yih, C. Meek, G. Zweig, and T. Mikolov.
2013. Combining heterogeneous models for mea-
suring relational similarity. In Proceedings of HLT-
NAACL.
</reference>
<page confidence="0.978022">
1753
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925130">
<title confidence="0.999635">Question Answering Using Enhanced Lexical Semantic Models</title>
<author confidence="0.984573">Wen-tau Yih Ming-Wei Chang Christopher Meek Andrzej</author>
<affiliation confidence="0.956317">Microsoft</affiliation>
<address confidence="0.998744">Redmond, WA 98052,</address>
<abstract confidence="0.9988778">this paper, we study the selection for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>E Alfonseca</author>
<author>K Hall</author>
<author>J Kravalova</author>
<author>M Pas¸ca</author>
<author>A Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNetbased approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>pages</pages>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas¸ca and A. Soroa. 2009. A study on similarity and relatedness using distributional and WordNetbased approaches. In Proceedings of NAACL, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilotti</author>
<author>P Ogilvie</author>
<author>J Callan</author>
<author>E Nyberg</author>
</authors>
<title>Structured retrieval for question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>351--358</pages>
<contexts>
<context position="2694" citStr="Bilotti et al., 2007" startWordPosition="417" endWordPosition="420">ovided directly to the user. Instead of outputting only the answer, returning the whole sentence often adds more value as the user can easily verify the correctness without reading a lengthy document. Answer sentence selection can be naturally reduced to a semantic text matching problem. Conceptually, we would like to measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matc</context>
</contexts>
<marker>Bilotti, Ogilvie, Callan, Nyberg, 2007</marker>
<rawString>M. Bilotti, P. Ogilvie, J. Callan, and E. Nyberg. 2007. Structured retrieval for question answering. In Proceedings of SIGIR, pages 351–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Blanco</author>
<author>D Moldovan</author>
</authors>
<title>Semantic representation of negation using focus detection.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT</booktitle>
<contexts>
<context position="14893" citStr="Blanco and Moldovan, 2011" startWordPosition="2404" endWordPosition="2407">age of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences</context>
</contexts>
<marker>Blanco, Moldovan, 2011</marker>
<rawString>E. Blanco and D. Moldovan. 2011. Semantic representation of negation using focus detection. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bollacker</author>
<author>C Evans</author>
<author>P Paritosh</author>
<author>T Sturge</author>
<author>J Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In ACM Conference on Management of Data (SIGMOD),</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="15401" citStr="Bollacker et al., 2008" startWordPosition="2486" endWordPosition="2489">yms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coe</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In ACM Conference on Management of Data (SIGMOD), pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<pages>32--13</pages>
<contexts>
<context position="17344" citStr="Budanitsky and Hirst, 2006" startWordPosition="2793" endWordPosition="2796"> 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of the corresponding word vectors in these VSMs. Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). Following the setting suggested by Yih and Qazvinian (2012), we create termvectors representing about 1 million words by aggregating terms within a window of [−10,10] of each occurrence of the target word. The vectors are further refined by </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32:13–47, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>D Goldwasser</author>
<author>D Roth</author>
<author>V Srikumar</author>
</authors>
<title>Discriminative learning over constrained latent representations.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="3988" citStr="Chang et al., 2010" startWordPosition="615" endWordPosition="618">ample, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improving the shallow semantic compo1744 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744–1753, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics nent, lexical semantics. We formulate answer selection as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010) and conduct a series of experimental studies on leveraging recently proposed lexical semantic models. Our main contributions in this work are two key findings. First, by incorporating the abundant information from a variety of lexical semantic models, the answer selection system can be enhanced substantially, regardless of the choice of learning algorithms and settings. Compared to the previous work, our latent alignment model improves the result on a benchmark dataset by a wide margin – the mean average precision (MAP) and mean reciprocal rank (MRR) scores are increased by 25.6% and 18.8%, r</context>
<context position="22459" citStr="Chang et al., 2010" startWordPosition="3644" endWordPosition="3647">solved exactly using a simple greedy algorithm. Q: Which was the first movie that James Dean was in? A: James Dean, who began as an actor on TV dramas, didn’t make his screen debut until 1951’s “Fixed Bayonet.” While this sentence correctly answers the question, the fact that James Dean began as a TV actor is unrelated to the question. As a result, an “ideal” word alignment structure should not link words in this clause to those in the question. In order to leverage the latent structured information, we adapt a recently proposed framework of learning constrained latent representations (LCLR) (Chang et al., 2010). LCLR can be viewed as a variant of Latent-SVM (Felzenszwalb et al., 2009) with different learning formulations and a general inference framework. The idea of LCLR is to replace the decision function of a standard linear model BTO(x) with arg max BT O(x, h), (3) h where B represents the weight vector and h represents the latent variables. In this answer selection task, x = (q, s) represents a pair of question q and candidate sentence s. As described in Sec. 3, h refers to the latent alignment between q and s. The intuition behinds Eq. (3) is: candidate sentence s correctly answers question q </context>
</contexts>
<marker>Chang, Goldwasser, Roth, Srikumar, 2010</marker>
<rawString>M. Chang, D. Goldwasser, D. Roth, and V. Srikumar. 2010. Discriminative learning over constrained latent representations. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>The PASCAL Recognising Textual Entailment Challenge,</booktitle>
<volume>volume</volume>
<pages>3944</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="34714" citStr="Dagan et al., 2006" startWordPosition="5681" endWordPosition="5684">en-domain question answering in this work, we would like to apply the proposed technology to other QA scenarios, such as community-based QA (CQA). For instance, the sentence matching technique can help map a given question to some questions in an existing CQA database (e.g., Yahoo! Answers). Moreover, the answer sentence selection scheme could also be useful in extracting the most related sentences from the answer text to form a summary answer. Second, because the task of answer sentence selection is very similar to paraphrase detection (Dolan et al., 2004) and recognizing textual entailment (Dagan et al., 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well. Finally, we would like to improve our system for the answer sentence selection task and for question answering in general. In addition to following the directions suggested by the error analysis presented in Sec. 6.4, we plan to use logic-like semantic representations of questions and sentences, and explore the role of lexical semantics for handling questions that require inference. Acknowledgments We are grateful to Mengqiu Wang for providing the dataset</context>
</contexts>
<marker>Dagan, Glickman, Magnini, editors, 2006</marker>
<rawString>I. Dagan, O. Glickman, and B. Magnini, editors. 2006. The PASCAL Recognising Textual Entailment Challenge, volume 3944. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Dolan</author>
<author>C Quirk</author>
<author>C Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="34658" citStr="Dolan et al., 2004" startWordPosition="5672" endWordPosition="5675">ons. First, although we focus on improving TREC-style open-domain question answering in this work, we would like to apply the proposed technology to other QA scenarios, such as community-based QA (CQA). For instance, the sentence matching technique can help map a given question to some questions in an existing CQA database (e.g., Yahoo! Answers). Moreover, the answer sentence selection scheme could also be useful in extracting the most related sentences from the answer text to form a summary answer. Second, because the task of answer sentence selection is very similar to paraphrase detection (Dolan et al., 2004) and recognizing textual entailment (Dagan et al., 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well. Finally, we would like to improve our system for the answer sentence selection task and for question answering in general. In addition to following the directions suggested by the error analysis presented in Sec. 6.4, we plan to use logic-like semantic representations of questions and sentences, and explore the role of lexical semantics for handling questions that require inference. Acknowledgments W</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>W. Dolan, C. Quirk, and C. Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>A noisy-channel approach to question answering.</title>
<date>2003</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>16--23</pages>
<contexts>
<context position="6727" citStr="Echihabi and Marcu, 2003" startWordPosition="1053" endWordPosition="1056">e exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain question answering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. A short sentence or paragraph to justify the answer is not required in either TREC-QA or Jeopardy! As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improve</context>
</contexts>
<marker>Echihabi, Marcu, 2003</marker>
<rawString>A. Echihabi and D. Marcu. 2003. A noisy-channel approach to question answering. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 16–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
</authors>
<title>Search needs a shake-up.</title>
<date>2011</date>
<journal>Nature,</journal>
<volume>476</volume>
<issue>7358</issue>
<contexts>
<context position="1155" citStr="Etzioni, 2011" startWordPosition="167" endWordPosition="168"> and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin. 1 Introduction Open-domain question answering (QA), which fulfills a user’s information need by outputting direct answers to natural language queries, is a challenging but important problem (Etzioni, 2011). State-of-the-art QA systems often implement a complicated pipeline architecture, consisting of question analysis, document or passage retrieval, answer selection and verification (Ferrucci, 2012; Moldovan et al., 2003). In this paper, we focus on one of the key subtasks – answer sentence selection. Given a question and a set of candidate sentences, the task is to choose the correct sentence that contains the exact answer and can sufficiently support the answer choice. For instance, although both of the following sentences contain the answer “Jack Lemmon” to the question “Who won the best act</context>
</contexts>
<marker>Etzioni, 2011</marker>
<rawString>Oren Etzioni. 2011. Search needs a shake-up. Nature, 476(7358):25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Felzenszwalb</author>
<author>R Girshick</author>
<author>D McAllester</author>
<author>D Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part based models.</title>
<date>2009</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>99</volume>
<issue>1</issue>
<contexts>
<context position="22534" citStr="Felzenszwalb et al., 2009" startWordPosition="3657" endWordPosition="3660">st movie that James Dean was in? A: James Dean, who began as an actor on TV dramas, didn’t make his screen debut until 1951’s “Fixed Bayonet.” While this sentence correctly answers the question, the fact that James Dean began as a TV actor is unrelated to the question. As a result, an “ideal” word alignment structure should not link words in this clause to those in the question. In order to leverage the latent structured information, we adapt a recently proposed framework of learning constrained latent representations (LCLR) (Chang et al., 2010). LCLR can be viewed as a variant of Latent-SVM (Felzenszwalb et al., 2009) with different learning formulations and a general inference framework. The idea of LCLR is to replace the decision function of a standard linear model BTO(x) with arg max BT O(x, h), (3) h where B represents the weight vector and h represents the latent variables. In this answer selection task, x = (q, s) represents a pair of question q and candidate sentence s. As described in Sec. 3, h refers to the latent alignment between q and s. The intuition behinds Eq. (3) is: candidate sentence s correctly answers question q if and only if the decision can be supported by the best alignment h. The o</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2009</marker>
<rawString>P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. 2009. Object detection with discriminatively trained part based models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 99(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ferrucci</author>
</authors>
<title>Introduction to “This is Watson”.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development,</journal>
<pages>56--3</pages>
<contexts>
<context position="1351" citStr="Ferrucci, 2012" startWordPosition="190" endWordPosition="191">8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin. 1 Introduction Open-domain question answering (QA), which fulfills a user’s information need by outputting direct answers to natural language queries, is a challenging but important problem (Etzioni, 2011). State-of-the-art QA systems often implement a complicated pipeline architecture, consisting of question analysis, document or passage retrieval, answer selection and verification (Ferrucci, 2012; Moldovan et al., 2003). In this paper, we focus on one of the key subtasks – answer sentence selection. Given a question and a set of candidate sentences, the task is to choose the correct sentence that contains the exact answer and can sufficiently support the answer choice. For instance, although both of the following sentences contain the answer “Jack Lemmon” to the question “Who won the best actor Oscar in 1973?” only the first sentence is correct. A1: Jack Lemmon won the Academy Award for Best Actor for Save the Tiger (1973). A2: Oscar winner Kevin Spacey said that Jack Lemmon is rememb</context>
<context position="6355" citStr="Ferrucci, 2012" startWordPosition="991" endWordPosition="993">r a small and specific domain. The Text REtrieval Conference (TREC) Question Answering Track was arguably the first largescale evaluation of open-domain question answering (Voorhees and Tice, 2000). The task is designed in an information retrieval oriented setting. Given a factoid question along with a collection of documents, a system is required to return the exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain question answering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. A short sentence or paragraph to justify the answer is not required in either TREC-QA or Jeopardy! As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a sho</context>
</contexts>
<marker>Ferrucci, 2012</marker>
<rawString>D. Ferrucci. 2012. Introduction to “This is Watson”. IBM Journal of Research and Development, 56(3.4):1–1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedman</author>
</authors>
<title>Greedy function approximation: a gradient boosting machine.</title>
<date>2001</date>
<journal>Annals of Statistics,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="20740" citStr="Friedman, 2001" startWordPosition="3359" endWordPosition="3360"> pair of question q and sentence s. Let Vq = {wq1, wq2, · · · , wqmI and Vs = {ws1, ws2, · · · , wsnI be the sets of words in q and s, respectively. Given a word pair (wq, ws), where wq E Vq and ws E Vs, feature functions 01, · · · , 0d map it to a d-dimensional real-valued feature vector. We consider two aggregate functions for defining the feature vectors of the whole question/answer pair: average and max. Together, each question/sentence pair is represented by a 2d-dimensional feature vector. We tested two learning algorithms in this setting: logistic regression and boosted decision trees (Friedman, 2001). The former is the loglinear model widely used in the NLP community and the latter is a robust non-linear learning algorithm that has shown great empirical performance. The bag-of-words model does not require an additional inference stage as in structured learning, which may be computationally expensive. Nevertheless, its lack of structure information could limit the expressiveness of the model and make it difficult to capture more sophisticated semantics in the sentences. To address this concern, we investigate models of learning latent structures next. 5.2 Learning Latent Structures One obv</context>
</contexts>
<marker>Friedman, 2001</marker>
<rawString>J. Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of Statistics, 29(5):1189–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using Wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="16784" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="2708" endWordPosition="2711">ties make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) an</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>E. Gabrilovich and S. Markovitch. 2007. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>K Toutanova</author>
<author>W Yih</author>
</authors>
<title>Clickthrough-based latent semantic models for web search.</title>
<date>2011</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="18683" citStr="Gao et al., 2011" startWordPosition="3008" endWordPosition="3011">10) aims to estimate the probability of observing a word given its preceding context. However, one by-product of this model is the word embedding learned in its hidden-layer, which can be viewed as capturing the word meaning in some latent, conceptual space. As a result, vectors of related words tend to be close to each other. For this word similarity model, we take a 640-dimensional version of RNNLM vectors, which is trained using the Broadcast News corpus of 320M words.5 The final word relatedness model is a projection model learned from the click-through data of a commercial search engine (Gao et al., 2011). Unlike the previous two models, which are created or trained using a text corpus, the input for this model is pairs of aggregated queries and titles of pages users click. This parallel data is used to train a projection matrix for creating the mapping between words in queries and documents based on user feedback, using a Siamese neural network (Yih et al., 2011). Each row vector of this matrix is the dense vector representation of the corresponding word in the vocabulary. Perhaps due to its unique information source, we found this particular word embedding seems to complement the other two V</context>
</contexts>
<marker>Gao, Toutanova, Yih, 2011</marker>
<rawString>J. Gao, K. Toutanova, and W. Yih. 2011. Clickthrough-based latent semantic models for web search. In Proceedings of SIGIR, pages 675–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
</authors>
<title>Open-domain textual question answering. Tutorial of NAACL2001.</title>
<date>2001</date>
<contexts>
<context position="8808" citStr="Harabagiu and Moldovan, 2001" startWordPosition="1395" endWordPosition="1398">y overlooked.1 3 Problem Definition We consider the answer selection problem in a supervised learning setting. For a set of questions {q1, · · · , qm}, each question qi is associated with a list of labeled candidate answer sentences 1For example, Heilman and Smith (2010) emphasized that “The tree edit model, which does not use lexical semantics knowledge, produced the best result reported to date.” 1745 What is the fastest car in the world? The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet. Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan, 2001). Words connected by solid lines are clear synonyms or hyponym/hypernym; words with weaker semantic association are linked by dashed lines. f(yi1, si1), (yi1, si2), ··· , (yi , sin)I, where yi, = 1 indicates that sentence si, is a correct answer to question qi, and 0 otherwise. Using this labeled data, our goal is to learn a probabilistic classifier to predict the label of a new, unseen pair of question and sentence. Fundamentally, what the classifier predicts is whether the sentence “matches” the question semantically. In other words, does s have the answer that satisfies the semantic constra</context>
</contexts>
<marker>Harabagiu, Moldovan, 2001</marker>
<rawString>S. Harabagiu and D. Moldovan. 2001. Open-domain textual question answering. Tutorial of NAACL2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="15257" citStr="Hearst, 1992" startWordPosition="2465" endWordPosition="2466"> the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<contexts>
<context position="3016" citStr="Heilman and Smith, 2010" startWordPosition="466" endWordPosition="469"> measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improving</context>
<context position="7612" citStr="Heilman and Smith (2010)" startWordPosition="1202" endWordPosition="1206"> limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has been used in some of these approaches, the research has mainly focused on modeling the mapping between the syntactic structures of questions and sentences, produced from syntactic analysis. The potential improvement from enhanced lexical semantic models seems to have been deliberately overlooked.1 3 Problem Definiti</context>
<context position="25806" citStr="Heilman and Smith (2010)" startWordPosition="4215" endWordPosition="4218">only positive or negative candidate sentences. 6.2 Baseline Methods Several systems have been proposed and tested using this dataset. Wang et al. (2007) presented a generative probabilistic model based on a Quasi-synchronous Grammar formulation and was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, Heilman and 6Each word in the question needs to be linked to a word in the sentence. Each word in the sentence can be linked to zero or multiple words in the question. 1749 System MAP MRR Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Wang and Manning (2010) 0.5951 0.6951 Table 1: Test set results of existing methods, taken from Table 3 of (Wang and Manning, 2010). Dev Test Baseline MAP MRR MAP MRR Random 0.5243 0.5816 0.4708 0.5286 Word Cnt 0.6516 0.7216 0.6263 0.6822 Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071 Table 2: Results of three baseline methods. Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Table 1 summarizes their resu</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>M. Heilman and N. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1011–1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurgens</author>
<author>S Mohammad</author>
<author>P Turney</author>
<author>K Holyoak</author>
</authors>
<title>SemEval-2012 Task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>356--364</pages>
<contexts>
<context position="15742" citStr="Jurgens et al., 2012" startWordPosition="2540" endWordPosition="2543">edge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, p = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches p = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similari</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak. 2012. SemEval-2012 Task 2: Measuring degrees of relational similarity. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 356–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>M Karafi´at</author>
<author>L Burget</author>
<author>J Cernock´y</author>
<author>S Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Annual Conference of the International Speech Communication Association (INTERSPEECH),</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>T. Mikolov, M. Karafi´at, L. Burget, J. Cernock´y, and S. Khudanpur. 2010. Recurrent neural network based language model. In Annual Conference of the International Speech Communication Association (INTERSPEECH), pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>M Pas¸ca</author>
<author>S Harabagiu</author>
<author>M Surdeanu</author>
</authors>
<title>Performance issues and error analysis in an open-domain question answering system.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Moldovan, Pas¸ca, Harabagiu, Surdeanu, 2003</marker>
<rawString>D. Moldovan, M. Pas¸ca, S. Harabagiu, and M. Surdeanu. 2003. Performance issues and error analysis in an open-domain question answering system. ACM Transactions on Information Systems (TOIS), 21(2):133–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>C Clark</author>
<author>S Harabagiu</author>
<author>D Hodges</author>
</authors>
<title>COGEX: A semantically and contextually enriched logic prover for question answering.</title>
<date>2007</date>
<journal>Journal of Applied Logic,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="2939" citStr="Moldovan et al., 2007" startWordPosition="454" endWordPosition="457">reduced to a semantic text matching problem. Conceptually, we would like to measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-l</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Hodges, 2007</marker>
<rawString>D. Moldovan, C. Clark, S. Harabagiu, and D. Hodges. 2007. COGEX: A semantically and contextually enriched logic prover for question answering. Journal of Applied Logic, 5(1):49–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Morante</author>
<author>E Blanco</author>
</authors>
<title>SEM 2012 shared task: Resolving the scope and focus of negation.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>265--274</pages>
<contexts>
<context position="14865" citStr="Morante and Blanco, 2012" startWordPosition="2400" endWordPosition="2403"> and the lack of the coverage of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the freq</context>
</contexts>
<marker>Morante, Blanco, 2012</marker>
<rawString>R. Morante and E. Blanco. 2012. *SEM 2012 shared task: Resolving the scope and focus of negation. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, pages 265–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from wikipedia.</title>
<date>2007</date>
<booktitle>In AAAI Conference on Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="15446" citStr="Ponzetto and Strube, 2007" startWordPosition="2493" endWordPosition="2496">e scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, p = 0.619. In comparison, the</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from wikipedia. In AAAI Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Mapping dependencies trees: An application to question answering.</title>
<date>2004</date>
<booktitle>In International Symposium on Artificial Intelligence and Mathematics (AI</booktitle>
<publisher>Math).</publisher>
<contexts>
<context position="2990" citStr="Punyakanok et al., 2004" startWordPosition="462" endWordPosition="465">ptually, we would like to measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention</context>
<context position="7550" citStr="Punyakanok et al. (2004)" startWordPosition="1192" endWordPosition="1195">ole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has been used in some of these approaches, the research has mainly focused on modeling the mapping between the syntactic structures of questions and sentences, produced from syntactic analysis. The potential improvement from enhanced lexical semantic models s</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping dependencies trees: An application to question answering. In International Symposium on Artificial Intelligence and Mathematics (AI &amp; Math).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Radinsky</author>
<author>E Agichtein</author>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>A word at a time: computing word relatedness using temporal semantic analysis.</title>
<date>2011</date>
<booktitle>In WWW ’11,</booktitle>
<pages>337--346</pages>
<contexts>
<context position="16808" citStr="Radinsky et al., 2011" startWordPosition="2712" endWordPosition="2715"> semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space</context>
</contexts>
<marker>Radinsky, Agichtein, Gabrilovich, Markovitch, 2011</marker>
<rawString>K. Radinsky, E. Agichtein, E. Gabrilovich, and S. Markovitch. 2011. A word at a time: computing word relatedness using temporal semantic analysis. In WWW ’11, pages 337–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>R Mooney</author>
</authors>
<title>Multi-prototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="16750" citStr="Reisinger and Mooney, 2010" startWordPosition="2704" endWordPosition="2707">0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>J. Reisinger and R. Mooney. 2010. Multi-prototype vector-space models of word meaning. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In International Joint Conference on Artificial Intelligence (IJCAI).</booktitle>
<contexts>
<context position="14989" citStr="Resnik, 1995" startWordPosition="2422" endWordPosition="2423">lar sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>P. Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In International Joint Conference on Artificial Intelligence (IJCAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rink</author>
<author>S Harabagiu</author>
</authors>
<title>UTD: Determining relational similarity using lexical patterns.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>413--418</pages>
<contexts>
<context position="16094" citStr="Rink and Harabagiu, 2012" startWordPosition="2598" endWordPosition="2601">y of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, p = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches p = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity</context>
</contexts>
<marker>Rink, Harabagiu, 2012</marker>
<rawString>B. Rink and S. Harabagiu. 2012. UTD: Determining relational similarity using lexical patterns. In Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 413–418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Robertson</author>
<author>S Walker</author>
<author>S Jones</author>
<author>M HancockBeaulieu</author>
<author>M Gatford</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1995</date>
<booktitle>In Text REtrieval Conference (TREC),</booktitle>
<pages>109--109</pages>
<contexts>
<context position="27137" citStr="Robertson et al., 1995" startWordPosition="4436" endWordPosition="4439">d entity features. In order to further estimate the difficulty of this task and dataset, we tested three simple baselines. The first is random scoring, which simply assigns a random score to each candidate sentence. The second one, word count, is to count how many words in the question that also occur in the answer sentence, after removing stopwords7, and lowering the case. Finally, the last baseline method, weighted word count, is basically the same as identical word matching, but the count is re-weighted using the IDF value of the question word. This is similar to the BM25 ranking function (Robertson et al., 1995). The results of these three methods are shown in Table 1. Somewhat surprisingly, we find that word count is fairly strong and performs comparably to previous systems.8 In addition, weighting the question words with their IDF values further improves the results. 6.3 Incorporating Rich Lexical Semantics We test the effectiveness of adding rich lexical semantics information by creating examples of different feature sets. As described in Sec. 5, 7We used a list of 101 stopwords, including articles, pronouns and punctuation. 8The finding has been confirmed by the lead author of (Wang et al., 2007)</context>
</contexts>
<marker>Robertson, Walker, Jones, HancockBeaulieu, Gatford, 1995</marker>
<rawString>S. Robertson, S. Walker, S. Jones, M. HancockBeaulieu, and M. Gatford. 1995. Okapi at TREC-3. In Text REtrieval Conference (TREC), pages 109– 109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Global inference for entity and relation identification via a linear programming formulation.</title>
<date>2007</date>
<booktitle>In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23642" citStr="Roth and Yih, 2007" startWordPosition="3862" endWordPosition="3865">ence s correctly answers question q if and only if the decision can be supported by the best alignment h. The objective function of LCLR is defined as: minθ 2||B||2 + C 1 � i ξ2i s.t. ξi &gt; 1 − yi max BT O(x, h) h Note that the alignment is latent, so LCLR uses the binary labels in the training data as feedback to find the alignment for each example. The computational difficulty of the inference problem (Eq. (3)) largely depends on the constraints we enforce in the alignment. Complicated constraints may result in a difficult inference problem, which can be solved by integer linear programming (Roth and Yih, 2007). In this work, we considered several sets of constraints for the alignment task, including a two-layer phrase/word 6 Experiments We present our experimental results in this section by first introducing the data and evaluation metrics, followed by the results of existing systems and some baseline methods. We then show the positive impact of adding information of word relations from various lexical semantics models, with some discussion on the limitation of the word-matching approach. 6.1 Data &amp; Evaluation Metrics The answer selection dataset we used was originally created by Wang et al. (2007)</context>
</contexts>
<marker>Roth, Yih, 2007</marker>
<rawString>D. Roth and W. Yih. 2007. Global inference for entity and relation identification via a linear programming formulation. In Lise Getoor and Ben Taskar, editors, Introduction to Statistical Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>M Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>12--21</pages>
<contexts>
<context position="2915" citStr="Shen and Lapata, 2007" startWordPosition="450" endWordPosition="453">ction can be naturally reduced to a semantic text matching problem. Conceptually, we would like to measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead o</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>D. Shen and M. Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of EMNLP-CoNLL, pages 12–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>J Eisner</author>
</authors>
<title>Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="7294" citStr="Smith and Eisner, 2006" startWordPosition="1149" endWordPosition="1152">ents, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>D. Smith and J. Eisner. 2006. Quasi-synchronous grammars: Alignment by soft projection of syntactic dependencies. In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Song</author>
<author>H Wang</author>
<author>Z Wang</author>
<author>H Li</author>
<author>W Chen</author>
</authors>
<title>Short text conceptualization using a probabilistic knowledgebase.</title>
<date>2011</date>
<booktitle>In International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>2330--2336</pages>
<contexts>
<context position="14311" citStr="Song et al., 2011" startWordPosition="2301" endWordPosition="2304">”, it is crucial that the selected sentence mentions a specific kind of color, as in “Saturn is a giant gas planet with brown and beige clouds.” Another example is “Who wrote Moonlight Sonata?”, where compose in “Ludwig van Beethoven composed the Moonlight Sonata in 1801.” is one kind of write. Traditionally, WordNet taxonomy is the linguistic resource for identifying hypernyms and hyponyms, applied broadly to many NLP problems. However, WordNet has a number of well-known limitations including its rather limited or skewed concept distribution and the lack of the coverage of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do n</context>
</contexts>
<marker>Song, Wang, Wang, Li, Chen, 2011</marker>
<rawString>Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. 2011. Short text conceptualization using a probabilistic knowledgebase. In International Joint Conference on Artificial Intelligence (IJCAI), pages 2330–2336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tai</author>
</authors>
<title>The tree-to-tree correction problem.</title>
<date>1979</date>
<journal>J. ACM,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="3504" citStr="Tai, 1979" startWordPosition="545" endWordPosition="546">Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improving the shallow semantic compo1744 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744–1753, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics nent, lexical semantics. We formulate answer selection as a semantic matching problem with a latent word-alignment structure as in (Chang et al., 2010) and conduct a series of experimental studies on leveraging recently proposed lexical semantic models. Our main cont</context>
</contexts>
<marker>Tai, 1979</marker>
<rawString>K. Tai. 1979. The tree-to-tree correction problem. J. ACM, 26(3):422–433, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<pages>188</pages>
<contexts>
<context position="16913" citStr="Turney and Pantel, 2010" startWordPosition="2728" endWordPosition="2731">e introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of t</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141– 188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D Tice</author>
</authors>
<title>Building a question answering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="5937" citStr="Voorhees and Tice, 2000" startWordPosition="923" endWordPosition="926">explore are presented in Sec. 4 and Sec. 5, respectively. Our experimental results on a benchmark QA dataset is shown in Sec. 6. Finally, Sec. 7 concludes the paper. 2 Related Work While the task of question answering has a long history dated back to the dawn of artificial intelligence, early systems like STUDENT (Winograd, 1977) and LUNAR (Woods, 1973) are typically designed to demonstrate natural language understanding for a small and specific domain. The Text REtrieval Conference (TREC) Question Answering Track was arguably the first largescale evaluation of open-domain question answering (Voorhees and Tice, 2000). The task is designed in an information retrieval oriented setting. Given a factoid question along with a collection of documents, a system is required to return the exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain question answering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. A short sentence or paragraph to justify t</context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>E. Voorhees and D. Tice. 2000. Building a question answering test collection. In Proceedings of SIGIR, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>C Manning</author>
</authors>
<title>Probabilistic treeedit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="7355" citStr="Wang and Manning (2010)" startWordPosition="1159" endWordPosition="1163"> answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has been used in some of these approaches, the research has main</context>
<context position="25476" citStr="Wang and Manning (2010)" startWordPosition="4154" endWordPosition="4157">ly. The task is treated as a sentence ranking problem for each question and thus evaluated in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), using the official TREC evaluation program. Following (Wang et al., 2007), candidate sentences with more than 40 words are removed from evaluation, as well as questions with only positive or negative candidate sentences. 6.2 Baseline Methods Several systems have been proposed and tested using this dataset. Wang et al. (2007) presented a generative probabilistic model based on a Quasi-synchronous Grammar formulation and was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, Heilman and 6Each word in the question needs to be linked to a word in the sentence. Each word in the sentence can be linked to zero or multiple words in the question. 1749 System MAP MRR Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Wang and Manning (2010) 0.5951 0.6951 Table 1: Test set results of existing methods, taken from Table 3 of (Wang and Manning, 2010). Dev Test Baseline MAP MRR MAP MRR Random 0.5243 0.5816 0.4708 0.5286 Word Cnt 0.6516 0.7216 0.6263 0.6822 Wgt Word Cnt 0.7</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>M. Wang and C. Manning. 2010. Probabilistic treeedit models with structured latent variables for textual entailment and question answering. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasi-synchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="3065" citStr="Wang et al., 2007" startWordPosition="474" endWordPosition="477">ched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2L4), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improving the shallow semantic compo1744 Proceedings of th</context>
<context position="6867" citStr="Wang et al. (2007)" startWordPosition="1079" endWordPosition="1082">wering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. A short sentence or paragraph to justify the answer is not required in either TREC-QA or Jeopardy! As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching metho</context>
<context position="24242" citStr="Wang et al. (2007)" startWordPosition="3957" endWordPosition="3960">oth and Yih, 2007). In this work, we considered several sets of constraints for the alignment task, including a two-layer phrase/word 6 Experiments We present our experimental results in this section by first introducing the data and evaluation metrics, followed by the results of existing systems and some baseline methods. We then show the positive impact of adding information of word relations from various lexical semantics models, with some discussion on the limitation of the word-matching approach. 6.1 Data &amp; Evaluation Metrics The answer selection dataset we used was originally created by Wang et al. (2007) based on the QA track of past Text REtrieval Conferences (TREC-QA). Questions in this dataset are short factoid questions, such as “What is Crips’ gang color?” In average, each question is associated with approximately 33 answer candidate sentences. A pair of question and sentence is judged positive if the sentence contains the exact answer key and can provide sufficient context as supporting evidence. The training set of the data contains manually labeled 5,919 question/sentence pairs from TREC 8-12. The development and testing sets are both from TREC 13, which contain 1,374 and 1,866 pairs,</context>
<context position="25767" citStr="Wang et al. (2007)" startWordPosition="4209" endWordPosition="4212">ation, as well as questions with only positive or negative candidate sentences. 6.2 Baseline Methods Several systems have been proposed and tested using this dataset. Wang et al. (2007) presented a generative probabilistic model based on a Quasi-synchronous Grammar formulation and was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, Heilman and 6Each word in the question needs to be linked to a word in the sentence. Each word in the sentence can be linked to zero or multiple words in the question. 1749 System MAP MRR Wang et al. (2007) 0.6029 0.6852 Heilman and Smith (2010) 0.6091 0.6917 Wang and Manning (2010) 0.5951 0.6951 Table 1: Test set results of existing methods, taken from Table 3 of (Wang and Manning, 2010). Dev Test Baseline MAP MRR MAP MRR Random 0.5243 0.5816 0.4708 0.5286 Word Cnt 0.6516 0.7216 0.6263 0.6822 Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071 Table 2: Results of three baseline methods. Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features e</context>
<context position="27737" citStr="Wang et al., 2007" startWordPosition="4532" endWordPosition="4535">tson et al., 1995). The results of these three methods are shown in Table 1. Somewhat surprisingly, we find that word count is fairly strong and performs comparably to previous systems.8 In addition, weighting the question words with their IDF values further improves the results. 6.3 Incorporating Rich Lexical Semantics We test the effectiveness of adding rich lexical semantics information by creating examples of different feature sets. As described in Sec. 5, 7We used a list of 101 stopwords, including articles, pronouns and punctuation. 8The finding has been confirmed by the lead author of (Wang et al., 2007). all the features are based on the properties of the pair of a word from the question and a word from the candidate sentence. Stopwords are first removed from both questions and sentences and all words are lower-cased. Features used in the experiments can be categorized into six types: identical word matching (I), lemma matching (L), WordNet (WN), enhanced Lexical Semantics (LS), Named Entity matching (NE) and Answer type checking (Ans). Inspired by the weighted word count baseline, all features except (Ans) are weighted by the IDF value of the question word. In other words, the IDF values he</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. Smith, and T. Mitamura. 2007. What is the Jeopardy model? A quasi-synchronous grammar for QA. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Winograd</author>
</authors>
<title>Five lectures on artificial intelligence.</title>
<date>1977</date>
<booktitle>Linguistic Structures Processing,</booktitle>
<pages>399--520</pages>
<editor>In A. Zampolli, editor,</editor>
<publisher>North Holland.</publisher>
<contexts>
<context position="5644" citStr="Winograd, 1977" startWordPosition="881" endWordPosition="883"> this work. The rest of the paper is structured as follows. We first survey the related work in Sec. 2. Sec. 3 defines the problem of answer sentence selection, along with the high-level description of our solution. The enhanced lexical semantic models and the learning frameworks we explore are presented in Sec. 4 and Sec. 5, respectively. Our experimental results on a benchmark QA dataset is shown in Sec. 6. Finally, Sec. 7 concludes the paper. 2 Related Work While the task of question answering has a long history dated back to the dawn of artificial intelligence, early systems like STUDENT (Winograd, 1977) and LUNAR (Woods, 1973) are typically designed to demonstrate natural language understanding for a small and specific domain. The Text REtrieval Conference (TREC) Question Answering Track was arguably the first largescale evaluation of open-domain question answering (Voorhees and Tice, 2000). The task is designed in an information retrieval oriented setting. Given a factoid question along with a collection of documents, a system is required to return the exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain questio</context>
</contexts>
<marker>Winograd, 1977</marker>
<rawString>T. Winograd. 1977. Five lectures on artificial intelligence. In A. Zampolli, editor, Linguistic Structures Processing, pages 399–520. North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Progress in natural language understanding: An application to lunar geology.</title>
<date>1973</date>
<booktitle>In Proceedings of the National Computer Conference and Exposition (AFIPS),</booktitle>
<pages>441--450</pages>
<contexts>
<context position="5668" citStr="Woods, 1973" startWordPosition="886" endWordPosition="887"> paper is structured as follows. We first survey the related work in Sec. 2. Sec. 3 defines the problem of answer sentence selection, along with the high-level description of our solution. The enhanced lexical semantic models and the learning frameworks we explore are presented in Sec. 4 and Sec. 5, respectively. Our experimental results on a benchmark QA dataset is shown in Sec. 6. Finally, Sec. 7 concludes the paper. 2 Related Work While the task of question answering has a long history dated back to the dawn of artificial intelligence, early systems like STUDENT (Winograd, 1977) and LUNAR (Woods, 1973) are typically designed to demonstrate natural language understanding for a small and specific domain. The Text REtrieval Conference (TREC) Question Answering Track was arguably the first largescale evaluation of open-domain question answering (Voorhees and Tice, 2000). The task is designed in an information retrieval oriented setting. Given a factoid question along with a collection of documents, a system is required to return the exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain question answering setting, in </context>
</contexts>
<marker>Woods, 1973</marker>
<rawString>W. Woods. 1973. Progress in natural language understanding: An application to lunar geology. In Proceedings of the National Computer Conference and Exposition (AFIPS), pages 441–450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wu</author>
<author>H Li</author>
<author>H Wang</author>
<author>K Zhu</author>
</authors>
<title>Probase: a probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>In ACM Conference on Management of Data (SIGMOD),</booktitle>
<pages>481--492</pages>
<contexts>
<context position="15101" citStr="Wu et al., 2012" startWordPosition="2441" endWordPosition="2444">” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of rel</context>
</contexts>
<marker>Wu, Li, Wang, Zhu, 2012</marker>
<rawString>W. Wu, H. Li, H. Wang, and K. Zhu. 2012. Probase: a probabilistic taxonomy for text understanding. In ACM Conference on Management of Data (SIGMOD), pages 481–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>V Qazvinian</author>
</authors>
<title>Measuring word relatedness using heterogeneous vector space models.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT 2012,</booktitle>
<pages>616--620</pages>
<contexts>
<context position="16989" citStr="Yih and Qazvinian, 2012" startWordPosition="2742" endWordPosition="2745"> hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of the corresponding word vectors in these VSMs. Contextual term-vectors created</context>
</contexts>
<marker>Yih, Qazvinian, 2012</marker>
<rawString>W. Yih and V. Qazvinian. 2012. Measuring word relatedness using heterogeneous vector space models. In Proceedings of NAACL-HLT 2012, pages 616–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>K Toutanova</author>
<author>J Platt</author>
<author>C Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In ACL Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>247--256</pages>
<contexts>
<context position="19049" citStr="Yih et al., 2011" startWordPosition="3073" endWordPosition="3076">ke a 640-dimensional version of RNNLM vectors, which is trained using the Broadcast News corpus of 320M words.5 The final word relatedness model is a projection model learned from the click-through data of a commercial search engine (Gao et al., 2011). Unlike the previous two models, which are created or trained using a text corpus, the input for this model is pairs of aggregated queries and titles of pages users click. This parallel data is used to train a projection matrix for creating the mapping between words in queries and documents based on user feedback, using a Siamese neural network (Yih et al., 2011). Each row vector of this matrix is the dense vector representation of the corresponding word in the vocabulary. Perhaps due to its unique information source, we found this particular word embedding seems to complement the other two VSMs and tends to improve the word similarity measure in general. 5 Learning QA Matching Models In this section, we investigate the effectiveness of various learning models for matching questions and sentences, including the bag-of-words setting 5http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ and the framework of learning latent structures. 5.1 Bag-of-Words Model The ba</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>W. Yih, K. Toutanova, J. Platt, and C. Meek. 2011. Learning discriminative projections for text similarity measures. In ACL Conference on Natural Language Learning (CoNLL), pages 247–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>G Zweig</author>
<author>J Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLPCoNLL,</booktitle>
<pages>1212--1222</pages>
<contexts>
<context position="12012" citStr="Yih et al., 2012" startWordPosition="1917" endWordPosition="1920">nd needs to be handled reliably. Although sets of synonyms can be easily found in thesauri or WordNet synsets, such resources typically cover only strict synonyms. When comparing two words, it is more useful to estimate the degree of synonymy as well. For instance, ship and boat are not strict synonyms because a ship is usually viewed as a large boat. Knowing that two words are somewhat synonymous could be valuable in determining whether they should be mapped. In order to estimate the degree of synonymy, we leverage a recently proposed polarity-inducing latent semantic analysis (PILSA) model (Yih et al., 2012). Given a thesaurus, the model first constructs a signed d-by-n co-occurrence matrix W, where d is the number of word groups and n is the size of the vocabulary. Each row consists of a 2Proposed by an anonymous reviewer, one justification of this word-alignment approach, where syntactic analysis plays a less important role, is that there are often few sensible combinations of words. For instance, knowing only the set of words {”car”, ”fastest”, ”world”}, one may still guess correctly the question “What is the fastest car in the world?” 1746 group of synonyms and antonyms of a particular sense </context>
<context position="13280" citStr="Yih et al., 2012" startWordPosition="2136" endWordPosition="2139"> the elements in each row vector are the TFIDF values of the corresponding words in this group. The notion of polarity is then induced by making the values of words in the antonym groups negative, and the matrix is generalized by a lowrank approximation derived by singular-value decomposition (SVD) in the end. This design has an intriguing property – if the cosine score of two column vectors are positive, then the two corresponding words tend to be synonymous; if it’s negative, then the two words are antonymous. The degree is measured by the absolute value. Following the setting described in (Yih et al., 2012), we construct a PILSA model based on the Encarta thesaurus and enhance it with a discriminative projection matrix training method. The estimated degrees of both synonymy and antonymy are used our experiments.3 4.2 Hypernymy and Hyponymy The Class-Inclusion or Is-A relation is commonly observed between words in questions and answer sentences. For example, to correctly answer the question “What color is Saturn?”, it is crucial that the selected sentence mentions a specific kind of color, as in “Saturn is a giant gas planet with brown and beige clouds.” Another example is “Who wrote Moonlight So</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>W. Yih, G. Zweig, and J. Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of EMNLPCoNLL, pages 1212–1222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zhila</author>
<author>W Yih</author>
<author>C Meek</author>
<author>G Zweig</author>
<author>T Mikolov</author>
</authors>
<title>Combining heterogeneous models for measuring relational similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of HLTNAACL.</booktitle>
<contexts>
<context position="15784" citStr="Zhila et al., 2013" startWordPosition="2549" endWordPosition="2552">en 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, p = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches p = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word sim</context>
</contexts>
<marker>Zhila, Yih, Meek, Zweig, Mikolov, 2013</marker>
<rawString>A. Zhila, W. Yih, C. Meek, G. Zweig, and T. Mikolov. 2013. Combining heterogeneous models for measuring relational similarity. In Proceedings of HLTNAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>