<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000353">
<title confidence="0.49899625">
Grammar-based Corpus Annotation
Stefanie Dipper
Institut fiir maschinelle Sprachverarbeitung
Universitat Stuttgart
</title>
<sectionHeader confidence="0.981581" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998835451612903">
There is an increasing number of linguists in-
terested in large syntactically annotated cor-
pora (treebanks).1 Such corpora can serve as
a base for statistical applications and, at the
same time, may be used in theoretical lin-
guistics as a source for investigations about
language use.
The most important treebank nowadays
is the Penn Treebank (Marcus et al., 1993;
Marcus et al., 1994). Many statistical tag-
gers and parsers have been trained on this
treebank, e.g. (Ramshaw and Marcus, 1995;
Srinivas, 1997; Alshawi and Carter, 1994).
Furthermore, context-free and unification-
based grammars have been derived from the
Penn Treebank (Charniak, 1996; van Gen-
abith et al., 1999a; van Genabith et al.,
1999c; van Genabith et al., 1999b). These
parsers, trained or created by means of the
treebank, very successfully parse unseen text
with respect to correct POS tagging and
chunking, and hence can be applied for en-
larging the treebank.
However, the situation is different for lan-
guages other than English. Ongoing projects
are still in the process of building treebanks,
e.g. for German (NEGRA corpus (Skut et
al., 1997), now continued in the TIGER
project; the German treebank in Verbmo-
bil (Stegmann et al., 1998)), for Czech
(The Prague Dependency Treebank (Hajie,
</bodyText>
<footnote confidence="0.646687">
1I would like to thank an anonymous referee for
helpful comments on an earlier version of this pa-
per.
</footnote>
<bodyText confidence="0.98362028">
The work reported here has been partially funded
by the Deutsche Forschungsgemeinschaft, project
TIGER.
1998)); for French (Abellle et al., 2000). In
consequence, the base that parsers could be
trained on is still more or less missing. Hence
alternative ways of corpus annotation that
are not based on statistical parsers may be
investigated.
The NEGRA/TIGER corpus consists of
German newspaper texts. Currently about
30.000 sentences are annotated with depen-
dency structures. Large parts of the anno-
tation are performed by human annotators
supported by the tool annotate that inte-
grates a partial parser and a part-of-speech
tagger (Brants, 2000b).
As one part of the TIGER project, it is in-
vestigated to what extent a symbolic gram-
mar can be applied in annotation. In this ap-
proach an existing symbolic LFG grammar is
used to parse the corpus. After parsing, dis-
ambiguation has to be supported manually.
First results of this approach are the topic of
this paper.
</bodyText>
<sectionHeader confidence="0.844576" genericHeader="categories and subject descriptors">
2 Annotation by Grammar
</sectionHeader>
<subsectionHeader confidence="0.638045">
2.1 Scenario
</subsectionHeader>
<bodyText confidence="0.99994">
In the approach presented in this paper,
a broad coverage symbolic LFG grammar
(Lexical Functional Grammar, (Bresnan,
1982)) is used to parse the corpus. Usually,
the grammar output is ambiguous. Disam-
biguation is done partly manually, partly by
a grammar internal ranking mechanism. Fi-
nally, the correct reading is saved in PRO-
LOG format.
In our application, a transfer component
will convert the PROLOG output into the
NEGRA export format (Brants, 1997; Kuhn
et al., 2000), or into other representation for-
</bodyText>
<page confidence="0.994158">
56
</page>
<bodyText confidence="0.999214375">
mats such as an XML-based encoding format
(Mengel and Lezius, 2000).
In the following sections, LFG parsing
and disambiguation is presented, followed by
some remarks on grammar coverage and ro-
bustness, and annotation accuracy. To illus-
trate these remarks, parsing results are pre-
sented in the final section.
</bodyText>
<subsectionHeader confidence="0.996684">
2.2 Representations in LFG
</subsectionHeader>
<bodyText confidence="0.982520131578948">
The LFG grammar applied in parsing has
been developed using the Xerox Linguis-
tic Environment (XLE). The output of an
LFG grammar basically consists of two rep-
resentations, the constituent structure (c-
structure) of the sentence being parsed,
and its functional structure (f-structure),
containing information about predicate-
argument-structure, about attachment sites
of adjuncts, and about tense, mood etc. In
figure 1, c- and f-structure for Maria sieht
Hans (`Maria sees Hans&apos;) are displayed.
In case of an ambiguous sentence, XLE al-
lows for &amp;quot;packing&amp;quot; the different readings into
one complex f-structure representation. All
features are represented only once; feature
constraints that only hold in one of the read-
ings are marked by variables. The result is
an f-structure that is annotated with vari-
ables to show where alternatives are possi-
ble.
In figure 2, the alternative c-structures for
Maria sieht Hans mit dem Fernglas (`Maria
sees Hans with the telescope&apos;) are displayed.
The readings differ with respect to the at-
tachment site of the PP mit dem Fern glas,
either dominated by VP or by NP.
Figure 3 shows the corresponding f-
structures, combined in a single f-structure.
The PP&apos;s f-structure, embedded under the
feature ADJUNCT, is displayed only once.
In the example, variables a:1 and a:2 indi-
cate the alternative attachments.
The correct reading is selected by a human
annotator after parsing. Selection is done
either by picking the correct c-structure tree
or by clicking on the respective variables in
the f-structure.2
</bodyText>
<footnote confidence="0.865794">
2XLE provides various browsing tools applying
</footnote>
<subsectionHeader confidence="0.956795">
2.3 Semi-automatic Disambiguation
</subsectionHeader>
<bodyText confidence="0.999953375">
In the scenario sketched above, disambigua-
tion is exclusively done by a human annota-
tor. In fact, however, XLE provides a (non-
statistical) mechanism for suppressing cer-
tain ambiguities automatically. The mecha-
nism consists of a constraint ranking scheme
inspired by Optimality Theory (OT) (Frank
et al., 1998). Each rule and each lexicon en-
try can be marked by so-called OT marks.
When a sentence is parsed, each analysis is
annotated by a multi-set of OT marks. The
OT marks keep a record of all rules and lex-
icon entries being used during the parse to
arrive at the analysis in question. The gram-
mar contains a ranked list of all OT marks.
When an ambiguous sentence is parsed, the
OT mark multi-sets of all readings compete
with each other. A multi-set containing a
higher ranked OT mark than another multi-
set is filtered out.
An example is given in (1). In German,
the subject as well as the object can occupy
the first position (1a,b). If neither the sub-
ject nor the object is overtly case marked,
both readings are possible in principle (lc).
But in fact, the order subject — object is
far more frequent. Hence the second read-
ing can be suppressed by an OT mark. Note
that this does not generally exclude objects
in first position — as soon as objects are case-
marked in an unambiguous way, they are not
suppressed any more.
</bodyText>
<listItem confidence="0.8469653">
(1) a. der Hans sieht Maria.
the(nom.) H. sees M.
&apos;Hans sees Mary.&apos;
b. den Hans sieht Maria.
the(acc.) H. sees M.
&apos;It is Hans that Mary sees.&apos;
c. Hans sieht Maria.
H. sees M.
&apos;Hans sees Mary.&apos; (preferred)
&apos;It is Hans that Mary sees.&apos;
</listItem>
<footnote confidence="0.571169666666667">
to c-structure as well as to f-structure which can be
used for manual disambiguation (cf. (King et al.,
2000) where these tools are described extensively).
This is similar to the syntactic and semantic sen-
tence properties that are displayed by the disam-
biguation tool &amp;quot;TreeBanker&amp;quot; (Carter, 1997).
</footnote>
<page confidence="0.989742">
57
</page>
<figure confidence="0.999243846153846">
CP[std,−dep]
PERIOD
Vmorph[v,fin]
NP[std]
NPap
NAMES
V[v,fin]
VP[v,−h,inf]
CS 1: ROOT
Cat[name]
H[name]
NAMEbase
Maria
Cbar[fin]
.
sieht
NP[std]
NPap
NAMES
Cat[name]
H[name]
NAMEbase
Hans
NTYPE
NAME−TYPE first
PRED
’sehen&lt;[1:Maria], [101:Hans]&gt;’
&amp;quot;Maria sieht Hans.&amp;quot;
STMT−TYPE decl
TNS−ASP MOOD indicative, TENSE pres
’Maria’
PRED
1PERS 3, GEND fem, CASE nom, NUM sg
’Hans’
PRED
NAME−TYPE first
101 PERS 3, CASE acc, GEND masc, NUM sg
NTYPE
27
SUBJ
OBJ
sieht
NPap
PP[std]
mit
DETP[std]
D[std]
dem
V[v,fin]
VP[v,−h,inf]
Vmorph[v,fin]
NP[std]
NP[std]
NPap
NAMES
Cat[name]
H[name]
NAMEbase
Maria
Cat[name]
H[name]
NAMEbase
Hans
DETP[std]
D[std]
dem
NPap
Cat[noun]
NP[std]
NPap
NAMES
Cat[name]
H[name]
NAMEbase
Maria
Cat[name]
H[name]
NAMEbase
Hans
NPap
Cat[noun]
Fernglas
Cbar[fin]
Cbar[fin]
sieht
mit
.
.
COMPD[noun]
CS 1: ROOT CS 2: ROOT
CP[std,−dep] PERIOD CP[std,−dep] PERIOD
VP[v,−h,inf]
V[v,fin]
Vmorph[v,fin]
COMPD[noun]
Fernglas
NP[std]
NPap
NAMES
P[prae]
NP[std]
VP[v,−h,inf]
PP[std]
NAMES
P[prae]
NP[std]
&amp;quot;Maria sieht Hans mit dem Fernglas.&amp;quot;
PRED
’Hans’
103
PERS 3, CASE acc, GEND masc, NUM sg
PRED ’sehen&lt;[5:Maria], [103:Hans]&gt;’
STMT−TYPE decl
a:2
ADJUNCT
128
TNS−ASP MOOD indicative, TENSE pres
PTYPE adj−sem
PRED ’mit&lt;[151:Glas]&gt;’
PRED ’Glas’
OBJ
PRED ’Fern’
151 PERS 3, CASE dat, GEND neut, NUM sg
MOD 201
PRED ’Maria’
NAME−TYPE first
5 PERS 3, GEND fem, CASE nom, NUM sg
NTYPE
NTYPE NAME−TYPE first
ADJUNCT
a:1 [128:mit]
OBJ
SUBJ
29
&amp;quot;Maria sieht Hans mit dem Fernglas.&amp;quot;
a:3−4
ADJUNCT
128
PRED
’Hans’
PRED ’sehen&lt;[29−SUBJ:Hans], [29−OBJ:Hans]&gt;’
STMT−TYPE decl
&lt;{a:3|a:1} [5:Maria]&gt;
&lt;{a:4|a:2} [103:Hans]&gt;
&lt;{a:4|a:2} [5:Maria]&gt;
&lt;{a:3|a:1} [103:Hans]&gt;
TNS−ASP MOOD indicative, TENSE pres
SUBJ
OBJ
PTYPE adj−sem
PRED ’mit&lt;[151:Glas]&gt;’
PRED ’Glas’
OBJ
PRED ’Fern’
151 PERS 3, CASE dat, GEND neut, NUM sg
MOD 201
29
PRED ’Maria’
&lt;{a:4|a:2} acc&gt;
&lt;{a:3|a:1} nom&gt;
CASE
CASE
&lt;{a:3|a:1} acc&gt;
&lt;{a:4|a:2} nom&gt;
NTYPE NAME−TYPE first
5 PERS 3, GEND fem, NUM sg
NTYPE NAME−TYPE first
ADJUNCT
a:1−2 [128:mit]
</figure>
<bodyText confidence="0.999593166666667">
ing as an accusative object; etc.
Second, the grammar certainly is not
error-free and grammar internal errors may
carry over to the analyses but these errors
are systematic. If, for example, a proper
noun like Kohl is not listed as a name in the
grammar&apos;s lexicon, all analyses of sentences
about the person Helmut Kohl falsely con-
tain the reading of Kohl as a common noun
(`cabbage&apos;). But once the error is detected in
one analysis or in the grammar itself, it is of-
ten possible to automatically track down all
other instances of the same error occurring
previously in the annotation. Note, however,
that such errors may be difficult to detect.
Third, manual disambiguation of LFG
analyses usually does not impair accuracy of
the annotated corpus, since in many cases,
disambiguation is guided by prominent prop-
erties. When picking the correct read-
ing, the human annotator can make use of
clear, prominent properties of the analyses,
namely constituent structure and predicate-
argument-structure.
</bodyText>
<subsectionHeader confidence="0.998282">
2.6 Some Performance Data
</subsectionHeader>
<bodyText confidence="0.999608666666667">
To illustrate the findings of the preceding
sections, we present some figures indicating
the grammar&apos;s performance. Note, however,
that the grammar has not been tuned or
trained with respect to the corpus.
In a first experiment, 2000 sentences
from the TIGER corpus (German newspa-
per texts) were parsed. In a first pass, the
text was parsed without any preprocessing
(except for splitting the text into sentences).
In a second pass, header markers were added
and quotes were removed (since the gram-
mar currently does not accept quoted text;
the quotes can be easily recovered after pars-
ing).6 These text modifications were done
automatically. The grammar performance
improved considerably, cf. rows 1 and 2 in
figure 5.
</bodyText>
<footnote confidence="0.644634666666667">
6Quotes are problematic for several reasons:
They are ambiguous and either mark direct speech
or quote material in the running text. Quotes do not
always correspond to constituents boundaries and
matching pairs of quotes may be distributed over
distinct sentences.
</footnote>
<bodyText confidence="0.999964205128205">
Then the grammar was partly rewritten
with two major modifications: first, the
grammar was tuned for efficiency (without
affecting coverage); second, PP and adverb
attachment were allowed in a more general
way than in the previous grammar version.
This increased coverage as well as ambiguity,
as can be seen in the third row, reporting
about 6000 sentences (preprocessed in the
same way as in the second pass).
The first column shows the number of sen-
tences in the test corpus, the second column
shows the number of sentences that got a
parse (without checking for correctness). As
can be seen, in the first pass only 28% of
the sentences were parsed as opposed to 40%
after some text preprocessing. After some
general grammar modifications, 47% were
parsed.7
The third column contains the number
of analyses or readings per parsed sentence.
Only readings that were not filtered out by
the XLE internal disambiguation mechanism
are taken into account (hence &amp;quot;optimals&amp;quot;).
Both average as well as median are given. As
can be seen from figure 5, in the third pass
the average number of readings increased
massively. But nevertheless the median is
2, so most of the sentences are still easy to
disambiguate manually. Note that in this
experiment, it was not checked whether the
correct reading was among the analyses.
The forth column reports about the num-
ber of analyses that were suppressed by XLE
disambiguation (hence &amp;quot;suboptimals&amp;quot;).
Finally, average parsing time and number
of tokens per sentence are given.
In a second experiment, 300 sentences
were parsed and the analyses were evaluated.
</bodyText>
<footnote confidence="0.999319916666667">
7We are only aware of one sentence-based evalu-
ation involving a grammar with comparably deep
analyses: without tuning, the XTAG grammar
parsed 39.09% of 6364 sentences (&lt; 15 words long)
from the Wall Street Journal with an average of
7.53 analyses per sentence (Doran et al., 1994).
Other evaluations usually measure performance be-
low sentence level, such as chunking or (super)-
tagging (Srinivas, 1997; Ramshaw and Marcus, 1995;
Brants, 1999), and hence are not comparable with
our grammar that does not yield partial analyses
(yet).
</footnote>
<page confidence="0.996739">
61
</page>
<table confidence="0.7885648">
#sentences parsed optimals suboptimals time(sec) #tokens
0 Med 0 Med 0 Med 0
2000 553 (= 28%) 7 2 1689 7 17 1.8 15.5
2000 809 (= 40%) 6 2 3480 10 17 1.8 15.3
6000 2833 (= 47%) 28 2 34331 18 14 1.9 16.0
</table>
<figureCaption confidence="0.997733">
Figure 5: LFG parsing results for German newspaper sentences
</figureCaption>
<bodyText confidence="0.968737666666667">
160 sentences were parsed by the grammar;
among these, 120 parses contained the cor-
rect reading (the correct reading had to be
part of the &amp;quot;optimal&amp;quot; analyses), cf. figure 6.
We also did some preliminary evaluation
of the errors.
</bodyText>
<listItem confidence="0.963952545454546">
• 10% of the sentences were not parsed
because of gaps in the morphological an-
alyzer.&apos;
• 4% of the sentences failed because of
storage overflow or timeouts (with lim-
its set to 100 MB storage and 100 sec-
onds parsing time).
• More than 30% of the sentences failed
because gaps in the lexicon, which are
mostly due to missing subcategorization
frames.9
</listItem>
<bodyText confidence="0.977466303030303">
We decided not to manually disambiguate
sentences that get more than 20 analyses.
This is the case for 5.8% of the sentences.
&apos;We use a guesser mechanism for capitalized
words that also handles genitive and plural inflec-
tion. All morphological failures are due to non-
capitalized unknown words or else capitalized words
containing strings other than characters or numbers.
9The base lexicon is mainly extracted automati-
cally from corpora (Eckle-Kohler, 1999) and mostly
consists of subcategorization frames (in the TSNLP
format). There are 14.000 verb lemmata with 28.500
frames (115 different frames); 1100 adjective lem-
mata with 1650 frames (17 different); 780 noun lem-
mata with 970 frames (3 different). The TSNLP
frames are converted automatically into an LFG for-
mat (Broker and Dipper, 1999).
With this restriction, a trained human anno-
tator disambiguates about one sentence per
minute on average.&apos;
To sum up the findings of this section: in
the short-term, these data suggest the ne-
cessity of the following: further text prepro-
cessing such as correction of typing errors;
completion of the grammar&apos;s lexicon by ex-
tracting unknown words from the corpus.
However, in the long-term, we will have to
apply statistical disambiguation. This will
allow us to include robustness mechanisms.
In the meantime, the remainder of the sen-
tences that have not been correctly parsed by
our grammar are annotated by means of the
tool annotate.
</bodyText>
<sectionHeader confidence="0.925662" genericHeader="conclusions">
3 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.893753235294118">
We have presented first results in syntac-
tic annotation of a large German corpus
by a symbolic LFG grammar. On average,
the grammar parses 47% of the sentences.
Among these, 75% contain the correct read-
ing. Disambiguation is done partly by the
XLE internal ranking mechanism. Remain-
ing ambiguities (median: 2) are solved by
a human annotator. This takes about one
minute per sentence with an average length
of 16.0 tokens.
By means of a transfer component, LFG
representations can be converted into canon-
thThis result is very similar to that reported in
(Brants, 2000a), where a trained annotator needs
on average 50 seconds to annotate a sentence with
an average length of 17.5 tokens.
</bodyText>
<page confidence="0.993451">
62
</page>
<figure confidence="0.703274">
#sentences parsed correct reading
among optimals
300 160 (= 53%) 120 (= 40%)
</figure>
<figureCaption confidence="0.99984">
Figure 6: Evaluation of 300 sentences
</figureCaption>
<bodyText confidence="0.9960039">
ical treebank formats.
Coverage and robustness are weak points
in grammar-based annotation. The perfor-
mance data presented in 2.6 point to a need
to further exploit text preprocessing and to
complete the grammar&apos;s lexicon. In the
longer term, however, statistical disambigua-
tion and robustness mechanisms such as re-
laxation of certain restrictions have to be in-
vestigated.
</bodyText>
<sectionHeader confidence="0.986547" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.793419764705882">
Anne Abeille, Lionel Clement, and Alexan-
dra Kinyon. 2000. Building a treebank for
French. In Proceedings of the LREC 2000,
Athens, Greece.
Hiyan Alshawi and David Carter. 1994.
Training and scaling preference functions
for disambiguation. Computational Lin-
guistics, 20(4).
Thorsten Brants. 1997. The NeGra export
format for annotated corpora (version 3).
Technical report, NEGRA Project, Uni-
versitat des Saarlandes.
Thorsten Brants. 1999. Cascaded Markov
Models. In Proceedings of 9th Conference
of the EACL 1999, Bergen, Norway.
Thorsten Brants. 2000a. Inter-annotator
agreement for a German newspaper cor-
</bodyText>
<reference confidence="0.992415588235294">
pus. In Proceedings of the LREC 2000,
Athens, Greece.
Thorsten Brants. 2000b. Interactive corpus
annotation. In Proceedings of the LREC
2000, Athens, Greece.
Joan Bresnan, editor. 1982. The Mental
Representation of Grammatical Relations.
MIT Press.
Norbert Broker and Stefanie Dipper. 1999.
Zur Konstruktion von Lexika fiir die
maschinelle syntaktische Analyse. In
J. Gippert and P. Olivier, editors, Mul-
tilinguale Corpora - Codierung, Struk-
turierung, Analyse. 11. Jahrestagung der
Gesellschaft fuer Linguistische DatenVer-
arbeitung. Enigma corporation, Prag.
David Carter. 1997. The TreeBanker: a tool
for supervised training of parsed corpora.
In Proceedings of the ACL Workshop on
Computational Environments for Gram-
mar Development and Linguistic Engi-
neering, Madrid, Spain.
Eugene Charniak. 1996. Tree-bank gram-
mars. In AAAI-96. Proceedings of the
Thirteenth National Conference on Arti-
ficial Intelligence. MIT Press.
Christine Doran, Dania Egedi, Beth Ann
Hockey, B. Srinivas, and Martin Zaidel.
1994. XTAG system — a wide cover-
age grammar for English. In Proceedings
of International Conference on Computa-
tional Linguistics (COLING 94), Kyoto,
Japan.
Judith Eckle-Kohler. 1999. Linguisti-
sches Wissen zur automatischen Lexikon-
Akquisition aus deutschen Textkorpora.
Logos, Berlin.
Anette Frank, Tracy Holloway King,
Jonas Kuhn, and John Maxwell. 1998.
Optimality theory style constraint
ranking in large-scale LFG grammars.
In Proceedings of the LFG98 Con-
ference, Brisbane, Australia. CSLI
Online Publications, http://www-
csli.stanford.edu/publications.
Jan Hajie&apos;. 1998. Building a syntactically
annotated corpus: The Prague Depen-
dency Treebank. In Eva Hajie&apos;ova, editor,
Issues of Valency and Meaning. Studies
in Honour of Jarmila Pan evovd. Charles
University Press, Prag.
</reference>
<page confidence="0.994828">
63
</page>
<reference confidence="0.999683465116279">
Tracy Holloway King, Stefanie Dipper,
Anette Frank, Jonas Kuhn, and John
Maxwell. 2000. Ambiguity management
in grammar writing. In Proceedings of
the ESSLLI 2000 Workshop on Linguis-
tic Theory and Grammar Implementation,
Birmingham, Great Britain.
Jonas Kuhn, Heike Zinsmeister, and Mar-
tin Emele. 2000. From LFG structures to
TIGER treebank annotations. Presented
at the Workshop on Syntactic Annota-
tion of Electronic Corpora, University of
Tubingen.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn Treebank. Computational Linguis-
tics, 19(2).
Mitchell P. Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Mark
Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: An-
notating predicate argument structure. In
Proceedings of the Human Language Tech-
nology Workshop. Morgan Kaufmann.
John T. Maxwell and Christopher D. Man-
ning. 1996. A theory of non-constituent
coordination based on finite-state rules.
In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG96
Conference, Grenoble, France. CSLI
Online Publications, http://www-
csli.stanford.edu/publications.
Andreas Mengel and Wolfgang Lezius. 2000.
An XML-based encoding format for syn-
tactically annotated corpora. In Proceed-
ings of the LREC 2000, Athens, Greece.
Lance A. Ramshaw and Mitchell P.
Marcus. 1995. Text chunking using
transformation-based learning. In Pro-
ceedings of the Third ACL Workshop on
Very Large Corpora, Dublin, Ireland.
Stefan Riezler, Detlef Prescher, Jonas Kuhn,
and Mark Johnson. 2000. Lexicalized
stochastic modeling of constraint-based
grammars using log-linear measures and
EM training. In Proceedings of the ACL
2000, Hong Kong, China.
Carolyn Penstein Rosé and Alon Lavie. To
appear. Balancing robustness and effi-
ciency in unification-augmented context-
free parsers for large practical applica-
tions. In van Noord and Junqua, editors,
Robustness in Language and Speech Tech-
nology. Kluwer Academic Press.
Wojciech Skut, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997. An
annotation scheme for free word order
languages. In Proceedings of ANLP-97,
Washington.
B. Srinivas. 1997. Performance evaluation of
supertagging for partial parsing. In Pro-
ceedings of Fifth International Workshop
on Parsing Technology, Boston, USA.
Rosmary Stegmann, Heike Schulz, and Er-
hard Hinrichs. 1998. Stylebook for the
German treebank in Verbmobil. Verbmo-
bil, Universitat Tubingen.
Joseph van Genabith, Louisa Sadler, and
Andy Way. 1999a. Data-driven compi-
lation of LFG semantic forms. In Pro-
ceedings of the EACL 1999 Workshop on
Linguistically Interpreted Corpora (LINC-
99), Bergen, Norway.
Joseph van Genabith, Louisa Sadler, and
Andy Way. 1999b. Semi-automatic
generation of f-structures from tree-
banks. In Proceedings of the LFG99
Conference, Manchester, Great Britain.
CSLI Online Publications, http://www-
csli.stanford.edu/publications.
Joseph van Genabith, Louisa Sadler, and
Andy Way. 1999c. Structure preserving
CF-PSG compaction, LFG and treebanks.
In Proceedings of the ATALA Treebank
Workshop, Paris, France.
</reference>
<page confidence="0.999418">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938416">
<title confidence="0.997123">Grammar-based Corpus Annotation</title>
<author confidence="0.954124">Stefanie Dipper</author>
<affiliation confidence="0.9884835">Institut fiir maschinelle Universitat Stuttgart</affiliation>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>pus</author>
</authors>
<date>2000</date>
<booktitle>In Proceedings of the LREC</booktitle>
<location>Athens, Greece.</location>
<marker>pus, 2000</marker>
<rawString>pus. In Proceedings of the LREC 2000, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Interactive corpus annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of the LREC 2000,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2141" citStr="Brants, 2000" startWordPosition="332" endWordPosition="333">rtially funded by the Deutsche Forschungsgemeinschaft, project TIGER. 1998)); for French (Abellle et al., 2000). In consequence, the base that parsers could be trained on is still more or less missing. Hence alternative ways of corpus annotation that are not based on statistical parsers may be investigated. The NEGRA/TIGER corpus consists of German newspaper texts. Currently about 30.000 sentences are annotated with dependency structures. Large parts of the annotation are performed by human annotators supported by the tool annotate that integrates a partial parser and a part-of-speech tagger (Brants, 2000b). As one part of the TIGER project, it is investigated to what extent a symbolic grammar can be applied in annotation. In this approach an existing symbolic LFG grammar is used to parse the corpus. After parsing, disambiguation has to be supported manually. First results of this approach are the topic of this paper. 2 Annotation by Grammar 2.1 Scenario In the approach presented in this paper, a broad coverage symbolic LFG grammar (Lexical Functional Grammar, (Bresnan, 1982)) is used to parse the corpus. Usually, the grammar output is ambiguous. Disambiguation is done partly manually, partly </context>
<context position="15831" citStr="Brants, 2000" startWordPosition="2563" endWordPosition="2564"> the tool annotate. 3 Conclusion and Outlook We have presented first results in syntactic annotation of a large German corpus by a symbolic LFG grammar. On average, the grammar parses 47% of the sentences. Among these, 75% contain the correct reading. Disambiguation is done partly by the XLE internal ranking mechanism. Remaining ambiguities (median: 2) are solved by a human annotator. This takes about one minute per sentence with an average length of 16.0 tokens. By means of a transfer component, LFG representations can be converted into canonthThis result is very similar to that reported in (Brants, 2000a), where a trained annotator needs on average 50 seconds to annotate a sentence with an average length of 17.5 tokens. 62 #sentences parsed correct reading among optimals 300 160 (= 53%) 120 (= 40%) Figure 6: Evaluation of 300 sentences ical treebank formats. Coverage and robustness are weak points in grammar-based annotation. The performance data presented in 2.6 point to a need to further exploit text preprocessing and to complete the grammar&apos;s lexicon. In the longer term, however, statistical disambiguation and robustness mechanisms such as relaxation of certain restrictions have to be inv</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Thorsten Brants. 2000b. Interactive corpus annotation. In Proceedings of the LREC 2000, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<editor>Joan Bresnan, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1982</marker>
<rawString>Joan Bresnan, editor. 1982. The Mental Representation of Grammatical Relations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Broker</author>
<author>Stefanie Dipper</author>
</authors>
<title>Zur Konstruktion von Lexika fiir die maschinelle syntaktische Analyse.</title>
<date>1999</date>
<booktitle>Multilinguale Corpora - Codierung, Strukturierung, Analyse. 11. Jahrestagung der Gesellschaft fuer Linguistische DatenVerarbeitung. Enigma corporation, Prag.</booktitle>
<editor>In J. Gippert and P. Olivier, editors,</editor>
<contexts>
<context position="14602" citStr="Broker and Dipper, 1999" startWordPosition="2358" endWordPosition="2361">ords that also handles genitive and plural inflection. All morphological failures are due to noncapitalized unknown words or else capitalized words containing strings other than characters or numbers. 9The base lexicon is mainly extracted automatically from corpora (Eckle-Kohler, 1999) and mostly consists of subcategorization frames (in the TSNLP format). There are 14.000 verb lemmata with 28.500 frames (115 different frames); 1100 adjective lemmata with 1650 frames (17 different); 780 noun lemmata with 970 frames (3 different). The TSNLP frames are converted automatically into an LFG format (Broker and Dipper, 1999). With this restriction, a trained human annotator disambiguates about one sentence per minute on average.&apos; To sum up the findings of this section: in the short-term, these data suggest the necessity of the following: further text preprocessing such as correction of typing errors; completion of the grammar&apos;s lexicon by extracting unknown words from the corpus. However, in the long-term, we will have to apply statistical disambiguation. This will allow us to include robustness mechanisms. In the meantime, the remainder of the sentences that have not been correctly parsed by our grammar are anno</context>
</contexts>
<marker>Broker, Dipper, 1999</marker>
<rawString>Norbert Broker and Stefanie Dipper. 1999. Zur Konstruktion von Lexika fiir die maschinelle syntaktische Analyse. In J. Gippert and P. Olivier, editors, Multilinguale Corpora - Codierung, Strukturierung, Analyse. 11. Jahrestagung der Gesellschaft fuer Linguistische DatenVerarbeitung. Enigma corporation, Prag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Carter</author>
</authors>
<title>The TreeBanker: a tool for supervised training of parsed corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Environments for Grammar Development and Linguistic Engineering,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="6817" citStr="Carter, 1997" startWordPosition="1112" endWordPosition="1113"> as objects are casemarked in an unambiguous way, they are not suppressed any more. (1) a. der Hans sieht Maria. the(nom.) H. sees M. &apos;Hans sees Mary.&apos; b. den Hans sieht Maria. the(acc.) H. sees M. &apos;It is Hans that Mary sees.&apos; c. Hans sieht Maria. H. sees M. &apos;Hans sees Mary.&apos; (preferred) &apos;It is Hans that Mary sees.&apos; to c-structure as well as to f-structure which can be used for manual disambiguation (cf. (King et al., 2000) where these tools are described extensively). This is similar to the syntactic and semantic sentence properties that are displayed by the disambiguation tool &amp;quot;TreeBanker&amp;quot; (Carter, 1997). 57 CP[std,−dep] PERIOD Vmorph[v,fin] NP[std] NPap NAMES V[v,fin] VP[v,−h,inf] CS 1: ROOT Cat[name] H[name] NAMEbase Maria Cbar[fin] . sieht NP[std] NPap NAMES Cat[name] H[name] NAMEbase Hans NTYPE NAME−TYPE first PRED ’sehen&lt;[1:Maria], [101:Hans]&gt;’ &amp;quot;Maria sieht Hans.&amp;quot; STMT−TYPE decl TNS−ASP MOOD indicative, TENSE pres ’Maria’ PRED 1PERS 3, GEND fem, CASE nom, NUM sg ’Hans’ PRED NAME−TYPE first 101 PERS 3, CASE acc, GEND masc, NUM sg NTYPE 27 SUBJ OBJ sieht NPap PP[std] mit DETP[std] D[std] dem V[v,fin] VP[v,−h,inf] Vmorph[v,fin] NP[std] NP[std] NPap NAMES Cat[name] H[name] NAMEbase Maria Cat</context>
</contexts>
<marker>Carter, 1997</marker>
<rawString>David Carter. 1997. The TreeBanker: a tool for supervised training of parsed corpora. In Proceedings of the ACL Workshop on Computational Environments for Grammar Development and Linguistic Engineering, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In AAAI-96. Proceedings of the Thirteenth National Conference on Artificial Intelligence.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="776" citStr="Charniak, 1996" startWordPosition="112" endWordPosition="113"> linguists interested in large syntactically annotated corpora (treebanks).1 Such corpora can serve as a base for statistical applications and, at the same time, may be used in theoretical linguistics as a source for investigations about language use. The most important treebank nowadays is the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994). Many statistical taggers and parsers have been trained on this treebank, e.g. (Ramshaw and Marcus, 1995; Srinivas, 1997; Alshawi and Carter, 1994). Furthermore, context-free and unificationbased grammars have been derived from the Penn Treebank (Charniak, 1996; van Genabith et al., 1999a; van Genabith et al., 1999c; van Genabith et al., 1999b). These parsers, trained or created by means of the treebank, very successfully parse unseen text with respect to correct POS tagging and chunking, and hence can be applied for enlarging the treebank. However, the situation is different for languages other than English. Ongoing projects are still in the process of building treebanks, e.g. for German (NEGRA corpus (Skut et al., 1997), now continued in the TIGER project; the German treebank in Verbmobil (Stegmann et al., 1998)), for Czech (The Prague Dependency </context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In AAAI-96. Proceedings of the Thirteenth National Conference on Artificial Intelligence. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>Dania Egedi</author>
<author>Beth Ann Hockey</author>
<author>B Srinivas</author>
<author>Martin Zaidel</author>
</authors>
<title>XTAG system — a wide coverage grammar for English.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on Computational Linguistics (COLING 94),</booktitle>
<location>Kyoto, Japan.</location>
<contexts>
<context position="12707" citStr="Doran et al., 1994" startWordPosition="2037" endWordPosition="2040">checked whether the correct reading was among the analyses. The forth column reports about the number of analyses that were suppressed by XLE disambiguation (hence &amp;quot;suboptimals&amp;quot;). Finally, average parsing time and number of tokens per sentence are given. In a second experiment, 300 sentences were parsed and the analyses were evaluated. 7We are only aware of one sentence-based evaluation involving a grammar with comparably deep analyses: without tuning, the XTAG grammar parsed 39.09% of 6364 sentences (&lt; 15 words long) from the Wall Street Journal with an average of 7.53 analyses per sentence (Doran et al., 1994). Other evaluations usually measure performance below sentence level, such as chunking or (super)- tagging (Srinivas, 1997; Ramshaw and Marcus, 1995; Brants, 1999), and hence are not comparable with our grammar that does not yield partial analyses (yet). 61 #sentences parsed optimals suboptimals time(sec) #tokens 0 Med 0 Med 0 Med 0 2000 553 (= 28%) 7 2 1689 7 17 1.8 15.5 2000 809 (= 40%) 6 2 3480 10 17 1.8 15.3 6000 2833 (= 47%) 28 2 34331 18 14 1.9 16.0 Figure 5: LFG parsing results for German newspaper sentences 160 sentences were parsed by the grammar; among these, 120 parses contained the</context>
</contexts>
<marker>Doran, Egedi, Hockey, Srinivas, Zaidel, 1994</marker>
<rawString>Christine Doran, Dania Egedi, Beth Ann Hockey, B. Srinivas, and Martin Zaidel. 1994. XTAG system — a wide coverage grammar for English. In Proceedings of International Conference on Computational Linguistics (COLING 94), Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Eckle-Kohler</author>
</authors>
<title>Linguistisches Wissen zur automatischen LexikonAkquisition aus deutschen Textkorpora. Logos,</title>
<date>1999</date>
<location>Berlin.</location>
<contexts>
<context position="14264" citStr="Eckle-Kohler, 1999" startWordPosition="2306" endWordPosition="2307">ge and 100 seconds parsing time). • More than 30% of the sentences failed because gaps in the lexicon, which are mostly due to missing subcategorization frames.9 We decided not to manually disambiguate sentences that get more than 20 analyses. This is the case for 5.8% of the sentences. &apos;We use a guesser mechanism for capitalized words that also handles genitive and plural inflection. All morphological failures are due to noncapitalized unknown words or else capitalized words containing strings other than characters or numbers. 9The base lexicon is mainly extracted automatically from corpora (Eckle-Kohler, 1999) and mostly consists of subcategorization frames (in the TSNLP format). There are 14.000 verb lemmata with 28.500 frames (115 different frames); 1100 adjective lemmata with 1650 frames (17 different); 780 noun lemmata with 970 frames (3 different). The TSNLP frames are converted automatically into an LFG format (Broker and Dipper, 1999). With this restriction, a trained human annotator disambiguates about one sentence per minute on average.&apos; To sum up the findings of this section: in the short-term, these data suggest the necessity of the following: further text preprocessing such as correctio</context>
</contexts>
<marker>Eckle-Kohler, 1999</marker>
<rawString>Judith Eckle-Kohler. 1999. Linguistisches Wissen zur automatischen LexikonAkquisition aus deutschen Textkorpora. Logos, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Tracy Holloway King</author>
<author>Jonas Kuhn</author>
<author>John Maxwell</author>
</authors>
<title>Optimality theory style constraint ranking in large-scale LFG grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of the LFG98 Conference,</booktitle>
<publisher>CSLI Online Publications,</publisher>
<location>Brisbane, Australia.</location>
<contexts>
<context position="5267" citStr="Frank et al., 1998" startWordPosition="828" endWordPosition="831">ate the alternative attachments. The correct reading is selected by a human annotator after parsing. Selection is done either by picking the correct c-structure tree or by clicking on the respective variables in the f-structure.2 2XLE provides various browsing tools applying 2.3 Semi-automatic Disambiguation In the scenario sketched above, disambiguation is exclusively done by a human annotator. In fact, however, XLE provides a (nonstatistical) mechanism for suppressing certain ambiguities automatically. The mechanism consists of a constraint ranking scheme inspired by Optimality Theory (OT) (Frank et al., 1998). Each rule and each lexicon entry can be marked by so-called OT marks. When a sentence is parsed, each analysis is annotated by a multi-set of OT marks. The OT marks keep a record of all rules and lexicon entries being used during the parse to arrive at the analysis in question. The grammar contains a ranked list of all OT marks. When an ambiguous sentence is parsed, the OT mark multi-sets of all readings compete with each other. A multi-set containing a higher ranked OT mark than another multiset is filtered out. An example is given in (1). In German, the subject as well as the object can oc</context>
</contexts>
<marker>Frank, King, Kuhn, Maxwell, 1998</marker>
<rawString>Anette Frank, Tracy Holloway King, Jonas Kuhn, and John Maxwell. 1998. Optimality theory style constraint ranking in large-scale LFG grammars. In Proceedings of the LFG98 Conference, Brisbane, Australia. CSLI Online Publications, http://wwwcsli.stanford.edu/publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajie&apos;</author>
</authors>
<title>Building a syntactically annotated corpus: The Prague Dependency Treebank.</title>
<date>1998</date>
<booktitle>Issues of Valency and Meaning. Studies in Honour of Jarmila Pan evovd.</booktitle>
<editor>In Eva Hajie&apos;ova, editor,</editor>
<publisher>Charles University Press, Prag.</publisher>
<marker>Hajie&apos;, 1998</marker>
<rawString>Jan Hajie&apos;. 1998. Building a syntactically annotated corpus: The Prague Dependency Treebank. In Eva Hajie&apos;ova, editor, Issues of Valency and Meaning. Studies in Honour of Jarmila Pan evovd. Charles University Press, Prag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tracy Holloway King</author>
<author>Stefanie Dipper</author>
<author>Anette Frank</author>
<author>Jonas Kuhn</author>
<author>John Maxwell</author>
</authors>
<title>Ambiguity management in grammar writing.</title>
<date>2000</date>
<booktitle>In Proceedings of the ESSLLI 2000 Workshop on Linguistic Theory and Grammar Implementation,</booktitle>
<location>Birmingham, Great Britain.</location>
<contexts>
<context position="6631" citStr="King et al., 2000" startWordPosition="1082" endWordPosition="1085"> fact, the order subject — object is far more frequent. Hence the second reading can be suppressed by an OT mark. Note that this does not generally exclude objects in first position — as soon as objects are casemarked in an unambiguous way, they are not suppressed any more. (1) a. der Hans sieht Maria. the(nom.) H. sees M. &apos;Hans sees Mary.&apos; b. den Hans sieht Maria. the(acc.) H. sees M. &apos;It is Hans that Mary sees.&apos; c. Hans sieht Maria. H. sees M. &apos;Hans sees Mary.&apos; (preferred) &apos;It is Hans that Mary sees.&apos; to c-structure as well as to f-structure which can be used for manual disambiguation (cf. (King et al., 2000) where these tools are described extensively). This is similar to the syntactic and semantic sentence properties that are displayed by the disambiguation tool &amp;quot;TreeBanker&amp;quot; (Carter, 1997). 57 CP[std,−dep] PERIOD Vmorph[v,fin] NP[std] NPap NAMES V[v,fin] VP[v,−h,inf] CS 1: ROOT Cat[name] H[name] NAMEbase Maria Cbar[fin] . sieht NP[std] NPap NAMES Cat[name] H[name] NAMEbase Hans NTYPE NAME−TYPE first PRED ’sehen&lt;[1:Maria], [101:Hans]&gt;’ &amp;quot;Maria sieht Hans.&amp;quot; STMT−TYPE decl TNS−ASP MOOD indicative, TENSE pres ’Maria’ PRED 1PERS 3, GEND fem, CASE nom, NUM sg ’Hans’ PRED NAME−TYPE first 101 PERS 3, CAS</context>
</contexts>
<marker>King, Dipper, Frank, Kuhn, Maxwell, 2000</marker>
<rawString>Tracy Holloway King, Stefanie Dipper, Anette Frank, Jonas Kuhn, and John Maxwell. 2000. Ambiguity management in grammar writing. In Proceedings of the ESSLLI 2000 Workshop on Linguistic Theory and Grammar Implementation, Birmingham, Great Britain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Kuhn</author>
<author>Heike Zinsmeister</author>
<author>Martin Emele</author>
</authors>
<title>From LFG structures to TIGER treebank annotations. Presented at the Workshop on Syntactic Annotation of Electronic Corpora,</title>
<date>2000</date>
<institution>University of Tubingen.</institution>
<contexts>
<context position="2972" citStr="Kuhn et al., 2000" startWordPosition="470" endWordPosition="473">, disambiguation has to be supported manually. First results of this approach are the topic of this paper. 2 Annotation by Grammar 2.1 Scenario In the approach presented in this paper, a broad coverage symbolic LFG grammar (Lexical Functional Grammar, (Bresnan, 1982)) is used to parse the corpus. Usually, the grammar output is ambiguous. Disambiguation is done partly manually, partly by a grammar internal ranking mechanism. Finally, the correct reading is saved in PROLOG format. In our application, a transfer component will convert the PROLOG output into the NEGRA export format (Brants, 1997; Kuhn et al., 2000), or into other representation for56 mats such as an XML-based encoding format (Mengel and Lezius, 2000). In the following sections, LFG parsing and disambiguation is presented, followed by some remarks on grammar coverage and robustness, and annotation accuracy. To illustrate these remarks, parsing results are presented in the final section. 2.2 Representations in LFG The LFG grammar applied in parsing has been developed using the Xerox Linguistic Environment (XLE). The output of an LFG grammar basically consists of two representations, the constituent structure (cstructure) of the sentence b</context>
</contexts>
<marker>Kuhn, Zinsmeister, Emele, 2000</marker>
<rawString>Jonas Kuhn, Heike Zinsmeister, and Martin Emele. 2000. From LFG structures to TIGER treebank annotations. Presented at the Workshop on Syntactic Annotation of Electronic Corpora, University of Tubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Human Language Technology Workshop.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell P. Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the Human Language Technology Workshop. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John T Maxwell</author>
<author>Christopher D Manning</author>
</authors>
<title>A theory of non-constituent coordination based on finite-state rules.</title>
<date>1996</date>
<booktitle>In Miriam Butt and Tracy Holloway King, editors, Proceedings of the LFG96 Conference,</booktitle>
<publisher>CSLI Online Publications,</publisher>
<location>Grenoble, France.</location>
<marker>Maxwell, Manning, 1996</marker>
<rawString>John T. Maxwell and Christopher D. Manning. 1996. A theory of non-constituent coordination based on finite-state rules. In Miriam Butt and Tracy Holloway King, editors, Proceedings of the LFG96 Conference, Grenoble, France. CSLI Online Publications, http://wwwcsli.stanford.edu/publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Mengel</author>
<author>Wolfgang Lezius</author>
</authors>
<title>An XML-based encoding format for syntactically annotated corpora.</title>
<date>2000</date>
<booktitle>In Proceedings of the LREC</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="3076" citStr="Mengel and Lezius, 2000" startWordPosition="487" endWordPosition="490">s paper. 2 Annotation by Grammar 2.1 Scenario In the approach presented in this paper, a broad coverage symbolic LFG grammar (Lexical Functional Grammar, (Bresnan, 1982)) is used to parse the corpus. Usually, the grammar output is ambiguous. Disambiguation is done partly manually, partly by a grammar internal ranking mechanism. Finally, the correct reading is saved in PROLOG format. In our application, a transfer component will convert the PROLOG output into the NEGRA export format (Brants, 1997; Kuhn et al., 2000), or into other representation for56 mats such as an XML-based encoding format (Mengel and Lezius, 2000). In the following sections, LFG parsing and disambiguation is presented, followed by some remarks on grammar coverage and robustness, and annotation accuracy. To illustrate these remarks, parsing results are presented in the final section. 2.2 Representations in LFG The LFG grammar applied in parsing has been developed using the Xerox Linguistic Environment (XLE). The output of an LFG grammar basically consists of two representations, the constituent structure (cstructure) of the sentence being parsed, and its functional structure (f-structure), containing information about predicateargument-</context>
</contexts>
<marker>Mengel, Lezius, 2000</marker>
<rawString>Andreas Mengel and Wolfgang Lezius. 2000. An XML-based encoding format for syntactically annotated corpora. In Proceedings of the LREC 2000, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third ACL Workshop on Very Large Corpora,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="12855" citStr="Ramshaw and Marcus, 1995" startWordPosition="2058" endWordPosition="2061">isambiguation (hence &amp;quot;suboptimals&amp;quot;). Finally, average parsing time and number of tokens per sentence are given. In a second experiment, 300 sentences were parsed and the analyses were evaluated. 7We are only aware of one sentence-based evaluation involving a grammar with comparably deep analyses: without tuning, the XTAG grammar parsed 39.09% of 6364 sentences (&lt; 15 words long) from the Wall Street Journal with an average of 7.53 analyses per sentence (Doran et al., 1994). Other evaluations usually measure performance below sentence level, such as chunking or (super)- tagging (Srinivas, 1997; Ramshaw and Marcus, 1995; Brants, 1999), and hence are not comparable with our grammar that does not yield partial analyses (yet). 61 #sentences parsed optimals suboptimals time(sec) #tokens 0 Med 0 Med 0 Med 0 2000 553 (= 28%) 7 2 1689 7 17 1.8 15.5 2000 809 (= 40%) 6 2 3480 10 17 1.8 15.3 6000 2833 (= 47%) 28 2 34331 18 14 1.9 16.0 Figure 5: LFG parsing results for German newspaper sentences 160 sentences were parsed by the grammar; among these, 120 parses contained the correct reading (the correct reading had to be part of the &amp;quot;optimal&amp;quot; analyses), cf. figure 6. We also did some preliminary evaluation of the errors</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third ACL Workshop on Very Large Corpora, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL</booktitle>
<location>Hong Kong, China.</location>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proceedings of the ACL 2000, Hong Kong, China.</rawString>
</citation>
<citation valid="false">
<title>Carolyn Penstein Rosé and Alon Lavie. To appear. Balancing robustness and efficiency in unification-augmented contextfree parsers for large practical applications.</title>
<booktitle>In van Noord and Junqua, editors, Robustness in Language and Speech Technology.</booktitle>
<publisher>Kluwer Academic Press.</publisher>
<marker></marker>
<rawString>Carolyn Penstein Rosé and Alon Lavie. To appear. Balancing robustness and efficiency in unification-augmented contextfree parsers for large practical applications. In van Noord and Junqua, editors, Robustness in Language and Speech Technology. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Skut</author>
<author>Brigitte Krenn</author>
<author>Thorsten Brants</author>
<author>Hans Uszkoreit</author>
</authors>
<title>An annotation scheme for free word order languages.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP-97,</booktitle>
<location>Washington.</location>
<contexts>
<context position="1246" citStr="Skut et al., 1997" startWordPosition="189" endWordPosition="192">s, 1997; Alshawi and Carter, 1994). Furthermore, context-free and unificationbased grammars have been derived from the Penn Treebank (Charniak, 1996; van Genabith et al., 1999a; van Genabith et al., 1999c; van Genabith et al., 1999b). These parsers, trained or created by means of the treebank, very successfully parse unseen text with respect to correct POS tagging and chunking, and hence can be applied for enlarging the treebank. However, the situation is different for languages other than English. Ongoing projects are still in the process of building treebanks, e.g. for German (NEGRA corpus (Skut et al., 1997), now continued in the TIGER project; the German treebank in Verbmobil (Stegmann et al., 1998)), for Czech (The Prague Dependency Treebank (Hajie, 1I would like to thank an anonymous referee for helpful comments on an earlier version of this paper. The work reported here has been partially funded by the Deutsche Forschungsgemeinschaft, project TIGER. 1998)); for French (Abellle et al., 2000). In consequence, the base that parsers could be trained on is still more or less missing. Hence alternative ways of corpus annotation that are not based on statistical parsers may be investigated. The NEGR</context>
</contexts>
<marker>Skut, Krenn, Brants, Uszkoreit, 1997</marker>
<rawString>Wojciech Skut, Brigitte Krenn, Thorsten Brants, and Hans Uszkoreit. 1997. An annotation scheme for free word order languages. In Proceedings of ANLP-97, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Performance evaluation of supertagging for partial parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of Fifth International Workshop on Parsing Technology,</booktitle>
<location>Boston, USA.</location>
<contexts>
<context position="635" citStr="Srinivas, 1997" startWordPosition="93" endWordPosition="94">Annotation Stefanie Dipper Institut fiir maschinelle Sprachverarbeitung Universitat Stuttgart 1 Introduction There is an increasing number of linguists interested in large syntactically annotated corpora (treebanks).1 Such corpora can serve as a base for statistical applications and, at the same time, may be used in theoretical linguistics as a source for investigations about language use. The most important treebank nowadays is the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994). Many statistical taggers and parsers have been trained on this treebank, e.g. (Ramshaw and Marcus, 1995; Srinivas, 1997; Alshawi and Carter, 1994). Furthermore, context-free and unificationbased grammars have been derived from the Penn Treebank (Charniak, 1996; van Genabith et al., 1999a; van Genabith et al., 1999c; van Genabith et al., 1999b). These parsers, trained or created by means of the treebank, very successfully parse unseen text with respect to correct POS tagging and chunking, and hence can be applied for enlarging the treebank. However, the situation is different for languages other than English. Ongoing projects are still in the process of building treebanks, e.g. for German (NEGRA corpus (Skut et</context>
<context position="12829" citStr="Srinivas, 1997" startWordPosition="2056" endWordPosition="2057">pressed by XLE disambiguation (hence &amp;quot;suboptimals&amp;quot;). Finally, average parsing time and number of tokens per sentence are given. In a second experiment, 300 sentences were parsed and the analyses were evaluated. 7We are only aware of one sentence-based evaluation involving a grammar with comparably deep analyses: without tuning, the XTAG grammar parsed 39.09% of 6364 sentences (&lt; 15 words long) from the Wall Street Journal with an average of 7.53 analyses per sentence (Doran et al., 1994). Other evaluations usually measure performance below sentence level, such as chunking or (super)- tagging (Srinivas, 1997; Ramshaw and Marcus, 1995; Brants, 1999), and hence are not comparable with our grammar that does not yield partial analyses (yet). 61 #sentences parsed optimals suboptimals time(sec) #tokens 0 Med 0 Med 0 Med 0 2000 553 (= 28%) 7 2 1689 7 17 1.8 15.5 2000 809 (= 40%) 6 2 3480 10 17 1.8 15.3 6000 2833 (= 47%) 28 2 34331 18 14 1.9 16.0 Figure 5: LFG parsing results for German newspaper sentences 160 sentences were parsed by the grammar; among these, 120 parses contained the correct reading (the correct reading had to be part of the &amp;quot;optimal&amp;quot; analyses), cf. figure 6. We also did some preliminar</context>
</contexts>
<marker>Srinivas, 1997</marker>
<rawString>B. Srinivas. 1997. Performance evaluation of supertagging for partial parsing. In Proceedings of Fifth International Workshop on Parsing Technology, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosmary Stegmann</author>
<author>Heike Schulz</author>
<author>Erhard Hinrichs</author>
</authors>
<title>Stylebook for the German treebank in Verbmobil. Verbmobil,</title>
<date>1998</date>
<institution>Universitat Tubingen.</institution>
<contexts>
<context position="1340" citStr="Stegmann et al., 1998" startWordPosition="205" endWordPosition="208">s have been derived from the Penn Treebank (Charniak, 1996; van Genabith et al., 1999a; van Genabith et al., 1999c; van Genabith et al., 1999b). These parsers, trained or created by means of the treebank, very successfully parse unseen text with respect to correct POS tagging and chunking, and hence can be applied for enlarging the treebank. However, the situation is different for languages other than English. Ongoing projects are still in the process of building treebanks, e.g. for German (NEGRA corpus (Skut et al., 1997), now continued in the TIGER project; the German treebank in Verbmobil (Stegmann et al., 1998)), for Czech (The Prague Dependency Treebank (Hajie, 1I would like to thank an anonymous referee for helpful comments on an earlier version of this paper. The work reported here has been partially funded by the Deutsche Forschungsgemeinschaft, project TIGER. 1998)); for French (Abellle et al., 2000). In consequence, the base that parsers could be trained on is still more or less missing. Hence alternative ways of corpus annotation that are not based on statistical parsers may be investigated. The NEGRA/TIGER corpus consists of German newspaper texts. Currently about 30.000 sentences are annota</context>
</contexts>
<marker>Stegmann, Schulz, Hinrichs, 1998</marker>
<rawString>Rosmary Stegmann, Heike Schulz, and Erhard Hinrichs. 1998. Stylebook for the German treebank in Verbmobil. Verbmobil, Universitat Tubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph van Genabith</author>
<author>Louisa Sadler</author>
<author>Andy Way</author>
</authors>
<title>Data-driven compilation of LFG semantic forms.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL 1999 Workshop on Linguistically Interpreted Corpora (LINC99),</booktitle>
<location>Bergen, Norway.</location>
<marker>van Genabith, Sadler, Way, 1999</marker>
<rawString>Joseph van Genabith, Louisa Sadler, and Andy Way. 1999a. Data-driven compilation of LFG semantic forms. In Proceedings of the EACL 1999 Workshop on Linguistically Interpreted Corpora (LINC99), Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph van Genabith</author>
<author>Louisa Sadler</author>
<author>Andy Way</author>
</authors>
<title>Semi-automatic generation of f-structures from treebanks.</title>
<date>1999</date>
<booktitle>In Proceedings of the LFG99 Conference,</booktitle>
<publisher>CSLI Online Publications,</publisher>
<location>Manchester, Great Britain.</location>
<marker>van Genabith, Sadler, Way, 1999</marker>
<rawString>Joseph van Genabith, Louisa Sadler, and Andy Way. 1999b. Semi-automatic generation of f-structures from treebanks. In Proceedings of the LFG99 Conference, Manchester, Great Britain. CSLI Online Publications, http://wwwcsli.stanford.edu/publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph van Genabith</author>
<author>Louisa Sadler</author>
<author>Andy Way</author>
</authors>
<title>Structure preserving CF-PSG compaction, LFG and treebanks.</title>
<date>1999</date>
<booktitle>In Proceedings of the ATALA Treebank Workshop,</booktitle>
<location>Paris, France.</location>
<marker>van Genabith, Sadler, Way, 1999</marker>
<rawString>Joseph van Genabith, Louisa Sadler, and Andy Way. 1999c. Structure preserving CF-PSG compaction, LFG and treebanks. In Proceedings of the ATALA Treebank Workshop, Paris, France.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>