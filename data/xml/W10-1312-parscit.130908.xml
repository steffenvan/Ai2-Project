<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000200">
<title confidence="0.997286">
Collecting a Motion-Capture Corpus of American Sign Language
for Data-Driven Generation Research
</title>
<author confidence="0.998042">
Pengfei Lu
</author>
<affiliation confidence="0.998392">
Department of Computer Science
Graduate Center
City University of New York (CUNY)
</affiliation>
<address confidence="0.775827">
365 Fifth Ave, New York, NY 10016
</address>
<email confidence="0.999008">
pengfei.lu@qc.cuny.edu
</email>
<author confidence="0.990667">
Matt Huenerfauth
</author>
<affiliation confidence="0.999284">
Department of Computer Science
Queens College and Graduate Center
City University of New York (CUNY)
</affiliation>
<address confidence="0.859442">
65-30 Kissena Blvd, Flushing, NY 11367
</address>
<email confidence="0.999622">
matt@cs.qc.cuny.edu
</email>
<sectionHeader confidence="0.998565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997586666666667">
American Sign Language (ASL) generation
software can improve the accessibility of in-
formation and services for deaf individuals
with low English literacy. The understand-
ability of current ASL systems is limited; they
have been constructed without the benefit of
annotated ASL corpora that encode detailed
human movement. We discuss how linguistic
challenges in ASL generation can be ad-
dressed in a data-driven manner, and we de-
scribe our current work on collecting a
motion-capture corpus. To evaluate the qual-
ity of our motion-capture configuration, cali-
bration, and recording protocol, we conducted
an evaluation study with native ASL signers.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998094916666666">
American Sign Language (ASL) is the primary
means of communication for about one-half mil-
lion deaf people in the U.S. (Mitchell et al., 2006).
ASL has a distinct word-order, syntax, and lexicon
from English; it is not a representation of English
using the hands. Although reading is part of the
curriculum for deaf students, lack of auditory ex-
posure to English during the language-acquisition
years of childhood leads to lower literacy for many
adults. In fact, the majority of deaf high school
graduates in the U.S. have only a fourth-grade (age
10) English reading level (Traxler, 2000).
</bodyText>
<subsectionHeader confidence="0.996394">
1.1 Applications of ASL Generation Research
</subsectionHeader>
<bodyText confidence="0.969986">
Most technology used by the deaf does not address
this literacy issue; many deaf people find it diffi-
</bodyText>
<page confidence="0.996916">
89
</page>
<bodyText confidence="0.999864228571428">
cult to read the English text on a computer screen
or on a television with closed-captioning. Software
to present information in the form of animations of
ASL could make information and services more
accessible to deaf users, by displaying an animated
character performing ASL, rather than English
text. While writing systems for ASL have been
proposed (Newkirk, 1987; Sutton, 1998), none is
widely used in the Deaf community. Thus, an
ASL generation system cannot produce text output;
the system must produce an animation of a human
character performing sign language. Coordinating
the simultaneous 3D movements of parts of an
animated character’s body is challenging, and few
researchers have attempted to build such systems.
Prior work can be divided into two areas:
scripting and generation/translation. Scripting sys-
tems allow someone who knows sign language to
“word process” an animation by assembling a se-
quence of signs from a lexicon and adding facial
expressions. The eSIGN project created tools for
content developers to build sign databases and as-
semble scripts of signing for web pages (Ken-
naway et al., 2007). Sign Smith Studio (Vcom3D,
2010) is a commercial tool for scripting ASL (dis-
cussed in section 4). Others study generation or
machine translation (MT) of sign language (Chiu
et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al.,
2008; Huenerfauth, 2006; Karpouzis et al., 2007;
Marshall &amp; Safar, 2005; Shionome et al., 2005;
Sumihiro et al., 2000; van Zijl &amp; Barker, 2003).
Experimental evaluations of the understandabil-
ity of state-of-the-art ASL animation systems have
shown that native signers often find animations
difficult to understand (as measured by compre-
</bodyText>
<note confidence="0.9845215">
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9987676">
hension questions) or unnatural (as measured by
subjective evaluation questions) (Huenerfauth et
al., 2008). Errors include a lack of smooth inter-
sign transitions, lack of grammatically-required
facial expressions, and inaccurate sign perform-
ances related to morphological inflection of signs.
While current ASL animation systems have
limitations, there are several advantages in present-
ing sign language content in the form of animated
virtual human characters, rather than videos:
</bodyText>
<listItem confidence="0.997015793103448">
• Generation or MT software planning ASL sen-
tences cannot just concatenate videos of ASL.
Using video clips, it is difficult to produce
smooth transitions between signs, subtle mo-
tion variations in sign performances, or proper
combinations of facial expressions with signs.
• If content must be frequently modified or up-
dated, then a video performance would need to
be largely re-recorded for each modification.
Whereas, an animation (scripted by a human
author) could be further edited or modified.
• Because the face is used to indicate important
information in ASL, a human must reveal his
or her identity when producing an ASL video.
Instead, a virtual human character could per-
form sentences scripted by a human author.
• For wiki-style applications in which multiple
authors are collaborating on information con-
tent, ASL videos would be distracting: the per-
son performing each sentence may differ. A
virtual human would be more uniform.
• Animations can be appealing to children for
use in educational applications.
• Animations allow ASL to be viewed at differ-
ent angles, at different speeds, or by different
virtual humans – depending on the preferences
of the user. This can enable education applica-
tions in which students learning ASL can prac-
tice their ASL comprehension skills.
</listItem>
<subsectionHeader confidence="0.917802">
1.2 ASL is Challenging for NLP Research
</subsectionHeader>
<bodyText confidence="0.938883">
Natural Language Processing (NLP) researchers
often apply techniques originally designed for one
language to another, but research is not commonly
ported to sign languages. One reason is that with-
out a written form for ASL, NLP researchers must
produce animation and thus address several issues:
</bodyText>
<listItem confidence="0.9939485">
• Timing: An ASL performance’s speed consists
of: the speed of individual sign performances,
</listItem>
<bodyText confidence="0.841819">
the transitional time between signs, and the in-
sertion of pauses during signing – all of which
are based on linguistic factors such as syntactic
boundaries, repetition of signs in a discourse,
and the part-of-speech of signs (Grosjean et al.,
1979). ASL animations whose speed and paus-
ing are incorrect are significantly less under-
standable to ASL signers (Huenerfauth, 2009).
</bodyText>
<listItem confidence="0.904337">
• Spatial Reference: Signers arrange invisible
placeholders in the space around their body to
</listItem>
<bodyText confidence="0.854832666666667">
represent objects or persons under discussion
(Meier, 1990). To perform personal, posses-
sive, or reflexive pronouns that refer to these
entities, signers later point to these locations.
Signers may not repeat the identity of these en-
tities again; so, their conversational partner
must remember where they have been placed.
An ASL generator must select which entities
should be assigned 3D locations (and where).
</bodyText>
<listItem confidence="0.975580388888889">
• Inflection: Many verbs change their motion
paths to indicate the 3D location where a spa-
tial reference point has been established for
their subject, object, or both (Padden, 1988).
Generally, the motion paths of these inflecting
verbs change so that their direction goes from
the subject to the object (Figure 1); however,
their paths are more complex than this. Each
verb has a standard motion path that is affected
by the subject’s and the object’s 3D locations.
When a verb is inflected in this way, the signer
does not need to overtly state the subject/object
of a sentence. An ASL generator must produce
appropriately inflected verb paths based on the
layout of the spatial reference points.
Figure 1: An ASL inflecting verb “BLAME”:
(a.) (person on left) blames (person on right),
(b.) (person on right) blames (person on left).
</listItem>
<page confidence="0.862473">
90
</page>
<listItem confidence="0.99916924">
• Coarticulation: As in speech production, the
surrounding signs in a sentence affect finger,
hand, and body movements. ASL generators
that use overly simple interpolation rules to
produce these coarticulation effects yield un-
natural and non-fluent ASL animation output.
• Non-Manuals: Head-tilt and eye-gaze indicate
the 3D location of a verb’s subject and object
(or other information); facial expressions also
indicate negation, questions, topicalization,
and other essential syntactic phenomena not
conveyed by the hands (Neidle et al., 2000).
Animations without proper facial expressions
(and proper timing relative to manual signs)
cannot convey the proper meaning of ASL sen-
tences in a fluent and understandable manner.
• Evaluation: With no standard written form for
ASL, string-based metrics cannot be used to
evaluate ASL generation output automatically.
User-based experiments are necessary, but it is
difficult to accurately: screen for native sign-
ers, prevent English environmental influences
(that affect signer’s linguistic judgments), and
design questions that measure comprehension
of ASL animations (Huenerfauth et al., 2008).
</listItem>
<subsectionHeader confidence="0.822193">
1.3 Need for Data-Driven ASL Generation
</subsectionHeader>
<bodyText confidence="0.999964882352941">
Due to these challenges, most prior sign language
generation or MT projects have been short-lived,
producing few example outputs (Zhao et al., 2000;
Veale et al., 1998). Further developed systems also
have limited coverage; e.g., Marshall and Safar
(2005) hand-built translation transfer rules from
English to British Sign Language. Huenerfauth
(2006) surveys several rule-based systems and dis-
cusses how they generally: have limited coverage;
often merely concatenate signs; and do not address
the Coarticulation, Spatial Reference, Timing,
Non-Manuals, or Inflection issues (section 1.2).
Unfortunately, most prior work is not “data-
driven,” i.e. not based on statistical modeling of
corpora, the dominant successful modern NLP ap-
proach. The sign language generation research that
has thus far been the most data-driven includes:
</bodyText>
<listItem confidence="0.776360833333333">
• Some researchers have used motion-capture
(see section 3) to build lexicons of animations
of individual signs, e.g. (Cox et al., 2002).
However, their focus is recording a single cita-
tion form of each sign, not creating annotated
corpora of full sentences or discourse. Single-
</listItem>
<bodyText confidence="0.7035252">
sign recordings do not enable researchers to
examine the Timing, Coarticulation, Spatial
Reference, Non-Manuals, or Inflection phe-
nomena (section 1.2), which operate over mul-
tiple signs or sentences in an ASL discourse.
</bodyText>
<listItem confidence="0.989005666666667">
• Other researchers have examined how statisti-
cal MT techniques could be used to translate
from a written language to a sign language.
Morrissey and Way (2005) discuss an exam-
ple-based MT architecture for Irish Sign Lan-
guage, and Stein et al. (2006) apply simple
statistical MT approaches to German Sign
Language. Unfortunately, the sign language
“corpora” used in these studies consist of tran-
scriptions of the sequence of signs performed,
not recordings of actual human performances.
A transcription does not capture subtleties in
the 3D movements of the hands, facial move-
ments, or speed of an ASL performance. Such
information is needed in order to address the
Spatial Reference, Inflection, Coarticulation,
Timing, or Non-Manuals issues (section 1.2).
• Seguoat and Braffort (2009) derive models of
coarticulation for French Sign Language based
on a semi-automated “rotoscoping” annotation
of hand location from videos of signing.
</listItem>
<subsectionHeader confidence="0.913557">
1.4 Prior Sign Language Corpora Resources
</subsectionHeader>
<bodyText confidence="0.999954409090909">
The reason why most prior ASL generation re-
search has not been data-driven is that sufficiently
detailed and annotated sign language corpora are in
short supply and are time-consuming to construct.
Without a writing system in common use, it is not
possible to harvest some naturally arising source of
ASL “text”; instead, it is necessary to record the
performance of a signer (through video or a mo-
tion-capture suit). Human signers must then tran-
scribe and annotate this data by adding time-
stamped linguistic details. For ASL (Neidle et al.,
2000) and European sign languages (Bungeroth et
al., 2006; Crasborn et al., 2004, 2006; Efthimiou &amp;
Fotinea, 2007), signers have been videotaped and
experts marked time spans when events occur –
e.g. the right hand is performing the sign “CAT”
during time index 250-300 milliseconds, and the
eyebrows are raised during time index 270-300.
Such annotation is time-consuming to add; the
largest ASL corpus has a few thousand sentences.
In order to learn how to control the movements
of an animated virtual human based on a corpus,
</bodyText>
<page confidence="0.993748">
91
</page>
<bodyText confidence="0.999958846153846">
we need precise hand locations and joint angles of
the human signer’s body throughout the perform-
ance. Asking humans to write down 3D angles and
coordinates is time-consuming and inexact; some
researchers have used computer vision techniques
to model the signers’ movements (see survey in
(Loeding et al., 2004)). Unfortunately, the com-
plex shape of hands/face, rapid speed, and frequent
occlusion of parts of the body during ASL limit the
accuracy of vision-based recognition; it is not yet a
reliable way to build a 3D model of a signer for a
corpus. Motion-capture technology (discussed in
section 3) is required for this level of detail.
</bodyText>
<sectionHeader confidence="0.970647" genericHeader="method">
2 Research Goals &amp; Focus of This Paper
</sectionHeader>
<bodyText confidence="0.999721090909091">
To address the lack of sufficiently detailed and
linguistically annotated ASL corpora, we have be-
gun a multi-year project to collect and annotate a
motion-capture corpus of ASL (section 3). Digital
3D body movement and handshape data collected
from native signers will become a permanent re-
search resource for study by NLP researchers and
ASL linguists. This corpus will allow us to create
new ASL generation technologies in a data-driven
manner by analyzing the subtleties in the motion
data and its relationship to the linguistic structure.
Specifically, we plan to model where signers tend
to place spatial reference points around them in
space. We also plan to uncover patterns in the mo-
tion paths of inflecting verbs and model how they
relate to layout of spatial references points. These
models could be used in ASL generation software
or could be used to partially automate with work of
humans using ASL-scripting systems. To evaluate
our ASL models, native signers will be asked to
judge ASL animations produced using them. There
are several unique aspects of our research:
</bodyText>
<listItem confidence="0.950071125">
• We use a novel combination of hand, body,
head, and eye motion-tracking technologies
and simultaneous video recordings (section 3).
• We collect multi-sentence single-signer ASL
discourse, and we annotate novel linguistic in-
formation (relevant to spatial reference points).
• We involve ASL signers in the research in
several ways: as evaluators of our generation
</listItem>
<bodyText confidence="0.985507863636364">
software, as research assistants conducting
evaluation studies, and as corpus annotators.
This paper will focus on the first of these as-
pects of our project. Specifically, section 4 will
examine the following research question: Have we
successfully configured and calibrated our motion-
capture equipment so that we are recording good-
quality data that will be useful for NLP research?
Since the particular combination of motion-
capture equipment we are using is novel and be-
cause there have not been prior motion-capture-
based ASL corpora projects, section 4 will evaluate
whether the data we are collecting is of sufficient
quality to drive ASL animations of a virtual human
character. In corpus-creation projects for tradi-
tional written/spoken languages, researchers typi-
cally gather text, audio, or (sometimes) video of
human performances. The quality of the gathered
recordings is typically easier to verify and evalu-
ate; for motion-capture data collected with a com-
plex configuration of equipment, a more complex
experimental design is necessary (section 4).
</bodyText>
<sectionHeader confidence="0.995368" genericHeader="method">
3 Our Motion-Capture Configuration
</sectionHeader>
<bodyText confidence="0.999840666666667">
The first stage of our research is to accurately and
efficiently record 3D motion-capture data from
ASL signers. Assuming an ASL signer’s pelvis
bone is stationary in 3D space, we want to record
movement data for the upper body. We are inter-
ested in the shapes of each hand; the 3D location of
the hands; the 3D orientation of the palms; joint
angles for the wrists, elbows, shoulders, clavicle,
neck, and waist; and a vector representing the eye-
gaze aim. We are using a customized configura-
tion of several commercial motion-capture devices
(as shown in Figure 2, worn by a human signer):
</bodyText>
<listItem confidence="0.691707538461538">
• Two Immersion CyberGloves®: The 22 flexi-
ble sensor strips sewn into each of these
spandex gloves record finger joint angles so
that we can record the signer’s handshapes.
These gloves are ideal for recording ASL be-
cause they are flexible and lightweight. Hu-
mans viewing a subject wearing the gloves are
able to discern ASL fingerspelling and signing.
• Applied Science Labs H6 eye-tracker: This
lightweight head-mounted eye-tracker with a
near-eye camera records a signer’s eye gaze di-
rection. A camera on the headband aims down,
and a small clear plastic panel in front of the
</listItem>
<bodyText confidence="0.9556905">
cheek reflects the image of the subject’s eye.
When combined with the head tracking infor-
mation from the IS-900 system below, the H6
identifies a 3D vector of eye-gaze in a room.
</bodyText>
<page confidence="0.907023">
92
</page>
<figureCaption confidence="0.812964333333333">
a. b. c.
Figure 2: (a) Motion-capture equipment configuration, (b) animation produced from motion-capture data
(shown in evaluation study), and (c) animation produced using Sign Smith (shown in evaluation study).
</figureCaption>
<listItem confidence="0.877245176470588">
• Intersense IS-900: This acoustical/intertial mo-
tion-capture system uses a ceiling-mounted ul-
trasonic speaker array and a set of directional
microphones on a small sensor to record the
location and orientation of the signer’s head.
A sensor sits atop the helmet shown in Figure
2a. IS-900 data is used to compensate for head
movement when calculating eye-gaze direction
with the Applied Science Labs H6 eye-tracker.
• Animazoo IGS-190: This spandex bodysuit is
covered with soft Velcro to which small sen-
sors attach. A sensor placed on each segment
of the human’s body records inertial and mag-
netic information. Subjects wearing the suit
stand facing north with their arms down at
their sides at the beginning of the recording
session; given this known starting pose and di-
</listItem>
<bodyText confidence="0.987580666666667">
rection, the system calculates joint angles for
the wrists, elbows, shoulders, clavicle, neck,
and waist. We do not record leg/foot informa-
tion in our corpus. Prior to recording data, we
photograph subjects standing in a cube-shaped
rig of known size; this allows us to identify
bone lengths of the human subject, which are
needed for the IGS-190 system to accurately
calculate joint angles from the sensor data.
Motion-capture recording sessions are video-
taped to facilitate later linguistic analysis and an-
notation. Videotaping the session also facilitates
the “clean up” of the motion-capture data in post-
processing, during which algorithms are applied to
adjust synchronization of different sensors or re-
move “jitter” or other noise artifacts from the re-
cording. Three digital high-speed video cameras
film front view, facial close-up, and side views of
the signer – a setup that has been used in video-
based ASL-corpora-building projects (Neidle et al.,
2000). The front view is similar to Figure 2a (but
wider). The facial close-up view is useful when
later identifying specific non-manual facial expres-
sions during ASL performances, which are essen-
tial to correctly understanding and annotating the
collected data. To facilitate synchronizing the three
video files during post-processing, a strobe is
flashed once at the start of the recording session.
A “blue screen” curtain hangs on the back and
side walls of the motion-capture studio. If future
computer vision researchers wish to use this corpus
to study ASL recognition from video, it is useful to
have solid color walls for “chroma key” back-
ground removal. Photographic studio lighting with
spectra compatible with the eye-tracking system is
used to support high-quality video recording.
During data collection, a native ASL signer
(called the “prompter”) sits directly behind the
front-view camera to engage the participant wear-
ing the suit (the “performer”) in natural conversa-
tion. While the corpus we are collecting consists of
unscripted single-signer discourse, prior ASL cor-
pora projects have identified the importance of sur-
rounding signers with an ASL-centric environment
during data collection (Neidle et al., 2000). English
influence in the studio must be minimized to pre-
vent signers from inadvertently code-switching to
an English-like form of signing. Thus, it is impor-
tant that a native signer acts as the prompter, who
conversationally suggests topics for the performer
to discuss (to be recorded as part of the corpus).
</bodyText>
<page confidence="0.997461">
93
</page>
<bodyText confidence="0.999807117647059">
In our first year, we have collected and anno-
tated 58 passages from 6 signers (40 minutes). We
prefer to collect multi-sentence passages discuss-
ing varied numbers of topics and with few “classi-
fier predicates,” phenomena that aren’t our current
research focus. In (Huenerfauth &amp; Lu, 2010), we
discuss details of: the genre of discourse we re-
cord, our target linguistic phenomena to capture
(spatial reference points and inflected verbs), the
types of linguistic annotation added to the corpus,
and the effectiveness of different “prompts” used
to elicit the desired type of spontaneous discourse.
This paper focuses on verifying the quality of
the motion-capture data we can record using our
current equipment configuration and protocols. We
want to measure how well we have compensated
for several possible sources of error in recordings:
</bodyText>
<listItem confidence="0.994900407407408">
• If a sensor connection is temporarily lost, then
data gaps occur. We have selected equipment
that does not require line-of-sight connections
and tried to arrange the studio to avoid fre-
quent dropping of any wireless connections.
• We ask subjects to perform a quick head
movement and distinctive eye blink pattern at
the beginning of the recording session to facili-
tate “synchronization” of the various motion-
capture data streams during post-processing.
• Electronic and physical properties of sensors
can lead to “noise” in the data, which we at-
tempt to remove with smoothing algorithms.
• Differences between the bone lengths of the
human and the “virtual skeleton” of the ani-
mated character being recorded could lead to
“retargeting” errors, in which the body poses
of the human do not match the recording. We
must be careful in the measurement of the
bone lengths of the human participant and in
the design of the virtual animation skeleton.
• To compensate for differences in how equip-
ment sits on the body on different occasions or
on different humans, we must set “calibration”
values; e.g., we designed a novel protocol for
efficiently and accurately calibrating gloves for
ASL signers (Lu &amp; Huenerfauth, 2009).
</listItem>
<sectionHeader confidence="0.987025" genericHeader="method">
4 Evaluating Our Collected Motion Data
</sectionHeader>
<bodyText confidence="0.999496555555555">
If a speech synthesis researcher were using a novel
microphone technology to record audio perform-
ances from human speakers to build a corpus, that
researcher would want to experimentally confirm
that the audio recordings were of high enough
quality for research. Even when perfectly clear
audio recordings of human speech are recorded in
a corpus, the automatic speech synthesis models
trained on this data are not perfect. Degradations
in the quality of the corpus would yield even lower
quality speech synthesis systems. In the same way,
it is essential that we evaluate the quality of the
ASL motion-capture data we are collecting.
In an earlier study, we sought to collect motion-
data from humans and directly produce animations
from them as an “upper baseline” for an experi-
mental study (Huenerfauth, 2006). We were not
analyzing the collected data or using it for data-
driven generation, we merely wanted the data to
directly drive an animation of a virtual human
character as a “virtual puppet.” This earlier project
used a different configuration of motion-capture
equipment, including an earlier version of Cyber-
Gloves® and an optical motion-capture system that
required line-of-sight connections between infrared
emitters on the signer’s body and cameras around
the room. Unfortunately, the data collected was so
poor that the animations produced from the mo-
tion-capture were not an “upper” baseline – in fact,
they were barely understandable to native signers.
Errors arose from dropped connections, poor cali-
bration, and insufficient removal of data noise.
We have selected different equipment and have
designed better protocols for recording high quality
ASL data since that earlier study – to compensate
for the “noise,” “retargeting,” “synchronization,”
and “calibration” issues mentioned in section 3.
However, we know that under some recording
conditions, the quality of collected motion-capture
data is so poor that “virtual puppet” animations
synthesized from it are not understandable. We
expect that an even higher level of data quality is
needed for a motion-capture corpus, which will be
analyzed and manipulated in order to synthesize
novel ASL animations from it. Therefore, we con-
ducted a study (discussed below) to evaluate the
quality of our current motion-capture configura-
tion. As in our past study, we use the motion-
capture data to directly control the body move-
ments of a virtual human “puppet.” We then ask
native ASL signers to evaluate the understandabil-
ity and naturalness of the resulting animations (and
compare them to some baseline animations pro-
duced using ASL-animation scripting software).
</bodyText>
<page confidence="0.998944">
94
</page>
<figureCaption confidence="0.999853">
Figure 3: Evaluation and comprehension scores (asterisks mark significant pairwise differences).
</figureCaption>
<bodyText confidence="0.999985847222222">
In our prior work, a native ASL signer designed
a set of ASL stories and corresponding compre-
hension questions for use in evaluation studies
(Huenerfauth, 2009). The stories’ average length is
approximately 70 signs, and they consist of news
stories, encyclopedia articles, and short narratives.
We produced animations of each using Sign Smith
Studio (SSS), commercial ASL-animation script-
ing software (Vcom3D, 2010). Signs from SSS’s
lexicon are placed on a timeline, and linguistically
appropriate facial expressions are added. The soft-
ware synthesizes an animation of a virtual human
performing the story (Figure 2c). In earlier work,
we designed algorithms for determining sign-speed
and pause-insertion in ASL animations based on
linguistic features of the sentence. We conducted a
study to compare animations with default timing
settings (uniform pauses and speed) and anima-
tions governed by our timing algorithm – at vari-
ous speeds. The use of our timing algorithm
yielded ASL animations that native signers found
more understandable (Huenerfauth, 2009). We are
reusing these stories and animations as baselines
for comparison in a new evaluation study (below).
While we are collecting unscripted passages in
our corpus, it is easier to compare the quality of
different versions of animations when using a
common set of scripted stories. Thus, we used the
script from 10 of the stories above, and each was
performed by a native signer, a 22-year-old male
who learned ASL prior to age 2. He wore the full
set of motion-capture equipment, and we followed
the same calibration process and protocols as we
do when recording ASL passages for our corpus.
The signer rehearsed and memorized each story;
“cue cards” were also available when recording.
Autodesk MotionBuilder software was used to
produce a virtual human whose movements were
driven by the motion-capture data (see Figure 2b).
While our corpus contains video of facial expres-
sion, our motion-capture equipment does not digit-
ize it; so, the virtual human character has no facial
movements. The recorded signer moved at an av-
erage speed of 1.12 signs/second, and so for com-
parison, we selected the version of the scripted
ASL animations with the closest speed from our
earlier study: 1.2 signs/second. (Since the scripted
animations are slightly slower and include linguis-
tic facial expressions, we expected them to receive
higher understandability scores than our motion-
capture animations.) In our earlier work, we pro-
duced two versions of each scripted story: one with
default timing and one with our novel timing algo-
rithm. Both versions are used as baselines for
comparison in this new study; thus, we compare
three versions of the same set of 10 ASL stories.
Using questions designed to screen for native
ASL signers developed in prior work (Huenerfauth
et al., 2008), we recruited 12 participants to evalu-
ate the ASL animations. A native ASL signer con-
ducted the studies, in which participants viewed an
animation and were then asked two types of ques-
tions after each: (1) ten-point Likert-scale ques-
tions about the ASL animation’s grammatical
correctness, understandability, and naturalness of
movement and (2) multiple-choice comprehension
questions about basic facts from the story. The
comprehension questions were presented in the
form of scripted ASL animations (produced in
SSS), and answer choices were presented in the
form of clip-art images (so that strong English lit-
eracy was not necessary). Identical questions were
</bodyText>
<page confidence="0.997348">
95
</page>
<bodyText confidence="0.9996885">
used to evaluate the motion-capture animations and
the scripted animations. Examples of the questions
are included in (Huenerfauth, 2009).
Figure 3 displays results of the Likert-scale sub-
jective questions and comprehension-question suc-
cess scores for the three types of animations
evaluated in this study. The scripted animations
using our timing algorithm have higher compre-
hension scores, but the motion-capture animations
have higher naturalness scores. All of the other
scores for the animations are quite similar. Statisti-
cally significant differences are marked with an
asterisk (p&lt;0.05, Mann-Whitney pairwise compari-
sons with Bonferroni-corrected p-values). Non-
parameteric tests were selected because the Likert-
scale responses were not normally distributed.
</bodyText>
<sectionHeader confidence="0.99528" genericHeader="conclusions">
5 Conclusion and Future Research Goals
</sectionHeader>
<bodyText confidence="0.999990411764706">
The research question addressed by this paper was
whether our motion-capture configuration and re-
cording protocols enabled us to collect motion-data
of sufficient quality for data-driven ASL genera-
tion research. In our study, the evaluation scores of
the animations driven by the motion-capture data
were similar to those of animations produced using
state-of-the-art ASL animation scripting software.
This is a promising result, especially considering
the slightly faster speed and lack of facial expres-
sion information in the motion-capture animations.
While this suggests that the data we are collecting
is of good quality, the real test will be when this
corpus is used in future research. If we can build
useful ASL-animation generation software based
on analysis of this corpus, then we will know that
we have sufficient quality of motion-capture data.
</bodyText>
<subsectionHeader confidence="0.774004">
5.1 Our Long-Term Research Goal: Making
ASL Accessible to More NLP Researchers
</subsectionHeader>
<bodyText confidence="0.998214685185185">
It is our goal to produce high-quality broad-
coverage ASL generation software, which would
benefit many deaf individuals with low English
literacy. However, this ambition is too large for
any one team; for this technology to become real-
ity, ASL must become a language commonly stud-
ied by NLP researchers. For this reason, we seek
to build ASL software, models, and experimental
techniques to serve as a resource for other NLP
researchers. Our goal is to make ASL “accessible”
to the NLP community. By developing tools to
address some of the modality-specific and spatial
aspects of ASL, we can make it easier for other
researchers to transfer their new NLP techniques to
ASL. The goal is to “normalize” ASL in the eyes
of the NLP community. Bridging NLP and ASL
research will not only benefit deaf users: ASL will
push the limits of current NLP techniques and will
thus benefit other work in the field of NLP. Sec-
tion 1.2 listed six challenges for ASL NLP re-
search; we address several of these in our research:
We have conducted many experimental studies
in which signers evaluate the understandability and
naturalness of ASL animations (Huenerfauth et al.,
2008; Huenerfauth, 2009). To begin to address the
Evaluation issue (section 1.2), we have published
best-practices, survey materials, and experimental
protocols for effectively evaluating ASL animation
systems through the participation of native signers.
We have also published baseline comprehension
scores for ASL animations. We will continue to
produce such resources in future work.
Our earlier work on timing algorithms for ASL
animations (mentioned in section 4) was based on
data reported in the linguistics literature (Grosjean
et al., 1979). In future work, we want to learn tim-
ing models directly from our collected corpus – to
further address the Timing issue (section 1.2).
To address the issues of Spatial Reference and
Inflection (section 1.2), we plan on analyzing our
ASL corpus to build models that can predict where
in 3D space signers establish spatial reference
points. Further, we will analyze our corpus to ana-
lyze how certain ASL verbs are inflected based on
the 3D location of their subject and object. We
want to build a parameterized lexicon of ASL
verbs: given a 3D location for subject and object,
we want to predict a 3D motion-path for the char-
acter’s hands for a specific performance of a verb.
While addressing the issues of Coarticulation
and Non-Manuals (section 1.2) are not immediate
research priorities, we believe our ASL corpus may
also be useful in building computational models of
these phenomena for data-driven ASL generation.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.923177428571429">
This material is based upon work supported by the
National Science Foundation (Award #0746556),
Siemens (Go PLM Academic Grant), and Visage
Technologies AB (free academic license for soft-
ware). Jonathan Lamberton, Wesley Clarke, Kel-
sey Gallagher, Amanda Krieger, and Aaron Pagan
assisted with ASL data collection and experiments.
</reference>
<page confidence="0.998527">
96
</page>
<sectionHeader confidence="0.990267" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999618117117117">
J. Bungeroth, D. Stein, P. Dreuw, M. Zahedi, H. Ney.
2006. A German sign language corpus of the domain
weather report. Proc. LREC 2006 workshop on rep-
resentation &amp; processing of sign languages.
Y.H. Chiu, C.H. Wu, H.Y. Su, C.J. Cheng. 2007. Joint
optimization of word alignment and epenthesis gen-
eration for Chinese to Taiwanese sign synthesis.
IEEE Trans Pattern Anal Mach Intell 29(1):28-39.
S. Cox, M. Lincoln, J. Tryggvason, M. Nakisa, M.
Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to
aid communication with deaf people. Proc. ASSETS.
O. Crasborn, E. van der Kooij, D. Broeder, H. Brugman.
2004. Sharing sign language corpora online: propos-
als for transcription and metadata categories. Proc.
LREC 2004 workshop on representation &amp; process-
ing of sign languages, pp. 20-23.
O. Crasborn, H. Sloetjes, E. Auer, and P. Wittenburg.
2006. Combining video and numeric data in the
analysis of sign languages within the ELAN annota-
tion software. Proc. LREC 2006 workshop on repre-
sentation &amp; processing of sign languages, 82-87.
E. Efthimiou, S.E. Fotinea. 2007. GSLC: creation and
annotation of a Greek sign language corpus for HCI.
Proc. HCI International.
R. Elliot, J. Glauert. 2008. Linguistic modeling and lan-
guage-processing technologies for avatar-based sign
language presentation. Universal Access in the In-
formation Society 6(4):375-391.
S.E. Fotinea, E. Efthimiou, G. Caridakis, K. Karpouzis.
2008. A knowledge-based sign synthesis architecture.
Univ. Access in Information Society 6(4):405-418.
F. Grosjean, L. Grosjean, H. Lane. 1979. The patterns of
silence: Performance structures in sentence produc-
tion. Cognitive Psychology 11:58-81.
M. Huenerfauth. 2006. Generating American sign lan-
guage classifier predicates for English-to-ASL ma-
chine translation, dissertation, U. of Pennsylvania.
M. Huenerfauth, L. Zhao, E. Gu, J. Allbeck. 2008.
Evaluation of American sign language generation by
native ASL signers. ACM Trans Access Comput
1(1):1-27.
M. Huenerfauth. 2009. A linguistically motivated model
for speed and pausing in animations of American
sign language. ACM Trans Access Comput 2(2):1-31.
M. Huenerfauth, P. Lu. 2010. Annotating spatial refer-
ence in a motion-capture corpus of American sign
language discourse. Proc. LREC 2010 workshop on
representation &amp; processing of sign languages.
K. Karpouzis, G. Caridakis, S.E. Fotinea, E. Efthimiou.
2007. Educational resources and implementation of a
Greek sign language synthesis architecture. Comput-
ers &amp; Education 49(1):54-74.
J. Kennaway, J. Glauert, I. Zwitserlood. 2007. Providing
signed content on Internet by synthesized animation.
ACM Trans Comput-Hum Interact 14(3):15.
B. Loeding, S. Sarkar, A. Parashar, A. Karshmer. 2004.
Progress in automated computer recognition of sign
language, Proc. ICCHP, 1079-1087.
P. Lu, M. Huenerfauth. 2009. Accessible motion-
capture glove calibration protocol for recording sign
language data from deaf subjects. Proc. ASSETS.
I. Marshall, E. Safar. 2005. Grammar development for
sign language avatar-based synthesis. Proc. UAHCI.
R. Meier. 1990. Person deixis in American sign lan-
guage. In: S. Fischer &amp; P. Siple (eds.), Theoretical
issues in sign language research, vol. 1: Linguistics.
Chicago: University of Chicago Press, 175-190.
R. Mitchell, T. Young, B. Bachleda, M. Karchmer.
2006. How many people use ASL in the United
States? Sign Language Studies 6(3):306-335.
S. Morrissey, A. Way. 2005. An example-based ap-
proach to translating sign language. Proc. Workshop
on Example-Based Machine Translation, 109-116.
C. Neidle, D. Kegl, D. MacLaughlin, B. Bahan, &amp; R.G.
Lee. 2000. The syntax of ASL: functional categories
and hierarchical structure. Cambridge: MIT Press.
D. Newkirk. 1987. SignFont Handbook. San Diego:
Emerson and Associates.
C. Padden. 1988. Interaction of morphology &amp; syntax in
American sign language. Outstanding dissertations
in linguistics, series IV. New York: Garland Press.
J. Segouat, A. Braffort. 2009. Toward the study of sign
language coarticulation: methodology proposal. Proc
Advances in Comput.-Human Interactions, 369-374.
T. Shionome, K. Kamata, H. Yamamoto, S. Fischer.
2005. Effects of display size on perception of Japa-
nese sign language---Mobile access in signed lan-
guage. Proc. Human-Computer Interaction, 22-27.
D. Stein, J. Bungeroth, H. Ney. 2006. Morpho-syntax
based statistical methods for sign language transla-
tion. Proc. European Association for MT, 169-177.
K. Sumihiro, S. Yoshihisa, K. Takao. 2000. Synthesis of
sign animation with facial expression and its effects
on understanding of sign language. IEIC Technical
Report 100(331):31-36.
V. Sutton. 1998. The Signwriting Literacy Project. In
Impact of Deafness on Cognition AERA Conference.
C. Traxler. 2000. The Stanford achievement test, ninth
edition: national norming and performance standards
for deaf and hard-of-hearing students. J. Deaf Studies
and Deaf Education 5(4):337-348.
L. van Zijl, D. Barker. 2003. South African sign lan-
guage MT system. Proc. AFRIGRAPH, 49-52.
VCom3D. 2010. Sign Smith Studio.
http://www.vcom3d.com/signsmith.php
T. Veale, A. Conway, B. Collins. 1998. Challenges of
cross-modal translation: English to sign translation in
ZARDOZ system. Machine Translation 13:81-106.
L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler,
M. Palmer. 2000. A machine translation system from
English to American sign language. Proc. AMTA.
</reference>
<page confidence="0.999689">
97
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.374523">
<title confidence="0.995991">Collecting a Motion-Capture Corpus of American Sign for Data-Driven Generation Research</title>
<author confidence="0.980512">Pengfei</author>
<affiliation confidence="0.883936">Department of Computer Graduate City University of New York</affiliation>
<address confidence="0.991183">365 Fifth Ave, New York, NY 10016</address>
<email confidence="0.999268">pengfei.lu@qc.cuny.edu</email>
<author confidence="0.991096">Matt</author>
<affiliation confidence="0.9875475">Department of Computer Queens College and Graduate</affiliation>
<address confidence="0.8059755">City University of New York 65-30 Kissena Blvd, Flushing, NY 11367</address>
<email confidence="0.999295">matt@cs.qc.cuny.edu</email>
<abstract confidence="0.9989205">American Sign Language (ASL) generation software can improve the accessibility of information and services for deaf individuals with low English literacy. The understandability of current ASL systems is limited; they have been constructed without the benefit of annotated ASL corpora that encode detailed human movement. We discuss how linguistic challenges in ASL generation can be addressed in a data-driven manner, and we describe our current work on collecting a motion-capture corpus. To evaluate the quality of our motion-capture configuration, calibration, and recording protocol, we conducted an evaluation study with native ASL signers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>This material is based upon work supported by the National Science Foundation (Award #0746556), Siemens (Go PLM Academic Grant), and Visage Technologies AB (free academic license for software). Jonathan Lamberton, Wesley Clarke, Kelsey Gallagher, Amanda Krieger, and Aaron Pagan assisted with ASL data collection and experiments.</title>
<marker></marker>
<rawString>This material is based upon work supported by the National Science Foundation (Award #0746556), Siemens (Go PLM Academic Grant), and Visage Technologies AB (free academic license for software). Jonathan Lamberton, Wesley Clarke, Kelsey Gallagher, Amanda Krieger, and Aaron Pagan assisted with ASL data collection and experiments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bungeroth</author>
<author>D Stein</author>
<author>P Dreuw</author>
<author>M Zahedi</author>
<author>H Ney</author>
</authors>
<title>A German sign language corpus of the domain weather report.</title>
<date>2006</date>
<booktitle>Proc. LREC</booktitle>
<contexts>
<context position="11740" citStr="Bungeroth et al., 2006" startWordPosition="1804" endWordPosition="1807">uage Corpora Resources The reason why most prior ASL generation research has not been data-driven is that sufficiently detailed and annotated sign language corpora are in short supply and are time-consuming to construct. Without a writing system in common use, it is not possible to harvest some naturally arising source of ASL “text”; instead, it is necessary to record the performance of a signer (through video or a motion-capture suit). Human signers must then transcribe and annotate this data by adding timestamped linguistic details. For ASL (Neidle et al., 2000) and European sign languages (Bungeroth et al., 2006; Crasborn et al., 2004, 2006; Efthimiou &amp; Fotinea, 2007), signers have been videotaped and experts marked time spans when events occur – e.g. the right hand is performing the sign “CAT” during time index 250-300 milliseconds, and the eyebrows are raised during time index 270-300. Such annotation is time-consuming to add; the largest ASL corpus has a few thousand sentences. In order to learn how to control the movements of an animated virtual human based on a corpus, 91 we need precise hand locations and joint angles of the human signer’s body throughout the performance. Asking humans to write</context>
</contexts>
<marker>Bungeroth, Stein, Dreuw, Zahedi, Ney, 2006</marker>
<rawString>J. Bungeroth, D. Stein, P. Dreuw, M. Zahedi, H. Ney. 2006. A German sign language corpus of the domain weather report. Proc. LREC 2006 workshop on representation &amp; processing of sign languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y H Chiu</author>
<author>C H Wu</author>
<author>H Y Su</author>
<author>C J Cheng</author>
</authors>
<title>Joint optimization of word alignment and epenthesis generation for Chinese to Taiwanese sign synthesis.</title>
<date>2007</date>
<journal>IEEE Trans Pattern Anal Mach Intell</journal>
<pages>29--1</pages>
<contexts>
<context position="3143" citStr="Chiu et al., 2007" startWordPosition="486" endWordPosition="489">e attempted to build such systems. Prior work can be divided into two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) </context>
</contexts>
<marker>Chiu, Wu, Su, Cheng, 2007</marker>
<rawString>Y.H. Chiu, C.H. Wu, H.Y. Su, C.J. Cheng. 2007. Joint optimization of word alignment and epenthesis generation for Chinese to Taiwanese sign synthesis. IEEE Trans Pattern Anal Mach Intell 29(1):28-39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cox</author>
<author>M Lincoln</author>
<author>J Tryggvason</author>
<author>M Nakisa</author>
<author>M Wells</author>
<author>M Tutt</author>
<author>S Abbott</author>
</authors>
<title>Tessa, a system to aid communication with deaf people.</title>
<date>2002</date>
<booktitle>Proc. ASSETS.</booktitle>
<contexts>
<context position="9801" citStr="Cox et al., 2002" startWordPosition="1500" endWordPosition="1503">) surveys several rule-based systems and discusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2). Unfortunately, most prior work is not “datadriven,” i.e. not based on statistical modeling of corpora, the dominant successful modern NLP approach. The sign language generation research that has thus far been the most data-driven includes: • Some researchers have used motion-capture (see section 3) to build lexicons of animations of individual signs, e.g. (Cox et al., 2002). However, their focus is recording a single citation form of each sign, not creating annotated corpora of full sentences or discourse. Singlesign recordings do not enable researchers to examine the Timing, Coarticulation, Spatial Reference, Non-Manuals, or Inflection phenomena (section 1.2), which operate over multiple signs or sentences in an ASL discourse. • Other researchers have examined how statistical MT techniques could be used to translate from a written language to a sign language. Morrissey and Way (2005) discuss an example-based MT architecture for Irish Sign Language, and Stein et</context>
</contexts>
<marker>Cox, Lincoln, Tryggvason, Nakisa, Wells, Tutt, Abbott, 2002</marker>
<rawString>S. Cox, M. Lincoln, J. Tryggvason, M. Nakisa, M. Wells, M. Tutt, S. Abbott. 2002. Tessa, a system to aid communication with deaf people. Proc. ASSETS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Crasborn</author>
<author>E van der Kooij</author>
<author>D Broeder</author>
<author>H Brugman</author>
</authors>
<title>Sharing sign language corpora online: proposals for transcription and metadata categories.</title>
<date>2004</date>
<booktitle>Proc. LREC 2004 workshop on representation &amp; processing of sign languages,</booktitle>
<pages>20--23</pages>
<marker>Crasborn, van der Kooij, Broeder, Brugman, 2004</marker>
<rawString>O. Crasborn, E. van der Kooij, D. Broeder, H. Brugman. 2004. Sharing sign language corpora online: proposals for transcription and metadata categories. Proc. LREC 2004 workshop on representation &amp; processing of sign languages, pp. 20-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Crasborn</author>
<author>H Sloetjes</author>
<author>E Auer</author>
<author>P Wittenburg</author>
</authors>
<title>Combining video and numeric data in the analysis of sign languages within the ELAN annotation software.</title>
<date>2006</date>
<booktitle>Proc. LREC 2006 workshop on representation &amp; processing of sign languages,</booktitle>
<pages>82--87</pages>
<marker>Crasborn, Sloetjes, Auer, Wittenburg, 2006</marker>
<rawString>O. Crasborn, H. Sloetjes, E. Auer, and P. Wittenburg. 2006. Combining video and numeric data in the analysis of sign languages within the ELAN annotation software. Proc. LREC 2006 workshop on representation &amp; processing of sign languages, 82-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Efthimiou</author>
<author>S E Fotinea</author>
</authors>
<title>GSLC: creation and annotation of a Greek sign language corpus for HCI.</title>
<date>2007</date>
<publisher>Proc. HCI International.</publisher>
<contexts>
<context position="11797" citStr="Efthimiou &amp; Fotinea, 2007" startWordPosition="1813" endWordPosition="1816">generation research has not been data-driven is that sufficiently detailed and annotated sign language corpora are in short supply and are time-consuming to construct. Without a writing system in common use, it is not possible to harvest some naturally arising source of ASL “text”; instead, it is necessary to record the performance of a signer (through video or a motion-capture suit). Human signers must then transcribe and annotate this data by adding timestamped linguistic details. For ASL (Neidle et al., 2000) and European sign languages (Bungeroth et al., 2006; Crasborn et al., 2004, 2006; Efthimiou &amp; Fotinea, 2007), signers have been videotaped and experts marked time spans when events occur – e.g. the right hand is performing the sign “CAT” during time index 250-300 milliseconds, and the eyebrows are raised during time index 270-300. Such annotation is time-consuming to add; the largest ASL corpus has a few thousand sentences. In order to learn how to control the movements of an animated virtual human based on a corpus, 91 we need precise hand locations and joint angles of the human signer’s body throughout the performance. Asking humans to write down 3D angles and coordinates is time-consuming and ine</context>
</contexts>
<marker>Efthimiou, Fotinea, 2007</marker>
<rawString>E. Efthimiou, S.E. Fotinea. 2007. GSLC: creation and annotation of a Greek sign language corpus for HCI. Proc. HCI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Elliot</author>
<author>J Glauert</author>
</authors>
<title>Linguistic modeling and language-processing technologies for avatar-based sign language presentation.</title>
<date>2008</date>
<journal>Universal Access in the Information Society</journal>
<pages>6--4</pages>
<contexts>
<context position="3167" citStr="Elliot &amp; Glauert, 2008" startWordPosition="490" endWordPosition="493">d such systems. Prior work can be divided into two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measure</context>
</contexts>
<marker>Elliot, Glauert, 2008</marker>
<rawString>R. Elliot, J. Glauert. 2008. Linguistic modeling and language-processing technologies for avatar-based sign language presentation. Universal Access in the Information Society 6(4):375-391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Fotinea</author>
<author>E Efthimiou</author>
<author>G Caridakis</author>
<author>K Karpouzis</author>
</authors>
<title>A knowledge-based sign synthesis architecture.</title>
<date>2008</date>
<journal>Univ. Access in Information Society</journal>
<pages>6--4</pages>
<contexts>
<context position="3189" citStr="Fotinea et al., 2008" startWordPosition="494" endWordPosition="497">rk can be divided into two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evalua</context>
</contexts>
<marker>Fotinea, Efthimiou, Caridakis, Karpouzis, 2008</marker>
<rawString>S.E. Fotinea, E. Efthimiou, G. Caridakis, K. Karpouzis. 2008. A knowledge-based sign synthesis architecture. Univ. Access in Information Society 6(4):405-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Grosjean</author>
<author>L Grosjean</author>
<author>H Lane</author>
</authors>
<title>The patterns of silence: Performance structures in sentence production.</title>
<date>1979</date>
<journal>Cognitive Psychology</journal>
<pages>11--58</pages>
<contexts>
<context position="6174" citStr="Grosjean et al., 1979" startWordPosition="953" endWordPosition="956">e Processing (NLP) researchers often apply techniques originally designed for one language to another, but research is not commonly ported to sign languages. One reason is that without a written form for ASL, NLP researchers must produce animation and thus address several issues: • Timing: An ASL performance’s speed consists of: the speed of individual sign performances, the transitional time between signs, and the insertion of pauses during signing – all of which are based on linguistic factors such as syntactic boundaries, repetition of signs in a discourse, and the part-of-speech of signs (Grosjean et al., 1979). ASL animations whose speed and pausing are incorrect are significantly less understandable to ASL signers (Huenerfauth, 2009). • Spatial Reference: Signers arrange invisible placeholders in the space around their body to represent objects or persons under discussion (Meier, 1990). To perform personal, possessive, or reflexive pronouns that refer to these entities, signers later point to these locations. Signers may not repeat the identity of these entities again; so, their conversational partner must remember where they have been placed. An ASL generator must select which entities should be </context>
<context position="32063" citStr="Grosjean et al., 1979" startWordPosition="5014" endWordPosition="5017">understandability and naturalness of ASL animations (Huenerfauth et al., 2008; Huenerfauth, 2009). To begin to address the Evaluation issue (section 1.2), we have published best-practices, survey materials, and experimental protocols for effectively evaluating ASL animation systems through the participation of native signers. We have also published baseline comprehension scores for ASL animations. We will continue to produce such resources in future work. Our earlier work on timing algorithms for ASL animations (mentioned in section 4) was based on data reported in the linguistics literature (Grosjean et al., 1979). In future work, we want to learn timing models directly from our collected corpus – to further address the Timing issue (section 1.2). To address the issues of Spatial Reference and Inflection (section 1.2), we plan on analyzing our ASL corpus to build models that can predict where in 3D space signers establish spatial reference points. Further, we will analyze our corpus to analyze how certain ASL verbs are inflected based on the 3D location of their subject and object. We want to build a parameterized lexicon of ASL verbs: given a 3D location for subject and object, we want to predict a 3D</context>
</contexts>
<marker>Grosjean, Grosjean, Lane, 1979</marker>
<rawString>F. Grosjean, L. Grosjean, H. Lane. 1979. The patterns of silence: Performance structures in sentence production. Cognitive Psychology 11:58-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huenerfauth</author>
</authors>
<title>Generating American sign language classifier predicates for English-to-ASL machine translation, dissertation,</title>
<date>2006</date>
<institution>U. of Pennsylvania.</institution>
<contexts>
<context position="3208" citStr="Huenerfauth, 2006" startWordPosition="498" endWordPosition="499"> two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evaluation questions) (Hu</context>
<context position="9185" citStr="Huenerfauth (2006)" startWordPosition="1411" endWordPosition="1412">cult to accurately: screen for native signers, prevent English environmental influences (that affect signer’s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al., 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al., 2000; Veale et al., 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and discusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2). Unfortunately, most prior work is not “datadriven,” i.e. not based on statistical modeling of corpora, the dominant successful modern NLP approach. The sign language generation research that has thus far been the most data-driven includes: • Some researchers have used motion-capture (see section 3) to build lexicons of animations of individual signs, e.g. (C</context>
<context position="23192" citStr="Huenerfauth, 2006" startWordPosition="3643" endWordPosition="3644">rm that the audio recordings were of high enough quality for research. Even when perfectly clear audio recordings of human speech are recorded in a corpus, the automatic speech synthesis models trained on this data are not perfect. Degradations in the quality of the corpus would yield even lower quality speech synthesis systems. In the same way, it is essential that we evaluate the quality of the ASL motion-capture data we are collecting. In an earlier study, we sought to collect motiondata from humans and directly produce animations from them as an “upper baseline” for an experimental study (Huenerfauth, 2006). We were not analyzing the collected data or using it for datadriven generation, we merely wanted the data to directly drive an animation of a virtual human character as a “virtual puppet.” This earlier project used a different configuration of motion-capture equipment, including an earlier version of CyberGloves® and an optical motion-capture system that required line-of-sight connections between infrared emitters on the signer’s body and cameras around the room. Unfortunately, the data collected was so poor that the animations produced from the motion-capture were not an “upper” baseline – </context>
</contexts>
<marker>Huenerfauth, 2006</marker>
<rawString>M. Huenerfauth. 2006. Generating American sign language classifier predicates for English-to-ASL machine translation, dissertation, U. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huenerfauth</author>
<author>L Zhao</author>
<author>E Gu</author>
<author>J Allbeck</author>
</authors>
<title>Evaluation of American sign language generation by native ASL signers.</title>
<date>2008</date>
<journal>ACM Trans Access Comput</journal>
<pages>1--1</pages>
<contexts>
<context position="3831" citStr="Huenerfauth et al., 2008" startWordPosition="584" endWordPosition="587">06; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al., 2008). Errors include a lack of smooth intersign transitions, lack of grammatically-required facial expressions, and inaccurate sign performances related to morphological inflection of signs. While current ASL animation systems have limitations, there are several advantages in presenting sign language content in the form of animated virtual human characters, rather than videos: • Generation or MT software planning ASL sentences cannot just concatenate videos of ASL. Using video clips, it is difficult to produce smooth transitions between signs, subtle motion variations in sign performances, or prop</context>
<context position="8792" citStr="Huenerfauth et al., 2008" startWordPosition="1352" endWordPosition="1355"> hands (Neidle et al., 2000). Animations without proper facial expressions (and proper timing relative to manual signs) cannot convey the proper meaning of ASL sentences in a fluent and understandable manner. • Evaluation: With no standard written form for ASL, string-based metrics cannot be used to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native signers, prevent English environmental influences (that affect signer’s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al., 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al., 2000; Veale et al., 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and discusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or </context>
<context position="27947" citStr="Huenerfauth et al., 2008" startWordPosition="4383" endWordPosition="4386">losest speed from our earlier study: 1.2 signs/second. (Since the scripted animations are slightly slower and include linguistic facial expressions, we expected them to receive higher understandability scores than our motioncapture animations.) In our earlier work, we produced two versions of each scripted story: one with default timing and one with our novel timing algorithm. Both versions are used as baselines for comparison in this new study; thus, we compare three versions of the same set of 10 ASL stories. Using questions designed to screen for native ASL signers developed in prior work (Huenerfauth et al., 2008), we recruited 12 participants to evaluate the ASL animations. A native ASL signer conducted the studies, in which participants viewed an animation and were then asked two types of questions after each: (1) ten-point Likert-scale questions about the ASL animation’s grammatical correctness, understandability, and naturalness of movement and (2) multiple-choice comprehension questions about basic facts from the story. The comprehension questions were presented in the form of scripted ASL animations (produced in SSS), and answer choices were presented in the form of clip-art images (so that stron</context>
<context position="31518" citStr="Huenerfauth et al., 2008" startWordPosition="4936" endWordPosition="4939">he modality-specific and spatial aspects of ASL, we can make it easier for other researchers to transfer their new NLP techniques to ASL. The goal is to “normalize” ASL in the eyes of the NLP community. Bridging NLP and ASL research will not only benefit deaf users: ASL will push the limits of current NLP techniques and will thus benefit other work in the field of NLP. Section 1.2 listed six challenges for ASL NLP research; we address several of these in our research: We have conducted many experimental studies in which signers evaluate the understandability and naturalness of ASL animations (Huenerfauth et al., 2008; Huenerfauth, 2009). To begin to address the Evaluation issue (section 1.2), we have published best-practices, survey materials, and experimental protocols for effectively evaluating ASL animation systems through the participation of native signers. We have also published baseline comprehension scores for ASL animations. We will continue to produce such resources in future work. Our earlier work on timing algorithms for ASL animations (mentioned in section 4) was based on data reported in the linguistics literature (Grosjean et al., 1979). In future work, we want to learn timing models direct</context>
</contexts>
<marker>Huenerfauth, Zhao, Gu, Allbeck, 2008</marker>
<rawString>M. Huenerfauth, L. Zhao, E. Gu, J. Allbeck. 2008. Evaluation of American sign language generation by native ASL signers. ACM Trans Access Comput 1(1):1-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huenerfauth</author>
</authors>
<title>A linguistically motivated model for speed and pausing in animations of American sign language.</title>
<date>2009</date>
<journal>ACM Trans Access Comput</journal>
<pages>2--2</pages>
<contexts>
<context position="6301" citStr="Huenerfauth, 2009" startWordPosition="974" endWordPosition="975"> ported to sign languages. One reason is that without a written form for ASL, NLP researchers must produce animation and thus address several issues: • Timing: An ASL performance’s speed consists of: the speed of individual sign performances, the transitional time between signs, and the insertion of pauses during signing – all of which are based on linguistic factors such as syntactic boundaries, repetition of signs in a discourse, and the part-of-speech of signs (Grosjean et al., 1979). ASL animations whose speed and pausing are incorrect are significantly less understandable to ASL signers (Huenerfauth, 2009). • Spatial Reference: Signers arrange invisible placeholders in the space around their body to represent objects or persons under discussion (Meier, 1990). To perform personal, possessive, or reflexive pronouns that refer to these entities, signers later point to these locations. Signers may not repeat the identity of these entities again; so, their conversational partner must remember where they have been placed. An ASL generator must select which entities should be assigned 3D locations (and where). • Inflection: Many verbs change their motion paths to indicate the 3D location where a spati</context>
<context position="22341" citStr="Huenerfauth, 2009" startWordPosition="3506" endWordPosition="3507">es between the bone lengths of the human and the “virtual skeleton” of the animated character being recorded could lead to “retargeting” errors, in which the body poses of the human do not match the recording. We must be careful in the measurement of the bone lengths of the human participant and in the design of the virtual animation skeleton. • To compensate for differences in how equipment sits on the body on different occasions or on different humans, we must set “calibration” values; e.g., we designed a novel protocol for efficiently and accurately calibrating gloves for ASL signers (Lu &amp; Huenerfauth, 2009). 4 Evaluating Our Collected Motion Data If a speech synthesis researcher were using a novel microphone technology to record audio performances from human speakers to build a corpus, that researcher would want to experimentally confirm that the audio recordings were of high enough quality for research. Even when perfectly clear audio recordings of human speech are recorded in a corpus, the automatic speech synthesis models trained on this data are not perfect. Degradations in the quality of the corpus would yield even lower quality speech synthesis systems. In the same way, it is essential tha</context>
<context position="25273" citStr="Huenerfauth, 2009" startWordPosition="3960" endWordPosition="3961">nt motion-capture configuration. As in our past study, we use the motioncapture data to directly control the body movements of a virtual human “puppet.” We then ask native ASL signers to evaluate the understandability and naturalness of the resulting animations (and compare them to some baseline animations produced using ASL-animation scripting software). 94 Figure 3: Evaluation and comprehension scores (asterisks mark significant pairwise differences). In our prior work, a native ASL signer designed a set of ASL stories and corresponding comprehension questions for use in evaluation studies (Huenerfauth, 2009). The stories’ average length is approximately 70 signs, and they consist of news stories, encyclopedia articles, and short narratives. We produced animations of each using Sign Smith Studio (SSS), commercial ASL-animation scripting software (Vcom3D, 2010). Signs from SSS’s lexicon are placed on a timeline, and linguistically appropriate facial expressions are added. The software synthesizes an animation of a virtual human performing the story (Figure 2c). In earlier work, we designed algorithms for determining sign-speed and pause-insertion in ASL animations based on linguistic features of th</context>
<context position="28751" citStr="Huenerfauth, 2009" startWordPosition="4505" endWordPosition="4506">ions after each: (1) ten-point Likert-scale questions about the ASL animation’s grammatical correctness, understandability, and naturalness of movement and (2) multiple-choice comprehension questions about basic facts from the story. The comprehension questions were presented in the form of scripted ASL animations (produced in SSS), and answer choices were presented in the form of clip-art images (so that strong English literacy was not necessary). Identical questions were 95 used to evaluate the motion-capture animations and the scripted animations. Examples of the questions are included in (Huenerfauth, 2009). Figure 3 displays results of the Likert-scale subjective questions and comprehension-question success scores for the three types of animations evaluated in this study. The scripted animations using our timing algorithm have higher comprehension scores, but the motion-capture animations have higher naturalness scores. All of the other scores for the animations are quite similar. Statistically significant differences are marked with an asterisk (p&lt;0.05, Mann-Whitney pairwise comparisons with Bonferroni-corrected p-values). Nonparameteric tests were selected because the Likertscale responses we</context>
<context position="31538" citStr="Huenerfauth, 2009" startWordPosition="4940" endWordPosition="4941">patial aspects of ASL, we can make it easier for other researchers to transfer their new NLP techniques to ASL. The goal is to “normalize” ASL in the eyes of the NLP community. Bridging NLP and ASL research will not only benefit deaf users: ASL will push the limits of current NLP techniques and will thus benefit other work in the field of NLP. Section 1.2 listed six challenges for ASL NLP research; we address several of these in our research: We have conducted many experimental studies in which signers evaluate the understandability and naturalness of ASL animations (Huenerfauth et al., 2008; Huenerfauth, 2009). To begin to address the Evaluation issue (section 1.2), we have published best-practices, survey materials, and experimental protocols for effectively evaluating ASL animation systems through the participation of native signers. We have also published baseline comprehension scores for ASL animations. We will continue to produce such resources in future work. Our earlier work on timing algorithms for ASL animations (mentioned in section 4) was based on data reported in the linguistics literature (Grosjean et al., 1979). In future work, we want to learn timing models directly from our collecte</context>
</contexts>
<marker>Huenerfauth, 2009</marker>
<rawString>M. Huenerfauth. 2009. A linguistically motivated model for speed and pausing in animations of American sign language. ACM Trans Access Comput 2(2):1-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huenerfauth</author>
<author>P Lu</author>
</authors>
<title>Annotating spatial reference in a motion-capture corpus of American sign language discourse.</title>
<date>2010</date>
<booktitle>Proc. LREC</booktitle>
<contexts>
<context position="20569" citStr="Huenerfauth &amp; Lu, 2010" startWordPosition="3218" endWordPosition="3221">Neidle et al., 2000). English influence in the studio must be minimized to prevent signers from inadvertently code-switching to an English-like form of signing. Thus, it is important that a native signer acts as the prompter, who conversationally suggests topics for the performer to discuss (to be recorded as part of the corpus). 93 In our first year, we have collected and annotated 58 passages from 6 signers (40 minutes). We prefer to collect multi-sentence passages discussing varied numbers of topics and with few “classifier predicates,” phenomena that aren’t our current research focus. In (Huenerfauth &amp; Lu, 2010), we discuss details of: the genre of discourse we record, our target linguistic phenomena to capture (spatial reference points and inflected verbs), the types of linguistic annotation added to the corpus, and the effectiveness of different “prompts” used to elicit the desired type of spontaneous discourse. This paper focuses on verifying the quality of the motion-capture data we can record using our current equipment configuration and protocols. We want to measure how well we have compensated for several possible sources of error in recordings: • If a sensor connection is temporarily lost, th</context>
</contexts>
<marker>Huenerfauth, Lu, 2010</marker>
<rawString>M. Huenerfauth, P. Lu. 2010. Annotating spatial reference in a motion-capture corpus of American sign language discourse. Proc. LREC 2010 workshop on representation &amp; processing of sign languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Karpouzis</author>
<author>G Caridakis</author>
<author>S E Fotinea</author>
<author>E Efthimiou</author>
</authors>
<title>Educational resources and implementation of a Greek sign language synthesis architecture.</title>
<date>2007</date>
<journal>Computers &amp; Education</journal>
<pages>49--1</pages>
<contexts>
<context position="3232" citStr="Karpouzis et al., 2007" startWordPosition="500" endWordPosition="503">ng and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al., 2008).</context>
</contexts>
<marker>Karpouzis, Caridakis, Fotinea, Efthimiou, 2007</marker>
<rawString>K. Karpouzis, G. Caridakis, S.E. Fotinea, E. Efthimiou. 2007. Educational resources and implementation of a Greek sign language synthesis architecture. Computers &amp; Education 49(1):54-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kennaway</author>
<author>J Glauert</author>
<author>I Zwitserlood</author>
</authors>
<title>Providing signed content on Internet by synthesized animation.</title>
<date>2007</date>
<journal>ACM Trans Comput-Hum Interact</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="2956" citStr="Kennaway et al., 2007" startWordPosition="454" endWordPosition="458"> produce an animation of a human character performing sign language. Coordinating the simultaneous 3D movements of parts of an animated character’s body is challenging, and few researchers have attempted to build such systems. Prior work can be divided into two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 W</context>
</contexts>
<marker>Kennaway, Glauert, Zwitserlood, 2007</marker>
<rawString>J. Kennaway, J. Glauert, I. Zwitserlood. 2007. Providing signed content on Internet by synthesized animation. ACM Trans Comput-Hum Interact 14(3):15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Loeding</author>
<author>S Sarkar</author>
<author>A Parashar</author>
<author>A Karshmer</author>
</authors>
<title>Progress in automated computer recognition of sign language,</title>
<date>2004</date>
<booktitle>Proc. ICCHP,</booktitle>
<pages>1079--1087</pages>
<contexts>
<context position="12526" citStr="Loeding et al., 2004" startWordPosition="1932" endWordPosition="1935">orming the sign “CAT” during time index 250-300 milliseconds, and the eyebrows are raised during time index 270-300. Such annotation is time-consuming to add; the largest ASL corpus has a few thousand sentences. In order to learn how to control the movements of an animated virtual human based on a corpus, 91 we need precise hand locations and joint angles of the human signer’s body throughout the performance. Asking humans to write down 3D angles and coordinates is time-consuming and inexact; some researchers have used computer vision techniques to model the signers’ movements (see survey in (Loeding et al., 2004)). Unfortunately, the complex shape of hands/face, rapid speed, and frequent occlusion of parts of the body during ASL limit the accuracy of vision-based recognition; it is not yet a reliable way to build a 3D model of a signer for a corpus. Motion-capture technology (discussed in section 3) is required for this level of detail. 2 Research Goals &amp; Focus of This Paper To address the lack of sufficiently detailed and linguistically annotated ASL corpora, we have begun a multi-year project to collect and annotate a motion-capture corpus of ASL (section 3). Digital 3D body movement and handshape d</context>
</contexts>
<marker>Loeding, Sarkar, Parashar, Karshmer, 2004</marker>
<rawString>B. Loeding, S. Sarkar, A. Parashar, A. Karshmer. 2004. Progress in automated computer recognition of sign language, Proc. ICCHP, 1079-1087.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lu</author>
<author>M Huenerfauth</author>
</authors>
<title>Accessible motioncapture glove calibration protocol for recording sign language data from deaf subjects.</title>
<date>2009</date>
<booktitle>Proc. ASSETS.</booktitle>
<contexts>
<context position="22341" citStr="Lu &amp; Huenerfauth, 2009" startWordPosition="3504" endWordPosition="3507">erences between the bone lengths of the human and the “virtual skeleton” of the animated character being recorded could lead to “retargeting” errors, in which the body poses of the human do not match the recording. We must be careful in the measurement of the bone lengths of the human participant and in the design of the virtual animation skeleton. • To compensate for differences in how equipment sits on the body on different occasions or on different humans, we must set “calibration” values; e.g., we designed a novel protocol for efficiently and accurately calibrating gloves for ASL signers (Lu &amp; Huenerfauth, 2009). 4 Evaluating Our Collected Motion Data If a speech synthesis researcher were using a novel microphone technology to record audio performances from human speakers to build a corpus, that researcher would want to experimentally confirm that the audio recordings were of high enough quality for research. Even when perfectly clear audio recordings of human speech are recorded in a corpus, the automatic speech synthesis models trained on this data are not perfect. Degradations in the quality of the corpus would yield even lower quality speech synthesis systems. In the same way, it is essential tha</context>
</contexts>
<marker>Lu, Huenerfauth, 2009</marker>
<rawString>P. Lu, M. Huenerfauth. 2009. Accessible motioncapture glove calibration protocol for recording sign language data from deaf subjects. Proc. ASSETS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Marshall</author>
<author>E Safar</author>
</authors>
<title>Grammar development for sign language avatar-based synthesis.</title>
<date>2005</date>
<booktitle>Proc. UAHCI.</booktitle>
<contexts>
<context position="9089" citStr="Marshall and Safar (2005)" startWordPosition="1397" endWordPosition="1400"> to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native signers, prevent English environmental influences (that affect signer’s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al., 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al., 2000; Veale et al., 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and discusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2). Unfortunately, most prior work is not “datadriven,” i.e. not based on statistical modeling of corpora, the dominant successful modern NLP approach. The sign language generation research that has thus far been the most data-driven includes: • Some researchers have </context>
<context position="3256" citStr="Marshall &amp; Safar, 2005" startWordPosition="504" endWordPosition="507">ation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al., 2008). Errors include a lack o</context>
</contexts>
<marker>Marshall, Safar, 2005</marker>
<rawString>I. Marshall, E. Safar. 2005. Grammar development for sign language avatar-based synthesis. Proc. UAHCI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Meier</author>
</authors>
<title>Person deixis in American sign language.</title>
<date>1990</date>
<booktitle>Theoretical issues in sign language research,</booktitle>
<volume>1</volume>
<pages>175--190</pages>
<editor>In: S. Fischer &amp; P. Siple (eds.),</editor>
<publisher>Linguistics. Chicago: University of Chicago Press,</publisher>
<contexts>
<context position="6456" citStr="Meier, 1990" startWordPosition="996" endWordPosition="997">n ASL performance’s speed consists of: the speed of individual sign performances, the transitional time between signs, and the insertion of pauses during signing – all of which are based on linguistic factors such as syntactic boundaries, repetition of signs in a discourse, and the part-of-speech of signs (Grosjean et al., 1979). ASL animations whose speed and pausing are incorrect are significantly less understandable to ASL signers (Huenerfauth, 2009). • Spatial Reference: Signers arrange invisible placeholders in the space around their body to represent objects or persons under discussion (Meier, 1990). To perform personal, possessive, or reflexive pronouns that refer to these entities, signers later point to these locations. Signers may not repeat the identity of these entities again; so, their conversational partner must remember where they have been placed. An ASL generator must select which entities should be assigned 3D locations (and where). • Inflection: Many verbs change their motion paths to indicate the 3D location where a spatial reference point has been established for their subject, object, or both (Padden, 1988). Generally, the motion paths of these inflecting verbs change so </context>
</contexts>
<marker>Meier, 1990</marker>
<rawString>R. Meier. 1990. Person deixis in American sign language. In: S. Fischer &amp; P. Siple (eds.), Theoretical issues in sign language research, vol. 1: Linguistics. Chicago: University of Chicago Press, 175-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitchell</author>
<author>T Young</author>
<author>B Bachleda</author>
<author>M Karchmer</author>
</authors>
<date>2006</date>
<booktitle>How many people use ASL in the United States? Sign Language Studies</booktitle>
<pages>6--3</pages>
<contexts>
<context position="1235" citStr="Mitchell et al., 2006" startWordPosition="179" endWordPosition="182">current ASL systems is limited; they have been constructed without the benefit of annotated ASL corpora that encode detailed human movement. We discuss how linguistic challenges in ASL generation can be addressed in a data-driven manner, and we describe our current work on collecting a motion-capture corpus. To evaluate the quality of our motion-capture configuration, calibration, and recording protocol, we conducted an evaluation study with native ASL signers. 1 Introduction American Sign Language (ASL) is the primary means of communication for about one-half million deaf people in the U.S. (Mitchell et al., 2006). ASL has a distinct word-order, syntax, and lexicon from English; it is not a representation of English using the hands. Although reading is part of the curriculum for deaf students, lack of auditory exposure to English during the language-acquisition years of childhood leads to lower literacy for many adults. In fact, the majority of deaf high school graduates in the U.S. have only a fourth-grade (age 10) English reading level (Traxler, 2000). 1.1 Applications of ASL Generation Research Most technology used by the deaf does not address this literacy issue; many deaf people find it diffi89 cu</context>
</contexts>
<marker>Mitchell, Young, Bachleda, Karchmer, 2006</marker>
<rawString>R. Mitchell, T. Young, B. Bachleda, M. Karchmer. 2006. How many people use ASL in the United States? Sign Language Studies 6(3):306-335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Morrissey</author>
<author>A Way</author>
</authors>
<title>An example-based approach to translating sign language.</title>
<date>2005</date>
<booktitle>Proc. Workshop on Example-Based Machine Translation,</booktitle>
<pages>109--116</pages>
<contexts>
<context position="10322" citStr="Morrissey and Way (2005)" startWordPosition="1582" endWordPosition="1585">on-capture (see section 3) to build lexicons of animations of individual signs, e.g. (Cox et al., 2002). However, their focus is recording a single citation form of each sign, not creating annotated corpora of full sentences or discourse. Singlesign recordings do not enable researchers to examine the Timing, Coarticulation, Spatial Reference, Non-Manuals, or Inflection phenomena (section 1.2), which operate over multiple signs or sentences in an ASL discourse. • Other researchers have examined how statistical MT techniques could be used to translate from a written language to a sign language. Morrissey and Way (2005) discuss an example-based MT architecture for Irish Sign Language, and Stein et al. (2006) apply simple statistical MT approaches to German Sign Language. Unfortunately, the sign language “corpora” used in these studies consist of transcriptions of the sequence of signs performed, not recordings of actual human performances. A transcription does not capture subtleties in the 3D movements of the hands, facial movements, or speed of an ASL performance. Such information is needed in order to address the Spatial Reference, Inflection, Coarticulation, Timing, or Non-Manuals issues (section 1.2). • </context>
</contexts>
<marker>Morrissey, Way, 2005</marker>
<rawString>S. Morrissey, A. Way. 2005. An example-based approach to translating sign language. Proc. Workshop on Example-Based Machine Translation, 109-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neidle</author>
<author>D Kegl</author>
<author>D MacLaughlin</author>
<author>B Bahan</author>
<author>R G Lee</author>
</authors>
<title>The syntax of ASL: functional categories and hierarchical structure.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="8195" citStr="Neidle et al., 2000" startWordPosition="1268" endWordPosition="1271">mes (person on right), (b.) (person on right) blames (person on left). 90 • Coarticulation: As in speech production, the surrounding signs in a sentence affect finger, hand, and body movements. ASL generators that use overly simple interpolation rules to produce these coarticulation effects yield unnatural and non-fluent ASL animation output. • Non-Manuals: Head-tilt and eye-gaze indicate the 3D location of a verb’s subject and object (or other information); facial expressions also indicate negation, questions, topicalization, and other essential syntactic phenomena not conveyed by the hands (Neidle et al., 2000). Animations without proper facial expressions (and proper timing relative to manual signs) cannot convey the proper meaning of ASL sentences in a fluent and understandable manner. • Evaluation: With no standard written form for ASL, string-based metrics cannot be used to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native signers, prevent English environmental influences (that affect signer’s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al., 2008). 1</context>
<context position="11688" citStr="Neidle et al., 2000" startWordPosition="1796" endWordPosition="1799">cation from videos of signing. 1.4 Prior Sign Language Corpora Resources The reason why most prior ASL generation research has not been data-driven is that sufficiently detailed and annotated sign language corpora are in short supply and are time-consuming to construct. Without a writing system in common use, it is not possible to harvest some naturally arising source of ASL “text”; instead, it is necessary to record the performance of a signer (through video or a motion-capture suit). Human signers must then transcribe and annotate this data by adding timestamped linguistic details. For ASL (Neidle et al., 2000) and European sign languages (Bungeroth et al., 2006; Crasborn et al., 2004, 2006; Efthimiou &amp; Fotinea, 2007), signers have been videotaped and experts marked time spans when events occur – e.g. the right hand is performing the sign “CAT” during time index 250-300 milliseconds, and the eyebrows are raised during time index 270-300. Such annotation is time-consuming to add; the largest ASL corpus has a few thousand sentences. In order to learn how to control the movements of an animated virtual human based on a corpus, 91 we need precise hand locations and joint angles of the human signer’s bod</context>
<context position="18740" citStr="Neidle et al., 2000" startWordPosition="2932" endWordPosition="2935">190 system to accurately calculate joint angles from the sensor data. Motion-capture recording sessions are videotaped to facilitate later linguistic analysis and annotation. Videotaping the session also facilitates the “clean up” of the motion-capture data in postprocessing, during which algorithms are applied to adjust synchronization of different sensors or remove “jitter” or other noise artifacts from the recording. Three digital high-speed video cameras film front view, facial close-up, and side views of the signer – a setup that has been used in videobased ASL-corpora-building projects (Neidle et al., 2000). The front view is similar to Figure 2a (but wider). The facial close-up view is useful when later identifying specific non-manual facial expressions during ASL performances, which are essential to correctly understanding and annotating the collected data. To facilitate synchronizing the three video files during post-processing, a strobe is flashed once at the start of the recording session. A “blue screen” curtain hangs on the back and side walls of the motion-capture studio. If future computer vision researchers wish to use this corpus to study ASL recognition from video, it is useful to ha</context>
<context position="19966" citStr="Neidle et al., 2000" startWordPosition="3119" endWordPosition="3122">id color walls for “chroma key” background removal. Photographic studio lighting with spectra compatible with the eye-tracking system is used to support high-quality video recording. During data collection, a native ASL signer (called the “prompter”) sits directly behind the front-view camera to engage the participant wearing the suit (the “performer”) in natural conversation. While the corpus we are collecting consists of unscripted single-signer discourse, prior ASL corpora projects have identified the importance of surrounding signers with an ASL-centric environment during data collection (Neidle et al., 2000). English influence in the studio must be minimized to prevent signers from inadvertently code-switching to an English-like form of signing. Thus, it is important that a native signer acts as the prompter, who conversationally suggests topics for the performer to discuss (to be recorded as part of the corpus). 93 In our first year, we have collected and annotated 58 passages from 6 signers (40 minutes). We prefer to collect multi-sentence passages discussing varied numbers of topics and with few “classifier predicates,” phenomena that aren’t our current research focus. In (Huenerfauth &amp; Lu, 20</context>
</contexts>
<marker>Neidle, Kegl, MacLaughlin, Bahan, Lee, 2000</marker>
<rawString>C. Neidle, D. Kegl, D. MacLaughlin, B. Bahan, &amp; R.G. Lee. 2000. The syntax of ASL: functional categories and hierarchical structure. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Newkirk</author>
</authors>
<title>SignFont Handbook. San Diego: Emerson and Associates.</title>
<date>1987</date>
<contexts>
<context position="2200" citStr="Newkirk, 1987" startWordPosition="337" endWordPosition="338">ol graduates in the U.S. have only a fourth-grade (age 10) English reading level (Traxler, 2000). 1.1 Applications of ASL Generation Research Most technology used by the deaf does not address this literacy issue; many deaf people find it diffi89 cult to read the English text on a computer screen or on a television with closed-captioning. Software to present information in the form of animations of ASL could make information and services more accessible to deaf users, by displaying an animated character performing ASL, rather than English text. While writing systems for ASL have been proposed (Newkirk, 1987; Sutton, 1998), none is widely used in the Deaf community. Thus, an ASL generation system cannot produce text output; the system must produce an animation of a human character performing sign language. Coordinating the simultaneous 3D movements of parts of an animated character’s body is challenging, and few researchers have attempted to build such systems. Prior work can be divided into two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expre</context>
</contexts>
<marker>Newkirk, 1987</marker>
<rawString>D. Newkirk. 1987. SignFont Handbook. San Diego: Emerson and Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Padden</author>
</authors>
<title>Interaction of morphology &amp; syntax in American sign language. Outstanding dissertations in linguistics, series IV.</title>
<date>1988</date>
<publisher>Garland Press.</publisher>
<location>New York:</location>
<contexts>
<context position="6990" citStr="Padden, 1988" startWordPosition="1081" endWordPosition="1082">round their body to represent objects or persons under discussion (Meier, 1990). To perform personal, possessive, or reflexive pronouns that refer to these entities, signers later point to these locations. Signers may not repeat the identity of these entities again; so, their conversational partner must remember where they have been placed. An ASL generator must select which entities should be assigned 3D locations (and where). • Inflection: Many verbs change their motion paths to indicate the 3D location where a spatial reference point has been established for their subject, object, or both (Padden, 1988). Generally, the motion paths of these inflecting verbs change so that their direction goes from the subject to the object (Figure 1); however, their paths are more complex than this. Each verb has a standard motion path that is affected by the subject’s and the object’s 3D locations. When a verb is inflected in this way, the signer does not need to overtly state the subject/object of a sentence. An ASL generator must produce appropriately inflected verb paths based on the layout of the spatial reference points. Figure 1: An ASL inflecting verb “BLAME”: (a.) (person on left) blames (person on </context>
</contexts>
<marker>Padden, 1988</marker>
<rawString>C. Padden. 1988. Interaction of morphology &amp; syntax in American sign language. Outstanding dissertations in linguistics, series IV. New York: Garland Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Segouat</author>
<author>A Braffort</author>
</authors>
<title>Toward the study of sign language coarticulation: methodology proposal.</title>
<date>2009</date>
<booktitle>Proc Advances in Comput.-Human Interactions,</booktitle>
<pages>369--374</pages>
<marker>Segouat, Braffort, 2009</marker>
<rawString>J. Segouat, A. Braffort. 2009. Toward the study of sign language coarticulation: methodology proposal. Proc Advances in Comput.-Human Interactions, 369-374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Shionome</author>
<author>K Kamata</author>
<author>H Yamamoto</author>
<author>S Fischer</author>
</authors>
<title>Effects of display size on perception of Japanese sign language---Mobile access in signed language.</title>
<date>2005</date>
<booktitle>Proc. Human-Computer Interaction,</booktitle>
<pages>22--27</pages>
<contexts>
<context position="3279" citStr="Shionome et al., 2005" startWordPosition="508" endWordPosition="511"> allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al., 2008). Errors include a lack of smooth intersign tran</context>
</contexts>
<marker>Shionome, Kamata, Yamamoto, Fischer, 2005</marker>
<rawString>T. Shionome, K. Kamata, H. Yamamoto, S. Fischer. 2005. Effects of display size on perception of Japanese sign language---Mobile access in signed language. Proc. Human-Computer Interaction, 22-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Stein</author>
<author>J Bungeroth</author>
<author>H Ney</author>
</authors>
<title>Morpho-syntax based statistical methods for sign language translation.</title>
<date>2006</date>
<booktitle>Proc. European Association for MT,</booktitle>
<pages>169--177</pages>
<contexts>
<context position="10412" citStr="Stein et al. (2006)" startWordPosition="1598" endWordPosition="1601">., 2002). However, their focus is recording a single citation form of each sign, not creating annotated corpora of full sentences or discourse. Singlesign recordings do not enable researchers to examine the Timing, Coarticulation, Spatial Reference, Non-Manuals, or Inflection phenomena (section 1.2), which operate over multiple signs or sentences in an ASL discourse. • Other researchers have examined how statistical MT techniques could be used to translate from a written language to a sign language. Morrissey and Way (2005) discuss an example-based MT architecture for Irish Sign Language, and Stein et al. (2006) apply simple statistical MT approaches to German Sign Language. Unfortunately, the sign language “corpora” used in these studies consist of transcriptions of the sequence of signs performed, not recordings of actual human performances. A transcription does not capture subtleties in the 3D movements of the hands, facial movements, or speed of an ASL performance. Such information is needed in order to address the Spatial Reference, Inflection, Coarticulation, Timing, or Non-Manuals issues (section 1.2). • Seguoat and Braffort (2009) derive models of coarticulation for French Sign Language based</context>
</contexts>
<marker>Stein, Bungeroth, Ney, 2006</marker>
<rawString>D. Stein, J. Bungeroth, H. Ney. 2006. Morpho-syntax based statistical methods for sign language translation. Proc. European Association for MT, 169-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sumihiro</author>
<author>S Yoshihisa</author>
<author>K Takao</author>
</authors>
<title>Synthesis of sign animation with facial expression and its effects on understanding of sign language.</title>
<date>2000</date>
<tech>IEIC Technical Report 100(331):31-36.</tech>
<contexts>
<context position="3302" citStr="Sumihiro et al., 2000" startWordPosition="512" endWordPosition="515">s sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSIGN project created tools for content developers to build sign databases and assemble scripts of signing for web pages (Kennaway et al., 2007). Sign Smith Studio (Vcom3D, 2010) is a commercial tool for scripting ASL (discussed in section 4). Others study generation or machine translation (MT) of sign language (Chiu et al., 2007; Elliot &amp; Glauert, 2008; Fotinea et al., 2008; Huenerfauth, 2006; Karpouzis et al., 2007; Marshall &amp; Safar, 2005; Shionome et al., 2005; Sumihiro et al., 2000; van Zijl &amp; Barker, 2003). Experimental evaluations of the understandability of state-of-the-art ASL animation systems have shown that native signers often find animations difficult to understand (as measured by compreProceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 89–97, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics hension questions) or unnatural (as measured by subjective evaluation questions) (Huenerfauth et al., 2008). Errors include a lack of smooth intersign transitions, lack of gramma</context>
</contexts>
<marker>Sumihiro, Yoshihisa, Takao, 2000</marker>
<rawString>K. Sumihiro, S. Yoshihisa, K. Takao. 2000. Synthesis of sign animation with facial expression and its effects on understanding of sign language. IEIC Technical Report 100(331):31-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sutton</author>
</authors>
<title>The Signwriting Literacy Project.</title>
<date>1998</date>
<booktitle>In Impact of Deafness on Cognition AERA Conference.</booktitle>
<contexts>
<context position="2215" citStr="Sutton, 1998" startWordPosition="339" endWordPosition="340"> the U.S. have only a fourth-grade (age 10) English reading level (Traxler, 2000). 1.1 Applications of ASL Generation Research Most technology used by the deaf does not address this literacy issue; many deaf people find it diffi89 cult to read the English text on a computer screen or on a television with closed-captioning. Software to present information in the form of animations of ASL could make information and services more accessible to deaf users, by displaying an animated character performing ASL, rather than English text. While writing systems for ASL have been proposed (Newkirk, 1987; Sutton, 1998), none is widely used in the Deaf community. Thus, an ASL generation system cannot produce text output; the system must produce an animation of a human character performing sign language. Coordinating the simultaneous 3D movements of parts of an animated character’s body is challenging, and few researchers have attempted to build such systems. Prior work can be divided into two areas: scripting and generation/translation. Scripting systems allow someone who knows sign language to “word process” an animation by assembling a sequence of signs from a lexicon and adding facial expressions. The eSI</context>
</contexts>
<marker>Sutton, 1998</marker>
<rawString>V. Sutton. 1998. The Signwriting Literacy Project. In Impact of Deafness on Cognition AERA Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Traxler</author>
</authors>
<title>The Stanford achievement test, ninth edition: national norming and performance standards for deaf and hard-of-hearing students.</title>
<date>2000</date>
<journal>J. Deaf Studies and Deaf Education</journal>
<pages>5--4</pages>
<contexts>
<context position="1683" citStr="Traxler, 2000" startWordPosition="254" endWordPosition="255"> signers. 1 Introduction American Sign Language (ASL) is the primary means of communication for about one-half million deaf people in the U.S. (Mitchell et al., 2006). ASL has a distinct word-order, syntax, and lexicon from English; it is not a representation of English using the hands. Although reading is part of the curriculum for deaf students, lack of auditory exposure to English during the language-acquisition years of childhood leads to lower literacy for many adults. In fact, the majority of deaf high school graduates in the U.S. have only a fourth-grade (age 10) English reading level (Traxler, 2000). 1.1 Applications of ASL Generation Research Most technology used by the deaf does not address this literacy issue; many deaf people find it diffi89 cult to read the English text on a computer screen or on a television with closed-captioning. Software to present information in the form of animations of ASL could make information and services more accessible to deaf users, by displaying an animated character performing ASL, rather than English text. While writing systems for ASL have been proposed (Newkirk, 1987; Sutton, 1998), none is widely used in the Deaf community. Thus, an ASL generation</context>
</contexts>
<marker>Traxler, 2000</marker>
<rawString>C. Traxler. 2000. The Stanford achievement test, ninth edition: national norming and performance standards for deaf and hard-of-hearing students. J. Deaf Studies and Deaf Education 5(4):337-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van Zijl</author>
<author>D Barker</author>
</authors>
<date>2003</date>
<booktitle>South African sign language MT system. Proc. AFRIGRAPH,</booktitle>
<pages>49--52</pages>
<marker>van Zijl, Barker, 2003</marker>
<rawString>L. van Zijl, D. Barker. 2003. South African sign language MT system. Proc. AFRIGRAPH, 49-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>VCom3D</author>
</authors>
<title>Sign Smith Studio.</title>
<date>2010</date>
<note>http://www.vcom3d.com/signsmith.php</note>
<marker>VCom3D, 2010</marker>
<rawString>VCom3D. 2010. Sign Smith Studio. http://www.vcom3d.com/signsmith.php</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Veale</author>
<author>A Conway</author>
<author>B Collins</author>
</authors>
<title>Challenges of cross-modal translation: English to sign translation in ZARDOZ system.</title>
<date>1998</date>
<journal>Machine Translation</journal>
<pages>13--81</pages>
<contexts>
<context position="9002" citStr="Veale et al., 1998" startWordPosition="1385" endWordPosition="1388">ation: With no standard written form for ASL, string-based metrics cannot be used to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native signers, prevent English environmental influences (that affect signer’s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al., 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al., 2000; Veale et al., 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and discusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2). Unfortunately, most prior work is not “datadriven,” i.e. not based on statistical modeling of corpora, the dominant successful modern NLP approach. The sign language generation </context>
</contexts>
<marker>Veale, Conway, Collins, 1998</marker>
<rawString>T. Veale, A. Conway, B. Collins. 1998. Challenges of cross-modal translation: English to sign translation in ZARDOZ system. Machine Translation 13:81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhao</author>
<author>K Kipper</author>
<author>W Schuler</author>
<author>C Vogler</author>
<author>N Badler</author>
<author>M Palmer</author>
</authors>
<title>A machine translation system from English to American sign language.</title>
<date>2000</date>
<booktitle>Proc. AMTA.</booktitle>
<contexts>
<context position="8981" citStr="Zhao et al., 2000" startWordPosition="1381" endWordPosition="1384">ble manner. • Evaluation: With no standard written form for ASL, string-based metrics cannot be used to evaluate ASL generation output automatically. User-based experiments are necessary, but it is difficult to accurately: screen for native signers, prevent English environmental influences (that affect signer’s linguistic judgments), and design questions that measure comprehension of ASL animations (Huenerfauth et al., 2008). 1.3 Need for Data-Driven ASL Generation Due to these challenges, most prior sign language generation or MT projects have been short-lived, producing few example outputs (Zhao et al., 2000; Veale et al., 1998). Further developed systems also have limited coverage; e.g., Marshall and Safar (2005) hand-built translation transfer rules from English to British Sign Language. Huenerfauth (2006) surveys several rule-based systems and discusses how they generally: have limited coverage; often merely concatenate signs; and do not address the Coarticulation, Spatial Reference, Timing, Non-Manuals, or Inflection issues (section 1.2). Unfortunately, most prior work is not “datadriven,” i.e. not based on statistical modeling of corpora, the dominant successful modern NLP approach. The sign</context>
</contexts>
<marker>Zhao, Kipper, Schuler, Vogler, Badler, Palmer, 2000</marker>
<rawString>L. Zhao, K. Kipper, W. Schuler, C. Vogler, N. Badler, M. Palmer. 2000. A machine translation system from English to American sign language. Proc. AMTA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>