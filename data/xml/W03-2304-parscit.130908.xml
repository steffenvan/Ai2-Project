<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019704">
<title confidence="0.913084">
Learning to Order Facts for Discourse Planning in Natural Language
Generation
</title>
<author confidence="0.723255">
Aggeliki Dimitromanolaki Ion Androutsopoulos
</author>
<affiliation confidence="0.7724345">
Department of Information &amp; Department of Informatics
Communication Systems Engineering Athens University of Economics &amp;
University of the Aegean Business
Institute of Informatics &amp; Patission 76, 10434, Athens, Greece
</affiliation>
<address confidence="0.647348333333333">
Telecommunications ion@aueb.gr
NCSR &amp;quot;Demokritos&amp;quot;
15310, Ag.Paraskeui, Greece
</address>
<email confidence="0.986958">
adimit@iit.demokritos.gr
</email>
<sectionHeader confidence="0.99468" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965277777778">
This paper presents a machine learning
approach to discourse planning in natu-
ral language generation. More specifi-
cally, we address the problem of
learning the most natural ordering of
facts in discourse plans for a specific
domain. We discuss our methodology
and how it was instantiated using two
different machine learning algorithms.
A quantitative evaluation performed in
the domain of museum exhibit descrip-
tions indicates that our approach per-
forms significantly better than
manually constructed ordering rules.
Being retrainable, the resulting plan-
ners can be ported easily to other simi-
lar domains, without requiring
language technology expertise.
</bodyText>
<sectionHeader confidence="0.999034" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999892914285714">
Along the lines of Reiter and Dale (2000), we
view natural language generation (NLG) as
consisting of six tasks: content determination,
discourse planning, aggregation, lexicalization,
referring expression generation, and linguistic
realization. This paper is concerned with the
second task, i.e., discourse planning. Dis-
course planning determines the ordering and
rhetorical relations of the logical messages,
hereafter called facts, that the generated docu-
ment is intended to convey. Most existing ap-
proaches to discourse planning are based on
either rhetorical structure theory (RST) (Mann
and Thompson, 1988; Hovy, 1993) or sche-
mata (McKeown, 1985). In both cases, the
rules that determine the order and the rhetori-
cal relations are typically written by hand. This
is a time-consuming process, which requires
domain and linguistic expertise, and has to be
repeated whenever the system is ported to a
new domain; see also Rambow (1990).
This paper presents a machine learning
(ML) approach to the subtask of discourse
planning that attempts to find the most natural
ordering of facts in each generated document.
Our approach was motivated by experience
obtained from the M-PIRO project (Androut-
sopoulos et al., 2001). Building upon ILEX
(O&apos;Donnell et al., 2001), M-PIRO is develop-
ing technology that allows personalized de-
scriptions of museum exhibits to be generated
in several languages, starting from symbolic,
language-independent information stored in a
database, and small fragments of text (Isard et
al., 2003). One of M-PIRO&apos;s most ambitious
</bodyText>
<page confidence="0.996428">
23
</page>
<bodyText confidence="0.9999895">
goals is to develop authoring tools that will
allow domain experts, e.g., museum curators,
with no language technology expertise to con-
figure the system for new application domains.
While this goal has largely been achieved for
resources such as the domain-dependent parts
of the ontology, or domain-dependent settings
that affect content selection, lexicalization, and
referring expression generation (Androut-
sopoulos et al., 2002), designing tools that will
allow domain experts to edit discourse plan-
ning rules has proven difficult. In contrast,
domain experts, in our case museum curators,
were happy to reorder the clauses of sample
generated texts, thus indicating the preferred
orderings of the facts in the corresponding dis-
course plans. We have, therefore, opted for a
machine learning approach that allows fact-
ordering rules to be captured automatically
from sets of manually reordered facts. We
view this approach as a first step towards
learning richer discourse plans, which apart
from ordering information will also include
rhetorical relations, although the experience
from M-PIRO indicates that even just ordering
the facts in a natural way can lead to quite ac-
ceptable texts. Being automatically retrainable,
the planners that our approach produces can be
easily ported to other similar domains, e.g.,
descriptions of products for e-commerce cata-
logues, provided that samples of ideal fact or-
derings can be made available.
Our method introduces a new representation
of the fact-ordering task, and employs super-
vised learning algorithms. It is assumed that
the number of facts to be conveyed by each
generated document, in effect the desired
length of the generated texts, has been fixed to
a particular value; i.e., all the documents con-
tain the same number of facts. In ILEX and M-
PIRO, this number is provided by the user
model. Furthermore, it is assumed that a con-
tent determination module is available, which
selects the particular facts to be conveyed by
each document. Our method consists of a se-
quence of stages, the number of stages being
equal to the number of facts to be conveyed by
each document. Each stage is responsible for
the selection of the fact to be placed at the
corresponding position in the resulting
document. In our experiments, we set the
number of facts per document to six, which
per document to six, which seems to be an ap-
propriate value for our particular domain and
an average adult user, but this number could
vary depending on the application and user
type. Two learning algorithms, decision trees
(Quinlan, 1993) and instance-based learning
(Aha and Kibler, 1991), were explored. The
results are compared against two baselines: a
simple hand-crafted planner, which always
assigns a predefined order, and the majority
scheme. The latter selects, among the facts that
are available at each position, the fact that oc-
curred most frequently at that position in the
training data. Overall, the results indicate that
with either of the two learning algorithms our
method significantly outperforms both of the
baselines, and that there is no significant dif-
ference in the performance of the two learning
algorithms.
The remainder of this paper is organized as
follows. Section 2 presents previous learning
approaches to NLG, and discusses their rele-
vance to the work presented here. Section 3
describes our learning approach, including is-
sues such as data representation and system
architecture. Section 4 discusses our experi-
ments and their results. Section 5 concludes
and highlights plans for future work.
</bodyText>
<sectionHeader confidence="0.960457" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999873">
In recent years, ML approaches have been
introduced to NLG to address problems such
as the construction and maintenance of domain
and language resources, which is a time-
consuming process in systems that use hand-
crafted rules.&apos; To the best of our knowledge,
only two of these approaches (Duboue and
McKeown, 2001; Duboue and McKeown,
2002) consider discourse planning.
Duboue and McKeown (2001) present an
unsupervised ML algorithm based on pattern
matching and clustering, which is used to learn
ordering constraints among facts. The same
authors have also used evolutionary algorithms
to learn the tree representation of a planner
(Duboue and McKeown, 2002). These works
are similar to ours in that we also address the
problem of ordering facts. However, Duboue
</bodyText>
<footnote confidence="0.618615">
I For an extensive bibliography on statistical and machine learning ap-
proaches to NLG, see:
http://www.iit.demokritos.gr/—adimitibibliographv.html.
</footnote>
<page confidence="0.998944">
24
</page>
<bodyText confidence="0.999952272727273">
and McKeown follow the lines of schema-
based planning, where content determination is
not an independent stage, but is interleaved
with discourse planning. This means that the
discourse planner has the overall control of
content determination, and cannot handle in-
puts from an independent content determina-
tion module. In contrast, our method can be
used with any content determination mecha-
nism that returns a fixed number of facts. This
has the benefit that alternative content deter-
mination modules can be used without affect-
ing the discourse planner. Moreover, while
Duboue and McKeown (2002) learn a tree
structure representing the best sequence of
facts, our method directly manipulates facts.
Mellish et al. (1998) also experiment with
genetic algorithms to find the optimal RST
tree, which is then mapped to the correspond-
ing sequence of facts. Karamanis and Manu-
rung (2002) use a similar approach that
employs constraints from Centering Theory in
the genetic search. However, these approaches
do not involve any learning: the genetic search
is repeated every time the text planner is in-
voked, i.e., for each new document. In con-
trast, our method induces a single discourse
planner from the training data, which is then
used to order any set of facts provided by the
content determinator.
ML approaches to NLG have also been used
in syntactic and lexical realization (Langkilde
and Knight, 1998; Bangalore and Rambow,
2000; Ratnaparkhi, 2000; Varges and Mellish,
2001; Shaw and Hatzivassiloglou, 1999; Ma-
louf 2000), as well as in sentence planning
tasks (Walker et al., 2001; Poesio et al., 2000).
In the context of spoken dialogue systems,
learning techniques have been used to select
among different templates (Oh and Rudnicky,
2000; Walker, 2000). These approaches, how-
ever, are not directly relevant to discourse
planning
The problem of ordering semantic units has
also been addressed in the context of summari-
zation. Kan and McKeown (2002) use an n-
gram model to infer ordering constraints be-
tween facts, while Barzilay et al. (2002) manu-
ally identify constraints on ordering, using a
corpus of ordering preferences among subjects
and clustering techniques that identify corn-
monalities among these preferences. The ap-
proach presented here, instead of identifying
ordering constraints, &amp;quot;learns&amp;quot; the overall order-
ing of the input facts.
</bodyText>
<sectionHeader confidence="0.838985" genericHeader="method">
3 Learning to order facts
</sectionHeader>
<bodyText confidence="0.999955181818182">
In our approach, the discourse planner is
trained on manually ordered sequences of facts
of a fixed length. Once trained, it is able to
determine what it considers to be the most
natural ordering of any set of facts, as output
by a content determination module, provided
that the cardinality of the set is the same as the
length of the training sequences. This section
describes our approach in more detail, starting
from the required data and the pre-processing
that they undergo.
</bodyText>
<subsectionHeader confidence="0.995832">
3.1 Data and pre-processing
</subsectionHeader>
<bodyText confidence="0.999993208333334">
Our data was derived from the database of M-
PIRO. This database currently contains infor-
mation about 50 museum exhibits, each of
which is associated with a large number of
facts. For example, the left column of Table 1
shows the database facts associated with the
entity exhibit9. Each generated document is
intended to describe a museum exhibit. As al-
ready mentioned, in our experiments the num-
ber of facts to be conveyed by each document
was set to six. That is, when asked to describe
exhibit9, the content determination module
would choose six of the facts in the left column
of Table 1, possibly depending on user model-
ing information, such as the interests and
backgrounds of the users, or information indi-
cating which facts have already been conveyed
to the users. We did not use a particular con-
tent determination module, because we wanted
the discourse planner to be independent from
the content determination process. Our goal
was to be able to order any set of six facts that
could be provided as input by an arbitrary con-
tent determination module.
</bodyText>
<page confidence="0.996031">
25
</page>
<table confidence="0.9642014375">
Database facts Selected facts (input to discourse planner)
subclass(EXHIBIT9,RHYTON) fl: subclass(EXHIBIT9,RHYTON)
cun-ent-location(EXHIBIT9,MUS-DU-PETIT-PALAIS) f2: current-location(EXHIBIT9,MUS-DUPETITPALAIS)
original-location(EXH1B1T9,ATT1CA) f3: original-location(EXHIBIT9,ATTICA)
potter-is(EXHIBIT9,SOTADES) f4: painted-by(EXHIBIT9,PAINTER-OF-SOTADES)
exhibit-characteristics(EXHIBIT9,ENTITY-1796) f
painted-by(EXH1B1T9,PAINTER-OF-SOTADES) 5: creation-time(EXHIBIT9,DATE-1767)
creation-time(EXHIBIT9,DATE-1767) f6: &apos; creation-period(EXHIBIT9,CLASSICAL-PERIOD)
creation-period(EXHIBIT9,CLASSICAL-PERIOD)
painting-technique-used(EXHIBIT9,RED-FIG-TECHN)
exhibit-depicts(EXHIBIT9,ENTITY-1786)
opposite-technique(RED-FIG-TECHN,BLACK-FIG-TEC)
technique-description(RED-FIG-TECHN,ENTITY-2474)
person-information(SOTADES,ENTITY-2739)
museum-country(MUS-DU-PETIT-PALAIS,FRANCE)
period-story(CLASSICAL-PER10D,STORY-NODE4019)
</table>
<tableCaption confidence="0.998059">
Table 1: Database facts and facts selected as input to the discourse planner
</tableCaption>
<figureCaption confidence="0.996237">
Figure 1: Architecture diagram
</figureCaption>
<figure confidence="0.875269481481481">
1st-classifier 1st fact = subclass
&lt;sub class : 0,current-location : 1,original-location: 1,painted-
F = F - 1st-fact by:1,creati on-time:1,creati on-peri od:1,painting-techni que-
used:0, ...,/st-factsubclass&gt;
2nd fact = creation-period
&lt;sub class : 0,current-location : 1,original-location: 1,painted-
by:1 ,creation-time: 1 ,creati on-peri od:0,painting-techni que-
used:0, ... ,1 st-fact: subclass,2&apos;d-fact: creation-period&gt;
3rd fact = creation-time
&lt;sub class : 0,current-location : 1,original-location: 1,painted-
by:1 ,creati on-time: 0,creati on-peri od:0,painting-techni que-
used: 0, ... ,lst-fact: subclass,2&amp;quot;d-fact: creati on-period,r1-
fact:crett tion -time&gt;
4th fact = painted-by
&lt;sub class : 0,current-location :1, original-location: 1,painted-
by: 0,creati on-time: 0,creati on-peri od : 0,pa inting-techni que-
used:0,...,1st-fact:subclass,2nd-fact:creation-period,3rd-
fact:cregion-time,4th-fact.painted-by&gt;
5th fact = original-location
&lt;subclass:0,current-location:1,original-location:0,painted-
by: 0,creati on-time: 0,creati on-peri od : 0,pa inting-techni que-
used:0,...,1st-fact:subclass,2nd-fact:creation-period,3rd-
fact:creation-time,4th-fact:painted-by,5thfact:original-locat on&gt;
6111 fact = current-location
F = f61
&lt;su bclass:1,cun-ent-locati on: 1, origi-
nal-location:
</figure>
<equation confidence="0.971121555555556">
nal-locati on: 1, painted-by:1, creaFicr
time:1, creation-period: 1,painting-
techni que-used:0, ...&gt;
F2 = Fl - 2nd-fact
3rrt classifier
F3 = F2 - 3rd fact
F4 = F3 - 41h fact
5th classifier
F5 = F4 - 5 -fact
</equation>
<bodyText confidence="0.9999853">
In order to create the dataset of our experi-
ments, we used a program that yields all the
possible combinations of six facts for each ex-
hibit. The right column of Table 1 shows an
example set of six facts, which can be used as
input to the discourse planner. Many combina-
tions, however, looked unreasonable in our
domain; e.g., combinations that do not include
the subclass fact (descriptions in the museum
domain must always inform the reader about
the type of the exhibit), or combinations that
include facts providing background informa-
tion about an entity that is not present in the
discourse (for instance, combinations that in-
clude opposite-technique but not painting-
technique-used in Table 1). A refinement op-
eration was performed manually to discard
such combinations. We note that in real-life
applications, the combinations would be ob-
tained by calling several times a content de-
termination module; hence, no refinement
operation would be necessary, as the content
determination module would, presumably,
never return unreasonable combinations of
facts.
After the refinement operation, 880 combi-
nations of 6 facts were left. The facts of each
set were manually assigned an order, to reflect
what a domain expert considered to be the
most natural ordering of the corresponding
</bodyText>
<page confidence="0.978914">
26
</page>
<bodyText confidence="0.9999435">
clauses in the generated texts. Each one of the
880 sets was then used as an instance in the
learning algorithms, as will be explained in the
following section.
</bodyText>
<subsectionHeader confidence="0.764465">
3.2 Instance representation and plan-
ner architecture
</subsectionHeader>
<bodyText confidence="0.999000704918033">
Figure I shows the discourse planning archi-
tecture that our approach adopts, along with an
example of inputs and outputs at each stage.
We decompose the fact-ordering task into six
multi-class classification problems. Each of the
six classifiers selects the fact to be placed at
the corresponding position. Each input set of
six facts is represented as a vector in a multi-
dimensional space, where dimensions corre-
spond to values of attributes. 42 binary attrib-
utes, representing the fact types of the domain,
were used. The vector at the top left corner of
Figure 1 represents the set of six facts of the
right column of Table 1. Each attribute shows
whether a particular fact type exists in the in-
put (e.g., creation-period:1) or not (e.g., paint-
ing-technique-used:0). Classifiers 2-6 have
additional attributes representing the fact types
that have already been selected for positions l-
5. More specifically, as shown in Figure I, the
attribute l&apos;-fact is added from the 2nd classifier
onwards, the attribute 2&amp;quot;a-fact is added from
the 3rd classifier onwards, and so forth. There-
fore, the classifiers make their decisions based
on the fact types that are present in their inputs
(set of remaining facts to be ordered) and the
fact types that have been selected at the previ-
ous positions. We assume that it is not possible
to have more than one fact of the same type in
the input set of facts because this is the case in
the M-P1RO domain (e.g., we cannot have two
facts of type creation-period) as well as in
other similar domains. In a more general case,
however, one could differentiate between facts
of the same type, by enriching, for instance,
the attributes, so as to represent information
about the entities related with each fact, or by
adding new attributes.
The output of each classifier is the class
value representing the fact type that has been
selected for the corresponding position. In the
example of Figure 1, the classifiers select the
following order: subclass, creation-period,
creation-time, painted-by, original-location,
current-location. As shown in Figure 1, the
sixth classifier has no substantial role, since
there is only one fact left in the input, and,
consequently, this fact will be placed at the
sixth position.
In a similar manner, a sequence of n classi-
fiers can be used when each document is to
convey n, rather than 6, facts. A limitation of
this approach is that it cannot be used when n
varies across the documents. However, this is
not a problem in M-PIRO, where n, in effect
the length of the documents, is fixed for each
user type: if there are t user types, we train t
different document planners, one for each user
type; each planner is a sequence of ni classifi-
ers, where ni is the value of n for the corre-
sponding user type (i = 1, t).
</bodyText>
<sectionHeader confidence="0.974101" genericHeader="evaluation">
4 Experiments and results
</sectionHeader>
<bodyText confidence="0.9987248">
In order to evaluate our approach, we per-
formed four experiments. The first experiment
was conducted using the majority scheme,
where each classifier selects among the avail-
able classes (i.e., among the facts that are pre-
sent in the input set and have not been selected
by the previous classifiers) the class (i.e., fact)
that was most frequent in its training data.
However, this scheme is too primitive, and
could not be seen as a safe benchmark for our
experiments. For this reason, we constructed a
simple planner, hereafter base planner, which
always assigns a predefined fixed order de-
fined in collaboration with a museum expert;
e.g., subclass should always be placed before
creation-period, creation-period should al-
ways be placed before creation-time, etc. The
base planner was used as a second baseline. In
this way, we had a safer benchmark for the
performance of the learning schemes. In the
two remaining experiments we used instance-
based and decision-tree learning. More specifi-
cally, we experimented with the k-nearest
neighbour algorithm (Aha and Kibler, 1991),
with k = 1, and the C4.5 algorithm (Quinlan,
1993). All the experiments were performed
using the machine learning software of WEKA
(Witten and Frank, 1999).
Figure 2 presents the accuracy scores of
each of the six classifiers, for each learning
</bodyText>
<page confidence="0.995454">
27
</page>
<bodyText confidence="0.999975655172414">
scheme. The results were obtained using JO-
fold cross-validation. That is, the dataset (880
vectors) was divided into ten disjoint parts
(folds), and each experiment was repeated 10
times. Each time, a different part was used for
testing, and the remaining 9 parts were used
for training. The dataset was stratified, i.e. the
class distribution in each fold was approxi-
mately the same as in the full dataset. The re-
ported scores are averaged over the 10
iterations. Accuracy measures the percentage
of correct selections at each classifier (posi-
tion) compared to the selections made by the
human annotator. All schemes have 100% ac-
curacy at the selection of the 1st and 6fil fact.
This happens because the first classifier always
selects the fact subclass, which is always the
first fact in our domain, while the sixth classi-
fier has no alternative choice, since only one
fact has been left in the input. At the other po-
sitions, both C4.5 and 1-NN perform better
than the two baselines; C4.5 seems to have a
slightly better performance than 1-NN. Paired
two-tailed t-tests at p = 0.005 indicate that the
observed differences in accuracy between
baselines and ML schemes are statistically sig-
nificant; the only exception is the selection of
the 21 fact, where there is no significant dif-
ference between the base planner and 1-NN.
</bodyText>
<figureCaption confidence="0.996554">
Figure 2: Accuracy scores at each classification
</figureCaption>
<bodyText confidence="0.990140047619047">
stage
Figure 3 shows a text corresponding to the
ordering produced by C4.5. The surface text,
including aggregation and referring expression
generation, was generated by hand, though we
plan to automate this process using the corre-
sponding modules of M-PIRO. The ordering of
the facts, which are realized as natural lan-
guage clauses, looks quite reasonable. The
flow of information is not the optimal one, but
does not cause problems to the understandabil-
ity or readability of the text. Figure 4 shows
the text that corresponds to the ordering of the
human annotator. The two texts differ in the
placement of the fact made-of, which is ex-
pressed as &amp;quot;it is made of marble&amp;quot;; C4.5 places
this fact at the fourth position instead of the
second, which is the right position according to
the human annotator. The word &amp;quot;but&amp;quot; in the
human text of Figure 4 implies the use of a
rhetorical relation; the presence of this relation
suggests a possible explanation of why the
human text is ordered differently than the one
produced by the system. The misplaced fact is
penalized three times when computing the ac-
curacy scores of the six classifiers: at the sec-
ond classifier, where the fact exhibit-portrays
is selected instead of made-of, at the third clas-
sifier, where creation-period is selected in-
stead of exhibit-portrays, and at the fourth
classifier, where made-of is selected instead of
creation-period. This implies that the accuracy
scores that were presented above are a very
strict measure of the performance of our
method, and, in fact, our method may actually
be performing even better than what the scores
indicate.
This exhibit is a portrait. It portrays Alexander the Great
and was created during the Hellenistic period. It is made
of marble. What we see in the picture is a roman copy.
Today it is located at the archaeological museum of Thas-
sos.
</bodyText>
<figureCaption confidence="0.998934">
Figure 3: Ordering of facts produced using C4.5
</figureCaption>
<bodyText confidence="0.9545352">
This exhibit is a portrait. It is made of marble and por-
trays Alexander the Great. It was created during the Hel-
lenistic period, but what we see in the picture is a roman
copy. Today it is located in the archaeological museum of
Thassos.
</bodyText>
<figureCaption confidence="0.988977">
Figure 4: Ordering of facts as specified by the
human annotator
</figureCaption>
<bodyText confidence="0.9994226">
We are currently trying to devise evaluation
measures that are better suited to discourse
planning, and to NLG in general. More spe-
cifically, we plan to apply metrics that assign
different penalties depending on the impor-
tance of an error, based on the edit distance
between the output of the discourse planner
and the reference corpus. We also plan to cor-
relate these metrics with human evaluation as
proposed by Reiter and Sripada (2002).
</bodyText>
<figure confidence="0.972636636363637">
E
1st fact 2nd fact 3rd fact 4th fact 5th fact 6th fact
Positon
— — Majority - - -0- - • Base Planner
— -A— - 1-nn —13—C4.5
100
80
60
Ps 40
20
0
</figure>
<page confidence="0.992943">
28
</page>
<sectionHeader confidence="0.90119" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999963139534884">
This paper has presented a machine learning
approach to the fact-ordering subtask of dis-
course planning. We have decomposed the
problem into a sequence of multi-class classi-
fication stages, where each stage selects the
fact to be placed at the corresponding position.
Experiments performed using the C4.5 and k-
NN learning algorithms indicate that our
method performs significantly better than both
a sequence of simple majority classifiers and a
set of manually constructed ordering rules.
Our method can be used with any content
determination module that selects a fixed
number of facts per document and user type,
and gives rise to planners that can be easily
retrained for other similar application domains,
where sample manually ordered sequences of
facts can be obtained. Compared to approaches
that employ manually constructed rules, our
method has the advantage that it does not re-
quire language technology expertise, and,
hence, can be used to construct authoring tools
that will allow domain experts to control the
order of the facts in the generated documents.
Furthermore, unlike previous machine learning
approaches, our method does not interleave
fact ordering with content determination.
As already mentioned, we plan to move to-
wards learning richer discourse plans, which
apart from ordering information will also in-
clude rhetorical relations, although our experi-
ence so far indicates that even just ordering the
facts in a natural way can lead to quite accept-
able texts. We are currently investigating a
more flexible representation that will not be
limited by a fixed number of facts per page
and, apart from the absolute order of facts, will
take into account the relative ordering between
facts (e.g., by using n-grams). Further work is
planned in order to devise better evaluation
measures, and improve the performance of our
planners by considering other learning algo-
rithms
</bodyText>
<sectionHeader confidence="0.998219" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998988890909091">
Aha D., and Kibler D. 1991. Instance-based learn-
ing algorithms. Machine Learning, 6:37-66.
Androutsopoulos I., Kokkinaki V., Dimitro-
manolaki A., Calder J., Oberlander J. and Not E.
2001. Generating multilingual personalized de-
scriptions of museum exhibits — the M-PIRO
project. In Proceedings of the 29th Conference on
Computer Applications and Quantitative Meth-
ods in Archaeology, Gotland, Sweden.
Androutsopoulos I., Spiliotopoulos D., Stamatakis
K., Dimitromanolaki A., Karkaletsis V. and Spy-
ropoulos C.D. 2002. Symbolic authoring for
multilingual natural language generation. In
Proceedings of the 2&apos; Hellenic Conference on
Artificial Intelligence (SETN-02), Thessaloniki,
Greece.
Bangalore S. and Rambow 0. 2000. Exploiting a
probabilistic hierarchical model for generation.
In Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
2000), Saarbrucken, Germany.
Barzilay R., Elhadad N. and McKeown K. 2002.
Inferring Strategies For Sentence Ordering In
Multidocument News Summarization. Journal of
Artificial Intelligence Research, 17: 35-55.
Duboue P and McKeown K. 2002. Content Planner
Construction via Evolutionary Algorithms and a
Corpus-based Fitness Function. In Proceedings
of the 2nd International Natural Language Gen-
eration Conference (INLG&apos;02), New York, USA,
pp. 89-96.
Duboue P. and McKeown K. 2001. Empirically
estimating order constraints for content planning
in generation. In Proceedings of the 39th Annual
Meeting of the Association for Computational
Linguistics (ACL-2001), Toulouse, France, pp.
172-179.
Hovy E. 1993. Automated Discourse Generation
Using Discourse Structure Relations. Artificial
Intelligence, 63(1-2):341-386.
A. Isard, J. Oberlander, I. Androutsopoulos and C.
Matheson. 2003. &amp;quot;Speaking the Users&apos; Lan-
guages&amp;quot;. IEEE Intelligent Systems, 18(1):40-45.
Kan M. and McKeown K. 2002. Corpus-trained
text generation for summarization. In Proceed-
ings of the 2h1 International Natural Language
Generation Conference (INLG&apos;02), New York,
USA, pp. 1-8.
Karamanis N. and Manurung H. M. 2002. Stochas-
tic Text Structuring using the Principle of Conti-
nuity. In Proceedings of the 2hd International
Natural Language Generation Conference
(INLG&apos;02), New York, USA, pp. 81-88.
Langkilde I and Knight K. 1998. Generation that
Exploits Corpus-Based Statistical Knowledge. In
</reference>
<page confidence="0.97408">
29
</page>
<reference confidence="0.999520268292683">
Proceedings of the 36th Annual Meeting of the
Association fbr Computational Linguistics and
17th International Conference on Computational
Linguistics (COLING-ACL 1998), Montreal,
Canada, pp. 704-710.
Malouf R. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceed-
ings of the 38f h Annual Meeting of the
Association fbr Computational Linguistics (ACL-
00), Hong Kong, pp. 85-92.
Mann W. and Thompson S. 1988. Rhetorical struc-
ture theory: towards a functional theory of text
organization. Text, 3:243-281.
McKeown K. 1985. Discourse strategies for gener-
ating natural language text. Artificial Intelli-
gence, 27:1-42.
Marcu D. 1997. From local to global coherence: A
bottom-up approach to text planning. In Pro-
ceedings of the 14th National Conference on Ar-
tificial Intelligence, Providence, Rhode Island,
pp. 629-635.
Mellish C., Knott A., Oberlander J. and 0&apos; Donnell
M. 1998. Experiments using stochastic search
for text planning. In Proceedings of the 9th Inter-
national Workshop on Natural Language Gen-
eration, Ontario, Canada, pp. 97-108.
O&apos;Donnell M., Mellish C., Oberlander J. and Knott
A. 2001. ILEX: An Architecture for a Dynamic
Hypertext Generation System. Natural Lan-
guage Engineering, 7(3):225-250.
Oh A. and Rudnicky A. 2000. Stochastic language
generation for spoken dialogue systems. In Pro-
ceedings of the ANLP/NAACL 2000 Workshop
on Conversational Systems, Seattle, USA, pp.
27-32.
Poesio M., Henschel R. and Kibble R. 2000. Statis-
tical NP generation: a first report. In Proceed-
ings of the ESSLLI Workshop on NP Generation,
Utrecht, Netherlands.
Quinlan R. 1993. C4.5: programs for machine
learning. Morgan Kaufmann, 302 p.
Ramb ow 0. 1990. Domain Communication
Knowledge. In Proceedings of the 5th Interna-
tional Workshop on Natural Language Genera-
tion, Dawson, PA.
Ratnaparkhi A. 2000. Trainable methods for sur-
face natural language generation. In Proceedings
of the 6th Applied Natural Language Processing
Conference and the l&apos; Meeting of the North
American Chapter of ACL (ANLP-NAACL
2000), Seattle, USA, pp. 194-201.
Reiter E. and Dale R. 2000. Building natural lan-
guage generation systems. Cambridge Univer-
sity Press, England, 248 p.
Reiter E. and Sripada S. 2002. Should Corpora
Texts Be Gold Standards for NLG? In Proceed-
ings of the 2nd International Natural Language
Generation Conference (INLG&apos;02), New York,
USA, pp. 97-104.
Shaw J. and Hatzivassiloglou V. 1999. Ordering
among premodifiers. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-99), College Park,
Maryland, pp. 135-143.
Varges S. and Mellish C. 2001. Instance-based
natural language generation. In Proceedings of
the 2nd Meeting of the North American Chapter
of ACL (NAACL-200I), Carnegie Mellon Uni-
versity, Pittsburgh, PA.
Walker M. 2000. An application of reinforcement
learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Artifi-
cial Intelligence Research, 12:387-416.
Walker M., Rambow 0. and Rogati M. 2001.
SPoT: a trainable sentence planner. In Proceed-
ings of the 2nd Meeting of the North American
Chapter of the ACL (NAACL-2001), Carnegie
Mellon University, Pittsburgh, PA.
Witten I. and Frank E. 1999. Data mining: practi-
cal machine learning tools and techniques with
Java implementations. Morgan Kaufmann, 416
p.
</reference>
<page confidence="0.998761">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919161">
<title confidence="0.9989585">Learning to Order Facts for Discourse Planning in Natural Language Generation</title>
<author confidence="0.976535">Aggeliki Ion Androutsopoulos</author>
<affiliation confidence="0.9914548">Department of Information Department of Communication Systems Athens University of Economics University of the Aegean Patission 76, 10434, Athens, Greece Institute of Informatics ion@aueb.gr NCSR</affiliation>
<address confidence="0.99901">15310, Ag.Paraskeui, Greece</address>
<email confidence="0.988125">adimit@iit.demokritos.gr</email>
<abstract confidence="0.99970752631579">presents a machine learning approach to discourse planning in natural language generation. More specifically, we address the problem of learning the most natural ordering of facts in discourse plans for a specific domain. We discuss our methodology and how it was instantiated using two different machine learning algorithms. A quantitative evaluation performed in the domain of museum exhibit descriptions indicates that our approach performs significantly better than manually constructed ordering rules. Being retrainable, the resulting planners can be ported easily to other similar domains, without requiring language technology expertise.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aha</author>
<author>D Kibler</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="5295" citStr="Aha and Kibler, 1991" startWordPosition="803" endWordPosition="806"> method consists of a sequence of stages, the number of stages being equal to the number of facts to be conveyed by each document. Each stage is responsible for the selection of the fact to be placed at the corresponding position in the resulting document. In our experiments, we set the number of facts per document to six, which per document to six, which seems to be an appropriate value for our particular domain and an average adult user, but this number could vary depending on the application and user type. Two learning algorithms, decision trees (Quinlan, 1993) and instance-based learning (Aha and Kibler, 1991), were explored. The results are compared against two baselines: a simple hand-crafted planner, which always assigns a predefined order, and the majority scheme. The latter selects, among the facts that are available at each position, the fact that occurred most frequently at that position in the training data. Overall, the results indicate that with either of the two learning algorithms our method significantly outperforms both of the baselines, and that there is no significant difference in the performance of the two learning algorithms. The remainder of this paper is organized as follows. S</context>
<context position="19004" citStr="Aha and Kibler, 1991" startWordPosition="2858" endWordPosition="2861"> experiments. For this reason, we constructed a simple planner, hereafter base planner, which always assigns a predefined fixed order defined in collaboration with a museum expert; e.g., subclass should always be placed before creation-period, creation-period should always be placed before creation-time, etc. The base planner was used as a second baseline. In this way, we had a safer benchmark for the performance of the learning schemes. In the two remaining experiments we used instancebased and decision-tree learning. More specifically, we experimented with the k-nearest neighbour algorithm (Aha and Kibler, 1991), with k = 1, and the C4.5 algorithm (Quinlan, 1993). All the experiments were performed using the machine learning software of WEKA (Witten and Frank, 1999). Figure 2 presents the accuracy scores of each of the six classifiers, for each learning 27 scheme. The results were obtained using JOfold cross-validation. That is, the dataset (880 vectors) was divided into ten disjoint parts (folds), and each experiment was repeated 10 times. Each time, a different part was used for testing, and the remaining 9 parts were used for training. The dataset was stratified, i.e. the class distribution in eac</context>
</contexts>
<marker>Aha, Kibler, 1991</marker>
<rawString>Aha D., and Kibler D. 1991. Instance-based learning algorithms. Machine Learning, 6:37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>V Kokkinaki</author>
<author>A Dimitromanolaki</author>
<author>J Calder</author>
<author>J Oberlander</author>
<author>E Not</author>
</authors>
<title>Generating multilingual personalized descriptions of museum exhibits — the M-PIRO project.</title>
<date>2001</date>
<booktitle>In Proceedings of the 29th Conference on Computer Applications and Quantitative Methods in Archaeology,</booktitle>
<location>Gotland,</location>
<contexts>
<context position="2342" citStr="Androutsopoulos et al., 2001" startWordPosition="332" endWordPosition="336">nn and Thompson, 1988; Hovy, 1993) or schemata (McKeown, 1985). In both cases, the rules that determine the order and the rhetorical relations are typically written by hand. This is a time-consuming process, which requires domain and linguistic expertise, and has to be repeated whenever the system is ported to a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al., 2001). Building upon ILEX (O&apos;Donnell et al., 2001), M-PIRO is developing technology that allows personalized descriptions of museum exhibits to be generated in several languages, starting from symbolic, language-independent information stored in a database, and small fragments of text (Isard et al., 2003). One of M-PIRO&apos;s most ambitious 23 goals is to develop authoring tools that will allow domain experts, e.g., museum curators, with no language technology expertise to configure the system for new application domains. While this goal has largely been achieved for resources such as the domain-depend</context>
</contexts>
<marker>Androutsopoulos, Kokkinaki, Dimitromanolaki, Calder, Oberlander, Not, 2001</marker>
<rawString>Androutsopoulos I., Kokkinaki V., Dimitromanolaki A., Calder J., Oberlander J. and Not E. 2001. Generating multilingual personalized descriptions of museum exhibits — the M-PIRO project. In Proceedings of the 29th Conference on Computer Applications and Quantitative Methods in Archaeology, Gotland, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Androutsopoulos</author>
<author>D Spiliotopoulos</author>
<author>K Stamatakis</author>
<author>A Dimitromanolaki</author>
<author>V Karkaletsis</author>
<author>C D Spyropoulos</author>
</authors>
<title>Symbolic authoring for multilingual natural language generation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2&apos; Hellenic Conference on Artificial Intelligence (SETN-02),</booktitle>
<location>Thessaloniki, Greece.</location>
<contexts>
<context position="3111" citStr="Androutsopoulos et al., 2002" startWordPosition="444" endWordPosition="448">e generated in several languages, starting from symbolic, language-independent information stored in a database, and small fragments of text (Isard et al., 2003). One of M-PIRO&apos;s most ambitious 23 goals is to develop authoring tools that will allow domain experts, e.g., museum curators, with no language technology expertise to configure the system for new application domains. While this goal has largely been achieved for resources such as the domain-dependent parts of the ontology, or domain-dependent settings that affect content selection, lexicalization, and referring expression generation (Androutsopoulos et al., 2002), designing tools that will allow domain experts to edit discourse planning rules has proven difficult. In contrast, domain experts, in our case museum curators, were happy to reorder the clauses of sample generated texts, thus indicating the preferred orderings of the facts in the corresponding discourse plans. We have, therefore, opted for a machine learning approach that allows factordering rules to be captured automatically from sets of manually reordered facts. We view this approach as a first step towards learning richer discourse plans, which apart from ordering information will also in</context>
</contexts>
<marker>Androutsopoulos, Spiliotopoulos, Stamatakis, Dimitromanolaki, Karkaletsis, Spyropoulos, 2002</marker>
<rawString>Androutsopoulos I., Spiliotopoulos D., Stamatakis K., Dimitromanolaki A., Karkaletsis V. and Spyropoulos C.D. 2002. Symbolic authoring for multilingual natural language generation. In Proceedings of the 2&apos; Hellenic Conference on Artificial Intelligence (SETN-02), Thessaloniki, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="8593" citStr="Bangalore and Rambow, 2000" startWordPosition="1319" endWordPosition="1322">n mapped to the corresponding sequence of facts. Karamanis and Manurung (2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzila</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Bangalore S. and Rambow 0. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>N Elhadad</author>
<author>K McKeown</author>
</authors>
<title>Inferring Strategies For Sentence Ordering In Multidocument News Summarization.</title>
<date>2002</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>17</volume>
<pages>35--55</pages>
<contexts>
<context position="9208" citStr="Barzilay et al. (2002)" startWordPosition="1418" endWordPosition="1421">w, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of ordering preferences among subjects and clustering techniques that identify cornmonalities among these preferences. The approach presented here, instead of identifying ordering constraints, &amp;quot;learns&amp;quot; the overall ordering of the input facts. 3 Learning to order facts In our approach, the discourse planner is trained on manually ordered sequences of facts of a fixed length. Once trained, it is able to determine what it considers to be the most natural ordering of any set of facts, as output by a content determination module, provided t</context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2002</marker>
<rawString>Barzilay R., Elhadad N. and McKeown K. 2002. Inferring Strategies For Sentence Ordering In Multidocument News Summarization. Journal of Artificial Intelligence Research, 17: 35-55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Duboue</author>
<author>K McKeown</author>
</authors>
<title>Content Planner Construction via Evolutionary Algorithms and a Corpus-based Fitness Function.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference (INLG&apos;02),</booktitle>
<pages>89--96</pages>
<location>New York, USA,</location>
<contexts>
<context position="6588" citStr="Duboue and McKeown, 2002" startWordPosition="1008" endWordPosition="1011"> their relevance to the work presented here. Section 3 describes our learning approach, including issues such as data representation and system architecture. Section 4 discusses our experiments and their results. Section 5 concludes and highlights plans for future work. 2 Previous work In recent years, ML approaches have been introduced to NLG to address problems such as the construction and maintenance of domain and language resources, which is a timeconsuming process in systems that use handcrafted rules.&apos; To the best of our knowledge, only two of these approaches (Duboue and McKeown, 2001; Duboue and McKeown, 2002) consider discourse planning. Duboue and McKeown (2001) present an unsupervised ML algorithm based on pattern matching and clustering, which is used to learn ordering constraints among facts. The same authors have also used evolutionary algorithms to learn the tree representation of a planner (Duboue and McKeown, 2002). These works are similar to ours in that we also address the problem of ordering facts. However, Duboue I For an extensive bibliography on statistical and machine learning approaches to NLG, see: http://www.iit.demokritos.gr/—adimitibibliographv.html. 24 and McKeown follow the l</context>
</contexts>
<marker>Duboue, McKeown, 2002</marker>
<rawString>Duboue P and McKeown K. 2002. Content Planner Construction via Evolutionary Algorithms and a Corpus-based Fitness Function. In Proceedings of the 2nd International Natural Language Generation Conference (INLG&apos;02), New York, USA, pp. 89-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Duboue</author>
<author>K McKeown</author>
</authors>
<title>Empirically estimating order constraints for content planning in generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001),</booktitle>
<pages>172--179</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="6561" citStr="Duboue and McKeown, 2001" startWordPosition="1004" endWordPosition="1007">ches to NLG, and discusses their relevance to the work presented here. Section 3 describes our learning approach, including issues such as data representation and system architecture. Section 4 discusses our experiments and their results. Section 5 concludes and highlights plans for future work. 2 Previous work In recent years, ML approaches have been introduced to NLG to address problems such as the construction and maintenance of domain and language resources, which is a timeconsuming process in systems that use handcrafted rules.&apos; To the best of our knowledge, only two of these approaches (Duboue and McKeown, 2001; Duboue and McKeown, 2002) consider discourse planning. Duboue and McKeown (2001) present an unsupervised ML algorithm based on pattern matching and clustering, which is used to learn ordering constraints among facts. The same authors have also used evolutionary algorithms to learn the tree representation of a planner (Duboue and McKeown, 2002). These works are similar to ours in that we also address the problem of ordering facts. However, Duboue I For an extensive bibliography on statistical and machine learning approaches to NLG, see: http://www.iit.demokritos.gr/—adimitibibliographv.html. </context>
</contexts>
<marker>Duboue, McKeown, 2001</marker>
<rawString>Duboue P. and McKeown K. 2001. Empirically estimating order constraints for content planning in generation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001), Toulouse, France, pp. 172-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Automated Discourse Generation Using Discourse Structure Relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="1747" citStr="Hovy, 1993" startWordPosition="238" endWordPosition="239">es of Reiter and Dale (2000), we view natural language generation (NLG) as consisting of six tasks: content determination, discourse planning, aggregation, lexicalization, referring expression generation, and linguistic realization. This paper is concerned with the second task, i.e., discourse planning. Discourse planning determines the ordering and rhetorical relations of the logical messages, hereafter called facts, that the generated document is intended to convey. Most existing approaches to discourse planning are based on either rhetorical structure theory (RST) (Mann and Thompson, 1988; Hovy, 1993) or schemata (McKeown, 1985). In both cases, the rules that determine the order and the rhetorical relations are typically written by hand. This is a time-consuming process, which requires domain and linguistic expertise, and has to be repeated whenever the system is ported to a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al., 2001). Bui</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Hovy E. 1993. Automated Discourse Generation Using Discourse Structure Relations. Artificial Intelligence, 63(1-2):341-386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Isard</author>
<author>J Oberlander</author>
<author>I Androutsopoulos</author>
<author>C Matheson</author>
</authors>
<title>Speaking the Users&apos; Languages&amp;quot;.</title>
<date>2003</date>
<journal>IEEE Intelligent Systems,</journal>
<pages>18--1</pages>
<contexts>
<context position="2643" citStr="Isard et al., 2003" startWordPosition="377" endWordPosition="380"> a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al., 2001). Building upon ILEX (O&apos;Donnell et al., 2001), M-PIRO is developing technology that allows personalized descriptions of museum exhibits to be generated in several languages, starting from symbolic, language-independent information stored in a database, and small fragments of text (Isard et al., 2003). One of M-PIRO&apos;s most ambitious 23 goals is to develop authoring tools that will allow domain experts, e.g., museum curators, with no language technology expertise to configure the system for new application domains. While this goal has largely been achieved for resources such as the domain-dependent parts of the ontology, or domain-dependent settings that affect content selection, lexicalization, and referring expression generation (Androutsopoulos et al., 2002), designing tools that will allow domain experts to edit discourse planning rules has proven difficult. In contrast, domain experts,</context>
</contexts>
<marker>Isard, Oberlander, Androutsopoulos, Matheson, 2003</marker>
<rawString>A. Isard, J. Oberlander, I. Androutsopoulos and C. Matheson. 2003. &amp;quot;Speaking the Users&apos; Languages&amp;quot;. IEEE Intelligent Systems, 18(1):40-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kan</author>
<author>K McKeown</author>
</authors>
<title>Corpus-trained text generation for summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2h1 International Natural Language Generation Conference (INLG&apos;02),</booktitle>
<pages>1--8</pages>
<location>New York, USA,</location>
<contexts>
<context position="9115" citStr="Kan and McKeown (2002)" startWordPosition="1401" endWordPosition="1404">en used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of ordering preferences among subjects and clustering techniques that identify cornmonalities among these preferences. The approach presented here, instead of identifying ordering constraints, &amp;quot;learns&amp;quot; the overall ordering of the input facts. 3 Learning to order facts In our approach, the discourse planner is trained on manually ordered sequences of facts of a fixed length. Once trained, it is able to determine what it considers to be the most </context>
</contexts>
<marker>Kan, McKeown, 2002</marker>
<rawString>Kan M. and McKeown K. 2002. Corpus-trained text generation for summarization. In Proceedings of the 2h1 International Natural Language Generation Conference (INLG&apos;02), New York, USA, pp. 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
<author>H M Manurung</author>
</authors>
<title>Stochastic Text Structuring using the Principle of Continuity.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2hd International Natural Language Generation Conference (INLG&apos;02),</booktitle>
<pages>81--88</pages>
<location>New York, USA,</location>
<contexts>
<context position="8045" citStr="Karamanis and Manurung (2002)" startWordPosition="1228" endWordPosition="1232">t handle inputs from an independent content determination module. In contrast, our method can be used with any content determination mechanism that returns a fixed number of facts. This has the benefit that alternative content determination modules can be used without affecting the discourse planner. Moreover, while Duboue and McKeown (2002) learn a tree structure representing the best sequence of facts, our method directly manipulates facts. Mellish et al. (1998) also experiment with genetic algorithms to find the optimal RST tree, which is then mapped to the corresponding sequence of facts. Karamanis and Manurung (2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw </context>
</contexts>
<marker>Karamanis, Manurung, 2002</marker>
<rawString>Karamanis N. and Manurung H. M. 2002. Stochastic Text Structuring using the Principle of Continuity. In Proceedings of the 2hd International Natural Language Generation Conference (INLG&apos;02), New York, USA, pp. 81-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that Exploits Corpus-Based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association fbr Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL</booktitle>
<pages>704--710</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="8565" citStr="Langkilde and Knight, 1998" startWordPosition="1315" endWordPosition="1318">timal RST tree, which is then mapped to the corresponding sequence of facts. Karamanis and Manurung (2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I and Knight K. 1998. Generation that Exploits Corpus-Based Statistical Knowledge. In Proceedings of the 36th Annual Meeting of the Association fbr Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL 1998), Montreal, Canada, pp. 704-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Malouf</author>
</authors>
<title>The order of prenominal adjectives in natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38f h Annual Meeting of the Association fbr Computational Linguistics (ACL00), Hong Kong,</booktitle>
<pages>85--92</pages>
<contexts>
<context position="8685" citStr="Malouf 2000" startWordPosition="1333" endWordPosition="1335"> employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of ordering prefer</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Malouf R. 2000. The order of prenominal adjectives in natural language generation. In Proceedings of the 38f h Annual Meeting of the Association fbr Computational Linguistics (ACL00), Hong Kong, pp. 85-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: towards a functional theory of text organization.</title>
<date>1988</date>
<tech>Text,</tech>
<pages>3--243</pages>
<contexts>
<context position="1734" citStr="Mann and Thompson, 1988" startWordPosition="234" endWordPosition="237">ntroduction Along the lines of Reiter and Dale (2000), we view natural language generation (NLG) as consisting of six tasks: content determination, discourse planning, aggregation, lexicalization, referring expression generation, and linguistic realization. This paper is concerned with the second task, i.e., discourse planning. Discourse planning determines the ordering and rhetorical relations of the logical messages, hereafter called facts, that the generated document is intended to convey. Most existing approaches to discourse planning are based on either rhetorical structure theory (RST) (Mann and Thompson, 1988; Hovy, 1993) or schemata (McKeown, 1985). In both cases, the rules that determine the order and the rhetorical relations are typically written by hand. This is a time-consuming process, which requires domain and linguistic expertise, and has to be repeated whenever the system is ported to a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann W. and Thompson S. 1988. Rhetorical structure theory: towards a functional theory of text organization. Text, 3:243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Discourse strategies for generating natural language text.</title>
<date>1985</date>
<journal>Artificial Intelligence,</journal>
<pages>27--1</pages>
<contexts>
<context position="1775" citStr="McKeown, 1985" startWordPosition="243" endWordPosition="244">00), we view natural language generation (NLG) as consisting of six tasks: content determination, discourse planning, aggregation, lexicalization, referring expression generation, and linguistic realization. This paper is concerned with the second task, i.e., discourse planning. Discourse planning determines the ordering and rhetorical relations of the logical messages, hereafter called facts, that the generated document is intended to convey. Most existing approaches to discourse planning are based on either rhetorical structure theory (RST) (Mann and Thompson, 1988; Hovy, 1993) or schemata (McKeown, 1985). In both cases, the rules that determine the order and the rhetorical relations are typically written by hand. This is a time-consuming process, which requires domain and linguistic expertise, and has to be repeated whenever the system is ported to a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al., 2001). Building upon ILEX (O&apos;Donnell e</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown K. 1985. Discourse strategies for generating natural language text. Artificial Intelligence, 27:1-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From local to global coherence: A bottom-up approach to text planning.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence,</booktitle>
<pages>629--635</pages>
<location>Providence, Rhode Island,</location>
<marker>Marcu, 1997</marker>
<rawString>Marcu D. 1997. From local to global coherence: A bottom-up approach to text planning. In Proceedings of the 14th National Conference on Artificial Intelligence, Providence, Rhode Island, pp. 629-635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>A Knott</author>
<author>J Oberlander</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<pages>97--108</pages>
<location>Ontario, Canada,</location>
<contexts>
<context position="7884" citStr="Mellish et al. (1998)" startWordPosition="1202" endWordPosition="1205">dent stage, but is interleaved with discourse planning. This means that the discourse planner has the overall control of content determination, and cannot handle inputs from an independent content determination module. In contrast, our method can be used with any content determination mechanism that returns a fixed number of facts. This has the benefit that alternative content determination modules can be used without affecting the discourse planner. Moreover, while Duboue and McKeown (2002) learn a tree structure representing the best sequence of facts, our method directly manipulates facts. Mellish et al. (1998) also experiment with genetic algorithms to find the optimal RST tree, which is then mapped to the corresponding sequence of facts. Karamanis and Manurung (2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG hav</context>
</contexts>
<marker>Mellish, Knott, Oberlander, 1998</marker>
<rawString>Mellish C., Knott A., Oberlander J. and 0&apos; Donnell M. 1998. Experiments using stochastic search for text planning. In Proceedings of the 9th International Workshop on Natural Language Generation, Ontario, Canada, pp. 97-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M O&apos;Donnell</author>
<author>C Mellish</author>
<author>J Oberlander</author>
<author>A Knott</author>
</authors>
<title>ILEX: An Architecture for a Dynamic Hypertext Generation System. Natural Language Engineering,</title>
<date>2001</date>
<pages>7--3</pages>
<contexts>
<context position="2387" citStr="O&apos;Donnell et al., 2001" startWordPosition="340" endWordPosition="343">eown, 1985). In both cases, the rules that determine the order and the rhetorical relations are typically written by hand. This is a time-consuming process, which requires domain and linguistic expertise, and has to be repeated whenever the system is ported to a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al., 2001). Building upon ILEX (O&apos;Donnell et al., 2001), M-PIRO is developing technology that allows personalized descriptions of museum exhibits to be generated in several languages, starting from symbolic, language-independent information stored in a database, and small fragments of text (Isard et al., 2003). One of M-PIRO&apos;s most ambitious 23 goals is to develop authoring tools that will allow domain experts, e.g., museum curators, with no language technology expertise to configure the system for new application domains. While this goal has largely been achieved for resources such as the domain-dependent parts of the ontology, or domain-dependen</context>
</contexts>
<marker>O&apos;Donnell, Mellish, Oberlander, Knott, 2001</marker>
<rawString>O&apos;Donnell M., Mellish C., Oberlander J. and Knott A. 2001. ILEX: An Architecture for a Dynamic Hypertext Generation System. Natural Language Engineering, 7(3):225-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Oh</author>
<author>A Rudnicky</author>
</authors>
<title>Stochastic language generation for spoken dialogue systems.</title>
<date>2000</date>
<booktitle>In Proceedings of the ANLP/NAACL 2000 Workshop on Conversational Systems,</booktitle>
<pages>27--32</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="8905" citStr="Oh and Rudnicky, 2000" startWordPosition="1368" endWordPosition="1371">document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of ordering preferences among subjects and clustering techniques that identify cornmonalities among these preferences. The approach presented here, instead of identifying ordering constraints, &amp;quot;learns&amp;quot; the overall ordering of the input fa</context>
</contexts>
<marker>Oh, Rudnicky, 2000</marker>
<rawString>Oh A. and Rudnicky A. 2000. Stochastic language generation for spoken dialogue systems. In Proceedings of the ANLP/NAACL 2000 Workshop on Conversational Systems, Seattle, USA, pp. 27-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Henschel</author>
<author>R Kibble</author>
</authors>
<title>Statistical NP generation: a first report.</title>
<date>2000</date>
<booktitle>In Proceedings of the ESSLLI Workshop on NP Generation,</booktitle>
<location>Utrecht, Netherlands.</location>
<contexts>
<context position="8767" citStr="Poesio et al., 2000" startWordPosition="1347" endWordPosition="1350"> these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of ordering preferences among subjects and clustering techniques that identify cornmonalities among </context>
</contexts>
<marker>Poesio, Henschel, Kibble, 2000</marker>
<rawString>Poesio M., Henschel R. and Kibble R. 2000. Statistical NP generation: a first report. In Proceedings of the ESSLLI Workshop on NP Generation, Utrecht, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<volume>302</volume>
<pages>p.</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="5244" citStr="Quinlan, 1993" startWordPosition="798" endWordPosition="799">r facts to be conveyed by each document. Our method consists of a sequence of stages, the number of stages being equal to the number of facts to be conveyed by each document. Each stage is responsible for the selection of the fact to be placed at the corresponding position in the resulting document. In our experiments, we set the number of facts per document to six, which per document to six, which seems to be an appropriate value for our particular domain and an average adult user, but this number could vary depending on the application and user type. Two learning algorithms, decision trees (Quinlan, 1993) and instance-based learning (Aha and Kibler, 1991), were explored. The results are compared against two baselines: a simple hand-crafted planner, which always assigns a predefined order, and the majority scheme. The latter selects, among the facts that are available at each position, the fact that occurred most frequently at that position in the training data. Overall, the results indicate that with either of the two learning algorithms our method significantly outperforms both of the baselines, and that there is no significant difference in the performance of the two learning algorithms. The</context>
<context position="19056" citStr="Quinlan, 1993" startWordPosition="2870" endWordPosition="2871">ner, hereafter base planner, which always assigns a predefined fixed order defined in collaboration with a museum expert; e.g., subclass should always be placed before creation-period, creation-period should always be placed before creation-time, etc. The base planner was used as a second baseline. In this way, we had a safer benchmark for the performance of the learning schemes. In the two remaining experiments we used instancebased and decision-tree learning. More specifically, we experimented with the k-nearest neighbour algorithm (Aha and Kibler, 1991), with k = 1, and the C4.5 algorithm (Quinlan, 1993). All the experiments were performed using the machine learning software of WEKA (Witten and Frank, 1999). Figure 2 presents the accuracy scores of each of the six classifiers, for each learning 27 scheme. The results were obtained using JOfold cross-validation. That is, the dataset (880 vectors) was divided into ten disjoint parts (folds), and each experiment was repeated 10 times. Each time, a different part was used for testing, and the remaining 9 parts were used for training. The dataset was stratified, i.e. the class distribution in each fold was approximately the same as in the full dat</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan R. 1993. C4.5: programs for machine learning. Morgan Kaufmann, 302 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramb ow</author>
</authors>
<title>Domain Communication Knowledge.</title>
<date>1990</date>
<booktitle>In Proceedings of the 5th International Workshop on Natural Language Generation,</booktitle>
<location>Dawson, PA.</location>
<contexts>
<context position="2061" citStr="ow (1990)" startWordPosition="291" endWordPosition="292">lanning determines the ordering and rhetorical relations of the logical messages, hereafter called facts, that the generated document is intended to convey. Most existing approaches to discourse planning are based on either rhetorical structure theory (RST) (Mann and Thompson, 1988; Hovy, 1993) or schemata (McKeown, 1985). In both cases, the rules that determine the order and the rhetorical relations are typically written by hand. This is a time-consuming process, which requires domain and linguistic expertise, and has to be repeated whenever the system is ported to a new domain; see also Rambow (1990). This paper presents a machine learning (ML) approach to the subtask of discourse planning that attempts to find the most natural ordering of facts in each generated document. Our approach was motivated by experience obtained from the M-PIRO project (Androutsopoulos et al., 2001). Building upon ILEX (O&apos;Donnell et al., 2001), M-PIRO is developing technology that allows personalized descriptions of museum exhibits to be generated in several languages, starting from symbolic, language-independent information stored in a database, and small fragments of text (Isard et al., 2003). One of M-PIRO&apos;s </context>
</contexts>
<marker>ow, 1990</marker>
<rawString>Ramb ow 0. 1990. Domain Communication Knowledge. In Proceedings of the 5th International Workshop on Natural Language Generation, Dawson, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Trainable methods for surface natural language generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied Natural Language Processing Conference and the l&apos; Meeting of the North American Chapter of ACL (ANLP-NAACL 2000),</booktitle>
<pages>194--201</pages>
<location>Seattle, USA,</location>
<contexts>
<context position="8612" citStr="Ratnaparkhi, 2000" startWordPosition="1323" endWordPosition="1324">g sequence of facts. Karamanis and Manurung (2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) man</context>
</contexts>
<marker>Ratnaparkhi, 2000</marker>
<rawString>Ratnaparkhi A. 2000. Trainable methods for surface natural language generation. In Proceedings of the 6th Applied Natural Language Processing Conference and the l&apos; Meeting of the North American Chapter of ACL (ANLP-NAACL 2000), Seattle, USA, pp. 194-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building natural language generation systems.</title>
<date>2000</date>
<volume>248</volume>
<pages>p.</pages>
<publisher>Cambridge University Press,</publisher>
<location>England,</location>
<contexts>
<context position="1164" citStr="Reiter and Dale (2000)" startWordPosition="156" endWordPosition="159">generation. More specifically, we address the problem of learning the most natural ordering of facts in discourse plans for a specific domain. We discuss our methodology and how it was instantiated using two different machine learning algorithms. A quantitative evaluation performed in the domain of museum exhibit descriptions indicates that our approach performs significantly better than manually constructed ordering rules. Being retrainable, the resulting planners can be ported easily to other similar domains, without requiring language technology expertise. 1 Introduction Along the lines of Reiter and Dale (2000), we view natural language generation (NLG) as consisting of six tasks: content determination, discourse planning, aggregation, lexicalization, referring expression generation, and linguistic realization. This paper is concerned with the second task, i.e., discourse planning. Discourse planning determines the ordering and rhetorical relations of the logical messages, hereafter called facts, that the generated document is intended to convey. Most existing approaches to discourse planning are based on either rhetorical structure theory (RST) (Mann and Thompson, 1988; Hovy, 1993) or schemata (McK</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Reiter E. and Dale R. 2000. Building natural language generation systems. Cambridge University Press, England, 248 p.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
</authors>
<title>Should Corpora Texts Be Gold Standards for NLG?</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference (INLG&apos;02),</booktitle>
<pages>97--104</pages>
<location>New York, USA,</location>
<contexts>
<context position="23272" citStr="Reiter and Sripada (2002)" startWordPosition="3582" endWordPosition="3585">stic period, but what we see in the picture is a roman copy. Today it is located in the archaeological museum of Thassos. Figure 4: Ordering of facts as specified by the human annotator We are currently trying to devise evaluation measures that are better suited to discourse planning, and to NLG in general. More specifically, we plan to apply metrics that assign different penalties depending on the importance of an error, based on the edit distance between the output of the discourse planner and the reference corpus. We also plan to correlate these metrics with human evaluation as proposed by Reiter and Sripada (2002). E 1st fact 2nd fact 3rd fact 4th fact 5th fact 6th fact Positon — — Majority - - -0- - • Base Planner — -A— - 1-nn —13—C4.5 100 80 60 Ps 40 20 0 28 5 Conclusions and future work This paper has presented a machine learning approach to the fact-ordering subtask of discourse planning. We have decomposed the problem into a sequence of multi-class classification stages, where each stage selects the fact to be placed at the corresponding position. Experiments performed using the C4.5 and kNN learning algorithms indicate that our method performs significantly better than both a sequence of simple m</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>Reiter E. and Sripada S. 2002. Should Corpora Texts Be Gold Standards for NLG? In Proceedings of the 2nd International Natural Language Generation Conference (INLG&apos;02), New York, USA, pp. 97-104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Shaw</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Ordering among premodifiers.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>135--143</pages>
<location>College Park, Maryland,</location>
<contexts>
<context position="8671" citStr="Shaw and Hatzivassiloglou, 1999" startWordPosition="1329" endWordPosition="1332">2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of o</context>
</contexts>
<marker>Shaw, Hatzivassiloglou, 1999</marker>
<rawString>Shaw J. and Hatzivassiloglou V. 1999. Ordering among premodifiers. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), College Park, Maryland, pp. 135-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Varges</author>
<author>C Mellish</author>
</authors>
<title>Instance-based natural language generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of ACL (NAACL-200I),</booktitle>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="8638" citStr="Varges and Mellish, 2001" startWordPosition="1325" endWordPosition="1328">. Karamanis and Manurung (2002) use a similar approach that employs constraints from Centering Theory in the genetic search. However, these approaches do not involve any learning: the genetic search is repeated every time the text planner is invoked, i.e., for each new document. In contrast, our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints</context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>Varges S. and Mellish C. 2001. Instance-based natural language generation. In Proceedings of the 2nd Meeting of the North American Chapter of ACL (NAACL-200I), Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
</authors>
<title>An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email.</title>
<date>2000</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>12--387</pages>
<contexts>
<context position="8920" citStr="Walker, 2000" startWordPosition="1372" endWordPosition="1373">our method induces a single discourse planner from the training data, which is then used to order any set of facts provided by the content determinator. ML approaches to NLG have also been used in syntactic and lexical realization (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001; Shaw and Hatzivassiloglou, 1999; Malouf 2000), as well as in sentence planning tasks (Walker et al., 2001; Poesio et al., 2000). In the context of spoken dialogue systems, learning techniques have been used to select among different templates (Oh and Rudnicky, 2000; Walker, 2000). These approaches, however, are not directly relevant to discourse planning The problem of ordering semantic units has also been addressed in the context of summarization. Kan and McKeown (2002) use an ngram model to infer ordering constraints between facts, while Barzilay et al. (2002) manually identify constraints on ordering, using a corpus of ordering preferences among subjects and clustering techniques that identify cornmonalities among these preferences. The approach presented here, instead of identifying ordering constraints, &amp;quot;learns&amp;quot; the overall ordering of the input facts. 3 Learning</context>
</contexts>
<marker>Walker, 2000</marker>
<rawString>Walker M. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. Journal of Artificial Intelligence Research, 12:387-416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>Rambow</author>
</authors>
<title>SPoT: a trainable sentence planner.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the ACL (NAACL-2001),</booktitle>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<marker>Walker, Rambow, 2001</marker>
<rawString>Walker M., Rambow 0. and Rogati M. 2001. SPoT: a trainable sentence planner. In Proceedings of the 2nd Meeting of the North American Chapter of the ACL (NAACL-2001), Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data mining: practical machine learning tools and techniques with Java implementations.</title>
<date>1999</date>
<pages>416</pages>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="19161" citStr="Witten and Frank, 1999" startWordPosition="2884" endWordPosition="2887">ion with a museum expert; e.g., subclass should always be placed before creation-period, creation-period should always be placed before creation-time, etc. The base planner was used as a second baseline. In this way, we had a safer benchmark for the performance of the learning schemes. In the two remaining experiments we used instancebased and decision-tree learning. More specifically, we experimented with the k-nearest neighbour algorithm (Aha and Kibler, 1991), with k = 1, and the C4.5 algorithm (Quinlan, 1993). All the experiments were performed using the machine learning software of WEKA (Witten and Frank, 1999). Figure 2 presents the accuracy scores of each of the six classifiers, for each learning 27 scheme. The results were obtained using JOfold cross-validation. That is, the dataset (880 vectors) was divided into ten disjoint parts (folds), and each experiment was repeated 10 times. Each time, a different part was used for testing, and the remaining 9 parts were used for training. The dataset was stratified, i.e. the class distribution in each fold was approximately the same as in the full dataset. The reported scores are averaged over the 10 iterations. Accuracy measures the percentage of correc</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Witten I. and Frank E. 1999. Data mining: practical machine learning tools and techniques with Java implementations. Morgan Kaufmann, 416 p.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>