<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012196">
<title confidence="0.9991985">
Investigation in Statistical Language-Independent Approaches for
Opinion Detection in English, Chinese and Japanese
</title>
<author confidence="0.996514">
Olena Zubaryeva Jacques Savoy
</author>
<affiliation confidence="0.9998315">
Institute of Informatics Institute of Informatics
University of Neuchâtel University of Neuchâtel
</affiliation>
<address confidence="0.954069">
Emile-Argand, 11, 2009 Switzerland Emile-Argand, 11, 2009 Switzerland
</address>
<email confidence="0.993952">
olena.zubaryeva@unine.ch jacques.savoy@unine.ch
</email>
<sectionHeader confidence="0.995787" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994680142857143">
In this paper we present a new statistical ap-
proach to opinion detection and its’ evalua-
tion on the English, Chinese and Japanese
corpora. Besides, the proposed method is
compared with three baselines, namely
Naïve Bayes classifier, a language model
and an approach based on significant collo-
cations. These models being language inde-
pendent are improved with the use of lan-
guage-dependent technique on the example
of the English corpus. We show that our
method almost always gives better perfor-
mance compared to the considered base-
lines.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838066666667">
The task of opinion mining has received atten-
tion from the research community and industry
lately. The main reasons for extensive research in
the area are the growth of user needs and compa-
nies’ desire to analyze and exploit the user-gener-
ated content on the Web in the form of blogs and
discussions. Thus, users want to search for opin-
ions on various topics from products that they want
to buy to opinions about events and well-known
persons. A lot of businesses are interested in how
their services are perceived by their customers.
Therefore, the detection of subjectivity in the
searched information may add the additional value
to the interpretation of the results and their relevan-
cy to the searched topic. The growth of user activi-
</bodyText>
<page confidence="0.986844">
38
</page>
<bodyText confidence="0.999799846153846">
ty on the Web gives substantial amounts of data for
these purposes.
In the context of globalization the possibility to
provide search of opinionated information in dif-
ferent natural languages might be of great interest
to organizations and communities around the
world. Our goal is to design a fully automatic sys-
tem capable of working in a language-independent
manner. In order to compare our approach on dif-
ferent languages we chose English, traditional Chi-
nese and Japanese corpora. As a further possibility
to improve the effectiveness of the language inde-
pendent methods we also consider the additional
application of language dependent techniques spe-
cific to the particular natural language.
The related work in opinion detection is present-
ed in Section 2. We describe our approach in detail
with the three other baselines in Section 3. The
fourth section describes language specific ap-
proach used for the English corpus. In Section 5
we present the evaluation of the three models using
the NTCIR-6 and NTCIR-7 MOAT English, Chi-
nese and Japanese test collections (Seki et al.,
2008). The main findings of our study and future
research possibilities are discussed in the last sec-
tions.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995892">
The focus of our work is to propose a general
approach that can be easily deployed for different
natural languages. This task of opinion detection is
important in many areas of NLP such as
question/answering, information retrieval, docu-
</bodyText>
<note confidence="0.9511695">
Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 38–45,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999963867346939">
ment classification and summarization, and infor-
mation filtering. There are numerous challenges
when trying to solve the task of opinion detection.
Some of them include the fact that the distinction
between opinionated and factual could be denoted
by a single word in the underlying text (e.g., “The
iPhone price is $600.” vs. “The iPhone price is
high.”). Most importantly evaluating whether or
not a given sentence conveys an opinion could be
questionable when judged by different people. Fur-
ther, the opinion classification can be done on dif-
ferent levels, from documents to clauses in the sen-
tence.
We consider the opinion detection task on a sen-
tence level. After retrieving the relevant sentences
using any IR system we automatically classify a
sentence according to two classes: opinionated and
not opinionated (factual). When viewing an opin-
ion-finding task as a classification task, it is usual-
ly considered as a supervised learning problem
where a statistical model performs a learning task
by analyzing a pool of labeled sentences. Two
questions must therefore be solved, namely defin-
ing an effective classification algorithm and deter-
mining pertinent features that might effectively
discriminate between opinionated and factual sen-
tences. From this perspective, during the last
TREC opinion-finding task (Macdonald et al.,
2008) and the last NTCIR-7 workshop (Seki et al.,
2008), a series of suggestions surfaced.
As the language-dependent approach various
teams proposed using Levin defined verb cate-
gories (namely, characterize, declare, conjecture,
admire, judge, assess, say, complain, advise) and
their features (a verb corresponding to a given cat-
egory occurring in the analyzed information item)
that may be pertinent as a classification feature
(Bloom et al., 2007). However, words such as
these cannot always work correctly as clues, for
example with the word “said” in the two sentences
“There were crowds and crowds of people at the
concert, said Ann” and “There were more than
10,000 people at the concert, said Ann.” Both sen-
tences contain the clue word “said” but only the
first one contains an opinion on the target product.
Turney (2002) suggested comparing the frequency
of phrase co-occurrences with words predeter-
mined by the sentiment lexicon. Specific to the
opinion detection in Chinese language Ku et al.
(2006) propose a dictionary-based approach for ex-
traction and summarization. For the Japanese lan-
guage in the last NTCIR-6 and NTCIR-7 work-
shops the opinion finding methods included the use
of supervised machine learning approaches with
specific selection of certain parts-of-speech (POS)
and sentence parts in the form of n-gram features
to improve performance.
There has been a trend in applying language
models for opinion detection task (Lavrenko,
Croft, 2001). Pang &amp; Lee (2004) propose the use
of language models for sentiment analysis task and
subjectivity extraction. Usually, language models
are trained on the labeled data and as an output
they give probabilities of classified tokens belong-
ing to the class. Eguchi &amp; Lavrenko (2006) pro-
pose the use of probabilistic language models for
ranking the results not only by sentiment but also
by the topic relevancy.
As an alternative other teams during the last
TREC and NTCIR evaluation campaigns have sug-
gested variations of Naïve Bayes classifier, lan-
guage models and SVM, along with the use of such
heuristics as word order, punctuation, sentence
length, etc.
We might also mention OpinionFinder (Wilson
et al., 2005), a more complex system that performs
subjectivity analyses to identify opinions as well as
sentiments and other private states (speculations,
dreams, etc.). This system is based on various clas-
sical computational linguistics components (tok-
enization, part-of-speech (POS) tagging (Toutano-
va &amp; Manning, 2000) as well as classification
tools. For example, a Naïve Bayes classifier (Wit-
ten &amp; Frank, 2005) is used to distinguish between
subjective and objective sentences. A rule-based
system is included to identify both speech events
(“said,” “according to”) and direct subjective ex-
pressions (“is happy,” “fears”) within a given sen-
tence. Of course such learning system requires
both a training set and a deeper knowledge of a
given natural language (morphological compo-
nents, syntactic analyses, semantic thesaurus).
The lack of enough training data for the learn-
ing-based systems is clearly a drawback. More-
over, it is difficult to objectively establish when a
complex learning system has enough training data
(and to objectively measure the amount of training
data needed in a complex ML model).
</bodyText>
<page confidence="0.999621">
39
</page>
<sectionHeader confidence="0.987428" genericHeader="method">
3 Language Independent Approaches
</sectionHeader>
<bodyText confidence="0.999952">
In this section we propose our statistical ap-
proach for opinion detection as well as the descrip-
tion of the Naïve Bayes and language model (LM)
baselines.
</bodyText>
<subsectionHeader confidence="0.984758">
3.1 Logistic Model
</subsectionHeader>
<bodyText confidence="0.999933625">
Our system is based on two components: the ex-
traction and weighting of useful features (limited
to isolated words in this study) to allow an effec-
tive classification, and a classification scheme.
First, we present the feature extraction approach in
the Section 3.1.1. Next, we discuss our classifica-
tion model. Sections 3.2 and 3.3 describe the cho-
sen baselines.
</bodyText>
<subsubsectionHeader confidence="0.699722">
3.1.1 Features Extraction
</subsubsectionHeader>
<bodyText confidence="0.854643875">
In order to determine the features that can help
distinguishing between factual and opinionated
documents, we have selected the tokens. As shown
by Kilgarriff (2001), the selection of words (or in
general features) in an effort to characterize a par-
ticular category is a difficult task. The goal is
therefore to design a method capable of selecting
terms that clearly belong to one of the classes. The
approaches that use words and their frequencies or
distributions are usually based on a contingency ta-
ble (see Table 1).
S C-
ω a b a+b
not ω c d c+d
a+c b+d n=a+b+c
+d
</bodyText>
<tableCaption confidence="0.986928">
Table 1. Example of a contingency table.
</tableCaption>
<bodyText confidence="0.999739790697674">
In this table, the letter a represents the number of
occurrences (tokens) of the word ω in the docu-
ment set S (corresponding to a subset of the larger
corpus C in the current study). The letter b denotes
the number of tokens of the same word ω ❑in the
rest of the corpus (denoted C-) while a+b is the to-
tal number of occurrences in the entire corpus (de-
noted C with C=C-∪S). Similarly, a+c indicates
the total number of tokens in S. The entire corpus
C corresponds to the union of the subset S and C-
that contains n tokens (n = a+b+c+d).
Based on the MLE (Maximum Likelihood Esti-
mation) principle the values shown in a contingen-
cy table could be used to estimate various probabil-
ities. For example we might calculate the probabil-
ity of the occurrence of the word ω in the entire
corpus C as Pr(ω) = (a+b)/n or the probability of
finding in C a word belonging to the set S as Pr(S)
= (a+c)/n.
Now to define the discrimination power a term
ω, we suggest defining a weight attached to it ac-
cording to Muller&apos;s method (Muller, 1992). We as-
sume that the distribution of the number of tokens
of the word ω follows a binomial distribution with
the parameters p and n&apos;. The parameter p represent-
ed the probability of drawing a word ω also denot-
ed in the corpus C (or Pr(ω)) and could be estimat-
ed as (a+b)/n. If we repeat this drawing n&apos; = a+c
times, we will have an estimate of the number of
word ω included in the subset S by Pr(ω).n&apos;. On
the other hand, Table 1 gives also the number of
observations of the word ω in S, and this value is
denoted by a. A large difference between a and the
product Pr(ω).n&apos; is clearly an indication that the
presence of a occurrences of the term ω is not due
by chance but corresponds to an intrinsic charac-
teristic of the subset S compared to the subset C-.
In order to obtain a clear rule, we suggest com-
puting the Z score attached to each word ω. If the
mean of a binomial distribution is Pr(ω).n&apos;, its vari-
ance is n&apos;.Pr(ω).(1-Pr(ω)). These two elements are
needed to compute the standard score as described
in Equation 1.
</bodyText>
<equation confidence="0.999019666666667">
Zscore(ω ) = ) (1)
n `Pr( ) (1 Pr( ))
⋅ ω ⋅ −
</equation>
<bodyText confidence="0.999993571428572">
As a decision rule we consider the words having
a Z score between -2 and 2 as terms belonging to a
common vocabulary, as compared to the reference
corpus (as for example “will,” “with,” “many,”
“friend,” or “forced” in our example). This thresh-
old was chosen arbitrary. A word having a Z score
&gt; 2 would be considered as overused (e.g., “that,”
“should,” “must,” “not,” or “government” in
MOAT NTCIR-6 English corpus), while a Z score
&lt; -2 would be interpreted as an underused term
(e.g., “police,” “cell,” “year,” “died,” or “accord-
ing”). The arbitrary threshold limit of 2 corre-
sponds to the limit of the standard normal distribu-
tion, allowing us to find around 5% of the observa-
</bodyText>
<figure confidence="0.8161506">
a n
−
`
⋅
Pr(ω
</figure>
<page confidence="0.970089">
40
</page>
<bodyText confidence="0.9836134">
tions (around 2.5% less than -2 and 2.5% greater
than 2). As shown in Figure 1, the difference be-
tween our arbitrary limit of 2 (drawn in solid line)
and the limits delimiting the 2.5% of the observa-
tions (dotted line) are rather close.
</bodyText>
<figureCaption confidence="0.9965895">
Figure 1. Distribution of the Z score
(MOAT NTCIR-6 English corpus, opinionated).
</figureCaption>
<bodyText confidence="0.999843692307692">
Based on a training sample, we were able to
compute the Z score for different words and retain
only those having a large or small Z score value.
Such a procedure is repeated for all classification
categories (opinionated and factual). It is worth
mentioning that such a general scheme may work
with isolated words (as applied here) or n-gram
(that could be a sequence of either characters or
words), as well as with punctuations or other sym-
bols (numbers, dollar signs), syntactic patterns
(e.g., verb-adjective in comparative or superlative
forms) or other features (presence of proper names,
hyperlinks, etc.)
</bodyText>
<subsubsectionHeader confidence="0.403906">
3.1.2 Classification Model
</subsubsectionHeader>
<bodyText confidence="0.999874710526316">
When our system needs to determine the opin-
ionatedness of a sentence, we first represent this
sentence as a set of words. For each word, we can
then retrieve the Z scores for each category. If all
Z scores for all words are judged as belonging to
the general vocabulary, our classification proce-
dure selects the default category. If not, we may
increase the weight associated with the correspond-
ing category (e.g., for the opinionated class if the
underlying term is overused in this category).
Such a simple additive process could be viewed
as a first classification scheme, selecting the class
having the highest score after enumerating all
words occurring in a sentence. This approach as-
sumes that the word order does not have any im-
pact. We also assume that each sentence has a sim-
ilar length.
For this model, we can define two variables,
namely SumOP indicating the sum of the Z score
of terms overused in opinionated class (i.e. Z score
&gt; 2) and appearing in the input sentence. Similarly,
we can define SumNOOP for the other class. How-
ever, a large SumOP value can be obtained by a
single word or by a set of two (or more) words.
Thus, it could be useful to consider also the num-
ber of words (features) that are overused (or under-
used) in a sentence. Therefore, we can define
#OpOver indicated the number of terms in the
evaluated sentence that tends to be overused in
opinionated documents (i.e. Z score &gt; 2) while
#OpUnder indicated the number of terms that
tends to be underused in the class of opinionated
documents (i.e. Z score &lt; -2). Similarly, we can
define the variables #NoopOver, #NoopUnder, but
for the non-opinionated category.
With these additional explanatory variables, we
can compute the corresponding subjectivity score
for each sentence as follows:
</bodyText>
<equation confidence="0.7630425">
# OpOver
Noop score
_ = # NoopOver
+ #OpUnder (2)
# NoopOver
+ #NoopUnder
</equation>
<bodyText confidence="0.9999672">
As a better way to combine different judgments
we suggest following Le Calvé &amp; Savoy (2000)
and normalize the scores using the logistic regres-
sion. The logistic transformation π(x) given by
each logistic regression model is defined as:
</bodyText>
<equation confidence="0.995216666666667">
π (x) = e (3)
β0+∑k
i=1βixi
</equation>
<bodyText confidence="0.999872">
where βi are the coefficients obtained from the fit-
ting, xi are the variables, and k is the number of ex-
planatory variables. These coefficients reflect the
relative importance of each variable in the final
score.
For each sentence, we can compute the π(x) cor-
responding to the two possible categories and the
final decision is simply to classify the sentence ac-
cording to the max π(x) value. This approach takes
account of the fact that some explanatory variables
may have more importance than other in assigning
the correct category.
</bodyText>
<figure confidence="0.914802375">
Op score
_ =
# OpOver
β0 k
+ ∑ i i i
β x
= 1
1+ e
</figure>
<page confidence="0.995158">
41
</page>
<subsectionHeader confidence="0.960033">
3.2 Naïve Bayes
</subsectionHeader>
<bodyText confidence="0.999941214285714">
For comparison with our logistic model we
chose three baselines: Naïve Bayes and language
model and finding significant collocations. Despite
its simplicity Naïve Bayes classifier tends to per-
form relatively well for various text categorization
problems (Witten, Frank, 2005). In accordance
with our approach, we used word tokens as classi-
fication features for the English corpora. For the
Chinese and Japanese languages overlapping bi-
gram approach was used (Savoy, 2005). The train-
ing method estimates the relative frequency of the
probability that the chosen feature belongs to a
specific category using add-one smoothing tech-
nique.
</bodyText>
<subsectionHeader confidence="0.939981">
3.3 Language Model (LM)
</subsectionHeader>
<bodyText confidence="0.999924153846154">
As a second baseline we use the classification
based on the language model using overlapping n-
gram sequences (n was set to 8) as suggested by
Pang &amp; Lee (2004, 2005) for the English language.
Using the overlapping 4-gram sequence for the
word “company”, we obtain: “comp”, “ompa”,
“mpan”, etc. For the Chinese and Japanese corpora
bigram approach was applied. As in Naïve Bayes,
the language model gives the probability of the
sentence belonging to a specific class. Working
with relatively large n allows a lot of word tokens
to be processed as is, at least for the English lan-
guage.
</bodyText>
<subsectionHeader confidence="0.957651">
3.4 Significant Collocations (SC)
</subsectionHeader>
<bodyText confidence="0.999964555555556">
Another promising approach among the super-
vised learning schemes is the use of collocations of
two or more words or features (Manning &amp;
Schütze, 2000). This method allows classification
of instances based on significant collocations
learned from the labeled data. Some examples of
the frequent collocations in the corpora would be
“in the”, “of the”. The idea of the method is to find
significant collocations (SC) that occur more in the
opinionated corpus than in the non-opinionated
one. In order to do so the model returns the collo-
cations of two words for the English language
based on the degree to which their counts in the
opinionated corpus exceed their expected counts in
the not opinionated one. As an example for the En-
glish opinionated corpus the following collocations
were found: “are worried”, “pleaded guilty”, “ea-
ger to”, “expressed hope”. Clearly, overlooking the
list of new found collocations it is possible to
judge their relevancy. However, it is not clear how
to use this method with the Chinese and Japanese
texts, since these languages do not have white
space or other usual delimiters as in English. In or-
der to solve the problem of feature selection we
chose bigram indexing on the Chinese and
Japanese corpora and searched for significant new
collocations of bigrams.
</bodyText>
<sectionHeader confidence="0.990888" genericHeader="method">
4 Language Dependent Approach
</sectionHeader>
<bodyText confidence="0.999984173913043">
As the language dependent technique to improve
the obtained classification results we suggest the
use of SentiWordNet for the English language
(Esuli &amp; Sebastiani, 2006). Since the vocabulary of
words in SentiWordNet is quite limited it is not al-
ways clear how to combine the objectivity scores.
The SentiWordNet score was computed in the
following way: to define the opinionated score of
the sentence the sum of scores representing that the
word belongs to opinionated category for each
word in the sentence is calculated. The not opin-
ionated score of the sentence is computed in the
same way with the difference that it is divided by
the number of words in the sentence. Thus, if opin-
ionated score is more than not opinionated one,
there is an opinion, otherwise not. This is a heuris-
tic approach that intuitively takes account of the ra-
tionalization that there are more not opinionated
words than opinionated in the sentence. At the
same time the presence of opinionated word
weighs more than the presence of the not opinion-
ated ones. Especially, this approach seems to give
good result.
</bodyText>
<sectionHeader confidence="0.998565" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999947">
The experiment was carried out on the NTCIR-6
and NTCIR-7 English news corpora using 10-fold
cross-validation model on a lenient evaluation
standard as described in Seki et al. We do not
question the construction and structure of opinions
in this data set, since those questions were ad-
dressed at the NTCIR workshops. Using the Chi-
nese and Japanese corpora we can verify the quali-
ty of the suggested language-independent ap-
proaches.
</bodyText>
<page confidence="0.997539">
42
</page>
<subsectionHeader confidence="0.951951">
5.1 Feature Selection &amp; Evaluation in English
</subsectionHeader>
<bodyText confidence="0.99980775">
For the evaluation of sentences in English, the
assumption of isolated words (bag-of-words) pre-
viously stemmed was used by our system. The cor-
pora are comprised of more than 13,400 sentences,
4,859 (36.3%) of which are opinionated. As the
evaluation metrics precision, recall and F1-measure
were used based on gold standard evaluation pro-
vided by NTCIR workshops (Seki et al., 2008).
The precision and recall are weighted equally in
our experiment but it should be recognized that
based on the system&apos;s needs and focus there could
be more accent on precision or recall.
</bodyText>
<table confidence="0.9997462">
Model Precision Recall F1-measure
Logistic model 0.583 0.508 0.543
Naïve Bayes 0.415 0.364 0.388
LM 0.350 0.339 0.343
SC 0.979 0.360 0.527
</table>
<tableCaption confidence="0.978745">
Table 2. Evaluation results of 10-fold cross-valida-
tion on NTCIR-6 and NTCIR-7 English corpora.
</tableCaption>
<bodyText confidence="0.998050666666667">
Comparing the results in Table 2 to the baselines
of the Naïve Bayes classifier and LM evaluated on
the same training and testing sets, we see that lo-
gistic model outperforms the baselines. In our
opinion, this is due to the use of more explanatory
variables that better discriminate between opinion-
ated and factual sentences.
The use of language dependent techniques on
the other hand might further improve the results.
Especially, this seems promising observing the re-
sults when using the SentiWordNet on the English
corpus. In Table 3 one can see that the first three
models show improvement. Specifically, the preci-
sion of the logistic model increased from 0.583 to
0.766 (by 31.4%).
</bodyText>
<table confidence="0.9993228">
Model Precision Recall F1-measure
Logistic model 0.766 0.488 0.597
Naïve Bayes 0.667 0.486 0.562
LM 0.611 0.474 0.534
SC 0.979 0.420 0.588
</table>
<tableCaption confidence="0.980972333333333">
Table 3. Evaluation results of 10-fold cross-valida-
tion on NTCIR-6 and NTCIR-7 English corpora
with SentiWordNet.
</tableCaption>
<bodyText confidence="0.999977857142857">
When considering the F1-measure, the impact of
the language-dependent approach shows 9% of im-
provement, from 0.543 to 0.597.
The way that we incorporated the scores provid-
ed by SentiWordNet was done with the help of lin-
ear combination and normalization of scores for
each of the models.
</bodyText>
<subsectionHeader confidence="0.993398">
5.2 Feature Selection &amp; Evaluation in Chinese
</subsectionHeader>
<bodyText confidence="0.999969192307692">
We have assumed until now that words can be
extracted from a sentence in order to define the
needed features used to determine if the underlying
information item conveys an opinion or not. Work-
ing with the Chinese language this assumption
does no longer hold. Therefore, we need to deter-
mine indexing units by either applying an automat-
ing segmentation approach (based on either a mor-
phological (e.g., CSeg&amp;Tag) or a statistical
method (Murata &amp; Isahara, 2003)) or considering
n-gram indexing approach (unigram or bigram, for
example). Finally we may also consider a combi-
nation of both n-gram and word-based indexing
strategies.
Based on the work of Savoy, 2005 we experi-
mented with overlapping bigram and trigram in-
dexing schemes for Chinese. The experimental re-
sults show that bigram indexing outperforms tri-
gram on all three considered statistical methods.
Therefore, as features for Chinese we used over-
lapping bigrams.
The NTCIR-6 and NTCIR-7 Chinese corpora
consisted of more than 14,507 sentences, 9960
(68.7%) of which are opinionated. The results of
all three statistical models performed on the Chi-
nese corpora are presented in Table 4.
</bodyText>
<table confidence="0.9996092">
Model Precision Recall F1-measure
Logistic model 0.943 0.730 0.823
Naïve Bayes 0.729 0.538 0.619
LM 0.581 0.634 0.606
SC 0.313 0.898 0.464
</table>
<tableCaption confidence="0.927884333333333">
Table 4. Evaluation results of 10-fold cross-valida-
tion on NTCIR-6 and NTCIR-7 Chinese
corpora.
</tableCaption>
<bodyText confidence="0.999489285714286">
From the results in Table 4 we clearly see that
our approach gives better performance and con-
firms the results presented in Tables 2 and 3. The
significant improvement in scores could be due to
the fact that Chinese corpus contains more opin-
ionated sentences in relevance to not opinionated
once. Thus, the training set for opinionated classi-
</bodyText>
<page confidence="0.999249">
43
</page>
<bodyText confidence="0.9999578">
fication was much larger compared to the English
language. This proves the relevance of more train-
ing data for the learning-based systems. But the di-
rect comparison with the results on the English
corpus is not possible.
</bodyText>
<subsectionHeader confidence="0.970168">
5.3 Feature Selection &amp; Evaluation in Japanese
</subsectionHeader>
<bodyText confidence="0.9959872">
As with the Chinese language we face the same
challenges in feature definition for the Japanese
language. After experimenting with bigram and tri-
gram we chose bigram strategy for indexing and
feature selection.
The NTCIR-6 and NTCIR-7 Japanese corpora
consisted of more than 11,100 sentences with
4,622 opinionated sentences (representing 41.6%
of the corpus). The results of the statistical models
are shown in Table 5.
</bodyText>
<table confidence="0.9995992">
Model Precision Recall F-measure
Logistic model 0.527 0.761 0.623
Naïve Bayes 0.565 0.570 0.567
LM 0.657 0.667 0.662
SC 0.663 0.856 0.747
</table>
<tableCaption confidence="0.925814">
Table 5. Evaluation results of 10-fold cross-valida-
tion on NTCIR-6 and NTCIR-7 Japanese
corpora.
</tableCaption>
<bodyText confidence="0.999937571428571">
From the results we can see that the significant
collocations model outperforms the others. This
could be due to the fewer number of opinionated
sentences compared to the Chinese or English cor-
pora. This tends to indicate the necessity of an ex-
tensive training data for the logistic model in order
to provide reliable opinion estimates.
</bodyText>
<sectionHeader confidence="0.997755" genericHeader="discussions">
6 Future Work and Conclusion
</sectionHeader>
<bodyText confidence="0.9996558">
In this paper we presented our language-inde-
pendent approach based on using Z scores and the
logistic model to identify those terms that ade-
quately characterize subsets of the corpus belong-
ing to opinionated or non-opinionated classes. In
this selection, we focused only on the statistical as-
pect (distribution difference) of words or bigrams.
Our approach was compared to the three baselines,
namely Naïve Bayes classifier, language model
and an approach based on finding significant collo-
cations. We have also demonstrated on the English
corpora how we can use the language dependent
techniques to identify the possibility of opinion ex-
pressed in the sentences that otherwise were classi-
fied as not opinionated by the system. The use of
SentiWordNet (Esuli &amp; Sebastiani, 2006) in com-
bination with our methods yields better results for
the English language.
This study was limited to isolated words in En-
glish corpus but in further research we could easily
consider longer word sequences to include both
noun and verb phrases. The most useful terms
would also then be added to the query to improve
the rank of opinionated documents. As another ap-
proach, we could use the evaluation of co-occur-
rence terms of pronouns “I” and “you” mainly with
verbs (e.g., “believe,” “feel,” “think,” “hate”) using
part of speech tagging techniques in order to boost
the rank of retrieved items.
Using freely available POS taggers, we could
take POS information into account (Toutanova &amp;
Mannning, 2004) and hopefully develop a better
classifier. For example, the presence of proper
names and their frequency or distribution might
help us classify a document as being opinionated
or not. The presence of adjectives and adverbs, to-
gether with their superlative (e.g., best, most) or
comparative (e.g., greater, more) forms could also
be useful hints regarding the presence of opinionat-
ed versus factual information.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996565">
We would like to thank the MOAT task organiz-
ers at NTCIR-7 for their valuable work.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999658777777778">
Bloom, K., Stein, S., &amp; Argamon, S. 2007. Appraisal
extraction for news opinion analysis at NTCIR-6.
Proceedings NTCIR-6, NII, Tokyo, pp. 279-289.
Eguchi, K., Lavrenko, V. 2006. Sentiment retrieval us-
ing generative models. Proceedings of EMNLP, Syd-
ney, pp. 345-354.
Esuli, A., Sebastiani, F. 2006. SentiWordNet: A publicly
available lexical resource for opinion mining. Pro-
ceedings LREC’06, Genoa.
Kilgarriff, A. 2001. Comparing corpora. International
Journal of Corpus Linguistics, 6(1):97-133.
Ku, L.-W., Liang, Y.-T., Chen, H.-H. 2006. Opinion ex-
traction, summarization and tracking in news and
blog corpora. Proceedings of AAAI-2006 Spring
Symposium on Computational Approaches to Ana-
lyzing Weblogs, pp. 100-107.
Lavrenko, V., Croft, W.B. 2001. Relevance-based lanu-
age models. SIGIR, New Orleans, LA, pp. 120-127.
</reference>
<page confidence="0.979656">
44
</page>
<reference confidence="0.999558583333333">
Le Calvé, A., Savoy, J. 2000. Database merging strat-
egy based on logistic regression. Information Pro-
cessing &amp; Management, 36(3):341-359.
Macdonald, C., Ounis, I., &amp; Soboroff, I. 2008.
Overview of the TREC-2007 blog track. In Proceed-
ings TREC-2007, NIST Publication #500-274, pp.
1-13.
Manning, C. D., SchUtze, H. 2000. Foundations of Sta-
tistical Natural Language Processing. MIT Press.
Muller, C. 1992. Principes et méthodes de statistique
lexicale. Champion, Paris.
Murata, M., Ma, Q., &amp; Isahara, H. 2003. Applying mul-
tiple characteristics and techniques to obtain high
levels of performance in information retrieval. Pro-
ceedings of NTCIR-3, NII, Tokyo.
Pang, B., Lee, L. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization
based on minimum cuts. Proceedings of ACL,
Barcelona, pp. 271-278.
Pang, B., Lee, L. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the Associa-
tion for Computational Linguistics (ACL), pp.
115-124.
Savoy, J. 2005. Comparative study of monolingual
search models for use with asian languages. ACM
Transactions on Asian Language Information Pro-
cessing, 4(2):163-189.
Seki, Y., Evans, D. K., Ku, L.-W., Sun, L., Chen, H.-H.,
&amp; Kando, N. 2008. Overview of multilingual opin-
ion analysis task at NTCIR-7. Proceedings NTCIR-7,
NII, Tokyo, pp. 185-203.
Toutanova, K., &amp; Manning, C. 2000. Enriching the
Knowledge Sources Used in a Maximum Entropy
Part-of-Speech Tagging. Proceedings EMNLP /
VLC-2000, Hong Kong, pp. 63-70.
Turney, P. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of
reviews. Proceedings of the ACL, Philadelphia (PA),
pp. 417-424.
Wilson, T., Hoffmann, P., Somasundaran, S., Kessler,
J., Wiebe, J., Choi, Y., Cardie, C., Riloff, E., &amp; Pat-
wardhan, S., 2005. OpinionFinder: A system for
subjectivity analysis. Proceedings HLT/EMNLP,
Vancouver (BC), pp. 34-35.
Witten, I.A., &amp; Frank, E. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco (CA).
</reference>
<page confidence="0.999389">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.758779">
<title confidence="0.996394">Investigation in Statistical Language-Independent Approaches Opinion Detection in English, Chinese and Japanese</title>
<author confidence="0.997205">Olena Zubaryeva Jacques Savoy</author>
<affiliation confidence="0.999806">Institute of Informatics Institute of Informatics University of Neuchâtel University of Neuchâtel</affiliation>
<address confidence="0.990511">Emile-Argand, 11, 2009 Switzerland Emile-Argand, 11, 2009 Switzerland</address>
<email confidence="0.856992">olena.zubaryeva@unine.chjacques.savoy@unine.ch</email>
<abstract confidence="0.993059666666667">In this paper we present a new statistical approach to opinion detection and its’ evaluation on the English, Chinese and Japanese corpora. Besides, the proposed method is compared with three baselines, namely Naïve Bayes classifier, a language model and an approach based on significant collocations. These models being language independent are improved with the use of language-dependent technique on the example of the English corpus. We show that our method almost always gives better performance compared to the considered baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Bloom</author>
<author>S Stein</author>
<author>S Argamon</author>
</authors>
<title>Appraisal extraction for news opinion analysis at NTCIR-6.</title>
<date>2007</date>
<booktitle>Proceedings NTCIR-6,</booktitle>
<pages>279--289</pages>
<location>NII, Tokyo,</location>
<contexts>
<context position="5071" citStr="Bloom et al., 2007" startWordPosition="780" endWordPosition="783">tures that might effectively discriminate between opinionated and factual sentences. From this perspective, during the last TREC opinion-finding task (Macdonald et al., 2008) and the last NTCIR-7 workshop (Seki et al., 2008), a series of suggestions surfaced. As the language-dependent approach various teams proposed using Levin defined verb categories (namely, characterize, declare, conjecture, admire, judge, assess, say, complain, advise) and their features (a verb corresponding to a given category occurring in the analyzed information item) that may be pertinent as a classification feature (Bloom et al., 2007). However, words such as these cannot always work correctly as clues, for example with the word “said” in the two sentences “There were crowds and crowds of people at the concert, said Ann” and “There were more than 10,000 people at the concert, said Ann.” Both sentences contain the clue word “said” but only the first one contains an opinion on the target product. Turney (2002) suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon. Specific to the opinion detection in Chinese language Ku et al. (2006) propose a dictionary-based approach fo</context>
</contexts>
<marker>Bloom, Stein, Argamon, 2007</marker>
<rawString>Bloom, K., Stein, S., &amp; Argamon, S. 2007. Appraisal extraction for news opinion analysis at NTCIR-6. Proceedings NTCIR-6, NII, Tokyo, pp. 279-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Eguchi</author>
<author>V Lavrenko</author>
</authors>
<title>Sentiment retrieval using generative models.</title>
<date>2006</date>
<booktitle>Proceedings of EMNLP, Sydney,</booktitle>
<pages>345--354</pages>
<contexts>
<context position="6369" citStr="Eguchi &amp; Lavrenko (2006)" startWordPosition="988" endWordPosition="991">R-6 and NTCIR-7 workshops the opinion finding methods included the use of supervised machine learning approaches with specific selection of certain parts-of-speech (POS) and sentence parts in the form of n-gram features to improve performance. There has been a trend in applying language models for opinion detection task (Lavrenko, Croft, 2001). Pang &amp; Lee (2004) propose the use of language models for sentiment analysis task and subjectivity extraction. Usually, language models are trained on the labeled data and as an output they give probabilities of classified tokens belonging to the class. Eguchi &amp; Lavrenko (2006) propose the use of probabilistic language models for ranking the results not only by sentiment but also by the topic relevancy. As an alternative other teams during the last TREC and NTCIR evaluation campaigns have suggested variations of Naïve Bayes classifier, language models and SVM, along with the use of such heuristics as word order, punctuation, sentence length, etc. We might also mention OpinionFinder (Wilson et al., 2005), a more complex system that performs subjectivity analyses to identify opinions as well as sentiments and other private states (speculations, dreams, etc.). This sys</context>
</contexts>
<marker>Eguchi, Lavrenko, 2006</marker>
<rawString>Eguchi, K., Lavrenko, V. 2006. Sentiment retrieval using generative models. Proceedings of EMNLP, Sydney, pp. 345-354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>Proceedings LREC’06,</booktitle>
<location>Genoa.</location>
<contexts>
<context position="18357" citStr="Esuli &amp; Sebastiani, 2006" startWordPosition="3050" endWordPosition="3053">looking the list of new found collocations it is possible to judge their relevancy. However, it is not clear how to use this method with the Chinese and Japanese texts, since these languages do not have white space or other usual delimiters as in English. In order to solve the problem of feature selection we chose bigram indexing on the Chinese and Japanese corpora and searched for significant new collocations of bigrams. 4 Language Dependent Approach As the language dependent technique to improve the obtained classification results we suggest the use of SentiWordNet for the English language (Esuli &amp; Sebastiani, 2006). Since the vocabulary of words in SentiWordNet is quite limited it is not always clear how to combine the objectivity scores. The SentiWordNet score was computed in the following way: to define the opinionated score of the sentence the sum of scores representing that the word belongs to opinionated category for each word in the sentence is calculated. The not opinionated score of the sentence is computed in the same way with the difference that it is divided by the number of words in the sentence. Thus, if opinionated score is more than not opinionated one, there is an opinion, otherwise not.</context>
<context position="25650" citStr="Esuli &amp; Sebastiani, 2006" startWordPosition="4235" endWordPosition="4238">cterize subsets of the corpus belonging to opinionated or non-opinionated classes. In this selection, we focused only on the statistical aspect (distribution difference) of words or bigrams. Our approach was compared to the three baselines, namely Naïve Bayes classifier, language model and an approach based on finding significant collocations. We have also demonstrated on the English corpora how we can use the language dependent techniques to identify the possibility of opinion expressed in the sentences that otherwise were classified as not opinionated by the system. The use of SentiWordNet (Esuli &amp; Sebastiani, 2006) in combination with our methods yields better results for the English language. This study was limited to isolated words in English corpus but in further research we could easily consider longer word sequences to include both noun and verb phrases. The most useful terms would also then be added to the query to improve the rank of opinionated documents. As another approach, we could use the evaluation of co-occurrence terms of pronouns “I” and “you” mainly with verbs (e.g., “believe,” “feel,” “think,” “hate”) using part of speech tagging techniques in order to boost the rank of retrieved items</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, A., Sebastiani, F. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. Proceedings LREC’06, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>Comparing corpora.</title>
<date>2001</date>
<journal>International Journal of Corpus Linguistics,</journal>
<pages>6--1</pages>
<contexts>
<context position="8667" citStr="Kilgarriff (2001)" startWordPosition="1350" endWordPosition="1351">ve Bayes and language model (LM) baselines. 3.1 Logistic Model Our system is based on two components: the extraction and weighting of useful features (limited to isolated words in this study) to allow an effective classification, and a classification scheme. First, we present the feature extraction approach in the Section 3.1.1. Next, we discuss our classification model. Sections 3.2 and 3.3 describe the chosen baselines. 3.1.1 Features Extraction In order to determine the features that can help distinguishing between factual and opinionated documents, we have selected the tokens. As shown by Kilgarriff (2001), the selection of words (or in general features) in an effort to characterize a particular category is a difficult task. The goal is therefore to design a method capable of selecting terms that clearly belong to one of the classes. The approaches that use words and their frequencies or distributions are usually based on a contingency table (see Table 1). S Cω a b a+b not ω c d c+d a+c b+d n=a+b+c +d Table 1. Example of a contingency table. In this table, the letter a represents the number of occurrences (tokens) of the word ω in the document set S (corresponding to a subset of the larger corp</context>
</contexts>
<marker>Kilgarriff, 2001</marker>
<rawString>Kilgarriff, A. 2001. Comparing corpora. International Journal of Corpus Linguistics, 6(1):97-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-W Ku</author>
<author>Y-T Liang</author>
<author>H-H Chen</author>
</authors>
<title>Opinion extraction, summarization and tracking in news and blog corpora.</title>
<date>2006</date>
<booktitle>Proceedings of AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs,</booktitle>
<pages>100--107</pages>
<contexts>
<context position="5632" citStr="Ku et al. (2006)" startWordPosition="874" endWordPosition="877">tinent as a classification feature (Bloom et al., 2007). However, words such as these cannot always work correctly as clues, for example with the word “said” in the two sentences “There were crowds and crowds of people at the concert, said Ann” and “There were more than 10,000 people at the concert, said Ann.” Both sentences contain the clue word “said” but only the first one contains an opinion on the target product. Turney (2002) suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon. Specific to the opinion detection in Chinese language Ku et al. (2006) propose a dictionary-based approach for extraction and summarization. For the Japanese language in the last NTCIR-6 and NTCIR-7 workshops the opinion finding methods included the use of supervised machine learning approaches with specific selection of certain parts-of-speech (POS) and sentence parts in the form of n-gram features to improve performance. There has been a trend in applying language models for opinion detection task (Lavrenko, Croft, 2001). Pang &amp; Lee (2004) propose the use of language models for sentiment analysis task and subjectivity extraction. Usually, language models are t</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>Ku, L.-W., Liang, Y.-T., Chen, H.-H. 2006. Opinion extraction, summarization and tracking in news and blog corpora. Proceedings of AAAI-2006 Spring Symposium on Computational Approaches to Analyzing Weblogs, pp. 100-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>Relevance-based lanuage models.</title>
<date>2001</date>
<pages>120--127</pages>
<publisher>SIGIR,</publisher>
<location>New Orleans, LA,</location>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>Lavrenko, V., Croft, W.B. 2001. Relevance-based lanuage models. SIGIR, New Orleans, LA, pp. 120-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Le Calvé</author>
<author>J Savoy</author>
</authors>
<title>Database merging strategy based on logistic regression.</title>
<date>2000</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>36--3</pages>
<marker>Le Calvé, Savoy, 2000</marker>
<rawString>Le Calvé, A., Savoy, J. 2000. Database merging strategy based on logistic regression. Information Processing &amp; Management, 36(3):341-359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Macdonald</author>
<author>I Ounis</author>
<author>I Soboroff</author>
</authors>
<title>Overview of the TREC-2007 blog track.</title>
<date>2008</date>
<booktitle>In Proceedings TREC-2007, NIST Publication #500-274,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="4626" citStr="Macdonald et al., 2008" startWordPosition="714" endWordPosition="717">ystem we automatically classify a sentence according to two classes: opinionated and not opinionated (factual). When viewing an opinion-finding task as a classification task, it is usually considered as a supervised learning problem where a statistical model performs a learning task by analyzing a pool of labeled sentences. Two questions must therefore be solved, namely defining an effective classification algorithm and determining pertinent features that might effectively discriminate between opinionated and factual sentences. From this perspective, during the last TREC opinion-finding task (Macdonald et al., 2008) and the last NTCIR-7 workshop (Seki et al., 2008), a series of suggestions surfaced. As the language-dependent approach various teams proposed using Levin defined verb categories (namely, characterize, declare, conjecture, admire, judge, assess, say, complain, advise) and their features (a verb corresponding to a given category occurring in the analyzed information item) that may be pertinent as a classification feature (Bloom et al., 2007). However, words such as these cannot always work correctly as clues, for example with the word “said” in the two sentences “There were crowds and crowds o</context>
</contexts>
<marker>Macdonald, Ounis, Soboroff, 2008</marker>
<rawString>Macdonald, C., Ounis, I., &amp; Soboroff, I. 2008. Overview of the TREC-2007 blog track. In Proceedings TREC-2007, NIST Publication #500-274, pp. 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H SchUtze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<marker>Manning, SchUtze, 2000</marker>
<rawString>Manning, C. D., SchUtze, H. 2000. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Muller</author>
</authors>
<title>Principes et méthodes de statistique lexicale.</title>
<date>1992</date>
<location>Champion, Paris.</location>
<contexts>
<context position="10139" citStr="Muller, 1992" startWordPosition="1630" endWordPosition="1631">er of tokens in S. The entire corpus C corresponds to the union of the subset S and Cthat contains n tokens (n = a+b+c+d). Based on the MLE (Maximum Likelihood Estimation) principle the values shown in a contingency table could be used to estimate various probabilities. For example we might calculate the probability of the occurrence of the word ω in the entire corpus C as Pr(ω) = (a+b)/n or the probability of finding in C a word belonging to the set S as Pr(S) = (a+c)/n. Now to define the discrimination power a term ω, we suggest defining a weight attached to it according to Muller&apos;s method (Muller, 1992). We assume that the distribution of the number of tokens of the word ω follows a binomial distribution with the parameters p and n&apos;. The parameter p represented the probability of drawing a word ω also denoted in the corpus C (or Pr(ω)) and could be estimated as (a+b)/n. If we repeat this drawing n&apos; = a+c times, we will have an estimate of the number of word ω included in the subset S by Pr(ω).n&apos;. On the other hand, Table 1 gives also the number of observations of the word ω in S, and this value is denoted by a. A large difference between a and the product Pr(ω).n&apos; is clearly an indication th</context>
</contexts>
<marker>Muller, 1992</marker>
<rawString>Muller, C. 1992. Principes et méthodes de statistique lexicale. Champion, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Murata</author>
<author>Q Ma</author>
<author>H Isahara</author>
</authors>
<title>Applying multiple characteristics and techniques to obtain high levels of performance in information retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of NTCIR-3,</booktitle>
<location>NII, Tokyo.</location>
<marker>Murata, Ma, Isahara, 2003</marker>
<rawString>Murata, M., Ma, Q., &amp; Isahara, H. 2003. Applying multiple characteristics and techniques to obtain high levels of performance in information retrieval. Proceedings of NTCIR-3, NII, Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>Proceedings of ACL,</booktitle>
<pages>271--278</pages>
<location>Barcelona,</location>
<contexts>
<context position="6109" citStr="Pang &amp; Lee (2004)" startWordPosition="947" endWordPosition="950">ase co-occurrences with words predetermined by the sentiment lexicon. Specific to the opinion detection in Chinese language Ku et al. (2006) propose a dictionary-based approach for extraction and summarization. For the Japanese language in the last NTCIR-6 and NTCIR-7 workshops the opinion finding methods included the use of supervised machine learning approaches with specific selection of certain parts-of-speech (POS) and sentence parts in the form of n-gram features to improve performance. There has been a trend in applying language models for opinion detection task (Lavrenko, Croft, 2001). Pang &amp; Lee (2004) propose the use of language models for sentiment analysis task and subjectivity extraction. Usually, language models are trained on the labeled data and as an output they give probabilities of classified tokens belonging to the class. Eguchi &amp; Lavrenko (2006) propose the use of probabilistic language models for ranking the results not only by sentiment but also by the topic relevancy. As an alternative other teams during the last TREC and NTCIR evaluation campaigns have suggested variations of Naïve Bayes classifier, language models and SVM, along with the use of such heuristics as word order</context>
<context position="16401" citStr="Pang &amp; Lee (2004" startWordPosition="2732" endWordPosition="2735">tively well for various text categorization problems (Witten, Frank, 2005). In accordance with our approach, we used word tokens as classification features for the English corpora. For the Chinese and Japanese languages overlapping bigram approach was used (Savoy, 2005). The training method estimates the relative frequency of the probability that the chosen feature belongs to a specific category using add-one smoothing technique. 3.3 Language Model (LM) As a second baseline we use the classification based on the language model using overlapping ngram sequences (n was set to 8) as suggested by Pang &amp; Lee (2004, 2005) for the English language. Using the overlapping 4-gram sequence for the word “company”, we obtain: “comp”, “ompa”, “mpan”, etc. For the Chinese and Japanese corpora bigram approach was applied. As in Naïve Bayes, the language model gives the probability of the sentence belonging to a specific class. Working with relatively large n allows a lot of word tokens to be processed as is, at least for the English language. 3.4 Significant Collocations (SC) Another promising approach among the supervised learning schemes is the use of collocations of two or more words or features (Manning &amp; Sch</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, B., Lee, L. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. Proceedings of ACL, Barcelona, pp. 271-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>115--124</pages>
<marker>Pang, Lee, 2005</marker>
<rawString>Pang, B., Lee, L. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the Association for Computational Linguistics (ACL), pp. 115-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Savoy</author>
</authors>
<title>Comparative study of monolingual search models for use with asian languages.</title>
<date>2005</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<pages>4--2</pages>
<contexts>
<context position="16055" citStr="Savoy, 2005" startWordPosition="2675" endWordPosition="2676"> may have more importance than other in assigning the correct category. Op score _ = # OpOver β0 k + ∑ i i i β x = 1 1+ e 41 3.2 Naïve Bayes For comparison with our logistic model we chose three baselines: Naïve Bayes and language model and finding significant collocations. Despite its simplicity Naïve Bayes classifier tends to perform relatively well for various text categorization problems (Witten, Frank, 2005). In accordance with our approach, we used word tokens as classification features for the English corpora. For the Chinese and Japanese languages overlapping bigram approach was used (Savoy, 2005). The training method estimates the relative frequency of the probability that the chosen feature belongs to a specific category using add-one smoothing technique. 3.3 Language Model (LM) As a second baseline we use the classification based on the language model using overlapping ngram sequences (n was set to 8) as suggested by Pang &amp; Lee (2004, 2005) for the English language. Using the overlapping 4-gram sequence for the word “company”, we obtain: “comp”, “ompa”, “mpan”, etc. For the Chinese and Japanese corpora bigram approach was applied. As in Naïve Bayes, the language model gives the prob</context>
<context position="22516" citStr="Savoy, 2005" startWordPosition="3737" endWordPosition="3738">ed from a sentence in order to define the needed features used to determine if the underlying information item conveys an opinion or not. Working with the Chinese language this assumption does no longer hold. Therefore, we need to determine indexing units by either applying an automating segmentation approach (based on either a morphological (e.g., CSeg&amp;Tag) or a statistical method (Murata &amp; Isahara, 2003)) or considering n-gram indexing approach (unigram or bigram, for example). Finally we may also consider a combination of both n-gram and word-based indexing strategies. Based on the work of Savoy, 2005 we experimented with overlapping bigram and trigram indexing schemes for Chinese. The experimental results show that bigram indexing outperforms trigram on all three considered statistical methods. Therefore, as features for Chinese we used overlapping bigrams. The NTCIR-6 and NTCIR-7 Chinese corpora consisted of more than 14,507 sentences, 9960 (68.7%) of which are opinionated. The results of all three statistical models performed on the Chinese corpora are presented in Table 4. Model Precision Recall F1-measure Logistic model 0.943 0.730 0.823 Naïve Bayes 0.729 0.538 0.619 LM 0.581 0.634 0.</context>
</contexts>
<marker>Savoy, 2005</marker>
<rawString>Savoy, J. 2005. Comparative study of monolingual search models for use with asian languages. ACM Transactions on Asian Language Information Processing, 4(2):163-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Seki</author>
<author>D K Evans</author>
<author>L-W Ku</author>
<author>L Sun</author>
<author>H-H Chen</author>
<author>N Kando</author>
</authors>
<title>Overview of multilingual opinion analysis task at NTCIR-7.</title>
<date>2008</date>
<booktitle>Proceedings NTCIR-7,</booktitle>
<pages>185--203</pages>
<location>NII, Tokyo,</location>
<contexts>
<context position="2753" citStr="Seki et al., 2008" startWordPosition="428" endWordPosition="431">panese corpora. As a further possibility to improve the effectiveness of the language independent methods we also consider the additional application of language dependent techniques specific to the particular natural language. The related work in opinion detection is presented in Section 2. We describe our approach in detail with the three other baselines in Section 3. The fourth section describes language specific approach used for the English corpus. In Section 5 we present the evaluation of the three models using the NTCIR-6 and NTCIR-7 MOAT English, Chinese and Japanese test collections (Seki et al., 2008). The main findings of our study and future research possibilities are discussed in the last sections. 2 Related Work The focus of our work is to propose a general approach that can be easily deployed for different natural languages. This task of opinion detection is important in many areas of NLP such as question/answering, information retrieval, docuProceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 38–45, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics ment classification and summarization, and information filtering. T</context>
<context position="4676" citStr="Seki et al., 2008" startWordPosition="723" endWordPosition="726"> two classes: opinionated and not opinionated (factual). When viewing an opinion-finding task as a classification task, it is usually considered as a supervised learning problem where a statistical model performs a learning task by analyzing a pool of labeled sentences. Two questions must therefore be solved, namely defining an effective classification algorithm and determining pertinent features that might effectively discriminate between opinionated and factual sentences. From this perspective, during the last TREC opinion-finding task (Macdonald et al., 2008) and the last NTCIR-7 workshop (Seki et al., 2008), a series of suggestions surfaced. As the language-dependent approach various teams proposed using Levin defined verb categories (namely, characterize, declare, conjecture, admire, judge, assess, say, complain, advise) and their features (a verb corresponding to a given category occurring in the analyzed information item) that may be pertinent as a classification feature (Bloom et al., 2007). However, words such as these cannot always work correctly as clues, for example with the word “said” in the two sentences “There were crowds and crowds of people at the concert, said Ann” and “There were</context>
<context position="20157" citStr="Seki et al., 2008" startWordPosition="3351" endWordPosition="3354"> data set, since those questions were addressed at the NTCIR workshops. Using the Chinese and Japanese corpora we can verify the quality of the suggested language-independent approaches. 42 5.1 Feature Selection &amp; Evaluation in English For the evaluation of sentences in English, the assumption of isolated words (bag-of-words) previously stemmed was used by our system. The corpora are comprised of more than 13,400 sentences, 4,859 (36.3%) of which are opinionated. As the evaluation metrics precision, recall and F1-measure were used based on gold standard evaluation provided by NTCIR workshops (Seki et al., 2008). The precision and recall are weighted equally in our experiment but it should be recognized that based on the system&apos;s needs and focus there could be more accent on precision or recall. Model Precision Recall F1-measure Logistic model 0.583 0.508 0.543 Naïve Bayes 0.415 0.364 0.388 LM 0.350 0.339 0.343 SC 0.979 0.360 0.527 Table 2. Evaluation results of 10-fold cross-validation on NTCIR-6 and NTCIR-7 English corpora. Comparing the results in Table 2 to the baselines of the Naïve Bayes classifier and LM evaluated on the same training and testing sets, we see that logistic model outperforms th</context>
</contexts>
<marker>Seki, Evans, Ku, Sun, Chen, Kando, 2008</marker>
<rawString>Seki, Y., Evans, D. K., Ku, L.-W., Sun, L., Chen, H.-H., &amp; Kando, N. 2008. Overview of multilingual opinion analysis task at NTCIR-7. Proceedings NTCIR-7, NII, Tokyo, pp. 185-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Manning</author>
</authors>
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagging.</title>
<date>2000</date>
<booktitle>Proceedings EMNLP / VLC-2000, Hong Kong,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="7111" citStr="Toutanova &amp; Manning, 2000" startWordPosition="1101" endWordPosition="1105">c relevancy. As an alternative other teams during the last TREC and NTCIR evaluation campaigns have suggested variations of Naïve Bayes classifier, language models and SVM, along with the use of such heuristics as word order, punctuation, sentence length, etc. We might also mention OpinionFinder (Wilson et al., 2005), a more complex system that performs subjectivity analyses to identify opinions as well as sentiments and other private states (speculations, dreams, etc.). This system is based on various classical computational linguistics components (tokenization, part-of-speech (POS) tagging (Toutanova &amp; Manning, 2000) as well as classification tools. For example, a Naïve Bayes classifier (Witten &amp; Frank, 2005) is used to distinguish between subjective and objective sentences. A rule-based system is included to identify both speech events (“said,” “according to”) and direct subjective expressions (“is happy,” “fears”) within a given sentence. Of course such learning system requires both a training set and a deeper knowledge of a given natural language (morphological components, syntactic analyses, semantic thesaurus). The lack of enough training data for the learning-based systems is clearly a drawback. Mor</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Toutanova, K., &amp; Manning, C. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagging. Proceedings EMNLP / VLC-2000, Hong Kong, pp. 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>Proceedings of the ACL,</booktitle>
<pages>417--424</pages>
<location>Philadelphia (PA),</location>
<contexts>
<context position="5451" citStr="Turney (2002)" startWordPosition="849" endWordPosition="850">conjecture, admire, judge, assess, say, complain, advise) and their features (a verb corresponding to a given category occurring in the analyzed information item) that may be pertinent as a classification feature (Bloom et al., 2007). However, words such as these cannot always work correctly as clues, for example with the word “said” in the two sentences “There were crowds and crowds of people at the concert, said Ann” and “There were more than 10,000 people at the concert, said Ann.” Both sentences contain the clue word “said” but only the first one contains an opinion on the target product. Turney (2002) suggested comparing the frequency of phrase co-occurrences with words predetermined by the sentiment lexicon. Specific to the opinion detection in Chinese language Ku et al. (2006) propose a dictionary-based approach for extraction and summarization. For the Japanese language in the last NTCIR-6 and NTCIR-7 workshops the opinion finding methods included the use of supervised machine learning approaches with specific selection of certain parts-of-speech (POS) and sentence parts in the form of n-gram features to improve performance. There has been a trend in applying language models for opinion</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, P. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. Proceedings of the ACL, Philadelphia (PA), pp. 417-424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>P Hoffmann</author>
<author>S Somasundaran</author>
<author>J Kessler</author>
<author>J Wiebe</author>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Riloff</author>
<author>S Patwardhan</author>
</authors>
<title>OpinionFinder: A system for subjectivity analysis.</title>
<date>2005</date>
<booktitle>Proceedings HLT/EMNLP, Vancouver (BC),</booktitle>
<pages>34--35</pages>
<contexts>
<context position="6803" citStr="Wilson et al., 2005" startWordPosition="1059" endWordPosition="1062">tivity extraction. Usually, language models are trained on the labeled data and as an output they give probabilities of classified tokens belonging to the class. Eguchi &amp; Lavrenko (2006) propose the use of probabilistic language models for ranking the results not only by sentiment but also by the topic relevancy. As an alternative other teams during the last TREC and NTCIR evaluation campaigns have suggested variations of Naïve Bayes classifier, language models and SVM, along with the use of such heuristics as word order, punctuation, sentence length, etc. We might also mention OpinionFinder (Wilson et al., 2005), a more complex system that performs subjectivity analyses to identify opinions as well as sentiments and other private states (speculations, dreams, etc.). This system is based on various classical computational linguistics components (tokenization, part-of-speech (POS) tagging (Toutanova &amp; Manning, 2000) as well as classification tools. For example, a Naïve Bayes classifier (Witten &amp; Frank, 2005) is used to distinguish between subjective and objective sentences. A rule-based system is included to identify both speech events (“said,” “according to”) and direct subjective expressions (“is hap</context>
</contexts>
<marker>Wilson, Hoffmann, Somasundaran, Kessler, Wiebe, Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Wilson, T., Hoffmann, P., Somasundaran, S., Kessler, J., Wiebe, J., Choi, Y., Cardie, C., Riloff, E., &amp; Patwardhan, S., 2005. OpinionFinder: A system for subjectivity analysis. Proceedings HLT/EMNLP, Vancouver (BC), pp. 34-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical Machine Learning Tools and Techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco (CA).</location>
<contexts>
<context position="7205" citStr="Witten &amp; Frank, 2005" startWordPosition="1117" endWordPosition="1121"> suggested variations of Naïve Bayes classifier, language models and SVM, along with the use of such heuristics as word order, punctuation, sentence length, etc. We might also mention OpinionFinder (Wilson et al., 2005), a more complex system that performs subjectivity analyses to identify opinions as well as sentiments and other private states (speculations, dreams, etc.). This system is based on various classical computational linguistics components (tokenization, part-of-speech (POS) tagging (Toutanova &amp; Manning, 2000) as well as classification tools. For example, a Naïve Bayes classifier (Witten &amp; Frank, 2005) is used to distinguish between subjective and objective sentences. A rule-based system is included to identify both speech events (“said,” “according to”) and direct subjective expressions (“is happy,” “fears”) within a given sentence. Of course such learning system requires both a training set and a deeper knowledge of a given natural language (morphological components, syntactic analyses, semantic thesaurus). The lack of enough training data for the learning-based systems is clearly a drawback. Moreover, it is difficult to objectively establish when a complex learning system has enough trai</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, I.A., &amp; Frank, E. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco (CA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>