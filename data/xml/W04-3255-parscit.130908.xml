<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.996547">
Efficient Decoding for Statistical Machine Translation
with a Fully Expanded WFST Model
</title>
<author confidence="0.603755">
Hajime Tsukada
</author>
<affiliation confidence="0.312061">
NTT Communication Science Labs.
</affiliation>
<address confidence="0.359538">
2-4 Hikaridai Seika-cho Soraku-gun
Kyoto 619-0237
Japan
</address>
<email confidence="0.997631">
tsukada@cslab.kecl.ntt.co.jp
</email>
<note confidence="0.478645">
Masaaki Nagata
NTT Cyber Space Labs.
1-1 Hikari-no-Oka Yokosuka-shi
</note>
<author confidence="0.430614">
Kanagawa 239-0847
</author>
<affiliation confidence="0.443061">
Japan
</affiliation>
<email confidence="0.997654">
nagata.masaaki@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.99385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818230769231">
This paper proposes a novel method to compile sta-
tistical models for machine translation to achieve
efficient decoding. In our method, each statistical
submodel is represented by a weighted finite-state
transducer (WFST), and all of the submodels are ex-
panded into a composition model beforehand. Fur-
thermore, the ambiguity of the composition model
is reduced by the statistics of hypotheses while de-
coding. The experimental results show that the pro-
posed model representation drastically improves the
efficiency of decoding compared to the dynamic
composition of the submodels, which corresponds
to conventional approaches.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99986821875">
Recently, research on statistical machine translation
has grown along with the increase in computational
power as well as the amount of bilingual corpora.
The basic idea of modeling machine translation was
proposed by Brown et al. (1993), who assumed that
machine translation can be modeled on noisy chan-
nels. The source language is encoded from a target
language by a noisy channel, and translation is per-
formed as a decoding process from source language
to target language.
Knight (1999) showed that the translation prob-
lem defined by Brown et al. (1993) is NP-
complete. Therefore, with this model it is al-
most impossible to search for optimal solutions in
the decoding process. Several studies have pro-
posed methods for searching suboptimal solutions.
Berger et al. (1996) and Och et al. (2001) pro-
posed such depth-first search methods as stack de-
coders. Wand and Waibel (1997) and Tillmann and
Ney (2003) proposed breadth-first search methods,
i.e. beam search. Germann (2001) and Watanabe
and Sumita (2003) proposed greedy type decoding
methods. In all of these search algorithms, better
representation of the statistical model in systems
can improve the search efficiency.
For model representation, a search method based
on weighted finite-state transducer (WFST) (Mohri
et al., 2002) has achieved great success in the speech
recognition field. The basic idea is that each statis-
tical model is represented by a WFST and they are
composed beforehand; the composed model is op-
timized by WFST operations such as determiniza-
tion and minimization. This fully expanded model
permits efficient searches. Our motivation is to ap-
ply this approach to machine translation. However,
WFST optimization operations such as determiniza-
tion are nearly impossible to apply to WFSTs in ma-
chine translation because they are much more am-
biguous than speech recognition. To reduce the am-
biguity, we propose a WFST optimization method
that considers the statistics of hypotheses while de-
coding.
Some approaches have applied WFST to sta-
tistical machine translation. Knight and Al-
Onaizan (1998) proposed the representation of
IBM model 3 with WFSTs; Bangalore and Ric-
cardi (2001) studied WFST models in call-routing
tasks, and Kumar and Byrne (2003) modeled
phrase-based translation by WFSTs. All of these
studies mainly focused on the representation of each
submodel used in machine translation. However,
few studies have focued on the integration of each
WFST submodel to improve the decoding efficiency
of machine translation.
To this end, we propose a method that expands
all of the submodels into a composition model, re-
ducing the ambiguity of the expanded model by the
statistics of hypotheses while decoding. First, we
explain the translation model (Brown et al., 1993;
Knight and Al-Onaizan, 1998) that we used as a
base for our decoding research. Second, our pro-
posed method is introduced. Finally, experimental
results show that our proposed method drastically
improves decoding efficiency.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="method">
2 IBM Model
</sectionHeader>
<bodyText confidence="0.8611316">
For our decoding research, we assume the IBM-
style modeling for translation proposed in Brown et
al. (1993). In this model, translation from Japanese
to English attempts to find the that maximizes
. Using Bayes’ rule, is rewritten as
</bodyText>
<equation confidence="0.726695333333333">
tekisuto:text/
t(tekisuto|text)
ha:NULL/
t(ha|NULL)
kaku:each/
t(kaku|each)
</equation>
<figureCaption confidence="0.957802">
Figure 2: T Model
</figureCaption>
<bodyText confidence="0.981223272727273">
where is referred to as a language model and
is referred to as a translation model. In this
paper, we use word trigram for a language model
and IBM model 3 for a translation model.
The translation model is represented as follows
considering all possible word alignments.
The IBM model only assumes a one-to-many word
alignment, where a Japanese word in the -th po-
sition connects to the English word in the -th
position.
The IBM model 3 uses the following .
</bodyText>
<figure confidence="0.614269">
each:each/1.0
NULL:s/1-p0
tex:text/1.0
s:s/p0
</figure>
<figureCaption confidence="0.653874">
Figure 3: NULL Model
</figureCaption>
<equation confidence="0.942801">
(1)
</equation>
<bodyText confidence="0.9986869375">
the a number of words connecting to ,
and it is called fertility. Note, however, that
is the number of words connecting to null words.
is conditional probability where English
word connects to words in . is
called fertility probability. is conditional
probability where English word is translated to
Japanese word and called translation probability.
is conditional probability where the En-
glish word in the -th position connects to the the
Japanese word in the -th position on condition that
the length of the English sentence and Japanese
sentence are and , respectively.
is called distortion probability. In our experiment,
we used the IBM model 3 while assuming constant
distortion probability for simplicity.
</bodyText>
<sectionHeader confidence="0.999494" genericHeader="method">
3 WFST Cascade Model
</sectionHeader>
<bodyText confidence="0.999600823529412">
WFST is a finite-state device in which output sym-
bols and output weights are defined as well as in-
put symbols. Using composition (Pereira and Riley,
1997), we can obtain the combined WFST
by connecting each output of to an input of .
If we assume that each submodel of Equation (1) is
represented by a WFST, a conventional decoder can
be considered to compose submodels dynamically.
The main idea of the proposed approach is to com-
pute the composition beforehand.
Figure 1 shows the translation process modeled
by a WFST cascade. This WFST cascade model
(Knight and Al-Onaizan, 1998) represents the IBM
model 3 described in the previous section. Any
possible permutations of the Japanese sentence are
inputed to the cascade. First, T model(❉ ) trans-
lates the Japanese word to an English word. NULL
model( ) deletes special word NULL. Fertility
model( ) merges the same continuous words into
one word. At each stage, the probability represented
by the weight of a WFST is accumulated. Finally,
the weight of language model ( ) is accumulated.
If WFST represents all permutations of the input
sentence, decoding can be considered to search for
the best path of . Therefore, com-
puting in advance can improve the
efficiency of the decoder.
For , , and , we adopt the representation of
Knight and Al-Onaizan (1998). For , we adopt the
representation of Mohri et al. (2002). Figures 2–
5 show examples of submodel representation with
WFSTs. in Figure 5 stands for a back-off pa-
rameter. Conditional branches are represented by
nondeterministic paths in the WFST.
</bodyText>
<sectionHeader confidence="0.985746" genericHeader="method">
4 Ambiguity Reduction
</sectionHeader>
<bodyText confidence="0.949538">
If we can determinize a fully-expanded WFST, we
can achieve the best performance of the decoder.
kaku tekisuto ha ko-do ka sareru de SGML
each text NULL encoded encoded encoded in SGML
each text encoded encoded encoded in SGML
each text is encoded in SGML
</bodyText>
<figureCaption confidence="0.9999835">
Figure 1: Translation with WFST Cascade Model
Figure 4: Fertility Model
</figureCaption>
<figure confidence="0.987290277777778">
T Model (T)
NULL Model (N)
Fertility Model (F)
Language Model (L)
each text is encoded in SGML
encoded:encoded/
n(2|encoded)
encoded:encoded/
n(1|encoded)
&amp;:encoded/
n(0|encoded)
encoded:&amp;/1.0
encoded:&amp;/1.0
encoded:&amp;/1.0
encoded:&amp;/
n(3|encoded)/n(2|encoded)
encoded:&amp;/
n(4|encoded)/n(3|encoded)
</figure>
<bodyText confidence="0.9995188125">
However, the composed WFST for machine trans-
lation is not obviously determinizable. The word-
to-word translation model strongly contributes to
WFST’s ambiguity while the transition of other
submodels also contributes to ambiguity. Mohri et
al. (2002) proposed a technique that added special
symbols allowing the WFST to be determinizable.
Determinization using this technique, however, is
not expected to achieve efficient decoding in ma-
chine translation because the WFSTs of machine
translation are inherently ambiguous.
To overcome this problem, we propose a novel
WFST optimization approach that uses decoding in-
formation. First, our method merges WFST states
by considering the statistics of hypotheses while de-
coding. After merging the states, redundant edges
whose beginning states, end states, input symbols,
and output symbols are the same are also reduced.
IBM models consider all possible alignments while
a decoder searches for only the most appropriate
alignment. Therefore, there are many redundant
states in the full-expansion WFST from the view-
point of decoding.
We adopted a standard decoding algorithm in
the speech recognition field, where the forward is
beam-search and the backward is search. Since
beam-search is adopted in the forward pass, the ob-
tained results are not optimal but suboptimal. All
input permutations are represented by a finite-state
acceptor (Figure 6), where each state corresponds to
input positions that are already read. In the forward
search, hypotheses are maintained for each state of
</bodyText>
<figureCaption confidence="0.992902">
Figure 5: Trigram Language Model
</figureCaption>
<bodyText confidence="0.980769392857143">
the finite-state acceptor.
The WFST states that always appear together in
the same hypothesis list of the forward beam-search
should be equated if the states contribute to cor-
rect translation. Let be a full-expansion WFST
model and be a WFST that represents the cor-
rect translation of an input sentence . For each
, the states of that always appear together in
the same hypothesis list in the course of decoding
with are merged in our method. Simply
merging states of may increase model errors, but
corrects the errors caused by merging states.
Unlike ordinary FSA minimization, states are
merged without considering their successor states.
If the weight represents probability, thesum of the
weights of output transitions may not be 1.0 after
merging states, and then thecondition of probability
may be destroyed. Since the decoder does not sum
up all possible paths but searches for the most ap-
propriate paths, this kind of state merging does not
pose a serious problem in practice.
In the following experiment, we measured the
association between states by in Gale and
Church (1991). is a -like statistic that is
bounded between 0 and 1. If the of two states
is higher than the specified threshold, these two
states are merged. The definition of is as fol-
lows, where
</bodyText>
<figureCaption confidence="0.985479">
Figure 6: FSA for All Input Permutations
</figureCaption>
<bodyText confidence="0.999781">
Merging the beginning and end states of a tran-
sition whose input is ( transition for short) may
cause a problem when decoding. In our implemen-
tation, weight is basically minus probability, and
its lower bound is 0 in theory. However, there exists
negative transition that originated from the back-
off value of n-gram. If we merge the beginning and
end states of the negative transition, the search
process will not stop due to the negative loop. To
avoid this problem, we rounded the negative weight
to 0 if the negative loop appears during merging.
In the preliminary experiment, a weight-pushing
operation (Mohri and Riley, 2001) was also effec-
tive for deleting negative transition of our full-
expansion models. However, pushing causes an im-
balance of weights among paths if the WFST is not
deterministic. As a result of this imbalance, we can-
not compare path costs when pruning. In fact, our
preliminary experiment showed that pushed full-
expansion WFST does not work well. Therefore,
we adopted a simpler method to deal with a nega-
tive loop as described above.
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999815">
5.1 Effect of Full Expansion
</subsectionHeader>
<bodyText confidence="0.999668947368421">
To clarify the effectiveness of a full-expansion ap-
proach, we compared the computational costs while
using the same decoder with both dynamic com-
position and static composition, a full-expansion
model in other words. In the forward beam-search,
any hypothesis whose probability is lower than
of the top of the hypothesis list is pruned. In this ex-
periment, permutation is restricted, and words can
be moved 6 positions at most. The translation model
was trained by GIZA++ (Och and Ney, 2003), and
the trigram was trained by the CMU-Cambridge
Statistical Language Modeling Toolkit v2 (Clarkson
and Rosenfeld, 1997).
For the experiment, we used a Japanese-to-
English bilingual corpus consisting of example sen-
tences for a rule-based machine translation sys-
tem. Each language sentence is aligned in the cor-
pus. The total number of sentence pairs is 20,204.
We used 17,678 pairs for training and 2,526 pairs
</bodyText>
<figure confidence="0.760441851851852">
ab
b:b/P(b|ca)
e:e/b(ab)
b:b/P(b|a)
c:c/P(c|ab)
e:e/b(b)
a e:e/b(a) b
a:a/P(a)
e b:b/P(b)
c:c/P(c|b)
e:e/b(ca)
e:e/b(c) c:c/P(c)
c
ca a:a/P(a|c) e:e/b(bc) bc
a:a/P(a|bc)
( ) is the number of hypothesis lists in
which appears (both and appear).
, ,
, and .
is the total number of hypothesis lists.
{1}
{} {2}
{3}
{1,2}
{2,3}
{1,3}
{1,2,3}
</figure>
<bodyText confidence="0.99984795">
for the test. The average length of Japanese sen-
tences was 8.4 words, and that of English sentences
was 6.7 words. The Japanese vocabulary consisted
of 15,510 words, and the English vocabulary was
11,806 words. Table 1 shows the size of the WFSTs
used in the experiment. In these WFSTs, special
symbols that express beginning and end of sentence
are added to the WFSTs described in the previous
section. The NIST score (Doddington, 2002) and
BLEU Score (Papineni et al., 2002) were used to
measure translation accuracy.
Table 2 shows the experimental results. The full-
expansion model provided translations more than 10
times faster than conventional dynamic composition
submodels without degrading accuracy. However,
the NIST scores are slightly different. In the course
of composition, some paths that do not reach the fi-
nal states are produced. In the full-expansion model
these paths are trimmed. These trimmed paths may
cause a slight difference in NIST scores.
</bodyText>
<subsectionHeader confidence="0.999776">
5.2 Effect of Ambiguity Reduction
</subsectionHeader>
<bodyText confidence="0.999930038461539">
To show the effect of ambiguity reduction, we com-
pared the translation results of three different mod-
els. Model is the full-expansion model described
above. Model is a reduced model by using our
proposed method with a 0.9 threshold. Model
is a reduced model with the statistics of the de-
coder without using the correct translation WFST.
In other words, reduces the states of the full-
expansion model more roughly than . The
threshold for is set to 0.85 so that the size of
the produced WFST is almost the same as . Table
3 shows the model size. To obtain decoder statistics
for calculating , all of the sentence pairs in the
training set were used. When obtaining the statis-
tics, any hypothesis whose probability is lower than
of the top of the hypothesis list is pruned in
the forward beam-search.
The translation experiment was conducted by
successively changing the beam width of the for-
ward search. Figures 7 and 8 show the results of
the translation experiments, revealing that our pro-
posed model can reduce the decoding time by ap-
proximately half. This model can reduce decoding
time to a much greater extent than the rough reduc-
tion model, indicating that our state merging criteria
are valid.
</bodyText>
<sectionHeader confidence="0.997769" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999961052631579">
We proposed a method to compile statistical mod-
els to achieve efficient decoding in a machine trans-
lation system. In our method, each statistical sub-
model is represented by a WFST, and all submodels
are composed beforehand. To reduce the ambiguity
of the composed WFST, the states are merged ac-
cording to the statistics of hypotheses while decod-
ing. As a result, we reduced decoding time to ap-
proximately of dynamic composition of sub-
models, which corresponds to the conventional ap-
proach.
In this paper, we applied the state merging
method to a fully-expanded WFST and showed the
effectiveness of this approach. However, the state
merging method itself is general and independent
of the fully-expanded WFST. We can apply this
method to each submodel of machine translation.
More generally, we can apply it to all WFST-like
models, including HMMs.
</bodyText>
<sectionHeader confidence="0.98446" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999766">
We would like to thank F. J. Och for providing
GIZA++ and mkcls toolkits, and P. R. Clarkson for
the CMU-Cambridge statistical language modeling
toolkit v2. We also thank T. Hori for providing the
n-gram conversion program for WFSTs and F. Bond
and S. Fujita for providing the bilingual corpus.
</bodyText>
<sectionHeader confidence="0.836349" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.966265625">
Srinivas Bangalore and Giueseppe Riccardi. 2001.
A finite-state approach to machine translation. In
Proc. of North American Association of Compu-
tational Linguistics (NAACL 2001), May.
Adam L. Berger, Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, Andrew S. Kehler,
and Robert L. Mercer. 1996. Language transla-
tion apparatus and method of using context-based
translation models. United States Patent.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pitra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. Computational Linguis-
tics, 19(2):263–311.
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal language modeling using the cmu-cambridge
</bodyText>
<reference confidence="0.725731230769231">
toolkit. In Proc. of European Conference on
Speech Communication and Technology (EU-
ROSPEECH’97).
G. Doddington. 2002. Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proc. ofHLT 2002.
William A. Gale and Kenneth W. Church. 1991.
Identifying word correspondences in parallel
texts. In Proc. ofFourth DARPA Speech and Nat-
ural Language Processing Workshop, pages 152–
157.
Ulrich Germann, Michael Jahr, Kevin Knight,
Daniel Marcu, and Kenji Yamada. 2001. Fast de-
</reference>
<table confidence="0.9995965">
# of States # of Transitions
T Model( ) ) ) 3 59,026
NULL Model ( ) ) 4 11,810
Fertility Model ( 91,513 194,360
Language Model ( 14,532 30,140
Full Expansion( 233,045 2,452,621
</table>
<tableCaption confidence="0.996262">
Table 1: Submodel/Full-Expansion Model Size
</tableCaption>
<table confidence="0.999597333333333">
NIST Score BLEU Score Decoding Time (sec.)
Static Composition (Full-Expansion Model) 3.4 0.037 6,596
Dynamic Composition (Conventional Method) 3.5 0.037 84,753
</table>
<tableCaption confidence="0.997667">
Table 2: Static / Dynamic Composition
</tableCaption>
<reference confidence="0.734244766666666">
coding and optinal decoding for machine trans-
lation. In Proc. of the 39th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 228–235, July.
Kevin Knight and Yaser Al-Onaizan. 1998. Trans-
lation with finite-state devices. In Proc. of the 4th
AMTA Conference.
Kevin Knight. 1999. Decoding complexity in
word-replacement translation models. Computa-
tional Linguistics, 25(4):607–615.
Shankar Kumar and William Byrne. 2003. A
weighted finite state transducer implementation
of the alignment template model for statistical
machine translation. In Proc. of Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL), pages 142–149,
May - June.
Mehryar Mohri and Michael Riley. 2001. A weight
pushing algorithm for large vocabulary speech
recognition. In Proc. of European Conference
on Speech Communication and Technology (EU-
ROSPEECH’01), September.
Mehryar Mohri, Fernando C. N. Pereira, and
Michael Riley. 2002. Weighted finite-state trans-
ducers in speech recognition. Computer Speech
and Language, 16(1):69–88.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguistics,
29(1):19–51.
Franz Josef Och, Nicola Ueffing, and Hermann
Ney. 2001. An efficient search algorithm
for statistical machine translation. In Proc. of
the ACL2001 Workshop on Data-Driven Machine
Translation, pages 55–62, July.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLUE: a method for auto-
matic evaluation of machine translation. In Proc.
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 311–
318, July.
Fernando Pereira and Michael Riley. 1997. Speech
recognition by composition of weighted finite au-
tomata. In Emmanuel Roche and Yves Schabes,
editors, Finite-State Language Processing, chap-
ter 15, pages 431–453. MIT Press, Cambridge,
Massachusetts.
Christoph Tillmann and Hermann Ney. 2003. Word
reordering and a dynamic programming beam
search algorithm for statistical machine transla-
tion. Computational Linguistics, 29(1):97–133,
March.
Ye-Yi Wang and Alex Waibel. 1997. Decoding
algorithm in statistical machine translation. In
Proc. of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical machine
translation. In Proc. ofMT Summit IX.
</reference>
<table confidence="0.97590275">
# of States # of Transitions
Proposed Model ( ) 183,432 2,278,096
Rough Reduction Model ( ) 182,212 2,345,255
Original Model ( ) 233,045 2,452,621
</table>
<tableCaption confidence="0.997661">
Table 3: Original/Reduction Model Size
</tableCaption>
<figure confidence="0.9677225">
1000 2000 3000 4000 5000 6000 7000 8000 9000
CPU Time (sec.)
</figure>
<figureCaption confidence="0.995751">
Figure 7: Ambiguity Reduction (BLEU)
</figureCaption>
<footnote confidence="0.4745785">
1000 2000 3000 4000 5000 6000 7000 8000 9000
CPU Time (sec.)
</footnote>
<figureCaption confidence="0.997167">
Figure 8: Ambiguity Reduction (NIST)
</figureCaption>
<figure confidence="0.998646666666667">
BLEU Score
0.045
0.035
0.025
0.015
0.005
0.04
0.03
0.02
0.01
0
O (Original Model)
R2 (Rough Reduction)
R (Proposed Reduction)
NIST Score
3.5
2.5
0.5
1.5
4
3
2
0
1
O (Original Model)
R2 (Rough Reduction)
R (Proposed Reduction)
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.076594">
<title confidence="0.913787333333333">Efficient Decoding for Statistical Machine with a Fully Expanded WFST Model Hajime</title>
<affiliation confidence="0.610004">NTT Communication Science</affiliation>
<address confidence="0.556571">2-4 Hikaridai Seika-cho</address>
<email confidence="0.610387">Kyototsukada@cslab.kecl.ntt.co.jp</email>
<author confidence="0.736386">Masaaki</author>
<affiliation confidence="0.867714">NTT Cyber Space</affiliation>
<address confidence="0.8516205">1-1 Hikari-no-Oka Kanagawa</address>
<email confidence="0.984115">nagata.masaaki@lab.ntt.co.jp</email>
<abstract confidence="0.996800928571428">This paper proposes a novel method to compile statistical models for machine translation to achieve efficient decoding. In our method, each statistical submodel is represented by a weighted finite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand. Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding. The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>toolkit</author>
</authors>
<booktitle>In Proc. of European Conference on Speech Communication and Technology (EUROSPEECH’97).</booktitle>
<marker>toolkit, </marker>
<rawString>toolkit. In Proc. of European Conference on Speech Communication and Technology (EUROSPEECH’97).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ofHLT</booktitle>
<contexts>
<context position="13431" citStr="Doddington, 2002" startWordPosition="2134" endWordPosition="2135">a:a/P(a|bc) ( ) is the number of hypothesis lists in which appears (both and appear). , , , and . is the total number of hypothesis lists. {1} {} {2} {3} {1,2} {2,3} {1,3} {1,2,3} for the test. The average length of Japanese sentences was 8.4 words, and that of English sentences was 6.7 words. The Japanese vocabulary consisted of 15,510 words, and the English vocabulary was 11,806 words. Table 1 shows the size of the WFSTs used in the experiment. In these WFSTs, special symbols that express beginning and end of sentence are added to the WFSTs described in the previous section. The NIST score (Doddington, 2002) and BLEU Score (Papineni et al., 2002) were used to measure translation accuracy. Table 2 shows the experimental results. The fullexpansion model provided translations more than 10 times faster than conventional dynamic composition submodels without degrading accuracy. However, the NIST scores are slightly different. In the course of composition, some paths that do not reach the final states are produced. In the full-expansion model these paths are trimmed. These trimmed paths may cause a slight difference in NIST scores. 5.2 Effect of Ambiguity Reduction To show the effect of ambiguity reduc</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proc. ofHLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Identifying word correspondences in parallel texts.</title>
<date>1991</date>
<booktitle>In Proc. ofFourth DARPA Speech and Natural Language Processing Workshop,</booktitle>
<pages>152--157</pages>
<contexts>
<context position="10422" citStr="Gale and Church (1991)" startWordPosition="1628" endWordPosition="1631">s of may increase model errors, but corrects the errors caused by merging states. Unlike ordinary FSA minimization, states are merged without considering their successor states. If the weight represents probability, thesum of the weights of output transitions may not be 1.0 after merging states, and then thecondition of probability may be destroyed. Since the decoder does not sum up all possible paths but searches for the most appropriate paths, this kind of state merging does not pose a serious problem in practice. In the following experiment, we measured the association between states by in Gale and Church (1991). is a -like statistic that is bounded between 0 and 1. If the of two states is higher than the specified threshold, these two states are merged. The definition of is as follows, where Figure 6: FSA for All Input Permutations Merging the beginning and end states of a transition whose input is ( transition for short) may cause a problem when decoding. In our implementation, weight is basically minus probability, and its lower bound is 0 in theory. However, there exists negative transition that originated from the backoff value of n-gram. If we merge the beginning and end states of the negative </context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale and Kenneth W. Church. 1991. Identifying word correspondences in parallel texts. In Proc. ofFourth DARPA Speech and Natural Language Processing Workshop, pages 152– 157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optinal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of the 39th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>228--235</pages>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optinal decoding for machine translation. In Proc. of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 228–235, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Yaser Al-Onaizan</author>
</authors>
<title>Translation with finite-state devices.</title>
<date>1998</date>
<booktitle>In Proc. of the 4th AMTA Conference.</booktitle>
<contexts>
<context position="3775" citStr="Knight and Al-Onaizan, 1998" startWordPosition="571" endWordPosition="574">1) studied WFST models in call-routing tasks, and Kumar and Byrne (2003) modeled phrase-based translation by WFSTs. All of these studies mainly focused on the representation of each submodel used in machine translation. However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation. To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding. First, we explain the translation model (Brown et al., 1993; Knight and Al-Onaizan, 1998) that we used as a base for our decoding research. Second, our proposed method is introduced. Finally, experimental results show that our proposed method drastically improves decoding efficiency. 2 IBM Model For our decoding research, we assume the IBMstyle modeling for translation proposed in Brown et al. (1993). In this model, translation from Japanese to English attempts to find the that maximizes . Using Bayes’ rule, is rewritten as tekisuto:text/ t(tekisuto|text) ha:NULL/ t(ha|NULL) kaku:each/ t(kaku|each) Figure 2: T Model where is referred to as a language model and is referred to as a </context>
<context position="6155" citStr="Knight and Al-Onaizan, 1998" startWordPosition="960" endWordPosition="963">ability for simplicity. 3 WFST Cascade Model WFST is a finite-state device in which output symbols and output weights are defined as well as input symbols. Using composition (Pereira and Riley, 1997), we can obtain the combined WFST by connecting each output of to an input of . If we assume that each submodel of Equation (1) is represented by a WFST, a conventional decoder can be considered to compose submodels dynamically. The main idea of the proposed approach is to compute the composition beforehand. Figure 1 shows the translation process modeled by a WFST cascade. This WFST cascade model (Knight and Al-Onaizan, 1998) represents the IBM model 3 described in the previous section. Any possible permutations of the Japanese sentence are inputed to the cascade. First, T model(❉ ) translates the Japanese word to an English word. NULL model( ) deletes special word NULL. Fertility model( ) merges the same continuous words into one word. At each stage, the probability represented by the weight of a WFST is accumulated. Finally, the weight of language model ( ) is accumulated. If WFST represents all permutations of the input sentence, decoding can be considered to search for the best path of . Therefore, computing i</context>
</contexts>
<marker>Knight, Al-Onaizan, 1998</marker>
<rawString>Kevin Knight and Yaser Al-Onaizan. 1998. Translation with finite-state devices. In Proc. of the 4th AMTA Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
</authors>
<title>Decoding complexity in word-replacement translation models.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>4</issue>
<contexts>
<context position="1479" citStr="Knight (1999)" startWordPosition="211" endWordPosition="212">decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches. 1 Introduction Recently, research on statistical machine translation has grown along with the increase in computational power as well as the amount of bilingual corpora. The basic idea of modeling machine translation was proposed by Brown et al. (1993), who assumed that machine translation can be modeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, bett</context>
</contexts>
<marker>Knight, 1999</marker>
<rawString>Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computational Linguistics, 25(4):607–615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer implementation of the alignment template model for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>142--149</pages>
<contexts>
<context position="3219" citStr="Kumar and Byrne (2003)" startWordPosition="484" endWordPosition="487">otivation is to apply this approach to machine translation. However, WFST optimization operations such as determinization are nearly impossible to apply to WFSTs in machine translation because they are much more ambiguous than speech recognition. To reduce the ambiguity, we propose a WFST optimization method that considers the statistics of hypotheses while decoding. Some approaches have applied WFST to statistical machine translation. Knight and AlOnaizan (1998) proposed the representation of IBM model 3 with WFSTs; Bangalore and Riccardi (2001) studied WFST models in call-routing tasks, and Kumar and Byrne (2003) modeled phrase-based translation by WFSTs. All of these studies mainly focused on the representation of each submodel used in machine translation. However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation. To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding. First, we explain the translation model (Brown et al., 1993; Knight and Al-Onaizan, 1998) that we used as a base for our decoding res</context>
</contexts>
<marker>Kumar, Byrne, 2003</marker>
<rawString>Shankar Kumar and William Byrne. 2003. A weighted finite state transducer implementation of the alignment template model for statistical machine translation. In Proc. of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 142–149, May - June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>A weight pushing algorithm for large vocabulary speech recognition.</title>
<date>2001</date>
<booktitle>In Proc. of European Conference on Speech Communication and Technology (EUROSPEECH’01),</booktitle>
<contexts>
<context position="11278" citStr="Mohri and Riley, 2001" startWordPosition="1776" endWordPosition="1779">the beginning and end states of a transition whose input is ( transition for short) may cause a problem when decoding. In our implementation, weight is basically minus probability, and its lower bound is 0 in theory. However, there exists negative transition that originated from the backoff value of n-gram. If we merge the beginning and end states of the negative transition, the search process will not stop due to the negative loop. To avoid this problem, we rounded the negative weight to 0 if the negative loop appears during merging. In the preliminary experiment, a weight-pushing operation (Mohri and Riley, 2001) was also effective for deleting negative transition of our fullexpansion models. However, pushing causes an imbalance of weights among paths if the WFST is not deterministic. As a result of this imbalance, we cannot compare path costs when pruning. In fact, our preliminary experiment showed that pushed fullexpansion WFST does not work well. Therefore, we adopted a simpler method to deal with a negative loop as described above. 5 Experiments 5.1 Effect of Full Expansion To clarify the effectiveness of a full-expansion approach, we compared the computational costs while using the same decoder w</context>
</contexts>
<marker>Mohri, Riley, 2001</marker>
<rawString>Mehryar Mohri and Michael Riley. 2001. A weight pushing algorithm for large vocabulary speech recognition. In Proc. of European Conference on Speech Communication and Technology (EUROSPEECH’01), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted finite-state transducers in speech recognition.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="2279" citStr="Mohri et al., 2002" startWordPosition="334" endWordPosition="337"> process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based on weighted finite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field. The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by WFST operations such as determinization and minimization. This fully expanded model permits efficient searches. Our motivation is to apply this approach to machine translation. However, WFST optimization operations such as determinization are nearly impossible to apply to WFSTs in machine translation because they are much more ambiguous than speech recognition. To reduce the ambiguity, we propose</context>
<context position="6939" citStr="Mohri et al. (2002)" startWordPosition="1095" endWordPosition="1098">slates the Japanese word to an English word. NULL model( ) deletes special word NULL. Fertility model( ) merges the same continuous words into one word. At each stage, the probability represented by the weight of a WFST is accumulated. Finally, the weight of language model ( ) is accumulated. If WFST represents all permutations of the input sentence, decoding can be considered to search for the best path of . Therefore, computing in advance can improve the efficiency of the decoder. For , , and , we adopt the representation of Knight and Al-Onaizan (1998). For , we adopt the representation of Mohri et al. (2002). Figures 2– 5 show examples of submodel representation with WFSTs. in Figure 5 stands for a back-off parameter. Conditional branches are represented by nondeterministic paths in the WFST. 4 Ambiguity Reduction If we can determinize a fully-expanded WFST, we can achieve the best performance of the decoder. kaku tekisuto ha ko-do ka sareru de SGML each text NULL encoded encoded encoded in SGML each text encoded encoded encoded in SGML each text is encoded in SGML Figure 1: Translation with WFST Cascade Model Figure 4: Fertility Model T Model (T) NULL Model (N) Fertility Model (F) Language Model</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2002</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="12244" citStr="Och and Ney, 2003" startWordPosition="1937" endWordPosition="1940">efore, we adopted a simpler method to deal with a negative loop as described above. 5 Experiments 5.1 Effect of Full Expansion To clarify the effectiveness of a full-expansion approach, we compared the computational costs while using the same decoder with both dynamic composition and static composition, a full-expansion model in other words. In the forward beam-search, any hypothesis whose probability is lower than of the top of the hypothesis list is pruned. In this experiment, permutation is restricted, and words can be moved 6 positions at most. The translation model was trained by GIZA++ (Och and Ney, 2003), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 (Clarkson and Rosenfeld, 1997). For the experiment, we used a Japanese-toEnglish bilingual corpus consisting of example sentences for a rule-based machine translation system. Each language sentence is aligned in the corpus. The total number of sentence pairs is 20,204. We used 17,678 pairs for training and 2,526 pairs ab b:b/P(b|ca) e:e/b(ab) b:b/P(b|a) c:c/P(c|ab) e:e/b(b) a e:e/b(a) b a:a/P(a) e b:b/P(b) c:c/P(c|b) e:e/b(ca) e:e/b(c) c:c/P(c) c ca a:a/P(a|c) e:e/b(bc) bc a:a/P(a|bc) ( ) is the number </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>An efficient search algorithm for statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proc. of the ACL2001 Workshop on Data-Driven Machine Translation,</booktitle>
<pages>55--62</pages>
<contexts>
<context position="1786" citStr="Och et al. (2001)" startWordPosition="261" endWordPosition="264">ng machine translation was proposed by Brown et al. (1993), who assumed that machine translation can be modeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based on weighted finite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field. The basic idea is that each statistical model </context>
</contexts>
<marker>Och, Ueffing, Ney, 2001</marker>
<rawString>Franz Josef Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient search algorithm for statistical machine translation. In Proc. of the ACL2001 Workshop on Data-Driven Machine Translation, pages 55–62, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLUE: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="13470" citStr="Papineni et al., 2002" startWordPosition="2139" endWordPosition="2142">ypothesis lists in which appears (both and appear). , , , and . is the total number of hypothesis lists. {1} {} {2} {3} {1,2} {2,3} {1,3} {1,2,3} for the test. The average length of Japanese sentences was 8.4 words, and that of English sentences was 6.7 words. The Japanese vocabulary consisted of 15,510 words, and the English vocabulary was 11,806 words. Table 1 shows the size of the WFSTs used in the experiment. In these WFSTs, special symbols that express beginning and end of sentence are added to the WFSTs described in the previous section. The NIST score (Doddington, 2002) and BLEU Score (Papineni et al., 2002) were used to measure translation accuracy. Table 2 shows the experimental results. The fullexpansion model provided translations more than 10 times faster than conventional dynamic composition submodels without degrading accuracy. However, the NIST scores are slightly different. In the course of composition, some paths that do not reach the final states are produced. In the full-expansion model these paths are trimmed. These trimmed paths may cause a slight difference in NIST scores. 5.2 Effect of Ambiguity Reduction To show the effect of ambiguity reduction, we compared the translation resul</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLUE: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311– 318, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Speech recognition by composition of weighted finite automata.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 15,</booktitle>
<pages>431--453</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="5726" citStr="Pereira and Riley, 1997" startWordPosition="887" endWordPosition="890">where English word is translated to Japanese word and called translation probability. is conditional probability where the English word in the -th position connects to the the Japanese word in the -th position on condition that the length of the English sentence and Japanese sentence are and , respectively. is called distortion probability. In our experiment, we used the IBM model 3 while assuming constant distortion probability for simplicity. 3 WFST Cascade Model WFST is a finite-state device in which output symbols and output weights are defined as well as input symbols. Using composition (Pereira and Riley, 1997), we can obtain the combined WFST by connecting each output of to an input of . If we assume that each submodel of Equation (1) is represented by a WFST, a conventional decoder can be considered to compose submodels dynamically. The main idea of the proposed approach is to compute the composition beforehand. Figure 1 shows the translation process modeled by a WFST cascade. This WFST cascade model (Knight and Al-Onaizan, 1998) represents the IBM model 3 described in the previous section. Any possible permutations of the Japanese sentence are inputed to the cascade. First, T model(❉ ) translates</context>
</contexts>
<marker>Pereira, Riley, 1997</marker>
<rawString>Fernando Pereira and Michael Riley. 1997. Speech recognition by composition of weighted finite automata. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 15, pages 431–453. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word reordering and a dynamic programming beam search algorithm for statistical machine translation.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1897" citStr="Tillmann and Ney (2003)" startWordPosition="280" endWordPosition="283">odeled on noisy channels. The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based on weighted finite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field. The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by WFST operations s</context>
</contexts>
<marker>Tillmann, Ney, 2003</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational Linguistics, 29(1):97–133, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Decoding algorithm in statistical machine translation.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Wang, Waibel, 1997</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in statistical machine translation. In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Example-based decoding for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofMT Summit IX.</booktitle>
<contexts>
<context position="2000" citStr="Watanabe and Sumita (2003)" startWordPosition="294" endWordPosition="297">and translation is performed as a decoding process from source language to target language. Knight (1999) showed that the translation problem defined by Brown et al. (1993) is NPcomplete. Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process. Several studies have proposed methods for searching suboptimal solutions. Berger et al. (1996) and Och et al. (2001) proposed such depth-first search methods as stack decoders. Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search. Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods. In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency. For model representation, a search method based on weighted finite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field. The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by WFST operations such as determinization and minimization. This fully expanded model permits efficient searches. Our moti</context>
</contexts>
<marker>Watanabe, Sumita, 2003</marker>
<rawString>Taro Watanabe and Eiichiro Sumita. 2003. Example-based decoding for statistical machine translation. In Proc. ofMT Summit IX.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>