<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008621">
<title confidence="0.998784">
Comparing Semantic Role Labeling with Typed Dependency Parsing in
Computational Metaphor Identification
</title>
<author confidence="0.997415">
Eric P. S. Baumer
</author>
<affiliation confidence="0.998999">
Department of Informatics
Univ of California, Irvine
</affiliation>
<address confidence="0.7398875">
5029 Donald Bren Hall
Irvine, CA 92627-3440 USA
</address>
<email confidence="0.999459">
ebaumer@ics.uci.edu
</email>
<author confidence="0.978976">
James P. White
</author>
<affiliation confidence="0.986686333333333">
School of Information and
Computer Sciences
Univ of California, Irvine
</affiliation>
<address confidence="0.844757">
Irvine, CA 92627
</address>
<email confidence="0.999067">
jpwhite@uci.edu
</email>
<author confidence="0.994158">
Bill Tomlinson
</author>
<affiliation confidence="0.9990245">
Department of Informatics
Univ of California, Irvine
</affiliation>
<address confidence="0.7245355">
5068 Donald Bren Hall
Irvine, CA 92627-3440 USA
</address>
<email confidence="0.999405">
wmt@uci.edu
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999284909090909">
Most computational approaches to metaphor
have focused on discerning between
metaphorical and literal text. Recent work
on computational metaphor identification
(CMI) instead seeks to identify overarching
conceptual metaphors by mapping selectional
preferences between source and target cor-
pora. This paper explores using semantic role
labeling (SRL) in CMI. Its goals are two-fold:
first, to demonstrate that semantic roles can
effectively be used to identify conceptual
metaphors, and second, to compare SRL to
the current use of typed dependency parsing
in CMI. The results show that SRL can be
used to identify potential metaphors and
that it overcomes some of the limitations
of using typed dependencies, but also that
SRL introduces its own set of complications.
The paper concludes by suggesting future
directions, both for evaluating the use of SRL
in CMI, and for fostering critical and creative
thinking about metaphors.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973136363636">
Metaphor, the partial framing of one concept in
terms of another, pervades human language and
thought (Lakoff and Johnson, 1980; Lakoff, 1993).
A variety of computational approaches to metaphor-
ical language have been developed, e.g., (Martin,
1990; Fass, 1991; Gedigian et al., 2006; Krishnaku-
maran and Zhu, 2007). However, most such meth-
ods see metaphor as an obstacle to be overcome in
the task of discerning the actual, literal meaning of a
phrase or sentence.
In contrast, the work presented here approaches
conceptual metaphor not as an obstacle but as a
resource. Metaphor is an integral part in human
understanding of myriad abstract or complex con-
cepts (Lakoff and Johnson, 1980), and metaphori-
cal thinking can be a powerful component in crit-
ical and creative thinking, cf. (Gordon, 1974;
Oxman-Michelli, 1991). However, “because they
can be used so automatically and effortlessly, we
find it hard to question [metaphors], if we can
even notice them” (Lakoff and Turner, 1989, p.
65). Computational metaphor identification (CMI)
(Baumer, 2009; Baumer et al., under review) ad-
dresses this difficulty by identifying potential con-
ceptual metaphors in written text. Rather than at-
tempting to discern whether individual phrases are
metaphorical or literal, this technique instead iden-
tifies larger, overarching linguistic patterns. The
goal of CMI is not to state definitively the metaphor
present in a text, but rather to draw potential
metaphors to readers’ attention, thereby encourag-
ing both critical examination of current metaphors
and creative generation of alternative metaphors.
CMI identifies potential metaphors by mapping
selectional preferences (Resnik, 1993) from a source
corpus to a target corpus. Previous work on CMI
utilized typed dependency parses (de Marneffe et
al., 2006) to calculate these selectional preferences.
This paper explores the use of semantic role labeling
(SRL) (Gildea and Jurafsky, 2002; Johansson and
Nugues, 2008) to calculate selectional preferences.
Typed dependencies focus on syntactic structure and
grammatical relations, while semantic roles empha-
size conceptual and semantic structure, so SRL may
</bodyText>
<page confidence="0.990512">
14
</page>
<note confidence="0.990521">
Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14–22,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99969275">
be more effective for identifying potential concep-
tual metaphors. This paper describes how SRL was
incorporated into CMI and compares both the rela-
tional data and the metaphors identified with typed
dependency parsing and semantic role labeling. The
results show that semantic roles enabled effective
identification of potential metaphors. However, nei-
ther typed dependencies nor semantic roles were
necessarily superior. Rather, each provides certain
advantages, both in terms of identifying potential
metaphors, and in terms of promoting critical think-
ing and creativity.
</bodyText>
<sectionHeader confidence="0.999976" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.994927">
2.1 Computational Approaches to Metaphor
</subsectionHeader>
<bodyText confidence="0.999975237288136">
Many computational approaches have been taken
toward identifying metaphor in written text. MI-
DAS (Martin, 1990) attempts to detect when users of
the Unix Consultant command line help system use
metaphors, for example, “How do I enter Emacs?” is
interpreted as “How do I invoke Emacs?” Another
system, met* (Fass, 1991), is designed to distinguish
both metaphor and metonymy from literal text, pro-
viding special techniques for processing these in-
stances of figurative language. More recently, Gedi-
gian et al. (2006) used hand-annotated corpora to
train an automatic metaphor classifier. Krishnaku-
maran and Zhu (2007) used violations of WordNet-
based (Fellbaum, 1998) verb-noun expectations to
identify the presence of a metaphor, e.g., “he is a
brave lion,” would be considered metaphorical, be-
cause “he,” taken to mean a “person,” which is not a
WordNet hyponym of “lion.”
These and similar approaches ascribe to some
degree to the literal meaning hypothesis (Reddy,
1969), which states that every sentence has a lit-
eral meaning, as derived from the meanings of its
constituent words, while some also have a figurative
meaning that goes beyond the meanings of the words
themselves. In this view, a figurative interpretation
is only sought only after a literal interpretation has
been formed and found inconsistent, nonsensical, or
otherwise faulty. However, experimental evidence
has made this account suspect (Gibbs, 1984; Gen-
tner et al., 2001). Even distinguishing whether a
given expression is literal or figurative can be dif-
ficult at best. For example, “the rock is becoming
brittle with age” (Reddy, 1969, p. 242), has “a lit-
eral interpretation when uttered about a stone and a
metaphorical one when said about a decrepit profes-
sor emeritus” (Fass, 1991, p. 54).
One previous metaphor system avoids making
such literal/metaphorical distinctions. CorMet (Ma-
son, 2004) is designed to extract known con-
ventional metaphors from domain-specific textual
corpora, which are derived from Google queries.
CorMet calculates selectional preferences and asso-
ciations (Resnik, 1993) for each corpus’s character-
istic verbs, i.e., those verbs at least twice as frequent
in the corpus as in general English. Based on these
selectional associations, CorMet clusters the nouns
for which the characteristic verbs select. To iden-
tify metaphors, mappings are sought from clusters
in the source corpus to clusters in the target corpus,
based on the degree to which the same verbs select
for members of both clusters. For example, CorMet
was used to extract the metaphor MONEY IS A LIQ-
UID1 by mapping from a cluster for the concept liq-
uid in a corpus for the domain LABORATORY to
a cluster for the concept money in a corpus for the
domain FINANCE, based on the selectional associ-
ations of verbs such as “pour,” “flow,” “freeze,” and
“evaporate.” The CMI system described in this pa-
per is informed largely by CorMet (Mason, 2004).
</bodyText>
<subsectionHeader confidence="0.999328">
2.2 Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.9987815">
While interpretations vary somewhat, semantic role
labeling (SRL) generally aims to represent some-
thing about the meaning of a phrase at a deeper
level than surface syntactic structure. One of the
most common approaches to performing SRL au-
tomatically is to use a statistical classifier trained
on labeled corpora (Gildea and Jurafsky, 2002),
with FrameNet (Baker et al., 1998) and PropBank
(Palmer et al., 2005) being the primary sources. An
important result of the Gildea and Jurafsky work
was identifying the significant utility of using pre-
segmented constituents as input to their labeler, and
accordingly most SRL systems perform a syntactic
analysis as an initial step.
The principal alternative to using a statistical clas-
sifier is to use a rule-based labeler for operating on
</bodyText>
<footnote confidence="0.991483">
1SMALL CAPS are metaphors, italics are concepts, CAPS
are domains, and “quotes” are example phrases.
</footnote>
<page confidence="0.998774">
15
</page>
<bodyText confidence="0.999917611111111">
the syntactic parse tree. For example, Shi and Mi-
halcea (2004) extract explicit SRL rules by analyz-
ing FrameNet cases. Another system, RelEx (Fun-
del et al., 2006) also uses rules and is structured like
the implementation used here (see below for details),
but despite having the same name, is a different sys-
tem. Statistical and rule-based methods may also be
used within the same system, such as in LTH (Jo-
hansson and Nugues, 2008).
One reason for preferring a rule-based SRL sys-
tem is that rule-based approaches may be less sus-
ceptible to the loss of accuracy that statistically
trained classifiers suffer when applied to domains
that are different than the corpora they are trained
on (Johansson and Nugues, 2008). That problem is
compounded by the limited domain coverage pro-
vided by the labeled corpora currently available for
SRL classifier training (Gildea and Jurafsky, 2002).
</bodyText>
<sectionHeader confidence="0.989913" genericHeader="method">
3 Computational Metaphor Identification
</sectionHeader>
<bodyText confidence="0.999985405405405">
While space precludes a fully detailed description
of the algorithms involved, this section provides a
high-level summary of the techniques employed in
CMI (Baumer, 2009; Baumer et al., under review).
Metaphors are conceptual mappings wherein a
source concept partially structures the understand-
ing of a target concept. In ELECTION IS WAR, the
target concept election is partially framed in terms
of the source concept war. CMI begins by gather-
ing corpora for the source and target domains. In
this paper, the target corpus consists of posts from
political blogs, described in more detail in the meth-
ods section below. Source corpora are composed of
Wikipedia articles, as they provide a readily avail-
able, categorically organized, large source of con-
tent on a wide variety of topics. A source corpus
for a given domain consists of all the Wikipedia ar-
ticles in the category for that domain, as well as all
articles in its subcategories. All documents in the
source and target corpora are parsed to extract sen-
tence structure and typed dependencies (Klein and
Manning, 2003; de Marneffe et al., 2006).
The crux of CMI is selectional preference learn-
ing (Resnik, 1993), which quantifies the tendency of
particular words to appear with certain other classes
of words in specific grammatical relationships. For
example, words for the concept of food are often
the direct object of the verb “eat.” Using the parsed
documents, CMI calculates selectional preferences
of the characteristic nouns in a corpus, where char-
acteristic means that the noun is highly frequent in
the corpus relative to its frequency in general En-
glish, as derived from (Kilgarriff, 1996). Selectional
preference is quantified as the relative entropy of the
posterior distribution conditioned on a specific noun
and grammatical relation with respect to the prior
distribution of verbs in general English:
</bodyText>
<equation confidence="0.999656666666667">
P (v|c)
P (v|c) log(1)
P (v)
</equation>
<bodyText confidence="0.999986">
where c is a class of nouns (i.e., a concept like
food) and a grammatical relation (such as direct ob-
ject), and v ranges over all the verbs for which c ap-
pears in the given relation. These selectional prefer-
ence strengths are then divided among the verbs that
appear in each grammatical relation to determine the
noun class’s selectional association for each verb in
each relation (Resnik, 1993).
Selectional associations are calculated for classes
of words, but the corpora consist of words that may
represent many possible classes of nouns. Thus, in-
dividual nouns count as partial observations of each
word class that they might represent using WordNet
(Fellbaum, 1998). For example, “vote,” “primary,”
and “runoff” can all represent the concept of elec-
tion. Here we use a customized version of WordNet
that includes major political figures from the 2008
US Election. These word classes are then clustered
using two-nearest-neighbor clustering based on the
verbs for which they select. Each cluster represents
a coherent concept in the corpus, and each is auto-
matically labeled based on the synsets it contains.
This approach of using clustered hypernyms res-
onates with Lakoff’s argument that metaphorical
mappings occur not at the level of situational
specifics, but at the superordinate level. For exam-
ple, in the metaphor LOVE IS A JOURNEY, the re-
lationship is a vehicle. Although specific instantia-
tions of the metaphor may frame that vehicle var-
iously as a train (“off the track”), a car (“long,
bumpy road”), or a plane (“just taking off”), “the
categories mapped will tend to be at the superordi-
nate level rather than the basic level” (Lakoff, 1993,
p. 212). This method of counting each word ob-
served as a partial observation of each of the synsets
</bodyText>
<equation confidence="0.950299">
�
S(c) =
V
</equation>
<page confidence="0.953394">
16
</page>
<bodyText confidence="0.999992425">
it might represent causes observations at the basic
level to accumulate in the superordinate levels they
collectively represent. This is not to say that hier-
archical conceptual relations capture every possible
metaphor, but rather that these are the relations on
which we focus here.
To identify metaphors, CMI looks for correspon-
dences between conceptual clusters in the source
and target corpora. For example, in the Military cor-
pus, the cluster for war would frequently select to be
the direct object of “win,” the object of the preposi-
tion “during” with the verb “fight,” the object of the
preposition “in” with the verb “defeated,” and so on.
In some blog corpora, the cluster for election also se-
lects for those same verbs in the same grammatical
relationships. Based on the similarity of these selec-
tional associations, each mapping is given a confi-
dence score to indicate how likely the linguistic pat-
terns are to evidence a conceptual metaphor. One of
the strengths of CMI is that it works in the aggre-
gate. While individual instances of phrases such as
“fought during the election” and “defeated in the pri-
mary” may not at first glance appear metaphorical,
it is the systematicity of these patterns that becomes
compelling evidence for the existence of a metaphor.
An important aspect of CMI is that it identifies
only linguistic patterns potentially indicative of con-
ceptual metaphors, not the metaphors themselves.
As mentioned above, Lakoff (1993) emphasizes that
metaphor is primarily a cognitive phenomenon, and
that metaphorical language serves as evidence for
the cognitive phenomenon. CMI leverages computa-
tional power to search through large bodies of text to
identify patterns of potential interest, then presents
those patterns to a human user along with the po-
tential metaphors they might imply to foster critical
thinking about metaphor. To reiterate, this places the
job of finding patterns in the hands of the computer,
and the job of interpreting those patterns in the hands
of the human user.
</bodyText>
<sectionHeader confidence="0.978746" genericHeader="method">
4 CMI with Semantic Role Labeling
</sectionHeader>
<bodyText confidence="0.999974038461539">
The work presented in this paper attempts to en-
hance CMI by using SRL to expand the types of
relations between nouns and verbs that can be seen
as instantiating a metaphor. The prior CMI imple-
mentation treats each grammatical dependency type
as a distinct relation. For example, in the sentence,
“The city contained a sacred grove for performing
religious rites,” “rites” is the direct object of “per-
form,” as denoted by the dobj dependency. How-
ever, the sentence, “The religious rites were once
again performed openly,” uses a passive construc-
tion, meaning that “rites” is the passive subject, or
nsubjpass, of “perform.” With SRL, the relations
between “perform” and “rite” are the same for both
sentences; specifically, Intentionally act:Act (“rite”
is the intentional act being performed) and Transi-
tive action:Patient (“rite” is the recipient of a tran-
sitive action). Because the relations in FrameNet
are organized into an inheritance structure, both the
more general frame Transitive action and the more
specialized frame Intentionally act apply here.
This section describes how SRL was incorporated
into CMI, compares the component data derived
from SRL with the data derived from a typed de-
pendency parse, and compares resulting identified
metaphors.
</bodyText>
<subsectionHeader confidence="0.671541">
4.1 Implementation Methods
</subsectionHeader>
<bodyText confidence="0.99314376">
The CMI system used here takes the prior im-
plementation (described in section 3) and replaces
the Stanford typed dependency parser (de Marn-
effe et al., 2006) with the RelEx SRL system
(http://opencog.org/wiki/RelEx). RelEx performs a
full syntactic parse, then applies a set of syntactic
pattern rules to annotate the parse tree with role la-
bels based (not exactly or completely) on FrameNet.
This implementation uses a rule-based labeler be-
cause CMI hinges on differences in selectional pref-
erences in corpora from different domains, and sta-
tistically trained classifiers are biased by the distri-
butions of the corpora on which they are trained.
For syntactic parsing, RelEx uses the Link
Grammar Parser (LGP) which is based on the
Link Grammar model (Sleator and Temper-
ley, 1993). LGP produces output very sim-
ilar to typed dependencies. The version of
RelEx we use integrates the Another Nearly-
New Information Extraction (ANNIE) system
(http://gate.ac.uk/sale/tao/splitch6.html#chap:annie)
to tag named entities. Sentences are
split using the OpenNLP sentence splitter
(http://opennlp.sourceforge.net/).
Because CMI’s corpora are acquired from public
</bodyText>
<page confidence="0.973643">
17
</page>
<table confidence="0.99942575">
Blogs Religion (Wikipedia)
Docs 546 (604) 3289 (3294)
Sents 5732 (6708) 128,543 (145,193)
Words 148,619 3,300,455
</table>
<tableCaption confidence="0.997022666666667">
Table 1: Sizes of the target and source corpora; parenthe-
ses show totals including documents without valid sen-
tences and sentences with no relations.
</tableCaption>
<bodyText confidence="0.999303588235294">
Internet sources, the text must be cleaned to make
it suitable for parsing. Text from Wikipedia arti-
cles undergoes many small filtering steps in order
to remove wiki markup, omit article sections that
do not consist primarily of prose (e.g., “See Also”
and “References”), and decompose Unicode letters
and punctuation into compatibility form. Wikipedia
articles also tend to use bulleted lists in the middle
of sentences rather than comma-separated clauses.
We attempt to convert those constructions back into
sentences, which only sometimes results in a rea-
sonable sentence. However, it helps to ensure that
the following sentence is properly recognized by the
sentence splitter. For blog posts, HTML tags were
removed, which at times required multiple decoding
passes due to improperly configured blog feeds, and
characters decomposed into compatible form.
</bodyText>
<subsectionHeader confidence="0.963661">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.973850578947368">
Table 1 shows statistics on the sizes of the source
and target corpora. Numbers in parentheses are to-
tals, including blank documents and sentences with
no valid relations. There are some sentences for
which RelEx does not produce any parse, e.g., long
sentences that LGP deems ungrammatical. The
Stanford parser produced some result for every sen-
tence, because it will produce a result tree for any
kind of text, even if it does not recognize any gram-
matically valid tokens.
Table 2 lists the number of verb-noun relations
for each corpus, with parentheses showing aver-
age relations per word. Since RelEx often la-
bels the same verb-noun relation with multiple
hierarchically-related frames (as described above),
Table 2 also lists the number of unique verb-noun
pairs labeled. For the blogs corpus, the Stan-
ford parser generated 111 distinct dependency types,
while RelEx labeled 1446 distinct roles. The ten
</bodyText>
<table confidence="0.998948666666667">
Stanford Blogs Religion
Reln(v, n) 19,303 (2.88) 425,367 (2.93)
Unique(v, n) 19,303 (2.88) 425,367 (2.93)
RelEx Blogs Religion
Reln(v, n) 57,639 (8.59) 1,219,345 (8.40)
Unique(v, n) 20,962 (3.12) 482,997 (3.33)
</table>
<tableCaption confidence="0.905837">
Table 2: Relations for the target and source corpora;
parentheses show average relations per word.
</tableCaption>
<table confidence="0.999823416666667">
Stanford RelEx
Relation Freq Relation Freq
dobj 3815 Transitive action:Patient 4268
nsubj 3739 Transitive action:Agent 3597
prep in 1072 Inheritance:Item 2 1489
prep to 695 Categorization:Category 1489
prep on 563 Attributes:Attribute 1488
nsubjpass 528 Existence:Entity 1277
prep for 491 Categorization:Item 1270
prep with 435 Inheritance:Item 1 1269
prep at 285 Attributes:Entity 1268
dep 279 Purpose:Means 569
</table>
<tableCaption confidence="0.999833">
Table 3: Most common dependencies and frequencies.
</tableCaption>
<bodyText confidence="0.997601142857143">
most common of each are listed with their frequen-
cies in Table 3.
These data show that RelEx provides more infor-
mation, both in terms of successfully parsing more
sentences, and in terms of relations-per-word. The
next section explores the impact of these differences
on identified metaphors.
</bodyText>
<subsectionHeader confidence="0.919027">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999996153846154">
This section describes metaphors identified when
mapping from the RELIGION source corpus to the
political blogs target corpus. CMI results are usu-
ally culled to include only the upper one percentile
in terms of confidence, but space constraints prohibit
a full analysis of even this upper one percentile. In-
stead, this section compares mappings with the high-
est confidence score from the typed dependency data
and from the semantic role data. RELIGION was
chosen as the source domain because the highest
confidence metaphors from both typed dependencies
and semantic roles had similar target and source con-
cepts, facilitating a better comparison. This analysis
</bodyText>
<page confidence="0.998134">
18
</page>
<bodyText confidence="0.996593615384616">
Target (label and cluster) Source (label and cluster) Conf
medicine - {medicine, medical specialty}, {medicine,
medication, medicament, medicinal drug}, {music,
medicine}, {medicine, practice of medicine}, {learned
profession}, {drug}, {social control}, {profession},
{punishment, penalty, penalization, penalisation}, {life
science, bioscience}
sacrament - {sacrament},
{baptism}, {religious ceremony,
religious ritual}
ritual - {ceremony}, {practice,
pattern}, {custom, usage, usance},
{ritual, rite}, {survival}
</bodyText>
<page confidence="0.4591995">
1.968
1.465
</page>
<tableCaption confidence="0.977012">
Table 4: Metaphors for medicine from RELIGION using typed dependencies.
</tableCaption>
<bodyText confidence="0.998634877192983">
is not intended to demonstrate that either technique
is superior (for more on possible evaluation meth-
ods, see Discussion section below). Rather, it pro-
vides a detailed depiction of both to ascertain poten-
tial benefits and drawbacks of each.
Table 4 presents the strongest two mappings
from RELIGION: MEDICINE IS A SACRAMENT and
MEDICINE IS A RITUAL; these were the only map-
pings for medicine in the upper one percentile. Each
mapping lists both the automatically identified la-
bels and the full cluster contents for source and tar-
get, along with the confidence score. The table can
be read left-to-right, e.g., “medicine is like a sacra-
ment.” Confidence scores typically fall in the range
(0, 5) with a few high-confidence mappings and
many low-confidence mappings; see (Baumer, 2009;
Baumer et al., under review) for details of confi-
dence score calculation. Table 5 shows details for
each mapping, including the verb-relation pairs that
mediate the mapping, along with an example frag-
ment from the target and source corpora for each
verb-relation. These examples show why and how
medicine might be like, variously, a sacrament or
a ritual; both are “practiced,” “administered,” “per-
formed,” etc. Note that the passive subject and di-
rect object relations are treated as distinct, e.g., “Eu-
charist is variously administered” involves a differ-
ent grammatical relation than “administer the sacra-
ment,” even though the word for sacrament plays a
similar semantic role in both fragments.
Tables 6 and 7 show mappings resulting from se-
mantic roles labeled by RelEx, with formats simi-
lar to those of tables 4 and 5, except that the verb-
relations in table 7 are semantic roles rather than
grammatical relations. The mapping in table 6 was
the strongest mapping from RELIGION and the only
mapping for medication.
Table 7 shows how RelEx can treat different
grammatical relations as the same semantic role. For
example, “medicine is practiced” and “practice the
rites” use passive subjective and direct object, re-
spectively, but are both treated as the patient of a
transitive action. Such examples confirm that SRL
is, at least to some extent, performing the job for
which it was intended.
However, these results also expose some prob-
lems with SRL, or at least with RelEx’s implemen-
tation thereof. For example, the phrase “dispose of
prescription drugs” is labeled with four separate se-
mantic roles, which is an instance of a single verb-
noun relation being labeled with both a superordi-
nate relation, Physical entity:Entity, and a subordi-
nate relation, Physical entity:Constituents (the con-
stituents of a physical entity are themselves an en-
tity). While various approaches might avoid multi-
ple labels, e.g., using only the most general or most
specific frame, those are beyond the scope here.
</bodyText>
<sectionHeader confidence="0.998928" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999952133333333">
As mentioned above, these results do not provide
conclusive evidence that either typed dependencies
or semantic roles are more effective for identify-
ing potential metaphors. However, they do provide
an understanding of both techniques’ strengths and
weaknesses for this purpose, and they also suggest
ways in which each may be more or less effective at
fostering critical and creative thinking.
For metaphor identification, the previous sec-
tion described how typed dependency parsing treats
passive subjects and direct object as distinct rela-
tions, whereas SRL will at times conflate them into
identical patient roles. This means that the typed
dependency-based metaphors appear to be mediated
by a greater number of relations. However, it also
</bodyText>
<page confidence="0.99788">
19
</page>
<table confidence="0.884577375">
Target Source Verb-Reln Target Ex Frag Source Ex Frag
practice - “medicine is practiced” “rites were practiced”
nsubjpass
administer - “antibiotics are regularly “Eucharist is variously ad-
nsubjpass administered” ministered”
sacrament administer - dobj “administered medicines” “administer the sacra-
ment”
perform - dobj “perform defensive “performed the last rites”
medicine”
medicine
receive - dobj “received conventional “received the rites”
medicines”
perform - dobj “perform defensive “performed the last rites”
medicine”
practice - “medicine is practiced” “ceremonies are also prac-
ritual nsubjpass ticed”
</table>
<tableCaption confidence="0.997441">
Table 7: Details of RELIGION metaphors from semantic roles, including mediators and example phrases.
</tableCaption>
<figure confidence="0.9838205">
administer -
nsubjpass
“sacrament is ordinarily
administered”
“antibiotics are regularly
administered”
medication - {medicine, medication, medica-
ment, medicinal drug}, {drug}, {agent}
ceremony - {ceremony}, {sacrament}, {rite, reli-
gious rite}, {religious ceremony, religious ritual} 2.570
“medicine is prac-
ticed”
“practice the rites”
practice -
Transitive action:Patient
“perform defensive
medicine”
“perform most reli-
gious rites”
perform -
Transitive action:Patient
include -
Transitive action:Agent
“medicine including” “liturgies included”
dispose -
Physical entity:Constituents
dispose -
Inheritance:Instance
“dispose of prescrip-
tion drugs”
“dispose of prescrip-
tion drugs”
“disposed of without
ceremony”
“disposed of without
ceremony”
medication ceremony
administer - dobj “administered medicines” “administering the rites”
</figure>
<tableCaption confidence="0.938466">
Table 5: Details of RELIGION metaphors from typed dependencies, including mediators and example phrases.
Target (label and cluster) Source (label and cluster) Conf
Table 6: Metaphor for medication from RELIGION using semantic roles.
</tableCaption>
<figure confidence="0.936289076923077">
Target Source Verb-Reln Target Ex Frag Source Ex Frag
dispose -
Inheritance:Group
dispose -
Physical entity:Entity
“dispose of prescrip-
tion drugs”
“dispose of prescrip-
tion drugs”
“disposed of without
ceremony”
“disposed of without
ceremony”
</figure>
<page confidence="0.887236">
20
</page>
<bodyText confidence="0.999953705882353">
means that less data are available to the selection
preference calculation, in that there are fewer obser-
vations for each relation. On the other hand, SRL
is a much finer-grained classification than typed de-
pendencies. The implementation used here included
111 grammatical relations, whereas RelEx labeled
1446 distinct roles. Thus, overall, RelEx may be
providing fewer observations for each relation, but
those relations may have more semantic import.
For fostering critical thinking and creativity, a
key concern is making identified metaphors read-
ily comprehensible. Ortony (Ortony, 1980) and oth-
ers have suggested that selectional restriction vi-
olations are an important component of metaphor
comprehension. Therefore, tools that employ CMI
often present parallel source and target fragments
side-by-side to make clear the selectional restric-
tion violation, e.g., metaViz, a system for present-
ing computationally identified metaphors in politi-
cal blogs (Baumer et al., 2010). One might assume
that typed dependencies are more readily compre-
hensible, since they are expressed as relatively sim-
ple grammatical relations. However, when present-
ing example fragments to users, there is no need to
explicate the nature of the relationship being demon-
strated, but rather the parallel examples can simply
be placed side-by-side. It is an empirical question
whether users would see phrases such as “medicine
is practiced” and “practice the rites” as parallel ex-
amples of the same psycholinguistic relationship.
Thus, the question of whether typed dependencies
or semantic roles better facilitate metaphor compre-
hension may not be as important as the question of
whether example phrases are perceived as parallel.
</bodyText>
<sectionHeader confidence="0.999818" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.997160428571429">
This paper is only an initial exploration, demonstrat-
ing that semantic role labeling is viable for use in
CMI. For the sake of comparison, the analysis here
focuses on examples where metaphors identified us-
ing the two techniques were relatively similar. How-
ever, such similarity does not always occur. For
example, using MILITARY as the source domain,
typed dependencies led to results such as A NOM-
INEE IS A FORCE and A NOMINEE IS AN ARMY,
whereas semantic roles gave mappings including AN
INDIVIDUAL IS A WEAPON (here, the label “indi-
vidual” is a superordinate category including mostly
politicians), and THE US IS A SOLDIER. Future
work should analyze these differences in more de-
tail to provide a broad and deep comparison across
multiple source domains and target corpora.
But how should such an analysis be conducted?
That is, how does one determine which identified
metaphors are “better,” and by what standard? In
suggesting a number of potential evaluation methods
for CMI, Baumer et al. (under review) argue that the
most sensible approach is asking human subjects to
assess metaphors, potentially along a variety of cri-
teria. For example: Does the metaphor make sense?
Is it unexpected? Is it confusing? Such assess-
ments could help evaluate semantic roles vs. typed
dependencies in two ways. First, does either pars-
ing technique lead to metaphors that are consistently
assessed by subjects as better? Second, does ei-
ther parsing technique lead to better alignment (i.e.,
stronger correlations) between human assessments
and CMI confidence scores? Such subjective as-
sessments could provide evidence for an argument
that either typed dependencies or semantic roles are
more effective at identifying conceptual metaphors.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999957571428572">
This paper explores using semantic role labeling
(SRL) as a technique for improving computational
metaphor identification (CMI). The results show that
SRL can be successfully incorporated into CMI.
Furthermore, they suggest that SRL may be more
effective at identifying relationships with semantic
import than typed dependency parsing, but that SRL
may also make distinctions that are too fine-grained
to serve as effective input for the selectional pref-
erence learning involved in CMI. The results also
demonstrate that, even though the notion of seman-
tic roles may seem more complex than typed de-
pendencies from a user’s perspective, it is possi-
ble to present either in a way that may be readily
comprehensible. Thus, while more work is neces-
sary to compare these two parsing techniques more
fully, semantic role labeling may present an effective
means of improving CMI, both in terms of the tech-
nical process of identifying conceptual metaphors,
and in terms of the broader goal of fostering critical
thinking and creativity.
</bodyText>
<page confidence="0.998845">
21
</page>
<sectionHeader confidence="0.999212" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999231571428571">
This material is based on work supported by the
National Science Foundation under Grant No. IIS-
0757646, by the Donald Bren School of Informa-
tion and Computer Sciences, by the California Insti-
tute for Telecommunications and Information Tech-
nology (Calit2), and by the Undergraduate Research
Opportunities Program (UROP) at UCI.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999198242105263">
Colin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proc 17th
Int’l Conf on Comp Ling, pages 86–90, Montral, Que-
bec, Canada.
Eric P. S. Baumer, Jordan Sinclair, and Bill Tomlinson.
2010. “America is like Metamucil:” Critical and cre-
ative thinking about metaphor in political blogs. In
ACM SIGCHI Conf, Atlanta, GA. ACM Press.
Eric P. S. Baumer, David Hubin, and Bill Tomlinson. un-
der review. Computational metaphor identification.
Eric Baumer. 2009. Computational Metaphor Identifica-
tion to Foster Critical Thinking and Creativity. Dis-
sertation, University of California, Irvine, Department
of Informatics.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Lang
Res and Eval (LREC), Genoa, Italy.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Comp Ling,
17(1):49–90.
Christine Fellbaum. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2006.
RelEx-Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365–371.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In 3rd Work-
shop on Scalable Natural Language Understanding,
New York City. Assoc Comp Ling.
Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and
C. Boronat. 2001. Metaphor is like analogy. In Dedre
Gentner, Keith J. Holyoak, and Boicho Kokinov, edi-
tors, The Analogical Mind, pages 199–253. MIT Press,
Cambridge, MA.
Raymond W. Gibbs. 1984. Literal meaning and psycho-
logical theory. Cognitive Science, 8:275–304.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Comp Ling, 28(3):245–288.
W.J.J. Gordon. 1974. Some source material in discovery-
by-analogy. Journal of Creative Behavior, 8:239–257.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proc Conf on Empirical Meth in Nat Lang
Proc, pages 69–78, Honolulu, HI. Assoc Comp Ling.
Adam Kilgarriff. 1996. BNC word frequency list.
http://www.kilgarriff.co.uk/bnc-readme.html.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Mtg of the Assoc for Comp
Ling, Sapporo, Japan.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Xiaofei Lu and Anna Feldman, editors, Computa-
tional Approaches to Figurative Language, Workshop
at HLT/NAACL 2007, Rochester, NY.
George Lakoff and Mark Johnson. 1980. Metaphors We
Live By. University of Chicago Press, Chicago, IL,
2003 edition.
George Lakoff and Mark Turner. 1989. More Than Cool
Reason: A Field Guide to Poetic Metaphor. University
of Chicago Press, Chicago and London.
George Lakoff. 1993. The contemporary theory of
metaphor. In A. Ortony, editor, Metaphor and thought,
2nd. ed., pages 202–251. Cambridge Univ Press, New
York.
James H. Martin. 1990. A Computational Model of
Metaphor Interpretation. Acad Press, San Diego, CA.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Comp Ling, 30(1):23–44, March.
Andrew Ortony. 1980. Some psycholinguistic aspects of
metaphor. In R.P. Honeck and H.R. Robert, editors,
Cog and Fig Lang, pages 69–83. Erlbaum Associates,
Hillsdale, NJ.
Wendy Oxman-Michelli. 1991. Critical thinking as cre-
ativity. Technical Report SO 023 597, Montclair State,
Institute for Critical Thinking, Montclair, NJ.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Comp Ling, 31(1):71–106, March.
Michael J. Reddy. 1969. A semantic approach to
metaphor. In Chicago Linguistic Society Collected Pa-
pers, pages 240–251. Chicago Univ Press, Chicago.
Philip Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships. Dis-
sertation, University of Pennsylvania, Department of
Computer and Information Science.
Lei Shi and Rada Mihalcea. 2004. Open text seman-
tic parsing using FrameNet and WordNet. In Demon-
stration Papers at HLT-NAACL 2004, pages 19–22,
Boston. Assoc for Computational Linguistics.
Daniel Sleator and Davy Temperley. 1993. Parsing En-
glish with a Link Grammar. In Proc Third Interna-
tional Workshop on Parsing Technologies, pages 277–
292.
</reference>
<page confidence="0.999019">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962209">
<title confidence="0.999434">Comparing Semantic Role Labeling with Typed Dependency Parsing Computational Metaphor Identification</title>
<author confidence="0.999913">P S Eric</author>
<affiliation confidence="0.9999185">Department of Univ of California,</affiliation>
<address confidence="0.998841">5029 Donald Bren Irvine, CA 92627-3440</address>
<email confidence="0.999703">ebaumer@ics.uci.edu</email>
<author confidence="0.999698">P James</author>
<affiliation confidence="0.998697">School of Information Computer Univ of California,</affiliation>
<address confidence="0.987188">Irvine, CA</address>
<email confidence="0.99815">jpwhite@uci.edu</email>
<author confidence="0.994754">Bill</author>
<affiliation confidence="0.9998015">Department of Univ of California,</affiliation>
<address confidence="0.998634">5068 Donald Bren Irvine, CA 92627-3440</address>
<email confidence="0.999671">wmt@uci.edu</email>
<abstract confidence="0.999662217391304">Most computational approaches to metaphor have focused on discerning between metaphorical and literal text. Recent work on computational metaphor identification (CMI) instead seeks to identify overarching conceptual metaphors by mapping selectional preferences between source and target corpora. This paper explores using semantic role labeling (SRL) in CMI. Its goals are two-fold: first, to demonstrate that semantic roles can effectively be used to identify conceptual metaphors, and second, to compare SRL to the current use of typed dependency parsing in CMI. The results show that SRL can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proc 17th Int’l Conf on Comp Ling,</booktitle>
<pages>86--90</pages>
<location>Montral, Quebec, Canada.</location>
<contexts>
<context position="7744" citStr="Baker et al., 1998" startWordPosition="1176" endWordPosition="1179">a corpus for the domain FINANCE, based on the selectional associations of verbs such as “pour,” “flow,” “freeze,” and “evaporate.” The CMI system described in this paper is informed largely by CorMet (Mason, 2004). 2.2 Semantic Role Labeling While interpretations vary somewhat, semantic role labeling (SRL) generally aims to represent something about the meaning of a phrase at a deeper level than surface syntactic structure. One of the most common approaches to performing SRL automatically is to use a statistical classifier trained on labeled corpora (Gildea and Jurafsky, 2002), with FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) being the primary sources. An important result of the Gildea and Jurafsky work was identifying the significant utility of using presegmented constituents as input to their labeler, and accordingly most SRL systems perform a syntactic analysis as an initial step. The principal alternative to using a statistical classifier is to use a rule-based labeler for operating on 1SMALL CAPS are metaphors, italics are concepts, CAPS are domains, and “quotes” are example phrases. 15 the syntactic parse tree. For example, Shi and Mihalcea (2004) extract explicit SRL rules</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Colin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proc 17th Int’l Conf on Comp Ling, pages 86–90, Montral, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric P S Baumer</author>
<author>Jordan Sinclair</author>
<author>Bill Tomlinson</author>
</authors>
<title>America is like Metamucil:” Critical and creative thinking about metaphor in political blogs.</title>
<date>2010</date>
<booktitle>In ACM SIGCHI Conf,</booktitle>
<publisher>ACM Press.</publisher>
<location>Atlanta, GA.</location>
<contexts>
<context position="28274" citStr="Baumer et al., 2010" startWordPosition="4344" endWordPosition="4347">iding fewer observations for each relation, but those relations may have more semantic import. For fostering critical thinking and creativity, a key concern is making identified metaphors readily comprehensible. Ortony (Ortony, 1980) and others have suggested that selectional restriction violations are an important component of metaphor comprehension. Therefore, tools that employ CMI often present parallel source and target fragments side-by-side to make clear the selectional restriction violation, e.g., metaViz, a system for presenting computationally identified metaphors in political blogs (Baumer et al., 2010). One might assume that typed dependencies are more readily comprehensible, since they are expressed as relatively simple grammatical relations. However, when presenting example fragments to users, there is no need to explicate the nature of the relationship being demonstrated, but rather the parallel examples can simply be placed side-by-side. It is an empirical question whether users would see phrases such as “medicine is practiced” and “practice the rites” as parallel examples of the same psycholinguistic relationship. Thus, the question of whether typed dependencies or semantic roles bette</context>
</contexts>
<marker>Baumer, Sinclair, Tomlinson, 2010</marker>
<rawString>Eric P. S. Baumer, Jordan Sinclair, and Bill Tomlinson. 2010. “America is like Metamucil:” Critical and creative thinking about metaphor in political blogs. In ACM SIGCHI Conf, Atlanta, GA. ACM Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eric P S Baumer</author>
<author>David Hubin</author>
<author>Bill Tomlinson</author>
</authors>
<title>under review. Computational metaphor identification.</title>
<marker>Baumer, Hubin, Tomlinson, </marker>
<rawString>Eric P. S. Baumer, David Hubin, and Bill Tomlinson. under review. Computational metaphor identification.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Baumer</author>
</authors>
<title>Computational Metaphor Identification to Foster Critical Thinking and Creativity.</title>
<date>2009</date>
<institution>Dissertation, University of California, Irvine, Department of Informatics.</institution>
<contexts>
<context position="2495" citStr="Baumer, 2009" startWordPosition="374" endWordPosition="375"> of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so automatically and effortlessly, we find it hard to question [metaphors], if we can even notice them” (Lakoff and Turner, 1989, p. 65). Computational metaphor identification (CMI) (Baumer, 2009; Baumer et al., under review) addresses this difficulty by identifying potential conceptual metaphors in written text. Rather than attempting to discern whether individual phrases are metaphorical or literal, this technique instead identifies larger, overarching linguistic patterns. The goal of CMI is not to state definitively the metaphor present in a text, but rather to draw potential metaphors to readers’ attention, thereby encouraging both critical examination of current metaphors and creative generation of alternative metaphors. CMI identifies potential metaphors by mapping selectional p</context>
<context position="9349" citStr="Baumer, 2009" startWordPosition="1434" endWordPosition="1435">m is that rule-based approaches may be less susceptible to the loss of accuracy that statistically trained classifiers suffer when applied to domains that are different than the corpora they are trained on (Johansson and Nugues, 2008). That problem is compounded by the limited domain coverage provided by the labeled corpora currently available for SRL classifier training (Gildea and Jurafsky, 2002). 3 Computational Metaphor Identification While space precludes a fully detailed description of the algorithms involved, this section provides a high-level summary of the techniques employed in CMI (Baumer, 2009; Baumer et al., under review). Metaphors are conceptual mappings wherein a source concept partially structures the understanding of a target concept. In ELECTION IS WAR, the target concept election is partially framed in terms of the source concept war. CMI begins by gathering corpora for the source and target domains. In this paper, the target corpus consists of posts from political blogs, described in more detail in the methods section below. Source corpora are composed of Wikipedia articles, as they provide a readily available, categorically organized, large source of content on a wide var</context>
<context position="22518" citStr="Baumer, 2009" startWordPosition="3497" endWordPosition="3498">piction of both to ascertain potential benefits and drawbacks of each. Table 4 presents the strongest two mappings from RELIGION: MEDICINE IS A SACRAMENT and MEDICINE IS A RITUAL; these were the only mappings for medicine in the upper one percentile. Each mapping lists both the automatically identified labels and the full cluster contents for source and target, along with the confidence score. The table can be read left-to-right, e.g., “medicine is like a sacrament.” Confidence scores typically fall in the range (0, 5) with a few high-confidence mappings and many low-confidence mappings; see (Baumer, 2009; Baumer et al., under review) for details of confidence score calculation. Table 5 shows details for each mapping, including the verb-relation pairs that mediate the mapping, along with an example fragment from the target and source corpora for each verb-relation. These examples show why and how medicine might be like, variously, a sacrament or a ritual; both are “practiced,” “administered,” “performed,” etc. Note that the passive subject and direct object relations are treated as distinct, e.g., “Eucharist is variously administered” involves a different grammatical relation than “administer </context>
</contexts>
<marker>Baumer, 2009</marker>
<rawString>Eric Baumer. 2009. Computational Metaphor Identification to Foster Critical Thinking and Creativity. Dissertation, University of California, Irvine, Department of Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Lang Res and Eval (LREC),</booktitle>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Lang Res and Eval (LREC), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<title>Met*: A method for discriminating metonymy and metaphor by computer.</title>
<date>1991</date>
<journal>Comp Ling,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="1704" citStr="Fass, 1991" startWordPosition="247" endWordPosition="248">be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 1 Introduction Metaphor, the partial framing of one concept in terms of another, pervades human language and thought (Lakoff and Johnson, 1980; Lakoff, 1993). A variety of computational approaches to metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so a</context>
<context position="4746" citStr="Fass, 1991" startWordPosition="700" endWordPosition="701">s. However, neither typed dependencies nor semantic roles were necessarily superior. Rather, each provides certain advantages, both in terms of identifying potential metaphors, and in terms of promoting critical thinking and creativity. 2 Related Work 2.1 Computational Approaches to Metaphor Many computational approaches have been taken toward identifying metaphor in written text. MIDAS (Martin, 1990) attempts to detect when users of the Unix Consultant command line help system use metaphors, for example, “How do I enter Emacs?” is interpreted as “How do I invoke Emacs?” Another system, met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. More recently, Gedigian et al. (2006) used hand-annotated corpora to train an automatic metaphor classifier. Krishnakumaran and Zhu (2007) used violations of WordNetbased (Fellbaum, 1998) verb-noun expectations to identify the presence of a metaphor, e.g., “he is a brave lion,” would be considered metaphorical, because “he,” taken to mean a “person,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some d</context>
<context position="6186" citStr="Fass, 1991" startWordPosition="928" endWordPosition="929">eanings of the words themselves. In this view, a figurative interpretation is only sought only after a literal interpretation has been formed and found inconsistent, nonsensical, or otherwise faulty. However, experimental evidence has made this account suspect (Gibbs, 1984; Gentner et al., 2001). Even distinguishing whether a given expression is literal or figurative can be difficult at best. For example, “the rock is becoming brittle with age” (Reddy, 1969, p. 242), has “a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus” (Fass, 1991, p. 54). One previous metaphor system avoids making such literal/metaphorical distinctions. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. CorMet calculates selectional preferences and associations (Resnik, 1993) for each corpus’s characteristic verbs, i.e., those verbs at least twice as frequent in the corpus as in general English. Based on these selectional associations, CorMet clusters the nouns for which the characteristic verbs select. To identify metaphors, mappings are sought from clu</context>
</contexts>
<marker>Fass, 1991</marker>
<rawString>Dan Fass. 1991. Met*: A method for discriminating metonymy and metaphor by computer. Comp Ling, 17(1):49–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5092" citStr="Fellbaum, 1998" startWordPosition="751" endWordPosition="752">ard identifying metaphor in written text. MIDAS (Martin, 1990) attempts to detect when users of the Unix Consultant command line help system use metaphors, for example, “How do I enter Emacs?” is interpreted as “How do I invoke Emacs?” Another system, met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. More recently, Gedigian et al. (2006) used hand-annotated corpora to train an automatic metaphor classifier. Krishnakumaran and Zhu (2007) used violations of WordNetbased (Fellbaum, 1998) verb-noun expectations to identify the presence of a metaphor, e.g., “he is a brave lion,” would be considered metaphorical, because “he,” taken to mean a “person,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some degree to the literal meaning hypothesis (Reddy, 1969), which states that every sentence has a literal meaning, as derived from the meanings of its constituent words, while some also have a figurative meaning that goes beyond the meanings of the words themselves. In this view, a figurative interpretation is only sought only after a literal inter</context>
<context position="11753" citStr="Fellbaum, 1998" startWordPosition="1827" endWordPosition="1828">and a grammatical relation (such as direct object), and v ranges over all the verbs for which c appears in the given relation. These selectional preference strengths are then divided among the verbs that appear in each grammatical relation to determine the noun class’s selectional association for each verb in each relation (Resnik, 1993). Selectional associations are calculated for classes of words, but the corpora consist of words that may represent many possible classes of nouns. Thus, individual nouns count as partial observations of each word class that they might represent using WordNet (Fellbaum, 1998). For example, “vote,” “primary,” and “runoff” can all represent the concept of election. Here we use a customized version of WordNet that includes major political figures from the 2008 US Election. These word classes are then clustered using two-nearest-neighbor clustering based on the verbs for which they select. Each cluster represents a coherent concept in the corpus, and each is automatically labeled based on the synsets it contains. This approach of using clustered hypernyms resonates with Lakoff’s argument that metaphorical mappings occur not at the level of situational specifics, but a</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christine Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Fundel</author>
<author>Robert Kuffner</author>
<author>Ralf Zimmer</author>
</authors>
<title>RelEx-Relation extraction using dependency parse trees.</title>
<date>2006</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="8417" citStr="Fundel et al., 2006" startWordPosition="1284" endWordPosition="1288"> sources. An important result of the Gildea and Jurafsky work was identifying the significant utility of using presegmented constituents as input to their labeler, and accordingly most SRL systems perform a syntactic analysis as an initial step. The principal alternative to using a statistical classifier is to use a rule-based labeler for operating on 1SMALL CAPS are metaphors, italics are concepts, CAPS are domains, and “quotes” are example phrases. 15 the syntactic parse tree. For example, Shi and Mihalcea (2004) extract explicit SRL rules by analyzing FrameNet cases. Another system, RelEx (Fundel et al., 2006) also uses rules and is structured like the implementation used here (see below for details), but despite having the same name, is a different system. Statistical and rule-based methods may also be used within the same system, such as in LTH (Johansson and Nugues, 2008). One reason for preferring a rule-based SRL system is that rule-based approaches may be less susceptible to the loss of accuracy that statistically trained classifiers suffer when applied to domains that are different than the corpora they are trained on (Johansson and Nugues, 2008). That problem is compounded by the limited do</context>
</contexts>
<marker>Fundel, Kuffner, Zimmer, 2006</marker>
<rawString>Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2006. RelEx-Relation extraction using dependency parse trees. Bioinformatics, 23(3):365–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gedigian</author>
<author>John Bryant</author>
<author>Srini Narayanan</author>
<author>Branimir Ciric</author>
</authors>
<title>Catching metaphors.</title>
<date>2006</date>
<booktitle>In 3rd Workshop on Scalable Natural Language Understanding,</booktitle>
<location>New York City. Assoc Comp Ling.</location>
<contexts>
<context position="1727" citStr="Gedigian et al., 2006" startWordPosition="249" endWordPosition="252">dentify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 1 Introduction Metaphor, the partial framing of one concept in terms of another, pervades human language and thought (Lakoff and Johnson, 1980; Lakoff, 1993). A variety of computational approaches to metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so automatically and effort</context>
<context position="4942" citStr="Gedigian et al. (2006)" startWordPosition="727" endWordPosition="731">erms of promoting critical thinking and creativity. 2 Related Work 2.1 Computational Approaches to Metaphor Many computational approaches have been taken toward identifying metaphor in written text. MIDAS (Martin, 1990) attempts to detect when users of the Unix Consultant command line help system use metaphors, for example, “How do I enter Emacs?” is interpreted as “How do I invoke Emacs?” Another system, met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. More recently, Gedigian et al. (2006) used hand-annotated corpora to train an automatic metaphor classifier. Krishnakumaran and Zhu (2007) used violations of WordNetbased (Fellbaum, 1998) verb-noun expectations to identify the presence of a metaphor, e.g., “he is a brave lion,” would be considered metaphorical, because “he,” taken to mean a “person,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some degree to the literal meaning hypothesis (Reddy, 1969), which states that every sentence has a literal meaning, as derived from the meanings of its constituent words, while some also have a figurat</context>
</contexts>
<marker>Gedigian, Bryant, Narayanan, Ciric, 2006</marker>
<rawString>Matt Gedigian, John Bryant, Srini Narayanan, and Branimir Ciric. 2006. Catching metaphors. In 3rd Workshop on Scalable Natural Language Understanding, New York City. Assoc Comp Ling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
<author>Brian F Bowdle</author>
<author>Phillip Wolff</author>
<author>C Boronat</author>
</authors>
<title>Metaphor is like analogy.</title>
<date>2001</date>
<booktitle>The Analogical Mind,</booktitle>
<pages>199--253</pages>
<editor>In Dedre Gentner, Keith J. Holyoak, and Boicho Kokinov, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5872" citStr="Gentner et al., 2001" startWordPosition="872" endWordPosition="876">rson,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some degree to the literal meaning hypothesis (Reddy, 1969), which states that every sentence has a literal meaning, as derived from the meanings of its constituent words, while some also have a figurative meaning that goes beyond the meanings of the words themselves. In this view, a figurative interpretation is only sought only after a literal interpretation has been formed and found inconsistent, nonsensical, or otherwise faulty. However, experimental evidence has made this account suspect (Gibbs, 1984; Gentner et al., 2001). Even distinguishing whether a given expression is literal or figurative can be difficult at best. For example, “the rock is becoming brittle with age” (Reddy, 1969, p. 242), has “a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus” (Fass, 1991, p. 54). One previous metaphor system avoids making such literal/metaphorical distinctions. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. CorMet calculates selectional preferences a</context>
</contexts>
<marker>Gentner, Bowdle, Wolff, Boronat, 2001</marker>
<rawString>Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and C. Boronat. 2001. Metaphor is like analogy. In Dedre Gentner, Keith J. Holyoak, and Boicho Kokinov, editors, The Analogical Mind, pages 199–253. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond W Gibbs</author>
</authors>
<title>Literal meaning and psychological theory.</title>
<date>1984</date>
<journal>Cognitive Science,</journal>
<pages>8--275</pages>
<contexts>
<context position="5849" citStr="Gibbs, 1984" startWordPosition="870" endWordPosition="871">to mean a “person,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some degree to the literal meaning hypothesis (Reddy, 1969), which states that every sentence has a literal meaning, as derived from the meanings of its constituent words, while some also have a figurative meaning that goes beyond the meanings of the words themselves. In this view, a figurative interpretation is only sought only after a literal interpretation has been formed and found inconsistent, nonsensical, or otherwise faulty. However, experimental evidence has made this account suspect (Gibbs, 1984; Gentner et al., 2001). Even distinguishing whether a given expression is literal or figurative can be difficult at best. For example, “the rock is becoming brittle with age” (Reddy, 1969, p. 242), has “a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus” (Fass, 1991, p. 54). One previous metaphor system avoids making such literal/metaphorical distinctions. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. CorMet calculates se</context>
</contexts>
<marker>Gibbs, 1984</marker>
<rawString>Raymond W. Gibbs. 1984. Literal meaning and psychological theory. Cognitive Science, 8:275–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Comp Ling,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="3373" citStr="Gildea and Jurafsky, 2002" startWordPosition="501" endWordPosition="504">verarching linguistic patterns. The goal of CMI is not to state definitively the metaphor present in a text, but rather to draw potential metaphors to readers’ attention, thereby encouraging both critical examination of current metaphors and creative generation of alternative metaphors. CMI identifies potential metaphors by mapping selectional preferences (Resnik, 1993) from a source corpus to a target corpus. Previous work on CMI utilized typed dependency parses (de Marneffe et al., 2006) to calculate these selectional preferences. This paper explores the use of semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Johansson and Nugues, 2008) to calculate selectional preferences. Typed dependencies focus on syntactic structure and grammatical relations, while semantic roles emphasize conceptual and semantic structure, so SRL may 14 Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14–22, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics be more effective for identifying potential conceptual metaphors. This paper describes how SRL was incorporated into CMI and compares both the relational data and the metaphor</context>
<context position="7708" citStr="Gildea and Jurafsky, 2002" startWordPosition="1170" endWordPosition="1173">TORY to a cluster for the concept money in a corpus for the domain FINANCE, based on the selectional associations of verbs such as “pour,” “flow,” “freeze,” and “evaporate.” The CMI system described in this paper is informed largely by CorMet (Mason, 2004). 2.2 Semantic Role Labeling While interpretations vary somewhat, semantic role labeling (SRL) generally aims to represent something about the meaning of a phrase at a deeper level than surface syntactic structure. One of the most common approaches to performing SRL automatically is to use a statistical classifier trained on labeled corpora (Gildea and Jurafsky, 2002), with FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) being the primary sources. An important result of the Gildea and Jurafsky work was identifying the significant utility of using presegmented constituents as input to their labeler, and accordingly most SRL systems perform a syntactic analysis as an initial step. The principal alternative to using a statistical classifier is to use a rule-based labeler for operating on 1SMALL CAPS are metaphors, italics are concepts, CAPS are domains, and “quotes” are example phrases. 15 the syntactic parse tree. For example, Shi and Mihalc</context>
<context position="9138" citStr="Gildea and Jurafsky, 2002" startWordPosition="1403" endWordPosition="1406">ut despite having the same name, is a different system. Statistical and rule-based methods may also be used within the same system, such as in LTH (Johansson and Nugues, 2008). One reason for preferring a rule-based SRL system is that rule-based approaches may be less susceptible to the loss of accuracy that statistically trained classifiers suffer when applied to domains that are different than the corpora they are trained on (Johansson and Nugues, 2008). That problem is compounded by the limited domain coverage provided by the labeled corpora currently available for SRL classifier training (Gildea and Jurafsky, 2002). 3 Computational Metaphor Identification While space precludes a fully detailed description of the algorithms involved, this section provides a high-level summary of the techniques employed in CMI (Baumer, 2009; Baumer et al., under review). Metaphors are conceptual mappings wherein a source concept partially structures the understanding of a target concept. In ELECTION IS WAR, the target concept election is partially framed in terms of the source concept war. CMI begins by gathering corpora for the source and target domains. In this paper, the target corpus consists of posts from political b</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Comp Ling, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J J Gordon</author>
</authors>
<title>Some source material in discoveryby-analogy.</title>
<date>1974</date>
<journal>Journal of Creative Behavior,</journal>
<pages>8--239</pages>
<contexts>
<context position="2240" citStr="Gordon, 1974" startWordPosition="337" endWordPosition="338">o metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so automatically and effortlessly, we find it hard to question [metaphors], if we can even notice them” (Lakoff and Turner, 1989, p. 65). Computational metaphor identification (CMI) (Baumer, 2009; Baumer et al., under review) addresses this difficulty by identifying potential conceptual metaphors in written text. Rather than attempting to discern whether individual phrases are metaphorical or literal, this technique instead identifies larger, overarching linguistic patterns. The goal of CMI is not to state definitively the metaphor pr</context>
</contexts>
<marker>Gordon, 1974</marker>
<rawString>W.J.J. Gordon. 1974. Some source material in discoveryby-analogy. Journal of Creative Behavior, 8:239–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proc Conf on Empirical Meth in Nat Lang Proc,</booktitle>
<pages>69--78</pages>
<publisher>Assoc Comp Ling.</publisher>
<location>Honolulu, HI.</location>
<contexts>
<context position="3402" citStr="Johansson and Nugues, 2008" startWordPosition="505" endWordPosition="508">rns. The goal of CMI is not to state definitively the metaphor present in a text, but rather to draw potential metaphors to readers’ attention, thereby encouraging both critical examination of current metaphors and creative generation of alternative metaphors. CMI identifies potential metaphors by mapping selectional preferences (Resnik, 1993) from a source corpus to a target corpus. Previous work on CMI utilized typed dependency parses (de Marneffe et al., 2006) to calculate these selectional preferences. This paper explores the use of semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Johansson and Nugues, 2008) to calculate selectional preferences. Typed dependencies focus on syntactic structure and grammatical relations, while semantic roles emphasize conceptual and semantic structure, so SRL may 14 Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14–22, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics be more effective for identifying potential conceptual metaphors. This paper describes how SRL was incorporated into CMI and compares both the relational data and the metaphors identified with typed depen</context>
<context position="8687" citStr="Johansson and Nugues, 2008" startWordPosition="1331" endWordPosition="1335">ernative to using a statistical classifier is to use a rule-based labeler for operating on 1SMALL CAPS are metaphors, italics are concepts, CAPS are domains, and “quotes” are example phrases. 15 the syntactic parse tree. For example, Shi and Mihalcea (2004) extract explicit SRL rules by analyzing FrameNet cases. Another system, RelEx (Fundel et al., 2006) also uses rules and is structured like the implementation used here (see below for details), but despite having the same name, is a different system. Statistical and rule-based methods may also be used within the same system, such as in LTH (Johansson and Nugues, 2008). One reason for preferring a rule-based SRL system is that rule-based approaches may be less susceptible to the loss of accuracy that statistically trained classifiers suffer when applied to domains that are different than the corpora they are trained on (Johansson and Nugues, 2008). That problem is compounded by the limited domain coverage provided by the labeled corpora currently available for SRL classifier training (Gildea and Jurafsky, 2002). 3 Computational Metaphor Identification While space precludes a fully detailed description of the algorithms involved, this section provides a high</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of PropBank. In Proc Conf on Empirical Meth in Nat Lang Proc, pages 69–78, Honolulu, HI. Assoc Comp Ling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<date>1996</date>
<note>BNC word frequency list. http://www.kilgarriff.co.uk/bnc-readme.html.</note>
<contexts>
<context position="10836" citStr="Kilgarriff, 1996" startWordPosition="1678" endWordPosition="1679">endencies (Klein and Manning, 2003; de Marneffe et al., 2006). The crux of CMI is selectional preference learning (Resnik, 1993), which quantifies the tendency of particular words to appear with certain other classes of words in specific grammatical relationships. For example, words for the concept of food are often the direct object of the verb “eat.” Using the parsed documents, CMI calculates selectional preferences of the characteristic nouns in a corpus, where characteristic means that the noun is highly frequent in the corpus relative to its frequency in general English, as derived from (Kilgarriff, 1996). Selectional preference is quantified as the relative entropy of the posterior distribution conditioned on a specific noun and grammatical relation with respect to the prior distribution of verbs in general English: P (v|c) P (v|c) log(1) P (v) where c is a class of nouns (i.e., a concept like food) and a grammatical relation (such as direct object), and v ranges over all the verbs for which c appears in the given relation. These selectional preference strengths are then divided among the verbs that appear in each grammatical relation to determine the noun class’s selectional association for </context>
</contexts>
<marker>Kilgarriff, 1996</marker>
<rawString>Adam Kilgarriff. 1996. BNC word frequency list. http://www.kilgarriff.co.uk/bnc-readme.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Mtg of the Assoc for Comp Ling,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="10253" citStr="Klein and Manning, 2003" startWordPosition="1583" endWordPosition="1586">rpora for the source and target domains. In this paper, the target corpus consists of posts from political blogs, described in more detail in the methods section below. Source corpora are composed of Wikipedia articles, as they provide a readily available, categorically organized, large source of content on a wide variety of topics. A source corpus for a given domain consists of all the Wikipedia articles in the category for that domain, as well as all articles in its subcategories. All documents in the source and target corpora are parsed to extract sentence structure and typed dependencies (Klein and Manning, 2003; de Marneffe et al., 2006). The crux of CMI is selectional preference learning (Resnik, 1993), which quantifies the tendency of particular words to appear with certain other classes of words in specific grammatical relationships. For example, words for the concept of food are often the direct object of the verb “eat.” Using the parsed documents, CMI calculates selectional preferences of the characteristic nouns in a corpus, where characteristic means that the noun is highly frequent in the corpus relative to its frequency in general English, as derived from (Kilgarriff, 1996). Selectional pre</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Mtg of the Assoc for Comp Ling, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saisuresh Krishnakumaran</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Hunting elusive metaphors using lexical resources.</title>
<date>2007</date>
<booktitle>In Xiaofei Lu and Anna Feldman, editors, Computational Approaches to Figurative Language, Workshop at HLT/NAACL 2007,</booktitle>
<location>Rochester, NY.</location>
<contexts>
<context position="1758" citStr="Krishnakumaran and Zhu, 2007" startWordPosition="253" endWordPosition="257">hors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 1 Introduction Metaphor, the partial framing of one concept in terms of another, pervades human language and thought (Lakoff and Johnson, 1980; Lakoff, 1993). A variety of computational approaches to metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so automatically and effortlessly, we find it hard to ques</context>
<context position="5043" citStr="Krishnakumaran and Zhu (2007)" startWordPosition="741" endWordPosition="745">s to Metaphor Many computational approaches have been taken toward identifying metaphor in written text. MIDAS (Martin, 1990) attempts to detect when users of the Unix Consultant command line help system use metaphors, for example, “How do I enter Emacs?” is interpreted as “How do I invoke Emacs?” Another system, met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. More recently, Gedigian et al. (2006) used hand-annotated corpora to train an automatic metaphor classifier. Krishnakumaran and Zhu (2007) used violations of WordNetbased (Fellbaum, 1998) verb-noun expectations to identify the presence of a metaphor, e.g., “he is a brave lion,” would be considered metaphorical, because “he,” taken to mean a “person,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some degree to the literal meaning hypothesis (Reddy, 1969), which states that every sentence has a literal meaning, as derived from the meanings of its constituent words, while some also have a figurative meaning that goes beyond the meanings of the words themselves. In this view, a figurative interpr</context>
</contexts>
<marker>Krishnakumaran, Zhu, 2007</marker>
<rawString>Saisuresh Krishnakumaran and Xiaojin Zhu. 2007. Hunting elusive metaphors using lexical resources. In Xiaofei Lu and Anna Feldman, editors, Computational Approaches to Figurative Language, Workshop at HLT/NAACL 2007, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors We Live By.</title>
<date>1980</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL,</location>
<note>edition.</note>
<contexts>
<context position="1572" citStr="Lakoff and Johnson, 1980" startWordPosition="226" endWordPosition="229">to identify conceptual metaphors, and second, to compare SRL to the current use of typed dependency parsing in CMI. The results show that SRL can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 1 Introduction Metaphor, the partial framing of one concept in terms of another, pervades human language and thought (Lakoff and Johnson, 1980; Lakoff, 1993). A variety of computational approaches to metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a pow</context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors We Live By. University of Chicago Press, Chicago, IL, 2003 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Turner</author>
</authors>
<title>More Than Cool Reason: A Field Guide to Poetic Metaphor.</title>
<date>1989</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="2428" citStr="Lakoff and Turner, 1989" startWordPosition="364" endWordPosition="367"> obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so automatically and effortlessly, we find it hard to question [metaphors], if we can even notice them” (Lakoff and Turner, 1989, p. 65). Computational metaphor identification (CMI) (Baumer, 2009; Baumer et al., under review) addresses this difficulty by identifying potential conceptual metaphors in written text. Rather than attempting to discern whether individual phrases are metaphorical or literal, this technique instead identifies larger, overarching linguistic patterns. The goal of CMI is not to state definitively the metaphor present in a text, but rather to draw potential metaphors to readers’ attention, thereby encouraging both critical examination of current metaphors and creative generation of alternative met</context>
</contexts>
<marker>Lakoff, Turner, 1989</marker>
<rawString>George Lakoff and Mark Turner. 1989. More Than Cool Reason: A Field Guide to Poetic Metaphor. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
</authors>
<title>The contemporary theory of metaphor.</title>
<date>1993</date>
<pages>202--251</pages>
<editor>In A. Ortony, editor, Metaphor and thought, 2nd. ed.,</editor>
<publisher>Cambridge Univ Press,</publisher>
<location>New York.</location>
<contexts>
<context position="1587" citStr="Lakoff, 1993" startWordPosition="230" endWordPosition="231">aphors, and second, to compare SRL to the current use of typed dependency parsing in CMI. The results show that SRL can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 1 Introduction Metaphor, the partial framing of one concept in terms of another, pervades human language and thought (Lakoff and Johnson, 1980; Lakoff, 1993). A variety of computational approaches to metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component</context>
<context position="12739" citStr="Lakoff, 1993" startWordPosition="1989" endWordPosition="1990"> and each is automatically labeled based on the synsets it contains. This approach of using clustered hypernyms resonates with Lakoff’s argument that metaphorical mappings occur not at the level of situational specifics, but at the superordinate level. For example, in the metaphor LOVE IS A JOURNEY, the relationship is a vehicle. Although specific instantiations of the metaphor may frame that vehicle variously as a train (“off the track”), a car (“long, bumpy road”), or a plane (“just taking off”), “the categories mapped will tend to be at the superordinate level rather than the basic level” (Lakoff, 1993, p. 212). This method of counting each word observed as a partial observation of each of the synsets � S(c) = V 16 it might represent causes observations at the basic level to accumulate in the superordinate levels they collectively represent. This is not to say that hierarchical conceptual relations capture every possible metaphor, but rather that these are the relations on which we focus here. To identify metaphors, CMI looks for correspondences between conceptual clusters in the source and target corpora. For example, in the Military corpus, the cluster for war would frequently select to b</context>
<context position="14315" citStr="Lakoff (1993)" startWordPosition="2252" endWordPosition="2253">n a confidence score to indicate how likely the linguistic patterns are to evidence a conceptual metaphor. One of the strengths of CMI is that it works in the aggregate. While individual instances of phrases such as “fought during the election” and “defeated in the primary” may not at first glance appear metaphorical, it is the systematicity of these patterns that becomes compelling evidence for the existence of a metaphor. An important aspect of CMI is that it identifies only linguistic patterns potentially indicative of conceptual metaphors, not the metaphors themselves. As mentioned above, Lakoff (1993) emphasizes that metaphor is primarily a cognitive phenomenon, and that metaphorical language serves as evidence for the cognitive phenomenon. CMI leverages computational power to search through large bodies of text to identify patterns of potential interest, then presents those patterns to a human user along with the potential metaphors they might imply to foster critical thinking about metaphor. To reiterate, this places the job of finding patterns in the hands of the computer, and the job of interpreting those patterns in the hands of the human user. 4 CMI with Semantic Role Labeling The wo</context>
</contexts>
<marker>Lakoff, 1993</marker>
<rawString>George Lakoff. 1993. The contemporary theory of metaphor. In A. Ortony, editor, Metaphor and thought, 2nd. ed., pages 202–251. Cambridge Univ Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James H Martin</author>
</authors>
<title>A Computational Model of Metaphor Interpretation.</title>
<date>1990</date>
<publisher>Acad Press,</publisher>
<location>San Diego, CA.</location>
<contexts>
<context position="1692" citStr="Martin, 1990" startWordPosition="245" endWordPosition="246"> that SRL can be used to identify potential metaphors and that it overcomes some of the limitations of using typed dependencies, but also that SRL introduces its own set of complications. The paper concludes by suggesting future directions, both for evaluating the use of SRL in CMI, and for fostering critical and creative thinking about metaphors. 1 Introduction Metaphor, the partial framing of one concept in terms of another, pervades human language and thought (Lakoff and Johnson, 1980; Lakoff, 1993). A variety of computational approaches to metaphorical language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can </context>
<context position="4539" citStr="Martin, 1990" startWordPosition="665" endWordPosition="666">pares both the relational data and the metaphors identified with typed dependency parsing and semantic role labeling. The results show that semantic roles enabled effective identification of potential metaphors. However, neither typed dependencies nor semantic roles were necessarily superior. Rather, each provides certain advantages, both in terms of identifying potential metaphors, and in terms of promoting critical thinking and creativity. 2 Related Work 2.1 Computational Approaches to Metaphor Many computational approaches have been taken toward identifying metaphor in written text. MIDAS (Martin, 1990) attempts to detect when users of the Unix Consultant command line help system use metaphors, for example, “How do I enter Emacs?” is interpreted as “How do I invoke Emacs?” Another system, met* (Fass, 1991), is designed to distinguish both metaphor and metonymy from literal text, providing special techniques for processing these instances of figurative language. More recently, Gedigian et al. (2006) used hand-annotated corpora to train an automatic metaphor classifier. Krishnakumaran and Zhu (2007) used violations of WordNetbased (Fellbaum, 1998) verb-noun expectations to identify the presenc</context>
</contexts>
<marker>Martin, 1990</marker>
<rawString>James H. Martin. 1990. A Computational Model of Metaphor Interpretation. Acad Press, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zachary J Mason</author>
</authors>
<title>CorMet: a computational, corpus-based conventional metaphor extraction system.</title>
<date>2004</date>
<journal>Comp Ling,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="6299" citStr="Mason, 2004" startWordPosition="942" endWordPosition="944">interpretation has been formed and found inconsistent, nonsensical, or otherwise faulty. However, experimental evidence has made this account suspect (Gibbs, 1984; Gentner et al., 2001). Even distinguishing whether a given expression is literal or figurative can be difficult at best. For example, “the rock is becoming brittle with age” (Reddy, 1969, p. 242), has “a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus” (Fass, 1991, p. 54). One previous metaphor system avoids making such literal/metaphorical distinctions. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. CorMet calculates selectional preferences and associations (Resnik, 1993) for each corpus’s characteristic verbs, i.e., those verbs at least twice as frequent in the corpus as in general English. Based on these selectional associations, CorMet clusters the nouns for which the characteristic verbs select. To identify metaphors, mappings are sought from clusters in the source corpus to clusters in the target corpus, based on the degree to which the same verbs select f</context>
</contexts>
<marker>Mason, 2004</marker>
<rawString>Zachary J. Mason. 2004. CorMet: a computational, corpus-based conventional metaphor extraction system. Comp Ling, 30(1):23–44, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ortony</author>
</authors>
<title>Some psycholinguistic aspects of metaphor.</title>
<date>1980</date>
<booktitle>Cog and Fig Lang,</booktitle>
<pages>69--83</pages>
<editor>In R.P. Honeck and H.R. Robert, editors,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="27887" citStr="Ortony, 1980" startWordPosition="4290" endWordPosition="4291">without ceremony” 20 means that less data are available to the selection preference calculation, in that there are fewer observations for each relation. On the other hand, SRL is a much finer-grained classification than typed dependencies. The implementation used here included 111 grammatical relations, whereas RelEx labeled 1446 distinct roles. Thus, overall, RelEx may be providing fewer observations for each relation, but those relations may have more semantic import. For fostering critical thinking and creativity, a key concern is making identified metaphors readily comprehensible. Ortony (Ortony, 1980) and others have suggested that selectional restriction violations are an important component of metaphor comprehension. Therefore, tools that employ CMI often present parallel source and target fragments side-by-side to make clear the selectional restriction violation, e.g., metaViz, a system for presenting computationally identified metaphors in political blogs (Baumer et al., 2010). One might assume that typed dependencies are more readily comprehensible, since they are expressed as relatively simple grammatical relations. However, when presenting example fragments to users, there is no nee</context>
</contexts>
<marker>Ortony, 1980</marker>
<rawString>Andrew Ortony. 1980. Some psycholinguistic aspects of metaphor. In R.P. Honeck and H.R. Robert, editors, Cog and Fig Lang, pages 69–83. Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wendy Oxman-Michelli</author>
</authors>
<title>Critical thinking as creativity.</title>
<date>1991</date>
<tech>Technical Report SO 023 597,</tech>
<institution>Montclair State, Institute for Critical Thinking,</institution>
<location>Montclair, NJ.</location>
<contexts>
<context position="2263" citStr="Oxman-Michelli, 1991" startWordPosition="339" endWordPosition="340"> language have been developed, e.g., (Martin, 1990; Fass, 1991; Gedigian et al., 2006; Krishnakumaran and Zhu, 2007). However, most such methods see metaphor as an obstacle to be overcome in the task of discerning the actual, literal meaning of a phrase or sentence. In contrast, the work presented here approaches conceptual metaphor not as an obstacle but as a resource. Metaphor is an integral part in human understanding of myriad abstract or complex concepts (Lakoff and Johnson, 1980), and metaphorical thinking can be a powerful component in critical and creative thinking, cf. (Gordon, 1974; Oxman-Michelli, 1991). However, “because they can be used so automatically and effortlessly, we find it hard to question [metaphors], if we can even notice them” (Lakoff and Turner, 1989, p. 65). Computational metaphor identification (CMI) (Baumer, 2009; Baumer et al., under review) addresses this difficulty by identifying potential conceptual metaphors in written text. Rather than attempting to discern whether individual phrases are metaphorical or literal, this technique instead identifies larger, overarching linguistic patterns. The goal of CMI is not to state definitively the metaphor present in a text, but ra</context>
</contexts>
<marker>Oxman-Michelli, 1991</marker>
<rawString>Wendy Oxman-Michelli. 1991. Critical thinking as creativity. Technical Report SO 023 597, Montclair State, Institute for Critical Thinking, Montclair, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Paul Kingsbury</author>
<author>Daniel Gildea</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Comp Ling,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="7779" citStr="Palmer et al., 2005" startWordPosition="1182" endWordPosition="1185">ased on the selectional associations of verbs such as “pour,” “flow,” “freeze,” and “evaporate.” The CMI system described in this paper is informed largely by CorMet (Mason, 2004). 2.2 Semantic Role Labeling While interpretations vary somewhat, semantic role labeling (SRL) generally aims to represent something about the meaning of a phrase at a deeper level than surface syntactic structure. One of the most common approaches to performing SRL automatically is to use a statistical classifier trained on labeled corpora (Gildea and Jurafsky, 2002), with FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) being the primary sources. An important result of the Gildea and Jurafsky work was identifying the significant utility of using presegmented constituents as input to their labeler, and accordingly most SRL systems perform a syntactic analysis as an initial step. The principal alternative to using a statistical classifier is to use a rule-based labeler for operating on 1SMALL CAPS are metaphors, italics are concepts, CAPS are domains, and “quotes” are example phrases. 15 the syntactic parse tree. For example, Shi and Mihalcea (2004) extract explicit SRL rules by analyzing FrameNet cases. Anoth</context>
</contexts>
<marker>Palmer, Kingsbury, Gildea, 2005</marker>
<rawString>Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The Proposition Bank: An annotated corpus of semantic roles. Comp Ling, 31(1):71–106, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Reddy</author>
</authors>
<title>A semantic approach to metaphor. In Chicago Linguistic Society Collected Papers,</title>
<date>1969</date>
<pages>240--251</pages>
<publisher>Chicago Univ Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5399" citStr="Reddy, 1969" startWordPosition="801" endWordPosition="802">r and metonymy from literal text, providing special techniques for processing these instances of figurative language. More recently, Gedigian et al. (2006) used hand-annotated corpora to train an automatic metaphor classifier. Krishnakumaran and Zhu (2007) used violations of WordNetbased (Fellbaum, 1998) verb-noun expectations to identify the presence of a metaphor, e.g., “he is a brave lion,” would be considered metaphorical, because “he,” taken to mean a “person,” which is not a WordNet hyponym of “lion.” These and similar approaches ascribe to some degree to the literal meaning hypothesis (Reddy, 1969), which states that every sentence has a literal meaning, as derived from the meanings of its constituent words, while some also have a figurative meaning that goes beyond the meanings of the words themselves. In this view, a figurative interpretation is only sought only after a literal interpretation has been formed and found inconsistent, nonsensical, or otherwise faulty. However, experimental evidence has made this account suspect (Gibbs, 1984; Gentner et al., 2001). Even distinguishing whether a given expression is literal or figurative can be difficult at best. For example, “the rock is b</context>
</contexts>
<marker>Reddy, 1969</marker>
<rawString>Michael J. Reddy. 1969. A semantic approach to metaphor. In Chicago Linguistic Society Collected Papers, pages 240–251. Chicago Univ Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<institution>Dissertation, University of Pennsylvania, Department of Computer and Information Science.</institution>
<contexts>
<context position="3120" citStr="Resnik, 1993" startWordPosition="463" endWordPosition="464">al., under review) addresses this difficulty by identifying potential conceptual metaphors in written text. Rather than attempting to discern whether individual phrases are metaphorical or literal, this technique instead identifies larger, overarching linguistic patterns. The goal of CMI is not to state definitively the metaphor present in a text, but rather to draw potential metaphors to readers’ attention, thereby encouraging both critical examination of current metaphors and creative generation of alternative metaphors. CMI identifies potential metaphors by mapping selectional preferences (Resnik, 1993) from a source corpus to a target corpus. Previous work on CMI utilized typed dependency parses (de Marneffe et al., 2006) to calculate these selectional preferences. This paper explores the use of semantic role labeling (SRL) (Gildea and Jurafsky, 2002; Johansson and Nugues, 2008) to calculate selectional preferences. Typed dependencies focus on syntactic structure and grammatical relations, while semantic roles emphasize conceptual and semantic structure, so SRL may 14 Proceedings of the NAACL HLT 2010 Second Workshop on Computational Approaches to Linguistic Creativity, pages 14–22, Los Ang</context>
<context position="6502" citStr="Resnik, 1993" startWordPosition="970" endWordPosition="971">hing whether a given expression is literal or figurative can be difficult at best. For example, “the rock is becoming brittle with age” (Reddy, 1969, p. 242), has “a literal interpretation when uttered about a stone and a metaphorical one when said about a decrepit professor emeritus” (Fass, 1991, p. 54). One previous metaphor system avoids making such literal/metaphorical distinctions. CorMet (Mason, 2004) is designed to extract known conventional metaphors from domain-specific textual corpora, which are derived from Google queries. CorMet calculates selectional preferences and associations (Resnik, 1993) for each corpus’s characteristic verbs, i.e., those verbs at least twice as frequent in the corpus as in general English. Based on these selectional associations, CorMet clusters the nouns for which the characteristic verbs select. To identify metaphors, mappings are sought from clusters in the source corpus to clusters in the target corpus, based on the degree to which the same verbs select for members of both clusters. For example, CorMet was used to extract the metaphor MONEY IS A LIQUID1 by mapping from a cluster for the concept liquid in a corpus for the domain LABORATORY to a cluster fo</context>
<context position="10347" citStr="Resnik, 1993" startWordPosition="1601" endWordPosition="1602">al blogs, described in more detail in the methods section below. Source corpora are composed of Wikipedia articles, as they provide a readily available, categorically organized, large source of content on a wide variety of topics. A source corpus for a given domain consists of all the Wikipedia articles in the category for that domain, as well as all articles in its subcategories. All documents in the source and target corpora are parsed to extract sentence structure and typed dependencies (Klein and Manning, 2003; de Marneffe et al., 2006). The crux of CMI is selectional preference learning (Resnik, 1993), which quantifies the tendency of particular words to appear with certain other classes of words in specific grammatical relationships. For example, words for the concept of food are often the direct object of the verb “eat.” Using the parsed documents, CMI calculates selectional preferences of the characteristic nouns in a corpus, where characteristic means that the noun is highly frequent in the corpus relative to its frequency in general English, as derived from (Kilgarriff, 1996). Selectional preference is quantified as the relative entropy of the posterior distribution conditioned on a s</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Dissertation, University of Pennsylvania, Department of Computer and Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Shi</author>
<author>Rada Mihalcea</author>
</authors>
<title>Open text semantic parsing using FrameNet and WordNet. In Demonstration Papers at HLT-NAACL</title>
<date>2004</date>
<pages>pages</pages>
<institution>Boston. Assoc for Computational Linguistics.</institution>
<contexts>
<context position="8317" citStr="Shi and Mihalcea (2004)" startWordPosition="1267" endWordPosition="1271">urafsky, 2002), with FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005) being the primary sources. An important result of the Gildea and Jurafsky work was identifying the significant utility of using presegmented constituents as input to their labeler, and accordingly most SRL systems perform a syntactic analysis as an initial step. The principal alternative to using a statistical classifier is to use a rule-based labeler for operating on 1SMALL CAPS are metaphors, italics are concepts, CAPS are domains, and “quotes” are example phrases. 15 the syntactic parse tree. For example, Shi and Mihalcea (2004) extract explicit SRL rules by analyzing FrameNet cases. Another system, RelEx (Fundel et al., 2006) also uses rules and is structured like the implementation used here (see below for details), but despite having the same name, is a different system. Statistical and rule-based methods may also be used within the same system, such as in LTH (Johansson and Nugues, 2008). One reason for preferring a rule-based SRL system is that rule-based approaches may be less susceptible to the loss of accuracy that statistically trained classifiers suffer when applied to domains that are different than the co</context>
</contexts>
<marker>Shi, Mihalcea, 2004</marker>
<rawString>Lei Shi and Rada Mihalcea. 2004. Open text semantic parsing using FrameNet and WordNet. In Demonstration Papers at HLT-NAACL 2004, pages 19–22, Boston. Assoc for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a Link Grammar. In</title>
<date>1993</date>
<booktitle>Proc Third International Workshop on Parsing Technologies,</booktitle>
<pages>277--292</pages>
<contexts>
<context position="16977" citStr="Sleator and Temperley, 1993" startWordPosition="2671" endWordPosition="2675">006) with the RelEx SRL system (http://opencog.org/wiki/RelEx). RelEx performs a full syntactic parse, then applies a set of syntactic pattern rules to annotate the parse tree with role labels based (not exactly or completely) on FrameNet. This implementation uses a rule-based labeler because CMI hinges on differences in selectional preferences in corpora from different domains, and statistically trained classifiers are biased by the distributions of the corpora on which they are trained. For syntactic parsing, RelEx uses the Link Grammar Parser (LGP) which is based on the Link Grammar model (Sleator and Temperley, 1993). LGP produces output very similar to typed dependencies. The version of RelEx we use integrates the Another NearlyNew Information Extraction (ANNIE) system (http://gate.ac.uk/sale/tao/splitch6.html#chap:annie) to tag named entities. Sentences are split using the OpenNLP sentence splitter (http://opennlp.sourceforge.net/). Because CMI’s corpora are acquired from public 17 Blogs Religion (Wikipedia) Docs 546 (604) 3289 (3294) Sents 5732 (6708) 128,543 (145,193) Words 148,619 3,300,455 Table 1: Sizes of the target and source corpora; parentheses show totals including documents without valid sent</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Daniel Sleator and Davy Temperley. 1993. Parsing English with a Link Grammar. In Proc Third International Workshop on Parsing Technologies, pages 277– 292.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>