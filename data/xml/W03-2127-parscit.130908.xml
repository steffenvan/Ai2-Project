<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000109">
<title confidence="0.997809">
Annotating emotion in dialogue
</title>
<author confidence="0.997666">
Richard Craggs
</author>
<affiliation confidence="0.993065">
Department of Computer Science
Manchester University
</affiliation>
<email confidence="0.992352">
craggs@cs.man.ac.uk
</email>
<author confidence="0.993741">
Mary McGee Wood
</author>
<affiliation confidence="0.993016">
Department of Computer Science
Manchester University
</affiliation>
<email confidence="0.997578">
mary@cs.man.ac.uk
</email>
<sectionHeader confidence="0.995618" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997386">
Communication behaviour is affected by
emotion. Here we discuss how dialogue is
affected by participants’ emotion and how
expressions of emotion are manifested in
its content.
</bodyText>
<sectionHeader confidence="0.788523" genericHeader="keywords">
Keywords: Dialogue, Emotions, Annotation
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938238095238">
Dialogue annotation is a fundamental stage of much
of the research conducted on both Human-Human
and Human-Machine dialogue.
We are fortunate to have access to a valuable
corpus of 37 dialogues between nurses and pa-
tients, each comprising 200-1200 utterances (Wood,
2001). These consultations contain genuine emo-
tional speech and form the ideal basis for studies of
realistic conversational dialogue.
The emotional state of participants affects the
way in which the dialogue is conducted. I propose
that annotating emotion in dialogue alongside cur-
rently annotated phenomena will reveal interesting
and useful correlations that will improve our under-
standing of dialogue and benefit natural language
applications. The overall aim of this research is
to develop a scheme for annotating expressions of
emotion, to create an annotated corpus of dialogue
containing emotion and to study the effects that a
participant’s emotional state has on their commu-
nicative behaviour.
</bodyText>
<sectionHeader confidence="0.779685" genericHeader="method">
2 Effects of emotion in dialogue
</sectionHeader>
<bodyText confidence="0.999873066666667">
This research is motivated by observations made on
the consultation dialogues described above. These
are naturally occurring conversational dialogues
conducted under unusual circumstances, in which
the consultant’s goal is to elicit concerns from
the patient. They therefore contain an unusually
high level of emotional speech. When read with
a dialogue analyst’s eye it is apparent that certain
phenomena, interesting to the dialogue analysis
community, are affected by the changing level of
emotion. For example, grounding behaviour is more
protracted when a participant is discussing a subject
about which they feel emotional. This is manifested
in an increase in the number of clarification requests
and repetitions. E.g. -
</bodyText>
<figure confidence="0.824108333333333">
N. How do you feel when you look at your
scar?
P. Erm, it doesn’t bother me that much
N Okay
P. But I still, When I’m washing and ev-
erything I still get a funny feeling
N. You get a funny feeling, in which way?
P. It just feels strange, hollow,
N. Physically?
P. Physically yes,yes
N. Yes
P. It feels really weird
</figure>
<bodyText confidence="0.995841157894737">
Turn taking behaviour changes under these cir-
cumstances too. An emotional speaker will hold the
floor for an increased length of time when discussing
a topic about which they feel, for example, anxiety
or joy.
Although these are casual observations of a small
amount of dialogue, other studies have benefited
from investigation into a speaker’s behaviour when
emotional. For the Verbmobil project (Bub and
Schwinn, 1996) it was recognised that anger in
speakers changed the way in which they commu-
nicated (Fischer, 1999). Also applications such as
automated call centres would benefit from recogni-
tion of human emotion so that humans could inter-
vene when a customer becomes angry and frustrated
(Petrushin, 1999). However, these insights are lim-
ited to the vocal expression of the speaker. An anno-
tated corpus of emotional dialogue would allow us
to study all aspects of a speaker’s behaviour.
</bodyText>
<sectionHeader confidence="0.848497" genericHeader="method">
3 Annotating emotion in dialogue
</sectionHeader>
<bodyText confidence="0.999961045454546">
I envisage that a scheme to annotate dialogue would
constitute one or more layers augmenting an exist-
ing annotation scheme. There are plenty of other
schemes developed for previous dialogue research,
many of which are designed to investigate a particu-
lar phenomenon of communicative behaviour.
In this section we will look at some existing an-
notation schemes. We shall investigate if any of the
layers may accommodate emotion and which may
present interesting correlations with emotional tag-
ging.
Of course when looking for possible indicators of
emotional speech it is important to remember that
people exhibit different behaviours from each other
when they speak. For example some people are
more expressive than others and so a large number of
expletives from one person may be natural, and not
indicative of their emotional state. Prosodic studies
of emotion also suffer from this complexity and it
would be interesting to see if language use, and dia-
logue behaviour are more robust indicators of emo-
tion than prosody.
</bodyText>
<subsectionHeader confidence="0.998064">
3.1 Task and conversational dialogue
</subsectionHeader>
<bodyText confidence="0.999728904761905">
Most dialogue research concentrates either on task
based dialogue, where the participants converse in
order to achieve some set goal (e.g. Maptask (An-
derson et al., 1991) Coconut (Di Eugenio et al.,
1998)), or on conversational dialogue, which is of-
ten less structured and contains a richer use of lan-
guage (e.g. DAMSL (Core and Allen, 1997) and
Chat (MacWhinney, 1998)). It seems likely that we
would see more expressions of emotion in conver-
sational dialogue where people are discussing topics
of personal interest rather than the more mechanical
process of achieving a goal through communication.
These differences are reflected in the types of
phenomena that the schemes are designed to iden-
tify. Task based research may be more interested
in the structure of the dialogue and the way that it
represents the division of the task into sub-goals.
Schemes to annotate conversational dialogue are
more likely to require a greater breadth of dialogue
acts to describe the wider range of illocutionary acts
that may be performed in this type of speech.
</bodyText>
<subsectionHeader confidence="0.999815">
3.2 Current dialogue annotation schemes
</subsectionHeader>
<bodyText confidence="0.999631">
In order to learn how current annotation schemes
accommodate emotion, we aligned the layers in a
number of schemes. (Core and Allen, 1997; Di Eu-
genio et al., 1998; Traum, 1996; Walker et al., 1996;
MacWhinney, 1996; Jekat et al., 1995; Anderson et
al., 1991; Condon and Cech, 1996; van Vark et al.,
1996; Walker and Passonneau, 2001). Layers from
different schemes are grouped according to the sim-
ilar phenomena that they label. Table 1 shows this
alignment.
In this section we will look at these layers and
discuss how they may relate to annotating emotion
in dialogue.
Information level When analysing task dialogue,
we may be interested in knowing whether an utter-
ance pertains to the management of the communi-
cations channel, advancement of a task, discussing
of a task etc. In the previous section we suggested
that we are more likely to find emotional speech in
conversational rather than task dialogue because the
latter is more of a mechanical process than conver-
sation. Perhaps we may consider this layer as an ex-
tension of that distinction where sub-dialogues are
labelled according to how closely related to the task
they are.
This may reveal a correlation where the more re-
lated to the task a sub-dialogue is, the less emotional
speech becomes. There is evidence in our corpus
that when one participant is attempting to achieve
a goal, often the elicitation of information, then the
participants’ behaviour becomes more business like
and the language becomes more formal and less
expressive –
</bodyText>
<figure confidence="0.649109444444445">
N. Was that Dr Smith who you saw there?
P. Yes but it wasn’t Dr Smith it was an-
other doctor.
N. Right.
P. But I was under Dr Smith.
N. Right. So when did you actually have
those radiotherapy treatments?
P. I had the radiotherapy October 13th.
N. Right, thank you.
</figure>
<subsectionHeader confidence="0.633529">
Communications status
</subsectionHeader>
<bodyText confidence="0.9997836">
Communications status indicates whether an ut-
terance was successfully completed. It is used to
tag utterances that are abandoned or unintelligible
rather than whether the intention of a speech act was
achieved.
Although failure to perform a successful utter-
ance may be partly due to the emotional state of
the speaker, annotating such utterances for their
emotional content may be difficult, especially from
the textual content alone. This and the multiplicity
of reasons for unsuccessful communication means
that using communications status as an indication
of emotion in the speaker will produce unreliable
results.
In Human-Machine dialogue failure on behalf of
the machine to communicate can lead to frustration
and anger in the user. In these cases communication
status may signal behaviour that can result in
emotion in the listener which is also applicable to
Human-Human dialogue.
</bodyText>
<subsectionHeader confidence="0.741284">
Speech acts
</subsectionHeader>
<bodyText confidence="0.98682125">
All of the schemes that we examined annotated
the utterances for their illocutionary force. Since
this is the layer that contains most information re-
garding the semantic content of an utterance, this is
likely to be where we shall find the most interesting
correlations. We have already seen that high levels
of emotion in dialogue alters the frequencies of
dialogue acts compared with the more impassive
conversations conducted in the Switchboard corpus
(Wood and Craggs, 2002).
Forward communicative functions describe utter-
ances that intend to evoke some response from the
listener (such as believing a statement or answering
a question), perform an action (such as committing
to something) or similar dialogue advancing func-
tions. These types of utterance are likely to be mo-
tivated by some intention or belief on behalf of the
speaker, providing clues as to their cognitive state.
Forward communicative functions can play an
important role in eliciting emotional responses from
the listener. Open ended questions are more likely to
produce an emotional response than a yes/no ques-
tion (Maguire et al., 1996b). This is partly because
open questions hand the initiative to the listener al-
lowing them to express themselves. The relation-
ship between questions, initiative and emotion is dis-
cussed further in (Wood and Craggs, 2003).
The following extract from our corpus show how
an open question elicits an emotional response from
the listener.
N. How were you coping with that yourself?
P. Oh mentally I’ve never been down men-
tally
Backward communicative functions are used to la-
bel utterances that respond to something that has
been said to them. Some responses are required
by the previous utterance, for example an answer
following a question. In these cases the utterance
wasn’t motivated by a desire on behalf of the speaker
but rather an obligation to adhere to the rules of
engagement for communication. This of course
doesn’t mean that a response can not be emotional.
When faced with a proposal, question or offer the
listener is free to react as they wish and this includes
emotional responses.
Here a backward communicative act responds to
appreciation with sympathy.
N. Good okay. Well thank you as I say for
filling me in and...
P. poor girl you’ve got to listen to all
that
However, from observations of our emotional
dialogues it appears that short Question-Answer,
Offer-Acceptance exchanges tend to be formal.
Emotion tends to build though a sub-dialogue on a
topic that speakers find funny, feel anxious about etc.
Dialogue grammars are used to exploit the ex-
pected sequences of speech acts. These can be used
in dialogue act classification to predict the next act
in a series of utterances (Stolcke et al., 2000). It may
be possible that a complementary approach may be
used to automatically identify emotional utterances.
One way would be to develop grammars based
on patterns discovered in emotional sections of
dialogue where a particular sequence of acts may
indicate the proceedings have become emotional.
Another may be to apply established grammars to
dialogue so that deviations from the grammar may
highlight interesting or emotional passages.
Topic
Several annotation schemes contain a layer that
labels the topic discussed in an utterance. This is
usually in task domains where there is a finite num-
ber of subjects that will be discussed. For exam-
ple in the Alparon scheme for transport dialogues
(van Vark et al., 1996), the topic layer (called ’cod-
ing of information’), labels utterances according to
whether they relate to topics such as timetable, price,
time and locations.
For our corpus of cancer consultations it is ap-
parent that certain topics are more likely to invoke
emotion in people. However topic annotation is only
usually performed in the restricted domains of task
dialogues, where the range of topics that may be dis-
cussed is limited. However it is in these types of
dialogue that we expect the levels of emotion to be
low, and topics are chosen because of their necessity
for the task. Because of this we may not get to see
the correlation between topic and emotion that we
expect.
Topics may play a further role in identification
of emotion in dialogue since in our corpus, patients
tend to remain on the same topic for longer when
they emotional about it. Length of a topic, or return-
ing to a previously discussed topic are indications
of emotion.
</bodyText>
<subsectionHeader confidence="0.444007">
Phases
</subsectionHeader>
<bodyText confidence="0.999946666666667">
Some schemes distinguish between dialogue
phases such as opening, negotiation and query.
Emotion in dialogue also goes through phases and
it is possible that there are boundaries between the
phases of emotion that correspond to those tagged
using the phase layer.
An interesting area of research would be to iden-
tify how boundaries between the phases of different
levels and types of emotion are manifested in the
use of language. For instance psycho-oncology
research states that open ended questions are more
likely to elicit emotional responses than yes-no
questions (Maguire et al., 1996a). This may cause a
correlation between forward-looking functions and
the onset of phases.
</bodyText>
<subsectionHeader confidence="0.565082">
Surface form
</subsectionHeader>
<bodyText confidence="0.992597333333333">
Surface form tagging is used in David Traum’s
adaptation of the TRAINS annotation scheme
(Traum, 1996) and the Coconut scheme to tag utter-
ances for certain special features such as cue words
or negation.
It has been shown that certain syntactic features
of an utterance may be indicators of emotion. For
example in German use of modal particles such
as ‘eben’ and ‘denn’ colour the utterance with a
particular emotional attitude.
Although the surface form of utterances is depen-
dent on the style of the speaker, it does sometimes
contain indications of emotion.
P. Oh no, no no no no, I’m not in any dis-
comfort
</bodyText>
<sectionHeader confidence="0.516896" genericHeader="method">
Relatedness and Information relations
</sectionHeader>
<bodyText confidence="0.987806947368421">
The relatedness layer is used to show how utter-
ances relate to one another, usually by tagging an ut-
terance with the distance to the antecedent to which
it refers.
Information relation describes the relationship be-
tween utterances, for instance that one utterance
presents information in support of its antecedent.
These layers are more concerned with the struc-
ture of the dialogue than the semantic content and
are therefore less likely to correlate well with emo-
tional tags.
It would be interesting to see if the emotional
level of the dialogue or its participants has an affect
on the dialogue’s structure. In our corpus it appears
that discussion of emotional topics is often more
protracted, with speakers answering questions with
successive statements, each adding more detail to
their answer. This type of behaviour may show up
in the relatedness and information relations layers.
Grounding
Grounding describes the process by which com-
mon ground between the participants is established.
As with relatedness and information relations,
emotion in the dialogue may be manifested in this
layer by protracted grounding behaviour as people
reiterate points about which they feel emotional. In
our highly emotional corpus this resulted in four
times as many summaries and five times as many
repetitions than in the Switchboard corpus.
Besides the layers listed here there are other lay-
ers included in schemes that do not fit into any
of these categories. For instance Verbmobil (Jekat
et al., 1995) includes a layer for annotating the
propositional content of an utterance, and content
relevance in the Penn multi-party coding scheme
(Walker et al., 1996). Investigation on dialogue an-
notated for emotion will show whether there are any
interesting correlations with these layers.
</bodyText>
<sectionHeader confidence="0.978295" genericHeader="method">
4 Emotional speech corpora
</sectionHeader>
<bodyText confidence="0.999986105263158">
One of the difficulties in analysing emotion in com-
munication is in obtaining the material to study. For
studies into task dialogues, researchers can simply
record speakers performing the tasks. However cap-
turing conversational dialogue in general and espe-
cially emotional dialogue is a much more difficult
task.
Studies into emotional speech based on acoustic
features use three approaches to attain their data.
Ideally it is preferable to use genuine speech taken
without the speaker’s knowledge since you can be
confident that the resulting data will faithfully repre-
sent human behaviour. An example of research us-
ing this type of data is (Scherer and Ceschi, 2000).
This approach isn’t commonly adopted, partly be-
cause of the ethical issues concerned with recording
people without their consent and also because of the
difficulty in controlling variables such as recording
quality or establishing age, sex, etc. of the speaker.
For dialogue studies this would also be the desired
type of data. If we are interested communicative be-
haviour such as turn-taking and language use rather
than the acoustic features of the speech then we need
not be so concerned with the acoustic quality. If it
were possible to obtain recordings of police inter-
views, legal trails or calls to emergency services then
these would provide suitable material to study. Our
corpus of oncology consultations is a good example
of this type of dialogue.
A more common type of data used in speech stud-
ies is that of acted emotions. Actors deliver lines
expressed with different emotions (e.g. (Dellaert et
al., 1996)). The quality of this data is reliant on
the accuracy with which the emotion is acted. This
is suitable for establishing the prosodic features as-
sociated with various emotions but not for dialogue
studies. It would be much more difficult to recreate
the communicative behaviour of an emotional per-
son through acting than to simply sound emotional.
Finally, induced emotion, where participants are
provoked into an emotional state so that their speech
can be recorded (e.g. (Huber et al., 2000)) . This
provides natural emotion within a laboratory setting.
It is conceivable that this process could be adapted to
obtain induced emotional dialogue. One participant
may try to conduct a conversation during which the
other may behave emotionally. However it is likely
that the data derived from this would be unlike real
conversations.
It is apparent that when studying emotion in dia-
logue it would be desirable to obtain genuine con-
versations that contained some degree of emotion.
Attempting to induce emotion is likely to cause the
communicative behaviour to become unnatural. The
preferable option would be to use natural conversa-
tion in unusually emotional circumstances such as
those described above.
</bodyText>
<sectionHeader confidence="0.824691" genericHeader="method">
5 Toward an emotion annotation scheme
</sectionHeader>
<bodyText confidence="0.9986962">
In developing an annotation scheme our first step
will be to decide on the facets of emotion which we
would like to identify. Emotion is a very vague word
and so it is important that we polarise it into clear
and understandable aspects of human cognition. In
</bodyText>
<table confidence="0.9988586">
Layer DAMSL Coconut Traum Penn Maptask
Info level Info Level Info Level Info Type
Comm status Comms Status
Topic Topic Topic
Speech act Dialogue acts Comm function Illocutionary func Speech acts Moves
Info relations Info relations Info relations Argumentation
Relatedness Antecedent Link Relatedness Initiative
Grounding Grounding Info status
Surface form Surface features Surface form
Phases
Layer Verbmobil Chat Condon &amp; Cech Alparon Date
Info level Interchange type Metalanguage Domain
Comm status
Topic Info coding Subtasks
Speech act Dialogue acts Illocutionary force Move function Moves Speech acts
Info relations
Relatedness
Grounding
Surface form
Phases Phases Phases
</table>
<tableCaption confidence="0.999926">
Table 1: Annotation schemes and their layers
</tableCaption>
<bodyText confidence="0.999794473684211">
order for the annotation to be useful these aspects
must have some influence on their communicative
behaviour. They must also be identifiable from the
language of the dialogue. This will mean that the
scheme may consist of several layers each describ-
ing a different aspect of human emotion.
One of the differences between these types of lay-
ers and those current schemes is that rather than dis-
crete categories such as those used to label speech
acts we can observe varying levels of emotion. A
precedent for this type of annotation exists in the
labelling of expressions of concern in the oncol-
ogy consultation coding scheme of Psychological
Medicine Group at Manchester University (Heaven
and Green, 2001). where these cues are rated 0–3.
If this approach was adopted then we would have to
decide on the number of levels to chose from based
on a trade-off between ease of performing annota-
tion with getting a fine enough distinction between
different levels.
This would allow us to draw conclusions about
communicative behaviour under different levels of
emotion (e.g. “The length of utterances becomes
longer under increasing levels of anxiety”) and cor-
relations with other layers (e.g. “People ask open
questions when relaxed but closed questions when
agitated”). It would also allow us to plot the quanti-
tative level of emotions throughout the dialogue, in-
vestigate the way in which this changes and identify
the language phenomena that signal these changes.
If only for pragmatic reasons, it would be wise to
choose utterances as the basic unit for annotation.
By utterances here we refer to the common under-
standing described as ’a sequence of communicative
behaviour bounded by lack of activity’ (Allwood,
1996). This would not only allow us to apply other
schemes to emotionally annotated dialogue, but also
to use tools that have been developed to work on ut-
terances. It would therefore be necessary to chose
dimensions of emotion that can be applied to utter-
ances.
There is an interesting question of whether emo-
tion is a property of the participants or the dialogue.
Obviously two or more people participating in a di-
alogue will react differently to the proceedings and
will therefore exhibit different emotions. However
it is apparent from our corpus that the dialogue it-
self has its own levels of emotion. For instance, the
conversation may go through a phase of solemnity
during which the participants may exchange a joke.
The mood of the dialogue outlives this perturbation
and remains serious. It would appear that it may be
useful to track the emotional state of the dialogue
as well as the speakers since one will clearly have
an effect on the other. Quantitative annotation and
analysis of the flow of these levels would therefore
be useful here too.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="discussions">
6 Future work
</sectionHeader>
<bodyText confidence="0.999979366666667">
Our next step will be to design an annotation scheme
based on the observations and principles stated
throughout this paper. We could then start annotat-
ing our corpus for the emotional dimensions that we
had chosen.
In order to assess the correlations that we pro-
posed might exist in section 3, we would have to
annotate these dialogues with the layers of other
schemes. Since none of schemes contain all of the
layers, we would have to combine individual layers
based on our beliefs about which could be most use-
ful and the ease with which we would annotate the
dialogue. It would make sense to select layers from
schemes which have comprehensive coding manu-
als, which have been shown to be reliable and which
would be accommodated by annotation tools.
Before any claims about the effects of emotion in
dialogue can be made, the reliability of the scheme
must be established. Once this has been achieved
than analysis of the results can begin.
An annotated corpus would present us with the
opportunity to investigate correlations and attempt
to identify the effects the various types of emotion
on the behaviour of the participants. It is likely that
along with the possible effects that we have prof-
fered in this paper there will be other interesting pat-
terns that become apparent from the results of our
annotation. This will improve our understanding of
behaviour in dialogue and benefit dialogue applica-
tions.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999235542168674">
J Allwood. 1996. On dialogue cohesion. In Papers from
Thirteenth Scandinavian Conference ofLinguistics.
A Anderson, M Bader, E Bard, Boyle E, G Doherty,
S Garrod, S Isard, J Kowtko, J McAllister, J Miller,
C Sotillo, H Thompson, and R Weinert. 1991. The
HCRC Map Task corpus. Language and Speech,
34:351–66.
T Bub and J Schwinn. 1996. VERBMOBIL: The evo-
lution of a complex large speech-to-speech translation
system. In Proc. ICSLP ’96, volume 4, pages 2371–
2374, Philadelphia, PA.
S Condon and C Cech, 1996. Manual for Coding
Decision-Making Interactions.
Mark G Core and James F Allen. 1997. Coding dialogs
with the damsl annatation scheme. In AAAI Fall Sym-
posium on Communicative Action in Humans and Ma-
chines.
F. Dellaert, T. Polzin, and A. Waibel. 1996. Recognizing
emotions in speech. In Proc. ICSLP ’96, volume 3,
pages 1970–1973, Philadelphia, PA.
B Di Eugenio, P.W Jordan, and L Pylkkanen, 1998. The
COCONUT project: Dialogue Annotation Manual.
ISP Technical Report 98-1.
K Fischer. 1999. Annotating emotional langauge data.
Technical report, Verbmobil - report 236.
C Heaven and C Green, 2001. Medical Interview Aural
Rating Scale, CRC psychological medical group, Oc-
tober.
R. Huber, A. Batliner, J. Buckow, E. N¨oth, V. Warnke,
and H. Niemann. 2000. Recognition of Emotion in
a Realistic Dialogue Scenario. In Proc. Int. Conf. on
Spoken Language Processing, volume 1, pages 665–
668, Beijing, China, Oktober.
S Jekat, A Klein, E Maier, I Maleck, M Mast, and
J Quantz. 1995. Dialogue acts in Verbmobil.
Technical Report 65, DFKI Saarbrucken, Universitat
Stuttgart, Technische Universitat Berlin, Universitat
des Saarlandes.
B MacWhinney, 1996. The CHILDES System. Carnegie
Mellon University.
B MacWhinney, 1998. The CHILDES Project: Tools for
Analysing Talk. Carnegie Mellon University.
P Maguire, K Booth, C Elliott, and B Jones. 1996a.
Helping health professionals involved in cancer care
work shops acquire key interviewing skills – the im-
pact of workshops. European journal of cancer,
32A(9):1486–1489.
P Maguire, K Faulkner, K Booth, C Elliot, and V Hillier.
1996b. Helping cancer patients disclose their con-
cerns. European Journal of Cancer, 32(9):1486–1489.
V. Petrushin. 1999. Emotion in speech: Recognition and
application to call centers.
K.R Scherer and G Ceschi. 2000. Criteria for emo-
tion recognition from verbal and nonverbal expression:
studying baggage loss in the airport. Personality &amp; So-
cial Psychology Bulletin, 26(3):327, March.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema,
and M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339–373.
D Traum, 1996. Coding Schemes for Spoken Dialogue
Structure.
R.J van Vark, J.P.M de Vreught, and L.J.M Rothkrantz,
1996. Analysing OVR dialogue coding scheme 1.0.
Delft University of Technology.
M Walker and R Passonneau. 2001. DATE: a dialogue
act tagging scheme for evaluation of spoken dialogue
systems. In Proceedings: Human Language Technol-
ogy Conference, San Diego, March. AT&amp;T Shannon
Labs.
M Walker, E Maier, J Allen, J Carletta, S Condon,
G Flammia, J Hirschberg, S Isard, M Ishizaki, L Levin,
S Luperfoy, D Traum, and S Whittaker, 1996. Penn
multiparty standard coding scheme, Draft annotation
manual.
M.M Wood and R Craggs. 2002. Rare dialogue acts in
oncology consultations. In Submitted to SIGdial3.
M.M Wood and R Craggs. 2003. Initiative in health care
dialogues. In Submitted to DiaBruck 7th workshop on
the semantics and pragmatics of dialogue.
M.M Wood. 2001. Dialogue tagsets in oncology. In
Proceedings of Sigdial2.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.518897">
<title confidence="0.999804">Annotating emotion in dialogue</title>
<author confidence="0.999587">Richard</author>
<affiliation confidence="0.999913">Department of Computer</affiliation>
<address confidence="0.872782">Manchester</address>
<email confidence="0.992603">craggs@cs.man.ac.uk</email>
<author confidence="0.999947">Mary McGee</author>
<affiliation confidence="0.999927">Department of Computer</affiliation>
<address confidence="0.648692">Manchester</address>
<email confidence="0.997618">mary@cs.man.ac.uk</email>
<abstract confidence="0.999659333333333">Communication behaviour is affected by emotion. Here we discuss how dialogue is affected by participants’ emotion and how expressions of emotion are manifested in its content.</abstract>
<keyword confidence="0.92609">Keywords: Dialogue, Emotions, Annotation</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allwood</author>
</authors>
<title>On dialogue cohesion.</title>
<date>1996</date>
<booktitle>In Papers from Thirteenth Scandinavian Conference ofLinguistics.</booktitle>
<contexts>
<context position="21356" citStr="Allwood, 1996" startWordPosition="3434" endWordPosition="3435">longer under increasing levels of anxiety”) and correlations with other layers (e.g. “People ask open questions when relaxed but closed questions when agitated”). It would also allow us to plot the quantitative level of emotions throughout the dialogue, investigate the way in which this changes and identify the language phenomena that signal these changes. If only for pragmatic reasons, it would be wise to choose utterances as the basic unit for annotation. By utterances here we refer to the common understanding described as ’a sequence of communicative behaviour bounded by lack of activity’ (Allwood, 1996). This would not only allow us to apply other schemes to emotionally annotated dialogue, but also to use tools that have been developed to work on utterances. It would therefore be necessary to chose dimensions of emotion that can be applied to utterances. There is an interesting question of whether emotion is a property of the participants or the dialogue. Obviously two or more people participating in a dialogue will react differently to the proceedings and will therefore exhibit different emotions. However it is apparent from our corpus that the dialogue itself has its own levels of emotion.</context>
</contexts>
<marker>Allwood, 1996</marker>
<rawString>J Allwood. 1996. On dialogue cohesion. In Papers from Thirteenth Scandinavian Conference ofLinguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Anderson</author>
<author>M Bader</author>
<author>E Bard</author>
<author>E Boyle</author>
<author>G Doherty</author>
<author>S Garrod</author>
<author>S Isard</author>
<author>J Kowtko</author>
<author>J McAllister</author>
<author>J Miller</author>
<author>C Sotillo</author>
<author>H Thompson</author>
<author>R Weinert</author>
</authors>
<title>The HCRC Map Task corpus. Language and Speech,</title>
<date>1991</date>
<pages>34--351</pages>
<contexts>
<context position="4649" citStr="Anderson et al., 1991" startWordPosition="720" endWordPosition="724">bit different behaviours from each other when they speak. For example some people are more expressive than others and so a large number of expletives from one person may be natural, and not indicative of their emotional state. Prosodic studies of emotion also suffer from this complexity and it would be interesting to see if language use, and dialogue behaviour are more robust indicators of emotion than prosody. 3.1 Task and conversational dialogue Most dialogue research concentrates either on task based dialogue, where the participants converse in order to achieve some set goal (e.g. Maptask (Anderson et al., 1991) Coconut (Di Eugenio et al., 1998)), or on conversational dialogue, which is often less structured and contains a richer use of language (e.g. DAMSL (Core and Allen, 1997) and Chat (MacWhinney, 1998)). It seems likely that we would see more expressions of emotion in conversational dialogue where people are discussing topics of personal interest rather than the more mechanical process of achieving a goal through communication. These differences are reflected in the types of phenomena that the schemes are designed to identify. Task based research may be more interested in the structure of the di</context>
</contexts>
<marker>Anderson, Bader, Bard, Boyle, Doherty, Garrod, Isard, Kowtko, McAllister, Miller, Sotillo, Thompson, Weinert, 1991</marker>
<rawString>A Anderson, M Bader, E Bard, Boyle E, G Doherty, S Garrod, S Isard, J Kowtko, J McAllister, J Miller, C Sotillo, H Thompson, and R Weinert. 1991. The HCRC Map Task corpus. Language and Speech, 34:351–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bub</author>
<author>J Schwinn</author>
</authors>
<title>VERBMOBIL: The evolution of a complex large speech-to-speech translation system.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ’96,</booktitle>
<volume>4</volume>
<pages>2371--2374</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2912" citStr="Bub and Schwinn, 1996" startWordPosition="441" endWordPosition="444">n I’m washing and everything I still get a funny feeling N. You get a funny feeling, in which way? P. It just feels strange, hollow, N. Physically? P. Physically yes,yes N. Yes P. It feels really weird Turn taking behaviour changes under these circumstances too. An emotional speaker will hold the floor for an increased length of time when discussing a topic about which they feel, for example, anxiety or joy. Although these are casual observations of a small amount of dialogue, other studies have benefited from investigation into a speaker’s behaviour when emotional. For the Verbmobil project (Bub and Schwinn, 1996) it was recognised that anger in speakers changed the way in which they communicated (Fischer, 1999). Also applications such as automated call centres would benefit from recognition of human emotion so that humans could intervene when a customer becomes angry and frustrated (Petrushin, 1999). However, these insights are limited to the vocal expression of the speaker. An annotated corpus of emotional dialogue would allow us to study all aspects of a speaker’s behaviour. 3 Annotating emotion in dialogue I envisage that a scheme to annotate dialogue would constitute one or more layers augmenting </context>
</contexts>
<marker>Bub, Schwinn, 1996</marker>
<rawString>T Bub and J Schwinn. 1996. VERBMOBIL: The evolution of a complex large speech-to-speech translation system. In Proc. ICSLP ’96, volume 4, pages 2371– 2374, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Condon</author>
<author>C Cech</author>
</authors>
<title>Manual for Coding Decision-Making Interactions.</title>
<date>1996</date>
<contexts>
<context position="5850" citStr="Condon and Cech, 1996" startWordPosition="920" endWordPosition="923"> structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe the wider range of illocutionary acts that may be performed in this type of speech. 3.2 Current dialogue annotation schemes In order to learn how current annotation schemes accommodate emotion, we aligned the layers in a number of schemes. (Core and Allen, 1997; Di Eugenio et al., 1998; Traum, 1996; Walker et al., 1996; MacWhinney, 1996; Jekat et al., 1995; Anderson et al., 1991; Condon and Cech, 1996; van Vark et al., 1996; Walker and Passonneau, 2001). Layers from different schemes are grouped according to the similar phenomena that they label. Table 1 shows this alignment. In this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue. Information level When analysing task dialogue, we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of a task, discussing of a task etc. In the previous section we suggested that we are more likely to find emotional speech in conversation</context>
</contexts>
<marker>Condon, Cech, 1996</marker>
<rawString>S Condon and C Cech, 1996. Manual for Coding Decision-Making Interactions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>James F Allen</author>
</authors>
<title>Coding dialogs with the damsl annatation scheme.</title>
<date>1997</date>
<booktitle>In AAAI Fall Symposium on Communicative Action in Humans and Machines.</booktitle>
<contexts>
<context position="4820" citStr="Core and Allen, 1997" startWordPosition="751" endWordPosition="754"> natural, and not indicative of their emotional state. Prosodic studies of emotion also suffer from this complexity and it would be interesting to see if language use, and dialogue behaviour are more robust indicators of emotion than prosody. 3.1 Task and conversational dialogue Most dialogue research concentrates either on task based dialogue, where the participants converse in order to achieve some set goal (e.g. Maptask (Anderson et al., 1991) Coconut (Di Eugenio et al., 1998)), or on conversational dialogue, which is often less structured and contains a richer use of language (e.g. DAMSL (Core and Allen, 1997) and Chat (MacWhinney, 1998)). It seems likely that we would see more expressions of emotion in conversational dialogue where people are discussing topics of personal interest rather than the more mechanical process of achieving a goal through communication. These differences are reflected in the types of phenomena that the schemes are designed to identify. Task based research may be more interested in the structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of </context>
</contexts>
<marker>Core, Allen, 1997</marker>
<rawString>Mark G Core and James F Allen. 1997. Coding dialogs with the damsl annatation scheme. In AAAI Fall Symposium on Communicative Action in Humans and Machines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Dellaert</author>
<author>T Polzin</author>
<author>A Waibel</author>
</authors>
<title>Recognizing emotions in speech.</title>
<date>1996</date>
<booktitle>In Proc. ICSLP ’96,</booktitle>
<volume>3</volume>
<pages>1970--1973</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="17415" citStr="Dellaert et al., 1996" startWordPosition="2805" endWordPosition="2808"> be the desired type of data. If we are interested communicative behaviour such as turn-taking and language use rather than the acoustic features of the speech then we need not be so concerned with the acoustic quality. If it were possible to obtain recordings of police interviews, legal trails or calls to emergency services then these would provide suitable material to study. Our corpus of oncology consultations is a good example of this type of dialogue. A more common type of data used in speech studies is that of acted emotions. Actors deliver lines expressed with different emotions (e.g. (Dellaert et al., 1996)). The quality of this data is reliant on the accuracy with which the emotion is acted. This is suitable for establishing the prosodic features associated with various emotions but not for dialogue studies. It would be much more difficult to recreate the communicative behaviour of an emotional person through acting than to simply sound emotional. Finally, induced emotion, where participants are provoked into an emotional state so that their speech can be recorded (e.g. (Huber et al., 2000)) . This provides natural emotion within a laboratory setting. It is conceivable that this process could b</context>
</contexts>
<marker>Dellaert, Polzin, Waibel, 1996</marker>
<rawString>F. Dellaert, T. Polzin, and A. Waibel. 1996. Recognizing emotions in speech. In Proc. ICSLP ’96, volume 3, pages 1970–1973, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>P W Jordan</author>
<author>L Pylkkanen</author>
</authors>
<title>The COCONUT project: Dialogue Annotation Manual.</title>
<date>1998</date>
<tech>ISP Technical Report 98-1.</tech>
<marker>Di Eugenio, Jordan, Pylkkanen, 1998</marker>
<rawString>B Di Eugenio, P.W Jordan, and L Pylkkanen, 1998. The COCONUT project: Dialogue Annotation Manual. ISP Technical Report 98-1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fischer</author>
</authors>
<title>Annotating emotional langauge data.</title>
<date>1999</date>
<tech>Technical report, Verbmobil - report 236.</tech>
<contexts>
<context position="3012" citStr="Fischer, 1999" startWordPosition="460" endWordPosition="461">t feels strange, hollow, N. Physically? P. Physically yes,yes N. Yes P. It feels really weird Turn taking behaviour changes under these circumstances too. An emotional speaker will hold the floor for an increased length of time when discussing a topic about which they feel, for example, anxiety or joy. Although these are casual observations of a small amount of dialogue, other studies have benefited from investigation into a speaker’s behaviour when emotional. For the Verbmobil project (Bub and Schwinn, 1996) it was recognised that anger in speakers changed the way in which they communicated (Fischer, 1999). Also applications such as automated call centres would benefit from recognition of human emotion so that humans could intervene when a customer becomes angry and frustrated (Petrushin, 1999). However, these insights are limited to the vocal expression of the speaker. An annotated corpus of emotional dialogue would allow us to study all aspects of a speaker’s behaviour. 3 Annotating emotion in dialogue I envisage that a scheme to annotate dialogue would constitute one or more layers augmenting an existing annotation scheme. There are plenty of other schemes developed for previous dialogue res</context>
</contexts>
<marker>Fischer, 1999</marker>
<rawString>K Fischer. 1999. Annotating emotional langauge data. Technical report, Verbmobil - report 236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Heaven</author>
<author>C Green</author>
</authors>
<date>2001</date>
<booktitle>Medical Interview Aural Rating Scale, CRC psychological medical group,</booktitle>
<contexts>
<context position="20344" citStr="Heaven and Green, 2001" startWordPosition="3268" endWordPosition="3271">ir communicative behaviour. They must also be identifiable from the language of the dialogue. This will mean that the scheme may consist of several layers each describing a different aspect of human emotion. One of the differences between these types of layers and those current schemes is that rather than discrete categories such as those used to label speech acts we can observe varying levels of emotion. A precedent for this type of annotation exists in the labelling of expressions of concern in the oncology consultation coding scheme of Psychological Medicine Group at Manchester University (Heaven and Green, 2001). where these cues are rated 0–3. If this approach was adopted then we would have to decide on the number of levels to chose from based on a trade-off between ease of performing annotation with getting a fine enough distinction between different levels. This would allow us to draw conclusions about communicative behaviour under different levels of emotion (e.g. “The length of utterances becomes longer under increasing levels of anxiety”) and correlations with other layers (e.g. “People ask open questions when relaxed but closed questions when agitated”). It would also allow us to plot the quan</context>
</contexts>
<marker>Heaven, Green, 2001</marker>
<rawString>C Heaven and C Green, 2001. Medical Interview Aural Rating Scale, CRC psychological medical group, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Huber</author>
<author>A Batliner</author>
<author>J Buckow</author>
<author>E N¨oth</author>
<author>V Warnke</author>
<author>H Niemann</author>
</authors>
<title>Recognition of Emotion in a Realistic Dialogue Scenario.</title>
<date>2000</date>
<booktitle>In Proc. Int. Conf. on Spoken Language Processing,</booktitle>
<volume>1</volume>
<pages>665--668</pages>
<location>Beijing, China, Oktober.</location>
<marker>Huber, Batliner, Buckow, N¨oth, Warnke, Niemann, 2000</marker>
<rawString>R. Huber, A. Batliner, J. Buckow, E. N¨oth, V. Warnke, and H. Niemann. 2000. Recognition of Emotion in a Realistic Dialogue Scenario. In Proc. Int. Conf. on Spoken Language Processing, volume 1, pages 665– 668, Beijing, China, Oktober.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jekat</author>
<author>A Klein</author>
<author>E Maier</author>
<author>I Maleck</author>
<author>M Mast</author>
<author>J Quantz</author>
</authors>
<title>Dialogue acts in Verbmobil.</title>
<date>1995</date>
<tech>Technical Report 65,</tech>
<institution>DFKI Saarbrucken, Universitat Stuttgart, Technische Universitat Berlin, Universitat des Saarlandes.</institution>
<contexts>
<context position="5804" citStr="Jekat et al., 1995" startWordPosition="912" endWordPosition="915">ased research may be more interested in the structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe the wider range of illocutionary acts that may be performed in this type of speech. 3.2 Current dialogue annotation schemes In order to learn how current annotation schemes accommodate emotion, we aligned the layers in a number of schemes. (Core and Allen, 1997; Di Eugenio et al., 1998; Traum, 1996; Walker et al., 1996; MacWhinney, 1996; Jekat et al., 1995; Anderson et al., 1991; Condon and Cech, 1996; van Vark et al., 1996; Walker and Passonneau, 2001). Layers from different schemes are grouped according to the similar phenomena that they label. Table 1 shows this alignment. In this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue. Information level When analysing task dialogue, we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of a task, discussing of a task etc. In the previous section we suggested that we are more l</context>
<context position="15515" citStr="Jekat et al., 1995" startWordPosition="2501" endWordPosition="2504"> layers. Grounding Grounding describes the process by which common ground between the participants is established. As with relatedness and information relations, emotion in the dialogue may be manifested in this layer by protracted grounding behaviour as people reiterate points about which they feel emotional. In our highly emotional corpus this resulted in four times as many summaries and five times as many repetitions than in the Switchboard corpus. Besides the layers listed here there are other layers included in schemes that do not fit into any of these categories. For instance Verbmobil (Jekat et al., 1995) includes a layer for annotating the propositional content of an utterance, and content relevance in the Penn multi-party coding scheme (Walker et al., 1996). Investigation on dialogue annotated for emotion will show whether there are any interesting correlations with these layers. 4 Emotional speech corpora One of the difficulties in analysing emotion in communication is in obtaining the material to study. For studies into task dialogues, researchers can simply record speakers performing the tasks. However capturing conversational dialogue in general and especially emotional dialogue is a muc</context>
</contexts>
<marker>Jekat, Klein, Maier, Maleck, Mast, Quantz, 1995</marker>
<rawString>S Jekat, A Klein, E Maier, I Maleck, M Mast, and J Quantz. 1995. Dialogue acts in Verbmobil. Technical Report 65, DFKI Saarbrucken, Universitat Stuttgart, Technische Universitat Berlin, Universitat des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES System.</title>
<date>1996</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="5784" citStr="MacWhinney, 1996" startWordPosition="910" endWordPosition="911">o identify. Task based research may be more interested in the structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe the wider range of illocutionary acts that may be performed in this type of speech. 3.2 Current dialogue annotation schemes In order to learn how current annotation schemes accommodate emotion, we aligned the layers in a number of schemes. (Core and Allen, 1997; Di Eugenio et al., 1998; Traum, 1996; Walker et al., 1996; MacWhinney, 1996; Jekat et al., 1995; Anderson et al., 1991; Condon and Cech, 1996; van Vark et al., 1996; Walker and Passonneau, 2001). Layers from different schemes are grouped according to the similar phenomena that they label. Table 1 shows this alignment. In this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue. Information level When analysing task dialogue, we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of a task, discussing of a task etc. In the previous section we suggeste</context>
</contexts>
<marker>MacWhinney, 1996</marker>
<rawString>B MacWhinney, 1996. The CHILDES System. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES Project: Tools for Analysing Talk.</title>
<date>1998</date>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="4848" citStr="MacWhinney, 1998" startWordPosition="757" endWordPosition="758">their emotional state. Prosodic studies of emotion also suffer from this complexity and it would be interesting to see if language use, and dialogue behaviour are more robust indicators of emotion than prosody. 3.1 Task and conversational dialogue Most dialogue research concentrates either on task based dialogue, where the participants converse in order to achieve some set goal (e.g. Maptask (Anderson et al., 1991) Coconut (Di Eugenio et al., 1998)), or on conversational dialogue, which is often less structured and contains a richer use of language (e.g. DAMSL (Core and Allen, 1997) and Chat (MacWhinney, 1998)). It seems likely that we would see more expressions of emotion in conversational dialogue where people are discussing topics of personal interest rather than the more mechanical process of achieving a goal through communication. These differences are reflected in the types of phenomena that the schemes are designed to identify. Task based research may be more interested in the structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe th</context>
</contexts>
<marker>MacWhinney, 1998</marker>
<rawString>B MacWhinney, 1998. The CHILDES Project: Tools for Analysing Talk. Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Maguire</author>
<author>K Booth</author>
<author>C Elliott</author>
<author>B Jones</author>
</authors>
<title>Helping health professionals involved in cancer care work shops acquire key interviewing skills – the impact of workshops. European journal of cancer,</title>
<date>1996</date>
<pages>32--9</pages>
<contexts>
<context position="9373" citStr="Maguire et al., 1996" startWordPosition="1491" endWordPosition="1494">municative functions describe utterances that intend to evoke some response from the listener (such as believing a statement or answering a question), perform an action (such as committing to something) or similar dialogue advancing functions. These types of utterance are likely to be motivated by some intention or belief on behalf of the speaker, providing clues as to their cognitive state. Forward communicative functions can play an important role in eliciting emotional responses from the listener. Open ended questions are more likely to produce an emotional response than a yes/no question (Maguire et al., 1996b). This is partly because open questions hand the initiative to the listener allowing them to express themselves. The relationship between questions, initiative and emotion is discussed further in (Wood and Craggs, 2003). The following extract from our corpus show how an open question elicits an emotional response from the listener. N. How were you coping with that yourself? P. Oh mentally I’ve never been down mentally Backward communicative functions are used to label utterances that respond to something that has been said to them. Some responses are required by the previous utterance, for e</context>
<context position="13263" citStr="Maguire et al., 1996" startWordPosition="2136" endWordPosition="2139">dications of emotion. Phases Some schemes distinguish between dialogue phases such as opening, negotiation and query. Emotion in dialogue also goes through phases and it is possible that there are boundaries between the phases of emotion that correspond to those tagged using the phase layer. An interesting area of research would be to identify how boundaries between the phases of different levels and types of emotion are manifested in the use of language. For instance psycho-oncology research states that open ended questions are more likely to elicit emotional responses than yes-no questions (Maguire et al., 1996a). This may cause a correlation between forward-looking functions and the onset of phases. Surface form Surface form tagging is used in David Traum’s adaptation of the TRAINS annotation scheme (Traum, 1996) and the Coconut scheme to tag utterances for certain special features such as cue words or negation. It has been shown that certain syntactic features of an utterance may be indicators of emotion. For example in German use of modal particles such as ‘eben’ and ‘denn’ colour the utterance with a particular emotional attitude. Although the surface form of utterances is dependent on the style</context>
</contexts>
<marker>Maguire, Booth, Elliott, Jones, 1996</marker>
<rawString>P Maguire, K Booth, C Elliott, and B Jones. 1996a. Helping health professionals involved in cancer care work shops acquire key interviewing skills – the impact of workshops. European journal of cancer, 32A(9):1486–1489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Maguire</author>
<author>K Faulkner</author>
<author>K Booth</author>
<author>C Elliot</author>
<author>V Hillier</author>
</authors>
<title>Helping cancer patients disclose their concerns.</title>
<date>1996</date>
<journal>European Journal of Cancer,</journal>
<volume>32</volume>
<issue>9</issue>
<contexts>
<context position="9373" citStr="Maguire et al., 1996" startWordPosition="1491" endWordPosition="1494">municative functions describe utterances that intend to evoke some response from the listener (such as believing a statement or answering a question), perform an action (such as committing to something) or similar dialogue advancing functions. These types of utterance are likely to be motivated by some intention or belief on behalf of the speaker, providing clues as to their cognitive state. Forward communicative functions can play an important role in eliciting emotional responses from the listener. Open ended questions are more likely to produce an emotional response than a yes/no question (Maguire et al., 1996b). This is partly because open questions hand the initiative to the listener allowing them to express themselves. The relationship between questions, initiative and emotion is discussed further in (Wood and Craggs, 2003). The following extract from our corpus show how an open question elicits an emotional response from the listener. N. How were you coping with that yourself? P. Oh mentally I’ve never been down mentally Backward communicative functions are used to label utterances that respond to something that has been said to them. Some responses are required by the previous utterance, for e</context>
<context position="13263" citStr="Maguire et al., 1996" startWordPosition="2136" endWordPosition="2139">dications of emotion. Phases Some schemes distinguish between dialogue phases such as opening, negotiation and query. Emotion in dialogue also goes through phases and it is possible that there are boundaries between the phases of emotion that correspond to those tagged using the phase layer. An interesting area of research would be to identify how boundaries between the phases of different levels and types of emotion are manifested in the use of language. For instance psycho-oncology research states that open ended questions are more likely to elicit emotional responses than yes-no questions (Maguire et al., 1996a). This may cause a correlation between forward-looking functions and the onset of phases. Surface form Surface form tagging is used in David Traum’s adaptation of the TRAINS annotation scheme (Traum, 1996) and the Coconut scheme to tag utterances for certain special features such as cue words or negation. It has been shown that certain syntactic features of an utterance may be indicators of emotion. For example in German use of modal particles such as ‘eben’ and ‘denn’ colour the utterance with a particular emotional attitude. Although the surface form of utterances is dependent on the style</context>
</contexts>
<marker>Maguire, Faulkner, Booth, Elliot, Hillier, 1996</marker>
<rawString>P Maguire, K Faulkner, K Booth, C Elliot, and V Hillier. 1996b. Helping cancer patients disclose their concerns. European Journal of Cancer, 32(9):1486–1489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Petrushin</author>
</authors>
<title>Emotion in speech: Recognition and application to call centers.</title>
<date>1999</date>
<contexts>
<context position="3204" citStr="Petrushin, 1999" startWordPosition="490" endWordPosition="491">floor for an increased length of time when discussing a topic about which they feel, for example, anxiety or joy. Although these are casual observations of a small amount of dialogue, other studies have benefited from investigation into a speaker’s behaviour when emotional. For the Verbmobil project (Bub and Schwinn, 1996) it was recognised that anger in speakers changed the way in which they communicated (Fischer, 1999). Also applications such as automated call centres would benefit from recognition of human emotion so that humans could intervene when a customer becomes angry and frustrated (Petrushin, 1999). However, these insights are limited to the vocal expression of the speaker. An annotated corpus of emotional dialogue would allow us to study all aspects of a speaker’s behaviour. 3 Annotating emotion in dialogue I envisage that a scheme to annotate dialogue would constitute one or more layers augmenting an existing annotation scheme. There are plenty of other schemes developed for previous dialogue research, many of which are designed to investigate a particular phenomenon of communicative behaviour. In this section we will look at some existing annotation schemes. We shall investigate if a</context>
</contexts>
<marker>Petrushin, 1999</marker>
<rawString>V. Petrushin. 1999. Emotion in speech: Recognition and application to call centers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
<author>G Ceschi</author>
</authors>
<title>Criteria for emotion recognition from verbal and nonverbal expression: studying baggage loss in the airport.</title>
<date>2000</date>
<journal>Personality &amp; Social Psychology Bulletin,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="16493" citStr="Scherer and Ceschi, 2000" startWordPosition="2653" endWordPosition="2656">n in communication is in obtaining the material to study. For studies into task dialogues, researchers can simply record speakers performing the tasks. However capturing conversational dialogue in general and especially emotional dialogue is a much more difficult task. Studies into emotional speech based on acoustic features use three approaches to attain their data. Ideally it is preferable to use genuine speech taken without the speaker’s knowledge since you can be confident that the resulting data will faithfully represent human behaviour. An example of research using this type of data is (Scherer and Ceschi, 2000). This approach isn’t commonly adopted, partly because of the ethical issues concerned with recording people without their consent and also because of the difficulty in controlling variables such as recording quality or establishing age, sex, etc. of the speaker. For dialogue studies this would also be the desired type of data. If we are interested communicative behaviour such as turn-taking and language use rather than the acoustic features of the speech then we need not be so concerned with the acoustic quality. If it were possible to obtain recordings of police interviews, legal trails or c</context>
</contexts>
<marker>Scherer, Ceschi, 2000</marker>
<rawString>K.R Scherer and G Ceschi. 2000. Criteria for emotion recognition from verbal and nonverbal expression: studying baggage loss in the airport. Personality &amp; Social Psychology Bulletin, 26(3):327, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>R Martin</author>
<author>C Van Ess-Dykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--339</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykema, Meteer, 2000</marker>
<rawString>A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26:339–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Traum</author>
</authors>
<title>Coding Schemes for Spoken Dialogue Structure.</title>
<date>1996</date>
<contexts>
<context position="5745" citStr="Traum, 1996" startWordPosition="904" endWordPosition="905">na that the schemes are designed to identify. Task based research may be more interested in the structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe the wider range of illocutionary acts that may be performed in this type of speech. 3.2 Current dialogue annotation schemes In order to learn how current annotation schemes accommodate emotion, we aligned the layers in a number of schemes. (Core and Allen, 1997; Di Eugenio et al., 1998; Traum, 1996; Walker et al., 1996; MacWhinney, 1996; Jekat et al., 1995; Anderson et al., 1991; Condon and Cech, 1996; van Vark et al., 1996; Walker and Passonneau, 2001). Layers from different schemes are grouped according to the similar phenomena that they label. Table 1 shows this alignment. In this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue. Information level When analysing task dialogue, we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of a task, discussing of a task e</context>
<context position="13470" citStr="Traum, 1996" startWordPosition="2169" endWordPosition="2170"> the phases of emotion that correspond to those tagged using the phase layer. An interesting area of research would be to identify how boundaries between the phases of different levels and types of emotion are manifested in the use of language. For instance psycho-oncology research states that open ended questions are more likely to elicit emotional responses than yes-no questions (Maguire et al., 1996a). This may cause a correlation between forward-looking functions and the onset of phases. Surface form Surface form tagging is used in David Traum’s adaptation of the TRAINS annotation scheme (Traum, 1996) and the Coconut scheme to tag utterances for certain special features such as cue words or negation. It has been shown that certain syntactic features of an utterance may be indicators of emotion. For example in German use of modal particles such as ‘eben’ and ‘denn’ colour the utterance with a particular emotional attitude. Although the surface form of utterances is dependent on the style of the speaker, it does sometimes contain indications of emotion. P. Oh no, no no no no, I’m not in any discomfort Relatedness and Information relations The relatedness layer is used to show how utterances </context>
</contexts>
<marker>Traum, 1996</marker>
<rawString>D Traum, 1996. Coding Schemes for Spoken Dialogue Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J van Vark</author>
<author>J P M de Vreught</author>
<author>L J M Rothkrantz</author>
</authors>
<title>Analysing OVR dialogue coding scheme 1.0.</title>
<date>1996</date>
<institution>Delft University of Technology.</institution>
<marker>van Vark, de Vreught, Rothkrantz, 1996</marker>
<rawString>R.J van Vark, J.P.M de Vreught, and L.J.M Rothkrantz, 1996. Analysing OVR dialogue coding scheme 1.0. Delft University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>R Passonneau</author>
</authors>
<title>DATE: a dialogue act tagging scheme for evaluation of spoken dialogue systems.</title>
<date>2001</date>
<booktitle>In Proceedings: Human Language Technology Conference,</booktitle>
<location>San Diego, March. AT&amp;T Shannon Labs.</location>
<contexts>
<context position="5903" citStr="Walker and Passonneau, 2001" startWordPosition="929" endWordPosition="932"> represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe the wider range of illocutionary acts that may be performed in this type of speech. 3.2 Current dialogue annotation schemes In order to learn how current annotation schemes accommodate emotion, we aligned the layers in a number of schemes. (Core and Allen, 1997; Di Eugenio et al., 1998; Traum, 1996; Walker et al., 1996; MacWhinney, 1996; Jekat et al., 1995; Anderson et al., 1991; Condon and Cech, 1996; van Vark et al., 1996; Walker and Passonneau, 2001). Layers from different schemes are grouped according to the similar phenomena that they label. Table 1 shows this alignment. In this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue. Information level When analysing task dialogue, we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of a task, discussing of a task etc. In the previous section we suggested that we are more likely to find emotional speech in conversational rather than task dialogue because the latter is mo</context>
</contexts>
<marker>Walker, Passonneau, 2001</marker>
<rawString>M Walker and R Passonneau. 2001. DATE: a dialogue act tagging scheme for evaluation of spoken dialogue systems. In Proceedings: Human Language Technology Conference, San Diego, March. AT&amp;T Shannon Labs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>E Maier</author>
<author>J Allen</author>
<author>J Carletta</author>
<author>S Condon</author>
<author>G Flammia</author>
<author>J Hirschberg</author>
<author>S Isard</author>
<author>M Ishizaki</author>
<author>L Levin</author>
<author>S Luperfoy</author>
<author>D Traum</author>
<author>S Whittaker</author>
</authors>
<title>Penn multiparty standard coding scheme, Draft annotation manual.</title>
<date>1996</date>
<contexts>
<context position="5766" citStr="Walker et al., 1996" startWordPosition="906" endWordPosition="909">chemes are designed to identify. Task based research may be more interested in the structure of the dialogue and the way that it represents the division of the task into sub-goals. Schemes to annotate conversational dialogue are more likely to require a greater breadth of dialogue acts to describe the wider range of illocutionary acts that may be performed in this type of speech. 3.2 Current dialogue annotation schemes In order to learn how current annotation schemes accommodate emotion, we aligned the layers in a number of schemes. (Core and Allen, 1997; Di Eugenio et al., 1998; Traum, 1996; Walker et al., 1996; MacWhinney, 1996; Jekat et al., 1995; Anderson et al., 1991; Condon and Cech, 1996; van Vark et al., 1996; Walker and Passonneau, 2001). Layers from different schemes are grouped according to the similar phenomena that they label. Table 1 shows this alignment. In this section we will look at these layers and discuss how they may relate to annotating emotion in dialogue. Information level When analysing task dialogue, we may be interested in knowing whether an utterance pertains to the management of the communications channel, advancement of a task, discussing of a task etc. In the previous s</context>
<context position="15672" citStr="Walker et al., 1996" startWordPosition="2525" endWordPosition="2528">ions, emotion in the dialogue may be manifested in this layer by protracted grounding behaviour as people reiterate points about which they feel emotional. In our highly emotional corpus this resulted in four times as many summaries and five times as many repetitions than in the Switchboard corpus. Besides the layers listed here there are other layers included in schemes that do not fit into any of these categories. For instance Verbmobil (Jekat et al., 1995) includes a layer for annotating the propositional content of an utterance, and content relevance in the Penn multi-party coding scheme (Walker et al., 1996). Investigation on dialogue annotated for emotion will show whether there are any interesting correlations with these layers. 4 Emotional speech corpora One of the difficulties in analysing emotion in communication is in obtaining the material to study. For studies into task dialogues, researchers can simply record speakers performing the tasks. However capturing conversational dialogue in general and especially emotional dialogue is a much more difficult task. Studies into emotional speech based on acoustic features use three approaches to attain their data. Ideally it is preferable to use ge</context>
</contexts>
<marker>Walker, Maier, Allen, Carletta, Condon, Flammia, Hirschberg, Isard, Ishizaki, Levin, Luperfoy, Traum, Whittaker, 1996</marker>
<rawString>M Walker, E Maier, J Allen, J Carletta, S Condon, G Flammia, J Hirschberg, S Isard, M Ishizaki, L Levin, S Luperfoy, D Traum, and S Whittaker, 1996. Penn multiparty standard coding scheme, Draft annotation manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Wood</author>
<author>R Craggs</author>
</authors>
<title>Rare dialogue acts in oncology consultations.</title>
<date>2002</date>
<note>In Submitted to SIGdial3.</note>
<contexts>
<context position="8740" citStr="Wood and Craggs, 2002" startWordPosition="1391" endWordPosition="1394">munication status may signal behaviour that can result in emotion in the listener which is also applicable to Human-Human dialogue. Speech acts All of the schemes that we examined annotated the utterances for their illocutionary force. Since this is the layer that contains most information regarding the semantic content of an utterance, this is likely to be where we shall find the most interesting correlations. We have already seen that high levels of emotion in dialogue alters the frequencies of dialogue acts compared with the more impassive conversations conducted in the Switchboard corpus (Wood and Craggs, 2002). Forward communicative functions describe utterances that intend to evoke some response from the listener (such as believing a statement or answering a question), perform an action (such as committing to something) or similar dialogue advancing functions. These types of utterance are likely to be motivated by some intention or belief on behalf of the speaker, providing clues as to their cognitive state. Forward communicative functions can play an important role in eliciting emotional responses from the listener. Open ended questions are more likely to produce an emotional response than a yes/</context>
</contexts>
<marker>Wood, Craggs, 2002</marker>
<rawString>M.M Wood and R Craggs. 2002. Rare dialogue acts in oncology consultations. In Submitted to SIGdial3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Wood</author>
<author>R Craggs</author>
</authors>
<title>Initiative in health care dialogues.</title>
<date>2003</date>
<booktitle>In Submitted to DiaBruck 7th workshop on the</booktitle>
<contexts>
<context position="9594" citStr="Wood and Craggs, 2003" startWordPosition="1526" endWordPosition="1529">gue advancing functions. These types of utterance are likely to be motivated by some intention or belief on behalf of the speaker, providing clues as to their cognitive state. Forward communicative functions can play an important role in eliciting emotional responses from the listener. Open ended questions are more likely to produce an emotional response than a yes/no question (Maguire et al., 1996b). This is partly because open questions hand the initiative to the listener allowing them to express themselves. The relationship between questions, initiative and emotion is discussed further in (Wood and Craggs, 2003). The following extract from our corpus show how an open question elicits an emotional response from the listener. N. How were you coping with that yourself? P. Oh mentally I’ve never been down mentally Backward communicative functions are used to label utterances that respond to something that has been said to them. Some responses are required by the previous utterance, for example an answer following a question. In these cases the utterance wasn’t motivated by a desire on behalf of the speaker but rather an obligation to adhere to the rules of engagement for communication. This of course doe</context>
</contexts>
<marker>Wood, Craggs, 2003</marker>
<rawString>M.M Wood and R Craggs. 2003. Initiative in health care dialogues. In Submitted to DiaBruck 7th workshop on the semantics and pragmatics of dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Wood</author>
</authors>
<title>Dialogue tagsets in oncology.</title>
<date>2001</date>
<booktitle>In Proceedings of Sigdial2.</booktitle>
<contexts>
<context position="718" citStr="Wood, 2001" startWordPosition="96" endWordPosition="97">s.man.ac.uk Mary McGee Wood Department of Computer Science Manchester University mary@cs.man.ac.uk Abstract Communication behaviour is affected by emotion. Here we discuss how dialogue is affected by participants’ emotion and how expressions of emotion are manifested in its content. Keywords: Dialogue, Emotions, Annotation 1 Introduction Dialogue annotation is a fundamental stage of much of the research conducted on both Human-Human and Human-Machine dialogue. We are fortunate to have access to a valuable corpus of 37 dialogues between nurses and patients, each comprising 200-1200 utterances (Wood, 2001). These consultations contain genuine emotional speech and form the ideal basis for studies of realistic conversational dialogue. The emotional state of participants affects the way in which the dialogue is conducted. I propose that annotating emotion in dialogue alongside currently annotated phenomena will reveal interesting and useful correlations that will improve our understanding of dialogue and benefit natural language applications. The overall aim of this research is to develop a scheme for annotating expressions of emotion, to create an annotated corpus of dialogue containing emotion a</context>
</contexts>
<marker>Wood, 2001</marker>
<rawString>M.M Wood. 2001. Dialogue tagsets in oncology. In Proceedings of Sigdial2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>