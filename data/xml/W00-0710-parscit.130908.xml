<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001376">
<note confidence="0.808039">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 55-60, Lisbon, Portugal, 2000.
</note>
<title confidence="0.960557">
Learning Distributed Linguistic Classes
</title>
<author confidence="0.927806">
Stephan Raaijmakers
</author>
<affiliation confidence="0.85843025">
Netherlands Organisation for Applied Scientific Research (TNO)
Institute for Applied Physics
Delft
The Netherlands
</affiliation>
<email confidence="0.985289">
raaijmakers@tpdAno.n1
</email>
<sectionHeader confidence="0.986598" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999867133333333">
Error-correcting output codes (ECOC) have
emerged in machine learning as a success-
ful implementation of the idea of distributed
classes. Monadic class symbols are replaced
by bit strings, which are learned by an ensem-
ble of binary-valued classifiers (dichotomizers).
In this study, the idea of ECOC is applied to
memory-based language learning with local (k-
nearest neighbor) classifiers. Regression analy-
sis of the experimental results reveals that, in
order for ECOC to be successful for language
learning, the use of the Modified Value Differ-
ence Metric (MVDM) is an important factor,
which is explained in terms of population den-
sity of the class hyperspace.
</bodyText>
<sectionHeader confidence="0.995569" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999887727272727">
Supervised learning methods applied to natu-
ral language classification tasks commonly op-
erate on high-level symbolic representations,
with linguistic classes that are usually monadic,
without internal structure (Daelemans et al.,
1996; Cardie et al., 1999; Roth, 1998). This
contrasts with the distributed class encoding
commonly found in neural networks (Schmid,
1994). Error-correcting output codes (ECOC)
have been introduced to machine learning as
a principled and successful approach to dis-
tributed class encoding (Dietterich and Bakiri,
1995; Ricci and Aha, 1997; Berger, 1999). With
ECOC, monadic classes are replaced by code-
words, i.e. binary-valued vectors. An ensem-
ble of separate classifiers (dichotomizers) must
be trained to learn the binary subclassifications
for every instance in the training set. During
classification, the bit predictions of the vari-
ous dichotomizers are combined to produce a
codeword prediction. The class codeword which
has minimal Hamming distance to the predicted
codeword determines the classification of the in-
stance. Codewords are constructed such that
their Hamming distance is maximal. Extra bits
are added to allow for error recovery, allowing
the correct class to be determinable even if some
bits are wrong. An error-correcting output code
for a k-class problem constitutes a matrix with
k rows and 2k-1-1 columns. Rows are the code-
words corresponding to classes, and columns are
binary subclassifications or bit functions L such
that, for an instance e, and its codeword vector
</bodyText>
<equation confidence="0.859056">
(e) = (c) (1)
(7ri (v) the i-th coordinate of vector v). If
</equation>
<bodyText confidence="0.999642">
the minimum Hamming distance between ev-
ery codeword is d, then the code has an error-
correcting capability of LY J. Figure 1 shows
the 5 x 15 ECOC matrix, for a 5-class problem.
In this code, every codeword has a Hamming
distance of at least 8 to the other codewords,
so this code has an error-correcting capability
of 3 bits. ECOC have two natural interpreta-
</bodyText>
<table confidence="0.920113166666667">
1 1 0 0 0 1 0 0 0 0 0 0 0 1
0 1 1 0 1 0 0 1 0 1 1 1 0 1 0
1 0 1 0 0 0 1 1 1 0 1 0 1 1 0
[0
1 1 0 0 1 1 1 0 1 1 0 0 0 1 0
1 1 1 1 0 1 0 0 0 1 1 0 1 1 1
</table>
<figureCaption confidence="0.996035">
Figure 1: ECOC for a five-class problem.
</figureCaption>
<bodyText confidence="0.999957333333333">
tions. From an information-theoretic perspec-
tive, classification with ECOC is like channel
coding (Shannon, 1948): the class of a pattern
to be classified is a datum sent over a noisy com-
munication channel. The communication chan-
nel consists of the trained classifier. The noise
consists of the bias (systematic error) and vari-
ance (training set-dependent error) of the classi-
fier, which together make up for the overall error
</bodyText>
<page confidence="0.997492">
55
</page>
<bodyText confidence="0.9955">
of the classifier. The received message must be
decoded before it can be interpreted as a classi-
fication. Adding redundancy to a signal before
transmission is a well-known technique in digi-
tal communication to allow for the recovery of
errors due to noise in the channel, and this is
the key to the success of ECOC. From a ma-
chine learning perspective, an error-correcting
output code uniquely partitions the instances
in the training set into two disjoint subclasses,
0 or 1. This can be interpreted as learning a set
of class boundaries. To illustrate this, consider
the following binary code for a three-class prob-
lem. (This actually is a one-of-c code with no
error-correcting capability (the minimal Ham-
ming distance between the codewords is 1). As
such it is an error-correcting code with lowest
error correction, but it serves to illustrate the
point.)
</bodyText>
<equation confidence="0.81049125">
fi f2 13
Cl 0 0 1
C2 0 1 0
C3 1 0 0
</equation>
<bodyText confidence="0.999894565217391">
For every combination of classes (C1—C2, Cl—
C3, C2—C3), the Hamming distance between the
codewords is 2. These horizontal relations have
vertical repercussions as well: for every such
pair, two bit functions disagree in the classes
they select. For C1—C2, 12 selects C2 and h se-
lects Cl. For C1—C3, fi selects C3 and h selects
Cl. Finally, for C2—C3, fi selects C3 and f2 se-
lects C2. So, every class is selected two times,
and this implies that every class boundary asso-
ciated with that class in the feature hyperspace
is learned twice. In general (Kong and Diet-
terich, 1995), if the minimal Hamming distance
between the codewords of an (error-correcting)
code is d, then every class boundary is learned d
times. For the error-correcting code from above
this implies an error correction of zero: only two
votes support a class boundary, and no vote can
be favored in case of a conflict. The decoding
of the predicted bit string to a class symbol ap-
pears to be a form of voting over class bound-
aries (Kong and Dietterich, 1995), and is able to
reduce both bias and variance of the classifier.
</bodyText>
<sectionHeader confidence="0.990966" genericHeader="method">
2 Dichotomizer Ensembles
</sectionHeader>
<bodyText confidence="0.999878625">
Dichotomizer ensembles must be diverse apart
from accurate. Diversity is necessary in order
to decorrelate the predictions of the various di-
chotomizers. This is a consequence of the voting
mechanism underlying ECOC, where bit func-
tions can only outvote other bit functions if they
do not make similar predictions. Selecting dif-
ferent features per dichotomizer was proposed
for this purpose (Ricci and Aha, 1997). An-
other possibility is to add limited non-locality to
a local classifier, since classifiers that use global
information such as class probabilities during
classification, are much less vulnerable to cor-
related predictions. The following ideas were
tested empirically on a suite of natural language
learning tasks.
</bodyText>
<listItem confidence="0.9995786">
• A careful feature selection approach, where
every dichotomizer is trained to select (pos-
sibly) different features.
• A careless feature selection approach,
where every bit is predicted by a voting
committee of dichotomizers, each of which
randomly selects features (akin in spirit to
the Multiple Feature Subsets approach for
non-distributed classifiers (Bay, 1999).
• A careless feature selection approach,
</listItem>
<bodyText confidence="0.8194995">
where blocks of two adjacent bits are pre-
dicted by a voting committee of quadro-
tomizers, each of which randomly selects
features. Learning blocks of two bits al-
lows for bit codes that are twice as long
(larger error-correction), but with half as
many classifiers. Assuming a normal dis-
tribution of errors and bit values in every 2
bits-block, there is a 25% chance that both
bits in a 2-bit block are wrong. The other
75% chance of one bit wrong would pro-
duce performance equal to voting per bit.
Formally, this implies a switch from N two-
class problems to N/2 four-class problems,
where separate regions of the class land-
scape are learned jointly.
</bodyText>
<listItem confidence="0.877229333333333">
• Adding non-locality to 1-3 in the form of
larger values for k.
• The use of the Modified Value Difference
Metric, which alters the distribution of in-
stances over the hyperspace of features,
yielding different class boundaries.
</listItem>
<sectionHeader confidence="0.986083" genericHeader="method">
3 Memory-based learning
</sectionHeader>
<bodyText confidence="0.999301666666667">
The memory-based learning paradigm views
cognitive processing as reasoning by analogy.
Cognitive classification tasks are carried out by
</bodyText>
<figure confidence="0.387935">
(2)
</figure>
<page confidence="0.975935">
56
</page>
<bodyText confidence="0.999857380952381">
matching data to be classified with classified
data stored in a knowledge base. This latter
data set is called the training data, and its ele-
ments are called instances. Every instance con-
sists of a feature-value vector and a class label.
Learning under the memory-based paradigm is
lazy, and consists only of storing the training
instances in a suitable data structure. The in-
stance from the training set which resembles
the most the item to be classified determines
the classification of the latter. This instance is
called the nearest neighbor, and models based
on this approach to analogy are called nearest
neighbor models (Duda and Hart, 1973). So-
called k-nearest neighbor models select a winner
from the k nearest neighbors, where k is a pa-
rameter and winner selection is usually based on
class frequency. Resemblance between instances
is measured using distance metrics, which come
in many sorts. The simplest distance metric is
the overlap metric:
</bodyText>
<equation confidence="0.998355666666667">
= E 6(7k(Ii),71-k(I3))
6(vi,v3) = 0 if vi = v3 (3)
vi) = 1 if vi vi
</equation>
<bodyText confidence="0.9933335">
(ri(I) is the i-th projection of the feature vec-
tor /.) Another distance metric is the Mod-
ified Value Difference Metric (MVDM) (Cost
and Salzberg, 1993). The MVDM defines sim-
ilarity between two feature values in terms of
posterior probabilities:
</bodyText>
<equation confidence="0.9445235">
P(c — P(c vi) (4)
cECIasses
</equation>
<bodyText confidence="0.999452857142857">
When two values share more classes, they are
more similar, as 6 decreases. Memory-based
learning has fruitfully been applied to natu-
ral language processing, yielding state-of-the-
art performance on all levels of linguistic analy-
sis, including grapheme-to-phoneme conversion
(van den Bosch and Daelemans, 1993), PoS-
tagging (Daelemans et al., 1996), and shallow
parsing (Cardie et al., 1999). In this study,
the following memory-based models are used,
all available from the TIMBL package (Daele-
mans et al., 1999). IB1-IG is a k-nearest dis-
tance classifier which employs a weighted over-
lap metric:
</bodyText>
<equation confidence="0.812484">
(I, I) = wkork (h), 7rk(ii)) (5)
</equation>
<bodyText confidence="0.999969166666667">
In stead of drawing winners from the k-nearest
neighbors pool, IB1-IG selects from a pool of
instances for k nearest distances. Features are
separately weighted based on Quinlan&apos;s infor-
mation gain ratio (Quinlan, 1993), which mea-
sures the informativity of features for predicting
class labels. This can be computed by subtract-
ing the entropy of the knowledge of the feature
values from the general entropy of the class la-
bels. The first quantity is normalized with the a
priori probabilities of the various feature values
of feature F:
</bodyText>
<equation confidence="0.979257">
H(C) EvEV alues(F) P(v) x H(C[F=vi) (6)
Here, H(C) is the class entropy, defined as
H(C) — E P(c) log2 P(c). (7)
cECIass
</equation>
<bodyText confidence="0.999805166666667">
H(C[F=v]) is the class entropy computed over
the subset of instances that have v as value for
F. Normalization for features with many values
is obtained by dividing the information gain for
a feature by the entropy of its value set (called
the split info of feature F.
</bodyText>
<equation confidence="0.9981525">
P(v)X H(C[F=v])
Wi H (C)—Ev EV alues(Fi)
=
split—in f o(Fi)
split — in f o(Fi) = — E P(v) log2 P(v)
vEValues(F,)
</equation>
<bodyText confidence="0.953105">
(8)
IGTREE is a heuristic approximation of IB1-
IG which has comparable accuracy, but is op-
timized for speed. It is insensitive to k-values
larger than 1, and uses value-class cooccurrence
information when exact matches fail.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999942076923077">
The effects of a distributed class representa-
tion on generalization accuracy were measured
using an experimental matrix based on 5 lin-
guistic datasets, and 8 experimental condi-
tions, addressing feature selection-based ECOC
vs. voting-based ECOC, MVDM, values of
k larger than 1, and dichotomizer weight-
ing. The following linguistic tasks were used.
DIMIN is a Dutch diminutive formation task de-
rived from the Celex lexical database for Dutch
(Baayen et al., 1993). It predicts Dutch nomi-
nal diminutive suffixes from phonetic properties
(phonemes and stress markers) of maximally the
</bodyText>
<page confidence="0.992914">
57
</page>
<bodyText confidence="0.9999395">
last three syllables of the noun. The STRESS
task, also derived from the Dutch Celex lexi-
cal database, assigns primary stress on the ba-
sis of phonemic values. MORPH assigns mor-
phological boundaries (a.o. root morpheme,
stress-changing affix, inflectional morpheme),
based on English CELEX data. The WSJ-
NPVP task deals with NP-VP chunking of PoS-
tagged Wall Street Journal material. GRAPHON,
finally, is a grapheme-to-phoneme conversion
task for English based on the English Celex lex-
ical database. Numeric characteristics of the
different tasks are listed in table 1. All tasks
with the exception of GRAPHON happened to
be five-class problems; for GRAPHON, a five-
class subset was taken from the original training
set, in order to keep computational demands
manageable. The tasks were subjected to the
</bodyText>
<table confidence="0.9999025">
Data set Features Classes Instances
DIMIN 12 5 3,000
STRESS 12 5 3,000
MORPH 9 5 300,000
NPVP 8 5 200,000
GRAPHON 7 5 73,525
</table>
<tableCaption confidence="0.999857">
Table 1: Data sets.
</tableCaption>
<bodyText confidence="0.999891647058824">
8 different experimental situations of table 2.
For feature selection-based ECOC, backward se-
quential feature elimination was used (Raaij-
makers, 1999), repeatedly eliminating features
in turn and evaluating each elimination step
with 10-fold cross-validation. For dichotomizer
weighting, error information of the dichotomiz-
ers, determined from separate unweighted 10-
fold cross-validation experiments on a separate
training set, produced a weighted Hamming dis-
tance metric. Error-based weights were based
on raising a small constant # in the interval
[0, 1) to the power of the number of errors made
by the dichotomizer (Cesa-Bianchi et al., 1996).
Random feature selection drawing features with
replacement created feature sets of both differ-
ent size and composition for every dichotomizer.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.986342">
Table 3 lists the generalization accuracies for
the control groups, and table 4 for the ECOC
algorithms. All accuracy results are based on
10-fold cross-validation, with p &lt; 0.05 using
paired t-tests. The results show that dis-
</bodyText>
<table confidence="0.858974294117647">
ALGORITHM DESCRIPTION
El ECOC, feature selection per bit (15),
k=1, unweighted
E2 ECOC, feature selection per bit (15),
k=1, weighted
£3 ECOC, feature selection per bit (15),
MVDM, k=1, unweighted
£4 ECOC, feature selection per bit (15),
MVDM, k=1, weighted
E5 ECOC, feature selection per bit (15),
MVDM, k=3, unweighted
£6 ECOC, feature selection per bit (15),
MVDM, k=3, weighted
E7 ECOC, voting (100) per bit (30),
MVDM, k=3
£8 ECOC, voting (100) per bit block
(15), MVDM, k=3
</table>
<tableCaption confidence="0.954143">
Table 2: Algorithms
</tableCaption>
<table confidence="0.999574333333333">
GROUP I II III IV
IB1-IG IB1-IG IB1-IG IB1-IG
k=1 k=3 k=1 k=3
MVDM MVDM
DIMIN 98.1±0.5 95.8±0.5 97.7±0.7 98.1±0.5
STRESS 83.5±2.6 81.3±2.9 86.2±2.0 86.7±1.8
MORPH 92.5±1.4 92.0±1.4 92.5±1.4 92.5±1.4
NPVP 96.4±0.2 97.1±0.2 97.0±0.1 97.0±0.1
GRAPHON 97.1±2.4 97.2±2.3 97.7±0.7 97.7±0.8
</table>
<tableCaption confidence="0.999915">
Table 3: Generalization accuracies control groups.
</tableCaption>
<bodyText confidence="0.999857083333333">
tributed class representations can lead to sta-
tistically significant accuracy gains for a variety
of linguistic tasks. The ECOC algorithm based
on feature selection and weighted Hamming dis-
tance performs best. Voting-based ECOC per-
forms poorly on DIMIN and STRESS with vot-
ing per bit, but significant accuracy gains are
achieved by voting per block, putting it on a par
with the best performing algorithm. Regression
analysis was applied to investigate the effect of
the Modified Value Difference Metric on ECOC
accuracy. First, the accuracy gain of MVDM
as a function of the information gain ratio of
the features was computed. The results show a
high correlation (0.82, significant at p &lt; 0.05)
between these variables, indicating a linear re-
lation. This is in line with the idea underlying
MVDM: whenever two feature values are very
predictive of a shared class, they contribute to
the similarity between the instances they belong
to, which will lead to more accurate classifiers.
Next, regression analysis was applied to deter-
mine the effect of MVDM on ECOC, by relating
the accuracy gain of MVDM (k=3) compared to
</bodyText>
<page confidence="0.997796">
58
</page>
<table confidence="0.999454166666667">
TASK El (I) E2(I) £3 (III) £4 (III) E5 (IV) £6 (IV) E7 (IV) £8 (£6)
DIMIN 98.6±0.4,V 98.5±0.4-V 98.6±0.6 98.7±0.6.V 98.8±0.5 98.9±0.4.J 96.6±0.9x 98.4±0.4
STRESS 85.3±1.8V 86.3±2.0 88.2±1.7V 88.8±1.7-V 88.2±1.7.V 89.3±1.9,/ 86.5±2.3x 88.8±1.7
MORPH 93.2±1.6-V 93.2±1.5V 93.2±1.3V 93.2±1.3V 93.2±1.6V 93.2±1.5V 93.0±1.61/ 93.4±1.5V
NPVPi 96.8±0.1.V 96.9±0.2V 96.8±0.1 96.9±0.1 96.8±0.1 96.9±0.1 96.8±0.2x 96.8±0.2
GRAPHON 98.2±0.7 98.3±0.7 98.4±0.6V 98.3±0.5I 98.3±0.6V 98.5±0.5-V 97.6±0.7x 97.6±0.8x
</table>
<tableCaption confidence="0.996234">
Table 4: Generalization accuracies for feature selection-based ECOC (N/ indicates significant improvement over
control group (in round brackets) , and x deterioration at p &lt; 0.05 using paired t-tests). A t indicates 25 voters for
performance reasons.
</tableCaption>
<bodyText confidence="0.998346928571429">
control group II to the accuracy gain of ECOC
(algorithm E6, compared to control group IV).
The correlation between these two variables is
very high (0.93, significant at p &lt; 0.05), again
indicative of a linear relation. From the per-
spective of learning class boundaries, the strong
effect of MVDM on ECOC accuracy can be un-
derstood as follows. When the overlap metric is
used, members of a training set belonging to the
same class may be situated arbitrarily remote
from each other in the feature hyperspace. For
instance, consider the following two instances
taken from DIMIN:
) ) • „ „d,A,k,je
) ) „ ,d,A,x,je
(Hyphens indicate absence of feature values.)
These two instances encode the diminutive for-
mation of Dutch dakje (little roof) from dak
(roof), and dagje (lit, little day, proverbially
used) from dag (day). Here, the values k and x,
corresponding to the velar stop &apos;k&apos; and the ve-
lar fricative &apos;g&apos;, are minimally different from a
phonetic perspective. Yet, these two instances
have coordinates on the twelfth dimension of
the feature hyperspace that have nothing to do
with each other. The overlap treats the k-x
value clash just like any other value clash. This
phenomenon may lead to a situation where in-
habitants of the same class are scattered over
the feature hyperspace. In contrast, a value dif-
ference metric like MVDM which attempts to
group feature values on the basis of class cooc-
currence information, might group k and x to-
gether if they share enough classes. The effect
of MVDM on the density of the feature hyper-
space can be compared with the density ob-
tained with the overlap metric as follows. First,
plot a random numerical transform of a feature
space. For expository reasons, it is adequate
to restrict attention to a low-dimensional (e.g.
two-dimensional) subset of the feature space, for
a specific class C. Then, plot an MVDM trans-
form of this feature space, where every coordi-
nate (a,b) is transformed into (P(C I a), P(C I
b)). This idea is applied to a subset of DIMIN,
consisting of all instances classified as je (one
of the five diminutive suffixes for Dutch). The
features for this subset were limited to the last
two, consisting of the rhyme and coda of the
last syllable of the word, clearly the most infor-
mative features for this task. Figure 2 displays
the two scatter plots. As can be seen, instances
are widely scattered over the feature space for
the numerical transform, whereas the MVDM-
based transform forms many clusters and pro-
duces much higher density. In a condensed fea-
</bodyText>
<figure confidence="0.910606615384616">
1 I 1 t
0.9 *44 # # 41 -
20- 0.8
0.7
0.6
0.5
‘.2 0.4
0.3
1 0.2
0.1
0 I 1 • .1. I I
5 10 &apos;15 20 25 30 35 40 45 0 0.1 0.2 0.3 0.4 0.5 as 07
Feature 11 DIMIN (random) Feature 11 DIMIN (MVDM)
</figure>
<figureCaption confidence="0.99913425">
Figure 2: Random numerical transform of feature val-
ues based on the overlap metric (left) vs. numerical
transform of feature values based on MVDM (right), for
a two-features-one-class subset of DIMIN.
</figureCaption>
<bodyText confidence="0.999971333333333">
ture hyperspace the number of class boundaries
to be learned per bit function reduces. For in-
stance, figures 3 displays the class boundaries
for a relatively condensed feature hyperspace,
where classes form localized populations, and a
scattered feature hyperspace, with classes dis-
tributed over non-adjacent regions. The num-
ber of class boundaries in the scattered feature
space is much higher, and this will put an addi-
</bodyText>
<page confidence="0.996077">
59
</page>
<bodyText confidence="0.913365">
tional burden on the learning problems consti-
tuted by the various bit functions.
</bodyText>
<table confidence="0.996308111111111">
613 C3 b35 CS Cl 6121 C2 6241 C4
Cl 615 6151 11131 b23ii 6341
61211
bI2 i 623 b23i C3
6121i C3 635 ii
C2 6351
b24 CS
635 ii
Cl 6137i CS
61 5ii
Cl
612iii
Will
C4 613 ii 614
614 C3
b34ii
C4 b24ii C2
Fl Fl
</table>
<figureCaption confidence="0.9601465">
Figure 3: Condensed feature space (left) vs. scattered
feature space (right).
</figureCaption>
<sectionHeader confidence="0.998381" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999972777777778">
The use of error-correcting output codes
(ECOC) for representing natural language
classes has been empirically validated for a suite
of linguistic tasks. Results indicate that ECOC
can be useful for datasets with features with
high class predictivity. These sets typically tend
to benefit from the Modified Value Difference
Metric, which creates a condensed hyperspace
of features. This in turn leads to a lower num-
ber of class boundaries to be learned per bit
function, which simplifies the binary subclas-
sification tasks. A voting algorithm for learn-
ing blocks of bits proves as accurate as an ex-
pensive feature-selecting algorithm. Future re-
search will address further mechanisms of learn-
ing complex regions of the class boundary land-
scape, as well as alternative error-correcting ap-
proaches to classification.
</bodyText>
<sectionHeader confidence="0.997982" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999889166666667">
Thanks go to Francesco Ricci for assistance in
generating the error-correcting codes used in
this paper. David Aha and the members of the
Induction of Linguistic Knowledge (ILK) Group
of Tilburg University and Antwerp University
are thanked for helpful comments and criticism.
</bodyText>
<sectionHeader confidence="0.999524" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998942534482759">
H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX database on CD-ROM. Linguistic
Data Consortium. Philadelpha, PA.
S. Bay. 1999. Nearest neighbor classification from
multiple feature subsets. Intelligent Data Analy-
sis, 3(3):191-209.
A. Berger. 1999. Error-correcting output coding
for text classification. Proceedings of IJCAI&apos;99:
Workshop on machine learning for information
filtering.
C. Cardie, S. Mardis, and D. Pierce. 1999. Com-
bining error-driven pruning and classification for
partial parsing. Proceedings of the Sixteenth In-
ternational Conference on Machine Learning, pp.
87-96.
N. Cesa-Bianchi, Y. Freund, D. Helmbold, and
M. Warmuth. 1996. On-line prediction and con-
version strategies. Machine Learning 27:71-110.
S. Cost and S. Salzberg. 1993. A weighted near-
est neighbor algorithm for learning with symbolic
features. Machine Learning,10:57-78.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996. Mbt: A memory-based part of speech tag-
ger generator. Proceedings of the Fourth Work-
shop on Very Large Corpora, ACL SIGDAT.
W. Daelemans, J. Zavrel, K. Van der Sloot, and
A. Van den Bosch. 1999. Timbl: Tilburg memory
based learner, version 2.0, reference guide. ILK
Technical Report - ILK 99-01. Tilburg.
T. Dietterich and G. Bakiri. 1995. Solving multi-
class learning problems via error-correcting out-
put codes. Journal of Artificial Intelligence Re-
search, 2:263-286.
R. Duda and P. Hart. 1973. Pattern classification
and scene analysis. Wiley Press.
E. Kong and T. Dietterich. 1995. Error-correcting
output coding corrects bias and variance. Pro-
ceedings of the 12th International Conference on
Machine Learning.
J.R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, Ca.
S. Raaijmakers. 1999. Finding representations for
memory-based language learning. Proceedings of
CoNLL-1999.
F. Ricci and D. Aha. 1997. Extending local learners
with error-correcting output codes. Proceedings of
the 1.4th Conference on Machine Learning.
D. Roth. 1998. A learning approach to shallow pars-
ing. Proceedings EMNLP-WVLC&apos;99.
H. Schmid. 1994. Part-of-speech tagging with neu-
ral networks. Proceedings COLING-94.
C. Shannon. 1948. A mathematical theory of com-
munication. Bell System Technical Journal,27:7,
pp. 379-423, 27:10, pp. 623-656.
A. van den Bosch and W. Daelemans. 1993. Data-
oriented methods for grapheme-to-phoneme con-
version. Proceedings of the 6th Conference of the
EA CL.
</reference>
<page confidence="0.998195">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.317756">
<note confidence="0.967581">of CoNLL-2000 and LLL-2000, 55-60, Lisbon, Portugal, 2000.</note>
<title confidence="0.999974">Learning Distributed Linguistic Classes</title>
<author confidence="0.995849">Stephan</author>
<affiliation confidence="0.873896">Netherlands Organisation for Applied Scientific Research Institute for Applied The</affiliation>
<address confidence="0.361357">raaijmakers@tpdAno.n1</address>
<abstract confidence="0.99719775">Error-correcting output codes (ECOC) have emerged in machine learning as a successful implementation of the idea of distributed classes. Monadic class symbols are replaced by bit strings, which are learned by an ensemble of binary-valued classifiers (dichotomizers). In this study, the idea of ECOC is applied to language learning with local (knearest neighbor) classifiers. Regression analysis of the experimental results reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX database on CD-ROM. Linguistic Data Consortium.</booktitle>
<location>Philadelpha, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. The CELEX database on CD-ROM. Linguistic Data Consortium. Philadelpha, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bay</author>
</authors>
<title>Nearest neighbor classification from multiple feature subsets. Intelligent Data Analysis,</title>
<date>1999</date>
<pages>3--3</pages>
<contexts>
<context position="6639" citStr="Bay, 1999" startWordPosition="1113" endWordPosition="1114">r, since classifiers that use global information such as class probabilities during classification, are much less vulnerable to correlated predictions. The following ideas were tested empirically on a suite of natural language learning tasks. • A careful feature selection approach, where every dichotomizer is trained to select (possibly) different features. • A careless feature selection approach, where every bit is predicted by a voting committee of dichotomizers, each of which randomly selects features (akin in spirit to the Multiple Feature Subsets approach for non-distributed classifiers (Bay, 1999). • A careless feature selection approach, where blocks of two adjacent bits are predicted by a voting committee of quadrotomizers, each of which randomly selects features. Learning blocks of two bits allows for bit codes that are twice as long (larger error-correction), but with half as many classifiers. Assuming a normal distribution of errors and bit values in every 2 bits-block, there is a 25% chance that both bits in a 2-bit block are wrong. The other 75% chance of one bit wrong would produce performance equal to voting per bit. Formally, this implies a switch from N twoclass problems to </context>
</contexts>
<marker>Bay, 1999</marker>
<rawString>S. Bay. 1999. Nearest neighbor classification from multiple feature subsets. Intelligent Data Analysis, 3(3):191-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Berger</author>
</authors>
<title>Error-correcting output coding for text classification.</title>
<date>1999</date>
<booktitle>Proceedings of IJCAI&apos;99: Workshop on machine learning for information filtering.</booktitle>
<contexts>
<context position="1547" citStr="Berger, 1999" startWordPosition="219" endWordPosition="220">ss hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classification, the bit predictions of the various dichotomizers are combined to produce a codeword prediction. The class codeword which has minimal Hamming distance to the predicted codeword determines the classification of the instance. Codewords are constructed such that their Hamming distance is maximal. Extra bits are added to allow for error recovery, al</context>
</contexts>
<marker>Berger, 1999</marker>
<rawString>A. Berger. 1999. Error-correcting output coding for text classification. Proceedings of IJCAI&apos;99: Workshop on machine learning for information filtering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
<author>S Mardis</author>
<author>D Pierce</author>
</authors>
<title>Combining error-driven pruning and classification for partial parsing.</title>
<date>1999</date>
<booktitle>Proceedings of the Sixteenth International Conference on Machine Learning,</booktitle>
<pages>87--96</pages>
<contexts>
<context position="1219" citStr="Cardie et al., 1999" startWordPosition="170" endWordPosition="173">memory-based language learning with local (knearest neighbor) classifiers. Regression analysis of the experimental results reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classification, the bit prediction</context>
<context position="9414" citStr="Cardie et al., 1999" startWordPosition="1569" endWordPosition="1572">re vector /.) Another distance metric is the Modified Value Difference Metric (MVDM) (Cost and Salzberg, 1993). The MVDM defines similarity between two feature values in terms of posterior probabilities: P(c — P(c vi) (4) cECIasses When two values share more classes, they are more similar, as 6 decreases. Memory-based learning has fruitfully been applied to natural language processing, yielding state-of-theart performance on all levels of linguistic analysis, including grapheme-to-phoneme conversion (van den Bosch and Daelemans, 1993), PoStagging (Daelemans et al., 1996), and shallow parsing (Cardie et al., 1999). In this study, the following memory-based models are used, all available from the TIMBL package (Daelemans et al., 1999). IB1-IG is a k-nearest distance classifier which employs a weighted overlap metric: (I, I) = wkork (h), 7rk(ii)) (5) In stead of drawing winners from the k-nearest neighbors pool, IB1-IG selects from a pool of instances for k nearest distances. Features are separately weighted based on Quinlan&apos;s information gain ratio (Quinlan, 1993), which measures the informativity of features for predicting class labels. This can be computed by subtracting the entropy of the knowledge o</context>
</contexts>
<marker>Cardie, Mardis, Pierce, 1999</marker>
<rawString>C. Cardie, S. Mardis, and D. Pierce. 1999. Combining error-driven pruning and classification for partial parsing. Proceedings of the Sixteenth International Conference on Machine Learning, pp. 87-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cesa-Bianchi</author>
<author>Y Freund</author>
<author>D Helmbold</author>
<author>M Warmuth</author>
</authors>
<title>On-line prediction and conversion strategies.</title>
<date>1996</date>
<journal>Machine Learning</journal>
<pages>27--71</pages>
<contexts>
<context position="13112" citStr="Cesa-Bianchi et al., 1996" startWordPosition="2169" endWordPosition="2172">al situations of table 2. For feature selection-based ECOC, backward sequential feature elimination was used (Raaijmakers, 1999), repeatedly eliminating features in turn and evaluating each elimination step with 10-fold cross-validation. For dichotomizer weighting, error information of the dichotomizers, determined from separate unweighted 10- fold cross-validation experiments on a separate training set, produced a weighted Hamming distance metric. Error-based weights were based on raising a small constant # in the interval [0, 1) to the power of the number of errors made by the dichotomizer (Cesa-Bianchi et al., 1996). Random feature selection drawing features with replacement created feature sets of both different size and composition for every dichotomizer. 5 Results Table 3 lists the generalization accuracies for the control groups, and table 4 for the ECOC algorithms. All accuracy results are based on 10-fold cross-validation, with p &lt; 0.05 using paired t-tests. The results show that disALGORITHM DESCRIPTION El ECOC, feature selection per bit (15), k=1, unweighted E2 ECOC, feature selection per bit (15), k=1, weighted £3 ECOC, feature selection per bit (15), MVDM, k=1, unweighted £4 ECOC, feature selec</context>
</contexts>
<marker>Cesa-Bianchi, Freund, Helmbold, Warmuth, 1996</marker>
<rawString>N. Cesa-Bianchi, Y. Freund, D. Helmbold, and M. Warmuth. 1996. On-line prediction and conversion strategies. Machine Learning 27:71-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cost</author>
<author>S Salzberg</author>
</authors>
<title>A weighted nearest neighbor algorithm for learning with symbolic features.</title>
<date>1993</date>
<booktitle>Machine Learning,10:57-78.</booktitle>
<contexts>
<context position="8904" citStr="Cost and Salzberg, 1993" startWordPosition="1492" endWordPosition="1495">odels based on this approach to analogy are called nearest neighbor models (Duda and Hart, 1973). Socalled k-nearest neighbor models select a winner from the k nearest neighbors, where k is a parameter and winner selection is usually based on class frequency. Resemblance between instances is measured using distance metrics, which come in many sorts. The simplest distance metric is the overlap metric: = E 6(7k(Ii),71-k(I3)) 6(vi,v3) = 0 if vi = v3 (3) vi) = 1 if vi vi (ri(I) is the i-th projection of the feature vector /.) Another distance metric is the Modified Value Difference Metric (MVDM) (Cost and Salzberg, 1993). The MVDM defines similarity between two feature values in terms of posterior probabilities: P(c — P(c vi) (4) cECIasses When two values share more classes, they are more similar, as 6 decreases. Memory-based learning has fruitfully been applied to natural language processing, yielding state-of-theart performance on all levels of linguistic analysis, including grapheme-to-phoneme conversion (van den Bosch and Daelemans, 1993), PoStagging (Daelemans et al., 1996), and shallow parsing (Cardie et al., 1999). In this study, the following memory-based models are used, all available from the TIMBL </context>
</contexts>
<marker>Cost, Salzberg, 1993</marker>
<rawString>S. Cost and S. Salzberg. 1993. A weighted nearest neighbor algorithm for learning with symbolic features. Machine Learning,10:57-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>Mbt: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>Proceedings of the Fourth Workshop on Very Large Corpora, ACL SIGDAT.</booktitle>
<contexts>
<context position="1198" citStr="Daelemans et al., 1996" startWordPosition="166" endWordPosition="169">a of ECOC is applied to memory-based language learning with local (knearest neighbor) classifiers. Regression analysis of the experimental results reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classificatio</context>
<context position="9371" citStr="Daelemans et al., 1996" startWordPosition="1562" endWordPosition="1565"> vi (ri(I) is the i-th projection of the feature vector /.) Another distance metric is the Modified Value Difference Metric (MVDM) (Cost and Salzberg, 1993). The MVDM defines similarity between two feature values in terms of posterior probabilities: P(c — P(c vi) (4) cECIasses When two values share more classes, they are more similar, as 6 decreases. Memory-based learning has fruitfully been applied to natural language processing, yielding state-of-theart performance on all levels of linguistic analysis, including grapheme-to-phoneme conversion (van den Bosch and Daelemans, 1993), PoStagging (Daelemans et al., 1996), and shallow parsing (Cardie et al., 1999). In this study, the following memory-based models are used, all available from the TIMBL package (Daelemans et al., 1999). IB1-IG is a k-nearest distance classifier which employs a weighted overlap metric: (I, I) = wkork (h), 7rk(ii)) (5) In stead of drawing winners from the k-nearest neighbors pool, IB1-IG selects from a pool of instances for k nearest distances. Features are separately weighted based on Quinlan&apos;s information gain ratio (Quinlan, 1993), which measures the informativity of features for predicting class labels. This can be computed by</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. Mbt: A memory-based part of speech tagger generator. Proceedings of the Fourth Workshop on Very Large Corpora, ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K Van der Sloot</author>
<author>A Van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 2.0, reference guide.</title>
<date>1999</date>
<tech>ILK Technical Report - ILK 99-01. Tilburg.</tech>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 1999</marker>
<rawString>W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den Bosch. 1999. Timbl: Tilburg memory based learner, version 2.0, reference guide. ILK Technical Report - ILK 99-01. Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
<author>G Bakiri</author>
</authors>
<title>Solving multiclass learning problems via error-correcting output codes.</title>
<date>1995</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>2--263</pages>
<contexts>
<context position="1511" citStr="Dietterich and Bakiri, 1995" startWordPosition="211" endWordPosition="214">xplained in terms of population density of the class hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classification, the bit predictions of the various dichotomizers are combined to produce a codeword prediction. The class codeword which has minimal Hamming distance to the predicted codeword determines the classification of the instance. Codewords are constructed such that their Hamming distance is maximal. Extra bits are a</context>
</contexts>
<marker>Dietterich, Bakiri, 1995</marker>
<rawString>T. Dietterich and G. Bakiri. 1995. Solving multiclass learning problems via error-correcting output codes. Journal of Artificial Intelligence Research, 2:263-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
</authors>
<title>Pattern classification and scene analysis.</title>
<date>1973</date>
<publisher>Wiley Press.</publisher>
<contexts>
<context position="8376" citStr="Duda and Hart, 1973" startWordPosition="1400" endWordPosition="1403">ed with classified data stored in a knowledge base. This latter data set is called the training data, and its elements are called instances. Every instance consists of a feature-value vector and a class label. Learning under the memory-based paradigm is lazy, and consists only of storing the training instances in a suitable data structure. The instance from the training set which resembles the most the item to be classified determines the classification of the latter. This instance is called the nearest neighbor, and models based on this approach to analogy are called nearest neighbor models (Duda and Hart, 1973). Socalled k-nearest neighbor models select a winner from the k nearest neighbors, where k is a parameter and winner selection is usually based on class frequency. Resemblance between instances is measured using distance metrics, which come in many sorts. The simplest distance metric is the overlap metric: = E 6(7k(Ii),71-k(I3)) 6(vi,v3) = 0 if vi = v3 (3) vi) = 1 if vi vi (ri(I) is the i-th projection of the feature vector /.) Another distance metric is the Modified Value Difference Metric (MVDM) (Cost and Salzberg, 1993). The MVDM defines similarity between two feature values in terms of pos</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. Duda and P. Hart. 1973. Pattern classification and scene analysis. Wiley Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kong</author>
<author>T Dietterich</author>
</authors>
<title>Error-correcting output coding corrects bias and variance.</title>
<date>1995</date>
<booktitle>Proceedings of the 12th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="5003" citStr="Kong and Dietterich, 1995" startWordPosition="851" endWordPosition="855">e the point.) fi f2 13 Cl 0 0 1 C2 0 1 0 C3 1 0 0 For every combination of classes (C1—C2, Cl— C3, C2—C3), the Hamming distance between the codewords is 2. These horizontal relations have vertical repercussions as well: for every such pair, two bit functions disagree in the classes they select. For C1—C2, 12 selects C2 and h selects Cl. For C1—C3, fi selects C3 and h selects Cl. Finally, for C2—C3, fi selects C3 and f2 selects C2. So, every class is selected two times, and this implies that every class boundary associated with that class in the feature hyperspace is learned twice. In general (Kong and Dietterich, 1995), if the minimal Hamming distance between the codewords of an (error-correcting) code is d, then every class boundary is learned d times. For the error-correcting code from above this implies an error correction of zero: only two votes support a class boundary, and no vote can be favored in case of a conflict. The decoding of the predicted bit string to a class symbol appears to be a form of voting over class boundaries (Kong and Dietterich, 1995), and is able to reduce both bias and variance of the classifier. 2 Dichotomizer Ensembles Dichotomizer ensembles must be diverse apart from accurate</context>
</contexts>
<marker>Kong, Dietterich, 1995</marker>
<rawString>E. Kong and T. Dietterich. 1995. Error-correcting output coding corrects bias and variance. Proceedings of the 12th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, Ca.</location>
<contexts>
<context position="9872" citStr="Quinlan, 1993" startWordPosition="1646" endWordPosition="1647">including grapheme-to-phoneme conversion (van den Bosch and Daelemans, 1993), PoStagging (Daelemans et al., 1996), and shallow parsing (Cardie et al., 1999). In this study, the following memory-based models are used, all available from the TIMBL package (Daelemans et al., 1999). IB1-IG is a k-nearest distance classifier which employs a weighted overlap metric: (I, I) = wkork (h), 7rk(ii)) (5) In stead of drawing winners from the k-nearest neighbors pool, IB1-IG selects from a pool of instances for k nearest distances. Features are separately weighted based on Quinlan&apos;s information gain ratio (Quinlan, 1993), which measures the informativity of features for predicting class labels. This can be computed by subtracting the entropy of the knowledge of the feature values from the general entropy of the class labels. The first quantity is normalized with the a priori probabilities of the various feature values of feature F: H(C) EvEV alues(F) P(v) x H(C[F=vi) (6) Here, H(C) is the class entropy, defined as H(C) — E P(c) log2 P(c). (7) cECIass H(C[F=v]) is the class entropy computed over the subset of instances that have v as value for F. Normalization for features with many values is obtained by divid</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J.R. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, Ca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raaijmakers</author>
</authors>
<title>Finding representations for memory-based language learning.</title>
<date>1999</date>
<booktitle>Proceedings of CoNLL-1999.</booktitle>
<contexts>
<context position="12614" citStr="Raaijmakers, 1999" startWordPosition="2097" endWordPosition="2099">x lexical database. Numeric characteristics of the different tasks are listed in table 1. All tasks with the exception of GRAPHON happened to be five-class problems; for GRAPHON, a fiveclass subset was taken from the original training set, in order to keep computational demands manageable. The tasks were subjected to the Data set Features Classes Instances DIMIN 12 5 3,000 STRESS 12 5 3,000 MORPH 9 5 300,000 NPVP 8 5 200,000 GRAPHON 7 5 73,525 Table 1: Data sets. 8 different experimental situations of table 2. For feature selection-based ECOC, backward sequential feature elimination was used (Raaijmakers, 1999), repeatedly eliminating features in turn and evaluating each elimination step with 10-fold cross-validation. For dichotomizer weighting, error information of the dichotomizers, determined from separate unweighted 10- fold cross-validation experiments on a separate training set, produced a weighted Hamming distance metric. Error-based weights were based on raising a small constant # in the interval [0, 1) to the power of the number of errors made by the dichotomizer (Cesa-Bianchi et al., 1996). Random feature selection drawing features with replacement created feature sets of both different si</context>
</contexts>
<marker>Raaijmakers, 1999</marker>
<rawString>S. Raaijmakers. 1999. Finding representations for memory-based language learning. Proceedings of CoNLL-1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ricci</author>
<author>D Aha</author>
</authors>
<title>Extending local learners with error-correcting output codes.</title>
<date>1997</date>
<booktitle>Proceedings of the 1.4th Conference on Machine Learning.</booktitle>
<contexts>
<context position="1532" citStr="Ricci and Aha, 1997" startWordPosition="215" endWordPosition="218">on density of the class hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classification, the bit predictions of the various dichotomizers are combined to produce a codeword prediction. The class codeword which has minimal Hamming distance to the predicted codeword determines the classification of the instance. Codewords are constructed such that their Hamming distance is maximal. Extra bits are added to allow for err</context>
<context position="5956" citStr="Ricci and Aha, 1997" startWordPosition="1011" endWordPosition="1014">redicted bit string to a class symbol appears to be a form of voting over class boundaries (Kong and Dietterich, 1995), and is able to reduce both bias and variance of the classifier. 2 Dichotomizer Ensembles Dichotomizer ensembles must be diverse apart from accurate. Diversity is necessary in order to decorrelate the predictions of the various dichotomizers. This is a consequence of the voting mechanism underlying ECOC, where bit functions can only outvote other bit functions if they do not make similar predictions. Selecting different features per dichotomizer was proposed for this purpose (Ricci and Aha, 1997). Another possibility is to add limited non-locality to a local classifier, since classifiers that use global information such as class probabilities during classification, are much less vulnerable to correlated predictions. The following ideas were tested empirically on a suite of natural language learning tasks. • A careful feature selection approach, where every dichotomizer is trained to select (possibly) different features. • A careless feature selection approach, where every bit is predicted by a voting committee of dichotomizers, each of which randomly selects features (akin in spirit t</context>
</contexts>
<marker>Ricci, Aha, 1997</marker>
<rawString>F. Ricci and D. Aha. 1997. Extending local learners with error-correcting output codes. Proceedings of the 1.4th Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>A learning approach to shallow parsing.</title>
<date>1998</date>
<booktitle>Proceedings EMNLP-WVLC&apos;99.</booktitle>
<contexts>
<context position="1232" citStr="Roth, 1998" startWordPosition="174" endWordPosition="175"> learning with local (knearest neighbor) classifiers. Regression analysis of the experimental results reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classification, the bit predictions of the vari</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. A learning approach to shallow parsing. Proceedings EMNLP-WVLC&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Part-of-speech tagging with neural networks.</title>
<date>1994</date>
<booktitle>Proceedings COLING-94.</booktitle>
<contexts>
<context position="1333" citStr="Schmid, 1994" startWordPosition="188" endWordPosition="189">ts reveals that, in order for ECOC to be successful for language learning, the use of the Modified Value Difference Metric (MVDM) is an important factor, which is explained in terms of population density of the class hyperspace. 1 Introduction Supervised learning methods applied to natural language classification tasks commonly operate on high-level symbolic representations, with linguistic classes that are usually monadic, without internal structure (Daelemans et al., 1996; Cardie et al., 1999; Roth, 1998). This contrasts with the distributed class encoding commonly found in neural networks (Schmid, 1994). Error-correcting output codes (ECOC) have been introduced to machine learning as a principled and successful approach to distributed class encoding (Dietterich and Bakiri, 1995; Ricci and Aha, 1997; Berger, 1999). With ECOC, monadic classes are replaced by codewords, i.e. binary-valued vectors. An ensemble of separate classifiers (dichotomizers) must be trained to learn the binary subclassifications for every instance in the training set. During classification, the bit predictions of the various dichotomizers are combined to produce a codeword prediction. The class codeword which has minimal</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Part-of-speech tagging with neural networks. Proceedings COLING-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<booktitle>Bell System Technical Journal,27:7,</booktitle>
<pages>379--423</pages>
<contexts>
<context position="3214" citStr="Shannon, 1948" startWordPosition="541" endWordPosition="542">rd is d, then the code has an errorcorrecting capability of LY J. Figure 1 shows the 5 x 15 ECOC matrix, for a 5-class problem. In this code, every codeword has a Hamming distance of at least 8 to the other codewords, so this code has an error-correcting capability of 3 bits. ECOC have two natural interpreta1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 [0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 1 Figure 1: ECOC for a five-class problem. tions. From an information-theoretic perspective, classification with ECOC is like channel coding (Shannon, 1948): the class of a pattern to be classified is a datum sent over a noisy communication channel. The communication channel consists of the trained classifier. The noise consists of the bias (systematic error) and variance (training set-dependent error) of the classifier, which together make up for the overall error 55 of the classifier. The received message must be decoded before it can be interpreted as a classification. Adding redundancy to a signal before transmission is a well-known technique in digital communication to allow for the recovery of errors due to noise in the channel, and this is</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>C. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal,27:7, pp. 379-423, 27:10, pp. 623-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A van den Bosch</author>
<author>W Daelemans</author>
</authors>
<title>Dataoriented methods for grapheme-to-phoneme conversion.</title>
<date>1993</date>
<booktitle>Proceedings of the 6th Conference of the EA CL.</booktitle>
<marker>van den Bosch, Daelemans, 1993</marker>
<rawString>A. van den Bosch and W. Daelemans. 1993. Dataoriented methods for grapheme-to-phoneme conversion. Proceedings of the 6th Conference of the EA CL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>