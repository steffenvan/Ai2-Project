<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000238">
<title confidence="0.991365">
Person Name Disambiguation based on Topic Model
</title>
<author confidence="0.999554">
Jiashen Sun, Tianmin Wang and Li Li
</author>
<affiliation confidence="0.9995465">
Center of Intelligence Science and Technology
Beijing University of Posts and Telecommunications
</affiliation>
<email confidence="0.983063">
b.bigart911@gmail.com,
tianmin180@sina.com, wbg111@126.com
</email>
<author confidence="0.996511">
Xing Wu
</author>
<affiliation confidence="0.829018666666667">
School of Computer
Beijing University of Posts and
Telecommunications
</affiliation>
<email confidence="0.997133">
wuxing-6@163.com
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999038764705882">
In this paper we describe our
participation in the SIGHAN 2010 Task-
3 (Person Name Disambiguation) and
detail our approaches. Person Name
Disambiguation is typically viewed as an
unsupervised clustering problem where
the aim is to partition a name’s contexts
into different clusters, each representing
a real world people. The key point of
Clustering is the similarity measure of
context, which depends upon the features
selection and representation. Two
clustering algorithms, HAC and
DBSCAN, are investigated in our system.
The experiments show that the topic
features learned by LDA outperforms
token features and more robust.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999876314285714">
Most current web searches relate to person
names. A study of the query log of the
AllTheWeb and Altavista search sites gives an
idea of the relevance of the people search task:
11-17% of the queries were composed of a
person name with additional terms and 4% were
identified simply as person names (Spink et al.,
2004).
However, there is a high level of ambiguity
where multiple individuals share the same name
and thus the harvesting and the retrieval of
relevant information becomes more difficult.
This ambiguity has recently become an active
research topic and, simultaneously, a relevant
application domain for Web search services.
Zoominfo.com, Spock.com and 123people.com
are examples of sites which perform web people
search, although with limited disambiguation
capabilities (Artiles et al., 2009).
This issue directed current researchers
towards the definition of a new task called Web
People Search (WePS) or Personal Name
Disambiguation (PND). The key assumption
underlying the task is that the context
surrounding an ambiguous person name is
indicative of its ascription. The goal of the
clustering task was to group web pages
containing the target person&apos;s name, so that
pages referring to the same individual are
assigned to the same cluster. For this purpose a
large dataset was collected and manually
annotated.
Moreover, because of the ambiguity in word
segmentation in Chinese, person name detection
is necessary, which is subtask of Named Entity
Recognition (NER). NER is one of difficulties
of the study of natural language processing, of
which the main task is to identify person names,
place names, organization names, number, time
words, money and other entities. The main
difficulties of Chinese person name entity
recognition are embodied in the following points:
1) the diversity of names form; 2) the Chinese
character within names form words with each; 3)
names and their context form words; 4)
translation of foreign names require special
considerations.
In this paper we describe our system and
approach in the SIGHAN 2010 task-3 (Person
Name Disambiguation). A novel Bayesian
approach is adopt in our system, which
formalizes the disambiguation problem in a
generative model. For each ambiguous name we
first draw a distribution over person, and then
generate context words according to this
distribution. It is thus assumed that different
persons will correspond to distinct lexical
distributions. In this framework, Person Name
Disambiguation postulates that the observed data
(contexts) are explicitly intended to
communicate a latent topic distribution
corresponding to real world people.
The remainder of this paper is structured as
follows. We first present an overview of related
work (Section 2) and then describe our system
which consists of NER and clustering in more
details (Sections 3 and 4). Section 5 describes
the resources and evaluation results in our
experiments. We discuss our results and
conclude our work in Section 6.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999967402985074">
The most commonly used feature is the bag of
words in local or global context of the
ambiguous name (Ikeda et al., 2009; Romano et
al., 2009). Because the given corpus is often not
large enough to learn the realistic probabilities
or weights for those features, traditional
algorithm such as vector-based techniques used
in large-scale text will lead to data sparseness.
In recent years, more and more important
studies have attempted to overcome the problem
to get a better (semantic) similarity measures. A
lot of features such as syntactic chunks, named
entities, dependency parses, semantic role labels,
etc., were employed. However, these features
need many NLP preprocessing (Chen, 2009).
Many studies show that they can achieve state-
of-the-art performances only with lightweight
features. Pedersen et al. (2005) present
SenseClusters which represents the instances to
be clustered using second order co–occurrence
vectors. Kozareva (2008) focuses on the
resolution of the web people search problem
through the integration of domain information,
which can represent relationship between
contexts and is learned from WordNet. PoBOC
clustering (Cleuziou et al., 2004) is used which
builds a weighted graph with weights being the
similarity among the objects.
Another way is to utilize universal data
repositories as external knowledge sources (Rao
et al., 2007; Kalmar and Blume, 2007; Pedersen
and Kulkarni; 2007) in order to give more
realistic frequency for a proper name or measure
whether a bigram is a collocation.
Phan et al. (2008) presents a general
framework for building classifiers that deal with
short and sparse text and Web segments by
making the most of hidden topics discovered
from large-scale data collections. Samuel Brody
et al. (2009) adopt a novel Bayesian approach
and formalize the word sense induction problem
in a generative model.
Previous work using the WePS1 (Artiles et al.,
2007) or WePS2 data set (Artiles et al., 2009)
shows that standard document clustering
methods can deliver excellent performance if
similarity measure is enough good to represent
relationship of context.
The study in Chinese PND is still in its
infancy. Person Name detection is often
necessary in Chinese. At present, the main
technology of person name recognition is used
statistical models, and the hybrid approach. Liu
et al. (2000) designed a Chinese person name
recognition system based on statistical methods,
using samples of names from the text corpus and
the real amount of statistical data to improve the
system performance, while the shortcoming is
that samples of name database are too small,
resulting in low recall. Li et al. (2006) use the
combination of the boundary templates and local
statistics to recognize Chinese person name, the
recognition process is to use the boundary with
the frequency of template to identify potential
names, and to recognize the results spread to the
entire article in order to recall missing names
caused by sparse data.
</bodyText>
<sectionHeader confidence="0.963495" genericHeader="method">
3 Person Name Recognition
</sectionHeader>
<bodyText confidence="0.999610928571429">
In this section, we focus on Conditional Random
Fields (CRFs) algorithm to establish the
appropriate language model. Given of the input
text, we may detect the potential person names
in the text fragments, and then take various
features into account to recognize of Chinese
person names.
Conditional Random Fields as a sequence
learning method has been successfully applied in
many NLP tasks. More details of the its
principle can be referred in (Lafferty, McCallum,
and Pereira, 2001; Wallach, 2004). We here will
focus on how to apply CRFs in our person name
recognition task.
</bodyText>
<subsectionHeader confidence="0.998763">
3.1 CRFs-based name recognition
</subsectionHeader>
<bodyText confidence="0.999961153846154">
CRFs is used to get potential names as the first
stage name recognition outcome. To avoid the
interference that caused by word segmentation
errors, we use single Chinese character
information rather than word as discriminative
features for CRFs learning model.
We use BIEO label strategy to transfer the name
recognition as a sequence learning task. The
label set includes: B-Nr (Begin, the initial
character of name), I-Nr (In, the middle
character of name), E-Nr(End, the end character
of name) and O (Other, other characters that
aren’t name).
</bodyText>
<subsectionHeader confidence="0.994438">
3.2 Rule-based Correction
</subsectionHeader>
<bodyText confidence="0.999971407407407">
After labeling the potential names by CRFs
model, we apply a set of rules to boost
recognition result, which has been proved to be
the key to improve Chinese name recognition.
The error of the potential names outcome by
CRFs model is mainly divided into the following
categories: the initial character of name is not
recognized, the middle character of name is not
recognized, the end character of name is not
recognized, and their combinations of those
three errors. The other two extreme errors,
including non-name recognition for the anchor
name, and the name is not recognized as
potential names.
In the stage of rule-based correction, we first
conduct word segmentation for the text. The
segmentation process is also realized with the
method of CRFs, without using dictionaries and
other external knowledge. The detailed
description is beyond this paper, which can be
accessible in the paper (Lafferty, McCallum, and
Pereira, 2001). The only thing we should note is
that part of the error in such segmentation result
obtained in this way can be corrected through
the introduction of an external dictionary.
For each potential name, and we examine it
from the following two aspects:
</bodyText>
<listItem confidence="0.943719333333333">
1) It is reasonable to use the word in a person
name, including checking the surname and the
character used in names;
2) The left and right borders are correct.
Check the left and right sides of the cutting unit
can be added to the names, including the words
used before names, the words used behind
names and the surname and character used in
names.
</listItem>
<sectionHeader confidence="0.934757" genericHeader="method">
4 Clustering
</sectionHeader>
<subsectionHeader confidence="0.849609">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.998538">
The clustering features we used can be divided
into two types, one is token features, including
word (after stop-word removal), uni-character
and bi-character, the other is topic features,
which is topic-based distribution of global or
window context learned by LDA (Latent
Dirichlet Allocation) model.
</bodyText>
<subsubsectionHeader confidence="0.665195">
4.1.1 Token-based Features
</subsubsectionHeader>
<bodyText confidence="0.997663857142857">
Simple token-based features are used in almost
every disambiguation system. Here, we extract
three kinds of tokens: words, uni-char and bi-
char occurring in a given document.
Then, each token in each feature vector is
weighed by using a tf-idf weighting and entropy
weighting schemes defined as follows.
</bodyText>
<equation confidence="0.938497714285714">
tf-idf weighting:
N
• log( )
ni
entropy weighting:
§ aik log(fk + 1.0)• 1 + 1 N fu log(fj) »
¨ log(N) j 1¬ni ni 1/4 1
</equation>
<bodyText confidence="0.971215">
where is the frequency of term i in
document k, N is the number of document in
corpus, is the frequency of term i in corpus.
So,
</bodyText>
<equation confidence="0.994629">
N
1 ªf f
ij
¦ « log( )
ij
log(N) j 1¬ni ni
</equation>
<bodyText confidence="0.99945475">
is the average uncertainty or entropy of term i.
Entropy weighting is based on information
theoretic ideas and is the most sophisticated
weighting scheme.
</bodyText>
<subsubsectionHeader confidence="0.913299">
4.1.2 Features Selection
</subsubsectionHeader>
<bodyText confidence="0.9976895">
In this Section, we give a brief introduction on
two effective unsupervised feature selection
methods, DF and global tf-idf.
DF (Document frequency) is the number of
documents in which a term occurs in a dataset. It
is the simplest criterion for term selection and
easily scales to a large dataset with linear
computation complexity. It is a simple but
effective feature selection method for text
categorization (Yang &amp; Pedersen, 1997).
</bodyText>
<figure confidence="0.936488">
a f
ik ik
º
»
1/4
</figure>
<bodyText confidence="0.999398444444445">
We introduce a new feature selection method
called “global tf-idf” that takes the term weight
into account. Because DF assumes that each
term is of same importance in different
documents, it is easily biased by those common
terms which have high document frequency but
uniform distribution over different classes.
Global tf-idf is proposed to deal with this
problem:
</bodyText>
<equation confidence="0.9838625">
N
g tfidf
i ¦ ik
k 1
</equation>
<subsectionHeader confidence="0.606657">
4.1.3 Latent Dirichlet Allocation (LDA)
</subsectionHeader>
<bodyText confidence="0.999948066666667">
Our work is related to Latent Dirichlet
Allocation (LDA, Blei et al. 2003), a
probabilistic generative model of text generation.
LDA models each document using a mixture
over K topics, which are in turn characterized as
distributions over words. The main motivation is
that the task, fail to achieve high accuracy due to
the data sparseness.
LDA is a generative graphical model as
shown in Figure 1. It can be used to model and
discover underlying topic structures of any kind
of discrete data in which text is a typical
example. LDA was developed based on an
assumption of document generation process
depicted in both Figure 1 and Table 1.
</bodyText>
<figureCaption confidence="0.774575">
Figure 1 Generation Process for LDA
4.1.4 LDA Estimation with Gibbs Sampling
</figureCaption>
<bodyText confidence="0.999835636363636">
Estimating parameters for LDA by directly and
exactly maximizing the likelihood of the whole
data collection is intractable. The solution to this
is to use approximate estimation methods like
Gibbs Sampling (Griffiths and Steyvers, 2004).
Here, we only show the most important
formula that is used for topic sampling for words.
After finishing Gibbs Sampling, two matrices �
and O are computed as follows.
where O is the latent topic distribution
corresponding to real world people.
</bodyText>
<subsubsectionHeader confidence="0.468602">
4.1.5 Topic-based Features
</subsubsectionHeader>
<bodyText confidence="0.999972285714286">
Through the observation for the given corpus,
many key information, like occupation,
affiliation, mentor, location, and so on, in many
cases, around the target name. So, both local and
global context are choose to doing topic analysis.
Finally, the latent topic distributions are topic-
based representation of context.
</bodyText>
<subsectionHeader confidence="0.986095">
4.2 Clustering
</subsectionHeader>
<bodyText confidence="0.999579">
Our system trusts the result of Person Name
detection absolutely, so contexts need to do
clustering only if they refer to persons with the
same name. We experimented with two different
classical clustering methods: HAC and
DBSCAN.
</bodyText>
<sectionHeader confidence="0.506307" genericHeader="method">
4.2.1 HAC
</sectionHeader>
<bodyText confidence="0.9966295625">
At the heart of hierarchical clustering lies the
definition of similarity between clusters, which
based on similarity between individual
documents. In my system, a linear combination
of similarity based on both local and global
context is employed:
sim _•a simglobal + (1— a)simlocal
where, the general similarity between two
features-vector of documents di and dj is
defined as the cosine similarity:
We will now refine this algorithm for the
different similarity measures of single-link,
complete-link, group-average and centroid
clustering when clustering two smaller clusters
together. In our approach we used an overall
similarity stopping threshold.
</bodyText>
<sectionHeader confidence="0.68451" genericHeader="method">
4.2.2 DBSCAN
</sectionHeader>
<bodyText confidence="0.937785571428571">
In this section, we present the algorithm
DBSCAN (Density Based Spatial Clustering of
Applications with Noise) (Ester et al., 1996)
(Table 2) which is designed to discover the
clusters and the noise in a spatial database.
Table 2 Algorithm of DBSCAN
Arbitrary select a point p
Retrieve all points density-reachable from p wrt
Eps and MinPts.
If p is a core point, a cluster is formed.
If p is a border point, no points are density-
reachable from p and DBSCAN visits the
next point of the database.
Continue the process until all of the points
</bodyText>
<sectionHeader confidence="0.997306" genericHeader="evaluation">
5 Experiments and Results Analysis
</sectionHeader>
<bodyText confidence="0.9986525">
We run all experiments on SIGHAN 2010
training and test corpus.
</bodyText>
<subsectionHeader confidence="0.910549">
5.1 Preprocessing and Person Name
Recognition
</subsectionHeader>
<bodyText confidence="0.9994666">
Firstly, a word segmentation tool based on CRF
is used in each document. Then, person name
recognition is processing. The training data for
word segmentation and PNR is People&apos;s Daily in
January, 1998 and the whole 2000, respectively.
</bodyText>
<subsectionHeader confidence="0.998581">
5.2 Feature Space
</subsectionHeader>
<bodyText confidence="0.9999558">
Our experiments used five types of feature (uni-
char, bi-char, word and topic in local and global),
two feature weighting methods (tf-idf and
entropy) and two feature selection methods (DF
and global tf-idf).
</bodyText>
<subsectionHeader confidence="0.999506">
5.3 Model Selection in LDA
</subsectionHeader>
<bodyText confidence="0.999854777777778">
Our model is conditioned on the Dirichlet
hyperparameters a and8 , the number of topic
K and iterations. The value for the a was set to
0.2, which was optimized in tuning experiment
used training datasets. The 8 was set to 0.1,
which is often considered optimal in LDA-
related models (Griffiths and Steyvers, 2004).
The K was set to 200. The Gibbs sampler was
run for 1,000 iterations.
</bodyText>
<subsectionHeader confidence="0.99989">
5.4 Clustering Results and Analysis
</subsectionHeader>
<bodyText confidence="0.999961387096774">
Since the parameter setting for the clustering
system is very important, we focus only on the
B-cubed scoring (Artiles et al., 2009), and
acquire an overall optimal fixed stop-threshold
from the training data, and then use it in test data.
In this section, we report our results evaluated
by the clustering scoring provided by SIGNAN
2010 evaluation, which includes both the B-
cubed scoring and the purity-based scoring.
Table 3 and 4 demonstrate the performance (F
scores) of our system in different features
representation and clustering for the training
data of the SIGNAN 2010. In Table 3, the
numbers in parentheses are MinPts and Eps
respectively, and stop-threshold in Table 4. As
shown in Table 3, DBSCAN isn’t suitable for
this task, and the results are very sensitive to
parameters. So we didn’t submit DBSCAN-
based results.
Table 4 shows that the best averaged F-scores
for PND are based on topic model, which meet
our initial assumptions, and result based on
merging local and global information is a bit
better than both local and global information
independently. Also, the results based on topic
model are the most robust because the F-score of
variation is slightly with stop-threshold changing.
Conversely, the results based on token are not
like this. As the performance of segmentation is
not very satisfactory, results based on word are
worst, even worse than uni-char-based. In
</bodyText>
<figure confidence="0.685973">
di •d .
(di, dj) _ &apos;
d d
i j
sim
</figure>
<bodyText confidence="0.912099">
addition, it is found that global tf-idf is better
than DF, which is the simplest unsupervised
feature selection method. Entropy weighting is
more effective than tf-idf weighting.
Table 5 shows that the evaluation results in
test data on SIGHAN 2010, and the last two
lines are results in diagnosis test. We are in fifth
place. The evaluation results (F-score) of Person
Name Recognition in training data is 0.965.
</bodyText>
<table confidence="0.9998209375">
Features FS Weighting B-Cubed P-IP
precision recall F P IP F
word (0.19) DF tf-idf 79.05 79.68 76.49 83.25 85.84 82.72
word (0.2) 80.99 75.72 75.54 84.67 83.08 82.2
word (0.3) entropy 78.8 80.71 77.42 83.13 86.62 83.58
word (0.25) global tf-idf tf-idf 80.79 83.1 80.53 84.88 88.32 85.79
word (0.23) 79.45 84.49 79.66 83.76 89.25 85.08
uni-char (0.43) DF tf-idf 76.47 85.46 78.77 81.7 90.05 84.45
uni-char (0.5) 82.34 75.97 77 86.11 83.54 83.78
uni-char (0.48) 80.42 79.44 78.01 84.53 86.17 84.26
bi-char (0.35) 88.3 67.75 75.34 89.96 77.38 82.44
bi-char (0.315) 81.84 81.58 80.54 85.72 87.17 85.8
local topic (0.6) 78.76 86.8 80.63 83.27 91.16 85.88
global topic (0.4) 77.92 88.72 81.04 82.67 92.64 86.26
global topic (0.7) 80.54 88.43 83.55 84.76 92.55 88.02
merged topic (0.63) 81.39 87.82 83.88 85.42 91.94 88.21
</table>
<tableCaption confidence="0.932326">
Table 3 Performance of HAC
</tableCaption>
<table confidence="0.998397428571429">
B-Cubed P-IP
MinPts p recision recall F P IP F
and Eps
2 0.9 64.15 95.84 74.19 71.95 97.36 80.97
2 0.4 71.34 62.25 63.95 76.56 71.94 72.59
3 0.9 64.15 95.88 74.2 71.95 97.37 80.97
6 0.95 64.12 96.55 74.44 71.92 97.79 81.12
</table>
<tableCaption confidence="0.998584">
Table 4 Performance of DBSCAN
</tableCaption>
<sectionHeader confidence="0.986458" genericHeader="discussions">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999610083333333">
In this paper, we present implementation of our
systems for SIGHAN-2010 PND bekeoff,.The
experiments show that the topic features learned
by LDA outperform token features and exhibit
good robustness.
However, in our system, only given data is
exploited. We are going to collect a very large
external data as universal dataset to train topic
model, and then do clustering on both a small set
of training data and a rich set of hidden topics
discovered from universal dataset. The universal
dataset can be snippets returned by search
</bodyText>
<table confidence="0.997735333333333">
B-Cubed P-IP
precision recall F P IP F
80.33 94.52 85.79 85.1 96.46 89.77
80.56 92.56 85.29 85.34 95.19 89.5
80.43 95.41 86.18 85.07 97.06 89.96
80.82 93.41 85.77 85.62 95.76 89.91
</table>
<tableCaption confidence="0.998905">
Table 5 Evaluation Results in test data
</tableCaption>
<bodyText confidence="0.999494416666667">
engine or Wikipedia queried by target name and
some keywords, and so on.
We built our PDN system on the result of
person name recognition. However, it is not
appropriate to toally trust the result of Person
Name detection. So an algorithm that can correct
NER mistakes should be investigated in future
work..
Moreover, Cluster Ensemble system can
ensure the result to be more robust and accurate
accordingly, which is another direction of future
work..
</bodyText>
<sectionHeader confidence="0.994564" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9923148">
This research has been partially supported by the
National Science Foundation of China (NO.
NSFC90920006). We also thank Xiaojie Wang,
Caixia Yuan and Huixing Jiang for useful
discussion of this work.
</bodyText>
<sectionHeader confidence="0.993776" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995646696721311">
Spink, B. Jansen, and J. Pedersen. 2004.
Searching for people on web search engines.
Journal ofDocumentation, 60:266 -278.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2009. Weps 2 evaluation campaign: overview
of the web people search clustering task. In
WePS 2 Evaluation Workshop. WWW
Conference.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007. The semeval-2007 weps evaluation:
Establishing a benchmark for the web people
search task. In Proceedings of the Fourth
International Workshop on Semantic
Evaluations (SemEval-2007).ACL.
M. Ikeda, S. Ono, I. Sato, M. Yoshida, and H.
Nakagawa. 2009. Person name
disambiguation on the web by twostage
clustering. In 2nd Web People Search
Evaluation Workshop (WePS 2009), 18th
WWW Conference.
L. Romano, K. Buza, C. Giuliano, and L.
Schmidt-Thieme. 2009. Person name
disambiguation on the web by twostage
clustering. In 2nd Web People Search
Evaluation Workshop (WePS 2009), 18th
WWW Conference.
Y. Chen, S. Y. M. Lee, and C.-R. Huang. 2009.
Polyuhk: A robust information extraction
system for web personal names. In 2nd Web
People Search Evaluation Workshop (WePS
2009), 18th WWW Conference.
Z. Kozareva, R. Moraliyski, and G. Dias. 2008.
Web people search with domain ranking. In
TSD &apos;08: Proceedings of the 11th
international conference on Text, Speech and
Dialogue, 133-140, Berlin, Heidelberg.
Pedersen, Ted, Amruta Purandare, and Anagha
Kulkarni. 2005. Name Discrimination by
Clustering Similar Contexts. In Proceedings
of the Sixth International Conference on
Intelligent Text Processing and
Computational Linguistics, Mexico City,
Mexico.
G. Cleuziou, L. Martin, and C. Vrain. 2004.
Poboc: an overlapping clustering algorithm.
application to rule-based classification and
textual data, 440-444.
Kalmar, Paul and Matthias Blume. 2007. FICO:
Web Person Disambiguation Via Weighted
Similarity of Entity Contexts. In Proceedings
of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007).ACL.
Rao, Delip, Nikesh Garera and David Yarowsky.
2007. JHU1 : An Unsupervised Approach to
Person Name Disambiguation using Web
Snippets. In Proceedings of the Fourth
International Workshop on Semantic
Evaluations (SemEval-2007).ACL.
Pedersen, Ted and Anagha Kulkarni. 2007.
Unsupervised Discrimination of Person
Names in Web Contexts. In Proceedings of
the Eighth International Conference on
Intelligent Text Processing and
Computational Linguistics, Mexico City,
Mexico.
Phan, X., Nguyen, L. and Horiguchi. 2008.
Learning to Classify Short and Sparse Test &amp;
Web with Hidden Topics from large-scale
Data collection. In Proceedings of 17th
International World Wide Web Conference.
(Beijing, China, April 21-25, 2008). ACM
Press, New York, NY, 91-100.
Samuel Brody and Mirella Lapata. 2009.
Bayesian word sense induction In
Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics, 103-111.
Sun et al. 1995. Identifying Chinese Names in
Unrestricted Texts (in Chinese). In Journal of
Chinese Information Processing, 9(2):16-27.
Liu et al. 2000. Statistical Chinese Person
Names Identification (in Chinese). In Journal
of Chinese Information Processing, 14(3):16-
24.
Huang et al. 2001. Identification of Chinese
Names Based on Statistics (in Chinese). In
Journal of Chinese Information Processing,
15(2):31-37.
Li et al. 2006. Chinese Name Recognition Based
on Boundary Templates and Local Frequency
(in Chinese). In Journal of Chinese
Information Processing, 20(5):44-50.
Mao et al. 2007. Recognizing Chinese Person
Names Based on Hybrid Models (in Chinese).
In Journal of Chinese Information Processing,
21(2):22-27.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic
models for segmenting and labeling sequence
data. In Proc. ICML-01, 282-289, 2001.
Wallach, Hanna. 2004. Conditional random
fields: An introduction. Technical report,
University of Pennsylvania, Department of
Computer and Information Science.
Yang, Y. and Pedersen, J. O. 1997. A
comparative study on feature selection in text
categorization. In Proceedings of ICML-97,
14th International Conference on Machine
Learning (Nashville, US, 1997), 412–420.
Martin Ester, Hans-Peter Kriegel, Jörg Sander,
Xiaowei Xu. 1996. A density-based algorithm
for discovering clusters in large spatial
databases with noise. In Proceedings of the
Second International Conference on
Knowledge Discovery and Data Mining
(KDD-96). AAAI Press. 226-231.
Blei, David M., Andrew Y. Ng, and Michael I.
Jordan. 2003. Latent dirichlet allocation. In J.
Machine Learn. Res. 3, 993-1022.
T. Griffiths and M. Steyvers. 2004. Finding
scientific topics. In The National Academy of
Sciences, 101:5228-5235.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.490081">
<title confidence="0.993405">Person Name Disambiguation based on Topic Model</title>
<author confidence="0.95806">Jiashen Sun</author>
<author confidence="0.95806">Tianmin Wang</author>
<author confidence="0.95806">Li</author>
<affiliation confidence="0.995059">Center of Intelligence Science and Beijing University of Posts and Telecommunications</affiliation>
<email confidence="0.910978">tianmin180@sina.com,wbg111@126.com</email>
<author confidence="0.998748">Xing Wu</author>
<affiliation confidence="0.854827666666667">School of Computer Beijing University of Posts Telecommunications</affiliation>
<email confidence="0.991276">wuxing-6@163.com</email>
<abstract confidence="0.999166111111111">In this paper we describe our participation in the SIGHAN 2010 Task- 3 (Person Name Disambiguation) and detail our approaches. Person Name Disambiguation is typically viewed as an unsupervised clustering problem where the aim is to partition a name’s contexts into different clusters, each representing a real world people. The key point of Clustering is the similarity measure of context, which depends upon the features selection and representation. Two clustering algorithms, HAC and DBSCAN, are investigated in our system. The experiments show that the topic features learned by LDA outperforms token features and more robust.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Jansen Spink</author>
<author>J Pedersen</author>
</authors>
<title>Searching for people on web search engines.</title>
<date>2004</date>
<journal>Journal ofDocumentation,</journal>
<pages>60--266</pages>
<marker>Spink, Pedersen, 2004</marker>
<rawString>Spink, B. Jansen, and J. Pedersen. 2004. Searching for people on web search engines. Journal ofDocumentation, 60:266 -278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>Weps 2 evaluation campaign: overview of the web people search clustering task.</title>
<date>2009</date>
<booktitle>In WePS 2 Evaluation Workshop. WWW Conference.</booktitle>
<contexts>
<context position="1794" citStr="Artiles et al., 2009" startWordPosition="261" endWordPosition="264">f the queries were composed of a person name with additional terms and 4% were identified simply as person names (Spink et al., 2004). However, there is a high level of ambiguity where multiple individuals share the same name and thus the harvesting and the retrieval of relevant information becomes more difficult. This ambiguity has recently become an active research topic and, simultaneously, a relevant application domain for Web search services. Zoominfo.com, Spock.com and 123people.com are examples of sites which perform web people search, although with limited disambiguation capabilities (Artiles et al., 2009). This issue directed current researchers towards the definition of a new task called Web People Search (WePS) or Personal Name Disambiguation (PND). The key assumption underlying the task is that the context surrounding an ambiguous person name is indicative of its ascription. The goal of the clustering task was to group web pages containing the target person&apos;s name, so that pages referring to the same individual are assigned to the same cluster. For this purpose a large dataset was collected and manually annotated. Moreover, because of the ambiguity in word segmentation in Chinese, person na</context>
<context position="5933" citStr="Artiles et al., 2009" startWordPosition="907" endWordPosition="910">dge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007) in order to give more realistic frequency for a proper name or measure whether a bigram is a collocation. Phan et al. (2008) presents a general framework for building classifiers that deal with short and sparse text and Web segments by making the most of hidden topics discovered from large-scale data collections. Samuel Brody et al. (2009) adopt a novel Bayesian approach and formalize the word sense induction problem in a generative model. Previous work using the WePS1 (Artiles et al., 2007) or WePS2 data set (Artiles et al., 2009) shows that standard document clustering methods can deliver excellent performance if similarity measure is enough good to represent relationship of context. The study in Chinese PND is still in its infancy. Person Name detection is often necessary in Chinese. At present, the main technology of person name recognition is used statistical models, and the hybrid approach. Liu et al. (2000) designed a Chinese person name recognition system based on statistical methods, using samples of names from the text corpus and the real amount of statistical data to improve the system performance, while the </context>
<context position="15916" citStr="Artiles et al., 2009" startWordPosition="2544" endWordPosition="2547">election methods (DF and global tf-idf). 5.3 Model Selection in LDA Our model is conditioned on the Dirichlet hyperparameters a and8 , the number of topic K and iterations. The value for the a was set to 0.2, which was optimized in tuning experiment used training datasets. The 8 was set to 0.1, which is often considered optimal in LDArelated models (Griffiths and Steyvers, 2004). The K was set to 200. The Gibbs sampler was run for 1,000 iterations. 5.4 Clustering Results and Analysis Since the parameter setting for the clustering system is very important, we focus only on the B-cubed scoring (Artiles et al., 2009), and acquire an overall optimal fixed stop-threshold from the training data, and then use it in test data. In this section, we report our results evaluated by the clustering scoring provided by SIGNAN 2010 evaluation, which includes both the Bcubed scoring and the purity-based scoring. Table 3 and 4 demonstrate the performance (F scores) of our system in different features representation and clustering for the training data of the SIGNAN 2010. In Table 3, the numbers in parentheses are MinPts and Eps respectively, and stop-threshold in Table 4. As shown in Table 3, DBSCAN isn’t suitable for t</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2009</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009. Weps 2 evaluation campaign: overview of the web people search clustering task. In WePS 2 Evaluation Workshop. WWW Conference.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Satoshi Sekine</author>
</authors>
<title>The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).ACL.</booktitle>
<contexts>
<context position="5892" citStr="Artiles et al., 2007" startWordPosition="899" endWordPosition="902">rsal data repositories as external knowledge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007) in order to give more realistic frequency for a proper name or measure whether a bigram is a collocation. Phan et al. (2008) presents a general framework for building classifiers that deal with short and sparse text and Web segments by making the most of hidden topics discovered from large-scale data collections. Samuel Brody et al. (2009) adopt a novel Bayesian approach and formalize the word sense induction problem in a generative model. Previous work using the WePS1 (Artiles et al., 2007) or WePS2 data set (Artiles et al., 2009) shows that standard document clustering methods can deliver excellent performance if similarity measure is enough good to represent relationship of context. The study in Chinese PND is still in its infancy. Person Name detection is often necessary in Chinese. At present, the main technology of person name recognition is used statistical models, and the hybrid approach. Liu et al. (2000) designed a Chinese person name recognition system based on statistical methods, using samples of names from the text corpus and the real amount of statistical data to i</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007. The semeval-2007 weps evaluation: Establishing a benchmark for the web people search task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).ACL. M. Ikeda, S. Ono, I. Sato, M. Yoshida, and H. Nakagawa. 2009. Person name disambiguation on the web by twostage clustering. In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Romano</author>
<author>K Buza</author>
<author>C Giuliano</author>
<author>L Schmidt-Thieme</author>
</authors>
<title>Person name disambiguation on the web by twostage clustering.</title>
<date>2009</date>
<booktitle>In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference.</booktitle>
<contexts>
<context position="4118" citStr="Romano et al., 2009" startWordPosition="629" endWordPosition="632"> (contexts) are explicitly intended to communicate a latent topic distribution corresponding to real world people. The remainder of this paper is structured as follows. We first present an overview of related work (Section 2) and then describe our system which consists of NER and clustering in more details (Sections 3 and 4). Section 5 describes the resources and evaluation results in our experiments. We discuss our results and conclude our work in Section 6. 2 Related Work The most commonly used feature is the bag of words in local or global context of the ambiguous name (Ikeda et al., 2009; Romano et al., 2009). Because the given corpus is often not large enough to learn the realistic probabilities or weights for those features, traditional algorithm such as vector-based techniques used in large-scale text will lead to data sparseness. In recent years, more and more important studies have attempted to overcome the problem to get a better (semantic) similarity measures. A lot of features such as syntactic chunks, named entities, dependency parses, semantic role labels, etc., were employed. However, these features need many NLP preprocessing (Chen, 2009). Many studies show that they can achieve stateo</context>
</contexts>
<marker>Romano, Buza, Giuliano, Schmidt-Thieme, 2009</marker>
<rawString>L. Romano, K. Buza, C. Giuliano, and L. Schmidt-Thieme. 2009. Person name disambiguation on the web by twostage clustering. In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chen</author>
<author>S Y M Lee</author>
<author>C-R Huang</author>
</authors>
<title>Polyuhk: A robust information extraction system for web personal names.</title>
<date>2009</date>
<booktitle>In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference.</booktitle>
<marker>Chen, Lee, Huang, 2009</marker>
<rawString>Y. Chen, S. Y. M. Lee, and C.-R. Huang. 2009. Polyuhk: A robust information extraction system for web personal names. In 2nd Web People Search Evaluation Workshop (WePS 2009), 18th WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>R Moraliyski</author>
<author>G Dias</author>
</authors>
<title>Web people search with domain ranking.</title>
<date>2008</date>
<booktitle>In TSD &apos;08: Proceedings of the 11th international conference on Text, Speech and Dialogue,</booktitle>
<pages>133--140</pages>
<location>Berlin, Heidelberg.</location>
<marker>Kozareva, Moraliyski, Dias, 2008</marker>
<rawString>Z. Kozareva, R. Moraliyski, and G. Dias. 2008. Web people search with domain ranking. In TSD &apos;08: Proceedings of the 11th international conference on Text, Speech and Dialogue, 133-140, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Amruta Purandare</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Name Discrimination by Clustering Similar Contexts.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="4795" citStr="Pedersen et al. (2005)" startWordPosition="729" endWordPosition="732"> learn the realistic probabilities or weights for those features, traditional algorithm such as vector-based techniques used in large-scale text will lead to data sparseness. In recent years, more and more important studies have attempted to overcome the problem to get a better (semantic) similarity measures. A lot of features such as syntactic chunks, named entities, dependency parses, semantic role labels, etc., were employed. However, these features need many NLP preprocessing (Chen, 2009). Many studies show that they can achieve stateof-the-art performances only with lightweight features. Pedersen et al. (2005) present SenseClusters which represents the instances to be clustered using second order co–occurrence vectors. Kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from WordNet. PoBOC clustering (Cleuziou et al., 2004) is used which builds a weighted graph with weights being the similarity among the objects. Another way is to utilize universal data repositories as external knowledge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007)</context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>Pedersen, Ted, Amruta Purandare, and Anagha Kulkarni. 2005. Name Discrimination by Clustering Similar Contexts. In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cleuziou</author>
<author>L Martin</author>
<author>C Vrain</author>
</authors>
<title>Poboc: an overlapping clustering algorithm. application to rule-based classification and textual data,</title>
<date>2004</date>
<pages>440--444</pages>
<contexts>
<context position="5148" citStr="Cleuziou et al., 2004" startWordPosition="778" endWordPosition="781">, named entities, dependency parses, semantic role labels, etc., were employed. However, these features need many NLP preprocessing (Chen, 2009). Many studies show that they can achieve stateof-the-art performances only with lightweight features. Pedersen et al. (2005) present SenseClusters which represents the instances to be clustered using second order co–occurrence vectors. Kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from WordNet. PoBOC clustering (Cleuziou et al., 2004) is used which builds a weighted graph with weights being the similarity among the objects. Another way is to utilize universal data repositories as external knowledge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007) in order to give more realistic frequency for a proper name or measure whether a bigram is a collocation. Phan et al. (2008) presents a general framework for building classifiers that deal with short and sparse text and Web segments by making the most of hidden topics discovered from large-scale data collections. Samuel Brody et al. (2009) adopt a no</context>
</contexts>
<marker>Cleuziou, Martin, Vrain, 2004</marker>
<rawString>G. Cleuziou, L. Martin, and C. Vrain. 2004. Poboc: an overlapping clustering algorithm. application to rule-based classification and textual data, 440-444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kalmar</author>
<author>Matthias Blume</author>
</authors>
<title>FICO: Web Person Disambiguation Via Weighted Similarity of Entity Contexts.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).ACL.</booktitle>
<contexts>
<context position="5365" citStr="Kalmar and Blume, 2007" startWordPosition="813" endWordPosition="816">ly with lightweight features. Pedersen et al. (2005) present SenseClusters which represents the instances to be clustered using second order co–occurrence vectors. Kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from WordNet. PoBOC clustering (Cleuziou et al., 2004) is used which builds a weighted graph with weights being the similarity among the objects. Another way is to utilize universal data repositories as external knowledge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007) in order to give more realistic frequency for a proper name or measure whether a bigram is a collocation. Phan et al. (2008) presents a general framework for building classifiers that deal with short and sparse text and Web segments by making the most of hidden topics discovered from large-scale data collections. Samuel Brody et al. (2009) adopt a novel Bayesian approach and formalize the word sense induction problem in a generative model. Previous work using the WePS1 (Artiles et al., 2007) or WePS2 data set (Artiles et al., 2009) shows that standard document cl</context>
</contexts>
<marker>Kalmar, Blume, 2007</marker>
<rawString>Kalmar, Paul and Matthias Blume. 2007. FICO: Web Person Disambiguation Via Weighted Similarity of Entity Contexts. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delip Rao</author>
<author>Nikesh Garera</author>
<author>David Yarowsky</author>
</authors>
<title>JHU1 : An Unsupervised Approach to Person Name Disambiguation using Web Snippets.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).ACL.</booktitle>
<contexts>
<context position="5341" citStr="Rao et al., 2007" startWordPosition="809" endWordPosition="812">rt performances only with lightweight features. Pedersen et al. (2005) present SenseClusters which represents the instances to be clustered using second order co–occurrence vectors. Kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from WordNet. PoBOC clustering (Cleuziou et al., 2004) is used which builds a weighted graph with weights being the similarity among the objects. Another way is to utilize universal data repositories as external knowledge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007) in order to give more realistic frequency for a proper name or measure whether a bigram is a collocation. Phan et al. (2008) presents a general framework for building classifiers that deal with short and sparse text and Web segments by making the most of hidden topics discovered from large-scale data collections. Samuel Brody et al. (2009) adopt a novel Bayesian approach and formalize the word sense induction problem in a generative model. Previous work using the WePS1 (Artiles et al., 2007) or WePS2 data set (Artiles et al., 2009) shows t</context>
</contexts>
<marker>Rao, Garera, Yarowsky, 2007</marker>
<rawString>Rao, Delip, Nikesh Garera and David Yarowsky. 2007. JHU1 : An Unsupervised Approach to Person Name Disambiguation using Web Snippets. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007).ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Anagha Kulkarni</author>
</authors>
<title>Unsupervised Discrimination of Person Names in Web Contexts.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eighth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<location>Mexico City, Mexico.</location>
<marker>Pedersen, Kulkarni, 2007</marker>
<rawString>Pedersen, Ted and Anagha Kulkarni. 2007. Unsupervised Discrimination of Person Names in Web Contexts. In Proceedings of the Eighth International Conference on Intelligent Text Processing and Computational Linguistics, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Phan</author>
<author>L Nguyen</author>
<author>Horiguchi</author>
</authors>
<title>Learning to Classify Short and Sparse Test &amp; Web with Hidden Topics from large-scale Data collection.</title>
<date>2008</date>
<booktitle>In Proceedings of 17th International World Wide Web Conference.</booktitle>
<pages>91--100</pages>
<publisher>ACM Press,</publisher>
<location>Beijing, China,</location>
<contexts>
<context position="5520" citStr="Phan et al. (2008)" startWordPosition="840" endWordPosition="843">ors. Kozareva (2008) focuses on the resolution of the web people search problem through the integration of domain information, which can represent relationship between contexts and is learned from WordNet. PoBOC clustering (Cleuziou et al., 2004) is used which builds a weighted graph with weights being the similarity among the objects. Another way is to utilize universal data repositories as external knowledge sources (Rao et al., 2007; Kalmar and Blume, 2007; Pedersen and Kulkarni; 2007) in order to give more realistic frequency for a proper name or measure whether a bigram is a collocation. Phan et al. (2008) presents a general framework for building classifiers that deal with short and sparse text and Web segments by making the most of hidden topics discovered from large-scale data collections. Samuel Brody et al. (2009) adopt a novel Bayesian approach and formalize the word sense induction problem in a generative model. Previous work using the WePS1 (Artiles et al., 2007) or WePS2 data set (Artiles et al., 2009) shows that standard document clustering methods can deliver excellent performance if similarity measure is enough good to represent relationship of context. The study in Chinese PND is s</context>
</contexts>
<marker>Phan, Nguyen, Horiguchi, 2008</marker>
<rawString>Phan, X., Nguyen, L. and Horiguchi. 2008. Learning to Classify Short and Sparse Test &amp; Web with Hidden Topics from large-scale Data collection. In Proceedings of 17th International World Wide Web Conference. (Beijing, China, April 21-25, 2008). ACM Press, New York, NY, 91-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>103--111</pages>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, 103-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sun</author>
</authors>
<title>Identifying Chinese Names in Unrestricted Texts (in Chinese).</title>
<date>1995</date>
<booktitle>In Journal of Chinese Information Processing,</booktitle>
<pages>9--2</pages>
<marker>Sun, 1995</marker>
<rawString>Sun et al. 1995. Identifying Chinese Names in Unrestricted Texts (in Chinese). In Journal of Chinese Information Processing, 9(2):16-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liu</author>
</authors>
<title>Statistical Chinese Person Names Identification (in Chinese).</title>
<date>2000</date>
<booktitle>In Journal of Chinese Information Processing,</booktitle>
<pages>14--3</pages>
<marker>Liu, 2000</marker>
<rawString>Liu et al. 2000. Statistical Chinese Person Names Identification (in Chinese). In Journal of Chinese Information Processing, 14(3):16-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huang</author>
</authors>
<title>Identification of Chinese Names Based on Statistics (in Chinese).</title>
<date>2001</date>
<booktitle>In Journal of Chinese Information Processing,</booktitle>
<pages>15--2</pages>
<marker>Huang, 2001</marker>
<rawString>Huang et al. 2001. Identification of Chinese Names Based on Statistics (in Chinese). In Journal of Chinese Information Processing, 15(2):31-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li</author>
</authors>
<title>Chinese Name Recognition Based on Boundary Templates and Local Frequency (in Chinese).</title>
<date>2006</date>
<booktitle>In Journal of Chinese Information Processing,</booktitle>
<pages>20--5</pages>
<marker>Li, 2006</marker>
<rawString>Li et al. 2006. Chinese Name Recognition Based on Boundary Templates and Local Frequency (in Chinese). In Journal of Chinese Information Processing, 20(5):44-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mao</author>
</authors>
<title>Recognizing Chinese Person Names Based on Hybrid Models (in Chinese).</title>
<date>2007</date>
<booktitle>In Journal of Chinese Information Processing,</booktitle>
<pages>21--2</pages>
<marker>Mao, 2007</marker>
<rawString>Mao et al. 2007. Recognizing Chinese Person Names Based on Hybrid Models (in Chinese). In Journal of Chinese Information Processing, 21(2):22-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ICML-01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="7467" citStr="Lafferty, McCallum, and Pereira, 2001" startWordPosition="1150" endWordPosition="1154">ntial names, and to recognize the results spread to the entire article in order to recall missing names caused by sparse data. 3 Person Name Recognition In this section, we focus on Conditional Random Fields (CRFs) algorithm to establish the appropriate language model. Given of the input text, we may detect the potential person names in the text fragments, and then take various features into account to recognize of Chinese person names. Conditional Random Fields as a sequence learning method has been successfully applied in many NLP tasks. More details of the its principle can be referred in (Lafferty, McCallum, and Pereira, 2001; Wallach, 2004). We here will focus on how to apply CRFs in our person name recognition task. 3.1 CRFs-based name recognition CRFs is used to get potential names as the first stage name recognition outcome. To avoid the interference that caused by word segmentation errors, we use single Chinese character information rather than word as discriminative features for CRFs learning model. We use BIEO label strategy to transfer the name recognition as a sequence learning task. The label set includes: B-Nr (Begin, the initial character of name), I-Nr (In, the middle character of name), E-Nr(End, the</context>
<context position="9102" citStr="Lafferty, McCallum, and Pereira, 2001" startWordPosition="1412" endWordPosition="1416">ter of name is not recognized, the middle character of name is not recognized, the end character of name is not recognized, and their combinations of those three errors. The other two extreme errors, including non-name recognition for the anchor name, and the name is not recognized as potential names. In the stage of rule-based correction, we first conduct word segmentation for the text. The segmentation process is also realized with the method of CRFs, without using dictionaries and other external knowledge. The detailed description is beyond this paper, which can be accessible in the paper (Lafferty, McCallum, and Pereira, 2001). The only thing we should note is that part of the error in such segmentation result obtained in this way can be corrected through the introduction of an external dictionary. For each potential name, and we examine it from the following two aspects: 1) It is reasonable to use the word in a person name, including checking the surname and the character used in names; 2) The left and right borders are correct. Check the left and right sides of the cutting unit can be added to the names, including the words used before names, the words used behind names and the surname and character used in name</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML-01, 282-289, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Wallach</author>
</authors>
<title>Conditional random fields: An introduction.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania, Department of Computer and Information Science.</institution>
<contexts>
<context position="7483" citStr="Wallach, 2004" startWordPosition="1155" endWordPosition="1156">ts spread to the entire article in order to recall missing names caused by sparse data. 3 Person Name Recognition In this section, we focus on Conditional Random Fields (CRFs) algorithm to establish the appropriate language model. Given of the input text, we may detect the potential person names in the text fragments, and then take various features into account to recognize of Chinese person names. Conditional Random Fields as a sequence learning method has been successfully applied in many NLP tasks. More details of the its principle can be referred in (Lafferty, McCallum, and Pereira, 2001; Wallach, 2004). We here will focus on how to apply CRFs in our person name recognition task. 3.1 CRFs-based name recognition CRFs is used to get potential names as the first stage name recognition outcome. To avoid the interference that caused by word segmentation errors, we use single Chinese character information rather than word as discriminative features for CRFs learning model. We use BIEO label strategy to transfer the name recognition as a sequence learning task. The label set includes: B-Nr (Begin, the initial character of name), I-Nr (In, the middle character of name), E-Nr(End, the end character o</context>
</contexts>
<marker>Wallach, 2004</marker>
<rawString>Wallach, Hanna. 2004. Conditional random fields: An introduction. Technical report, University of Pennsylvania, Department of Computer and Information Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>J O Pedersen</author>
</authors>
<title>A comparative study on feature selection in text categorization.</title>
<date>1997</date>
<booktitle>In Proceedings of ICML-97, 14th International Conference on Machine Learning</booktitle>
<pages>412--420</pages>
<location>Nashville, US,</location>
<contexts>
<context position="11269" citStr="Yang &amp; Pedersen, 1997" startWordPosition="1786" endWordPosition="1789">¬ni ni is the average uncertainty or entropy of term i. Entropy weighting is based on information theoretic ideas and is the most sophisticated weighting scheme. 4.1.2 Features Selection In this Section, we give a brief introduction on two effective unsupervised feature selection methods, DF and global tf-idf. DF (Document frequency) is the number of documents in which a term occurs in a dataset. It is the simplest criterion for term selection and easily scales to a large dataset with linear computation complexity. It is a simple but effective feature selection method for text categorization (Yang &amp; Pedersen, 1997). a f ik ik º » 1/4 We introduce a new feature selection method called “global tf-idf” that takes the term weight into account. Because DF assumes that each term is of same importance in different documents, it is easily biased by those common terms which have high document frequency but uniform distribution over different classes. Global tf-idf is proposed to deal with this problem: N g tfidf i ¦ ik k 1 4.1.3 Latent Dirichlet Allocation (LDA) Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic generative model of text generation. LDA models each documen</context>
</contexts>
<marker>Yang, Pedersen, 1997</marker>
<rawString>Yang, Y. and Pedersen, J. O. 1997. A comparative study on feature selection in text categorization. In Proceedings of ICML-97, 14th International Conference on Machine Learning (Nashville, US, 1997), 412–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Ester</author>
<author>Hans-Peter Kriegel</author>
<author>Jörg Sander</author>
<author>Xiaowei Xu</author>
</authors>
<title>A density-based algorithm for discovering clusters in large spatial databases with noise.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96).</booktitle>
<pages>226--231</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="14319" citStr="Ester et al., 1996" startWordPosition="2271" endWordPosition="2274">n of similarity based on both local and global context is employed: sim _•a simglobal + (1— a)simlocal where, the general similarity between two features-vector of documents di and dj is defined as the cosine similarity: We will now refine this algorithm for the different similarity measures of single-link, complete-link, group-average and centroid clustering when clustering two smaller clusters together. In our approach we used an overall similarity stopping threshold. 4.2.2 DBSCAN In this section, we present the algorithm DBSCAN (Density Based Spatial Clustering of Applications with Noise) (Ester et al., 1996) (Table 2) which is designed to discover the clusters and the noise in a spatial database. Table 2 Algorithm of DBSCAN Arbitrary select a point p Retrieve all points density-reachable from p wrt Eps and MinPts. If p is a core point, a cluster is formed. If p is a border point, no points are densityreachable from p and DBSCAN visits the next point of the database. Continue the process until all of the points 5 Experiments and Results Analysis We run all experiments on SIGHAN 2010 training and test corpus. 5.1 Preprocessing and Person Name Recognition Firstly, a word segmentation tool based on C</context>
</contexts>
<marker>Ester, Kriegel, Sander, Xu, 1996</marker>
<rawString>Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press. 226-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>In J. Machine Learn. Res.</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="11791" citStr="Blei et al. 2003" startWordPosition="1877" endWordPosition="1880">a simple but effective feature selection method for text categorization (Yang &amp; Pedersen, 1997). a f ik ik º » 1/4 We introduce a new feature selection method called “global tf-idf” that takes the term weight into account. Because DF assumes that each term is of same importance in different documents, it is easily biased by those common terms which have high document frequency but uniform distribution over different classes. Global tf-idf is proposed to deal with this problem: N g tfidf i ¦ ik k 1 4.1.3 Latent Dirichlet Allocation (LDA) Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic generative model of text generation. LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words. The main motivation is that the task, fail to achieve high accuracy due to the data sparseness. LDA is a generative graphical model as shown in Figure 1. It can be used to model and discover underlying topic structures of any kind of discrete data in which text is a typical example. LDA was developed based on an assumption of document generation process depicted in both Figure 1 and Table 1. Figure 1 Generation Process for L</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. In J. Machine Learn. Res. 3, 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>In The National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="12672" citStr="Griffiths and Steyvers, 2004" startWordPosition="2021" endWordPosition="2024">ata sparseness. LDA is a generative graphical model as shown in Figure 1. It can be used to model and discover underlying topic structures of any kind of discrete data in which text is a typical example. LDA was developed based on an assumption of document generation process depicted in both Figure 1 and Table 1. Figure 1 Generation Process for LDA 4.1.4 LDA Estimation with Gibbs Sampling Estimating parameters for LDA by directly and exactly maximizing the likelihood of the whole data collection is intractable. The solution to this is to use approximate estimation methods like Gibbs Sampling (Griffiths and Steyvers, 2004). Here, we only show the most important formula that is used for topic sampling for words. After finishing Gibbs Sampling, two matrices � and O are computed as follows. where O is the latent topic distribution corresponding to real world people. 4.1.5 Topic-based Features Through the observation for the given corpus, many key information, like occupation, affiliation, mentor, location, and so on, in many cases, around the target name. So, both local and global context are choose to doing topic analysis. Finally, the latent topic distributions are topicbased representation of context. 4.2 Clust</context>
<context position="15676" citStr="Griffiths and Steyvers, 2004" startWordPosition="2503" endWordPosition="2506">ple&apos;s Daily in January, 1998 and the whole 2000, respectively. 5.2 Feature Space Our experiments used five types of feature (unichar, bi-char, word and topic in local and global), two feature weighting methods (tf-idf and entropy) and two feature selection methods (DF and global tf-idf). 5.3 Model Selection in LDA Our model is conditioned on the Dirichlet hyperparameters a and8 , the number of topic K and iterations. The value for the a was set to 0.2, which was optimized in tuning experiment used training datasets. The 8 was set to 0.1, which is often considered optimal in LDArelated models (Griffiths and Steyvers, 2004). The K was set to 200. The Gibbs sampler was run for 1,000 iterations. 5.4 Clustering Results and Analysis Since the parameter setting for the clustering system is very important, we focus only on the B-cubed scoring (Artiles et al., 2009), and acquire an overall optimal fixed stop-threshold from the training data, and then use it in test data. In this section, we report our results evaluated by the clustering scoring provided by SIGNAN 2010 evaluation, which includes both the Bcubed scoring and the purity-based scoring. Table 3 and 4 demonstrate the performance (F scores) of our system in di</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. Griffiths and M. Steyvers. 2004. Finding scientific topics. In The National Academy of Sciences, 101:5228-5235.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>