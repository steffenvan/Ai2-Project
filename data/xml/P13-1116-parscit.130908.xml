<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000101">
<title confidence="0.991634">
ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling
</title>
<author confidence="0.95331">
Egoitz Laparra
</author>
<affiliation confidence="0.94227">
IXA Group
University of the Basque Country
</affiliation>
<address confidence="0.905258">
San Sebastian, Spain
</address>
<email confidence="0.997694">
egoitz.laparra@ehu.es
</email>
<author confidence="0.891105">
German Rigau
</author>
<affiliation confidence="0.8837575">
IXA Group
University of the Basque Country
</affiliation>
<address confidence="0.861488">
San Sebastian, Spain
</address>
<email confidence="0.998037">
german.rigau@ehu.es
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999606">
This paper presents a novel deterministic
algorithm for implicit Semantic Role La-
beling. The system exploits a very sim-
ple but relevant discursive property, the ar-
gument coherence over different instances
of a predicate. The algorithm solves the
implicit arguments sequentially, exploit-
ing not only explicit but also the implicit
arguments previously solved. In addition,
we empirically demonstrate that the algo-
rithm obtains very competitive and robust
performances with respect to supervised
approaches that require large amounts of
costly training data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.982041047619048">
Traditionally, Semantic Role Labeling (SRL) sys-
tems have focused in searching the fillers of those
explicit roles appearing within sentence bound-
aries (Gildea and Jurafsky, 2000, 2002; Carreras
and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc
et al., 2009). These systems limited their search-
space to the elements that share a syntactical re-
lation with the predicate. However, when the par-
ticipants of a predicate are implicit this approach
obtains incomplete predicative structures with null
arguments. The following example includes the
gold-standard annotations for a traditional SRL
process:
(1) [arg0 The network] had been expected to have [np
losses] [arg1 of as much as $20 million] [arg3 on base-
ball this year]. It isn’t clear how much those [np losses]
may widen because of the short Series.
The previous analysis includes annotations for
the nominal predicate loss based on the NomBank
structure (Meyers et al., 2004). In this case the
annotator identifies, in the first sentence, the argu-
ments arg0, the entity losing something, arg1, the
thing lost, and arg3, the source of that loss. How-
ever, in the second sentence there is another in-
stance of the same predicate, loss, but in this case
no argument has been associated with it. Tradi-
tional SRL systems facing this type of examples
are not able to fill the arguments of a predicate
because their fillers are not in the same sentence
of the predicate. Moreover, these systems also let
unfilled arguments occurring in the same sentence,
like in the following example:
(2) Quest Medical Inc said it adopted [arg1 a sharehold-
ers’ rights] [np plan] in which rights to purchase shares
of common stock will be distributed as a dividend to
shareholders of record as of Oct 23.
For the predicate plan in the previous sentence,
a traditional SRL process only returns the filler for
the argument arg1, the theme of the plan.
However, in both examples, a reader could eas-
ily infer the missing arguments from the surround-
ing context of the predicate, and determine that
in (1) both instances of the predicate share the
same arguments and in (2) the missing argument
corresponds to the subject of the verb that domi-
nates the predicate, Quest Medical Inc. Obviously,
this additional annotations could contribute posi-
tively to its semantic analysis. In fact, Gerber and
Chai (2010) pointed out that implicit arguments
can increase the coverage of argument structures
in NomBank by 71%. However, current automatic
systems require large amounts of manually anno-
tated training data for each predicate. The effort
required for this manual annotation explains the
absence of generally applicable tools. This prob-
lem has become a main concern for many NLP
tasks. This fact explains a new trend to develop
accurate unsupervised systems that exploit sim-
ple but robust linguistic principles (Raghunathan
et al., 2010).
In this work, we study the coherence of the
predicate and argument realization in discourse. In
particular, we have followed a similar approach to
</bodyText>
<page confidence="0.933316">
1180
</page>
<note confidence="0.9133685">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1180–1189,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9986364">
the one proposed by Dahl et al. (1987) who filled
the arguments of anaphoric mentions of nominal
predicates using previous mentions of the same
predicate. We present an extension of this idea
assuming that in a coherent document the differ-
ent ocurrences of a predicate, including both ver-
bal and nominal forms, tend to be mentions of
the same event, and thus, they share the same
argument fillers. Following this approach, we
have developed a deterministic algorithm that ob-
tains competitive results with respect to supervised
methods. That is, our system can be applied to any
predicate without training data.
The main contributions of this work are the fol-
lowing:
</bodyText>
<listItem confidence="0.942623416666667">
• We empirically prove that there exists a
strong discourse relationship between the im-
plicit and explicit argument fillers of the same
predicates.
• We propose a deterministic approach that ex-
ploits this discoursive property in order to ob-
tain the fillers of implicit arguments.
• We adapt to the implicit SRL problem a clas-
sic algorithm for pronoun resolution.
• We develop a robust algorithm, ImpAr, that
obtains very competitive results with respect
to existing supervised systems. We release
</listItem>
<bodyText confidence="0.9412085">
an open source prototype implementing this
algorithm1.
The paper is structured as follows. Section 2
discusses the related work. Section 3 presents in
detail the data used in our experiments. Section
4 describes our algorithm for implicit argument
resolution. Section 5 presents some experiments
we have carried out to test the algorithm. Section
</bodyText>
<sectionHeader confidence="0.757841" genericHeader="introduction">
6 discusses the results obtained. Finally, section
</sectionHeader>
<bodyText confidence="0.9689545">
7 offers some concluding remarks and presents
some future research lines.
</bodyText>
<sectionHeader confidence="0.999783" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999850166666667">
The first attempt for the automatic annotation of
implicit semantic roles was proposed by Palmer
et al. (1986). This work applied selectional restric-
tions together with coreference chains, in a very
specific domain. In a similar approach, Whitte-
more et al. (1991) also attempted to solve implicit
</bodyText>
<footnote confidence="0.944545">
1http://adimen.si.ehu.es/web/ImpAr
</footnote>
<bodyText confidence="0.999837294117647">
arguments using some manually described seman-
tic constraints for each thematic role they tried to
cover. Another early approach was presented by
Tetreault (2002). Studying another specific do-
main, they obtained some probabilistic relations
between some roles. These early works agree that
the problem is, in fact, a special case of anaphora
or coreference resolution.
Recently, the task has been taken up again
around two different proposals. On the one
hand, Ruppenhofer et al. (2010) presented a task
in SemEval-2010 that included an implicit argu-
ment identification challenge based on FrameNet
(Baker et al., 1998). The corpus for this task
consisted in some novel chapters. They covered
a wide variety of nominal and verbal predicates,
each one having only a small number of instances.
Only two systems were presented for this sub-
task obtaining quite poor results (F1 below 0,02).
VENSES++ (Tonelli and Delmonte, 2010) applied
a rule based anaphora resolution procedure and se-
mantic similarity between candidates and thematic
roles using WordNet (Fellbaum, 1998). The sys-
tem was tuned in (Tonelli and Delmonte, 2011)
improving slightly its performance. SEMAFOR
(Chen et al., 2010) is a supervised system that
extended an existing semantic role labeler to en-
large the search window to other sentences, replac-
ing the features defined for regular arguments with
two new semantic features. Although this system
obtained the best performance in the task, data
sparseness strongly affected the results. Besides
the two systems presented to the task, some other
systems have used the same dataset and evaluation
metrics. Ruppenhofer et al. (2011), Laparra and
Rigau (2012), Gorinski et al. (2013) and Laparra
and Rigau (2013) explore alternative linguistic and
semantic strategies. These works obtained signifi-
cant gains over previous approaches. Silberer and
Frank (2012) adapted an entity-based coreference
resolution model to extend automatically the train-
ing corpus. Exploiting this additional data, their
system was able to improve previous results. Fol-
lowing this approach Moor et al. (2013) present a
corpus of predicate-specific annotations for verbs
in the FrameNet paradigm that are aligned with
PropBank and VerbNet.
On the other hand, Gerber and Chai (2010,
2012) studied the implicit argument resolution on
NomBank. They uses a set of syntactic, semantic
and coreferential features to train a logistic regres-
</bodyText>
<page confidence="0.987142">
1181
</page>
<bodyText confidence="0.999987514285714">
sion classifier. Unlike the dataset from SemEval-
2010 (Ruppenhofer et al., 2010), in this work the
authors focused on a small set of ten predicates.
But for those predicates, they annotated a large
amount of instances in the documents from the
Wall Street Journal that were already annotated
for PropBank (Palmer et al., 2005) and NomBank.
This allowed them to avoid the sparseness prob-
lems and generalize properly from the training
set. The results of this system were far better
than those obtained by the systems that faced the
SemEval-2010 dataset. This works represent the
deepest study so far of the features that charac-
terizes the implicit arguments 2. However, many
of the most important features are lexically depen-
dent on the predicate and cannot been generalized.
Thus, specific annotations are required for each
new predicate to be analyzed.
All the works presented in this section agree that
implicit arguments must be modeled as a particu-
lar case of coreference together with features that
include lexical-semantic information, to build se-
lectional preferences. Another common point is
the fact that these works try to solve each instance
of the implicit arguments independently, without
taking into account the previous realizations of
the same implicit argument in the document. We
propose that these realizations, together with the
explicit ones, must maintain a certain coherence
along the document and, in consequence, the filler
of an argument remains the same along the fol-
lowing instances of that argument until a stronger
evidence indicates a change. We also propose that
this feature can be exploited independently from
the predicate.
</bodyText>
<sectionHeader confidence="0.998552" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.991604485714286">
In our experiments, we have focused on the dataset
developed in Gerber and Chai (2010, 2012). This
dataset (hereinafter BNB which stands for ”Be-
yond NomBank”) extends existing predicate an-
notations for NomBank and ProbBank.
BNB presented the first annotation work of im-
plicit arguments based on PropBank and Nom-
Bank frames. This annotation was an extension
of the standard training, development and testing
sections of Penn TreeBank that have been typi-
cally used for SRL evaluation and were already
annotated with PropBank and NomBank predicate
2Gerber and Chai (2012) includes a set of 81 different fea-
tures.
structures. The authors selected a limited set of
predicates. These predicates are all nominaliza-
tions of other verbal predicates, without sense am-
biguity, that appear frequently in the corpus and
tend to have implicit arguments associated with
their instances. These constraints allowed them to
model enough occurrences of each implicit argu-
ment in order to cover adequately all the possible
cases appearing in a test document. For each miss-
ing argument position they went over all the pre-
ceding sentences and annotated all mentions of the
filler of that argument. In tables 3 and 4 we show
the list of predicates and the resulting figures of
this annotation.
In this work we also use the corpus provided
for the CoNLL-2008 task. These corpora cover
the same BNB documents and include annotated
predictions for syntactic dependencies and Super-
Sense labels as semantic tags. Unlike Gerber and
Chai (2010, 2012) we do not use the constituent
analysis from the Penn TreeBank.
</bodyText>
<sectionHeader confidence="0.998236" genericHeader="method">
4 ImpAr algorithm
</sectionHeader>
<subsectionHeader confidence="0.999009">
4.1 Discoursive coherence of predicates
</subsectionHeader>
<bodyText confidence="0.999982555555555">
Exploring the training dataset of BNB, we ob-
served a very strong discourse effect on the im-
plicit and explicit argument fillers of the predi-
cates. That is, if several instances of the same
predicate appear in a well-written discourse, it is
very likely that they maintain the same argument
fillers. This property holds when joining the dif-
ferent parts-of-speech of the predicates (nominal
or verbal) and the explicit or implicit realizations
of the argument fillers. For instance, we observed
that 46% of all implicit arguments share the same
filler with the previous instance of the same predi-
cate while only 14% of them have a different filler.
The remaining 40% of all implicit arguments cor-
respond to first occurrences of their predicates.
That is, these fillers can not be recovered from pre-
vious instances of their predicates.
The rationale behind this phenomena seems to
be simple. When referring to different aspects of
the same event, the writer of a coherent document
does not repeat redundant information. They re-
fer to previous predicate instances assuming that
the reader already recalls the involved participants.
That is, the filler of the different instances of a
predicate argument maintain a certain discourse
coherence. For instance, in example (1), all the ar-
gument positions of the second occurrence of the
</bodyText>
<page confidence="0.974342">
1182
</page>
<bodyText confidence="0.9923375625">
predicate loss are missing, but they can be easily
inferred from the previous instance of the same
predicate.
(1) [arg0 The network] had been expected to have [np
losses] [arg1 of as much as $20 million] [arg3 on base-
ball this year]. It isn’t clear how much those [np losses]
may widen because of the short Series.
Therefore, we propose to exploit this property
in order to capture correctly how the fillers of all
predicate arguments evolve through a document.
Our algorithm, ImpAr, processes the docu-
ments sentence by sentence, assuming that se-
quences of the same predicate (in its nominal or
verbal form) share the same argument fillers (ex-
plicit or implicit)3. Thus, for every core argument
arg,,, of a predicate, ImpAr stores its previous
known filler as a default value. If the arguments
of a predicate are explicit, they always replace de-
fault fillers previously captured. When there is no
antecedent for a particular implicit argument arg,,,,
the algorithm tries to find in the surrounding con-
text which participant is the most likely to be the
filler according to some salience factors (see Sec-
tion 4.2). For the following instances, without an
explicit filler for a particular argument position,
the algorithm repeats the same selection process
and compares the new implicit candidate with the
default one. That is, the default implicit argument
of a predicate with no antecedent can change ev-
ery time the algorithm finds a filler with a greater
salience. A damping factor is applied to reduce the
salience of distant predicates.
</bodyText>
<subsectionHeader confidence="0.9050785">
4.2 Filling arguments without explicit
antecedents
</subsectionHeader>
<bodyText confidence="0.999969071428571">
Filling the implicit arguments of a predicate has
been identified as a particular case of corefer-
ence, very close to pronoun resolution (Silberer
and Frank, 2012). Consequently, for those implicit
arguments that have not explicit antecedents, we
propose an adaptation of a classic algorithm for
deterministic pronoun resolution. This component
of our algorithm follows the RAP approach (Lap-
pin and Leass, 1994). When our algorithm needs
to fill an implicit predicate argument without an
explicit antecedent it considers a set of candidates
within a window formed by the sentence of the
predicate and the two previous sentences. Then,
the algorithm performs the following steps:
</bodyText>
<listItem confidence="0.862708769230769">
3Note that the algorithm could also consider sequences of
closely related predicates.
1. Apply two constraints to the candidate list:
(a) All candidates that are already explicit arguments
of the predicate are ruled out.
(b) All candidates commanded by the predicate in
the dependency tree are ruled out.
2. Select those candidates that are semantically consistent
with the semantic category of the implicit argument.
3. Assign a salience score to each candidate.
4. Sort the candidates by their proximity to the predicate
of the implicit argument.
5. Select the candidate with the highest salience value.
</listItem>
<bodyText confidence="0.990918289473684">
As a result, the candidate with the highest
salience value is selected as the filler of the im-
plicit argument. Thus, this filler with its corre-
sponding salience weight will be also considered
in subsequent instances of the same predicate.
Now, we explain each step in more detail using
example (2). In this example, argo is missing for
the predicate plan:
(2) Quest Medical Inc said it adopted [arg1 a sharehold-
ers’ rights] [np plan] in which rights to purchase shares
of common stock will be distributed as a dividend to
shareholders of record as of Oct 23.
Filtering. In the first step, the algorithm fil-
ters out the candidates that are actual explicit argu-
ments of the predicate or have a syntactic depen-
dency with the predicate, and therefore, they are in
the search space of a traditional SRL system.
In our example, the filtering process would re-
move [a shareholders’ rights] because it is already
the explicit argument argi, and [in which rights
to purchase shares of common stock will be dis-
tributed as a dividend to shareholders of record as
of Oct 23] because it is syntactically commanded
by the predicate plan.
Semantic consistency. To determine the se-
mantic coherence between the potential candidates
and a predicate argument arg,,,, we have exploited
the selectional preferences in the same way as
in previous SRL and implicit argument resolution
works. First, we have designed a list of very
general semantic categories. Second, we have
semi-automatically assigned one of them to every
predicate argument arg,,, in PropBank and Nom-
Bank. For this, we have used the semantic an-
notation provided by the training documents of
the CoNLL-2008 dataset. This annotation was
performed automatically using the SuperSense-
Tagger (Ciaramita and Altun, 2006) and includes
</bodyText>
<page confidence="0.947255">
1183
</page>
<bodyText confidence="0.999458421052631">
named-entities and WordNet Super-Senses4. We
have also defined a mapping between the semantic
classes provided by the SuperSenseTagger and our
seven semantic categories (see Table 1 for more
details). Then, we have acquired the most com-
mon categories of each predicate argument arg,.
ImpAr algorithm also uses the SuperSenseTagger
over the documents to be processed from BNB
to check if the candidate belongs to the expected
semantic category of the implicit argument to be
filled.
Following the example above, [Quest Medi-
cal Inc] is tagged as an ORGANIZATION by the
SuperSenseTagger. Therefore, it belongs to our
semantic category COGNITIVE. As the seman-
tic category for the implicit argument argo for
the predicate plan has been recognized to be also
COGNITIVE, [Quest Medical Inc] remains in the
list of candidates as a possible filler.
</bodyText>
<figure confidence="0.904698473684211">
Semantic category Name-entities Super-Senses
PERSON noun.person
ORGANIZATION noun.group
ANIMAL noun.animal
... ...
PRODUCT noun.artifact
TANGIBLE SUBSTANCE noun.object
... ...
GAME noun.act
EVENTIVE DISEASE noun.communication
... ...
noun.shape
RELATIVE noun.attribute
...
LOCATIVE LOCATION noun.location
TIME DATE noun.time
QUANTITY noun.quantity
MESURABLE PERCENT
...
</figure>
<tableCaption confidence="0.9910975">
Table 1: Links between the semantic categories and some
name-entities and super-senses.
</tableCaption>
<bodyText confidence="0.999813642857143">
Salience weighting. In this process, the algo-
rithm assigns to each candidate a set of salience
factors that scores its prominence. The sentence
recency factor prioritizes the candidates that oc-
cur close to the same sentence of the predicate.
The subject, direct object, indirect object and non-
adverbial factors weight the salience of the candi-
date depending on the syntactic role they belong
to. Additionally, the head of these syntactic roles
are prioritized by the head factor. We have used
the same weights, listed in table 2, proposed by
Lappin and Leass (1994).
In the example, candidate [Quest Medical Inc]
is in the same sentence as the predicate plan, it
</bodyText>
<footnote confidence="0.851549">
4Lexicographic files according to WordNet terminology.
</footnote>
<table confidence="0.990888428571428">
Factor type weight
Sentence recency 100
Subject 80
Direct object 50
Indirect object 40
Head 80
Non-adverbial 50
</table>
<tableCaption confidence="0.999467">
Table 2: Weights assigned to each salience factor.
</tableCaption>
<bodyText confidence="0.965179">
belongs to a subject, and, indeed, it is the head
of that subject. Hence, the salience score for this
candidate is: 100 + 80 + 80 = 260.
</bodyText>
<subsectionHeader confidence="0.8419255">
4.3 Damping the salience of the default
candidate
</subsectionHeader>
<bodyText confidence="0.988418054054054">
As the algorithm maintains the default candidate
until an explicit filler appears, potential errors pro-
duced in the automatic selection process explained
above can spread to distant implicit instances, spe-
cially when the salience score of the default can-
didate is high. In order to reduce the impact of
these errors we have included a damping factor
that is applied sentence by sentence to the salience
value of the default candidate. ImpAr applies that
damping factor, r, as follows. It assumes that, in-
dependently of the initial salience assigned, 100
points of the salience score came from the sen-
tence recency factor. Then, the algorithm changes
this value multiplying it by r. So, given a salience
score s, the value of the score in a following sen-
tence, s&apos;, is:
s&apos; = s − 100 + 100 · r
Obviously, the value of r must be defined with-
out harming excessively those cases where the de-
fault candidate has been correctly identified. For
this, we studied in the training dataset the cases
of implicit arguments filled with the default can-
didate. Figure 1 shows that the influence of the
default filler is much higher in near sentences that
in more distance ones.
We tried to mimic a damping factor following
this distribution. That is, to maintain high score
salience for the near sentences while strongly de-
creasing them in the subsequent ones. In this way,
if the filler of the implicit argument is wrongly
identified, the error only spreads to the nearest in-
stances. If the identification is correct, a lower
score for more distance sentences is not too harm-
ful. The distribution shown in figure 1 follows
an exponential decay, therefore we have described
the damping factor as a curve like the following,
where α must be a value within 0 and 1:
</bodyText>
<equation confidence="0.263088">
COGNITIVE
</equation>
<page confidence="0.97489">
1184
</page>
<figureCaption confidence="0.995655">
Figure 1: Distances between the implicit argument and the
default candidate. The y axis indicate the percentage of cases
occurring in each sentence distance, expressed in x
</figureCaption>
<equation confidence="0.982585">
r = αd
</equation>
<bodyText confidence="0.999909">
In this function, d stands for the sentence dis-
tance and r for the damping factor to apply in that
sentence. In this paper, we have decided to set the
value of α to 0.5.
</bodyText>
<equation confidence="0.880412">
r = 0.5d
</equation>
<bodyText confidence="0.985786388888889">
This value maintains the influence of the default
fillers with high salience in near sentences. But it
decreases that influence strongly in the following.
In order to illustrate the whole process we will
use the previous example. In that case, [Quest
Medical Inc] is selected as the arg0 of plan with
a salience score of 260. Therefore [Quest Medi-
cal Inc] becomes the default arg0 of plan. In the
following sentence the damping factor is:
0.5 = 0.51
Therefore, its salience score changes to 260 −
100+100·0.5 = 210. Then, the algorithm changes
the default filler for arg0 only if it finds a candi-
date that scores higher in their current context. At
two sentence distance, the resulting score for the
default filler is 260 − 100 + 100 · 0.25 = 185. In
this way, at more distance sentences, the influence
of the default filler of arg0 becomes smaller.
</bodyText>
<sectionHeader confidence="0.998641" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.99990275">
In order to evaluate the performance of the Im-
pAr algorithm, we have followed the evaluation
method presented by Gerber and Chai (2010,
2012). For every argument position in the gold-
standard the scorer expects a single predicted con-
stituent to fill in. In order to evaluate the correct
span of a constituent, a prediction is scored using
the Dice coefficient:
</bodyText>
<equation confidence="0.935356">
2|Predicted ∩ True|
|Predicted |+ |True|
</equation>
<bodyText confidence="0.9999739375">
The function above relates the set of tokens that
form a predicted constituent, Predicted, and the
set of tokens that are part of an annotated con-
stituent in the gold-standard, True. For each
missing argument, the gold-standard includes the
whole coreference chain of the filler. Therefore,
the scorer selects from all coreferent mentions the
highest Dice value. If the predicted span does not
cover the head of the annotated filler, the scorer re-
turns zero. Then, Precision is calculated by the
sum of all prediction scores divided by the number
of attempts carried out by the system. Recall is
equal to the sum of the prediction scores divided
by the number of actual annotations in the gold-
standard. F-measure is calculated as the harmonic
mean of recall and precision.
Traditionally, there have been two approaches
to develop SRL systems, one based on constituent
trees and the other one based on syntactic depen-
dencies. Additionally, the evaluation of both types
of systems has been performed differently. For
constituent based SRL systems the scorers eval-
uate the correct span of the filler, while for depen-
dency based systems the scorer just check if the
systems are able to capture the head token of the
filler. As shown above, previous works in implicit
argument resolution proposed a metric that in-
volves the correct identification of the whole span
of the filler. ImpAr algorithm works with syntac-
tic dependencies and therefore it only returns the
head token of the filler. In order to compare our
results with previous works, we had to apply some
simple heuristics to guess the correct span of the
filler. Obviously, this process inserts some noise
in the final evaluation.
We have performed a first evaluation over the
test set used in (Gerber and Chai, 2010). This
dataset contains 437 predicate instances but just
246 argument positions are implicitly filled. Table
3 includes the results obtained by ImpAr, the re-
sults of the system presented by Gerber and Chai
(2010) and the baseline proposed for the task. Best
results are marked in bold5. For all predicates,
ImpAr improves over the baseline (19.3 points
higher in the overall F1). Our system also out-
performs the one presented by Gerber and Chai
(2010). Interestingly, both systems present very
different performances predicate by predicate. For
</bodyText>
<footnote confidence="0.9499815">
5No proper significance test can be carried out without the
the full predictions of all systems involved.
</footnote>
<page confidence="0.964546">
1185
</page>
<table confidence="0.999895692307692">
Baseline Gerber &amp; Chai ImpAr
#Inst. #Imp. F1 P R F1 P R F1
sale 64 65 36.2 47.2 41.7 44.2 41.2 39.4 40.3
price 121 53 15.4 36.0 32.6 34.2 53.3 53.3 53.3
investor 78 35 9.8 36.8 40.0 38.4 43.0 39.5 41.2
bid 19 26 32.3 23.8 19.2 21.3 52.9 51.0 52.0
plan 25 20 38.5 78.6 55.0 64.7 40.7 40.7 40.7
cost 25 17 34.8 61.1 64.7 62.9 56.1 50.2 53.0
loss 30 12 52.6 83.3 83.3 83.3 68.4 63.5 65.8
loan 11 9 18.2 42.9 33.3 37.5 25.0 20.0 22.2
investment 21 8 0.0 40.0 25.0 30.8 47.6 35.7 40.8
fund 43 6 0.0 14.3 16.7 15.4 66.7 33.3 44.4
Overall 437 246 26.5 44.5 40.4 42.3 47.9 43.8 45.8
</table>
<tableCaption confidence="0.999568">
Table 3: Evaluation with the test. The results from (Gerber and Chai, 2010) are included.
</tableCaption>
<table confidence="0.999952384615385">
Baseline Gerber &amp; Chai ImpAr
#Inst. #Imp. F1 P R F1 P R F1
sale 184 181 37.3 59.2 44.8 51.0 44.3 43.3 43.8
price 216 138 34.6 56.0 48.7 52.1 55.0 54.5 54.7
investor 160 108 5.1 46.7 39.8 43.0 28.2 27.0 27.6
bid 88 124 23.8 60.0 36.3 45.2 48.4 41.8 45.0
plan 100 77 32.3 59.6 44.1 50.7 47.0 47.0 47.0
cost 101 86 17.8 62.5 50.9 56.1 49.2 43.7 46.2
loss 104 62 54.7 72.5 59.7 65.5 63.0 58.2 60.5
loan 84 82 31.2 67.2 50.0 57.3 56.4 45.6 50.6
investment 102 52 15.5 32.9 34.2 33.6 41.2 30.9 35.4
fund 108 56 15.5 80.0 35.7 49.4 55.6 44.6 49.5
Overall 1,247 966 28.9 57.9 44.5 50.3 47.7 43.0 45.3
</table>
<tableCaption confidence="0.999982">
Table 4: Evaluation with the full dataset. The results from (Gerber and Chai, 2012) are included.
</tableCaption>
<bodyText confidence="0.999970475">
instance, our system obtains much higher results
for the predicates bid and fund, while much lower
for loss and loan. In general, ImpAr seems to be
more robust since it obtains similar performances
for all predicates. In fact, the standard deviation,
Q , of F1 measure is 10.98 for ImpAr while this
value for the (Gerber and Chai, 2010) system is
20.00.
In a more recent work, Gerber and Chai (2012)
presented some improvements of their previous
results. In this work, they extended the evalua-
tion of their model using the whole dataset and
not just the testing documents. Applying a cross-
validated approach they tried to solve some prob-
lems that they found in the previous evaluation,
like the small size of the testing set. For this work,
they also studied a wider set of features, specially,
they experimented with some statistics learnt from
parts of GigaWord automatically annotated. Table
4 shows that the improvement over their previous
system was remarkable. The system also seems
to be more stable across predicates. For compar-
ison purposes, we also included the performance
of ImpAr applied over the whole dataset.
The results in table 4 show that, although ImpAr
still achieves the best results in some cases, this
time, it cannot beat the overall results obtained by
the supervised model. In fact, both systems obtain
a very similar recall, but the system from (Gerber
and Chai, 2012) obtains much higher precision.
In both cases, the Q value of F1 is reduced, 8.81
for ImpAr and 8.21 for (Gerber and Chai, 2012).
However, ImpAr obtains very similar performance
independently of the testing dataset what proves
the robustness of the algorithm. This suggests
that our algorithm can obtain strong results also
for other corpus and predicates. Instead, the su-
pervised approach would need a large amount of
manual annotations for every predicate to be pro-
cessed.
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999969">
6.1 Component Analysis
</subsectionHeader>
<bodyText confidence="0.9999315">
In order to assess the contribution of each sys-
tem component, we also tested the performance
of ImpAr algorithm when disabling only one of
its components. With this evaluations we pretend
to sight the particular contribution of each compo-
nent. In table 5 we present the results obtained in
the following experiments for the two testing sets
explained in section 5:
</bodyText>
<listItem confidence="0.997325">
• Exp1: The damping factor is disabled. All se-
lected fillers maintain the same salience over
</listItem>
<page confidence="0.947253">
1186
</page>
<bodyText confidence="0.514576">
all sentences.
</bodyText>
<listItem confidence="0.99028675">
• Exp2: Only explicit fillers are considered as
candidates6.
• Exp3: No default fillers are considered as
candidates.
</listItem>
<bodyText confidence="0.999834111111111">
As expected, we observe a very similar perfor-
mances in both datasets. Additionally, the high-
est loss appears when the default fillers are ruled
out (Exp3). In particular, it also seems that the
explicit information from previous predicates pro-
vides the most correct evidence (Exp2). Also note
that for Exp2, the system obtains the highest preci-
sion. This means that the most accurate cases are
obtained by previous explicit antecedents.
</bodyText>
<table confidence="0.999452444444444">
test full
P R Fl P R Fl
full 47.9 43.8 45.8 47.7 43.0 45.3
Exp1 45.7 41.8 43.6 47.1 42.5 44.8
Exp2 51.2 24.6 33.2 55.3 25.5 34.9
Exp3 34.6 29.7 31.9 34.8 28.9 31.5
Exp4 42.6 37.9 40.1 37.5 31.2 34.1
Exp5 38.8 34.5 36.5 35.7 29.7 32.4
Exp6 53.3 48.7 50.9 52.4 47.2 49.6
</table>
<tableCaption confidence="0.98519625">
Table 5: Exp1, Exp2 and Exp3 correspond to ablations of the
components. Exp3 and Exp4 are experiments over the cases
that are not solved by explicit antecedents. Exp6 evaluates
the system capturing just the head tokens of the constituents.
</tableCaption>
<bodyText confidence="0.9942085">
As Exp1 also includes instances with explicit
antecedents, and for these cases the damping fac-
tor component has no effect, we have designed two
additional experiments:
</bodyText>
<listItem confidence="0.986515">
• Exp4: Full system for the cases not solved by
explicit antecedents.
• Exp5: As in Exp4 but with the damping fac-
tor disabled.
</listItem>
<bodyText confidence="0.999822666666667">
As expected, now the contribution of the dump-
ing factor seems to be more relevant, in particular,
for the test dataset.
</bodyText>
<subsectionHeader confidence="0.999643">
6.2 Correct span of the fillers
</subsectionHeader>
<bodyText confidence="0.9997645">
As explained in Section 5, our algorithm works
with syntactic dependencies and its predictions
only return the head token of the filler. Obtaining
the correct constituents from syntactic dependen-
cies is not trivial. In this work we have applied
a simple heuristic that returns all the descendant
</bodyText>
<footnote confidence="0.946116">
6That is, implicit arguments without explicit antecedents
are not filled.
</footnote>
<bodyText confidence="0.999064388888889">
tokens of the predicted head token. This naive
process inserts some noise to the evaluation of the
system. For example, from the following sentence
our system gives the following prediction for an
implicit argi of an instance of the predicate sale:
Ports of Call Inc. reached agreements to sell its re-
maining seven aircraft [argl to buyers] that weren’t
disclosed.
But the actual gold-standard annotation is:
[argi buyers that weren’t disclosed]. Although the
head of the constituent, buyers, is correctly cap-
tured by ImpAr, the final prediction is heavily pe-
nalized by the scoring method. Table 5 presents
the results of ImpAr when evaluating the head to-
kens of the constituents only (Exp6). These results
show that the current performance of our system
can be easily improved applying a more accurate
process for capturing the correct span.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999983517241379">
In this work we have presented a robust determin-
istic approach for implicit Semantic Role Label-
ing. The method exploits a very simple but rel-
evant discoursive coherence property that holds
over explicit and implicit arguments of closely re-
lated nominal and verbal predicates. This prop-
erty states that if several instances of the same
predicate appear in a well-written discourse, it is
very likely that they maintain the same argument
fillers. We have shown the importance of this phe-
nomenon for recovering the implicit information
about semantic roles. To our knowledge, this is the
first empirical study that proves this phenomenon.
Based on these observations, we have devel-
oped a new deterministic algorithm, ImpAr, that
obtains very competitive and robust performances
with respect to supervised approaches. That is, it
can be applied where there is no available manual
annotations to train. The code of this algorithm is
publicly available and can be applied to any docu-
ment. As input it only needs the document with
explicit semantic role labeling and Super-Sense
annotations. These annotations can be easily ob-
tained from plain text using available tools7, what
makes this algorithm the first effective tool avail-
able for implicit SRL.
As it can be easily seen, ImpAr has a large
margin for improvement. For instance, providing
more accurate spans for the fillers. We also plan
</bodyText>
<footnote confidence="0.9940955">
7We recommend mate-tools (Bj¨orkelund et al., 2009) and
SuperSenseTagger (Ciaramita and Altun, 2006).
</footnote>
<page confidence="0.994936">
1187
</page>
<bodyText confidence="0.999958857142857">
to test alternative approaches to solve the argu-
ments without explicit antecedents. For instance,
our system can also profit from additional annota-
tions like coreference, that has proved its utility in
previous works. Finally, we also plan to study our
approach on different languages and datasets (for
instance, the SemEval-2010 dataset).
</bodyText>
<sectionHeader confidence="0.995567" genericHeader="acknowledgments">
8 Acknowledgment
</sectionHeader>
<bodyText confidence="0.9988244">
We are grateful to the anonymous reviewers
for their insightful comments. This work has
been partially funded by SKaTer (TIN2012-
38584-C06-02), OpeNER (FP7-ICT-2011-SME-
DCL-296451) and NewsReader (FP7-ICT-2011-
8-316404), as well as the READERS project
with the financial support of MINECO, ANR
(convention ANR-12-CHRI-0004-03) and EPSRC
(EP/K017845/1) in the framework of ERA-NET
CHIST-ERA (UE FP7/2007-2013).
</bodyText>
<sectionHeader confidence="0.972101" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.942654733333333">
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998).
The berkeley framenet project. In Proceedings
of the 36th Annual Meeting of the Association
for Computational Linguistics and 17th Inter-
national Conference on Computational Linguis-
tics, ACL ’98, Montreal, Quebec, Canada, pp.
86–90.
Bj¨orkelund, A., L. Hafdell, and P. Nugues (2009).
Multilingual semantic role labeling. In Pro-
ceedings of the Thirteenth Conference on Com-
putational Natural Language Learning: Shared
Task, CoNLL ’09, Boulder, Colorado, USA, pp.
43–48.
Carreras, X. and L. M`arquez (2005). Introduction
to the conll-2005 shared task: Semantic role la-
</bodyText>
<reference confidence="0.981217354838709">
beling. In Proceedings of the 9th Conference
on Computational Natural Language Learning,
CoNLL ’05, Ann Arbor, Michigan, USA, pp.
152–164.
Chen, D., N. Schneider, D. Das, and N. A. Smith
(2010). Semafor: Frame argument resolution
with log-linear models. In Proceedings of the
5th International Workshop on Semantic Eval-
uation, SemEval ’10, Los Angeles, California,
USA, pp. 264–267.
Ciaramita, M. and Y. Altun (2006). Broad-
coverage sense disambiguation and information
extraction with a supersense sequence tagger. In
Proceedings of the 2006 Conference on Empir-
ical Methods in Natural Language Processing,
EMNLP ’06, Sydney, Australia, pp. 594–602.
Dahl, D. A., M. S. Palmer, and R. J. Passonneau
(1987). Nominalizations in pundit. In In Pro-
ceedings of the 25th Annual Meeting of the As-
sociation for Computational Linguistics, ACL
’87, Stanford, California, USA, pp. 131–139.
Fellbaum, C. (1998). WordNet: an electronic lexi-
cal database. MIT Press.
Gerber, M. and J. Chai (2012, December). Se-
mantic role labeling of implicit arguments for
nominal predicates. Computational Linguis-
tics 38(4), 755–798.
Gerber, M. and J. Y. Chai (2010). Beyond nom-
bank: a study of implicit arguments for nomi-
nal predicates. In Proceedings of the 48th An-
nual Meeting of the Association for Computa-
tional Linguistics, ACL ’10, Uppsala, Sweden,
pp. 1583–1592.
Gildea, D. and D. Jurafsky (2000). Automatic la-
beling of semantic roles. In Proceedings of the
38th Annual Meeting on Association for Com-
putational Linguistics, ACL ’00, Hong Kong,
pp. 512–520.
Gildea, D. and D. Jurafsky (2002, September).
Automatic labeling of semantic roles. Compu-
tational Linguistics 28(3), 245–288.
Gorinski, P., J. Ruppenhofer, and C. Sporleder
(2013). Towards weakly supervised resolution
of null instantiations. In Proceedings of the 10th
International Conference on Computational Se-
mantics, IWCS ’13, Potsdam, Germany, pp.
119–130.
Hajiˇc, J., M. Ciaramita, R. Johansson, D. Kawa-
hara, M. A. Marti, L. M`arquez, A. Meyers,
J. Nivre, S. Pad´o, J. ˇStˇep´anek, P. Straˇn´ak,
M. Surdeanu, N. Xue, and Y. Zhang (2009).
The CoNLL-2009 shared task: Syntactic and
semantic dependencies in multiple languages.
In Proceedings of the Thirteenth Conference
on Computational Natural Language Learning:
Shared Task, CoNLL ’09, Boulder, Colorado,
USA, pp. 1–18.
Laparra, E. and G. Rigau (2012). Exploiting ex-
plicit annotations and semantic types for im-
plicit argument resolution. In 6th IEEE Inter-
national Conference on Semantic Computing,
ICSC ’12, Palermo, Italy, pp. 75–78.
</reference>
<page confidence="0.881342">
1188
</page>
<reference confidence="0.994805682352942">
Laparra, E. and G. Rigau (2013). Sources of evi-
dence for implicit argument resolution. In Pro-
ceedings of the 10th International Conference
on Computational Semantics, IWCS ’13, Pots-
dam, Germany, pp. 155–166.
Lappin, S. and H. J. Leass (1994, December). An
algorithm for pronominal anaphora resolution.
Computational Linguistics 20(4), 535–561.
Meyers, A., R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman
(2004). The nombank project: An interim re-
port. In In Proceedings of the NAACL/HLT
Workshop on Frontiers in Corpus Annota-
tion, HLT-NAACL ’04, Boston, Massachusetts,
USA, pp. 24–31.
Moor, T., M. Roth, and A. Frank (2013).
Predicate-specific annotations for implicit role
binding: Corpus annotation, data analysis and
evaluation experiments. In Proceedings of
the 10th International Conference on Compu-
tational Semantics, IWCS ’13, Potsdam, Ger-
many, pp. 369–375.
Palmer, M., D. Gildea, and P. Kingsbury (2005,
March). The proposition bank: An annotated
corpus of semantic roles. Computational Lin-
guistics 31(1), 71–106.
Palmer, M. S., D. A. Dahl, R. J. Schiffman,
L. Hirschman, M. Linebarger, and J. Dowding
(1986). Recovering implicit information. In
Proceedings of the 24th annual meeting on As-
sociation for Computational Linguistics, ACL
’86, New York, New York, USA, pp. 10–19.
Raghunathan, K., H. Lee, S. Rangarajan,
N. Chambers, M. Surdeanu, D. Jurafsky, and
C. Manning (2010). A multi-pass sieve for
coreference resolution. In Proceedings of the
2010 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’10, Cam-
bridge, Massachusetts, USA, pp. 492–501.
Ruppenhofer, J., P. Gorinski, and C. Sporleder
(2011). In search of missing arguments: A lin-
guistic approach. In Proceedings of the Inter-
national Conference Recent Advances in Nat-
ural Language Processing 2011, RANLP ’11,
Hissar, Bulgaria, pp. 331–338.
Ruppenhofer, J., C. Sporleder, R. Morante,
C. Baker, and M. Palmer (2010). Semeval-2010
task 10: Linking events and their participants
in discourse. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, Se-
mEval ’10, Los Angeles, California, USA, pp.
45–50.
Silberer, C. and A. Frank (2012). Casting implicit
role linking as an anaphora resolution task. In
Proceedings of the First Joint Conference on
Lexical and Computational Semantics, *SEM
’12, Montr´eal, Canada, pp. 1–10.
Surdeanu, M., R. Johansson, A. Meyers,
L. M`arquez, and J. Nivre (2008). The CoNLL-
2008 shared task on joint parsing of syntactic
and semantic dependencies. In Proceedings of
the Twelfth Conference on Natural Language
Learning, CoNLL ’08, Manchester, United
Kingdom, pp. 159–177.
Tetreault, J. R. (2002). Implicit role reference.
In International Symposium on Reference Res-
olution for Natural Language Processing, Ali-
cante, Spain, pp. 109–115.
Tonelli, S. and R. Delmonte (2010). Venses++:
Adapting a deep semantic processing system to
the identification of null instantiations. In Pro-
ceedings of the 5th International Workshop on
Semantic Evaluation, SemEval ’10, Los Ange-
les, California, USA, pp. 296–299.
Tonelli, S. and R. Delmonte (2011). Desperately
seeking implicit arguments in text. In Proceed-
ings of the ACL 2011 Workshop on Relational
Models of Semantics, RELMS ’11, Portland,
Oregon, USA, pp. 54–62.
Whittemore, G., M. Macpherson, and G. Carlson
(1991). Event-building through role-filling and
anaphora resolution. In Proceedings of the 29th
annual meeting on Association for Computa-
tional Linguistics, ACL ’91, Berkeley, Califor-
nia, USA, pp. 17–24.
</reference>
<page confidence="0.996785">
1189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482235">
<title confidence="0.999523">ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</title>
<author confidence="0.734829">Egoitz</author>
<affiliation confidence="0.932230666666667">IXA University of the Basque San Sebastian,</affiliation>
<email confidence="0.965253">egoitz.laparra@ehu.es</email>
<author confidence="0.975081">German</author>
<affiliation confidence="0.947531">IXA University of the Basque San Sebastian,</affiliation>
<email confidence="0.995025">german.rigau@ehu.es</email>
<abstract confidence="0.999824866666667">This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>beling</author>
</authors>
<booktitle>In Proceedings of the 9th Conference on Computational Natural Language Learning, CoNLL ’05,</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, Michigan, USA,</location>
<marker>beling, </marker>
<rawString>beling. In Proceedings of the 9th Conference on Computational Natural Language Learning, CoNLL ’05, Ann Arbor, Michigan, USA, pp. 152–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chen</author>
<author>N Schneider</author>
<author>D Das</author>
<author>N A Smith</author>
</authors>
<title>Semafor: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>264--267</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="7214" citStr="Chen et al., 2010" startWordPosition="1127" endWordPosition="1130">on challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These</context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Chen, D., N. Schneider, D. Das, and N. A. Smith (2010). Semafor: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, Los Angeles, California, USA, pp. 264–267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broadcoverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="17731" citStr="Ciaramita and Altun, 2006" startWordPosition="2814" endWordPosition="2817">onsistency. To determine the semantic coherence between the potential candidates and a predicate argument arg,,,, we have exploited the selectional preferences in the same way as in previous SRL and implicit argument resolution works. First, we have designed a list of very general semantic categories. Second, we have semi-automatically assigned one of them to every predicate argument arg,,, in PropBank and NomBank. For this, we have used the semantic annotation provided by the training documents of the CoNLL-2008 dataset. This annotation was performed automatically using the SuperSenseTagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses4. We have also defined a mapping between the semantic classes provided by the SuperSenseTagger and our seven semantic categories (see Table 1 for more details). Then, we have acquired the most common categories of each predicate argument arg,. ImpAr algorithm also uses the SuperSenseTagger over the documents to be processed from BNB to check if the candidate belongs to the expected semantic category of the implicit argument to be filled. Following the example above, [Quest Medical Inc] is tagged as an ORGANIZATION by the SuperSenseTagg</context>
<context position="33911" citStr="Ciaramita and Altun, 2006" startWordPosition="5547" endWordPosition="5550">ailable manual annotations to train. The code of this algorithm is publicly available and can be applied to any document. As input it only needs the document with explicit semantic role labeling and Super-Sense annotations. These annotations can be easily obtained from plain text using available tools7, what makes this algorithm the first effective tool available for implicit SRL. As it can be easily seen, ImpAr has a large margin for improvement. For instance, providing more accurate spans for the fillers. We also plan 7We recommend mate-tools (Bj¨orkelund et al., 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). 1187 to test alternative approaches to solve the arguments without explicit antecedents. For instance, our system can also profit from additional annotations like coreference, that has proved its utility in previous works. Finally, we also plan to study our approach on different languages and datasets (for instance, the SemEval-2010 dataset). 8 Acknowledgment We are grateful to the anonymous reviewers for their insightful comments. This work has been partially funded by SKaTer (TIN2012- 38584-C06-02), OpeNER (FP7-ICT-2011-SMEDCL-296451) and NewsReader (FP7-ICT-2011- 8-316404), as well as the</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Ciaramita, M. and Y. Altun (2006). Broadcoverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, Sydney, Australia, pp. 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Dahl</author>
<author>M S Palmer</author>
<author>R J Passonneau</author>
</authors>
<title>Nominalizations in pundit. In</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, ACL ’87,</booktitle>
<pages>131--139</pages>
<location>Stanford, California, USA,</location>
<contexts>
<context position="4086" citStr="Dahl et al. (1987)" startWordPosition="639" endWordPosition="642">enerally applicable tools. This problem has become a main concern for many NLP tasks. This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al., 2010). In this work, we study the coherence of the predicate and argument realization in discourse. In particular, we have followed a similar approach to 1180 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1180–1189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics the one proposed by Dahl et al. (1987) who filled the arguments of anaphoric mentions of nominal predicates using previous mentions of the same predicate. We present an extension of this idea assuming that in a coherent document the different ocurrences of a predicate, including both verbal and nominal forms, tend to be mentions of the same event, and thus, they share the same argument fillers. Following this approach, we have developed a deterministic algorithm that obtains competitive results with respect to supervised methods. That is, our system can be applied to any predicate without training data. The main contributions of t</context>
</contexts>
<marker>Dahl, Palmer, Passonneau, 1987</marker>
<rawString>Dahl, D. A., M. S. Palmer, and R. J. Passonneau (1987). Nominalizations in pundit. In In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, ACL ’87, Stanford, California, USA, pp. 131–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7096" citStr="Fellbaum, 1998" startWordPosition="1110" endWordPosition="1111">ne hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (20</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998). WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gerber</author>
<author>J Chai</author>
</authors>
<title>Semantic role labeling of implicit arguments for nominal predicates.</title>
<date>2012</date>
<journal>Computational Linguistics</journal>
<volume>38</volume>
<issue>4</issue>
<pages>755--798</pages>
<contexts>
<context position="10690" citStr="Gerber and Chai (2012)" startWordPosition="1670" endWordPosition="1673">xploited independently from the predicate. 3 Datasets In our experiments, we have focused on the dataset developed in Gerber and Chai (2010, 2012). This dataset (hereinafter BNB which stands for ”Beyond NomBank”) extends existing predicate annotations for NomBank and ProbBank. BNB presented the first annotation work of implicit arguments based on PropBank and NomBank frames. This annotation was an extension of the standard training, development and testing sections of Penn TreeBank that have been typically used for SRL evaluation and were already annotated with PropBank and NomBank predicate 2Gerber and Chai (2012) includes a set of 81 different features. structures. The authors selected a limited set of predicates. These predicates are all nominalizations of other verbal predicates, without sense ambiguity, that appear frequently in the corpus and tend to have implicit arguments associated with their instances. These constraints allowed them to model enough occurrences of each implicit argument in order to cover adequately all the possible cases appearing in a test document. For each missing argument position they went over all the preceding sentences and annotated all mentions of the filler of that ar</context>
<context position="27270" citStr="Gerber and Chai, 2012" startWordPosition="4451" endWordPosition="4454">R F1 sale 184 181 37.3 59.2 44.8 51.0 44.3 43.3 43.8 price 216 138 34.6 56.0 48.7 52.1 55.0 54.5 54.7 investor 160 108 5.1 46.7 39.8 43.0 28.2 27.0 27.6 bid 88 124 23.8 60.0 36.3 45.2 48.4 41.8 45.0 plan 100 77 32.3 59.6 44.1 50.7 47.0 47.0 47.0 cost 101 86 17.8 62.5 50.9 56.1 49.2 43.7 46.2 loss 104 62 54.7 72.5 59.7 65.5 63.0 58.2 60.5 loan 84 82 31.2 67.2 50.0 57.3 56.4 45.6 50.6 investment 102 52 15.5 32.9 34.2 33.6 41.2 30.9 35.4 fund 108 56 15.5 80.0 35.7 49.4 55.6 44.6 49.5 Overall 1,247 966 28.9 57.9 44.5 50.3 47.7 43.0 45.3 Table 4: Evaluation with the full dataset. The results from (Gerber and Chai, 2012) are included. instance, our system obtains much higher results for the predicates bid and fund, while much lower for loss and loan. In general, ImpAr seems to be more robust since it obtains similar performances for all predicates. In fact, the standard deviation, Q , of F1 measure is 10.98 for ImpAr while this value for the (Gerber and Chai, 2010) system is 20.00. In a more recent work, Gerber and Chai (2012) presented some improvements of their previous results. In this work, they extended the evaluation of their model using the whole dataset and not just the testing documents. Applying a c</context>
<context position="28681" citStr="Gerber and Chai, 2012" startWordPosition="4688" endWordPosition="4691">features, specially, they experimented with some statistics learnt from parts of GigaWord automatically annotated. Table 4 shows that the improvement over their previous system was remarkable. The system also seems to be more stable across predicates. For comparison purposes, we also included the performance of ImpAr applied over the whole dataset. The results in table 4 show that, although ImpAr still achieves the best results in some cases, this time, it cannot beat the overall results obtained by the supervised model. In fact, both systems obtain a very similar recall, but the system from (Gerber and Chai, 2012) obtains much higher precision. In both cases, the Q value of F1 is reduced, 8.81 for ImpAr and 8.21 for (Gerber and Chai, 2012). However, ImpAr obtains very similar performance independently of the testing dataset what proves the robustness of the algorithm. This suggests that our algorithm can obtain strong results also for other corpus and predicates. Instead, the supervised approach would need a large amount of manual annotations for every predicate to be processed. 6 Discussion 6.1 Component Analysis In order to assess the contribution of each system component, we also tested the performa</context>
</contexts>
<marker>Gerber, Chai, 2012</marker>
<rawString>Gerber, M. and J. Chai (2012, December). Semantic role labeling of implicit arguments for nominal predicates. Computational Linguistics 38(4), 755–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gerber</author>
<author>J Y Chai</author>
</authors>
<title>Beyond nombank: a study of implicit arguments for nominal predicates.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1583--1592</pages>
<location>Uppsala,</location>
<contexts>
<context position="3178" citStr="Gerber and Chai (2010)" startWordPosition="501" endWordPosition="504">s of record as of Oct 23. For the predicate plan in the previous sentence, a traditional SRL process only returns the filler for the argument arg1, the theme of the plan. However, in both examples, a reader could easily infer the missing arguments from the surrounding context of the predicate, and determine that in (1) both instances of the predicate share the same arguments and in (2) the missing argument corresponds to the subject of the verb that dominates the predicate, Quest Medical Inc. Obviously, this additional annotations could contribute positively to its semantic analysis. In fact, Gerber and Chai (2010) pointed out that implicit arguments can increase the coverage of argument structures in NomBank by 71%. However, current automatic systems require large amounts of manually annotated training data for each predicate. The effort required for this manual annotation explains the absence of generally applicable tools. This problem has become a main concern for many NLP tasks. This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al., 2010). In this work, we study the coherence of the predicate and argument real</context>
<context position="8295" citStr="Gerber and Chai (2010" startWordPosition="1291" endWordPosition="1294"> Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that were already annotated for PropBank (Palmer et al., 2005) and NomBank. This allowed them to avoid the sparseness problems and generalize properly from the training set. The r</context>
<context position="10207" citStr="Gerber and Chai (2010" startWordPosition="1595" endWordPosition="1598">olve each instance of the implicit arguments independently, without taking into account the previous realizations of the same implicit argument in the document. We propose that these realizations, together with the explicit ones, must maintain a certain coherence along the document and, in consequence, the filler of an argument remains the same along the following instances of that argument until a stronger evidence indicates a change. We also propose that this feature can be exploited independently from the predicate. 3 Datasets In our experiments, we have focused on the dataset developed in Gerber and Chai (2010, 2012). This dataset (hereinafter BNB which stands for ”Beyond NomBank”) extends existing predicate annotations for NomBank and ProbBank. BNB presented the first annotation work of implicit arguments based on PropBank and NomBank frames. This annotation was an extension of the standard training, development and testing sections of Penn TreeBank that have been typically used for SRL evaluation and were already annotated with PropBank and NomBank predicate 2Gerber and Chai (2012) includes a set of 81 different features. structures. The authors selected a limited set of predicates. These predica</context>
<context position="11635" citStr="Gerber and Chai (2010" startWordPosition="1825" endWordPosition="1828">lowed them to model enough occurrences of each implicit argument in order to cover adequately all the possible cases appearing in a test document. For each missing argument position they went over all the preceding sentences and annotated all mentions of the filler of that argument. In tables 3 and 4 we show the list of predicates and the resulting figures of this annotation. In this work we also use the corpus provided for the CoNLL-2008 task. These corpora cover the same BNB documents and include annotated predictions for syntactic dependencies and SuperSense labels as semantic tags. Unlike Gerber and Chai (2010, 2012) we do not use the constituent analysis from the Penn TreeBank. 4 ImpAr algorithm 4.1 Discoursive coherence of predicates Exploring the training dataset of BNB, we observed a very strong discourse effect on the implicit and explicit argument fillers of the predicates. That is, if several instances of the same predicate appear in a well-written discourse, it is very likely that they maintain the same argument fillers. This property holds when joining the different parts-of-speech of the predicates (nominal or verbal) and the explicit or implicit realizations of the argument fillers. For </context>
<context position="23234" citStr="Gerber and Chai (2010" startWordPosition="3730" endWordPosition="3733">arg0 of plan. In the following sentence the damping factor is: 0.5 = 0.51 Therefore, its salience score changes to 260 − 100+100·0.5 = 210. Then, the algorithm changes the default filler for arg0 only if it finds a candidate that scores higher in their current context. At two sentence distance, the resulting score for the default filler is 260 − 100 + 100 · 0.25 = 185. In this way, at more distance sentences, the influence of the default filler of arg0 becomes smaller. 5 Evaluation In order to evaluate the performance of the ImpAr algorithm, we have followed the evaluation method presented by Gerber and Chai (2010, 2012). For every argument position in the goldstandard the scorer expects a single predicted constituent to fill in. In order to evaluate the correct span of a constituent, a prediction is scored using the Dice coefficient: 2|Predicted ∩ True| |Predicted |+ |True| The function above relates the set of tokens that form a predicted constituent, Predicted, and the set of tokens that are part of an annotated constituent in the gold-standard, True. For each missing argument, the gold-standard includes the whole coreference chain of the filler. Therefore, the scorer selects from all coreferent men</context>
<context position="25273" citStr="Gerber and Chai, 2010" startWordPosition="4070" endWordPosition="4073">heck if the systems are able to capture the head token of the filler. As shown above, previous works in implicit argument resolution proposed a metric that involves the correct identification of the whole span of the filler. ImpAr algorithm works with syntactic dependencies and therefore it only returns the head token of the filler. In order to compare our results with previous works, we had to apply some simple heuristics to guess the correct span of the filler. Obviously, this process inserts some noise in the final evaluation. We have performed a first evaluation over the test set used in (Gerber and Chai, 2010). This dataset contains 437 predicate instances but just 246 argument positions are implicitly filled. Table 3 includes the results obtained by ImpAr, the results of the system presented by Gerber and Chai (2010) and the baseline proposed for the task. Best results are marked in bold5. For all predicates, ImpAr improves over the baseline (19.3 points higher in the overall F1). Our system also outperforms the one presented by Gerber and Chai (2010). Interestingly, both systems present very different performances predicate by predicate. For 5No proper significance test can be carried out without</context>
<context position="26579" citStr="Gerber and Chai, 2010" startWordPosition="4311" endWordPosition="4314"> #Inst. #Imp. F1 P R F1 P R F1 sale 64 65 36.2 47.2 41.7 44.2 41.2 39.4 40.3 price 121 53 15.4 36.0 32.6 34.2 53.3 53.3 53.3 investor 78 35 9.8 36.8 40.0 38.4 43.0 39.5 41.2 bid 19 26 32.3 23.8 19.2 21.3 52.9 51.0 52.0 plan 25 20 38.5 78.6 55.0 64.7 40.7 40.7 40.7 cost 25 17 34.8 61.1 64.7 62.9 56.1 50.2 53.0 loss 30 12 52.6 83.3 83.3 83.3 68.4 63.5 65.8 loan 11 9 18.2 42.9 33.3 37.5 25.0 20.0 22.2 investment 21 8 0.0 40.0 25.0 30.8 47.6 35.7 40.8 fund 43 6 0.0 14.3 16.7 15.4 66.7 33.3 44.4 Overall 437 246 26.5 44.5 40.4 42.3 47.9 43.8 45.8 Table 3: Evaluation with the test. The results from (Gerber and Chai, 2010) are included. Baseline Gerber &amp; Chai ImpAr #Inst. #Imp. F1 P R F1 P R F1 sale 184 181 37.3 59.2 44.8 51.0 44.3 43.3 43.8 price 216 138 34.6 56.0 48.7 52.1 55.0 54.5 54.7 investor 160 108 5.1 46.7 39.8 43.0 28.2 27.0 27.6 bid 88 124 23.8 60.0 36.3 45.2 48.4 41.8 45.0 plan 100 77 32.3 59.6 44.1 50.7 47.0 47.0 47.0 cost 101 86 17.8 62.5 50.9 56.1 49.2 43.7 46.2 loss 104 62 54.7 72.5 59.7 65.5 63.0 58.2 60.5 loan 84 82 31.2 67.2 50.0 57.3 56.4 45.6 50.6 investment 102 52 15.5 32.9 34.2 33.6 41.2 30.9 35.4 fund 108 56 15.5 80.0 35.7 49.4 55.6 44.6 49.5 Overall 1,247 966 28.9 57.9 44.5 50.3 47.7 43</context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Gerber, M. and J. Y. Chai (2010). Beyond nombank: a study of implicit arguments for nominal predicates. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, Uppsala, Sweden, pp. 1583–1592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, Hong Kong,</booktitle>
<pages>512--520</pages>
<contexts>
<context position="1026" citStr="Gildea and Jurafsky, 2000" startWordPosition="141" endWordPosition="144">ut relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data. 1 Introduction Traditionally, Semantic Role Labeling (SRL) systems have focused in searching the fillers of those explicit roles appearing within sentence boundaries (Gildea and Jurafsky, 2000, 2002; Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These systems limited their searchspace to the elements that share a syntactical relation with the predicate. However, when the participants of a predicate are implicit this approach obtains incomplete predicative structures with null arguments. The following example includes the gold-standard annotations for a traditional SRL process: (1) [arg0 The network] had been expected to have [np losses] [arg1 of as much as $20 million] [arg3 on baseball this year]. It isn’t clear how much those [np losses] may widen beca</context>
</contexts>
<marker>Gildea, Jurafsky, 2000</marker>
<rawString>Gildea, D. and D. Jurafsky (2000). Automatic labeling of semantic roles. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, Hong Kong, pp. 512–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>3</issue>
<pages>245--288</pages>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. and D. Jurafsky (2002, September). Automatic labeling of semantic roles. Computational Linguistics 28(3), 245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gorinski</author>
<author>J Ruppenhofer</author>
<author>C Sporleder</author>
</authors>
<title>Towards weakly supervised resolution of null instantiations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics, IWCS ’13,</booktitle>
<pages>119--130</pages>
<location>Potsdam, Germany,</location>
<contexts>
<context position="7723" citStr="Gorinski et al. (2013)" startWordPosition="1207" endWordPosition="1210"> system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit</context>
</contexts>
<marker>Gorinski, Ruppenhofer, Sporleder, 2013</marker>
<rawString>Gorinski, P., J. Ruppenhofer, and C. Sporleder (2013). Towards weakly supervised resolution of null instantiations. In Proceedings of the 10th International Conference on Computational Semantics, IWCS ’13, Potsdam, Germany, pp. 119–130.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Hajiˇc</author>
<author>M Ciaramita</author>
<author>R Johansson</author>
<author>D Kawahara</author>
<author>M A Marti</author>
<author>L M`arquez</author>
<author>A Meyers</author>
<author>J Nivre</author>
<author>S Pad´o</author>
<author>J ˇStˇep´anek</author>
<author>P Straˇn´ak</author>
<author>M Surdeanu</author>
<author>N Xue</author>
<author>Y Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09,</booktitle>
<pages>1--18</pages>
<location>Boulder, Colorado, USA,</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Marti, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Hajiˇc, J., M. Ciaramita, R. Johansson, D. Kawahara, M. A. Marti, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, P. Straˇn´ak, M. Surdeanu, N. Xue, and Y. Zhang (2009). The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning: Shared Task, CoNLL ’09, Boulder, Colorado, USA, pp. 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Laparra</author>
<author>G Rigau</author>
</authors>
<title>Exploiting explicit annotations and semantic types for implicit argument resolution.</title>
<date>2012</date>
<booktitle>In 6th IEEE International Conference on Semantic Computing, ICSC ’12,</booktitle>
<pages>75--78</pages>
<location>Palermo, Italy,</location>
<contexts>
<context position="7699" citStr="Laparra and Rigau (2012)" startWordPosition="1203" endWordPosition="1206">dNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 20</context>
</contexts>
<marker>Laparra, Rigau, 2012</marker>
<rawString>Laparra, E. and G. Rigau (2012). Exploiting explicit annotations and semantic types for implicit argument resolution. In 6th IEEE International Conference on Semantic Computing, ICSC ’12, Palermo, Italy, pp. 75–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Laparra</author>
<author>G Rigau</author>
</authors>
<title>Sources of evidence for implicit argument resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics, IWCS ’13,</booktitle>
<pages>155--166</pages>
<location>Potsdam, Germany,</location>
<contexts>
<context position="7752" citStr="Laparra and Rigau (2013)" startWordPosition="1212" endWordPosition="1215">li and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBa</context>
</contexts>
<marker>Laparra, Rigau, 2013</marker>
<rawString>Laparra, E. and G. Rigau (2013). Sources of evidence for implicit argument resolution. In Proceedings of the 10th International Conference on Computational Semantics, IWCS ’13, Potsdam, Germany, pp. 155–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lappin</author>
<author>H J Leass</author>
</authors>
<title>An algorithm for pronominal anaphora resolution.</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<volume>20</volume>
<issue>4</issue>
<pages>535--561</pages>
<contexts>
<context position="15098" citStr="Lappin and Leass, 1994" startWordPosition="2385" endWordPosition="2389"> antecedent can change every time the algorithm finds a filler with a greater salience. A damping factor is applied to reduce the salience of distant predicates. 4.2 Filling arguments without explicit antecedents Filling the implicit arguments of a predicate has been identified as a particular case of coreference, very close to pronoun resolution (Silberer and Frank, 2012). Consequently, for those implicit arguments that have not explicit antecedents, we propose an adaptation of a classic algorithm for deterministic pronoun resolution. This component of our algorithm follows the RAP approach (Lappin and Leass, 1994). When our algorithm needs to fill an implicit predicate argument without an explicit antecedent it considers a set of candidates within a window formed by the sentence of the predicate and the two previous sentences. Then, the algorithm performs the following steps: 3Note that the algorithm could also consider sequences of closely related predicates. 1. Apply two constraints to the candidate list: (a) All candidates that are already explicit arguments of the predicate are ruled out. (b) All candidates commanded by the predicate in the dependency tree are ruled out. 2. Select those candidates </context>
<context position="19613" citStr="Lappin and Leass (1994)" startWordPosition="3096" endWordPosition="3099">antic categories and some name-entities and super-senses. Salience weighting. In this process, the algorithm assigns to each candidate a set of salience factors that scores its prominence. The sentence recency factor prioritizes the candidates that occur close to the same sentence of the predicate. The subject, direct object, indirect object and nonadverbial factors weight the salience of the candidate depending on the syntactic role they belong to. Additionally, the head of these syntactic roles are prioritized by the head factor. We have used the same weights, listed in table 2, proposed by Lappin and Leass (1994). In the example, candidate [Quest Medical Inc] is in the same sentence as the predicate plan, it 4Lexicographic files according to WordNet terminology. Factor type weight Sentence recency 100 Subject 80 Direct object 50 Indirect object 40 Head 80 Non-adverbial 50 Table 2: Weights assigned to each salience factor. belongs to a subject, and, indeed, it is the head of that subject. Hence, the salience score for this candidate is: 100 + 80 + 80 = 260. 4.3 Damping the salience of the default candidate As the algorithm maintains the default candidate until an explicit filler appears, potential erro</context>
</contexts>
<marker>Lappin, Leass, 1994</marker>
<rawString>Lappin, S. and H. J. Leass (1994, December). An algorithm for pronominal anaphora resolution. Computational Linguistics 20(4), 535–561.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The nombank project: An interim report. In</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation, HLT-NAACL ’04,</booktitle>
<pages>24--31</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="1777" citStr="Meyers et al., 2004" startWordPosition="261" endWordPosition="264">ements that share a syntactical relation with the predicate. However, when the participants of a predicate are implicit this approach obtains incomplete predicative structures with null arguments. The following example includes the gold-standard annotations for a traditional SRL process: (1) [arg0 The network] had been expected to have [np losses] [arg1 of as much as $20 million] [arg3 on baseball this year]. It isn’t clear how much those [np losses] may widen because of the short Series. The previous analysis includes annotations for the nominal predicate loss based on the NomBank structure (Meyers et al., 2004). In this case the annotator identifies, in the first sentence, the arguments arg0, the entity losing something, arg1, the thing lost, and arg3, the source of that loss. However, in the second sentence there is another instance of the same predicate, loss, but in this case no argument has been associated with it. Traditional SRL systems facing this type of examples are not able to fill the arguments of a predicate because their fillers are not in the same sentence of the predicate. Moreover, these systems also let unfilled arguments occurring in the same sentence, like in the following example</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman (2004). The nombank project: An interim report. In In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation, HLT-NAACL ’04, Boston, Massachusetts, USA, pp. 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Moor</author>
<author>M Roth</author>
<author>A Frank</author>
</authors>
<title>Predicate-specific annotations for implicit role binding: Corpus annotation, data analysis and evaluation experiments.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics, IWCS ’13,</booktitle>
<pages>369--375</pages>
<location>Potsdam, Germany,</location>
<contexts>
<context position="8124" citStr="Moor et al. (2013)" startWordPosition="1264" endWordPosition="1267">ly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that wer</context>
</contexts>
<marker>Moor, Roth, Frank, 2013</marker>
<rawString>Moor, T., M. Roth, and A. Frank (2013). Predicate-specific annotations for implicit role binding: Corpus annotation, data analysis and evaluation experiments. In Proceedings of the 10th International Conference on Computational Semantics, IWCS ’13, Potsdam, Germany, pp. 369–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<issue>1</issue>
<pages>71--106</pages>
<contexts>
<context position="8778" citStr="Palmer et al., 2005" startWordPosition="1369" endWordPosition="1372">cific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that were already annotated for PropBank (Palmer et al., 2005) and NomBank. This allowed them to avoid the sparseness problems and generalize properly from the training set. The results of this system were far better than those obtained by the systems that faced the SemEval-2010 dataset. This works represent the deepest study so far of the features that characterizes the implicit arguments 2. However, many of the most important features are lexically dependent on the predicate and cannot been generalized. Thus, specific annotations are required for each new predicate to be analyzed. All the works presented in this section agree that implicit arguments mu</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., D. Gildea, and P. Kingsbury (2005, March). The proposition bank: An annotated corpus of semantic roles. Computational Linguistics 31(1), 71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Palmer</author>
<author>D A Dahl</author>
<author>R J Schiffman</author>
<author>L Hirschman</author>
<author>M Linebarger</author>
<author>J Dowding</author>
</authors>
<title>Recovering implicit information.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th annual meeting on Association for Computational Linguistics, ACL ’86,</booktitle>
<pages>10--19</pages>
<location>New York, New York, USA,</location>
<contexts>
<context position="5808" citStr="Palmer et al. (1986)" startWordPosition="913" endWordPosition="916">ed systems. We release an open source prototype implementing this algorithm1. The paper is structured as follows. Section 2 discusses the related work. Section 3 presents in detail the data used in our experiments. Section 4 describes our algorithm for implicit argument resolution. Section 5 presents some experiments we have carried out to test the algorithm. Section 6 discusses the results obtained. Finally, section 7 offers some concluding remarks and presents some future research lines. 2 Related Work The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al. (1986). This work applied selectional restrictions together with coreference chains, in a very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, </context>
</contexts>
<marker>Palmer, Dahl, Schiffman, Hirschman, Linebarger, Dowding, 1986</marker>
<rawString>Palmer, M. S., D. A. Dahl, R. J. Schiffman, L. Hirschman, M. Linebarger, and J. Dowding (1986). Recovering implicit information. In Proceedings of the 24th annual meeting on Association for Computational Linguistics, ACL ’86, New York, New York, USA, pp. 10–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Raghunathan</author>
<author>H Lee</author>
<author>S Rangarajan</author>
<author>N Chambers</author>
<author>M Surdeanu</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A multi-pass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>492--501</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="3705" citStr="Raghunathan et al., 2010" startWordPosition="582" endWordPosition="585"> annotations could contribute positively to its semantic analysis. In fact, Gerber and Chai (2010) pointed out that implicit arguments can increase the coverage of argument structures in NomBank by 71%. However, current automatic systems require large amounts of manually annotated training data for each predicate. The effort required for this manual annotation explains the absence of generally applicable tools. This problem has become a main concern for many NLP tasks. This fact explains a new trend to develop accurate unsupervised systems that exploit simple but robust linguistic principles (Raghunathan et al., 2010). In this work, we study the coherence of the predicate and argument realization in discourse. In particular, we have followed a similar approach to 1180 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1180–1189, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics the one proposed by Dahl et al. (1987) who filled the arguments of anaphoric mentions of nominal predicates using previous mentions of the same predicate. We present an extension of this idea assuming that in a coherent document the different ocurrences of </context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Raghunathan, K., H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu, D. Jurafsky, and C. Manning (2010). A multi-pass sieve for coreference resolution. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, Cambridge, Massachusetts, USA, pp. 492–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>P Gorinski</author>
<author>C Sporleder</author>
</authors>
<title>In search of missing arguments: A linguistic approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, RANLP ’11,</booktitle>
<pages>331--338</pages>
<location>Hissar, Bulgaria,</location>
<contexts>
<context position="7673" citStr="Ruppenhofer et al. (2011)" startWordPosition="1199" endWordPosition="1202">nd thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand,</context>
</contexts>
<marker>Ruppenhofer, Gorinski, Sporleder, 2011</marker>
<rawString>Ruppenhofer, J., P. Gorinski, and C. Sporleder (2011). In search of missing arguments: A linguistic approach. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2011, RANLP ’11, Hissar, Bulgaria, pp. 331–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>C Sporleder</author>
<author>R Morante</author>
<author>C Baker</author>
<author>M Palmer</author>
</authors>
<title>Semeval-2010 task 10: Linking events and their participants in discourse.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>45--50</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="6515" citStr="Ruppenhofer et al. (2010)" startWordPosition="1018" endWordPosition="1021">a very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tu</context>
<context position="8532" citStr="Ruppenhofer et al., 2010" startWordPosition="1327" endWordPosition="1330"> an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEval2010 (Ruppenhofer et al., 2010), in this work the authors focused on a small set of ten predicates. But for those predicates, they annotated a large amount of instances in the documents from the Wall Street Journal that were already annotated for PropBank (Palmer et al., 2005) and NomBank. This allowed them to avoid the sparseness problems and generalize properly from the training set. The results of this system were far better than those obtained by the systems that faced the SemEval-2010 dataset. This works represent the deepest study so far of the features that characterizes the implicit arguments 2. However, many of the</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>Ruppenhofer, J., C. Sporleder, R. Morante, C. Baker, and M. Palmer (2010). Semeval-2010 task 10: Linking events and their participants in discourse. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, Los Angeles, California, USA, pp. 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Silberer</author>
<author>A Frank</author>
</authors>
<title>Casting implicit role linking as an anaphora resolution task.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics, *SEM ’12,</booktitle>
<pages>1--10</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="7899" citStr="Silberer and Frank (2012)" startWordPosition="1231" endWordPosition="1234">ole labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (2013) explore alternative linguistic and semantic strategies. These works obtained significant gains over previous approaches. Silberer and Frank (2012) adapted an entity-based coreference resolution model to extend automatically the training corpus. Exploiting this additional data, their system was able to improve previous results. Following this approach Moor et al. (2013) present a corpus of predicate-specific annotations for verbs in the FrameNet paradigm that are aligned with PropBank and VerbNet. On the other hand, Gerber and Chai (2010, 2012) studied the implicit argument resolution on NomBank. They uses a set of syntactic, semantic and coreferential features to train a logistic regres1181 sion classifier. Unlike the dataset from SemEv</context>
<context position="14850" citStr="Silberer and Frank, 2012" startWordPosition="2350" endWordPosition="2353">ing instances, without an explicit filler for a particular argument position, the algorithm repeats the same selection process and compares the new implicit candidate with the default one. That is, the default implicit argument of a predicate with no antecedent can change every time the algorithm finds a filler with a greater salience. A damping factor is applied to reduce the salience of distant predicates. 4.2 Filling arguments without explicit antecedents Filling the implicit arguments of a predicate has been identified as a particular case of coreference, very close to pronoun resolution (Silberer and Frank, 2012). Consequently, for those implicit arguments that have not explicit antecedents, we propose an adaptation of a classic algorithm for deterministic pronoun resolution. This component of our algorithm follows the RAP approach (Lappin and Leass, 1994). When our algorithm needs to fill an implicit predicate argument without an explicit antecedent it considers a set of candidates within a window formed by the sentence of the predicate and the two previous sentences. Then, the algorithm performs the following steps: 3Note that the algorithm could also consider sequences of closely related predicates</context>
</contexts>
<marker>Silberer, Frank, 2012</marker>
<rawString>Silberer, C. and A. Frank (2012). Casting implicit role linking as an anaphora resolution task. In Proceedings of the First Joint Conference on Lexical and Computational Semantics, *SEM ’12, Montr´eal, Canada, pp. 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Natural Language Learning, CoNLL ’08,</booktitle>
<pages>159--177</pages>
<location>Manchester, United Kingdom,</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Surdeanu, M., R. Johansson, A. Meyers, L. M`arquez, and J. Nivre (2008). The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the Twelfth Conference on Natural Language Learning, CoNLL ’08, Manchester, United Kingdom, pp. 159–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Tetreault</author>
</authors>
<title>Implicit role reference.</title>
<date>2002</date>
<booktitle>In International Symposium on Reference Resolution for Natural Language Processing,</booktitle>
<pages>109--115</pages>
<location>Alicante,</location>
<contexts>
<context position="6191" citStr="Tetreault (2002)" startWordPosition="969" endWordPosition="970">esults obtained. Finally, section 7 offers some concluding remarks and presents some future research lines. 2 Related Work The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al. (1986). This work applied selectional restrictions together with coreference chains, in a very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only </context>
</contexts>
<marker>Tetreault, 2002</marker>
<rawString>Tetreault, J. R. (2002). Implicit role reference. In International Symposium on Reference Resolution for Natural Language Processing, Alicante, Spain, pp. 109–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tonelli</author>
<author>R Delmonte</author>
</authors>
<title>Venses++: Adapting a deep semantic processing system to the identification of null instantiations.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10,</booktitle>
<pages>296--299</pages>
<location>Los Angeles, California, USA,</location>
<contexts>
<context position="6952" citStr="Tonelli and Delmonte, 2010" startWordPosition="1088" endWordPosition="1091">oblem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two syste</context>
</contexts>
<marker>Tonelli, Delmonte, 2010</marker>
<rawString>Tonelli, S. and R. Delmonte (2010). Venses++: Adapting a deep semantic processing system to the identification of null instantiations. In Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, Los Angeles, California, USA, pp. 296–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tonelli</author>
<author>R Delmonte</author>
</authors>
<title>Desperately seeking implicit arguments in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics, RELMS ’11,</booktitle>
<pages>54--62</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7150" citStr="Tonelli and Delmonte, 2011" startWordPosition="1118" endWordPosition="1121">ed a task in SemEval-2010 that included an implicit argument identification challenge based on FrameNet (Baker et al., 1998). The corpus for this task consisted in some novel chapters. They covered a wide variety of nominal and verbal predicates, each one having only a small number of instances. Only two systems were presented for this subtask obtaining quite poor results (F1 below 0,02). VENSES++ (Tonelli and Delmonte, 2010) applied a rule based anaphora resolution procedure and semantic similarity between candidates and thematic roles using WordNet (Fellbaum, 1998). The system was tuned in (Tonelli and Delmonte, 2011) improving slightly its performance. SEMAFOR (Chen et al., 2010) is a supervised system that extended an existing semantic role labeler to enlarge the search window to other sentences, replacing the features defined for regular arguments with two new semantic features. Although this system obtained the best performance in the task, data sparseness strongly affected the results. Besides the two systems presented to the task, some other systems have used the same dataset and evaluation metrics. Ruppenhofer et al. (2011), Laparra and Rigau (2012), Gorinski et al. (2013) and Laparra and Rigau (201</context>
</contexts>
<marker>Tonelli, Delmonte, 2011</marker>
<rawString>Tonelli, S. and R. Delmonte (2011). Desperately seeking implicit arguments in text. In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics, RELMS ’11, Portland, Oregon, USA, pp. 54–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Whittemore</author>
<author>M Macpherson</author>
<author>G Carlson</author>
</authors>
<title>Event-building through role-filling and anaphora resolution.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th annual meeting on Association for Computational Linguistics, ACL ’91,</booktitle>
<pages>17--24</pages>
<location>Berkeley, California, USA,</location>
<contexts>
<context position="5961" citStr="Whittemore et al. (1991)" startWordPosition="936" endWordPosition="940">k. Section 3 presents in detail the data used in our experiments. Section 4 describes our algorithm for implicit argument resolution. Section 5 presents some experiments we have carried out to test the algorithm. Section 6 discusses the results obtained. Finally, section 7 offers some concluding remarks and presents some future research lines. 2 Related Work The first attempt for the automatic annotation of implicit semantic roles was proposed by Palmer et al. (1986). This work applied selectional restrictions together with coreference chains, in a very specific domain. In a similar approach, Whittemore et al. (1991) also attempted to solve implicit 1http://adimen.si.ehu.es/web/ImpAr arguments using some manually described semantic constraints for each thematic role they tried to cover. Another early approach was presented by Tetreault (2002). Studying another specific domain, they obtained some probabilistic relations between some roles. These early works agree that the problem is, in fact, a special case of anaphora or coreference resolution. Recently, the task has been taken up again around two different proposals. On the one hand, Ruppenhofer et al. (2010) presented a task in SemEval-2010 that include</context>
</contexts>
<marker>Whittemore, Macpherson, Carlson, 1991</marker>
<rawString>Whittemore, G., M. Macpherson, and G. Carlson (1991). Event-building through role-filling and anaphora resolution. In Proceedings of the 29th annual meeting on Association for Computational Linguistics, ACL ’91, Berkeley, California, USA, pp. 17–24.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>