<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002218">
<title confidence="0.9771185">
Reinterpretation of an existing NLG system in a Generic Generation
Architecture
</title>
<author confidence="0.9334">
L. Cahill, C. Doran; R. Evans, C. Mellish, D. Paiva,M. Reape, D. Scott, N. Tipper
</author>
<affiliation confidence="0.833862">
.Universities of Brighton and Edinburgh.
</affiliation>
<bodyText confidence="0.547839">
Email rags @itri . brighton . ac . uk
</bodyText>
<sectionHeader confidence="0.984385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995910666666667">
The RAGS project aims to define a reference ar-
chitecture for Natural Language Generation (NLG)
systems. Currently the major part of this archi-
tecture consists of a set of datatype definitions for
specifying the input and output formats for mod-
ules within NLG systems. In this paper we describe
our efforts to reinterpret an existing NLG system in
terms of these definitions. The system chosen was
the Caption Generation System.
</bodyText>
<sectionHeader confidence="0.999387" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.961431793103448">
The RAGS project aims to define a reference ar-
chitecture for natural language generation systems.
Currently the major part of this architecture consists
of a set of datatype definitions for specifying the
input and output formats for modules within NLG
systems. The intention is that such representations
can be used to assist in reusability of components
of NLG systems. System components that adhere
to these representations, or use a format that can be
translated into such representations relatively eas-
ily, can then, in principle, be substituted into other
systems. Also, individual components could be de-
veloped without the need for a complete system if
datasets, based on the representations, were made
available.
In this paper we describe an attempt to reinterpret
an existing NLG system in terms of the RAGS data
definitions. The point of this exercise was to learn:
1. Whether these data structures were sufficient
to describe the input and output functionality
of an existing, independently developed, ap-
plied 2 NLG system.
&amp;quot; Now at the MITRE Corporation, Bedford, MA, USA,
cdoran:_imitre.org.
&apos;This work was supported by ESPRC grants GR/L7704 1
(Edinburgh) and GR/L77102 (Brighton), RAGS: Reference Ar-
chitecture for Generation Systems.
&apos;See (Paiva, 1998) for a definition of applied in this specific
context.
</bodyText>
<listItem confidence="0.948402125">
2. Which aspects of the RAGS repertoire would
actually be. requiredforsuch-a-reinterpretation,
which would be unnecessary and which addi-
tions to the RAGS repertoire would be moti-
vated.
3. Whether studying the system would generate
good ideas about possible reusable generation
modules that could be developed.
</listItem>
<bodyText confidence="0.932131">
In this exercise it was important to choose a sys-
tem that had been developed by people outside the
RAGS project. Equally, it was important to have
sufficient clear information about the system in the
available literature, and/or by means of personal
contact with the developers. The system chosen was
the Caption Generation System (Mittal et al., 1995;
Mittal et al., 1998)3. This system was chosen be-
cause, as well as fulfilling the criteria above, it ap-
peared to be a relatively simple pipeline, thus avoid-
ing complex control issues, with individual modules
performing the varied linguistic tasks that the RAGS
data structures had been designed to handle.
The reinterpretation exercise took the form of
coming up with an account of how the interfaces
to the CGS modules corresponded to the RAGS
model and reimplementing a working version of
each module (apart from Text Planning and Realisa-
tion) which was tested to ensure that, given appro-
priate input, its output was correct (i.e. conforming
to the global account) on key examples. Naturally,
given the scope of this exercise, we had to gloss over
some interesting implementational issues. The aim
was not to produce a complete system or a system
as good as CGS, but merely to demonstrate that the
broad functionality of the system could be repro-
- duced-within-the RAGS structures.
In this paper we first describe the RAGS data
structures. We then describe the CGS system
3In addition to these published sources, we were greatly
helped by the developers of the system who gave us the ben-
efit of their own expertise as well as access to the original code
of the system and a technical report that included implementa-
tional details such as system traces.
</bodyText>
<page confidence="0.999053">
69
</page>
<bodyText confidence="0.999736333333333">
followed by our reinterpretation of the system in
RAGS terms. Finally we di6euss..the
for RAGS of this exercise.
</bodyText>
<sectionHeader confidence="0.777691" genericHeader="method">
2 The RAGS datatypes
</sectionHeader>
<bodyText confidence="0.996912540540541">
The RAGS project initially set out to develop a ref-
erence architecture based on the three-stage pipeline
suggested by Reiter (Reiter, 1994). However, a
detailed analysis of existing applied NLG systems
(Cahill and Reape.„_1998).suggestedAhatsuchan
chitecture was not specific enough and not closely
enough adhered to by the majority of the systems
surveyed for this to be used as the basis of the archi-
tecture.
The abstract functionality of a generation system
can be specified without specific reference to pro-
cessing. The RAGS approach to this is to develop a
data model, that is, to define the functional modules
entirely in terms of the datatypes they manipulate
and the operations they can perform on them. On
top of such a model, more specific process models
can be created in terms of constraints on the order
and level of instantiation of different types of data in
the data model. A &apos;rational reconstruction&apos; of some
pipeline model might then be produced, but other
process models would also be possible.
The RAGS levels of representation are as fol-
lows4:
Conceptual The conceptual level of representa-
tion is defined only indirectly through an API via
which a knowledge base (providing the content
from which generation takes place) can be viewed
as if it were defined in a simple KL-ONE (Brach-
man and Schmolze, 1985) like system.
Abstract Semantic Abstract semantic representa-
tions are the first level at which semantic predicates
are associated with arguments. At this level, seman-
tic predicates and roles are those used in the API to
query the knowledge base and arguments are knowl-
edge base entities.
Semantic (Concrete) semantic representations
provide a complete notation for &amp;quot;logical forms&amp;quot;
where there is no longer any reference to the knowl-
edge base. The representations are based on sys-
tems such as SPL (Kasper, 1989) and DRT (Kamp
and Rey le, 1993).
&apos;More details can be found in (Cahill et
al., 1999) and at the RAGS project web site:
http://www.itri.brighton.ac.uk/rags.
Abstract Rhetorical Abstract Rhetorical Repre-
sentations are- free:structures with .rhetorical .rela-
tions at the internal nodes and Abstract Rhetorical
trees or Abstract Semantic Representations at the
leaves.
Rhetorical Abstract Rhetorical Representations
are viewed as descriptions of sets of possible
Rhetorical Representations. Each one may be trans-
formed into some subset of the possible Rhetori-
,• cal Representations .by means .of aset (If pernrtitted
transformations, e.g. reversing the order of nucleus
and satellite or changing the rhetorical relation to
one within a permitted set.
Abstract Document Document structure defines
the linear ordering of the constituents of the Rhetor-
ical Representation with a POSITION feature, as
well as two other features, TEXT-LEVEL, which
takes values such as paragraph or sentence; and
LAYOUT, which takes values such as wrapped-text
and vertical list. It takes the form of a tree, usu-
ally, but not necessarily, isomorphic to the Rhetor-
ical Representation and linked to it, but with these
three features at the nodes instead of rhetorical rela-
tions.
Abstract Syntactic Abstract Syntactic Represen-
tations capture high-level aspects of syntactic struc-
ture in terms of notions such as lexical head, speci-
fiers, modifiers and complements. This level of rep-
resentation is compatible with approaches such as
LFG f-structure, HPSG and Meteer&apos;s Text Structure.
</bodyText>
<sectionHeader confidence="0.95342" genericHeader="method">
3 Partial and Mixed Representations
</sectionHeader>
<bodyText confidence="0.999897166666667">
For all of the RAGS levels partial representations
are possible. Without this, it is not possible for a
module to pass any result to another until that re-
sult is completely determined, and this would im-
pose an unwanted bias towards simple pipeline ar-
chitectures into the model. There are many cases
in NLG where a representation is built collabora-
tively by several modules. For instance, many sys-
tems have a referring expression generation module
whose task is to complete a semantic representation
which lacks those structures which will be realised
as NPs. Such a functionality cannot be described
unless partially complete semantic representations
can be communicated.
In addition, mixed representations are possible,
where (possibly partial) representations at several
levels are combined with explicit links between the
elements. Many NLG modules have to be sensi-
</bodyText>
<page confidence="0.987975">
70
</page>
<bodyText confidence="0.9999499">
live to a number of levels at once (consider, for
.instance, _aggregation, zeferring.expression..genera-
tion and lexicalisation, all of which need to take
into account rhetorical, semantic and syntactic con-
straints). The input to most reusable realisation sys-
tems is also best viewed as a mixture of semantic
and abstract syntactic information.
The extra flexibility of having partial and mixed
representations turned out to be vital in the recon-
struction of the CGS system (Mellish et al., 2000).
</bodyText>
<sectionHeader confidence="0.984458" genericHeader="method">
4 The CGS system
</sectionHeader>
<bodyText confidence="0.997559194444445">
The Caption Generation System (CGS) generates
explanatory captions of graphical presentations (2-
D charts and graphs). Its architecture is a pipeline
with several modules, shown in the left hand part of
Figure 1. An example of a diagram and its accom-
panying text are given in Figure 2. The propositions
are numbered for ease of reference throughout the
paper.
The input to CGS is a picture representation
(graphical elements and its mapping from the data
set) generated by SAGE plus its complexity metric.
The text planning module (Moore and Paris (1993))
plans an explanation in terms of high level discourse
goals. The output of the planner is a partially or-
dered plan with speech-acts as leaves.
The ordering module receives as input the dis-
course plan with links specifying the ordering re-
lations between sub-trees and specifies an order for
them based on heuristics such as that the description
should be done from left to right in the visual space.
The aggregation module &amp;quot;only conjoins pairs of
contiguous propositions about the same grapheme
type5 in the same space&amp;quot; (Mittal et al., 1999) and
inserts cue phrases compatible with the propositions
(e.g., &amp;quot;whereas&amp;quot; for contrastive ones). The internal
order of the sentence constituents is determined by
the centering module using an extension of the cen-
tering theory of Grosz and colleagues (Grosz et al.,
1995).
The referring expression module uses Dale and
Reiter&apos;s (Dale and Reiter, 1995) algorithm to con-
struct the set of attributes that can uniquely identify
a referent. There are two situations where the text
planning module helps specifically in the generation
of referring expressions: (1) when the complexity
for expressing a graphic demands an example and
</bodyText>
<footnote confidence="0.972098333333333">
5&amp;quot;Graphemes are the basic building blocks for constructing
pictures. Marks, text, lines and bars are some of the different
grapheme classes available in SAGE.- (Mittal et al., 1999).
</footnote>
<figure confidence="0.982080181818182">
CGS architecture RAGS representations
SAGE
Module
Centering
Referring
Expression
Module
-ma(
IV V
III in II
FUF
</figure>
<figureCaption confidence="0.95597725">
Figure 1: A RAGS view of the CGS system. The
labels for the RAGS representations refer to the fol-
lowing: I = conceptual; II = semantic; III = rhetori-
cal; IV = document; V = syntactic.
</figureCaption>
<bodyText confidence="0.999029133333333">
it signals this both to SAGE (for highlighting the
corresponding grapheme) and to the rest of the text
generation modules; and (2) when in a specific sit-
uation the referring algorithm would need several
interactions for detecting that an entity is unique in
a certain visual space and the planning could detect
it in the construction of the description of this space.
When this occurs, the text planner &amp;quot;circumvents the
problem for the.referring expression module at the
planning stage itself, processing the speech-acts ap-
propriately to avoid this situation completely&amp;quot;.
After lexicalisation, which adds lexeme and ma-
jor category information, the resulting functional
descriptions are passed to the FUF/SURGE realiser
that generates texts like the caption of Figure 2.
</bodyText>
<figure confidence="0.989932">
I II III Iv V
Lexical
Choice
Module
</figure>
<page confidence="0.691085">
71
</page>
<figureCaption confidence="0.9963632">
Figure 2: (1) These two charts present information about house sales from data-set ts-1740. (2) In the two
charts, the y-axis indicates the houses. (7) In the first chart, the left edge of the bar shows the house&apos;s selling
price whereas (8) the right edge shows the asking price. (3) The horizontal position of the mark shows the
agency estimate. (4) The color shows the neighbourhood and (5) shape shows the listing agency. (6) Size
shows the number of rooms. (9) The second chart shows the number of days on the market.
</figureCaption>
<figure confidence="0.991671777777778">
•
444:0•CDO
VEDDDMCDO
$7, ttttt D It,:
:DLLD4D 411
•:•elsaoD
ttttt 44411
4111474 47,
11,111,-• 1404
</figure>
<sectionHeader confidence="0.793412" genericHeader="method">
5 Reinterpretation of CGS in RAGS
</sectionHeader>
<bodyText confidence="0.99988556097561">
Our reinterpretation of the CGS system defines the
interfaces between the modules of CGS in terms
of the RAGS data structures discussed above. In
this section we discuss the input and output inter-
faces for each CGS module in turn as well as any
problems we encountered in mapping the structures
into RAGS structures. Figure 1 shows the incre-
mental build-up of the RAGS data levels across
the pipeline. Here we have collapsed the Abstract
Rhetorical and Rhetorical and the Abstract Seman-
tic and Semantic. It is- interesting to note that the
build up of levels of representation does not tend to
correspond exactly with module boundaries.
One of the major issues we faced in our reinter-
pretation was where to produce representations (or
partial representations) whose emergence was not
defined clearly in the descriptions of CGS. For in-
stance, many decisions about document structure
are made only implicitly by the system. In most
cases we have opted to produce all types of repre-
sentations at the earliest point where they can con-
ceivably have any content. This means, for instance,
that our reimplementation assumes an (unimple-
mented) text planner which produces an Abstract
Rhetorical Representation with Abstract Semantic
leaves and an Abstract Document Representation.
Text Planner The input to the Longbow text plan-
ner discussed in section 4 above is a representation
of a picture in SAGE format (which has been an-
notated to indicate the types of complexity of each
grapheme) together with a goal, which can typi-
cally be interpreted as &amp;quot;describe&amp;quot;. It outputs an es-
sentially flat sequence of plan operators, each of
which corresponds in the output text 10 .a .speech
act. In our reinterpretation, we have assumed that
this flat structure needs to be translated into an Ab-
stract Rhetorical Representation with (at least) min-
imal structure. Such a structure is implicit in the
plan steps, and our interpretation of the rhetorical
structure for the example text corresponds closely to
that of the post-processing trace produced by CGS.
</bodyText>
<page confidence="0.997687">
72
</page>
<figureCaption confidence="0.993907">
Figure 3: Initial Document Structure
</figureCaption>
<figure confidence="0.996645933333333">
(8)
(7)
POSITION. 2
0
POSITION: POSITION: 2 POSMON. 1 POSMON:
LAYOUT. wrapped sat LAYOUT. ,apped sat LAYOUT. &apos;,yapped text LAYOUT u-rapped text
TEXT-LI VTL ? TEXT-LEVEL TEXT-LEVEL. TEXT-LEVEL:1
(3) (4) (5) (6)
POSITION&apos; I
LAYOUT. &apos;,yapped so
TEXT-LEVEL&apos;
POSMON 2
LAYOUT. wramsal text
TEXT-LEVEL?
IN2SMON. I
</figure>
<bodyText confidence="0.999734327272728">
However, we are still not entirely sure about where
exactly CGS creates this structure, so we have im-
posed it at the very beginning, onto the output of the
text planner.
Already at this stage it is necessary to make use
of mixed RAGS representations. As well as this
Abstract Rhetorical Representation, the text planner
has to produce an Abstract Document Representa-
tion, linked to the Abstract Rhetorical Representa-
tion. This is already partially ordered — although the
exact value of POSITION features cannot be speci-
fied at this stage, the document tree is constructed
so that propositions are already grouped together.
In addition, we make explicit certain default infor-
mation that the CGS leaves implicit at this stage,
namely, that the LAYOUT feature is always wrapped
text and that the TEXT-LEVEL feature of the top
node is always paragraph.
Ordering The ordering module takes the Abstract
Document Representation and the Abstract Rhetor-
ical Representation as input and outputs an Abstract
Document Representation with the POSITION fea-
ture&apos;s value filledlor all the nodes. That is, it fixes,
the linear order of the final output of the speech acts.
In our example, the ordering is changed so that steps
7 and 8 are promoted to appear before 3, 4, 5 and 6.
The resulting structure is shown in figure 36.
Aggregation Although aggregation might seem
like a self-contained process within NLG, in prac-
tice it can make changes at a number of levels of
representation and indeed it may be the last opera-
tion that has an effect on several levels. The aggre-
gation module in our reinterpretation thus has the fi-
nal responsibility to convert an Abstract Rhetorical
Representation with Abstract Semantic Represen-
tation leaves into a Rhetorical Representation with
Semantic Representation leaves. The new Rhetori-
cal Representation may be different from before as
a result of speech acts being aggregated but whether
different or not, it can now be considered final as
it will no longer be changed by the system. The
resulting Semantic Representations are no longer
Abstract because further structure may have been
determined for arguments to predicates. On the
other hand, referring expressions have not yet been
generated and so the (Concrete) Semantic Repre-
sentations cannot be complete. The reconstruc-
tion creates. partial Semantic ,Representations with
-holes&amp;quot; where the referring expressions (Semantic
Representations) will be inserted. These &amp;quot;holes&amp;quot; are
linked back to the knowledge base entities that they
correspond to.
Because Aggregation affects text levels, it also af-
fects the Abstract Document Representation, which
has its TEXT-LEVEL feature&apos;s values all filled at this
</bodyText>
<footnote confidence="0.960428">
6In this and the.following diagrams, objects are represented tween them. Dashed arrows indicate links between different
by circles with (labelled) arrows indicating the relations be- levels of representation.
</footnote>
<page confidence="0.998691">
73
</page>
<figureCaption confidence="0.998974">
Figure 4: Syntactic representations constructed by Centering
</figureCaption>
<figure confidence="0.9991129">
Adis
Lex
2-el
4-el
Adis
Args
Args
SemRep
3-el
set(SemPred)
a e
0 I Un(ROle,SeMReP)
affected
, SemRep
AbsSynRep 4.0
AbsSynRep
SemRep
1-el
FVM
forward-looking-center
tun(Funs.ArgSpec)
forward-looking-center
backward-looking-center
fun(Funs,ArgSpec)
FVM
backward-looking-center
el
DR
el
present
</figure>
<bodyText confidence="0.99969193220339">
point. It may also need to change the structure
of the Abstract Document Representation, for in-
stance, adding in a node for a sentence above two,
now aggregated, clause nodes.
Centering Because Centering comes before Re-
ferring Expression generation and Realisation, all it
can do is establish constraints that must be heeded
by the later modules. At one stage, it seemed as if
this required communicating a kind of information
that was not covered by the RAGS datatypes. How-
ever, the fact that an NP corresponds (or not) to a
center of some kind can be regarded as a kind of
abstract syntactic information. The reconstruction
therefore has the centering module building a partial
(unconnected) Abstract Syntactic representation for
each Semantic Representation that will be realised
as an NP, inserting a feature that specifies whether
it constitutes a forward- or backward-facing cen-
ter, approximately following Grosz et al (Grosz et
al., 1995). This information is used to determine
whether active or passive voice will be used. An
example of such a partial Abstract Syntactic Repre-
sentation is given in Figure 4.
Referring Expression In our reconstruction of
the CGS system, we have deviated from reproduc-
ing the exact functionality for the referring expres-
sion module and part of the lexical choice module.
In the CGS system, the referring expression module
computes association lists which can be used by the
lexical choice module to construct referring expres-
sions suitable for realisation. In our reconstruction,
however, the referring expression module directly
computes the Semantic Representations of referring
expressions.
We believe that this is a good example of a
case where developing a system with the RAGS
data structures in mind simplifies the task. There
are undoubtedly many different ways in which the
same results could be achieved, and there are many
(linguistic, engineering etc.) reasons for choosing
one rather than another. Our particular choice is
driven by the desire for conceptual simplicity, rather
than any strictly linguistic or computational motiva-
tions. We considered for each module which RAGS
level(s) it contributed to and then implemented it to
manipulate that (or those) level(s). In this case, that
meant a much more conceptually simple module
which just adds information to the Semantic Rep-
resentations.
Lexical Choice In CGS, this module performs a
range of tasks, including what we might call the
later stages of referring expression generation and
lexical choice, before converting the plan leaves
into FDs (Functional Descriptions), which serve as
the input to the FUF/SURGE module. In the re-
construction, on the other hand, referring expres-
sions have already been computed and the Rhetor-
ical Representation, with its now complete Seman-
tic Representations, needs to be lexicalised&amp;quot; and
</bodyText>
<page confidence="0.991008">
74
</page>
<figure confidence="0.605563">
Son,lop
</figure>
<figureCaption confidence="0.999469">
Figure 5: Combined Semantic and Abstract Syntactic Representation
</figureCaption>
<bodyText confidence="0.999875466666667">
translated into FUF/SURGE format. LexicaLisa-
tion in our terms involves adding the lexeme and
major category information to the Abstract Syntac-
tic Representations for the semantic predicates in
each Semantic Representation. The FUF/SURGE
input format was regarded as a combination of Se-
mantic and Abstract Syntactic information, and this
can easily be produced from the RAGS representa-
tions. The combined Semantic and Abstract Syn-
tactic Representations for the plan step &amp;quot;These two
charts present information about house sales from
data set ts-1740&amp;quot; is shown in Figure 5. The boxes
indicate suppressed subgraphs of the lexemes cor-
responding to the word in the boxes and triangles
indicate suppressed subgraphs of the two adjuncts.
</bodyText>
<sectionHeader confidence="0.999532" genericHeader="method">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9978833">
The reconstruction of CGS has taken the form of
working, out in detail the RAGS representations
passed between modules at each stage for a set
of key examples and reimplementing the modules
(apart from the Planner and Realiser) in a way that
correctly reproduces these representations. The ac-
tual implementation used an incrementally growing
data store for the RAGS representations which the
modules accessed in turn, though the passing of data
could also have been achieved in other ways.
The fact that the reconstruction has been success-
ful indicates that the RAGS architecture is broadly
adequate to redescribe this NLG system:
o No changes to the existing levels of represen-
tation were needed, though it was necessary to
make extensive use of partial and mixed repre-
sentations.
o No new levels of representation needed to be
introduced to capture the inter-module com-
munication of the system.
o All of the levels of representation apart from
the Conceptual level were used significantly in
the reconstruction.
In some ways, it is unfortunate that none of the
inter-module interfaces of CGS turned out to use a
single level of RAGS representation. Given the mo-
tivation for partial and mixed representations above,
however, this did not really come as a surprise. It
may well be that any really useful reusable modules
for NLG will have to have this complexity.
</bodyText>
<page confidence="0.995956">
75
</page>
<bodyText confidence="0.987898571428571">
In spite of the successful testing of the RAGS data
model, some difficulties were encountered:
• It was difficult to determine the exact nature
of the representations produced by the Planner,
though in the end we were able to develop a
system to automatically translate these into a
format we could deal with.
</bodyText>
<listItem confidence="0.9218492">
• Although the theoretical model of CGS has a
simple modular structure, in practice the mod-
ules are very tightTy integrated and making, the
exact interfaces explicit was not always easy.
• Referring expression generation requires fur-
</listItem>
<bodyText confidence="0.985506333333333">
ther access to the &amp;quot;knowledge base&amp;quot; holding
information about the graphic to be produced.
This knowledge was only available via interac-
tions with SAGE, and so it was not possible to
determine whether the RAGS view of Concep-
tual Representations was applicable. Our own
implementation of referring expression gener-
ation had to work around this problem in a non-
portable way.
</bodyText>
<listItem confidence="0.762226625">
• It became clear that there are many housekeep-
ing tasks that an NLG system must perform
following Lexical Choice in order for the final
Semantic and Abstract Syntactic Representa-
tions to be appropriate for direct input to a re-
alisation system such as FUF.
• The fact that the system was driving
FUF/SURGE seems to have had a signif-
icant effect on the internal representations
used by CGS. The reconstruction echoed this
and as a result may not be as general as could
be desired.
• Even though CGS only performs simple types
of Aggregation, it is clear that this is a critical
module for determining the final form of sev-
eral levels of representation.
</listItem>
<bodyText confidence="0.998176">
The division of CGS into modules is different from
that used in any NLG systems we have previously
worked on and so has been a useful stimulus to think
about ways in which reusable modules can be de-
signed. We envisage reusing at least the reimple-
mentation of the Centering module in our further
work.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.99807622">
R. Brachman and J. Schmolze. 1985. An overview of the KL-
ONE knowledge representation system. Cognitive Science,
9:171-216.
Lynne Cahill and Mike Reape. 1998. Component tasks
in applied NLG .systems. Technical Report [TRI-
99-05, University of Brighton. obtainable at
http://www.itri.brighton.ac.uklprojects/rags/.
Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish,
Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper.
1999. In Search of a Reference Architecture for NLG Sys-
tems. In Proceedings of the 7th European Workshop on Nat-
ural Language Generation, pages 77-85, Toulouse.
Robert Dale and Ehud Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of referring
expressions. Cognitive Science, 18:233-263.
BJ. Grosz, A.K. joShi;and S.&apos; Weinstein. 1995. Centering: a
framework for modelling the local coherence of discourse.
Computational Linguistics, 21(2):203-226.
H. Kamp and U. Reyle. 1993. From discourse to logic: Intro-
duction to model theoretic semantics of natural language,
formal logic and discourse representation theory. Kluwer,
Dordrecht; London.
R. T. Kasper. 1989. A flexible interface for linking applica-
tions to penman&apos;s sentence generator. In Proceedings of the
DARPA Speech and Natural Language Workshop, Philadel-
phia.
C. Mellish, R. Evans, L. Cahill, C. Doran, D. Paiva, M. Reape,
D. Scott, and N. Tipper. 2000. A representation for complex
and evolving data dependencies in generation. In Proceed-
ings of the Applied Natural Language Processing (ANLP-
NAACL2000) Conference, Seattle.
V. 0. Mittal, S. Roth, J. D. Moore, J. Mattis, and G. Carenini.
1995. Generating explanatory captions for information
graphics. In Proceedings of the 15th International Joint
Conference on Artificial Intelligence (MCA!&apos; 95 ), pages
1276-1283, Montreal, Canada, August.
V. 0. Mittal, J. D. Moore, G. Carenini, and S. Roth. 1998.
Describing complex charts in natural language: A caption
generation system. Computational Linguistics, 24(3): 431-
4.68.
Daniel Paiva. 1998. A survey of applied natural lan-
guage generation systems. Technical Report ITRI-
98-03, Information Technology Research Insti-
tute (ITRI), University of Brighton. Available at
http://www.itri.brighton.ac.uk/techreports.
Ehud Reiter. 1994. Has a consensus NL generation architec-
ture appeared and is it psycholinguistically plausible? In
Proceedings of the Seventh International Workshop on Nat-
ural Language Generation, pages 163-170, Kennebunkport,
Maine.
</reference>
<sectionHeader confidence="0.99746" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999954833333333">
We would like to thank the numerous people who have
helped us in this work. The developers of CGS, especially
Giuseppe Carenini and Vibhu Mittal; the RAGS consultants
and other colleagues at Brighton and Edinburgh, who have con-
tributed greatly to our development of the representations; and
finally to the anonymous reviewers of this paper.
</bodyText>
<page confidence="0.981828">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.198032">
<title confidence="0.935031">Reinterpretation of an existing NLG system in a Generic Generation Architecture</title>
<author confidence="0.602613">L Cahill</author>
<author confidence="0.602613">C Doran</author>
<author confidence="0.602613">R Evans</author>
<author confidence="0.602613">C Mellish</author>
<author confidence="0.602613">D Paiva</author>
<author confidence="0.602613">M Reape</author>
<author confidence="0.602613">D Scott</author>
<author confidence="0.602613">N</author>
<affiliation confidence="0.583358">Universities of Brighton and</affiliation>
<abstract confidence="0.94497">rags @itri brighton . ac . uk Abstract The RAGS project aims to define a reference architecture for Natural Language Generation (NLG) systems. Currently the major part of this architecture consists of a set of datatype definitions for specifying the input and output formats for modules within NLG systems. In this paper we describe our efforts to reinterpret an existing NLG system in terms of these definitions. The system chosen was the Caption Generation System.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Brachman</author>
<author>J Schmolze</author>
</authors>
<title>An overview of the KLONE knowledge representation system.</title>
<date>1985</date>
<journal>Cognitive Science,</journal>
<pages>9--171</pages>
<contexts>
<context position="5494" citStr="Brachman and Schmolze, 1985" startWordPosition="888" endWordPosition="892">form on them. On top of such a model, more specific process models can be created in terms of constraints on the order and level of instantiation of different types of data in the data model. A &apos;rational reconstruction&apos; of some pipeline model might then be produced, but other process models would also be possible. The RAGS levels of representation are as follows4: Conceptual The conceptual level of representation is defined only indirectly through an API via which a knowledge base (providing the content from which generation takes place) can be viewed as if it were defined in a simple KL-ONE (Brachman and Schmolze, 1985) like system. Abstract Semantic Abstract semantic representations are the first level at which semantic predicates are associated with arguments. At this level, semantic predicates and roles are those used in the API to query the knowledge base and arguments are knowledge base entities. Semantic (Concrete) semantic representations provide a complete notation for &amp;quot;logical forms&amp;quot; where there is no longer any reference to the knowledge base. The representations are based on systems such as SPL (Kasper, 1989) and DRT (Kamp and Rey le, 1993). &apos;More details can be found in (Cahill et al., 1999) and </context>
</contexts>
<marker>Brachman, Schmolze, 1985</marker>
<rawString>R. Brachman and J. Schmolze. 1985. An overview of the KLONE knowledge representation system. Cognitive Science, 9:171-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Cahill</author>
<author>Mike Reape</author>
</authors>
<title>Component tasks in applied NLG .systems.</title>
<date>1998</date>
<tech>Technical Report [TRI99-05,</tech>
<institution>University of Brighton.</institution>
<note>obtainable at http://www.itri.brighton.ac.uklprojects/rags/.</note>
<marker>Cahill, Reape, 1998</marker>
<rawString>Lynne Cahill and Mike Reape. 1998. Component tasks in applied NLG .systems. Technical Report [TRI99-05, University of Brighton. obtainable at http://www.itri.brighton.ac.uklprojects/rags/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynne Cahill</author>
<author>Christy Doran</author>
<author>Roger Evans</author>
<author>Chris Mellish</author>
<author>Daniel Paiva</author>
<author>Mike Reape</author>
<author>Donia Scott</author>
<author>Neil Tipper</author>
</authors>
<title>In Search of a Reference Architecture for NLG Systems.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation,</booktitle>
<pages>77--85</pages>
<location>Toulouse.</location>
<contexts>
<context position="6089" citStr="Cahill et al., 1999" startWordPosition="987" endWordPosition="990">chman and Schmolze, 1985) like system. Abstract Semantic Abstract semantic representations are the first level at which semantic predicates are associated with arguments. At this level, semantic predicates and roles are those used in the API to query the knowledge base and arguments are knowledge base entities. Semantic (Concrete) semantic representations provide a complete notation for &amp;quot;logical forms&amp;quot; where there is no longer any reference to the knowledge base. The representations are based on systems such as SPL (Kasper, 1989) and DRT (Kamp and Rey le, 1993). &apos;More details can be found in (Cahill et al., 1999) and at the RAGS project web site: http://www.itri.brighton.ac.uk/rags. Abstract Rhetorical Abstract Rhetorical Representations are- free:structures with .rhetorical .relations at the internal nodes and Abstract Rhetorical trees or Abstract Semantic Representations at the leaves. Rhetorical Abstract Rhetorical Representations are viewed as descriptions of sets of possible Rhetorical Representations. Each one may be transformed into some subset of the possible Rhetori,• cal Representations .by means .of aset (If pernrtitted transformations, e.g. reversing the order of nucleus and satellite or c</context>
</contexts>
<marker>Cahill, Doran, Evans, Mellish, Paiva, Reape, Scott, Tipper, 1999</marker>
<rawString>Lynne Cahill, Christy Doran, Roger Evans, Chris Mellish, Daniel Paiva, Mike Reape, Donia Scott, and Neil Tipper. 1999. In Search of a Reference Architecture for NLG Systems. In Proceedings of the 7th European Workshop on Natural Language Generation, pages 77-85, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>18--233</pages>
<contexts>
<context position="10428" citStr="Dale and Reiter, 1995" startWordPosition="1661" endWordPosition="1664">ecifies an order for them based on heuristics such as that the description should be done from left to right in the visual space. The aggregation module &amp;quot;only conjoins pairs of contiguous propositions about the same grapheme type5 in the same space&amp;quot; (Mittal et al., 1999) and inserts cue phrases compatible with the propositions (e.g., &amp;quot;whereas&amp;quot; for contrastive ones). The internal order of the sentence constituents is determined by the centering module using an extension of the centering theory of Grosz and colleagues (Grosz et al., 1995). The referring expression module uses Dale and Reiter&apos;s (Dale and Reiter, 1995) algorithm to construct the set of attributes that can uniquely identify a referent. There are two situations where the text planning module helps specifically in the generation of referring expressions: (1) when the complexity for expressing a graphic demands an example and 5&amp;quot;Graphemes are the basic building blocks for constructing pictures. Marks, text, lines and bars are some of the different grapheme classes available in SAGE.- (Mittal et al., 1999). CGS architecture RAGS representations SAGE Module Centering Referring Expression Module -ma( IV V III in II FUF Figure 1: A RAGS view of the </context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 18:233-263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K joShi Grosz</author>
<author>S &apos; Weinstein</author>
</authors>
<title>Centering: a framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<marker>Grosz, Weinstein, 1995</marker>
<rawString>BJ. Grosz, A.K. joShi;and S.&apos; Weinstein. 1995. Centering: a framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From discourse to logic: Introduction to model theoretic semantics of natural language, formal logic and discourse representation theory.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht; London.</location>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. From discourse to logic: Introduction to model theoretic semantics of natural language, formal logic and discourse representation theory. Kluwer, Dordrecht; London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T Kasper</author>
</authors>
<title>A flexible interface for linking applications to penman&apos;s sentence generator.</title>
<date>1989</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="6004" citStr="Kasper, 1989" startWordPosition="972" endWordPosition="973">ation takes place) can be viewed as if it were defined in a simple KL-ONE (Brachman and Schmolze, 1985) like system. Abstract Semantic Abstract semantic representations are the first level at which semantic predicates are associated with arguments. At this level, semantic predicates and roles are those used in the API to query the knowledge base and arguments are knowledge base entities. Semantic (Concrete) semantic representations provide a complete notation for &amp;quot;logical forms&amp;quot; where there is no longer any reference to the knowledge base. The representations are based on systems such as SPL (Kasper, 1989) and DRT (Kamp and Rey le, 1993). &apos;More details can be found in (Cahill et al., 1999) and at the RAGS project web site: http://www.itri.brighton.ac.uk/rags. Abstract Rhetorical Abstract Rhetorical Representations are- free:structures with .rhetorical .relations at the internal nodes and Abstract Rhetorical trees or Abstract Semantic Representations at the leaves. Rhetorical Abstract Rhetorical Representations are viewed as descriptions of sets of possible Rhetorical Representations. Each one may be transformed into some subset of the possible Rhetori,• cal Representations .by means .of aset (I</context>
</contexts>
<marker>Kasper, 1989</marker>
<rawString>R. T. Kasper. 1989. A flexible interface for linking applications to penman&apos;s sentence generator. In Proceedings of the DARPA Speech and Natural Language Workshop, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mellish</author>
<author>R Evans</author>
<author>L Cahill</author>
<author>C Doran</author>
<author>D Paiva</author>
<author>M Reape</author>
<author>D Scott</author>
<author>N Tipper</author>
</authors>
<title>A representation for complex and evolving data dependencies in generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Applied Natural Language Processing (ANLPNAACL2000) Conference,</booktitle>
<location>Seattle.</location>
<contexts>
<context position="8961" citStr="Mellish et al., 2000" startWordPosition="1421" endWordPosition="1424"> representations at several levels are combined with explicit links between the elements. Many NLG modules have to be sensi70 live to a number of levels at once (consider, for .instance, _aggregation, zeferring.expression..generation and lexicalisation, all of which need to take into account rhetorical, semantic and syntactic constraints). The input to most reusable realisation systems is also best viewed as a mixture of semantic and abstract syntactic information. The extra flexibility of having partial and mixed representations turned out to be vital in the reconstruction of the CGS system (Mellish et al., 2000). 4 The CGS system The Caption Generation System (CGS) generates explanatory captions of graphical presentations (2- D charts and graphs). Its architecture is a pipeline with several modules, shown in the left hand part of Figure 1. An example of a diagram and its accompanying text are given in Figure 2. The propositions are numbered for ease of reference throughout the paper. The input to CGS is a picture representation (graphical elements and its mapping from the data set) generated by SAGE plus its complexity metric. The text planning module (Moore and Paris (1993)) plans an explanation in </context>
</contexts>
<marker>Mellish, Evans, Cahill, Doran, Paiva, Reape, Scott, Tipper, 2000</marker>
<rawString>C. Mellish, R. Evans, L. Cahill, C. Doran, D. Paiva, M. Reape, D. Scott, and N. Tipper. 2000. A representation for complex and evolving data dependencies in generation. In Proceedings of the Applied Natural Language Processing (ANLPNAACL2000) Conference, Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roth Mittal</author>
<author>J D Moore</author>
<author>J Mattis</author>
<author>G Carenini</author>
</authors>
<title>Generating explanatory captions for information graphics.</title>
<date>1995</date>
<booktitle>In Proceedings of the 15th International Joint Conference on Artificial Intelligence (MCA!&apos; 95 ),</booktitle>
<pages>1276--1283</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="2675" citStr="Mittal et al., 1995" startWordPosition="418" endWordPosition="421">uld actually be. requiredforsuch-a-reinterpretation, which would be unnecessary and which additions to the RAGS repertoire would be motivated. 3. Whether studying the system would generate good ideas about possible reusable generation modules that could be developed. In this exercise it was important to choose a system that had been developed by people outside the RAGS project. Equally, it was important to have sufficient clear information about the system in the available literature, and/or by means of personal contact with the developers. The system chosen was the Caption Generation System (Mittal et al., 1995; Mittal et al., 1998)3. This system was chosen because, as well as fulfilling the criteria above, it appeared to be a relatively simple pipeline, thus avoiding complex control issues, with individual modules performing the varied linguistic tasks that the RAGS data structures had been designed to handle. The reinterpretation exercise took the form of coming up with an account of how the interfaces to the CGS modules corresponded to the RAGS model and reimplementing a working version of each module (apart from Text Planning and Realisation) which was tested to ensure that, given appropriate in</context>
</contexts>
<marker>Mittal, Moore, Mattis, Carenini, 1995</marker>
<rawString>V. 0. Mittal, S. Roth, J. D. Moore, J. Mattis, and G. Carenini. 1995. Generating explanatory captions for information graphics. In Proceedings of the 15th International Joint Conference on Artificial Intelligence (MCA!&apos; 95 ), pages 1276-1283, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Moore Mittal</author>
<author>G Carenini</author>
<author>S Roth</author>
</authors>
<title>Describing complex charts in natural language: A caption generation system.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<pages>431--4</pages>
<contexts>
<context position="2697" citStr="Mittal et al., 1998" startWordPosition="422" endWordPosition="425">iredforsuch-a-reinterpretation, which would be unnecessary and which additions to the RAGS repertoire would be motivated. 3. Whether studying the system would generate good ideas about possible reusable generation modules that could be developed. In this exercise it was important to choose a system that had been developed by people outside the RAGS project. Equally, it was important to have sufficient clear information about the system in the available literature, and/or by means of personal contact with the developers. The system chosen was the Caption Generation System (Mittal et al., 1995; Mittal et al., 1998)3. This system was chosen because, as well as fulfilling the criteria above, it appeared to be a relatively simple pipeline, thus avoiding complex control issues, with individual modules performing the varied linguistic tasks that the RAGS data structures had been designed to handle. The reinterpretation exercise took the form of coming up with an account of how the interfaces to the CGS modules corresponded to the RAGS model and reimplementing a working version of each module (apart from Text Planning and Realisation) which was tested to ensure that, given appropriate input, its output was co</context>
</contexts>
<marker>Mittal, Carenini, Roth, 1998</marker>
<rawString>V. 0. Mittal, J. D. Moore, G. Carenini, and S. Roth. 1998. Describing complex charts in natural language: A caption generation system. Computational Linguistics, 24(3): 431-4.68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Paiva</author>
</authors>
<title>A survey of applied natural language generation systems.</title>
<date>1998</date>
<tech>Technical Report ITRI98-03,</tech>
<institution>Information Technology Research Institute (ITRI), University of Brighton.</institution>
<note>Available at http://www.itri.brighton.ac.uk/techreports.</note>
<contexts>
<context position="1959" citStr="Paiva, 1998" startWordPosition="308" endWordPosition="309">em if datasets, based on the representations, were made available. In this paper we describe an attempt to reinterpret an existing NLG system in terms of the RAGS data definitions. The point of this exercise was to learn: 1. Whether these data structures were sufficient to describe the input and output functionality of an existing, independently developed, applied 2 NLG system. &amp;quot; Now at the MITRE Corporation, Bedford, MA, USA, cdoran:_imitre.org. &apos;This work was supported by ESPRC grants GR/L7704 1 (Edinburgh) and GR/L77102 (Brighton), RAGS: Reference Architecture for Generation Systems. &apos;See (Paiva, 1998) for a definition of applied in this specific context. 2. Which aspects of the RAGS repertoire would actually be. requiredforsuch-a-reinterpretation, which would be unnecessary and which additions to the RAGS repertoire would be motivated. 3. Whether studying the system would generate good ideas about possible reusable generation modules that could be developed. In this exercise it was important to choose a system that had been developed by people outside the RAGS project. Equally, it was important to have sufficient clear information about the system in the available literature, and/or by mea</context>
</contexts>
<marker>Paiva, 1998</marker>
<rawString>Daniel Paiva. 1998. A survey of applied natural language generation systems. Technical Report ITRI98-03, Information Technology Research Institute (ITRI), University of Brighton. Available at http://www.itri.brighton.ac.uk/techreports.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Has a consensus NL generation architecture appeared and is it psycholinguistically plausible?</title>
<date>1994</date>
<booktitle>In Proceedings of the Seventh International Workshop on Natural Language Generation,</booktitle>
<pages>163--170</pages>
<location>Kennebunkport, Maine.</location>
<contexts>
<context position="4307" citStr="Reiter, 1994" startWordPosition="694" endWordPosition="695">e RAGS data structures. We then describe the CGS system 3In addition to these published sources, we were greatly helped by the developers of the system who gave us the benefit of their own expertise as well as access to the original code of the system and a technical report that included implementational details such as system traces. 69 followed by our reinterpretation of the system in RAGS terms. Finally we di6euss..the for RAGS of this exercise. 2 The RAGS datatypes The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). However, a detailed analysis of existing applied NLG systems (Cahill and Reape.„_1998).suggestedAhatsuchan chitecture was not specific enough and not closely enough adhered to by the majority of the systems surveyed for this to be used as the basis of the architecture. The abstract functionality of a generation system can be specified without specific reference to processing. The RAGS approach to this is to develop a data model, that is, to define the functional modules entirely in terms of the datatypes they manipulate and the operations they can perform on them. On top of such a model, mor</context>
</contexts>
<marker>Reiter, 1994</marker>
<rawString>Ehud Reiter. 1994. Has a consensus NL generation architecture appeared and is it psycholinguistically plausible? In Proceedings of the Seventh International Workshop on Natural Language Generation, pages 163-170, Kennebunkport, Maine.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>