<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001426">
<title confidence="0.990361">
Investigating Content Selection for Language Generation using Machine
Learning
</title>
<author confidence="0.996796">
Colin Kelly
</author>
<affiliation confidence="0.9927235">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.7870215">
15 JJ Thomson Avenue
Cambridge, UK
</address>
<author confidence="0.804219">
Ann Copestake
</author>
<affiliation confidence="0.9654515">
Computer Laboratory
University of Cambridge
</affiliation>
<address confidence="0.762978">
15 JJ Thomson Avenue
Cambridge, UK
</address>
<author confidence="0.867698">
Nikiforos Karamanis
</author>
<affiliation confidence="0.987935">
Department of Computer Science
Trinity College Dublin
</affiliation>
<address confidence="0.9218995">
Dublin 2
Ireland
</address>
<email confidence="0.999478">
{colin.kelly,ann.copestake,nikiforos.karamanis}@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999950625">
The content selection component of a nat-
ural language generation system decides
which information should be communi-
cated in its output. We use informa-
tion from reports on the game of cricket.
We first describe a simple factoid-to-text
alignment algorithm then treat content se-
lection as a collective classification prob-
lem and demonstrate that simple ‘group-
ing’ of statistics at various levels of granu-
larity yields substantially improved results
over a probabilistic baseline. We addi-
tionally show that holding back of specific
types of input data, and linking database
structures with commonality further in-
crease performance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956953488373">
Content selection is the task executed by a natu-
ral language generation (NLG) system of decid-
ing, given a knowledge-base, which subset of the
information available should be conveyed in the
generated document (Reiter and Dale, 2000).
Consider the task of generating a cricket match
report, given the scorecard for that match. Such
a scorecard would typically contain a large num-
ber of statistics pertaining to the game as a whole
as well as individual players (e.g. see Figure 1).
Our aim is to identify which statistics should be
selected by the NLG system.
Much work has been done in the field of con-
tent selection, in a diverse range of domains e.g.
weather forecasts (Coch, 1998). Approaches are
usually domain specific and predominantly based
on structured tables of well-defined input data.
Duboue and McKeown (2003) attempted a sta-
tistical approach to content selection using a sub-
stantial corpus of biographical summaries paired
with selected content, where they extracted rules
and patterns linking the two. They then used ma-
chine learning to ascertain what was relevant.
Barzilay and Lapata (2005) extended this ap-
proach but applying it to a sports domain (Amer-
ican football), similarly viewing content selection
as a classification task and additionally taking ac-
count of contextual dependencies between data,
and found that this improved results compared to
a content-agnostic baseline. We aim throughout
to extend and improve upon Barzilay and Lapata’s
methods.
We emphasise that content selection through
statistical machine learning is a relatively new area
– approaches prior to Duboue and McKeown’s are,
in principle, much less portable – and as such there
is not an enormous body of work to build upon.
This work offers a novel algorithm for data-to-
text alignment, presents a new ‘grouping’ method
for sharing knowledge across similar but distinct
learning instances and shows that holding back
certain data from the machine learner, and rein-
troducing it later on can improve results.
</bodyText>
<sectionHeader confidence="0.981408" genericHeader="introduction">
2 Data Acquisition &amp; Alignment
</sectionHeader>
<bodyText confidence="0.777167095238095">
We first must obtain appropriately aligned cricket
data, for the purposes of machine learning.
Our data comes from the online Wisden al-
manack (Cricinfo, 2007), which we used to down-
load 133 match report/scorecard pairs. We em-
ployed an HTML parser to extract the main text
from the match report webpage, and the match
data-tables from the scorecard webpage. An ex-
ample scorecard can be found in Figure 11.
1Cricket is a bat-and-ball sport, contested by two oppos-
ing teams of eleven players. Each side’s objective is to score
more ‘runs’ than their opponents. An ‘innings’ refers to the
collective performance of the batting team, and (usually) ends
when all eleven players have batted.
In Figure 1, in the batting section R stands for ‘runs made’,
M for ‘minutes played on the field’, B for ‘number of balls
faced’. 4s and 6s are set numbers of runs awarded for hit-
ting balls that reach the boundary. SR is the number of runs
per 100 balls. In the bowling section, O stands for ‘overs
Proceedings of the 12th European Workshop on Natural Language Generation, pages 130–137,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.993177">
130
</page>
<table confidence="0.996542869565217">
Result India won by 63 runs
India innings (50 overs maximum)
SC Ganguly* run out (Silva/Sangakarra†)
V Sehwag run out (Fernando)
D Mongia b Samaraweera
SR Tendulkar c Chandana b Vaas
. . .
Extras (lb 6, w 12, nb 7)
Total (all out; 50 overs; 223 mins)
R M B 4s 6s SR
9 37 19 2 0 47.36
39 61 40 6 0 97.50
48 91 63 6 0 76.19
113 141 102 12 1 110.78
25
304
Fall of wickets 1-32 (Ganguly, 6.5 ov), 2-73 (Sehwag, 11.2 ov), 3-172 (Mongia,
27.4 ov), 4-199 (Dravid, 32.1 ov), ..., 10-304 (Nehra, 49.6 ov)
Bowling O M R W Econ
WPUJC Vaas 10 1 64 1 6.40 (2w)
DNT Zoysa 10 0 66 1 6.60 (6nb, 2w)
. . .
TT Samaraweera 8 0 39 2 4.87 (2w)
</table>
<figureCaption confidence="0.990474">
Figure 1: Statistics in a typical cricket scorecard.
</figureCaption>
<subsectionHeader confidence="0.96498">
2.1 Report Alignment
</subsectionHeader>
<bodyText confidence="0.997433230769231">
We use a supervised method to train our data, and
thus need to find all ‘links’ between the scorecard
and match report. We execute this alignment by
first creating tags with tag attributes according to
the common structure of the scorecards, and tag
values according to the data within a particular
scorecard. We then attempt to automatically align
the values of those tags with factoids, single pieces
of information found in the report.
For example, from Figure 1 the fact that Ten-
dulkar was the fourth player to bat on the first team
is captured by constructing a tag with tag attribute
team] player4, and tag value ‘SR Tendulkar’. The
fact he achieved 113 runs is encapsulated by an-
other tag, with tag attribute as team] player4 R
and tag value as ‘113’. Then if the report con-
tained the phrase ‘Tendulkar made 113 off 102
balls’ we would hope to match the ‘Tendulkar’
factoid with our tag value ‘SR Tendulkar’, the
‘113’ factoid with our tag value ‘113’ and replace
both factoids with their respective tag attributes, in
this case team] player4 and team] player4 R re-
spectively. Similar methods for this problem have
been employed by Barzilay and Lapata (2005) and
Duboue and McKeown (2003).
The basic idea behind our 6-step process for
alignment is that we align those factoids we are
bowled’, M for ‘maiden overs’, R for ‘runs conceded’ and W
for ‘wickets taken’. Econ is ‘economy rate’, or number of
runs per over.
It is important to note that Figure 1 omits the opposing
team’s innings (comprising new instances of the ‘Batting’,
‘Fall of Wickets’ and ‘Bowling’ sections), and some addi-
tional statistics found at the bottom of the scorecard.
most certain of first. The main obstacle we face
when aligning is the large incidence of repeated
numbers occurring within the scorecard, as this
would imply we have multiple, different tags all
with the same tag values. It is wholly possible
(and quite typical) that single figures will be re-
peated many times within a single scorecard2.
Therefore it would be advantageous for us to
have some means to differentiate amongst tags,
and hopefully select the correct tag when encoun-
tering a factoid which corresponds to repeated tag
values. Our algorithm is as follows:
Preprocessing We began by converting all ver-
balised numbers to their cardinal equivalents, e.g.
‘one’, ‘two’ to ‘1’, ‘2’, and selected instances of
‘a’ into ‘1’.
Proper Nouns In the first round of tagging we
attempt to match proper names from the scorecard
with strings within the report. Additionally, we
maintain a list of all players referenced thus far.
Player-Relevant Details Using the list of play-
ers we have accumulated, we search the report for
matches on tag values relating to only those play-
ers. This step was based on the assumption that a
factoid about a specific player is unlikely to appear
unless that player has been named.
Non-Player-Relevant Details The next stage
involves attempting to match factoids to tag values
whose attributes don’t refer to a particular player
e.g., more general match information as well as
team statistics.
</bodyText>
<footnote confidence="0.994426666666667">
2For example in Figure 1 we can see the number 6 appear-
ing four times: twice as the number of 4s for two different
players, once as an lb statistic and once as an nb statistic.
</footnote>
<page confidence="0.997195">
131
</page>
<bodyText confidence="0.999709545454546">
Anchor-Based Matching We next use sur-
rounding text anchor-based matching: for exam-
ple, if a sentence contains the string ‘he bowled
for 3 overs’ we will preferentially attempt to match
the factoid ‘3’ with tag values from tags which we
know refer to overs.
Remaining Matches The final step acts as our
‘catch-all’ – we proceed through all remaining
words in the report and try to match each poten-
tial factoid with the first (if any) tag found whose
tag value is the same.
</bodyText>
<subsectionHeader confidence="0.994584">
2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.99999555">
The output of our program is the original text with
all aligned figures and strings (factoids) replaced
with their corresponding tag attributes. We can see
an extract from an aligned report in Figure 2 where
we show the aligned factoids in bold, and their cor-
responding tag attributes in italics. We also note at
this point that much of commentary shown does
not in fact appear in the scorecard, and therefore
additional knowledge sources would typically be
required to generate a full match report – this is
beyond the scope of our paper, but Robin (1995)
attempts to deal with this problem in the domain
of basketball using revision-based techniques for
including additional content.
We asked a domain expert to evaluate five of
our aligned match reports – he did this by creat-
ing his own ‘gold standard’ for each report, a list
of aligned tags. Compared to our automatically
aligned tags, we obtained 79.0% average preci-
sion, 75.8% average recall and a mean F of 77.0%.
</bodyText>
<sectionHeader confidence="0.995641" genericHeader="method">
3 Categorization
</sectionHeader>
<bodyText confidence="0.999993">
We are using the methods of Barzilay and Lapata
(henceforth B&amp;L) as our starting point, so we de-
scribe what we did to emulate and extend them.
</bodyText>
<subsectionHeader confidence="0.999654">
3.1 Barzilay and Lapata’s Method
</subsectionHeader>
<bodyText confidence="0.929707655172413">
B&amp;L’s corpus was composed of a relational
database of football statistics. Within the database
were multiple tables, which we will refer to as
‘categories’ (actions within a game, e.g. touch-
downs and fumbles). Each category was com-
posed of ‘groups’ (the rows within a category ta-
ble), with each row referring to a distinct player,
and each column referring to different types of ac-
tion within that category (‘attributes’).
B&amp;L’s technique for the purposes of the ma-
chine learning was to assign a ‘1’ or ‘0’ to each
NatWest Series (series), match 9 (team1 player1 R)
India v Sri Lanka (matchtitle)
At Bristol (venue town), July 11 (date) (day/night
(daynight)).
India (team1) won by 63 runs (winmethod).
India (team1) 5 (team1 points) pts.
Toss: India (team1).
The highlight of a meaningless match was a sublime in-
nings from Tendulkar (team1 player4), who resumed
his fleeting love affair with Nevil Road to the delight
of a flag-waving crowd. On India (team1)’s only other
visit to Bristol (venue town), for a World Cup game
in 1999 against Kenya, Tendulkar (team1 player4)
had creamed an unbeaten 140, and this time he drove
with elan to make 113 (team1 player4 R) off just 102
(team1 player4 B) balls with 12 (team1 player4 4s)
fours and a (team1 player4 6s) six.
. . .
</bodyText>
<figureCaption confidence="0.99753">
Figure 2: Aligned match report extract
</figureCaption>
<bodyText confidence="0.999992476190476">
row, where a row would receive the value ‘1’ if
one or more of the entries in the row was ver-
balised in the report. In the context of our data
we could apply a similar division, for example, by
constructing a category entitled ‘Batting’ with at-
tributes (columns) ‘Runs’, ‘Balls’, ‘Minutes’, ‘4s’
and ‘6s’ etc., and rows corresponding to players.
In this case a group within that category would
correspond to one line of the ‘Innings’ table in Fig-
ure 1.
We note that B&amp;L were selecting content on a
row basis, while we are aiming to select individual
tag attributes (i.e., specific row/column cell refer-
ences) within the categories, a more difficult task.
We discuss this further in Section 6.
The technique above allows the machine learn-
ing algorithm to be aware that different statistics
are semantically related – i.e., each group within a
category contains the same ‘type’ of information.
We therefore think this is a logical starting point
for our work, and we aim to expand upon it.
</bodyText>
<subsectionHeader confidence="0.999765">
3.2 Classifying Tags
</subsectionHeader>
<bodyText confidence="0.999465777777778">
The key step was deciding upon an appropriate
division of our scorecard into various categories
and the groups for each category in the style of
B&amp;L. As can be seen from Figure 1 our input in-
formation is a mixture of structured (e.g. Bowling,
Batting sections), semi-structured (Fall of Wickets
section) and almost unstructured (Result) informa-
tion. This is somewhat unlike B&amp;L’s data, which
was fully structured in database form. We deal
</bodyText>
<page confidence="0.987539">
132
</page>
<table confidence="0.982391666666667">
Category Attributes Verb
Batting 9 47.0
Bowling 11 10.2
Fall of Wickets 8 46.4
Match Details 11 75.2
Match Result 8 45.1
Officials 8 6.0
Partnerships 11 75.5
Team Statistics 13 46.2
</table>
<tableCaption confidence="0.985006">
Table 1: Number of attributes per category with
percent verbalised (Verb)
</tableCaption>
<bodyText confidence="0.999869966666667">
with this by enforcing a stronger structure – di-
viding the information into eight of our own ‘cat-
egories’, based roughly on the formatting of the
webpages. These are outlined in Table 1.
The first three categories in the table are quite
intuitive and implicit from the respective sections
of the scorecard. There is additional information
in a typical scorecard (not shown in Figure 1),
which we must also categorise. The ‘Team Statis-
tics’ category contains details about the ‘extras’3
scored by each team, as well as the number of
points gained by the team towards that particular
series4. We divide the remaining tag attributes as
follows into three categories: ‘Officials’ – persons
participating in the match, other than the teams
(e.g. umpires, referees); ‘Match Details’ – infor-
mation that would have been known before the
match was played (e.g. venue, date, season); and
‘Match Result’ – data that could only be known
once the match was over (e.g. final result, player
of the match).
Finally we have an additional ‘Partnerships’5
category which is given explicitly on a separate
webpage referenced from each scorecard, but is
also implicit from information contained in the
‘Fall of Wickets’ and ‘Batting’ sections. We an-
ticipate that this category will help us manage the
issue of data sparsity. For instance, in our domain
we could group partnerships (which could con-
tain a multitude of player combinations and there-
</bodyText>
<footnote confidence="0.9166466">
3Additional runs awarded to the batting team for specific
actions executed by the bowling team. There are four types:
No Ball, Wide, Bye, Leg Bye.
4Each cricket game is part of a specific ‘series’ of games.
e.g. India would receive five points for their win within the
NatWest series.
5A ‘partnership’ refers to a pair of players who bat to-
gether, and usually comprises information such as the num-
ber of runs scored between them, the number of deliveries
faced and so on.
</footnote>
<bodyText confidence="0.9998971">
fore distinct tags) with the various possible binary
combinations of players together for shared learn-
ing. We discuss this further in Section 8.3.
Within 5 of the categories described above, we
are further able to divide the data into ‘groups’ -
the Batting, Bowling, Fall of Wickets and Partner-
ships categories refer to multiple players and thus
have multiple rows. The Team Statistics category
contains two groups, one for each team. The other
categories merely form one-line tables.
</bodyText>
<sectionHeader confidence="0.997964" genericHeader="method">
4 Machine Learning
</sectionHeader>
<bodyText confidence="0.9999646875">
Our task is to establish which tag attributes (and
hence tag values) should be included in the final
match report, and is a multi-label classification
problem. We chose to use BoosTexter (Schapire
and Singer, 2000) as it has been shown to be an
effective classifier (Yang, 1999), and it is one of
the few text classification tools which directly sup-
ports multi-label classification. This is also what
B&amp;L used.
Schapire and Singer’s BoosTexter (2000) uses
‘decision stumps’, or single level decision trees
to classify its input data. The predicates of these
stumps are defined, for text, by the presence or
absence of a single term, and, for numerical at-
tributes, whether the attribute exceeds a given
threshold, decided dynamically.
</bodyText>
<subsectionHeader confidence="0.999276">
4.1 Running BoosTexter
</subsectionHeader>
<bodyText confidence="0.999889428571429">
BoosTexter requires two input files to train a hy-
pothesis, ‘Names’ and ‘Data’.
Names The Names file contains, for each pos-
sible tag attribute, t, across all scorecards, the type
of its corresponding tag value. These are contin-
uous for numbers and text for normal text. From
our 133 scorecards we extracted a total of 61,063
tag values, of which 82.2% were continuous, the
remainder being text.
Data The Data file contains, for each scorecard,
a comma-delimited list of all tag values for a par-
ticular scorecard, with a ‘?’ for unknown values,
followed by a list of the verbalised tag attributes.
Testing We can now run BoosTexter with a
user-defined number of rounds, T, which creates
a hypothesis file. Using this hypothesis file and
a test ‘data’ file (without the list of verbalised tag
attributes), BoosTexter will give its hypothesized
predictions, a value f for each tag attribute t. The
sign of f determines whether the classifier be-
lieves the tag value corresponding to t is relevant
</bodyText>
<page confidence="0.998186">
133
</page>
<bodyText confidence="0.995494">
to the test scorecard, while |f |is a measure of the
confidence the classifier has in its assertion.
</bodyText>
<subsectionHeader confidence="0.996344">
4.2 Data Sparsity
</subsectionHeader>
<bodyText confidence="0.999951928571429">
The very nature of the data means that there are
a large number of tag values which do not occur
in every scorecard – the average scorecard con-
tained 24 values, yet our ‘names’ file contained
1193 possible tag attributes. A lot of this was due
to partnership tag attributes which formed 43.6%
of the ‘names’ entries. This large figure is because
a large number of all possible binary combinations
of players existed in the training data across both
teams6. This implies we will be unable to train for
a significant number of tag attributes as many spe-
cific tag values occur very rarely. Indeed we found
that of 158,669 entries, 97,666 (61.55%) were ‘un-
known’.
</bodyText>
<sectionHeader confidence="0.997214" genericHeader="method">
5 Evaluation Baselines
</sectionHeader>
<bodyText confidence="0.999938333333333">
It is not clear what constitutes a suitable baseline
so we considered multiple options. The issue of
ambiguous reference baselines is not specific to
the cricket domain, as there is no standardized
baseline approach across the prior literature. We
employ ten-fold cross validation throughout.
</bodyText>
<subsectionHeader confidence="0.99655">
5.1 Majority Baseline
</subsectionHeader>
<bodyText confidence="0.979452272727273">
B&amp;L created a ‘majority baseline’ whereby they
returned those categories (i.e., tables) which were
verbalised more than half of the time in their
aligned reports.
As explained in Section 3.2 we divided our tag
attributes into 8 categories. We emulated B&amp;L’s
baseline method as follows: For each category, if
any of the tag values within a particular ‘group’
was tagged as verbalised, we counted that as a
‘vote’ for that particular category. We then cal-
culated the total number of ‘votes’ divided by the
total number of ‘groups’ within each category. All
categories which had a ratio of 50% or greater
in this calculation were considered to be ‘major-
ity categories’. Our baseline Bmaj then consisted
of all tag attributes forming part of those majority
categories. As shown in Table 1 there were only
two categories which exceeded the 50% threshold,
‘Match Details’ and ‘Partnerships’.
We can see that this baseline performs
abysmally. The reason for this poor behaviour is
693 of the possible 2 P10
</bodyText>
<equation confidence="0.7291105">
��1 i = 110 combinations oc-
curred.
</equation>
<table confidence="0.9796">
Bmaj µ min max Q
Precision 0.0966 0.0333 0.1583 0.0250
Recall 0.4879 0.2727 0.7895 0.0977
F 0.1603 0.0620 0.2568 0.0384
</table>
<tableCaption confidence="0.998569">
Table 2: Majority Baseline, Bmaj
</tableCaption>
<bodyText confidence="0.999012666666667">
that since so many tag attributes contribute to the
categories we are including far too many possibil-
ities in our baseline.
</bodyText>
<subsectionHeader confidence="0.999296">
5.2 Probabilistic Baseline
</subsectionHeader>
<bodyText confidence="0.999117333333333">
This baseline is based on the premise that those
tag attributes which occur with highest frequency
across the training data refer to those tag values
which will often occur in a typical match report.
To create our baseline set of tag attributes Bprob
we extract the a most frequently verbalised tag at-
tributes across all the training data where a is the
average length of the verbalised tag attribute lists
for each report/scorecard pair.
</bodyText>
<table confidence="0.99665525">
Bprob µ min max Q
Precision 0.5157 0.2174 0.7391 0.1010
Recall 0.5157 0.1389 0.7647 0.0990
F 0.5100 0.1695 0.6939 0.0852
</table>
<tableCaption confidence="0.999811">
Table 3: Probabilistic Baseline, Bprob
</tableCaption>
<bodyText confidence="0.999911625">
This baseline achieves a mean F score of 51%,
however the tag attributes being returned are very
inconsistent with a typical match report – they
correspond in the majority to player names but
not one refers to any other tag attributes relevant
to those players. This renders the output mostly
meaningless in terms of our aim to select content
for an NLG system.
</bodyText>
<subsectionHeader confidence="0.994313">
5.3 No-Player Probabilistic Baseline
</subsectionHeader>
<bodyText confidence="0.999988545454546">
Taking the above into account we create a base-
line which derives its choice of tag attributes from
match statistics only. This baseline is similar to
the Probabilistic Baseline above, with the excep-
tion that when summing the numbers of tag at-
tributes in the sets we do not consider player-name
tag attributes in our counts. Instead, we extract
the a&apos; most frequent tag attributes, where a&apos; is
the average size of the sets excluding player-name
tag attributes. To finally obtain our baseline set
Bnops we merge our at most frequent tag attributes
</bodyText>
<page confidence="0.997121">
134
</page>
<bodyText confidence="0.9441875">
with any and all corresponding player-name tag at-
tributes7.
</bodyText>
<table confidence="0.99865275">
Bnops µ m n max Q
Precision 0.4923 0.1765 0.6875 0.0922
Recall 0.3529 0.1111 0.5625 0.0842
F 0.4064 0.1538 0.5946 0.0767
</table>
<tableCaption confidence="0.998872">
Table 4: No-Player Probabilistic Baseline, Bnops
</tableCaption>
<bodyText confidence="0.996813428571429">
As can be seen from Table 4, this method suffers
an absolute F-score drop of more than 10% from
the previous method. However if we analyse the
output more closely we can see that although the
accuracy has dropped, the returned tag attributes
are more thematically consistent with the training
data. This is our preferred baseline.
</bodyText>
<sectionHeader confidence="0.992741" genericHeader="method">
6 Evaluation Paradigm
</sectionHeader>
<bodyText confidence="0.998622333333334">
The main difficulty we encountered arose when
we came to assessing the Precision and Recall fig-
ures as we have yet to decide on what level we are
considering the output of our system to be correct.
We see three possibilities for the level:
Category We could simply count the ‘votes’
predicted on a per category basis (as described
in sections 3.1 and 5.1), and evaluate categories
based on the number of votes given for each. We
would expect this to generate very good results as
we are effectively overgrouping, once on a group
basis (grouping together all attributes) and once on
a category basis (unifying all groups within a cate-
gory), but the output would be so general and triv-
ial (effectively stating something to the effect that
“a match report should contain information about
batting, bowling and team statistics”) that it would
be of no use in an NLG system.
Groups Here we compare which ‘groups’ were
verbalised within each category, and which were
predicted to be verbalised (as we did for the Major-
ity Baseline of Section 5.1). Our implicit grouping
means that we do not have to necessarily return the
correct statistic pertaining to a group since each
group acts as a basket for the statistics contained
within it, and is susceptible to ‘false positives’.
This method is most similar to B&amp;L’s.
Tags Since we are trying to establish which tag
attributes should be included rather than which
groups are likely to contain verbalised tag at-
tributes, we could say that even the above method
7e.g., if team] player4 R is in a&apos; then we would also in-
clude team] player4 in our final set.
is too liberal in its definition of correctness. Thus
we also evaluate our groups on the basis of their
component parts, i.e., if a particular group of tag
attributes is estimated to be verbalised by Boos-
Texter, then we include all attributes from that
group.
</bodyText>
<sectionHeader confidence="0.98856" genericHeader="method">
7 Initial Results
</sectionHeader>
<bodyText confidence="0.999961125">
Our ‘categorized’ results are derived from present-
ing BoosTexter with each individual category as
described in Section 3.2, then merging the selected
tag attributes together and evaluating based on the
criteria described above. We then show BoosTex-
ter’s performance ‘as is’, by running the program
on the full output of our alignment stage with no
categorization/grouping.
</bodyText>
<subsectionHeader confidence="0.990951">
7.1 Categorized – Groups Level
</subsectionHeader>
<bodyText confidence="0.999885733333333">
Our ‘Categorized Groups’ results can be found in
Figure 3 and Table 5. For each of our tests we vary
the value of T (the number of rounds) to see how
it affects our accuracy.
Here we see we have a maximum F score of
0.7039 for T = 25. This is a very high result,
performing far better than all our baselines, how-
ever we feel the ‘basketing’ mentioned in Section
6 means that the results are not particularly in-
structive – instead of specific ‘interesting’ tag at-
tributes, we return a grouped list of tag attributes,
only some of which are likely to be ‘interesting’.
Thus we decide to no longer pursue ‘grouping’
as a valid evaluation method, and evaluate all our
methods at the ‘tag attribute’ level.
</bodyText>
<table confidence="0.99950825">
Best µ Q
Precision 0.7620 0.7473 0.0320
CG Recall 0.6795 0.6680 0.0322
F 0.7039 0.6897 0.0106
</table>
<tableCaption confidence="0.9393535">
Table 5: Categorized Groups with Best value for
T = 25.
</tableCaption>
<subsectionHeader confidence="0.991842">
7.2 Categorized – Tags Level
</subsectionHeader>
<bodyText confidence="0.99992975">
What is notable here is that, for all values of T
which we ran our tests on (ranging from 1 to
3000), we obtained just one set of results for ‘Cat-
egorized Tags’, displayed in Table 6.
This behaviour indicates that the boosting is not
helping to improve the results. Rather, it is repeat-
edly producing the same hypotheses, with vary-
ing confidence levels. The low F score is due to
</bodyText>
<page confidence="0.993661">
135
</page>
<figure confidence="0.9632">
1 10 100 1000
T
</figure>
<figureCaption confidence="0.978764">
Figure 3: All F scores Results
</figureCaption>
<table confidence="0.995515">
µ min max Q
Precision 0.0880 0.0496 0.1933 0.0223
Recall 0.7872 0.5417 1.0000 0.1096
F 0.1575 0.0924 0.3151 0.0361
</table>
<tableCaption confidence="0.997926">
Table 6: Categorized Tags Results
</tableCaption>
<bodyText confidence="0.9997955">
the very low Precision value. This method is ef-
fectively a direct application of B&amp;L’s method to
our domain, however because of our strict accu-
racy measurement, it does not perform particularly
well. In fact it is even worse than Bmaj, our worst-
performing baseline. We believe this is because
the Majority Baseline is limited in the breadth of
tags returned, whereas this method returns very
large sets of over 200 tag attributes (due to the
many contributing tag attributes of each category)
while the average size of the training sets is 24.
Ideally we want to strike a balance between
the improved granularity of the Categorized Tags
evaluation (without the low accuracy) with the
excellent performance of the Categorized Groups
evaluation (without the too-broad basketing).
</bodyText>
<subsectionHeader confidence="0.993941">
7.3 Unassisted Boosting
</subsectionHeader>
<bodyText confidence="0.9998444">
Our results are in Table 7 (row UB) and Figure 3.
We can see F values are increasing on the whole,
and that we have nearly reached our Probabilis-
tic Baseline. Inspecting the contents of the sets
returned by BoosTexter, we see they are slightly
more in line with a typical training set, but still suf-
fer from an over-emphasis on player names. We
also believe the high number of rounds required
for our best result (T = 2250) is caused by the
sparsity issue described in Section 4.2.
</bodyText>
<table confidence="0.9998306">
Best µ Q
Precision 0.4965 0.4730 0.0253
UB Recall 0.4961 0.4723 0.0252
F 0.4907 0.4673 0.0249
Precision 0.4128 0.3976 0.0094
NP Recall 0.4759 0.4633 0.0126
F 0.4367 0.4227 0.0091
Precision 0.4440 0.4318 0.0136
EC Recall 0.5127 0.4753 0.0271
F 0.4703 0.4467 0.0194
</table>
<tableCaption confidence="0.965419666666667">
Table 7: Unassisted Boosting (UB), No Players
(NP) and Enhanced Categorization (EC). Best val-
ues for T = 2250, 250 and 20 respectively.
</tableCaption>
<sectionHeader confidence="0.998412" genericHeader="method">
8 No-Players &amp; Enhanced
Categorization
</sectionHeader>
<bodyText confidence="0.9997615">
We now consider alternative, novel methods for
improving our results.
</bodyText>
<subsectionHeader confidence="0.976272">
8.1 Player Exclusion
</subsectionHeader>
<bodyText confidence="0.999961869565217">
We have thus far ignored coherency in our data
– for example we want to make sure that player
statistics will be accompanied by their correspond-
ing player name.
One problem so far with our approach has been
that we are effectively double-counting the play-
ers. Our methods inspect which player names
should appear at the same time as finding ap-
propriate match statistics, whereas we believe we
should instead be finding relevant statistics in the
first instance, holding back player names, then in-
cluding only those players to whom the statistics
refer. Thus we restate our task in this way.
This is also sensible as in previous incarnations
the learning algorithm had been learning from the
literal strings of the player names. Although a
player could be more likely to be named for vari-
ous reasons, these reasons would not appear in the
scorecard and we feel the strings are best ignored.
Thus we decide to remove all player names
from the machine learning input, reinstating only
relevant ones once BoosTexter has selected its
chosen tag attributes.
</bodyText>
<subsectionHeader confidence="0.969185">
8.2 Player Exclusion Results
</subsectionHeader>
<bodyText confidence="0.9993954">
As can be seen from Table 7 (row NP) and Figure
3, we have a maximum F value of 0.4367 when
T = 250, and have achieved a 3% absolute in-
crease, over our Bnopg baseline, a static implemen-
tation of the above ideas.
</bodyText>
<figure confidence="0.998266666666667">
Unassisted Boosting
Categorized Groups
No Players
Enhanced Categorization
0.8
0.7
0.6
0.5
0.4
</figure>
<page confidence="0.977192">
136
</page>
<subsectionHeader confidence="0.785153">
8.3 Enhanced Categorization
</subsectionHeader>
<bodyText confidence="0.99995956">
Our final method combines the ideas of Section
8.1 above with the benefits of categorization, and
handles data sparsity issues.
The method is identical to that of Section 3.1,
with two important exceptions: The first is that
we reintroduce player names after the learning, as
above. The second is that instead of just a bi-
nary include/don’t-include decision for each tag
attribute, we offer a list of verbalised tag attributes
to the learner, but anonymising them with respect
to the group in which they appear. This enables
the learner to, given any group, predict which tag
attributes should be returned, independent of the
group in question. This means groups with often-
empty tag values are able to leverage the informa-
tion from groups with usually populated tag val-
ues, hence solving our data-sparsity issues. For
example, this will solve the issue, referenced in
Section 4.2 of a lack of training data for particular
player-combination partnerships.
Having held back the group to which the tag at-
tributes belong, we reintroduce them enabling dis-
covery of the original tag attribute. This offers the
benefits of categorization, but with a finer-grained
approach to the returned sets of tag attributes.
</bodyText>
<subsectionHeader confidence="0.701603">
8.4 Enhanced Categorization Results
</subsectionHeader>
<bodyText confidence="0.999987272727273">
Our results are in Table 7 (row EC) and Figure
3. We achieved our best F score result of 0.4703
for a relatively low value of T = 20, and we can
clearly see that boosting establishes a reasonable
ruleset after a small number of iterations – we be-
lieve we have resolved the issue of data sparsity.
The fact that this grouping has improved our re-
sults compared to feeding the information in ‘flat’
(as in Section 7.3) emphasises that the construc-
tion and make-up of the categories play a key role
in defining performance.
</bodyText>
<sectionHeader confidence="0.997896" genericHeader="conclusions">
9 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.999990875">
This paper has presented an exploration of various
methods which could prove useful when select-
ing content given a partially structured database
of statistics and output text to emulate. We be-
gan by acquiring the necessary domain data, in the
form of scorecards and reports, and employed a
six-step process to align scorecard statistics ver-
balised in the reports. We next categorised our
statistics based on the scorecard format. We es-
tablished three baselines – one ‘unthinking’ proba-
bilistic baseline, a ‘sensible’ probabilistic one, and
another using categorization.
We found that unassisted boosting actually per-
formed worse than our comparable probabilistic
baseline, Bprob, but its output was marginally
more in line with the typical training data. We
explored how categorization affected our results,
and showed that by grouping similar sets of tag
attributes together we achieved a 7.4% improve-
ment over the comparable baseline value, Bnops
(Table 4). We further improved this technique in
a novel way by sharing structural information be-
tween learning instances, and by holding back cer-
tain information from the learner. Our final best F-
value marked a relative 15.7% increase on Bnops.
There are multiple avenues still available for ex-
ploration. One possibility would be to further in-
vestigate the effects of categorization from Section
3.2, for example by varying the size and number of
categories. We would also like to apply our meth-
ods to another domain (e.g. rugby games) to es-
tablish the relative generality of our approach.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9315545">
This paper is based on Colin Kelly’s M.Phil. thesis, written
towards his completion of the University of Cambridge Com-
puter Laboratory’s Computer Speech, Text and Internet Tech-
nology course. Grateful thanks go to the EPSRC for funding.
</bodyText>
<sectionHeader confidence="0.997055" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999291083333333">
Regina Barzilay and Mirella Lapata. 2005. Collective Con-
tent Selection for Concept-To-Text Generation. In HLT
‘05, pages 331–338. Association for Computational Lin-
guistics.
Jose Coch. 1998. Multimeteo: multilingual production of
weather forecasts. ELRA Newsletter, 3(2).
Cricinfo. 2007. Wisden Almanack.
http://cricinfo.com/wisdenalmanack. Retrieved 28
April 2007. Registration required.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Statis-
tical Acquisition of Content Selection Rules for Natural
Language Generation. EMNLP ‘03, pages 121–128.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press.
Jacques Robin. 1995. Revision-based generation of natu-
ral language summaries providing historical background:
corpus-based analysis, design, implementation and evalu-
ation. Ph.D. thesis, Columbia University.
Robert E. Schapire and Yoram Singer. 2000. BoosTexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135–168.
Yiming Yang. 1999. An evaluation of statistical approaches
to text categorization. Information Retrieval, 1(1/2):69–
90.
</reference>
<page confidence="0.997816">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.148101">
<title confidence="0.999795">Investigating Content Selection for Language Generation using Machine Learning</title>
<author confidence="0.932693">Colin</author>
<affiliation confidence="0.995163">Computer University of</affiliation>
<address confidence="0.913254">15 JJ Thomson Cambridge, UK</address>
<author confidence="0.740183">Ann</author>
<affiliation confidence="0.986866">Computer University of</affiliation>
<address confidence="0.913937">15 JJ Thomson Cambridge, UK</address>
<email confidence="0.64773">Nikiforos</email>
<affiliation confidence="0.997843333333333">Department of Computer Trinity College Dublin</affiliation>
<address confidence="0.514959">Ireland</address>
<abstract confidence="0.997383352941177">The content selection component of a natural language generation system decides which information should be communicated in its output. We use information from reports on the game of cricket. We first describe a simple factoid-to-text alignment algorithm then treat content selection as a collective classification problem and demonstrate that simple ‘grouping’ of statistics at various levels of granularity yields substantially improved results over a probabilistic baseline. We additionally show that holding back of specific types of input data, and linking database structures with commonality further increase performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Collective Content Selection for Concept-To-Text Generation.</title>
<date>2005</date>
<booktitle>In HLT ‘05,</booktitle>
<pages>331--338</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2175" citStr="Barzilay and Lapata (2005)" startWordPosition="323" endWordPosition="326">Our aim is to identify which statistics should be selected by the NLG system. Much work has been done in the field of content selection, in a diverse range of domains e.g. weather forecasts (Coch, 1998). Approaches are usually domain specific and predominantly based on structured tables of well-defined input data. Duboue and McKeown (2003) attempted a statistical approach to content selection using a substantial corpus of biographical summaries paired with selected content, where they extracted rules and patterns linking the two. They then used machine learning to ascertain what was relevant. Barzilay and Lapata (2005) extended this approach but applying it to a sports domain (American football), similarly viewing content selection as a classification task and additionally taking account of contextual dependencies between data, and found that this improved results compared to a content-agnostic baseline. We aim throughout to extend and improve upon Barzilay and Lapata’s methods. We emphasise that content selection through statistical machine learning is a relatively new area – approaches prior to Duboue and McKeown’s are, in principle, much less portable – and as such there is not an enormous body of work t</context>
<context position="6126" citStr="Barzilay and Lapata (2005)" startWordPosition="1018" endWordPosition="1021"> captured by constructing a tag with tag attribute team] player4, and tag value ‘SR Tendulkar’. The fact he achieved 113 runs is encapsulated by another tag, with tag attribute as team] player4 R and tag value as ‘113’. Then if the report contained the phrase ‘Tendulkar made 113 off 102 balls’ we would hope to match the ‘Tendulkar’ factoid with our tag value ‘SR Tendulkar’, the ‘113’ factoid with our tag value ‘113’ and replace both factoids with their respective tag attributes, in this case team] player4 and team] player4 R respectively. Similar methods for this problem have been employed by Barzilay and Lapata (2005) and Duboue and McKeown (2003). The basic idea behind our 6-step process for alignment is that we align those factoids we are bowled’, M for ‘maiden overs’, R for ‘runs conceded’ and W for ‘wickets taken’. Econ is ‘economy rate’, or number of runs per over. It is important to note that Figure 1 omits the opposing team’s innings (comprising new instances of the ‘Batting’, ‘Fall of Wickets’ and ‘Bowling’ sections), and some additional statistics found at the bottom of the scorecard. most certain of first. The main obstacle we face when aligning is the large incidence of repeated numbers occurrin</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Collective Content Selection for Concept-To-Text Generation. In HLT ‘05, pages 331–338. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jose Coch</author>
</authors>
<title>Multimeteo: multilingual production of weather forecasts.</title>
<date>1998</date>
<journal>ELRA Newsletter,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="1751" citStr="Coch, 1998" startWordPosition="262" endWordPosition="263">(NLG) system of deciding, given a knowledge-base, which subset of the information available should be conveyed in the generated document (Reiter and Dale, 2000). Consider the task of generating a cricket match report, given the scorecard for that match. Such a scorecard would typically contain a large number of statistics pertaining to the game as a whole as well as individual players (e.g. see Figure 1). Our aim is to identify which statistics should be selected by the NLG system. Much work has been done in the field of content selection, in a diverse range of domains e.g. weather forecasts (Coch, 1998). Approaches are usually domain specific and predominantly based on structured tables of well-defined input data. Duboue and McKeown (2003) attempted a statistical approach to content selection using a substantial corpus of biographical summaries paired with selected content, where they extracted rules and patterns linking the two. They then used machine learning to ascertain what was relevant. Barzilay and Lapata (2005) extended this approach but applying it to a sports domain (American football), similarly viewing content selection as a classification task and additionally taking account of </context>
</contexts>
<marker>Coch, 1998</marker>
<rawString>Jose Coch. 1998. Multimeteo: multilingual production of weather forecasts. ELRA Newsletter, 3(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cricinfo</author>
</authors>
<title>Wisden Almanack.</title>
<date>2007</date>
<journal>http://cricinfo.com/wisdenalmanack. Retrieved</journal>
<volume>28</volume>
<note>Registration required.</note>
<contexts>
<context position="3260" citStr="Cricinfo, 2007" startWordPosition="496" endWordPosition="497">proaches prior to Duboue and McKeown’s are, in principle, much less portable – and as such there is not an enormous body of work to build upon. This work offers a novel algorithm for data-totext alignment, presents a new ‘grouping’ method for sharing knowledge across similar but distinct learning instances and shows that holding back certain data from the machine learner, and reintroducing it later on can improve results. 2 Data Acquisition &amp; Alignment We first must obtain appropriately aligned cricket data, for the purposes of machine learning. Our data comes from the online Wisden almanack (Cricinfo, 2007), which we used to download 133 match report/scorecard pairs. We employed an HTML parser to extract the main text from the match report webpage, and the match data-tables from the scorecard webpage. An example scorecard can be found in Figure 11. 1Cricket is a bat-and-ball sport, contested by two opposing teams of eleven players. Each side’s objective is to score more ‘runs’ than their opponents. An ‘innings’ refers to the collective performance of the batting team, and (usually) ends when all eleven players have batted. In Figure 1, in the batting section R stands for ‘runs made’, M for ‘minu</context>
</contexts>
<marker>Cricinfo, 2007</marker>
<rawString>Cricinfo. 2007. Wisden Almanack. http://cricinfo.com/wisdenalmanack. Retrieved 28 April 2007. Registration required.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo A Duboue</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Statistical Acquisition of Content Selection Rules for Natural Language Generation.</title>
<date>2003</date>
<journal>EMNLP</journal>
<volume>03</volume>
<pages>121--128</pages>
<contexts>
<context position="1890" citStr="Duboue and McKeown (2003)" startWordPosition="279" endWordPosition="282">ed document (Reiter and Dale, 2000). Consider the task of generating a cricket match report, given the scorecard for that match. Such a scorecard would typically contain a large number of statistics pertaining to the game as a whole as well as individual players (e.g. see Figure 1). Our aim is to identify which statistics should be selected by the NLG system. Much work has been done in the field of content selection, in a diverse range of domains e.g. weather forecasts (Coch, 1998). Approaches are usually domain specific and predominantly based on structured tables of well-defined input data. Duboue and McKeown (2003) attempted a statistical approach to content selection using a substantial corpus of biographical summaries paired with selected content, where they extracted rules and patterns linking the two. They then used machine learning to ascertain what was relevant. Barzilay and Lapata (2005) extended this approach but applying it to a sports domain (American football), similarly viewing content selection as a classification task and additionally taking account of contextual dependencies between data, and found that this improved results compared to a content-agnostic baseline. We aim throughout to ex</context>
<context position="6156" citStr="Duboue and McKeown (2003)" startWordPosition="1023" endWordPosition="1026"> with tag attribute team] player4, and tag value ‘SR Tendulkar’. The fact he achieved 113 runs is encapsulated by another tag, with tag attribute as team] player4 R and tag value as ‘113’. Then if the report contained the phrase ‘Tendulkar made 113 off 102 balls’ we would hope to match the ‘Tendulkar’ factoid with our tag value ‘SR Tendulkar’, the ‘113’ factoid with our tag value ‘113’ and replace both factoids with their respective tag attributes, in this case team] player4 and team] player4 R respectively. Similar methods for this problem have been employed by Barzilay and Lapata (2005) and Duboue and McKeown (2003). The basic idea behind our 6-step process for alignment is that we align those factoids we are bowled’, M for ‘maiden overs’, R for ‘runs conceded’ and W for ‘wickets taken’. Econ is ‘economy rate’, or number of runs per over. It is important to note that Figure 1 omits the opposing team’s innings (comprising new instances of the ‘Batting’, ‘Fall of Wickets’ and ‘Bowling’ sections), and some additional statistics found at the bottom of the scorecard. most certain of first. The main obstacle we face when aligning is the large incidence of repeated numbers occurring within the scorecard, as thi</context>
</contexts>
<marker>Duboue, McKeown, 2003</marker>
<rawString>Pablo A. Duboue and Kathleen R. McKeown. 2003. Statistical Acquisition of Content Selection Rules for Natural Language Generation. EMNLP ‘03, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building Natural Language Generation Systems.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1300" citStr="Reiter and Dale, 2000" startWordPosition="180" endWordPosition="183"> then treat content selection as a collective classification problem and demonstrate that simple ‘grouping’ of statistics at various levels of granularity yields substantially improved results over a probabilistic baseline. We additionally show that holding back of specific types of input data, and linking database structures with commonality further increase performance. 1 Introduction Content selection is the task executed by a natural language generation (NLG) system of deciding, given a knowledge-base, which subset of the information available should be conveyed in the generated document (Reiter and Dale, 2000). Consider the task of generating a cricket match report, given the scorecard for that match. Such a scorecard would typically contain a large number of statistics pertaining to the game as a whole as well as individual players (e.g. see Figure 1). Our aim is to identify which statistics should be selected by the NLG system. Much work has been done in the field of content selection, in a diverse range of domains e.g. weather forecasts (Coch, 1998). Approaches are usually domain specific and predominantly based on structured tables of well-defined input data. Duboue and McKeown (2003) attempted</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="9257" citStr="Robin (1995)" startWordPosition="1556" endWordPosition="1557">) tag found whose tag value is the same. 2.2 Evaluation The output of our program is the original text with all aligned figures and strings (factoids) replaced with their corresponding tag attributes. We can see an extract from an aligned report in Figure 2 where we show the aligned factoids in bold, and their corresponding tag attributes in italics. We also note at this point that much of commentary shown does not in fact appear in the scorecard, and therefore additional knowledge sources would typically be required to generate a full match report – this is beyond the scope of our paper, but Robin (1995) attempts to deal with this problem in the domain of basketball using revision-based techniques for including additional content. We asked a domain expert to evaluate five of our aligned match reports – he did this by creating his own ‘gold standard’ for each report, a list of aligned tags. Compared to our automatically aligned tags, we obtained 79.0% average precision, 75.8% average recall and a mean F of 77.0%. 3 Categorization We are using the methods of Barzilay and Lapata (henceforth B&amp;L) as our starting point, so we describe what we did to emulate and extend them. 3.1 Barzilay and Lapata</context>
</contexts>
<marker>Robin, 1995</marker>
<rawString>Jacques Robin. 1995. Revision-based generation of natural language summaries providing historical background: corpus-based analysis, design, implementation and evaluation. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<pages>39--2</pages>
<contexts>
<context position="15502" citStr="Schapire and Singer, 2000" startWordPosition="2608" endWordPosition="2611">red learning. We discuss this further in Section 8.3. Within 5 of the categories described above, we are further able to divide the data into ‘groups’ - the Batting, Bowling, Fall of Wickets and Partnerships categories refer to multiple players and thus have multiple rows. The Team Statistics category contains two groups, one for each team. The other categories merely form one-line tables. 4 Machine Learning Our task is to establish which tag attributes (and hence tag values) should be included in the final match report, and is a multi-label classification problem. We chose to use BoosTexter (Schapire and Singer, 2000) as it has been shown to be an effective classifier (Yang, 1999), and it is one of the few text classification tools which directly supports multi-label classification. This is also what B&amp;L used. Schapire and Singer’s BoosTexter (2000) uses ‘decision stumps’, or single level decision trees to classify its input data. The predicates of these stumps are defined, for text, by the presence or absence of a single term, and, for numerical attributes, whether the attribute exceeds a given threshold, decided dynamically. 4.1 Running BoosTexter BoosTexter requires two input files to train a hypothesis</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>90</pages>
<contexts>
<context position="15566" citStr="Yang, 1999" startWordPosition="2622" endWordPosition="2623">s described above, we are further able to divide the data into ‘groups’ - the Batting, Bowling, Fall of Wickets and Partnerships categories refer to multiple players and thus have multiple rows. The Team Statistics category contains two groups, one for each team. The other categories merely form one-line tables. 4 Machine Learning Our task is to establish which tag attributes (and hence tag values) should be included in the final match report, and is a multi-label classification problem. We chose to use BoosTexter (Schapire and Singer, 2000) as it has been shown to be an effective classifier (Yang, 1999), and it is one of the few text classification tools which directly supports multi-label classification. This is also what B&amp;L used. Schapire and Singer’s BoosTexter (2000) uses ‘decision stumps’, or single level decision trees to classify its input data. The predicates of these stumps are defined, for text, by the presence or absence of a single term, and, for numerical attributes, whether the attribute exceeds a given threshold, decided dynamically. 4.1 Running BoosTexter BoosTexter requires two input files to train a hypothesis, ‘Names’ and ‘Data’. Names The Names file contains, for each po</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yiming Yang. 1999. An evaluation of statistical approaches to text categorization. Information Retrieval, 1(1/2):69– 90.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>