<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000700">
<note confidence="0.614755">
Lexical Access Based on Underspecified Input
Michael ZOCK
LIF-CNRS
´Equipe TALEP
163, Avenue de Luminy
F-13288 Marseille Cedex 9
</note>
<email confidence="0.994296">
michael.zock@lif.univ-mrs.fr
</email>
<note confidence="0.968381">
Didier SCHWAB
Groupe GETALP
Laboratoire d’Informatique de Grenoble
385 avenue de la Bibliothque - BP 53
</note>
<address confidence="0.564578">
F-38041 Grenoble Cedex 9
</address>
<email confidence="0.994758">
didier.schwab@imag.fr
</email>
<sectionHeader confidence="0.997322" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997562962963">
Words play a major role in language pro-
duction, hence finding them is of vital im-
portance, be it for speaking or writing.
Words are stored in a dictionary, and the
general belief holds, the bigger the bet-
ter. Yet, to be truly useful the resource
should contain not only many entries and a
lot of information concerning each one of
them, but also adequate means to reveal the
stored information. Information access de-
pends crucially on the organization of the
data (words) and on the navigational tools.
It also depends on the grouping, ranking
and indexing of the data, a factor too often
overlooked.
We will present here some preliminary re-
sults, showing how an existing electronic
dictionary could be enhanced to support
language producers to find the word they
are looking for. To this end we have started
to build a corpus-based association ma-
trix, composed of target words and ac-
cess keys (meaning elements, related con-
cepts/words), the two being connected at
their intersection in terms of weight and
type of link, information used subsequently
for grouping, ranking and navigation.
</bodyText>
<sectionHeader confidence="0.952319" genericHeader="categories and subject descriptors">
1 Context and problem
</sectionHeader>
<bodyText confidence="0.99960575">
When speaking or writing we encounter basi-
cally either of the following two situations: one
where everything works automatically, somehow
like magic, words popping up one after another
</bodyText>
<footnote confidence="0.902951">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.975528088235294">
like spring water, and another where we look de-
liberately and often painstakingly for a specific,
possibly known word. We will be concerned here
with this latter situation: a speaker/ writer using
an electronic dictionary to look for such a word.
Unfortunately, alphabetically organized dictionar-
ies are not well suited for this kind of reverse
lookup where the inputs are meanings (elements of
the word’s definition) or conceptually related ele-
ments (collocations, associations), and the outputs
the target words.
Without any doubt, lexicographers have made
considerable efforts to assist language users, build-
ing huge resources, composed of many words and
lots of information associated with each one of
them. Still, it is not unfair to say most dictionar-
ies have been conceived from the reader’s point of
view. The lexicographers have hardly taken into
account the language producer’s perspective,1 con-
sidering conceptual input, incomplete as it may be,
as starting point. While readers start with words,
looking generally for their corresponding mean-
ings, speakers or writers usually start with the op-
posite, meanings or concepts,2 which should be the
entry points of a dictionary, which ideally is neu-
tral in terms of access direction.3
The problem is that we still don’t know very
well what concepts are, whether they are compo-
sitional and if so, how many primitives there are
(Wilks, 1977; Wierzbicka, 1996; Goddard, 1998).
1Roget’s thesaurus (Roget, 1852), Miller and Fellbaum’s
WordNet (Fellbaum, 1998) and Longman’s Language Activa-
tor (Summers, 1993), being notable exceptions (For more de-
tails, see next section).
</bodyText>
<footnote confidence="0.985984571428571">
2Of course, this does not preclude, that we may have to
use words to refer to them in a concept-based query.
3While we agree with Polgu`ere theoretically when he
pleads for dictionary neutrality with regard to lexical access
(Polgu`ere, 2006), from a practical point of view the situation
is obviously quite different for the speaker and listener, even
if both of them draw on the same resource.
</footnote>
<page confidence="0.914229">
9
</page>
<note confidence="0.850158">
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9–17
Manchester, August 2008
</note>
<bodyText confidence="0.99958324137931">
Neither do we know how to represent them. Yet,
there are ways around this problem as we will
show. Whether concepts and words are organized
and accessed differently is a question we cannot
answer here. We can agree though on the fact
that getting information concerning words is fairly
unproblematic when reading, at least in the case
of most western languages. Words can gener-
ally be found easily in a dictionary, provided the
user knows the spelling, the alphabet and how to
build lemma starting from an inflected form. Un-
like words, which are organized alphabetically (in
western languages) or by form (stroke counts in
Chinese), concepts are organized topically: they
are clustered into functional groups according to
their role in real world, or our perception of it.
Psychologist have studied the difficulties peo-
ple have when trying to produce or access words
(Aitchinson, 2003). In particular, they have stud-
ied the tip-of-the-tongue phenomenon (Brown and
McNeill, 1996) and the effects an input can have
on the quality of an output (error analysis (Cutler,
1982)) and on the ease of its production: positive
or negative priming effect (activation/inhibition).
Obviously, these findings allow certain conclu-
sions, and they might guide us when developing
tools to help people find the needed word. In par-
ticular, they reveal two facts highly relevant for our
goal:
</bodyText>
<listItem confidence="0.998262">
1. even if people fail to access a given word, they
might know a lot about it: origin, meaning
(word definition, role played in a given sit-
uation), part of speech, number of syllables,
similar sounding words, etc. Yet, despite all
this knowledge, they seem to lack some cru-
cial information to be able to produce the pho-
netic form. The word gets blocked at the very
last moment, even though it has reached the
tip-of-the-tongue. This kind of nuisance is all
the more likely as the target word is rare and
primed by a similar sounding word.
2. unlike words in printed or electronic dictio-
naries, words in our mind may be inexis-
tent as tokens. What we seem to have in
our minds are decomposed, abstract entities
which need to be synthesized over time.4 Ac-
</listItem>
<footnote confidence="0.9582896">
4This may be very surprising, yet, this need not be the case
if we consider the fact that speech errors are nearly always
due to competing elements from the same level or an adja-
cent one, unless they are the result of a surrounding concept
which has been activated, or which is about to be translated
</footnote>
<bodyText confidence="0.999501944444444">
cording to Levelt (Levelt, 1996) the genera-
tion of words (synthesis) involves the follow-
ing stages: conceptual preparation, lexical se-
lection, phonological- and phonetic encoding,
articulation. Bear in mind that having per-
formed ’lexical selection’ does not imply ac-
cess to the phonetic form (see the experiments
on the tip-of-the-tongue phenomenon).
What can be concluded from these observa-
tions? It seems that underspecified input is suffi-
ciently frequent to be considered as normal. Hence
we should accept it, and make the best out of it by
using whatever information is available (accessi-
ble), no matter how incomplete, since it may still
contribute to find the wanted information, be it by
reducing the search space. Obviously, the more in-
formation we have the better, as this reduces the
number of words among which to choose.
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="method">
2 Related work and goal
</sectionHeader>
<bodyText confidence="0.999177305555555">
While more dictionaries have been built for the
reader than for the writer, there have been some
onomasiological attempts as early as in the mid-
dle of the 19th century. For example, Roget’s
Thesaurus (Roget, 1852), T’ong’s Chinese and
English instructor (T’ong, 1862), or Boissiere’s
analogical dictionary (Boissi`ere, 1862).5 Newer
work includes Mel’ˇcuk’s ECD (Mel’ˇcuk et al.,
1999), Miller and Fellbaum’s WordNet (Fellbaum,
1998), Richardson and Dolan’s MindNet (Richard-
son et al., 1998), Dong’s HowNet (Dong and
Dong, 2006) and Longman’s Language Activa-
tor (Summers, 1993). There is also the work of
into words. Put differently, we do not store words at all in
our mind, at least not in the layman’s or lexicographer’s sense
who consider word-forms and their meanings as one. If we
are right, than rather continue to consider the human mind as
a word store we could consider it as a word factory. Indeed,
by looking at some of the work done by psychologists who try
to emulate the mental lexicon (for a good survey see (Harley,
2004), pages 359-374) one gets the impression that words are
synthesized rather than located and read out. Taking a look at
all this work, generally connectionist models, one may con-
clude that, rather than having words in our mind we have a
set of more or less abstract features (concepts, syntactic infor-
mation, phonemes), distributed across various layers, which
need to be synthesized over time. To do so we proceed from
abstract meanings to concrete sounds, which at some point
were also just abstract features. By propagating energy rather
than data (as there is no message passing, transformation or
cumulation of information, there is only activation spreading,
that is, changes of energy levels, call it weights, electronic
impulses, or whatever), that we propagate signals, activating
ultimately certain peripheral organs (larynx, tongue, mouth,
lips, hands) in such a way as to produce movements or sounds,
that, not knowing better, we call words.
</bodyText>
<footnote confidence="0.933847">
5For a more recent proposal see (Robert et al., 1993).
</footnote>
<page confidence="0.998663">
10
</page>
<bodyText confidence="0.995157623188406">
(Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008),
various collocation dictionaries (BBI, OECD) and
Bernstein’s Reverse Dictionary.6 Finally, there is
M. Rundell’s MEDAL, a thesaurus produced with
the help of Kilgariff’s Sketch Engine (Kilgarriff et
al., 2004).
As one can see, a lot of progress has been ac-
complished over the last few years, yet more can be
done, especially with regard to unifying linguistic
and encyclopedic knowledge. Let’s take an exam-
ple to illustrate our point.
Suppose, you were looking for a word express-
ing the following ideas: ’superior dark coffee made
from beans from Arabia’, and that you knew that
the target word was neither espresso nor cappuc-
cino. While none of this would lead you directly
to the intended word, mocha, the information at
hand, i.e. the word’s definition or some of its ele-
ments, could certainly be used. In addition, people
draw on knowledge concerning the role a concept
(or word) plays in language and in real world, i.e.
the associations it evokes. For example, they may
know that they are looking for a noun standing for
a beverage that people take under certain circum-
stances, that the liquid has certain properties, etc.
In sum, people have in their mind an encyclope-
dia: all words, concepts or ideas being highly con-
nected. Hence, any one of them has the potential to
evoke the others. The likelihood for this to happen
depends, of course, on factors such as frequency
(associative strength), distance (direct vs. indirect
access), prominence (saliency), etc.
How is this supposed to work for a dictionary
user? Suppose you were looking for the word
mocha (target word: tw), yet the only token com-
ing to your mind were computer (source word:
sw). Taking this latter as starting point, the system
would show all the connected words, for example,
Java, Perl, Prolog (programing languages), mouse,
printer (hardware), Mac, PC (type of machines),
etc. querying the user to decide on the direction of
search by choosing one of these words. After all,
s/he knows best which of them comes closest to the
tw. Having started from the sw ’computer’, and
knowing that the tw is neither some kind of soft-
ware nor a type of computer, s/he would probably
choose Java, which is not only a programming lan-
guage but also an island. Taking this latter as the
6There is also at least one electronic incarnation
of a dictionary with reverse access, combining a dic-
tionary (WordNet) and an encyclopedia (Wikipedia)
(http://www.onelook.com/reverse-dictionary.shtml).
new starting point s/he might choose coffee (since
s/he is looking for some kind of beverage, possibly
made from an ingredient produced in Java, coffee),
and finally mocha, a type of beverage made from
these beans. Of course, the word Java might just
as well trigger Kawa which not only rhymes with
the sw, but also evokes Kawa Igen, a javanese vol-
cano, or familiar word of coffee in French.
As one can see, this approach allows word ac-
cess via multiple routes: there are many ways lead-
ing to Rome. Also, while the distance covered
in our example is quite unusual, it is possible to
reach the goal quickly. It took us actually very
few moves, four, to find an indirect link, between
two, fairly remotely related terms: computer and
mocha. Of course, cyber-coffee fans might be even
quicker in reaching their goal.
</bodyText>
<sectionHeader confidence="0.987781" genericHeader="method">
3 The lexical matrix revisited
</sectionHeader>
<bodyText confidence="0.99999178125">
The main question that we are interested in here
is how, or in what terms, to index the dictionary
in order to allow for quick and intuitive access to
words. Access should be possible on the basis
of meaning (or meaning elements), various kinds
of associations (most prominently ’syntagmatic’
ones) and, more generally speaking, underspeci-
fied input. To this end we have started to build an
association matrix (henceforth AM), akin to, yet
different from G. Miller’s initial proposal of WN
(Miller et al., 1990). He suggested to build a lex-
ical matrix by putting on one axis all the forms,
i.e. words of the language, and on the other, their
corresponding meanings. The latter being defined
in terms of synsets. The corresponding meaning-
form relations are signaled via a boolean (pres-
ence/absence). Hence, looking at the intersection
of meanings and forms, one can see which mean-
ings are expressed by, or converge toward what
forms, or conversely, what form expresses which
meanings. Whether this is the way WN is actually
implemented is not clear to us, though we believe
that it is not. Anyhow, our approach is different,
and we hope the reader will understand in a mo-
ment the reasons why.
We will also put on one axis all the form ele-
ments, i.e. the lemmata or expressions of a given
language (we refer to them as target words, hence-
forth tw). On the other axis we will place the trig-
gers or access-words (henceforth aw), that is, the
words or concepts capable and likely to evoke the
tw. These are typically the kind of words psy-
</bodyText>
<page confidence="0.995398">
11
</page>
<bodyText confidence="0.999979413043478">
chologists have gathered in their association ex-
periments (Jung and Riklin, 1906; Deese, 1965;
Schvaneveldt, 1989). Note, that instead of putting
a boolean value at the intersection of the tw and the
aw, we will put weights and the type of link hold-
ing between the co-occurring terms. This gives us
quadruplets. For example, an utterance like ”this
is the key of the door” might yield the aw (key),
the tw(door), the link type lt(part of), and a weight
(let’s say 15).
The fact that we have these two kinds of in-
formation is very important later on, as it allows
the search engine to cluster by type the possible
answers to be given in response to a user query
(word(s) provided as input) and to rank them.
Since the number of hits, i.e. words from which
the user must choose, may be substantial (depend-
ing on the degree of specification of the input), it is
important to group and rank them to ease naviga-
tion, allowing the user to find directly and quickly
the desired word, or at least the word with which
to continue search.
Obviously, different word senses (homographs),
require different entries (bank-money vs bank-
river), but so will synonyms, as every word-form,
synonym or not, is likely to be evoked by a differ-
ent key- or access-word (similarity of sound).7
Also, we will need a new line for every different
relation between a aw and a tw. Whether more than
one line is needed in the case of identical links be-
ing expressed by different linguistic resources (the
lock of the door vs. the door’s lock vs. the door
has a lock) remains an open empirical question.
Let us see quickly how our AM is supposed
to work. Imagine you wanted to find the word
for the following concept: hat of a bishop. In
such a case, any of the following concepts or
words might come to your mind: church, Vati-
can, abbot, monk, monastery, ceremony, ribbon,
and of course rhyming words like: brighter, fighter,
lighter, righter, tighter, writer,8 as, indeed, any of
them could remind us of the tw: mitre. Hence, all
of them are possible aw.
Once this resource is built, access is quite
straightforward. The user gives as input all the
words coming to his mind when thinking of a given
</bodyText>
<footnote confidence="0.930681333333333">
7Take, for example, the nouns rubbish and garbage which
can be considered as synonyms. Yet, while the former may
remind you of a rabbit or (horse)-radish, the latter may evoke
the word cabbage.
8The question, whether rhyming words should be com-
puted is not crucial at this stage.
</footnote>
<bodyText confidence="0.968817363636364">
idea or concept,9 and the system will display all
connected words. If the user can find the item he
is looking for in this list, search stops, otherwise
it will continue, the user giving other words of the
list, or words evoked by them.
Of course, remains the question of how to build
this resource, in particular, how to populate the
axis devoted to the trigger words, i.e. access-
keys. At present we consider three approaches:
one, where we use the words occurring in word
definitions (see also, (Dutoit and Nugues, 2002;
Bilac et al., 2004)), the other is to mine a well-
balanced corpus, to find co-occurrences within a
given window (Ferret and Zock, 2006), the size
depending a bit on the text type (encyclopedia) or
type of corpus. Still another solution would be
to draw on the association lists produced by psy-
chologists, see for example http://www.usf.edu/, or
http://www.eat.rl.ac.uk.
Of course, the idea of using matrices in linguis-
tics is not new. There are at least two authors who
have proposed its use: M. Gross (Gross, 1984)
used it for coding the syntactic behavior of lex-
ical items, hence the term lexicon-grammar, and
G. Miller, the father of WN (Miller et al., 1990)
suggested it to support lexical access. While the
former work is not relevant for us here, Miller’s
proposal is. What are the differences between his
proposal and ours? There are basically four main
differences:
1. we use, collocations or access-words, i.e aws
rather than synsets; Hence, any of the follow-
ing aws (cat, grey, computer device, cheese,
Speedy Gonzales) could point toward the tw
’mouse’, none of them are part of the mean-
ing, leave alone synonyms.
2. we mark explicitly the weight and the type of
link between the tw and the aw (isa, part of,
etc.),10 whereas WN uses only a binary value.
Both the weight and link are necessary infor-
mation for ranking and grouping, i.e. naviga-
tion.
3. our AM is corpus-sensitive (see below),
hence, we can, at least in principle, accommo-
</bodyText>
<footnote confidence="0.996760857142857">
9The quantifier all shouldn’t be taken too literally. What
we have in mind are ”salient” words available in the speaker’s
mind at a given moment
10Hence, if several links are possible between the tw and
the aw, several cells will be used. Think of the many possible
relations between a city and a country, example: Paris and
France (part of, biggest city of, located in, etc.)
</footnote>
<page confidence="0.997309">
12
</page>
<bodyText confidence="0.9994741">
date the fact that a speaker is changing topics,
adapting the weight of a given word or find a
more adequate aw in this new context. Think
of ’piano’ in the contexts of a concert or mov-
ing your household. Only the latter would
evoke the notion of weight.
4. relying on a corpus, we can take advantage of
syntagmatic associations (often encyclopedic
knowledge), something which is difficult to
obtain for WN.
</bodyText>
<sectionHeader confidence="0.98833" genericHeader="method">
4 Keep the set of lexical candidates small
</sectionHeader>
<bodyText confidence="0.999993125">
Here and in the next section we describe how the
idea of the AM has been computationally dealt
with. The goal is to reduce the number of hits,
i.e. possible tws (output), as a function of the in-
put, i.e, the number of relevant aws given by the
speaker/writer. To achieve this goal we apply lex-
ical functions to the aws, considering the intersec-
tion of the obtained sets to be the relevant tws.
</bodyText>
<subsectionHeader confidence="0.997695">
4.1 Lexical Functions
</subsectionHeader>
<bodyText confidence="0.993733303030303">
The usefulness of lexical functions for linguistics
in general and for language production in particu-
lar has been shown by Mel’ˇcuk (Mel’ˇcuk, 1996).
We will use them here, as they seem to fit also our
needs of information extraction or lexical access.
Mel’ˇcuk has coined the term lexical functions to
refer to the fact that two terms are systematically
related. For example, the lexical function Gener
refers to the fact that some term (let’s say ,cat-)
can be replaced by a more general term (let’s say
,animal-).
Lexical functions encode the combinability of
words. While ’big’ and ’strong’ express the same
idea (intensity, magnitude), they cannot be com-
bined freely with any noun: strong can be as-
sociated with fever, whereas big cannot. Of
course, this kind of combinability between lexical
terms is language specific, because unlike in En-
glish, in French one can say grosse fi`evre or forte
fi`evre, both being correct (Schwab and Lafourcade,
2007). Our AM handles, of course these kind of
functions. Here is a list of some of them:
- paradigmatic associations: hypernymy
(,cat- - ,animal-), hyponymy, synonymy, or
antonymy,... ;
- syntagmatic associations: collocations (,fear-
being associated with ,strong- or ,little-);
- morphological relations ie. terms being de-
rived from another part of speech: applying
the change-part-of-speech lexical function
fcpos to ,garden- will yield: fcpos(,garden-) =
{,to garden-, ,gardener-, ... }
- sound-related items: homophones, rhymes.
</bodyText>
<subsectionHeader confidence="0.999254">
4.2 Assumptions concerning search
</subsectionHeader>
<bodyText confidence="0.9987065">
The purpose of using lexical functions is to reduce
the number of possible outcomes from which the
user must choose. The list contains either the tw
or another promising aw the user may want use to
continue search. Hence, lexical functions are use-
ful for search provided that:
</bodyText>
<listItem confidence="0.9712296">
1. the speaker/writer is able to specify the kind
of relations s/he wants to use. The problem
here lies in the nature and number of the func-
tions, some of them being very well specified,
while others are not.
2. the larger the number of trigger words the
smaller the list of words from which to
choose: the speaker/writer can add or delete
words to broaden or narrow the scope of
his/her query.
</listItem>
<bodyText confidence="0.9960616">
These hypotheses are being modeled by using
set properties of lexical functions. The idea is to
apply all functions, or a selection of them, to the
aws and to give the speaker/writer the intersection
as result (see section 5.3.5 for an example)
</bodyText>
<sectionHeader confidence="0.999139" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999985363636364">
We have started with a simple, preliminary exper-
iment. Only one lexical function was used: neigh-
borhood (henceforth fneig). Let fneig be the func-
tion producing the set of co-occurring terms within
a given window (sentence or a paragraph).11 The
result produced by the system and returned to the
user is the intersection of the application of fneig
to the aws. In the next section we explain how this
function is applied to two corpora (Wordnet and
Wikipedia), to show their respective qualities and
shortcomings for this specific task.
</bodyText>
<subsectionHeader confidence="0.75572">
5.1 WordNet
5.1.1 Description
</subsectionHeader>
<bodyText confidence="0.989725">
WordNet (henceforth WN) is a lexical database
for English developed under the guidance of G.
11The scope or window size will vary with the text type
(normal text vs. encyclopedia). The optimal size is at this
point still an empirical question.
</bodyText>
<page confidence="0.997981">
13
</page>
<bodyText confidence="0.999721">
Miller (Miller et al., 1990). One of his goals was
to support lexical access akin to the human mind,
association-based. Knowledge is stored in a net-
work composed of nodes and links (nodes being
words or concepts and the links are the means of
connecting them) and access to knowledge, i.e.
search, takes place by entering the network at some
point and follow the links until one has reached the
goal (unless one has given up before). This kind
of navigation in a huge conceptual/lexical network
can be considered equivalent to spreading activa-
tion taking place in our brain.
Of course, such a network has to be built, and
navigational support must be provided to find the
location where knowledge or words are stored.
This is what Miller and his coworkers did by build-
ing WN. The resource has been built manually, and
it contains at present about 150.000 entries.
The structure of the dictionary is different from
conventional, alphabetical resources. Words are
organized in WN in two ways. Semantically sim-
ilar words, i.e. synonyms, are grouped as clus-
ters. These sets of synonyms, called synsets, are
then linked in various ways, depending on the
kind of relationship they entertain with the ad-
jacent synset. For example, their neighbors can
be more general or specific (hyperonymy vs. hy-
ponymy), they can be part of some reference ob-
ject (meronymy: car-motor), they can be the op-
posite (antonymy: hot-cold), etc. While WN is a
resource it can also be seen as a corpus.
</bodyText>
<subsubsectionHeader confidence="0.825293">
5.1.2 Using WN as a corpus
</subsubsectionHeader>
<bodyText confidence="0.999936352941177">
There are many good reasons to use WN for
learning f,,,. For one, there are many extensions,
and second, the one we are using, eXtended WN
(Mihalcea and Moldovan, 2001) spares us the trou-
ble of having to address issues like: (a) seg-
mentation: we do not need to identify sentence
boundaries ; (b) semantic ambiguity: words being
tagged, we get good precision; (c) lemmatization:
since only verbs, nouns, adjectives and adverbs are
tagged, we need neither a stoplist nor a lemmatizer.
Despite all these qualities, two important prob-
lems remain nevertheless for this kind of corpus:
(a) size: though, all words are tagged, the cor-
pus remains small as it contains only 63.941 dif-
ferent words; (b) in consequence, the corpus lacks
many syntagmatic associations encoding encyclo-
pedic knowledge.
</bodyText>
<subsectionHeader confidence="0.999493">
5.2 Using Wikipedia as corpus
</subsectionHeader>
<bodyText confidence="0.999937642857143">
Wikipedia is a free, multilingual encyclopedia, ac-
cessible on the Web.12 For our experiment we have
chosen the English version which of this day (12th
of may 2008) contains 2,369,180 entries.
Wikipedia has exactly the opposite properties of
WN. While it covers well encyclopedic relations, it
is only raw text. Hence problems like text segmen-
tation, lemmatisation and stoplist definition need
to be addressed.
Our experiments with Wikipedia were very rudi-
mentary, given that we considered only 1000 doc-
uments. These latter were obtained in response to
the term ,wine-, by following the links obtained for
about 72.000 words.
</bodyText>
<subsectionHeader confidence="0.9813935">
5.3 Prototype
5.3.1 Building the resource and using it.
</subsectionHeader>
<bodyText confidence="0.999978785714286">
Building the resource requires processing a cor-
pus and building the database. Given a corpus
we apply our neighborhood function to a prede-
termined window (a paragraph in the case of ency-
clopedias).13 The result, i.e. the co-occurrences,
will be stored in the database, together with their
weight, i.e. number of times two terms appear to-
gether, and the type of link. As mentionned above,
both kinds of information are needed later on for
ranking and navigation.14
At present, cooccurences are stored as triplets
(t,,,, a,,,, times), where times represents the number
of times the two terms cooccur in the corpus, the
scope of coccurence being here the paragraph.
</bodyText>
<subsectionHeader confidence="0.897418">
5.3.2 Processing of the Wikipedia page
</subsectionHeader>
<bodyText confidence="0.9999367">
For each Wikipedia page, a preprocessor
converts HTML pages into plain text. Next,
a part-of-speech tagger (http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/) is used
to annotate all the words of the paragraph under
consideration. This allows the filtering of all
irrelevant words, to keep but a bag of words,
that is, the nouns, adjectives, verbs and adverbs
occuring in the paragraph. These words will be
used to fill the triplets of our database.
</bodyText>
<footnote confidence="0.994866125">
12http://www.wikipedia.org
13The optimal window-size depends probably on the text
type (encyclopedia vs. unformatted text). Yet, in the absence
of clear criteria, we consider the optimal window-size as an
open, empirical question.
14This latter aspect is not implemented yet, but will be
added in the future, as it is a necessary component for easy
navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007).
</footnote>
<page confidence="0.995658">
14
</page>
<table confidence="0.961844958333333">
Input WordNet Wikipedia
488 words 1845
words
alcoholic country
god characteristics
regulation grape
appellation system
bottled like
christian track
. . . . . .
wine grape sweet
serve france
small fruit
dry bottle
produce red
bread hold
. . . . . .
30 words 983 words
month fish produce grain
grape revolutionary autumn farms
calendar festival energy cut
harvest butterfish dollar combine ground
person make balance rain
wine first amount rich
</table>
<subsectionHeader confidence="0.487241">
5.3.3 Corpus Building
</subsectionHeader>
<bodyText confidence="0.994914785714286">
We start arbitrarily from some page (for our ex-
periment, we have chosen ”wine” as input), apply
the algorithm outlined here above and pick then
randomly a noun within this page to fetch with this
input a new page on Wikipedia. This process is re-
peated until a given sample size is obtained (in our
case 1000 pages). Of course, instead of picking
randomly a noun, we could have decided to pro-
cess all the nouns of a given page, and to add then
incrementally the nouns of the next pages. Yet,
doing this would have led us to privilege a specific
topic (in our case ’wine’) instead of a more general
one.
. . . . . . . . . . . .
</bodyText>
<subsectionHeader confidence="0.827712">
5.3.4 Usage
</subsectionHeader>
<bodyText confidence="0.999996166666667">
We have developed a website in Java as a
servlet. Interactions with humans are simple: peo-
ple can add or delete a word from the current list
(see Input in the figure on top of the next page).
The example presented shows that with very few
words, hence very quickly, we can obtain the de-
sired word.
Given some input, the system provides the user
with a list of words cooccuring with the aws. The
output is an ordered list of words, the order de-
pending on the overall score, i.e. number of cooc-
currences between the aw and the tw. For exam-
ple, if the aws ’wine’ and ’harvest’ co-occur with
the tw ’bunch’ respectively 5 and 8 times, then
the overall score of cooccurence of ’bunch’ is 13:
((wine, harvest), bunch, 13). Hence, all words with
a higher score will precede it, while those with a
lower score will follow it.
</bodyText>
<subsectionHeader confidence="0.8874925">
5.3.5 Examples and Comparison of the
results of the two corpora
</subsectionHeader>
<bodyText confidence="0.999990066666667">
Here below are the examples extracted from the
WN corpus (see figure-1). Our goal was to find
the word ,vintage-. Trigger words are ,wine- and
,harvest-, yielding respectively 488 and 30 hits, i.e.
words. As one can see ,harvest- is a better ac-
cess term than ,wine-. Combining the two will re-
duce the list to 6 items. Please note that the tw
,vintage- is not among them, eventhough it exists
in WordNet, which illustrates nicely the fact that
storage does not guarantee accessibility (Sinopal-
nikova and Smrz, 2006).
Looking at figure-1 you will see that the results
have improved considerably with Wikipedia. The
same input, ,wine- evokes many more words (1845
as opposed to 488). For ,harvest- we get 983 hits in-
</bodyText>
<figure confidence="0.995936625">
6 words 45 words
make grape grape vintage
wine fish someone bottle produce
+harvest commemorate person fermentation juice
. . . . . . Beaujolais taste
viticulture France
Bordeaux vineyard
. . . . . .
</figure>
<figureCaption confidence="0.8179035">
Figure 1: Comparing two corpora (eXtended
WordNet and Wikipedia) with various inputs
</figureCaption>
<bodyText confidence="0.999974">
stead of 30 (the intersection containing 62 words).
Combining the two reduces the set to 45 items
among which we will find, of course, the target
word.
We hope that this example is clear enough to
convince the reader that it makes sense to use real
text as corpus to extract from it the kind of in-
formation (associations) people are likely to give
when looking for a word.
</bodyText>
<sectionHeader confidence="0.992361" genericHeader="conclusions">
6 Conclusion and perspectives
</sectionHeader>
<bodyText confidence="0.9999908">
We have addressed in this paper the problem of
word finding for speakers or writers. Conclud-
ing that most dictionaries are not well suited to al-
low for this kind of reverse access based on mean-
ings (or meaning related elements, associations),
we looked at work done by psychologists to get
some inspiration. Next we tried to clarify which
of these findings could help us build the dictionary
of tomorrow, that is, a tool integrating linguistic
and encyclopedic knowledge, allowing navigation
by taking either or as starting point. While lin-
guistic knowledge is more prominent for analysis
(reading), encyclopedic facts are more relevant for
production. We’ve presented then our ideas of how
to build a resource, allowing lexical access based
</bodyText>
<page confidence="0.992441">
15
</page>
<bodyText confidence="0.999981272727273">
on underspecified, i.e. imperfect input. To achieve
this goal we’ve started building an AM composed
of form elements (the words and expressions of
a given language) and aws. The role of the lat-
ter being to lead to or to evoke the tw. In the last
part we’ve described briefly the results obtained by
comparing two resources (WN and Wikipedia) and
various inputs. Given the fact that the project is
still quite young, only preliminary results can be
shown at this point.
Our next steps will be to take a closer look at the
following work: clustering of similar words (Lin,
1998), topic signatures (Lin and Hovy, 2000) and
Kilgariff’s sketch engine (Kilgarriff et al., 2004).
We plan also to add other lexical functions to en-
rich our database with aws. We plan to experiment
with corpora, trying to find out which ones are best
for our purpose15 and we will certainly experiment
with the window size 16 to see which size is best
for which text type. Finally, we plan to insert in
our AM the relations holding between the aw and
the tw. As these links are contained in our corpus,
we should be able to identify and type them. The
question is, to what extent this can be done auto-
matically.
Obviously, the success of our resource will de-
pend on the quality of the corpus, the quality of
the aws, weights and links, and the representativ-
ity of all this for a given population. While we do
believe in the justification of our intuitions, more
work is needed to reveal the true potential of the
approach. The ultimate judge being, of course, the
future user.
</bodyText>
<footnote confidence="0.9878775">
15For example, we could consider a resource like Con-
ceptNet of the Open Mind Common-Sense project (Liuh and
Singh, 2004).
16For example, it would have been interesting to consider
coocurrences beyond the scope of the paragraph, by consider-
ing the logical structure of the Wikipedia document. Anyhow,
our experiment needs to be redone with more data than just
1000 pages, the size chosen here for lack of time. Indeed one
could consider using the entire corpus of Wikipedia or mixed
corpora
</footnote>
<sectionHeader confidence="0.997052" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995717190476191">
Aitchinson, Jean. 2003. Words in the Mind: an Intro-
duction to the Mental Lexicon (3d edition). Black-
well, Oxford.
Bilac, S., W. Watanabe, T. Hashimoto, T. Tokunaga,
and H. Tanaka. 2004. Dictionary search based
on the target word description. In Proc. of the
Tenth Annual Meeting of The Association for NLP
(NLP2004), pages 556–559, Tokyo, Japan.
Boissi`ere, P. 1862. Dictionnaire analogique de la
langue franc¸aise : r´epertoire complet des mots par
les id´ees et des id´ees par les mots. Larousse et A.
Boyer, Paris.
Brown, R. and D. McNeill. 1996. The tip of the tounge
phenomenon. Journal of Verbal Learning and Ver-
bal Behaviour, 5:325–337.
Cutler, A, editor, 1982. Slips of the Tongue and Lan-
guage Production. Mouton, Amsterdam.
Deese, James. 1965. The structure of associations in
language and thought. Johns Hopkins Press.
Dong, Zhendong and Qiang Dong. 2006. HOWNET
and the computation of meaning. World Scientific,
London.
Dutoit, Dominique and P. Nugues. 2002. A lexical
network and an algorithm to find words from defini-
tions. In van Harmelen, F., editor, ECAI2002, Proc.
of the 15th European Conference on Artificial Intel-
ligence, pages 450–454, Lyon. IOS Press, Amster-
dam.
Fellbaum, Christiane, editor, 1998. WordNet: An Elec-
tronic Lexical Database and some of its Applica-
tions. MIT Press.
Ferret, Olivier and Michael Zock. 2006. Enhancing
electronic dictionaries with an index based on associ-
ations. In ACL ’06: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the ACL, pages 281–288.
Fontenelle, Thierry. 1997. Using a bilingual dictionary
to create semantic networks. International Journal
of Lexicography, 10(4):275–303.
Goddard, Cliff. 1998. Bad arguments against seman-
tic primitives. Theoretical Linguistics, 24(2-3):129–
156.
</reference>
<page confidence="0.973885">
16
</page>
<reference confidence="0.993716009615385">
Gross, Maurice. 1984. Lexicon-grammar and the anal-
ysis of french. In Proc. of the 11th COLING, pages
275–282, Stanford, CA.
Richardson, S., W. Dolan, and L. Vanderwende. 1998.
Mindnet: Acquiring and structuring semantic infor-
mation from text. In ACL-COLING’98, pages 1098–
1102.
Harley, Trevor. 2004. The psychology of language.
Psychology Press, Taylor and Francis, Hove and
New York.
Jung, Carl and F. Riklin. 1906. Experimentelle
Untersuchungen ¨uber Assoziationen Gesunder. In
Jung, C. G., editor, Diagnostische Assoziationsstu-
dien, pages 7–145. Barth, Leipzig, Germany.
Kilgarriff, Adam, Pavel Rychl´y, Pavel Smrˇz, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of the Eleventh EURALEX International Congress,
pages 105–116, Lorient, France.
Levelt, Willem. 1996. A theory of lexical access
in speech production. In Proc. of the 16th Con-
ference on Computational Linguistics, Copenhagen,
Denmark.
Lin, Chin-Yew and Eduard H. Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In COLING, pages 495–501. Morgan Kauf-
mann.
Lin, Dekang. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768–774,
Montreal.
Liuh, H. and P. Singh. 2004. ConceptNet: a practi-
cal commonsense reasoning toolkit. BT Technology
Journal.
Mel’ˇcuk, I., N. Arbatchewsky-Jumarie, L. Iordanskaja,
S. Mantha, and A. Polgu`ere. 1999. Dictionnaire
explicatif et combinatoire du franc¸ais contemporain
Recherches lexico-s´emantiques IV. Les Presses de
l’Universit´e de Montr´eal, Montr´eal.
Mel’ˇcuk, Igor. 1996. Lexical functions: A tool for
the description of lexical relations in the lexicon. In
Wanner, L., editor, Lexical Functions in Lexicogra-
phy and Natural Language Processing, pages 37–
102. Benjamins, Amsterdam/Philadelphia.
Mihalcea, Rada and Dan Moldovan. 2001. Extended
WordNet: progress report. In NAACL 2001 - Work-
shop on WordNet and Other Lexical Resources, Pitts-
burgh, USA.
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-
line lexical database. International Journal of Lexi-
cography, 3(4), pages 235–244.
Moerdijk, Fons. 2008. Frames and semagrams; Mean-
ing description in the general dutch dictionary. In
Proceedings of the Thirteenth Euralex International
Congress, EURALEX, Barcelona.
Polgu`ere, Alain. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. Sidney. Coling workshop ’Multilingual Lan-
guage Resources and Interoperability’.
Robert, Paul, Alain Rey, and J. Rey-Debove. 1993.
Dictionnaire alphabetique et analogique de la
Langue Franc¸aise. Le Robert, Paris.
Roget, P. 1852. Thesaurus of English Words and
Phrases. Longman, London.
Schvaneveldt, R., editor, 1989. Pathfinder Associa-
tive Networks: studies in knowledge organization.
Ablex, Norwood, New Jersey, US.
Schwab, Didier and Mathieu Lafourcade. 2007. Mod-
elling, detection and exploitation of lexical functions
for analysis. ECTI Transactions Journal on Com-
puter and Information Technology, 2(2):97–108.
Sierra, Gerardo. 2000. The onomasiological dictio-
nary: a gap in lexicography. In Proceedings of the
Ninth Euralex International Congress, pages 223–
235, IMS, Universit¨at Stuttgart.
Sinopalnikova, Anna and Pavel Smrz. 2006. Knowing
a word vs. accessing a word: Wordnet and word as-
sociation norms as interfaces to electronic dictionar-
ies. In Proceedings of the Third International Word-
Net Conference, pages 265–272, Korea.
Summers, Della. 1993. Language Activator: the
world’s first production dictionary. Longman, Lon-
don.
T’ong, Ting-K¨u. 1862. Ying u¨ tsap ts’¨un (The Chinese
and English Instructor). Canton.
Wierzbicka, Anna. 1996. Semantics: Primes and Uni-
versals. Oxford University Press, Oxford.
Wilks, Yorick. 1977. Good and bad arguments about
semantic primitives. Communication and Cognition,
10(3–4):181–221.
Zock, Michael and Slaven Bilac. 2004. Word lookup
on the basis of associations : from an idea to a
roadmap. In Workshop on ’Enhancing and using
electronic dictionaries’, pages 29–35, Geneva. COL-
ING.
Zock, Michael. 2006. Navigational aids, a critical
factor for the success of electronic dictionaries. In
Rapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp,
editors, Perspectives on Cognition: A Festschrift for
Manfred Wettler, pages 397–414. Pabst Science Pub-
lishers, Lengerich.
Zock, Michael. 2007. If you care to find what you
are looking for, make an index: the case of lexical
access. ECTI, Transaction on Computer and Infor-
mation Technology, 2(2):71–80.
</reference>
<page confidence="0.99941">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.995241">Lexical Access Based on Underspecified Input</title>
<author confidence="0.973084">Michael</author>
<affiliation confidence="0.817031">Equipe</affiliation>
<address confidence="0.747358">163, Avenue de F-13288 Marseille Cedex</address>
<email confidence="0.833517">michael.zock@lif.univ-mrs.fr</email>
<author confidence="0.529737">Didier</author>
<affiliation confidence="0.723673">Groupe Laboratoire d’Informatique de</affiliation>
<address confidence="0.8635505">385 avenue de la Bibliothque - BP F-38041 Grenoble Cedex</address>
<email confidence="0.938459">didier.schwab@imag.fr</email>
<abstract confidence="0.993238394422312">Words play a major role in language production, hence finding them is of vital importance, be it for speaking or writing. Words are stored in a dictionary, and the general belief holds, the bigger the better. Yet, to be truly useful the resource should contain not only many entries and a lot of information concerning each one of them, but also adequate means to reveal the stored information. Information access depends crucially on the organization of the data (words) and on the navigational tools. It also depends on the grouping, ranking and indexing of the data, a factor too often overlooked. We will present here some preliminary results, showing how an existing electronic dictionary could be enhanced to support language producers to find the word they are looking for. To this end we have started build a corpus-based macomposed of words ackeys elements, related concepts/words), the two being connected at intersection in terms of of information used subsequently for grouping, ranking and navigation. 1 Context and problem When speaking or writing we encounter basically either of the following two situations: one where everything works automatically, somehow like magic, words popping up one after another Licensed under the Commons Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. like spring water, and another where we look deliberately and often painstakingly for a specific, possibly known word. We will be concerned here with this latter situation: a speaker/ writer using an electronic dictionary to look for such a word. Unfortunately, alphabetically organized dictionarare not well suited for this kind of the inputs are meanings (elements of the word’s definition) or conceptually related elements (collocations, associations), and the outputs the target words. Without any doubt, lexicographers have made considerable efforts to assist language users, building huge resources, composed of many words and lots of information associated with each one of them. Still, it is not unfair to say most dictionaries have been conceived from the reader’s point of view. The lexicographers have hardly taken into the language producer’s considering conceptual input, incomplete as it may be, starting point. While with words, looking generally for their corresponding meanstart with the opmeanings or which should be the entry points of a dictionary, which ideally is neuin terms of access The problem is that we still don’t know very what whether they are compoand if so, how many are (Wilks, 1977; Wierzbicka, 1996; Goddard, 1998). 1852), Miller and Fellbaum’s 1998) and Longman’s Activa- 1993), being notable exceptions (For more details, see next section). course, this does not preclude, that we may have to refer to them in a concept-based query. we agree with Polgu`ere theoretically when he pleads for dictionary neutrality with regard to lexical access (Polgu`ere, 2006), from a practical point of view the situation is obviously quite different for the speaker and listener, even if both of them draw on the same resource. 9 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), Manchester, August 2008 Neither do we know how to represent them. Yet, there are ways around this problem as we will Whether organized and accessed differently is a question we cannot answer here. We can agree though on the fact that getting information concerning words is fairly unproblematic when reading, at least in the case of most western languages. Words can generally be found easily in a dictionary, provided the user knows the spelling, the alphabet and how to build lemma starting from an inflected form. Unwhich are organized alphabetically (in western languages) or by form (stroke counts in organized topically: they are clustered into functional groups according to their role in real world, or our perception of it. Psychologist have studied the difficulties people have when trying to produce or access words (Aitchinson, 2003). In particular, they have studthe phenomenon and McNeill, 1996) and the effects an input can have the an output (error analysis (Cutler, and on the its production: positive or negative priming effect (activation/inhibition). Obviously, these findings allow certain conclusions, and they might guide us when developing tools to help people find the needed word. In particular, they reveal two facts highly relevant for our goal: 1. even if people fail to access a given word, they know a lot about it: (word definition, role played in a given sitof of sounding etc. Yet, despite all this knowledge, they seem to lack some crucial information to be able to produce the phonetic form. The word gets blocked at the very last moment, even though it has reached the This kind of nuisance is all the more likely as the target word is rare and primed by a similar sounding word. 2. unlike words in printed or electronic dictionaries, words in our mind may be inexistent as tokens. What we seem to have in our minds are decomposed, abstract entities need to be synthesized over Acmay be very surprising, yet, this need not be the case if we consider the fact that speech errors are nearly always due to competing elements from the same level or an adjacent one, unless they are the result of a surrounding concept which has been activated, or which is about to be translated cording to Levelt (Levelt, 1996) the generation of words (synthesis) involves the following stages: conceptual preparation, lexical selection, phonologicaland phonetic encoding, articulation. Bear in mind that having performed ’lexical selection’ does not imply access to the phonetic form (see the experiments the What can be concluded from these observations? It seems that underspecified input is sufficiently frequent to be considered as normal. Hence we should accept it, and make the best out of it by using whatever information is available (accessible), no matter how incomplete, since it may still contribute to find the wanted information, be it by reducing the search space. Obviously, the more information we have the better, as this reduces the number of words among which to choose. 2 Related work and goal While more dictionaries have been built for the reader than for the writer, there have been some onomasiological attempts as early as in the middle of the 19th century. For example, Roget’s 1852), T’ong’s and instructor 1862), or Boissiere’s dictionary Newer includes Mel’ˇcuk’s et al., Miller and Fellbaum’s Richardson and Dolan’s (Richardet al., 1998), Dong’s and 2006) and Longman’s Activa- 1993). There is also the work of into words. Put differently, we do not store words at all in our mind, at least not in the layman’s or lexicographer’s sense who consider word-forms and their meanings as one. If we are right, than rather continue to consider the human mind as store could consider it as a Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (for a good survey see (Harley, 2004), pages 359-374) one gets the impression that words are synthesized rather than located and read out. Taking a look at all this work, generally connectionist models, one may conclude that, rather than having words in our mind we have a set of more or less abstract features (concepts, syntactic information, phonemes), distributed across various layers, which need to be synthesized over time. To do so we proceed from abstract meanings to concrete sounds, which at some point were also just abstract features. By propagating energy rather than data (as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever), that we propagate signals, activating ultimately certain peripheral organs (larynx, tongue, mouth, lips, hands) in such a way as to produce movements or sounds, that, not knowing better, we call words. a more recent proposal see (Robert et al., 1993). 10 (Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008), dictionaries OECD) and Finally, there is M. Rundell’s MEDAL, a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). As one can see, a lot of progress has been accomplished over the last few years, yet more can be especially with regard to unifying Let’s take an example to illustrate our point. Suppose, you were looking for a word expressing the following ideas: ’superior dark coffee made from beans from Arabia’, and that you knew that target word was neither cappuc- While none of this would lead you directly the intended word, the information at hand, i.e. the word’s definition or some of its elements, could certainly be used. In addition, people on knowledge concerning the concept (or word) plays in language and in real world, i.e. the associations it evokes. For example, they may that they are looking for a for under certain circumthat the certain properties, etc. In sum, people have in their mind an encyclopedia: all words, concepts or ideas being highly connected. Hence, any one of them has the potential to evoke the others. The likelihood for this to happen of course, on factors such as strength), vs. indirect etc. How is this supposed to work for a dictionary user? Suppose you were looking for the word word: yet the only token comto your mind were word: Taking this latter as starting point, the system would show all the connected words, for example, Perl, Prolog languages), PC of machines), etc. querying the user to decide on the direction of search by choosing one of these words. After all, s/he knows best which of them comes closest to the Having started from the ’computer’, and that the is neither some of softa of s/he would probably which is not only a lanalso an Taking this latter as the is also at least one electronic incarnation of a dictionary with reverse access, combining a dictionary (WordNet) and an encyclopedia (Wikipedia) (http://www.onelook.com/reverse-dictionary.shtml). starting point s/he might choose is looking for some kind of possibly made from an ingredient produced in Java, coffee), finally a type of from beans. Of course, the word just well trigger not only rhymes with but also evokes a javanese volor familiar word of French. As one can see, this approach allows word access via multiple routes: there are many ways leading to Rome. Also, while the distance covered in our example is quite unusual, it is possible to reach the goal quickly. It took us actually very few moves, four, to find an indirect link, between fairly remotely related terms: Of course, might be even quicker in reaching their goal. 3 The lexical matrix revisited The main question that we are interested in here is how, or in what terms, to index the dictionary in order to allow for quick and intuitive access to words. Access should be possible on the basis of meaning (or meaning elements), various kinds of associations (most prominently ’syntagmatic’ ones) and, more generally speaking, underspecified input. To this end we have started to build an matrix AM), akin to, yet different from G. Miller’s initial proposal of WN (Miller et al., 1990). He suggested to build a lexmatrix by putting on one axis all the i.e. words of the language, and on the other, their The latter being defined in terms of synsets. The corresponding meaningform relations are signaled via a boolean (presence/absence). Hence, looking at the intersection of meanings and forms, one can see which meanings are expressed by, or converge toward what forms, or conversely, what form expresses which meanings. Whether this is the way WN is actually implemented is not clear to us, though we believe that it is not. Anyhow, our approach is different, and we hope the reader will understand in a moment the reasons why. We will also put on one axis all the form elei.e. the expressions of a given (we refer to them as hence- On the other axis we will place the trigthat is, the words or concepts capable and likely to evoke the These are typically the kind of words psy- 11 chologists have gathered in their association experiments (Jung and Riklin, 1906; Deese, 1965; Schvaneveldt, 1989). Note, that instead of putting boolean value at the intersection of the and the we will put the of link holding between the co-occurring terms. This gives us quadruplets. For example, an utterance like ”this the key of the door” might yield the (key), the link type of), and a weight (let’s say 15). The fact that we have these two kinds of information is very important later on, as it allows the search engine to cluster by type the possible answers to be given in response to a user query (word(s) provided as input) and to rank them. Since the number of hits, i.e. words from which the user must choose, may be substantial (depending on the degree of specification of the input), it is important to group and rank them to ease navigation, allowing the user to find directly and quickly the desired word, or at least the word with which to continue search. Obviously, different word senses (homographs), require different entries (bank-money vs bankriver), but so will synonyms, as every word-form, synonym or not, is likely to be evoked by a differkeyor access-word (similarity of Also, we will need a new line for every different between a and a Whether more than one line is needed in the case of identical links being expressed by different linguistic resources (the door vs. the vs. the door lock) remains an open empirical question. Let us see quickly how our AM is supposed to work. Imagine you wanted to find the word the following concept: of a In such a case, any of the following concepts or words might come to your mind: church, Vatican, abbot, monk, monastery, ceremony, ribbon, and of course rhyming words like: brighter, fighter, righter, tighter, as, indeed, any of could remind us of the Hence, all them are possible Once this resource is built, access is quite straightforward. The user gives as input all the words coming to his mind when thinking of a given for example, the nouns can be considered as synonyms. Yet, while the former may you of a the latter may evoke word question, whether rhyming words should be computed is not crucial at this stage. or and the system will display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the devoted to the trigger words, i.e. access- At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexitems, hence the term and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not relevant for us here, Miller’s proposal is. What are the differences between his proposal and ours? There are basically four main differences: we use, collocations or i.e than Hence, any of the followgrey, computer device, cheese, Gonzales) could point toward the ’mouse’, none of them are part of the meaning, leave alone synonyms. we mark explicitly the the of the and the (isa, part of, whereas WN uses only a binary value. the necessary information for ranking and grouping, i.e. navigation. 3. our AM is corpus-sensitive (see below), we can, at least in principle, accommoquantifier be taken too literally. What we have in mind are ”salient” words available in the speaker’s mind at a given moment if several links are possible between the and several cells will be used. Think of the many possible between a city and a country, example: of, biggest city of, located in, etc.) 12 date the fact that a speaker is changing topics, adapting the weight of a given word or find a adequate in this new context. Think of ’piano’ in the contexts of a concert or moving your household. Only the latter would evoke the notion of weight. 4. relying on a corpus, we can take advantage of associations encyclopedic knowledge), something which is difficult to obtain for WN. 4 Keep the set of lexical candidates small Here and in the next section we describe how the idea of the AM has been computationally dealt with. The goal is to reduce the number of hits, possible as a function of the ini.e, the number of relevant by the speaker/writer. To achieve this goal we apply lexfunctions to the considering the intersecof the obtained sets to be the relevant 4.1 Lexical Functions usefulness of functions linguistics in general and for language production in particular has been shown by Mel’ˇcuk (Mel’ˇcuk, 1996). We will use them here, as they seem to fit also our needs of information extraction or lexical access. has coined the term functions refer to the fact that two terms are systematically For example, the lexical function to the fact that some term (let’s say can be replaced by a more general term (let’s say Lexical functions encode the combinability of words. While ’big’ and ’strong’ express the same idea (intensity, magnitude), they cannot be comfreely with any noun: be aswith whereas Of course, this kind of combinability between lexical terms is language specific, because unlike in Enin French one can say fi`evre both being correct (Schwab and Lafourcade, 2007). Our AM handles, of course these kind of functions. Here is a list of some of them: hypernymy hyponymy, synonymy, or antonymy,... ; collocations associated with relations terms being derived from another part of speech: applying function yield: = ... homophones, rhymes. 4.2 Assumptions concerning search The purpose of using lexical functions is to reduce the number of possible outcomes from which the must choose. The list contains either the another promising the user may want use to continue search. Hence, lexical functions are useful for search provided that: 1. the speaker/writer is able to specify the kind of relations s/he wants to use. The problem here lies in the nature and number of the functions, some of them being very well specified, while others are not. 2. the larger the number of trigger words the smaller the list of words from which to choose: the speaker/writer can add or delete words to broaden or narrow the scope of his/her query. These hypotheses are being modeled by using set properties of lexical functions. The idea is to apply all functions, or a selection of them, to the to give the speaker/writer the intersection as result (see section 5.3.5 for an example) 5 Experiment We have started with a simple, preliminary experiment. Only one lexical function was used: neigh- (henceforth Let the function producing the set of co-occurring terms within given window (sentence or a The result produced by the system and returned to the is the intersection of the application of the In the next section we explain how this function is applied to two corpora (Wordnet and Wikipedia), to show their respective qualities and shortcomings for this specific task. 5.1 WordNet 5.1.1 Description WordNet (henceforth WN) is a lexical database for English developed under the guidance of G. scope or window size will vary with the text type (normal text vs. encyclopedia). The optimal size is at this point still an empirical question. 13 Miller (Miller et al., 1990). One of his goals was to support lexical access akin to the human mind, stored in a network composed of nodes and links (nodes being words or concepts and the links are the means of them) and to i.e. search, takes place by entering the network at some point and follow the links until one has reached the goal (unless one has given up before). This kind a huge conceptual/lexical network be considered equivalent to activaplace in our brain. Of course, such a network has to be built, and navigational support must be provided to find the location where knowledge or words are stored. This is what Miller and his coworkers did by building WN. The resource has been built manually, and it contains at present about 150.000 entries. The structure of the dictionary is different from conventional, alphabetical resources. Words are organized in WN in two ways. Semantically similar words, i.e. synonyms, are grouped as clus- These sets of synonyms, called are then linked in various ways, depending on the kind of relationship they entertain with the adjacent synset. For example, their neighbors can more vs. hythey can be of reference ob- (meronymy: car-motor), they can be the ophot-cold), etc. While WN is a resource it can also be seen as a corpus. 5.1.2 Using WN as a corpus There are many good reasons to use WN for For one, there are many extensions, and second, the one we are using, eXtended WN (Mihalcea and Moldovan, 2001) spares us the trouble of having to address issues like: (a) segmentation: we do not need to identify sentence boundaries ; (b) semantic ambiguity: words being tagged, we get good precision; (c) lemmatization: since only verbs, nouns, adjectives and adverbs are tagged, we need neither a stoplist nor a lemmatizer. Despite all these qualities, two important problems remain nevertheless for this kind of corpus: (a) size: though, all words are tagged, the corpus remains small as it contains only 63.941 different words; (b) in consequence, the corpus lacks associations encyclopedic knowledge. 5.2 Using Wikipedia as corpus Wikipedia is a free, multilingual encyclopedia, acon the For our experiment we have chosen the English version which of this day (12th of may 2008) contains 2,369,180 entries. Wikipedia has exactly the opposite properties of WN. While it covers well encyclopedic relations, it is only raw text. Hence problems like text segmentation, lemmatisation and stoplist definition need to be addressed. Our experiments with Wikipedia were very rudimentary, given that we considered only 1000 documents. These latter were obtained in response to term by following the links obtained for about 72.000 words. 5.3 Prototype 5.3.1 Building the resource and using it. Building the resource requires processing a corpus and building the database. Given a corpus we apply our neighborhood function to a predetermined window (a paragraph in the case of ency- The result, i.e. the co-occurrences, will be stored in the database, together with their i.e. number of times two terms appear toand the of As mentionned above, both kinds of information are needed later on for At present, cooccurences are stored as triplets where the number of times the two terms cooccur in the corpus, the scope of coccurence being here the paragraph. 5.3.2 Processing of the Wikipedia page For each Wikipedia page, a preprocessor converts HTML pages into plain text. Next, a part-of-speech tagger (http://www.ims.unistuttgart.de/projekte/corplex/TreeTagger/) is used to annotate all the words of the paragraph under consideration. This allows the filtering of all irrelevant words, to keep but a bag of words, that is, the nouns, adjectives, verbs and adverbs occuring in the paragraph. These words will be used to fill the triplets of our database. optimal window-size depends probably on the text type (encyclopedia vs. unformatted text). Yet, in the absence of clear criteria, we consider the optimal window-size as an open, empirical question. latter aspect is not implemented yet, but will be added in the future, as it is a necessary component for easy navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). 14 Input WordNet Wikipedia 488 words 1845 words alcoholic country god characteristics regulation grape appellation system bottled like christian track . . . . . . wine grape sweet serve france small fruit dry bottle produce red bread hold . . . . . . 30 words 983 words month fish produce grain grape revolutionary autumn farms calendar festival energy cut dollar combine ground person make balance rain wine first amount rich 5.3.3 Corpus Building We start arbitrarily from some page (for our experiment, we have chosen ”wine” as input), apply the algorithm outlined here above and pick then randomly a noun within this page to fetch with this input a new page on Wikipedia. This process is repeated until a given sample size is obtained (in our case 1000 pages). Of course, instead of picking randomly a noun, we could have decided to process all the nouns of a given page, and to add then incrementally the nouns of the next pages. Yet, doing this would have led us to privilege a specific topic (in our case ’wine’) instead of a more general one. . . . . . . . . . . . . 5.3.4 Usage We have developed a website in Java as a servlet. Interactions with humans are simple: people can add or delete a word from the current list the figure on top of the next page). The example presented shows that with very few words, hence very quickly, we can obtain the desired word. Given some input, the system provides the user a list of words cooccuring with the The output is an ordered list of words, the order depending on the overall score, i.e. number of coocbetween the and the For examif the and ’harvest’ co-occur with ’bunch’ respectively 5 and 8 times, then the overall score of cooccurence of ’bunch’ is 13: ((wine, harvest), bunch, 13). Hence, all words with a higher score will precede it, while those with a lower score will follow it. 5.3.5 Examples and Comparison of the results of the two corpora Here below are the examples extracted from the WN corpus (see figure-1). Our goal was to find word Trigger words are yielding respectively 488 and 30 hits, i.e. As one can see a better acterm than Combining the two will rethe list to 6 items. Please note that the not among them, eventhough it exists in WordNet, which illustrates nicely the fact that storage does not guarantee accessibility (Sinopalnikova and Smrz, 2006). Looking at figure-1 you will see that the results have improved considerably with Wikipedia. The input, many more words (1845 opposed to 488). For get 983 hits in- 6 words 45 words make grape grape vintage someone bottle produce person fermentation juice . . . . . . Beaujolais taste viticulture France Bordeaux vineyard . . . . . . 1: Comparing two corpora with various inputs stead of 30 (the intersection containing 62 words). Combining the two reduces the set to 45 items among which we will find, of course, the target word. We hope that this example is clear enough to convince the reader that it makes sense to use real text as corpus to extract from it the kind of information (associations) people are likely to give when looking for a word. 6 Conclusion and perspectives We have addressed in this paper the problem of word finding for speakers or writers. Concluding that most dictionaries are not well suited to allow for this kind of reverse access based on meanings (or meaning related elements, associations), we looked at work done by psychologists to get some inspiration. Next we tried to clarify which of these findings could help us build the dictionary of tomorrow, that is, a tool integrating linguistic and encyclopedic knowledge, allowing navigation by taking either or as starting point. While linguistic knowledge is more prominent for analysis (reading), encyclopedic facts are more relevant for production. We’ve presented then our ideas of how to build a resource, allowing lexical access based 15 on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of given language) and The role of the latbeing to lead to or to evoke the In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enour database with We plan to experiment with corpora, trying to find out which ones are best our and we will certainly experiment the window size 16to see which size is best for which text type. Finally, we plan to insert in AM the relations holding between the and As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done automatically. Obviously, the success of our resource will depend on the quality of the corpus, the quality of weights and links, and the representativity of all this for a given population. While we do believe in the justification of our intuitions, more work is needed to reveal the true potential of the approach. The ultimate judge being, of course, the future user. example, we could consider a resource like ConceptNet of the Open Mind Common-Sense project (Liuh and Singh, 2004). example, it would have been interesting to consider coocurrences beyond the scope of the paragraph, by considering the logical structure of the Wikipedia document. Anyhow, our experiment needs to be redone with more data than just 1000 pages, the size chosen here for lack of time. Indeed one could consider using the entire corpus of Wikipedia or mixed corpora References Jean. 2003. in the Mind: an Introto the Mental Lexicon (3d Blackwell, Oxford. Bilac, S., W. Watanabe, T. Hashimoto, T. Tokunaga, and H. Tanaka. 2004. Dictionary search based the target word description. In of the Tenth Annual Meeting of The Association for NLP pages 556–559, Tokyo, Japan. P. 1862. analogique de la : r´epertoire complet des mots par id´ees et des id´ees par les Larousse et A.</abstract>
<address confidence="0.69703">Boyer, Paris.</address>
<note confidence="0.85812640625">Brown, R. and D. McNeill. 1996. The tip of the tounge of Verbal Learning and Ver- 5:325–337. A, editor, 1982. of the Tongue and Lan- Mouton, Amsterdam. James. 1965. structure of associations in and Johns Hopkins Press. Zhendong and Qiang Dong. 2006. the computation of World Scientific, London. Dutoit, Dominique and P. Nugues. 2002. A lexical network and an algorithm to find words from defini- In van Harmelen, F., editor, Proc. of the 15th European Conference on Artificial Intelpages 450–454, Lyon. IOS Press, Amsterdam. Christiane, editor, 1998. An Electronic Lexical Database and some of its Applica- MIT Press. Ferret, Olivier and Michael Zock. 2006. Enhancing electronic dictionaries with an index based on associ- In ’06: Proceedings of the 21st International Conference on Computational Linguistics and 44th annual meeting of the pages 281–288. Fontenelle, Thierry. 1997. Using a bilingual dictionary create semantic networks. Journal 10(4):275–303. Goddard, Cliff. 1998. Bad arguments against semanprimitives. 24(2-3):129– 156. 16 Gross, Maurice. 1984. Lexicon-grammar and the analof french. In of the 11th pages 275–282, Stanford, CA. Richardson, S., W. Dolan, and L. Vanderwende. 1998. Mindnet: Acquiring and structuring semantic inforfrom text. In pages 1098– 1102. Trevor. 2004. psychology of Psychology Press, Taylor and Francis, Hove and New York. Jung, Carl and F. Riklin. 1906. Experimentelle Untersuchungen ¨uber Assoziationen Gesunder. In C. G., editor, Assoziationsstupages 7–145. Barth, Leipzig, Germany. Kilgarriff, Adam, Pavel Rychl´y, Pavel Smrˇz, and David 2004. The Sketch Engine. In the Eleventh EURALEX International pages 105–116, Lorient, France. Levelt, Willem. 1996. A theory of lexical access speech production. In of the 16th Conon Computational Copenhagen, Denmark. Lin, Chin-Yew and Eduard H. Hovy. 2000. The automated acquisition of topic signatures for text summa- In pages 495–501. Morgan Kaufmann. Lin, Dekang. 1998. Automatic retrieval and clustering similar words. In pages 768–774, Montreal. Liuh, H. and P. Singh. 2004. ConceptNet: a practicommonsense reasoning toolkit. Technology Mel’ˇcuk, I., N. Arbatchewsky-Jumarie, L. Iordanskaja, Mantha, and A. Polgu`ere. 1999.</note>
<abstract confidence="0.573124636363636">et combinatoire du contemporain lexico-s´emantiques Les Presses de l’Universit´e de Montr´eal, Montr´eal. Mel’ˇcuk, Igor. 1996. Lexical functions: A tool for the description of lexical relations in the lexicon. In L., editor, Functions in Lexicograand Natural Language pages 37– 102. Benjamins, Amsterdam/Philadelphia. Mihalcea, Rada and Dan Moldovan. 2001. Extended progress report. In 2001 - Workon WordNet and Other Lexical Pitts-</abstract>
<address confidence="0.899572">burgh, USA.</address>
<author confidence="0.38571">G A Miller</author>
<author confidence="0.38571">R Beckwith</author>
<author confidence="0.38571">C Fellbaum</author>
<author confidence="0.38571">D Gross</author>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Aitchinson</author>
</authors>
<title>Words in the Mind: an Introduction to the Mental Lexicon (3d edition).</title>
<date>2003</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4828" citStr="Aitchinson, 2003" startWordPosition="757" endWordPosition="758">g words is fairly unproblematic when reading, at least in the case of most western languages. Words can generally be found easily in a dictionary, provided the user knows the spelling, the alphabet and how to build lemma starting from an inflected form. Unlike words, which are organized alphabetically (in western languages) or by form (stroke counts in Chinese), concepts are organized topically: they are clustered into functional groups according to their role in real world, or our perception of it. Psychologist have studied the difficulties people have when trying to produce or access words (Aitchinson, 2003). In particular, they have studied the tip-of-the-tongue phenomenon (Brown and McNeill, 1996) and the effects an input can have on the quality of an output (error analysis (Cutler, 1982)) and on the ease of its production: positive or negative priming effect (activation/inhibition). Obviously, these findings allow certain conclusions, and they might guide us when developing tools to help people find the needed word. In particular, they reveal two facts highly relevant for our goal: 1. even if people fail to access a given word, they might know a lot about it: origin, meaning (word definition, </context>
</contexts>
<marker>Aitchinson, 2003</marker>
<rawString>Aitchinson, Jean. 2003. Words in the Mind: an Introduction to the Mental Lexicon (3d edition). Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bilac</author>
<author>W Watanabe</author>
<author>T Hashimoto</author>
<author>T Tokunaga</author>
<author>H Tanaka</author>
</authors>
<title>Dictionary search based on the target word description.</title>
<date>2004</date>
<booktitle>In Proc. of the Tenth Annual Meeting of The Association for NLP (NLP2004),</booktitle>
<pages>556--559</pages>
<location>Tokyo, Japan.</location>
<contexts>
<context position="17150" citStr="Bilac et al., 2004" startWordPosition="2861" endWordPosition="2864">tion, whether rhyming words should be computed is not crucial at this stage. idea or concept,9 and the system will display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Mille</context>
</contexts>
<marker>Bilac, Watanabe, Hashimoto, Tokunaga, Tanaka, 2004</marker>
<rawString>Bilac, S., W. Watanabe, T. Hashimoto, T. Tokunaga, and H. Tanaka. 2004. Dictionary search based on the target word description. In Proc. of the Tenth Annual Meeting of The Association for NLP (NLP2004), pages 556–559, Tokyo, Japan.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Boissi`ere</author>
</authors>
<title>Dictionnaire analogique de la langue franc¸aise : r´epertoire complet des mots par les id´ees et des id´ees par les mots. Larousse et A. Boyer,</title>
<location>Paris.</location>
<marker>Boissi`ere, </marker>
<rawString>Boissi`ere, P. 1862. Dictionnaire analogique de la langue franc¸aise : r´epertoire complet des mots par les id´ees et des id´ees par les mots. Larousse et A. Boyer, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brown</author>
<author>D McNeill</author>
</authors>
<title>The tip of the tounge phenomenon.</title>
<date>1996</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<pages>5--325</pages>
<contexts>
<context position="4921" citStr="Brown and McNeill, 1996" startWordPosition="768" endWordPosition="771">guages. Words can generally be found easily in a dictionary, provided the user knows the spelling, the alphabet and how to build lemma starting from an inflected form. Unlike words, which are organized alphabetically (in western languages) or by form (stroke counts in Chinese), concepts are organized topically: they are clustered into functional groups according to their role in real world, or our perception of it. Psychologist have studied the difficulties people have when trying to produce or access words (Aitchinson, 2003). In particular, they have studied the tip-of-the-tongue phenomenon (Brown and McNeill, 1996) and the effects an input can have on the quality of an output (error analysis (Cutler, 1982)) and on the ease of its production: positive or negative priming effect (activation/inhibition). Obviously, these findings allow certain conclusions, and they might guide us when developing tools to help people find the needed word. In particular, they reveal two facts highly relevant for our goal: 1. even if people fail to access a given word, they might know a lot about it: origin, meaning (word definition, role played in a given situation), part of speech, number of syllables, similar sounding word</context>
</contexts>
<marker>Brown, McNeill, 1996</marker>
<rawString>Brown, R. and D. McNeill. 1996. The tip of the tounge phenomenon. Journal of Verbal Learning and Verbal Behaviour, 5:325–337.</rawString>
</citation>
<citation valid="true">
<date>1982</date>
<booktitle>Slips of the Tongue and Language Production.</booktitle>
<editor>Cutler, A, editor,</editor>
<location>Mouton, Amsterdam.</location>
<marker>1982</marker>
<rawString>Cutler, A, editor, 1982. Slips of the Tongue and Language Production. Mouton, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Deese</author>
</authors>
<title>The structure of associations in language and thought.</title>
<date>1965</date>
<publisher>Johns Hopkins Press.</publisher>
<contexts>
<context position="14252" citStr="Deese, 1965" startWordPosition="2345" endWordPosition="2346">ally implemented is not clear to us, though we believe that it is not. Anyhow, our approach is different, and we hope the reader will understand in a moment the reasons why. We will also put on one axis all the form elements, i.e. the lemmata or expressions of a given language (we refer to them as target words, henceforth tw). On the other axis we will place the triggers or access-words (henceforth aw), that is, the words or concepts capable and likely to evoke the tw. These are typically the kind of words psy11 chologists have gathered in their association experiments (Jung and Riklin, 1906; Deese, 1965; Schvaneveldt, 1989). Note, that instead of putting a boolean value at the intersection of the tw and the aw, we will put weights and the type of link holding between the co-occurring terms. This gives us quadruplets. For example, an utterance like ”this is the key of the door” might yield the aw (key), the tw(door), the link type lt(part of), and a weight (let’s say 15). The fact that we have these two kinds of information is very important later on, as it allows the search engine to cluster by type the possible answers to be given in response to a user query (word(s) provided as input) and </context>
</contexts>
<marker>Deese, 1965</marker>
<rawString>Deese, James. 1965. The structure of associations in language and thought. Johns Hopkins Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>HOWNET and the computation of meaning. World Scientific,</title>
<date>2006</date>
<location>London.</location>
<contexts>
<context position="7753" citStr="Dong and Dong, 2006" startWordPosition="1243" endWordPosition="1246"> better, as this reduces the number of words among which to choose. 2 Related work and goal While more dictionaries have been built for the reader than for the writer, there have been some onomasiological attempts as early as in the middle of the 19th century. For example, Roget’s Thesaurus (Roget, 1852), T’ong’s Chinese and English instructor (T’ong, 1862), or Boissiere’s analogical dictionary (Boissi`ere, 1862).5 Newer work includes Mel’ˇcuk’s ECD (Mel’ˇcuk et al., 1999), Miller and Fellbaum’s WordNet (Fellbaum, 1998), Richardson and Dolan’s MindNet (Richardson et al., 1998), Dong’s HowNet (Dong and Dong, 2006) and Longman’s Language Activator (Summers, 1993). There is also the work of into words. Put differently, we do not store words at all in our mind, at least not in the layman’s or lexicographer’s sense who consider word-forms and their meanings as one. If we are right, than rather continue to consider the human mind as a word store we could consider it as a word factory. Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (for a good survey see (Harley, 2004), pages 359-374) one gets the impression that words are synthesized rather than located an</context>
</contexts>
<marker>Dong, Dong, 2006</marker>
<rawString>Dong, Zhendong and Qiang Dong. 2006. HOWNET and the computation of meaning. World Scientific, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominique Dutoit</author>
<author>P Nugues</author>
</authors>
<title>A lexical network and an algorithm to find words from definitions. In</title>
<date>2002</date>
<booktitle>ECAI2002, Proc. of the 15th European Conference on Artificial Intelligence,</booktitle>
<pages>450--454</pages>
<editor>van Harmelen, F., editor,</editor>
<publisher>IOS Press,</publisher>
<location>Lyon.</location>
<contexts>
<context position="17129" citStr="Dutoit and Nugues, 2002" startWordPosition="2857" endWordPosition="2860">e word cabbage. 8The question, whether rhyming words should be computed is not crucial at this stage. idea or concept,9 and the system will display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-</context>
</contexts>
<marker>Dutoit, Nugues, 2002</marker>
<rawString>Dutoit, Dominique and P. Nugues. 2002. A lexical network and an algorithm to find words from definitions. In van Harmelen, F., editor, ECAI2002, Proc. of the 15th European Conference on Artificial Intelligence, pages 450–454, Lyon. IOS Press, Amsterdam.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and some of its Applications.</title>
<date>1998</date>
<editor>Fellbaum, Christiane, editor,</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Fellbaum, Christiane, editor, 1998. WordNet: An Electronic Lexical Database and some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Ferret</author>
<author>Michael Zock</author>
</authors>
<title>Enhancing electronic dictionaries with an index based on associations.</title>
<date>2006</date>
<booktitle>In ACL ’06: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>281--288</pages>
<contexts>
<context position="17265" citStr="Ferret and Zock, 2006" startWordPosition="2881" endWordPosition="2884">ll display all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not rele</context>
</contexts>
<marker>Ferret, Zock, 2006</marker>
<rawString>Ferret, Olivier and Michael Zock. 2006. Enhancing electronic dictionaries with an index based on associations. In ACL ’06: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 281–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Fontenelle</author>
</authors>
<title>Using a bilingual dictionary to create semantic networks.</title>
<date>1997</date>
<journal>International Journal of Lexicography,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="9291" citStr="Fontenelle, 1997" startWordPosition="1501" endWordPosition="1502">d from abstract meanings to concrete sounds, which at some point were also just abstract features. By propagating energy rather than data (as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever), that we propagate signals, activating ultimately certain peripheral organs (larynx, tongue, mouth, lips, hands) in such a way as to produce movements or sounds, that, not knowing better, we call words. 5For a more recent proposal see (Robert et al., 1993). 10 (Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008), various collocation dictionaries (BBI, OECD) and Bernstein’s Reverse Dictionary.6 Finally, there is M. Rundell’s MEDAL, a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). As one can see, a lot of progress has been accomplished over the last few years, yet more can be done, especially with regard to unifying linguistic and encyclopedic knowledge. Let’s take an example to illustrate our point. Suppose, you were looking for a word expressing the following ideas: ’superior dark coffee made from beans from Arabia’, and that yo</context>
</contexts>
<marker>Fontenelle, 1997</marker>
<rawString>Fontenelle, Thierry. 1997. Using a bilingual dictionary to create semantic networks. International Journal of Lexicography, 10(4):275–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cliff Goddard</author>
</authors>
<title>Bad arguments against semantic primitives. Theoretical Linguistics,</title>
<date>1998</date>
<pages>24--2</pages>
<contexts>
<context position="3221" citStr="Goddard, 1998" startWordPosition="498" endWordPosition="499">he lexicographers have hardly taken into account the language producer’s perspective,1 considering conceptual input, incomplete as it may be, as starting point. While readers start with words, looking generally for their corresponding meanings, speakers or writers usually start with the opposite, meanings or concepts,2 which should be the entry points of a dictionary, which ideally is neutral in terms of access direction.3 The problem is that we still don’t know very well what concepts are, whether they are compositional and if so, how many primitives there are (Wilks, 1977; Wierzbicka, 1996; Goddard, 1998). 1Roget’s thesaurus (Roget, 1852), Miller and Fellbaum’s WordNet (Fellbaum, 1998) and Longman’s Language Activator (Summers, 1993), being notable exceptions (For more details, see next section). 2Of course, this does not preclude, that we may have to use words to refer to them in a concept-based query. 3While we agree with Polgu`ere theoretically when he pleads for dictionary neutrality with regard to lexical access (Polgu`ere, 2006), from a practical point of view the situation is obviously quite different for the speaker and listener, even if both of them draw on the same resource. 9 Coling</context>
</contexts>
<marker>Goddard, 1998</marker>
<rawString>Goddard, Cliff. 1998. Bad arguments against semantic primitives. Theoretical Linguistics, 24(2-3):129– 156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Lexicon-grammar and the analysis of french.</title>
<date>1984</date>
<booktitle>In Proc. of the 11th COLING,</booktitle>
<pages>275--282</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="17645" citStr="Gross, 1984" startWordPosition="2946" endWordPosition="2947">s: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not relevant for us here, Miller’s proposal is. What are the differences between his proposal and ours? There are basically four main differences: 1. we use, collocations or access-words, i.e aws rather than synsets; Hence, any of the following aws (cat, grey, computer device, cheese, Speedy Gonzales) could point toward the tw ’mouse’, none of them are part of the meaning, leave alone </context>
</contexts>
<marker>Gross, 1984</marker>
<rawString>Gross, Maurice. 1984. Lexicon-grammar and the analysis of french. In Proc. of the 11th COLING, pages 275–282, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Richardson</author>
<author>W Dolan</author>
<author>L Vanderwende</author>
</authors>
<title>Mindnet: Acquiring and structuring semantic information from text.</title>
<date>1998</date>
<booktitle>In ACL-COLING’98,</booktitle>
<pages>1098--1102</pages>
<contexts>
<context position="7716" citStr="Richardson et al., 1998" startWordPosition="1236" endWordPosition="1240">viously, the more information we have the better, as this reduces the number of words among which to choose. 2 Related work and goal While more dictionaries have been built for the reader than for the writer, there have been some onomasiological attempts as early as in the middle of the 19th century. For example, Roget’s Thesaurus (Roget, 1852), T’ong’s Chinese and English instructor (T’ong, 1862), or Boissiere’s analogical dictionary (Boissi`ere, 1862).5 Newer work includes Mel’ˇcuk’s ECD (Mel’ˇcuk et al., 1999), Miller and Fellbaum’s WordNet (Fellbaum, 1998), Richardson and Dolan’s MindNet (Richardson et al., 1998), Dong’s HowNet (Dong and Dong, 2006) and Longman’s Language Activator (Summers, 1993). There is also the work of into words. Put differently, we do not store words at all in our mind, at least not in the layman’s or lexicographer’s sense who consider word-forms and their meanings as one. If we are right, than rather continue to consider the human mind as a word store we could consider it as a word factory. Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (for a good survey see (Harley, 2004), pages 359-374) one gets the impression that words a</context>
</contexts>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Richardson, S., W. Dolan, and L. Vanderwende. 1998. Mindnet: Acquiring and structuring semantic information from text. In ACL-COLING’98, pages 1098– 1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Harley</author>
</authors>
<title>The psychology of language.</title>
<date>2004</date>
<publisher>Psychology Press, Taylor and Francis,</publisher>
<location>Hove and New York.</location>
<contexts>
<context position="8263" citStr="Harley, 2004" startWordPosition="1338" endWordPosition="1339">m, 1998), Richardson and Dolan’s MindNet (Richardson et al., 1998), Dong’s HowNet (Dong and Dong, 2006) and Longman’s Language Activator (Summers, 1993). There is also the work of into words. Put differently, we do not store words at all in our mind, at least not in the layman’s or lexicographer’s sense who consider word-forms and their meanings as one. If we are right, than rather continue to consider the human mind as a word store we could consider it as a word factory. Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (for a good survey see (Harley, 2004), pages 359-374) one gets the impression that words are synthesized rather than located and read out. Taking a look at all this work, generally connectionist models, one may conclude that, rather than having words in our mind we have a set of more or less abstract features (concepts, syntactic information, phonemes), distributed across various layers, which need to be synthesized over time. To do so we proceed from abstract meanings to concrete sounds, which at some point were also just abstract features. By propagating energy rather than data (as there is no message passing, transformation or</context>
</contexts>
<marker>Harley, 2004</marker>
<rawString>Harley, Trevor. 2004. The psychology of language. Psychology Press, Taylor and Francis, Hove and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Jung</author>
<author>F Riklin</author>
</authors>
<title>Experimentelle Untersuchungen ¨uber Assoziationen Gesunder.</title>
<date>1906</date>
<booktitle>Diagnostische Assoziationsstudien,</booktitle>
<pages>7--145</pages>
<editor>In Jung, C. G., editor,</editor>
<location>Barth, Leipzig, Germany.</location>
<contexts>
<context position="14239" citStr="Jung and Riklin, 1906" startWordPosition="2341" endWordPosition="2344">s is the way WN is actually implemented is not clear to us, though we believe that it is not. Anyhow, our approach is different, and we hope the reader will understand in a moment the reasons why. We will also put on one axis all the form elements, i.e. the lemmata or expressions of a given language (we refer to them as target words, henceforth tw). On the other axis we will place the triggers or access-words (henceforth aw), that is, the words or concepts capable and likely to evoke the tw. These are typically the kind of words psy11 chologists have gathered in their association experiments (Jung and Riklin, 1906; Deese, 1965; Schvaneveldt, 1989). Note, that instead of putting a boolean value at the intersection of the tw and the aw, we will put weights and the type of link holding between the co-occurring terms. This gives us quadruplets. For example, an utterance like ”this is the key of the door” might yield the aw (key), the tw(door), the link type lt(part of), and a weight (let’s say 15). The fact that we have these two kinds of information is very important later on, as it allows the search engine to cluster by type the possible answers to be given in response to a user query (word(s) provided a</context>
</contexts>
<marker>Jung, Riklin, 1906</marker>
<rawString>Jung, Carl and F. Riklin. 1906. Experimentelle Untersuchungen ¨uber Assoziationen Gesunder. In Jung, C. G., editor, Diagnostische Assoziationsstudien, pages 7–145. Barth, Leipzig, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychl´y</author>
<author>Pavel Smrˇz</author>
<author>David Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eleventh EURALEX International Congress,</booktitle>
<pages>105--116</pages>
<location>Lorient, France.</location>
<marker>Kilgarriff, Rychl´y, Smrˇz, Tugwell, 2004</marker>
<rawString>Kilgarriff, Adam, Pavel Rychl´y, Pavel Smrˇz, and David Tugwell. 2004. The Sketch Engine. In Proceedings of the Eleventh EURALEX International Congress, pages 105–116, Lorient, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Levelt</author>
</authors>
<title>A theory of lexical access in speech production.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th Conference on Computational Linguistics,</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="6400" citStr="Levelt, 1996" startWordPosition="1032" endWordPosition="1033"> as the target word is rare and primed by a similar sounding word. 2. unlike words in printed or electronic dictionaries, words in our mind may be inexistent as tokens. What we seem to have in our minds are decomposed, abstract entities which need to be synthesized over time.4 Ac4This may be very surprising, yet, this need not be the case if we consider the fact that speech errors are nearly always due to competing elements from the same level or an adjacent one, unless they are the result of a surrounding concept which has been activated, or which is about to be translated cording to Levelt (Levelt, 1996) the generation of words (synthesis) involves the following stages: conceptual preparation, lexical selection, phonological- and phonetic encoding, articulation. Bear in mind that having performed ’lexical selection’ does not imply access to the phonetic form (see the experiments on the tip-of-the-tongue phenomenon). What can be concluded from these observations? It seems that underspecified input is sufficiently frequent to be considered as normal. Hence we should accept it, and make the best out of it by using whatever information is available (accessible), no matter how incomplete, since it</context>
</contexts>
<marker>Levelt, 1996</marker>
<rawString>Levelt, Willem. 1996. A theory of lexical access in speech production. In Proc. of the 16th Conference on Computational Linguistics, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard H Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In COLING,</booktitle>
<pages>495--501</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="32404" citStr="Lin and Hovy, 2000" startWordPosition="5454" endWordPosition="5457"> 15 on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of a given language) and aws. The role of the latter being to lead to or to evoke the tw. In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enrich our database with aws. We plan to experiment with corpora, trying to find out which ones are best for our purpose15 and we will certainly experiment with the window size 16 to see which size is best for which text type. Finally, we plan to insert in our AM the relations holding between the aw and the tw. As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done automatically. Obviously, the success of our</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Lin, Chin-Yew and Eduard H. Hovy. 2000. The automated acquisition of topic signatures for text summarization. In COLING, pages 495–501. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<location>Montreal.</location>
<contexts>
<context position="32365" citStr="Lin, 1998" startWordPosition="5450" endWordPosition="5451"> allowing lexical access based 15 on underspecified, i.e. imperfect input. To achieve this goal we’ve started building an AM composed of form elements (the words and expressions of a given language) and aws. The role of the latter being to lead to or to evoke the tw. In the last part we’ve described briefly the results obtained by comparing two resources (WN and Wikipedia) and various inputs. Given the fact that the project is still quite young, only preliminary results can be shown at this point. Our next steps will be to take a closer look at the following work: clustering of similar words (Lin, 1998), topic signatures (Lin and Hovy, 2000) and Kilgariff’s sketch engine (Kilgarriff et al., 2004). We plan also to add other lexical functions to enrich our database with aws. We plan to experiment with corpora, trying to find out which ones are best for our purpose15 and we will certainly experiment with the window size 16 to see which size is best for which text type. Finally, we plan to insert in our AM the relations holding between the aw and the tw. As these links are contained in our corpus, we should be able to identify and type them. The question is, to what extent this can be done autom</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, Dekang. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768–774, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liuh</author>
<author>P Singh</author>
</authors>
<title>ConceptNet: a practical commonsense reasoning toolkit.</title>
<date>2004</date>
<journal>BT Technology Journal.</journal>
<marker>Liuh, Singh, 2004</marker>
<rawString>Liuh, H. and P. Singh. 2004. ConceptNet: a practical commonsense reasoning toolkit. BT Technology Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’ˇcuk</author>
<author>N Arbatchewsky-Jumarie</author>
<author>L Iordanskaja</author>
<author>S Mantha</author>
<author>A Polgu`ere</author>
</authors>
<title>Dictionnaire explicatif et combinatoire du franc¸ais contemporain Recherches lexico-s´emantiques IV. Les Presses de l’Universit´e de</title>
<date>1999</date>
<location>Montr´eal, Montr´eal.</location>
<marker>Mel’ˇcuk, Arbatchewsky-Jumarie, Iordanskaja, Mantha, Polgu`ere, 1999</marker>
<rawString>Mel’ˇcuk, I., N. Arbatchewsky-Jumarie, L. Iordanskaja, S. Mantha, and A. Polgu`ere. 1999. Dictionnaire explicatif et combinatoire du franc¸ais contemporain Recherches lexico-s´emantiques IV. Les Presses de l’Universit´e de Montr´eal, Montr´eal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Lexical functions: A tool for the description of lexical relations in the lexicon.</title>
<date>1996</date>
<booktitle>Lexical Functions in Lexicography and Natural Language Processing,</booktitle>
<pages>37--102</pages>
<editor>In Wanner, L., editor,</editor>
<publisher>Benjamins, Amsterdam/Philadelphia.</publisher>
<marker>Mel’ˇcuk, 1996</marker>
<rawString>Mel’ˇcuk, Igor. 1996. Lexical functions: A tool for the description of lexical relations in the lexicon. In Wanner, L., editor, Lexical Functions in Lexicography and Natural Language Processing, pages 37– 102. Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan Moldovan</author>
</authors>
<title>Extended WordNet: progress report.</title>
<date>2001</date>
<booktitle>In NAACL 2001 - Workshop on WordNet and Other Lexical Resources,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="24753" citStr="Mihalcea and Moldovan, 2001" startWordPosition="4154" endWordPosition="4157">rouped as clusters. These sets of synonyms, called synsets, are then linked in various ways, depending on the kind of relationship they entertain with the adjacent synset. For example, their neighbors can be more general or specific (hyperonymy vs. hyponymy), they can be part of some reference object (meronymy: car-motor), they can be the opposite (antonymy: hot-cold), etc. While WN is a resource it can also be seen as a corpus. 5.1.2 Using WN as a corpus There are many good reasons to use WN for learning f,,,. For one, there are many extensions, and second, the one we are using, eXtended WN (Mihalcea and Moldovan, 2001) spares us the trouble of having to address issues like: (a) segmentation: we do not need to identify sentence boundaries ; (b) semantic ambiguity: words being tagged, we get good precision; (c) lemmatization: since only verbs, nouns, adjectives and adverbs are tagged, we need neither a stoplist nor a lemmatizer. Despite all these qualities, two important problems remain nevertheless for this kind of corpus: (a) size: though, all words are tagged, the corpus remains small as it contains only 63.941 different words; (b) in consequence, the corpus lacks many syntagmatic associations encoding enc</context>
</contexts>
<marker>Mihalcea, Moldovan, 2001</marker>
<rawString>Mihalcea, Rada and Dan Moldovan. 2001. Extended WordNet: progress report. In NAACL 2001 - Workshop on WordNet and Other Lexical Resources, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--244</pages>
<contexts>
<context position="13134" citStr="Miller et al., 1990" startWordPosition="2144" endWordPosition="2147">a. Of course, cyber-coffee fans might be even quicker in reaching their goal. 3 The lexical matrix revisited The main question that we are interested in here is how, or in what terms, to index the dictionary in order to allow for quick and intuitive access to words. Access should be possible on the basis of meaning (or meaning elements), various kinds of associations (most prominently ’syntagmatic’ ones) and, more generally speaking, underspecified input. To this end we have started to build an association matrix (henceforth AM), akin to, yet different from G. Miller’s initial proposal of WN (Miller et al., 1990). He suggested to build a lexical matrix by putting on one axis all the forms, i.e. words of the language, and on the other, their corresponding meanings. The latter being defined in terms of synsets. The corresponding meaningform relations are signaled via a boolean (presence/absence). Hence, looking at the intersection of meanings and forms, one can see which meanings are expressed by, or converge toward what forms, or conversely, what form expresses which meanings. Whether this is the way WN is actually implemented is not clear to us, though we believe that it is not. Anyhow, our approach i</context>
<context position="17791" citStr="Miller et al., 1990" startWordPosition="2970" endWordPosition="2973">ne a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not relevant for us here, Miller’s proposal is. What are the differences between his proposal and ours? There are basically four main differences: 1. we use, collocations or access-words, i.e aws rather than synsets; Hence, any of the following aws (cat, grey, computer device, cheese, Speedy Gonzales) could point toward the tw ’mouse’, none of them are part of the meaning, leave alone synonyms. 2. we mark explicitly the weight and the type of link between the tw and the aw (isa, part of, etc.),10 whereas WN uses only a binary va</context>
<context position="23114" citStr="Miller et al., 1990" startWordPosition="3871" endWordPosition="3874"> or a paragraph).11 The result produced by the system and returned to the user is the intersection of the application of fneig to the aws. In the next section we explain how this function is applied to two corpora (Wordnet and Wikipedia), to show their respective qualities and shortcomings for this specific task. 5.1 WordNet 5.1.1 Description WordNet (henceforth WN) is a lexical database for English developed under the guidance of G. 11The scope or window size will vary with the text type (normal text vs. encyclopedia). The optimal size is at this point still an empirical question. 13 Miller (Miller et al., 1990). One of his goals was to support lexical access akin to the human mind, association-based. Knowledge is stored in a network composed of nodes and links (nodes being words or concepts and the links are the means of connecting them) and access to knowledge, i.e. search, takes place by entering the network at some point and follow the links until one has reached the goal (unless one has given up before). This kind of navigation in a huge conceptual/lexical network can be considered equivalent to spreading activation taking place in our brain. Of course, such a network has to be built, and naviga</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, and K. Miller. 1990. Introduction to WordNet: An online lexical database. International Journal of Lexicography, 3(4), pages 235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fons Moerdijk</author>
</authors>
<title>Frames and semagrams; Meaning description in the general dutch dictionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the Thirteenth Euralex International Congress, EURALEX,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="9322" citStr="Moerdijk, 2008" startWordPosition="1505" endWordPosition="1506">rete sounds, which at some point were also just abstract features. By propagating energy rather than data (as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever), that we propagate signals, activating ultimately certain peripheral organs (larynx, tongue, mouth, lips, hands) in such a way as to produce movements or sounds, that, not knowing better, we call words. 5For a more recent proposal see (Robert et al., 1993). 10 (Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008), various collocation dictionaries (BBI, OECD) and Bernstein’s Reverse Dictionary.6 Finally, there is M. Rundell’s MEDAL, a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). As one can see, a lot of progress has been accomplished over the last few years, yet more can be done, especially with regard to unifying linguistic and encyclopedic knowledge. Let’s take an example to illustrate our point. Suppose, you were looking for a word expressing the following ideas: ’superior dark coffee made from beans from Arabia’, and that you knew that the target word was</context>
</contexts>
<marker>Moerdijk, 2008</marker>
<rawString>Moerdijk, Fons. 2008. Frames and semagrams; Meaning description in the general dutch dictionary. In Proceedings of the Thirteenth Euralex International Congress, EURALEX, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alain Polgu`ere</author>
</authors>
<title>Structural properties of lexical systems: Monolingual and multilingual perspectives. Sidney. Coling workshop ’Multilingual Language Resources and Interoperability’.</title>
<date>2006</date>
<marker>Polgu`ere, 2006</marker>
<rawString>Polgu`ere, Alain. 2006. Structural properties of lexical systems: Monolingual and multilingual perspectives. Sidney. Coling workshop ’Multilingual Language Resources and Interoperability’.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Robert</author>
<author>Alain Rey</author>
<author>J Rey-Debove</author>
</authors>
<title>Dictionnaire alphabetique et analogique de la Langue Franc¸aise. Le Robert,</title>
<date>1993</date>
<location>Paris.</location>
<contexts>
<context position="9269" citStr="Robert et al., 1993" startWordPosition="1496" endWordPosition="1499">r time. To do so we proceed from abstract meanings to concrete sounds, which at some point were also just abstract features. By propagating energy rather than data (as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever), that we propagate signals, activating ultimately certain peripheral organs (larynx, tongue, mouth, lips, hands) in such a way as to produce movements or sounds, that, not knowing better, we call words. 5For a more recent proposal see (Robert et al., 1993). 10 (Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008), various collocation dictionaries (BBI, OECD) and Bernstein’s Reverse Dictionary.6 Finally, there is M. Rundell’s MEDAL, a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). As one can see, a lot of progress has been accomplished over the last few years, yet more can be done, especially with regard to unifying linguistic and encyclopedic knowledge. Let’s take an example to illustrate our point. Suppose, you were looking for a word expressing the following ideas: ’superior dark coffee made from beans fro</context>
</contexts>
<marker>Robert, Rey, Rey-Debove, 1993</marker>
<rawString>Robert, Paul, Alain Rey, and J. Rey-Debove. 1993. Dictionnaire alphabetique et analogique de la Langue Franc¸aise. Le Robert, Paris.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Roget</author>
</authors>
<title>Thesaurus of English Words and Phrases.</title>
<location>Longman, London.</location>
<marker>Roget, </marker>
<rawString>Roget, P. 1852. Thesaurus of English Words and Phrases. Longman, London.</rawString>
</citation>
<citation valid="true">
<title>Pathfinder Associative Networks: studies in knowledge organization.</title>
<date>1989</date>
<editor>Schvaneveldt, R., editor,</editor>
<publisher>Ablex,</publisher>
<location>Norwood, New Jersey, US.</location>
<marker>1989</marker>
<rawString>Schvaneveldt, R., editor, 1989. Pathfinder Associative Networks: studies in knowledge organization. Ablex, Norwood, New Jersey, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Schwab</author>
<author>Mathieu Lafourcade</author>
</authors>
<title>Modelling, detection and exploitation of lexical functions for analysis.</title>
<date>2007</date>
<journal>ECTI Transactions Journal on Computer and Information Technology,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="20781" citStr="Schwab and Lafourcade, 2007" startWordPosition="3487" endWordPosition="3490">o terms are systematically related. For example, the lexical function Gener refers to the fact that some term (let’s say ,cat-) can be replaced by a more general term (let’s say ,animal-). Lexical functions encode the combinability of words. While ’big’ and ’strong’ express the same idea (intensity, magnitude), they cannot be combined freely with any noun: strong can be associated with fever, whereas big cannot. Of course, this kind of combinability between lexical terms is language specific, because unlike in English, in French one can say grosse fi`evre or forte fi`evre, both being correct (Schwab and Lafourcade, 2007). Our AM handles, of course these kind of functions. Here is a list of some of them: - paradigmatic associations: hypernymy (,cat- - ,animal-), hyponymy, synonymy, or antonymy,... ; - syntagmatic associations: collocations (,fearbeing associated with ,strong- or ,little-); - morphological relations ie. terms being derived from another part of speech: applying the change-part-of-speech lexical function fcpos to ,garden- will yield: fcpos(,garden-) = {,to garden-, ,gardener-, ... } - sound-related items: homophones, rhymes. 4.2 Assumptions concerning search The purpose of using lexical functions</context>
</contexts>
<marker>Schwab, Lafourcade, 2007</marker>
<rawString>Schwab, Didier and Mathieu Lafourcade. 2007. Modelling, detection and exploitation of lexical functions for analysis. ECTI Transactions Journal on Computer and Information Technology, 2(2):97–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerardo Sierra</author>
</authors>
<title>The onomasiological dictionary: a gap in lexicography.</title>
<date>2000</date>
<booktitle>In Proceedings of the Ninth Euralex International Congress,</booktitle>
<pages>223--235</pages>
<institution>IMS, Universit¨at Stuttgart.</institution>
<contexts>
<context position="9305" citStr="Sierra, 2000" startWordPosition="1503" endWordPosition="1504">anings to concrete sounds, which at some point were also just abstract features. By propagating energy rather than data (as there is no message passing, transformation or cumulation of information, there is only activation spreading, that is, changes of energy levels, call it weights, electronic impulses, or whatever), that we propagate signals, activating ultimately certain peripheral organs (larynx, tongue, mouth, lips, hands) in such a way as to produce movements or sounds, that, not knowing better, we call words. 5For a more recent proposal see (Robert et al., 1993). 10 (Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008), various collocation dictionaries (BBI, OECD) and Bernstein’s Reverse Dictionary.6 Finally, there is M. Rundell’s MEDAL, a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). As one can see, a lot of progress has been accomplished over the last few years, yet more can be done, especially with regard to unifying linguistic and encyclopedic knowledge. Let’s take an example to illustrate our point. Suppose, you were looking for a word expressing the following ideas: ’superior dark coffee made from beans from Arabia’, and that you knew that th</context>
</contexts>
<marker>Sierra, 2000</marker>
<rawString>Sierra, Gerardo. 2000. The onomasiological dictionary: a gap in lexicography. In Proceedings of the Ninth Euralex International Congress, pages 223– 235, IMS, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Sinopalnikova</author>
<author>Pavel Smrz</author>
</authors>
<title>Knowing a word vs. accessing a word: Wordnet and word association norms as interfaces to electronic dictionaries.</title>
<date>2006</date>
<booktitle>In Proceedings of the Third International WordNet Conference,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="30157" citStr="Sinopalnikova and Smrz, 2006" startWordPosition="5069" endWordPosition="5073">it, while those with a lower score will follow it. 5.3.5 Examples and Comparison of the results of the two corpora Here below are the examples extracted from the WN corpus (see figure-1). Our goal was to find the word ,vintage-. Trigger words are ,wine- and ,harvest-, yielding respectively 488 and 30 hits, i.e. words. As one can see ,harvest- is a better access term than ,wine-. Combining the two will reduce the list to 6 items. Please note that the tw ,vintage- is not among them, eventhough it exists in WordNet, which illustrates nicely the fact that storage does not guarantee accessibility (Sinopalnikova and Smrz, 2006). Looking at figure-1 you will see that the results have improved considerably with Wikipedia. The same input, ,wine- evokes many more words (1845 as opposed to 488). For ,harvest- we get 983 hits in6 words 45 words make grape grape vintage wine fish someone bottle produce +harvest commemorate person fermentation juice . . . . . . Beaujolais taste viticulture France Bordeaux vineyard . . . . . . Figure 1: Comparing two corpora (eXtended WordNet and Wikipedia) with various inputs stead of 30 (the intersection containing 62 words). Combining the two reduces the set to 45 items among which we wil</context>
</contexts>
<marker>Sinopalnikova, Smrz, 2006</marker>
<rawString>Sinopalnikova, Anna and Pavel Smrz. 2006. Knowing a word vs. accessing a word: Wordnet and word association norms as interfaces to electronic dictionaries. In Proceedings of the Third International WordNet Conference, pages 265–272, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Summers</author>
</authors>
<title>Language Activator: the world’s first production dictionary.</title>
<date>1993</date>
<location>Longman, London.</location>
<contexts>
<context position="3352" citStr="Summers, 1993" startWordPosition="515" endWordPosition="516">it may be, as starting point. While readers start with words, looking generally for their corresponding meanings, speakers or writers usually start with the opposite, meanings or concepts,2 which should be the entry points of a dictionary, which ideally is neutral in terms of access direction.3 The problem is that we still don’t know very well what concepts are, whether they are compositional and if so, how many primitives there are (Wilks, 1977; Wierzbicka, 1996; Goddard, 1998). 1Roget’s thesaurus (Roget, 1852), Miller and Fellbaum’s WordNet (Fellbaum, 1998) and Longman’s Language Activator (Summers, 1993), being notable exceptions (For more details, see next section). 2Of course, this does not preclude, that we may have to use words to refer to them in a concept-based query. 3While we agree with Polgu`ere theoretically when he pleads for dictionary neutrality with regard to lexical access (Polgu`ere, 2006), from a practical point of view the situation is obviously quite different for the speaker and listener, even if both of them draw on the same resource. 9 Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9–17 Manchester, August 2008 Neither d</context>
<context position="7802" citStr="Summers, 1993" startWordPosition="1252" endWordPosition="1253">h to choose. 2 Related work and goal While more dictionaries have been built for the reader than for the writer, there have been some onomasiological attempts as early as in the middle of the 19th century. For example, Roget’s Thesaurus (Roget, 1852), T’ong’s Chinese and English instructor (T’ong, 1862), or Boissiere’s analogical dictionary (Boissi`ere, 1862).5 Newer work includes Mel’ˇcuk’s ECD (Mel’ˇcuk et al., 1999), Miller and Fellbaum’s WordNet (Fellbaum, 1998), Richardson and Dolan’s MindNet (Richardson et al., 1998), Dong’s HowNet (Dong and Dong, 2006) and Longman’s Language Activator (Summers, 1993). There is also the work of into words. Put differently, we do not store words at all in our mind, at least not in the layman’s or lexicographer’s sense who consider word-forms and their meanings as one. If we are right, than rather continue to consider the human mind as a word store we could consider it as a word factory. Indeed, by looking at some of the work done by psychologists who try to emulate the mental lexicon (for a good survey see (Harley, 2004), pages 359-374) one gets the impression that words are synthesized rather than located and read out. Taking a look at all this work, gener</context>
</contexts>
<marker>Summers, 1993</marker>
<rawString>Summers, Della. 1993. Language Activator: the world’s first production dictionary. Longman, London.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ting-K¨u T’ong</author>
</authors>
<title>Ying u¨ tsap ts’¨un (The Chinese and English Instructor).</title>
<location>Canton.</location>
<marker>T’ong, </marker>
<rawString>T’ong, Ting-K¨u. 1862. Ying u¨ tsap ts’¨un (The Chinese and English Instructor). Canton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Wierzbicka</author>
</authors>
<title>Semantics: Primes and Universals.</title>
<date>1996</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="3205" citStr="Wierzbicka, 1996" startWordPosition="496" endWordPosition="497">s point of view. The lexicographers have hardly taken into account the language producer’s perspective,1 considering conceptual input, incomplete as it may be, as starting point. While readers start with words, looking generally for their corresponding meanings, speakers or writers usually start with the opposite, meanings or concepts,2 which should be the entry points of a dictionary, which ideally is neutral in terms of access direction.3 The problem is that we still don’t know very well what concepts are, whether they are compositional and if so, how many primitives there are (Wilks, 1977; Wierzbicka, 1996; Goddard, 1998). 1Roget’s thesaurus (Roget, 1852), Miller and Fellbaum’s WordNet (Fellbaum, 1998) and Longman’s Language Activator (Summers, 1993), being notable exceptions (For more details, see next section). 2Of course, this does not preclude, that we may have to use words to refer to them in a concept-based query. 3While we agree with Polgu`ere theoretically when he pleads for dictionary neutrality with regard to lexical access (Polgu`ere, 2006), from a practical point of view the situation is obviously quite different for the speaker and listener, even if both of them draw on the same re</context>
</contexts>
<marker>Wierzbicka, 1996</marker>
<rawString>Wierzbicka, Anna. 1996. Semantics: Primes and Universals. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
</authors>
<title>Good and bad arguments about semantic primitives.</title>
<date>1977</date>
<journal>Communication and Cognition,</journal>
<pages>10--3</pages>
<contexts>
<context position="3187" citStr="Wilks, 1977" startWordPosition="494" endWordPosition="495">m the reader’s point of view. The lexicographers have hardly taken into account the language producer’s perspective,1 considering conceptual input, incomplete as it may be, as starting point. While readers start with words, looking generally for their corresponding meanings, speakers or writers usually start with the opposite, meanings or concepts,2 which should be the entry points of a dictionary, which ideally is neutral in terms of access direction.3 The problem is that we still don’t know very well what concepts are, whether they are compositional and if so, how many primitives there are (Wilks, 1977; Wierzbicka, 1996; Goddard, 1998). 1Roget’s thesaurus (Roget, 1852), Miller and Fellbaum’s WordNet (Fellbaum, 1998) and Longman’s Language Activator (Summers, 1993), being notable exceptions (For more details, see next section). 2Of course, this does not preclude, that we may have to use words to refer to them in a concept-based query. 3While we agree with Polgu`ere theoretically when he pleads for dictionary neutrality with regard to lexical access (Polgu`ere, 2006), from a practical point of view the situation is obviously quite different for the speaker and listener, even if both of them d</context>
</contexts>
<marker>Wilks, 1977</marker>
<rawString>Wilks, Yorick. 1977. Good and bad arguments about semantic primitives. Communication and Cognition, 10(3–4):181–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
<author>Slaven Bilac</author>
</authors>
<title>Word lookup on the basis of associations : from an idea to a roadmap.</title>
<date>2004</date>
<booktitle>In Workshop on ’Enhancing and using electronic dictionaries’,</booktitle>
<pages>29--35</pages>
<location>Geneva. COLING.</location>
<contexts>
<context position="27628" citStr="Zock and Bilac, 2004" startWordPosition="4609" endWordPosition="4612">nder consideration. This allows the filtering of all irrelevant words, to keep but a bag of words, that is, the nouns, adjectives, verbs and adverbs occuring in the paragraph. These words will be used to fill the triplets of our database. 12http://www.wikipedia.org 13The optimal window-size depends probably on the text type (encyclopedia vs. unformatted text). Yet, in the absence of clear criteria, we consider the optimal window-size as an open, empirical question. 14This latter aspect is not implemented yet, but will be added in the future, as it is a necessary component for easy navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). 14 Input WordNet Wikipedia 488 words 1845 words alcoholic country god characteristics regulation grape appellation system bottled like christian track . . . . . . wine grape sweet serve france small fruit dry bottle produce red bread hold . . . . . . 30 words 983 words month fish produce grain grape revolutionary autumn farms calendar festival energy cut harvest butterfish dollar combine ground person make balance rain wine first amount rich 5.3.3 Corpus Building We start arbitrarily from some page (for our experiment, we have chosen ”wine” as input), apply the algor</context>
</contexts>
<marker>Zock, Bilac, 2004</marker>
<rawString>Zock, Michael and Slaven Bilac. 2004. Word lookup on the basis of associations : from an idea to a roadmap. In Workshop on ’Enhancing and using electronic dictionaries’, pages 29–35, Geneva. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
</authors>
<title>Navigational aids, a critical factor for the success of electronic dictionaries.</title>
<date>2006</date>
<booktitle>Perspectives on Cognition: A Festschrift for Manfred Wettler,</booktitle>
<pages>397--414</pages>
<editor>In Rapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp, editors,</editor>
<publisher>Pabst Science Publishers,</publisher>
<location>Lengerich.</location>
<contexts>
<context position="17265" citStr="Zock, 2006" startWordPosition="2883" endWordPosition="2884">all connected words. If the user can find the item he is looking for in this list, search stops, otherwise it will continue, the user giving other words of the list, or words evoked by them. Of course, remains the question of how to build this resource, in particular, how to populate the axis devoted to the trigger words, i.e. accesskeys. At present we consider three approaches: one, where we use the words occurring in word definitions (see also, (Dutoit and Nugues, 2002; Bilac et al., 2004)), the other is to mine a wellbalanced corpus, to find co-occurrences within a given window (Ferret and Zock, 2006), the size depending a bit on the text type (encyclopedia) or type of corpus. Still another solution would be to draw on the association lists produced by psychologists, see for example http://www.usf.edu/, or http://www.eat.rl.ac.uk. Of course, the idea of using matrices in linguistics is not new. There are at least two authors who have proposed its use: M. Gross (Gross, 1984) used it for coding the syntactic behavior of lexical items, hence the term lexicon-grammar, and G. Miller, the father of WN (Miller et al., 1990) suggested it to support lexical access. While the former work is not rele</context>
<context position="27640" citStr="Zock, 2006" startWordPosition="4613" endWordPosition="4614">is allows the filtering of all irrelevant words, to keep but a bag of words, that is, the nouns, adjectives, verbs and adverbs occuring in the paragraph. These words will be used to fill the triplets of our database. 12http://www.wikipedia.org 13The optimal window-size depends probably on the text type (encyclopedia vs. unformatted text). Yet, in the absence of clear criteria, we consider the optimal window-size as an open, empirical question. 14This latter aspect is not implemented yet, but will be added in the future, as it is a necessary component for easy navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). 14 Input WordNet Wikipedia 488 words 1845 words alcoholic country god characteristics regulation grape appellation system bottled like christian track . . . . . . wine grape sweet serve france small fruit dry bottle produce red bread hold . . . . . . 30 words 983 words month fish produce grain grape revolutionary autumn farms calendar festival energy cut harvest butterfish dollar combine ground person make balance rain wine first amount rich 5.3.3 Corpus Building We start arbitrarily from some page (for our experiment, we have chosen ”wine” as input), apply the algorithm outline</context>
</contexts>
<marker>Zock, 2006</marker>
<rawString>Zock, Michael. 2006. Navigational aids, a critical factor for the success of electronic dictionaries. In Rapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp, editors, Perspectives on Cognition: A Festschrift for Manfred Wettler, pages 397–414. Pabst Science Publishers, Lengerich.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
</authors>
<title>If you care to find what you are looking for, make an index: the case of lexical access.</title>
<date>2007</date>
<journal>ECTI, Transaction on Computer and Information Technology,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="27653" citStr="Zock, 2007" startWordPosition="4615" endWordPosition="4616">e filtering of all irrelevant words, to keep but a bag of words, that is, the nouns, adjectives, verbs and adverbs occuring in the paragraph. These words will be used to fill the triplets of our database. 12http://www.wikipedia.org 13The optimal window-size depends probably on the text type (encyclopedia vs. unformatted text). Yet, in the absence of clear criteria, we consider the optimal window-size as an open, empirical question. 14This latter aspect is not implemented yet, but will be added in the future, as it is a necessary component for easy navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007). 14 Input WordNet Wikipedia 488 words 1845 words alcoholic country god characteristics regulation grape appellation system bottled like christian track . . . . . . wine grape sweet serve france small fruit dry bottle produce red bread hold . . . . . . 30 words 983 words month fish produce grain grape revolutionary autumn farms calendar festival energy cut harvest butterfish dollar combine ground person make balance rain wine first amount rich 5.3.3 Corpus Building We start arbitrarily from some page (for our experiment, we have chosen ”wine” as input), apply the algorithm outlined here above </context>
</contexts>
<marker>Zock, 2007</marker>
<rawString>Zock, Michael. 2007. If you care to find what you are looking for, make an index: the case of lexical access. ECTI, Transaction on Computer and Information Technology, 2(2):71–80.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>