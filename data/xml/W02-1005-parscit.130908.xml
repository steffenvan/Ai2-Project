<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.443964">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 33-40.
Association for Computational Linguistics.
</note>
<title confidence="0.995675">
Augmented Mixture Models for Lexical Disambiguation
</title>
<author confidence="0.993866">
Silviu Cucerzan and David Yarowsky
</author>
<affiliation confidence="0.926509">
Department of Computer Science and
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.731975">
Baltimore, MD 21218, USA
</address>
<email confidence="0.999334">
{silviu,yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.995648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912375">
This paper investigates several augmented mixture
models that are competitive alternatives to standard
Bayesian models and prove to be very suitable to
word sense disambiguation and related classifica-
tion tasks. We present a new classification correc-
tion technique that successfully addresses the prob-
lem of under-estimation of infrequent classes in the
training data. We show that the mixture models are
boosting-friendly and that both Adaboost and our
original correction technique can improve the re-
sults of the raw model significantly, achieving state-
of-the-art performance on several standard test sets
in four languages. With substantially different out-
put to Naïve Bayes and other statistical methods, the
investigated models are also shown to be effective
participants in classifier combination.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999547177777778">
The focus tasks of this paper are two re-
lated problems in lexical ambiguity resolution:
Word Sense Disambiguation (WSD) and Context-
Sensitive Spelling Correction (CSSC).
Word Sense Disambiguation has a long history as
a computational task (Kelly and Stone, 1975), and
the field has recently supported large-scale interna-
tional system evaluation exercises in multiple lan-
guages (SENSEVAL-1, Kilgarriff and Palmer (2000),
and SENSEVAL-2, Edmonds and Cotton (2001)).
General purpose Spelling Correction is also a
long-standing task (e.g. McIlroy, 1982), tradi-
tionally focusing on resolving typographical errors
such as transposition and deletion to find the clos-
est “valid” word (in a dictionary or a morpholog-
ical variant), typically ignoring context. Yet Ku-
kich (1992) observed that about 25-50% of the
spelling errors found in modern documents are ei-
ther context-inappropriate misuses or substitutions
of valid words (such as principal and principle)
which are not detected by traditional spelling cor-
rectors. Previous work has addressed the problem
of CSSC from a machine learning perspective, in-
cluding Bayesian and Decision List models (Gold-
ing, 1995), Winnow (Golding and Roth, 1996) and
Transformation-Based Learning (Mangu and Brill,
1997).
Generally, both tasks involve the selection be-
tween a relatively small set of alternatives per key-
word (e.g. sense id’s such as church/BUILDING
and church/INSTITUTION or commonly confused
spellings such as quiet and quite), and are dependent
on local and long-distance collocational and syntac-
tic patterns to resolve between the set of alterna-
tives. Thus both tasks can share a common feature
space, data representation and algorithm infrastruc-
ture. We present a framework of doing so, while in-
vestigating the use of mixture models in conjunction
with a new error-correction technique as competi-
tive alternatives to Bayesian models. While several
authors have observed the fundamental similarities
between CSSC and WSD (e.g. Berleant, 1995 and
Roth, 1998), to our knowledge no previous com-
parative empirical study has tackled these two prob-
lems in a single unified framework.
</bodyText>
<sectionHeader confidence="0.96234" genericHeader="method">
2 Problem Formulation. Feature Space
</sectionHeader>
<bodyText confidence="0.999422909090909">
The problem of lexical disambiguation can be mod-
eled as a classification task, in which each in-
stance of the word to be disambiguated (target word,
henceforth), identified by its context, has to be la-
beled with one of the established sense labels
.1 The approaches we investigate
are statistical methods , out-
putting conditional probability distributions over the
sense set given a context . The classifica-
tion of a context is generally made by choosing
, but we also present an alterna-
</bodyText>
<footnote confidence="0.856814333333333">
1In the case of spelling correction, the classification labels
are represented by the confusion set rather than sense labels
(for example ).
</footnote>
<bodyText confidence="0.585282666666667">
... same table as the others but moved into
the other bar with my pint and my ...
Feature type Word POS Lemma
</bodyText>
<note confidence="0.418426">
Contextfeatures
</note>
<table confidence="0.997257571428572">
Context moved/VBD VBD move/V
Context into/IN IN into/I
Context the/DT DT the/D
Context other/JJ JJ other/J
Target bar/NN NN bar/N
Context with/IN IN with/I
Context my/PRP$ PRP$ my/P
Context pint/NN NN pint/N
Syntactic (predicate-argument) features
ObjectTo moved/VBD VBD move/V
Modifier other/JJ JJ other/J
Bigram collocational features
-1 Bigram other/JJ JJ other/J
+1 Bigram with/IN IN with/IN
</table>
<figureCaption confidence="0.999441">
Figure 1: Example context for WSD SENSEVAL-2 target
word bar (inventory of 21 senses) and extracted features
</figureCaption>
<bodyText confidence="0.996430333333333">
tive approach in Section 4.1.
The contexts are represented as a collection
of features. Previous work in WSD and CSSC
(Golding, 1995; Bruce et al., 1996; Yarowsky,
1996; Golding and Roth, 1996; Pedersen, 1998)
has found diverse feature types to be useful, in-
cluding inflected words, lemmas and part-of-speech
(POS) in a variety of collocational and syntactic re-
lationships, including local bigrams and trigrams,
predicate-argument relationships, and wide-context
bag-of-words associations. Examples of the feature
types we employ are illustrated in Figures 1 and 2.
The syntactic features are intended to capture
the predicate-argument relationships in the syn-
tactic window in which the target word occurs.
Different relations are considered depending on
the target word’s POS. For nouns, these relations
are: verb-object, subject-verb, modifier-noun, and
noun-modified_noun; for verbs: verb-object, verb-
particle/preposition, verb-prepositional_object; for
adjectives: modifying_adjective-noun. Also, words
with the same POS as the target word that are linked
to the target word by coordinating conjunctions are
extracted as sibling features. The extraction pro-
cess is performed using simple heuristic patterns
and regular expressions over the POS environment.
As Figure 2 shows, we considered for the CSSC
task the POS bigrams of the immediate left and right
word pairs as additional features in order to solve
POS ambiguity and capture more of the syntactic
environment in which the target word occurs (the
elements of a confusion set often have disjoint or
very different syntactic functions).
</bodyText>
<table confidence="0.9041646">
... presents another {piece,peace} of the problem ...
Feature type Word POS Lemma
Contextfeatures
Context presents VBZ present/V
Context another DT another/D
Target {peace,piece} NN /N
Context of IN of/I
Context the DT the/D
Context problem NN problem/N
Syntactic (predicate-argument) features
ObjectTo presents VBZ present/V
Modifier problem NN problem/N
Bigram collocationalfeatures
-1 Bigram another DT another/D
+1 Bigram of IN of/I
</table>
<equation confidence="0.846678333333333">
Bigram POS environment
POS-2-1 - VBZ+DT -
POS+1+2 - IN+DT -
</equation>
<figureCaption confidence="0.939728">
Figure 2: Example context for the spelling confusion set
{piece,peace} and extracted features
</figureCaption>
<sectionHeader confidence="0.986412" genericHeader="method">
3 Mixture Models (MM)
</sectionHeader>
<bodyText confidence="0.991143521276596">
We investigate in this Section a direct statistical
model that uses the same starting point as the algo-
rithm presented in Walker (1987). We then compare
the functionality and the performance of this model
to those of the widely used Naïve Bayes model for
the WSD task (Gale et al., 1992; Mooney, 1996;
Pedersen, 1998), enhanced with the full richer fea-
ture space beyond the traditional unordered bag-of-
words.
Algorithm 1 Naïve Bayes Model
It is known that Bayes decision rule is optimal if
the distribution of the data of each class is known
(Duda and Hart, 1973, ch. 2). However, the class-
conditional distributions of the data are not known
and have to be estimated. Both Naïve Bayes and
the mixture model we investigated estimate
starting from mathematically correct formulations,
and thus would be equivalent if the assumptions
they make were correct. Naïve Bayes makes the as-
sumption (used to transform Equation (1) into (2))
that the features are conditionally independent given
a sense label. The mixture model makes a simi-
lar assumption, by regarding a document as being
completely described by a union of independent fea-
tures (Equation (3)). In practice, these are not true.
Given the strong correlation and common redun-
dancy of the features in the case of WSD-related
tasks, in conjunction with the limited training data
on which the probabilities are estimated and the
high dimensionality of the feature space, these as-
sumptions lead to substantial modeling problems.
Another important observation is that very many
of the frequencies involved in the probability esti-
mation are zero because of the very sparse feature
space. Naïve Bayes depends heavily on probabil-
ities not being zero and therefore it has to rely on
smoothing. On the other hand, the mixture model is
more robust to unseen events, without the need for
explicit smoothing.
Under the proposed mixture model, the condi-
tional probability of a sensegiven a target word
in a contextis estimated as a mixture of the condi-
tional sense probability distributions for individual
context features:
Algorithm 2 Mixture Model
as opposed to the Naïve Bayes model in which the
probability of a sensegiven a contextis derived
from the prior probability ofweighted by the con-
ditional probabilities of the contextual features
given the sense.
The probabilities in (4) and in (2)
can be computed as maximum likelihood estimates
(MLE), by counting the co-occurrences of and
versus the occurrences of , respectively in the
training data. An extension to this classical estima-
tion method is to use distance-weighted counts in-
stead of raw counts for the relative frequencies:
denotes the training contexts of word and
the subset of corresponding to sense. When
is a syntactic headword, is computed by
raw count. When is a context word, is
computed as a function of the positionof the target
word inand the positions where oc-
curs in:
toregardless of the distance then MLE es-
timates are obtained. There are various other ways
of choosing the weighting measure. One natural
way is to transform the distance into a close-
ness measure by considering
(Manning and Schütze, 1999, ch. 14.1). This mea-
sure proves to be effective for the spelling correc-
tion task, where the words in the immediate vicinity
are far more important than the rest of the context
words2, but imposes counterproductive differences
between the much wider context positions (such as
+30 vs. +31) used in WSD, especially when con-
sidering large context windows. Experimental re-
sults indicate that it is more effective to level out
the local positional differences given by a continu-
ous weighting, by instead using weight-equivalent
regions which can be described with a simple step-
function , ( is a constant3).
A filtering process based on the overall impor-
tance of a word for the disambiguation of
is also employed, using alterations of the form
, with proportional to the
number of senses of target word which it co-
occurs with in the training set.4 In this way, the
words that occur only once in the training set, as
well as those that occur with most of the senses of
a word, providing no relevant information about the
sense itself, are penalized.
Improvements obtained using weighted frequen-
cies and filtering over MLE are shown in Table 1.
</bodyText>
<table confidence="0.9902486">
Bayes Mixture
MLE bag-of-words only 55.55 56.31
MLE with syntactic features 61.62 62.27
+ Weighting + Filtering 63.28 63.06
+ Collocational Senses5 65.70 65.41
</table>
<tableCaption confidence="0.998561">
Table 1: The increase in performance for successive variants
</tableCaption>
<bodyText confidence="0.7457715">
of Bayes and Mixture Model as evaluated by 5-fold cross vali-
dation on SENSEVAL-2 English data
can be seen as weighting factors in the
mixture model formula (4). When is a word,
</bodyText>
<footnote confidence="0.839780272727273">
2Golding and Schabes (1996) show that the most important
words for CSSC are contained within a window of .
3The results shown were obtained for with term
weights doubled within a context window. Various
other functions and parameters values were tried on held-out
parameter-optimization data for SENSEVAL-2.
4A normalization step is required to output probability dis-
tributions.
5The collocational sense information is specific to the
SENSEVAL-2 English task and relies on the given inventory of
collocation sense labels (e.g. art_gallery%1:06:00::).
</footnote>
<bodyText confidence="0.986302785714286">
. If are set
expresses the positional relationship be-
tween the occurrences of and the target word
in, and is computed using step-functions as de-
scribed previously. When is a syntactic head-
word, is chosen as the average value of two
ratios expressing the usefulness of the headword
type for the given target word and respectively for
the POS-class of the target word (adjective, noun,
verb). These ratios are estimated by using a jack-
knife (hold-one-out) procedure on the training set
and counting the number times the headword type
is a good predictor versus the number of times it is
a bad predictor.
</bodyText>
<table confidence="0.833060071428572">
Feature Type Value DMM Naïve Bayes
(position)
Lemma/POS
Syntactic Features
SubjectTo move/V 0 0
Modifier other/J 0 0
Bigrams
-1 Bigram other/J 0 0
+1 Bigram with/I 0.4444 0.0007
Contextual Features
Context(-17) pub/N 0.3677 0.0007
Context(-13) sit/V 0.5708 0.0028
Context(-9) table/N 0.7173 0.0008
Context(-4) move/V 0.2990 0.0007
</table>
<figure confidence="0.891806444444444">
Context(-3) into/I - -
Context(-2) the/D - -
Context(-1) other/J - -
Target bar/N 0.4296 [0.0530]
Context(+1) with/I - -
Context(+2) my/P - -
Context(+3) pint/N 0.3333 0.0001
... ... ... ...
Posterior probability : =.46 =.29
</figure>
<figureCaption confidence="0.80406">
Figure 3: A WSD example that shows the influence of
syntactic, collocational and long-distance context features, the
probability estimates used by Naïve Bayes and MM and their
associated weights (), and the posterior probabilities of the
true sense as computed by the two models.
</figureCaption>
<bodyText confidence="0.998751285714286">
As shown in Table 1, Bayes and mixture models
yield comparable results for the given task. How-
ever, they capture the properties of the feature space
in distinct ways (example applications of the two
models on the sentence in Figure 1 are illustrated in
Figure 3) and therefore, are very appropriate to be
used together in combination (see Section 5.4).
</bodyText>
<sectionHeader confidence="0.940041" genericHeader="method">
4 Classification Correction and Boosting
</sectionHeader>
<bodyText confidence="0.9999784">
We first present an original classification correction
method based on the variation of posterior probabil-
ity estimates across data and then the adaptation of
the Adaboost method (Freund and Schapire, 1997)
to the task of lexical classification.
</bodyText>
<subsectionHeader confidence="0.9839225">
4.1 The Maximum Variance Correction
Method (MVC)
</subsectionHeader>
<bodyText confidence="0.999791404761905">
One problem arising from the sparseness of training
data is that mixture models tend to excessively fa-
vor the best represented senses in the training set. A
probable cause is that spurious words, which can not
be considered general stopwords but do not carry
sense-disambiguation information for a particular
target word, may occur only by chance both in train-
ing and test data.6 Another cause is the fact that
mixture models search for decision surfaces linear
in the feature space7; therefore, they can not make
only correct classifications (unless the feature space
can be divided by linear conditions) and the sam-
ples for the under-represented senses are likely to
be interpreted as outliers.
To address this estimation problem, a second
classification step is employed, based on the obser-
vation that the deviation of a component of the pos-
terior distribution from its expected value (as com-
puted over the training set) can be as relevant as the
maximum of the distribution . In-
stead of classifying each test context independently
after estimating its sense probability distribution,
we classify it by comparing it with the whole space
of training contexts, for which the posterior distri-
butions are computed using a jackknife procedure.
Figure 4(a) illustrates such an example: each line
in the table represents the posterior distribution over
senses given a context, each column contains the
values corresponding to a particular sense in the
posterior distributions of all contexts. Intuitively,
sense may be preferred to the most likely sense
for the test context despite the fact that
the is smaller than because
of the analogy with and the “expected val-
ues” of the components corresponding toand .
Unfortunately, we face again the problem of
under-representation in the training data: the ex-
pected values in the posterior distributions for the
under-represented senses when they express the cor-
rect classification can not be accurately estimated.
Therefore, we have to look at the problem from an-
other angle.
</bodyText>
<footnote confidence="0.99679425">
6For example, assuming that every context contains approx-
imately the same number of such words, then given two senses,
one represented in the training set by 20 examples, and the
other one by 4, it is five times more likely that a spurious word
in a test context co-occurs with the larger sampled sense.
7Roth (1998) shows that Bayes, TBL and Decision Lists
also search for a decision surface which is a linear function in
the feature space
</footnote>
<figure confidence="0.992748515463917">
3
8
2
1
.3
.5
.5
1
-
-
-
2
-
-
2
. . .
. . .
. . .
. . .
. . .
. . .
0.44
0.41
. . .
. . .
. . .
. . .
Ts
Ts
. . .
. . .
. . .
. . .
0.31
0.24
Senses:
c (art)
1
c (art)
2
c (art)
3
Training
contexts
c (art)
4
c (art)
5
c (art)
6
Test
context
c (art)
157
Variational Coefficients cs,c
P(s|c)
s1 . . . s n . . . sk−1 sk
s . . . s n
1 . . . sk−1 sk
c (art) −0.6 . . . +1.6 . . . . . . . . .
1 −0.4 . . . +1.2 . . . . . . . . .
c (art) +1.2 . . . −0.8 . . . +2.3 . . .
2 +2.9 . . . −0.4 . . . . . . . . .
c (art) −0.6 . . . +0.5 . . . . . . . . .
3 −0.2 . . . −0.4 . . . . . . +1.8
c (art) . . . . . . . . . . . . . . . . . .
4
c (art)
5
c (art)
6
c (art)
157
+3.5 . . . −0.2 . . . . . . . . .
0.26 0.33
. . .
0.29
0.36
0.29 . . . . . . 0.26
. . . . . . . . . . . . . . . . . .
0.04
0.05
0.13
0.21
0.04
0.06
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
(a) Probability distributions computed by MM using jack- (b) The variational coefficients for the example
knife on the training set and a test context on the left
</figure>
<figureCaption confidence="0.9407245">
Figure 4: WSD example showing the utility of the MVC method. A sensewith a high variational coefficient is preferred to
the mode of the MM distribution (the fields corresponding to the true sense are highlighted)
</figureCaption>
<bodyText confidence="0.992318352941177">
The mathematical support is provided by Cheby-
shev’s inequality , which
allows us to place an upper bound on the probabil-
ity that the value of a random variable is larger
than a set value, given the mean and variance of
. Considering a finite selection from a
distribution for which and exist and can be
estimated8 as the empirical mean
and empirical variance ,
and given another set , the elements of
that are least probable as being generated from
are those for which the variational coefficients
are large.
To apply this assumption to the disambiguation
task, a set containing the values for all
contexts in the training set that are not labeled
is built for every sense (see Figure 4(a)). In this
way, the problem of poor representation of some
senses is overcome and the selectionsare large
for all senses. An instance in the test set is consid-
ered more likely to correspond to a sense if the
estimated value is an outlier with respect to
(see Figure 4(b)) and thus it is viewed as a can-
didate for having its classification changed to.
Assuming that the selections are representa-
tive and there exist first and second order moments
for the underlying distributions (conditions which
we call “good statistical properties”), an improve-
ment in the accuracy of the classifier can
be expected when choosing a sense with a varia-
tional coefficient instead of the clas-
sifier distribution’s mode (if such
a sense exists). For example, knowing that the per-
formance of the mixture model for SENSEVAL-2 is
</bodyText>
<footnote confidence="0.98694">
8It is hard to judge how well estimated these statistics are
without making any distributional assumptions.
</footnote>
<bodyText confidence="0.999619695652174">
approximatively , the threshold for variational
coefficients is set to . Because spurious words
not only favor the better represented senses in the
training set, but also can affect the variational coef-
ficients of unlikely senses, some restrictions had to
be imposed in our implementation to avoid the other
extreme of favoring unlikely senses.
The mixture model does not guarantee the re-
quirements imposed by the MVC method are met,
but it has the advantage over the Bayesian model
that each of the components of the posterior distri-
bution it computes can be seen as a weighted mix-
ture of random variables corresponding to the indi-
vidual features. In the simplest case, when consid-
ering binary features, these variables are Bernoulli
trials. Furthermore, if the trials have the same
probability-mass function then a component of the
posterior distribution will follow a binomial distri-
bution, and therefore would have good statistical
properties. In general, the underlying distributions
can not be computed, but our experiments show that
they usually have good statistical properties as re-
quired by MVC.
</bodyText>
<subsectionHeader confidence="0.974306">
4.2 AdaBoost
</subsectionHeader>
<bodyText confidence="0.999886071428571">
AdaBoost is an iterative boosting algorithm intro-
duced by Freund and Schapire (1997) shown to be
successful for several natural language classifica-
tion tasks. AdaBoost successively builds classifiers
based on a weak learner (base learning algorithm)
by weighting differently the examples in the training
space, and outputs the final classification by mix-
ing the predictions of the iteratively built classifiers.
Because sense disambiguation is a multi-class prob-
lem, we chose to use version AdaBoost.M2.
We could not apply AdaBoost straightforwardly
to the problem of sense disambiguation because of
the high dimensionality and sparseness of the fea-
ture space. Superficial modeling of the training
set can easily be achieved because of the singu-
larity/rarity of many feature values in the context
space, but this largely represents overfitting of the
training data. In order to solve this problem, we
use AdaBoost in conjunction with jackknife and a
partial updating technique. At each round, clas-
sifiers are built using as training all the examples in
the training set except the one to be classified, and
the weights are updated at feature level rather than
context level. This modified Adaboost algorithm
could only be implemented for the mixture model,
which “perceives” the contexts as additive mixture
of features. The Adaboost-enhanced mixture model
is called AdaMixt henceforth.
</bodyText>
<sectionHeader confidence="0.997086" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999975961538461">
We present a comparative study for four languages
(English, Swedish, Spanish, and Basque) by per-
forming 5-fold cross-validation on the SENSEVAL-2
lexical-sample training data, using the fine-grained
sense inventory. For English and Swedish, for
which POS-tagged training data was available to
us, the fnTBL algorithm (Ngai and Florian, 2001)
based on Brill (1995) was used to annotate the data,
while for Spanish a mildly-supervised POS-tagging
system similar to the one presented in Cucerzan and
Yarowsky (2000) was employed. We also present
the results obtained by the different algorithms on
another WSD standard set, SENSEVAL-1, also by
performing 5-fold cross validation on the original
training data. For CSSC, we tested our system
on the identical data from the Brown corpus used
by Golding (1995), Golding and Roth (1996) and
Mangu and Brill (1997). Finally, we present the re-
sults obtained by the investigated methods on a sin-
gle run on the Senseval-1 and Senseval-2 test data.
The described models were initially trained and
tested by performing 5-fold cross-validation on the
SENSEVAL-2 English lexical-sample-task training
data. When parameters needed to be estimated,
jackknife or a 3-1-1 split (training and/or parame-
ter estimation - testing) were used.
</bodyText>
<subsectionHeader confidence="0.931283">
5.1 SENSEVAL-2
</subsectionHeader>
<bodyText confidence="0.9998845">
The English training set for SENSEVAL-2 is com-
posed of 8861 instances representing 73 target
words with an average number of 12.5 senses per
word. Table 2 illustrates the performance of each
of the studied models broken down by part-of-
speech. As observed in most experiments, the
feature-enhanced Naïve Bayes has the tendency
to outperform by a small margin the raw mixture
model, but because the latter proved to be boosting-
friendly, its augmented versions achieved the high-
est final accuracies. The difference between MMVC
and enhanced Naïve Bayes is significant (McNemar
</bodyText>
<table confidence="0.914006285714286">
rejection risk of ).
Adjectives Nouns Verbs Overall
Most Likely 52.11 52.01 27.28 41.79
Naïve Bayes (FE) 73.18 72.74 55.54 65.70
Mixture 73.90 71.09 56.16 65.41
AdaMixt 74.68 72.17 56.41 66.09
MMVC 74.68 73.06 57.06 66.72
</table>
<tableCaption confidence="0.979682">
Table 2: Results using 5-fold cross validation on SENSEVAL-
2 English lexical-sample training data
</tableCaption>
<bodyText confidence="0.996270714285714">
Figure 5 shows both the performance of the mix-
ture model alone and in conjunction with MVC,
and highlights the improvement in performance
achieved by the latter for each of the 4 languages.
All MMVC versus MM differences are statistically
significant (for SENSEVAL-2 English data, the rejec-
tion probability of a paired McNemar test is ).
</bodyText>
<subsectionHeader confidence="0.539002">
English Spanish Swedish Basque
</subsectionHeader>
<figureCaption confidence="0.987151">
Figure 5: MM and MMVC performance by performing 5-
fold cross validation on SENSEVAL-2 data for 4 languages
</figureCaption>
<bodyText confidence="0.994563769230769">
Figure 6 shows what is generally a log-linear in-
crease in performance of MM alone and in combi-
nation with the MVC method over increasing train-
ing sizes. Because of the way the smallest training
sets were created to include at least one example for
each sense, they were more balanced as a side effect,
and the compensations introduced by MVC were
less productive as a result. Given more training data,
MMVC starts to improve relative to the raw model
both because the training sets become more unbal-
anced in their sense distributions and because the
empirical moments and the variational coefficients
on which the method relies are better estimated.
</bodyText>
<subsectionHeader confidence="0.887867">
5.2 SENSEVAL-1
</subsectionHeader>
<bodyText confidence="0.94211">
The systems used for SENSEVAL-2 English data
were also evaluated on the SENSEVAL-1 training
</bodyText>
<figure confidence="0.994660214285714">
Sense Classification Accuracy
45
40
75
70
65
60
55
50
� �
41.79
65.41
Most Likely
MM
MMVC
66.72
� �
45.94
65.58
66.71
46.25
59.66
61.84
62.75
68.61
69.68
20 40 60 80
Percent of Available Training Data
</figure>
<figureCaption confidence="0.9981935">
Figure 6: Learning Curve for MM and MMVC on
SENSEVAL-2 English (cross-validated on heldout data)
</figureCaption>
<bodyText confidence="0.994324375">
data (30 words, 12479 instances, with an average
of 10.8 senses per word) by using 5-fold cross val-
idation. There was no further tuning of the feature
space or model parameters to adapt them to the par-
ticularities of this new test set. Comparative perfor-
mance is shown in Table 3. The difference between
MMVC and enhanced Naïve Bayes is statistically
significant (McNemar rejection risk 0.036).
</bodyText>
<table confidence="0.995779666666667">
Adjectives Nouns Verbs Overall
Most Likely 63.43 66.52 57.6 63.09
Naïve Bayes (FE) 75.67 84.15 76.65 80.16
Mixture 76.45 81.57 75.9 78.79
AdaMixt 76.83 83.39 77.10 80.16
MMVC 78.49 84.79 76.81 81.06
</table>
<tableCaption confidence="0.9726815">
Table 3: Results using 5-fold cross validation on SENSEVAL-
1 training data (English)
</tableCaption>
<subsectionHeader confidence="0.994986">
5.3 Spelling Correction
</subsectionHeader>
<bodyText confidence="0.998762176470588">
Both MM and the enhanced Bayes model obtain vir-
tually the same overall performance9 as the TriB-
ayes system reported in (Golding and Schabes,
1996), which uses a similar feature space. The
correction and boosting methods we investigated
marginally improve the performance of the mixture
model, as can be seen in Table 4 but they do not
achieve the performance of RuleS 93.1% (Mangu
and Brill, 1997) and Winnow 93.5% (Golding and
Roth, 1996; Golding and Roth, 1999), methods
that include features more directly specialized for
spelling correction. Because of the small size of the
test set, the differences in performance are due to
only 14 and 20 more incorrectly classified exam-
ples respectively. More important than this differ-
ence10 may be the fact that the systems built for
WSD were able to achieve competitive performance
</bodyText>
<footnote confidence="0.95802475">
9All figures reported are for the standard 14 confusion sets;
the accuracies for the 18 sets are generally higher.
10We did not have the actual classifications from the other
systems to check the significance of the difference.
</footnote>
<bodyText confidence="0.997062333333333">
with little to no adaptation (we only enriched the
feature space by adding the POS bigrams to the left
and right of the target word and changed the weight-
ing model as presented in Section 3 because spelling
correction relies more on the immediate than long-
distance context). Another important aspect that can
</bodyText>
<table confidence="0.997351529411765">
test M.L. Bayes MM AdaMixt MMVC
size
accept 50 70.0 92.0 90.0 90.0 94.2
affect 49 91.8 95.9 98.0 98.0 93.9
among 186 71.5 80.6 78.5 81.2 80.6
amount 123 71.5 79.7 79.7 82.9 83.7
begin 146 93.2 96.6 96.6 97.3 96.6
country 62 91.9 93.5 95.2 93.5 93.5
lead 49 46.9 93.9 91.8 95.9 91.8
past 74 68.9 86.5 93.2 93.2 93.2
peace 50 44.0 78.0 80.0 78.0 80.0
principal 34 58.8 82.3 88.2 85.3 88.2
quiet 66 83.3 93.9 93.9 93.9 95.5
raise 39 64.1 87.2 84.6 84.6 87.2
than 514 63.4 96.9 96.5 96.5 96.5
weather 61 86.9 98.4 95.1 96.7 98.4
Overall 1503 71.1 91.2 91.2 91.8 92.2
</table>
<tableCaption confidence="0.999896">
Table 4: Results on the standard 14 CSSC data sets
</tableCaption>
<bodyText confidence="0.98784575">
be seen in Table 4 is that there was no model that
constantly performed best in all situations, suggest-
ing the advantage of developing a diverse space of
models for classifier combination.
</bodyText>
<subsectionHeader confidence="0.996336">
5.4 Using MMVC in Classifier Combination
</subsectionHeader>
<bodyText confidence="0.991873142857143">
The investigated MMVC model proves to be a
very effective participant in classifier combination,
with substantially different output to Naïve Bayes
(9.6% averaged complementary rate, as defined in
Brill and Wu (1998)). Table 5 shows the im-
provement obtained by adding the MMVC model
to empirically the best voting system we had us-
ing Bayes, BayesRatio, TBL and Decision Lists
(all classifier combination methods tried and their
results are presented exhaustively in Florian and
Yarowsky (2002)). The improvement is significant
in both cases, as measured by a paired McNemar
test: for SENSEVAL-1 data,
for SENSEVAL-2 data.
</bodyText>
<table confidence="0.965686">
without with error
MMVC MMVC reduction
Senseval1 82.26 83.06 4.5%
Senseval2 67.53 68.66 3.5%
</table>
<tableCaption confidence="0.988127333333333">
Table 5: The contribution of MMVC in a rank-based classi-
fier combination on SENSEVAL-1 and SENSEVAL-2 English as
computed by 5-fold cross validation over training data
</tableCaption>
<bodyText confidence="0.5305715">
MMVC is also the top performer of the 5 sys-
tems mentioned above on SENSEVAL-2 English test
</bodyText>
<figure confidence="0.983100444444445">
66
64
62
60
58
56
MMVC
MM
Sense Classification Accuracy
</figure>
<bodyText confidence="0.99548375">
data, with an accuracy of 62.5%. Table 6 contrasts
the performance obtained by the MMVC method to
the average and best system performance in the two
SENSEVAL exercises.
</bodyText>
<table confidence="0.938769125">
SENSEVAL-1(30 target words, 7446 instances)
Average / Best SENSEVAL-1 Competitor 73.1 2.9 / 77.1
MMVC alone 76.9
Classifier combination with MMVC 80.0
SENSEVAL-2 (73 target words, 4328 instances)
Average / Best SENSEVAL-2 Competitor 55.7 5.3 / 64.2
MMVC alone 62.5
Classifier combination with MMVC 66.5
</table>
<tableCaption confidence="0.986497333333333">
Table 6: Accuracy on SENSEVAL-1 and SENSEVAL-2 En-
glish test data (only the supervised systems with a coverage of
at least 97% were used to compute the mean and variance)
</tableCaption>
<sectionHeader confidence="0.998616" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999989210526316">
We investigated the properties and performance of
mixture models and two augmenting methods in an
unified framework for Word Sense Disambiguation
and Context-Sensitive Spelling Correction, showing
experimentally that such joint models can success-
fully match and exceed the performance of feature-
enhanced Bayesian models. The new classifica-
tion correction method (MVC) we propose suc-
cessfully addresses the problem of under-estimation
of less likely classes, consistently and significantly
improving the performance of the main mixture
model across all tasks and languages. Finally, since
the mixture model and its improvements performed
well on two major tasks and several multilingual
data sets, we believe that they can be productively
applied to other related high-dimensionality lexi-
cal classification problems, including named-entity
classification, topic classification, and lexical choice
in machine translation.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999962084507042">
D. Berleant. 1995. Engineering &amp;quot;word experts&amp;quot; for word disam-
biguation. Natural Language Engineering, 1(4):339–362.
E. Brill and J. Wu. 1998. Classifier combination for improved
lexical disambiguation. In Proceedings of COLING-ACL’98,
pages 191–195.
E. Brill. 1995. Transformation-based error-driven learning and
natural language processing: A case study in part of speech
tagging. Computational Linguistics, 21(4):543–565.
R. Bruce, J. Wiebe, and T. Pedersen. 1996. The measure of a
model. In Proceedings ofEMNLP-1996, pages 101–112.
S. Cucerzan and D. Yarowsky. 2000. Language independent
minimally supervised induction of lexical probabilities. In
Proceedings ofACL-2000, pages 270–277.
R. O. Duda and P. E. Hart. 1973. Pattern Classification and
Scene Analysis. Wiley.
P. Edmonds and S. Cotton. 2001. SENSEVAL-2 overview. In
Proceedings of SENSEVAL-2, pages 1–6.
R. Florian and D. Yarowsky. 2002. Modeling consensus: Classi-
fier combination for word sense disambiguation. In Proceed-
ings of EMNLP-2002.
Y. Freund and R. E. Schapire. 1997. A decision-theoretic gener-
alization of on-line learning and application to boosting. Jour-
nal of Computer and System Sciences, 55:119–139.
W. Gale, K. Church, and D. Yarowsky. 1992. A method for
disambiguating word senses in a large corpus. Computers and
the Humanities, 26:415–439.
A. R. Golding and D. Roth. 1996. Applying winnow to context-
sensitive spelling correction. In Machine Learning: Proceed-
ings of the 13th International Conference, pages 182–190.
A. R. Golding and D. Roth. 1999. A winnow-based approach
to context-sensitive spelling correction. Machine Learning,
34(1-3):107–130.
A. R. Golding and Y. Schabes. 1996. Combining trigram-based
and feature-based methods for context-sensitive spelling cor-
rection. In Proceedings ofACL-1996, pages 71–78.
A. R. Golding. 1995. A Bayesian hybrid method for context-
sensitive spelling correction. In Proceedings of the Third
Workshop on Very Large Corpora, pages 39–53.
E. F. Kelly and P. J. Stone. 1975. Computer Recognition of
English Word Senses. North Holland Press.
A. Kilgarriff and M. Palmer. 2000. Introduction to the special
issue on SENSEVAL. Computers and the Humanities, 34(1-
2):1–13.
K. Kukich. 1992. Techniques for automatically correcting words
in text. ACM Computing Surveys, 24(4):377–439.
L. Mangu and E. Brill. 1997. Automatic rule acquisition for
spelling correction. In Proceedings of the 14th International
Conference on Machine Learning, pages 734–741.
C.D. Manning and H. Schütze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press.
M. D. McIlroy. 1982. Development of a spelling list. j-IEEE-
TRANS-COMM, COM-30(1):91–99.
R. J. Mooney. 1996. Comparative experiments on disambiguat-
ing word senses: An illustration of the role of bias in machine
learning. In Proceedings of EMNLP-1996, pages 82–91.
G. Ngai and R. Florian. 2001. Transformation-based learning in
the fast lane. In Proceedings ofNAACL-2001, pages 40–47.
T. Pedersen. 1998. Naïve Bayes as a satisficing model. In Work-
ing Notes of the AAAI Spring Symposium on Satisficing Mod-
els, pages 60–67.
D. Roth. 1998. Learning to resolve natural language ambiguities:
a unified approach. In Proceedings of the 15th Conference of
the AAAI, pages 806–813.
D. E. Walker. 1987. Knowledge resource tools for accessing
large text files. In Sergei Nirenburg, editor, Machine Trans-
lation: Theoretical and Methodogical Issues, pages 247–261.
Cambridge University Press.
D. Yarowsky. 1996. Homograph disambiguation in speech
synthesis. In J. Olive J. van Santen, R. Sproat and
J. Hirschberg, editors, Progress in Speech Synthesis, pages
159–175. Springer-Verlag.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.435830">
<note confidence="0.940476666666667">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 33-40. Association for Computational Linguistics.</note>
<title confidence="0.933386">Augmented Mixture Models for Lexical Disambiguation</title>
<author confidence="0.952333">Silviu Cucerzan</author>
<author confidence="0.952333">David</author>
<affiliation confidence="0.876515">Department of Computer Science Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.992624">Baltimore, MD 21218,</address>
<email confidence="0.999818">silviu@cs.jhu.edu</email>
<email confidence="0.999818">yarowsky@cs.jhu.edu</email>
<abstract confidence="0.994635764705882">This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classification tasks. We present a new classification correction technique that successfully addresses the problem of under-estimation of infrequent classes in the training data. We show that the mixture models are boosting-friendly and that both Adaboost and our original correction technique can improve the results of the raw model significantly, achieving stateof-the-art performance on several standard test sets in four languages. With substantially different output to Naïve Bayes and other statistical methods, the investigated models are also shown to be effective participants in classifier combination.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Berleant</author>
</authors>
<title>Engineering &amp;quot;word experts&amp;quot; for word disambiguation.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>4</issue>
<contexts>
<context position="3226" citStr="Berleant, 1995" startWordPosition="466" endWordPosition="467">as church/BUILDING and church/INSTITUTION or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntactic patterns to resolve between the set of alternatives. Thus both tasks can share a common feature space, data representation and algorithm infrastructure. We present a framework of doing so, while investigating the use of mixture models in conjunction with a new error-correction technique as competitive alternatives to Bayesian models. While several authors have observed the fundamental similarities between CSSC and WSD (e.g. Berleant, 1995 and Roth, 1998), to our knowledge no previous comparative empirical study has tackled these two problems in a single unified framework. 2 Problem Formulation. Feature Space The problem of lexical disambiguation can be modeled as a classification task, in which each instance of the word to be disambiguated (target word, henceforth), identified by its context, has to be labeled with one of the established sense labels .1 The approaches we investigate are statistical methods , outputting conditional probability distributions over the sense set given a context . The classification of a context is</context>
</contexts>
<marker>Berleant, 1995</marker>
<rawString>D. Berleant. 1995. Engineering &amp;quot;word experts&amp;quot; for word disambiguation. Natural Language Engineering, 1(4):339–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="28856" citStr="Brill and Wu (1998)" startWordPosition="4785" endWordPosition="4788">4.6 84.6 87.2 than 514 63.4 96.9 96.5 96.5 96.5 weather 61 86.9 98.4 95.1 96.7 98.4 Overall 1503 71.1 91.2 91.2 91.8 92.2 Table 4: Results on the standard 14 CSSC data sets be seen in Table 4 is that there was no model that constantly performed best in all situations, suggesting the advantage of developing a diverse space of models for classifier combination. 5.4 Using MMVC in Classifier Combination The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naïve Bayes (9.6% averaged complementary rate, as defined in Brill and Wu (1998)). Table 5 shows the improvement obtained by adding the MMVC model to empirically the best voting system we had using Bayes, BayesRatio, TBL and Decision Lists (all classifier combination methods tried and their results are presented exhaustively in Florian and Yarowsky (2002)). The improvement is significant in both cases, as measured by a paired McNemar test: for SENSEVAL-1 data, for SENSEVAL-2 data. without with error MMVC MMVC reduction Senseval1 82.26 83.06 4.5% Senseval2 67.53 68.66 3.5% Table 5: The contribution of MMVC in a rank-based classifier combination on SENSEVAL-1 and SENSEVAL-2</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of COLING-ACL’98, pages 191–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="22385" citStr="Brill (1995)" startWordPosition="3711" endWordPosition="3712">her than context level. This modified Adaboost algorithm could only be implemented for the mixture model, which “perceives” the contexts as additive mixture of features. The Adaboost-enhanced mixture model is called AdaMixt henceforth. 5 Evaluation We present a comparative study for four languages (English, Swedish, Spanish, and Basque) by performing 5-fold cross-validation on the SENSEVAL-2 lexical-sample training data, using the fine-grained sense inventory. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 an</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
<author>T Pedersen</author>
</authors>
<title>The measure of a model.</title>
<date>1996</date>
<booktitle>In Proceedings ofEMNLP-1996,</booktitle>
<pages>101--112</pages>
<contexts>
<context position="4810" citStr="Bruce et al., 1996" startWordPosition="720" endWordPosition="723">to/IN IN into/I Context the/DT DT the/D Context other/JJ JJ other/J Target bar/NN NN bar/N Context with/IN IN with/I Context my/PRP$ PRP$ my/P Context pint/NN NN pint/N Syntactic (predicate-argument) features ObjectTo moved/VBD VBD move/V Modifier other/JJ JJ other/J Bigram collocational features -1 Bigram other/JJ JJ other/J +1 Bigram with/IN IN with/IN Figure 1: Example context for WSD SENSEVAL-2 target word bar (inventory of 21 senses) and extracted features tive approach in Section 4.1. The contexts are represented as a collection of features. Previous work in WSD and CSSC (Golding, 1995; Bruce et al., 1996; Yarowsky, 1996; Golding and Roth, 1996; Pedersen, 1998) has found diverse feature types to be useful, including inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic relationships, including local bigrams and trigrams, predicate-argument relationships, and wide-context bag-of-words associations. Examples of the feature types we employ are illustrated in Figures 1 and 2. The syntactic features are intended to capture the predicate-argument relationships in the syntactic window in which the target word occurs. Different relations are considered depending </context>
</contexts>
<marker>Bruce, Wiebe, Pedersen, 1996</marker>
<rawString>R. Bruce, J. Wiebe, and T. Pedersen. 1996. The measure of a model. In Proceedings ofEMNLP-1996, pages 101–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cucerzan</author>
<author>D Yarowsky</author>
</authors>
<title>Language independent minimally supervised induction of lexical probabilities.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL-2000,</booktitle>
<pages>270--277</pages>
<contexts>
<context position="22534" citStr="Cucerzan and Yarowsky (2000)" startWordPosition="3732" endWordPosition="3735">xts as additive mixture of features. The Adaboost-enhanced mixture model is called AdaMixt henceforth. 5 Evaluation We present a comparative study for four languages (English, Swedish, Spanish, and Basque) by performing 5-fold cross-validation on the SENSEVAL-2 lexical-sample training data, using the fine-grained sense inventory. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 and Senseval-2 test data. The described models were initially trained and tested by performing 5-fold cross-validation on the SENSEVAL-2 English lexica</context>
</contexts>
<marker>Cucerzan, Yarowsky, 2000</marker>
<rawString>S. Cucerzan and D. Yarowsky. 2000. Language independent minimally supervised induction of lexical probabilities. In Proceedings ofACL-2000, pages 270–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="7430" citStr="Duda and Hart, 1973" startWordPosition="1114" endWordPosition="1117">iece,peace} and extracted features 3 Mixture Models (MM) We investigate in this Section a direct statistical model that uses the same starting point as the algorithm presented in Walker (1987). We then compare the functionality and the performance of this model to those of the widely used Naïve Bayes model for the WSD task (Gale et al., 1992; Mooney, 1996; Pedersen, 1998), enhanced with the full richer feature space beyond the traditional unordered bag-ofwords. Algorithm 1 Naïve Bayes Model It is known that Bayes decision rule is optimal if the distribution of the data of each class is known (Duda and Hart, 1973, ch. 2). However, the classconditional distributions of the data are not known and have to be estimated. Both Naïve Bayes and the mixture model we investigated estimate starting from mathematically correct formulations, and thus would be equivalent if the assumptions they make were correct. Naïve Bayes makes the assumption (used to transform Equation (1) into (2)) that the features are conditionally independent given a sense label. The mixture model makes a similar assumption, by regarding a document as being completely described by a union of independent features (Equation (3)). In practice,</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. O. Duda and P. E. Hart. 1973. Pattern Classification and Scene Analysis. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Edmonds</author>
<author>S Cotton</author>
</authors>
<title>SENSEVAL-2 overview.</title>
<date>2001</date>
<booktitle>In Proceedings of SENSEVAL-2,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="1701" citStr="Edmonds and Cotton (2001)" startWordPosition="234" endWordPosition="237">t output to Naïve Bayes and other statistical methods, the investigated models are also shown to be effective participants in classifier combination. 1 Introduction The focus tasks of this paper are two related problems in lexical ambiguity resolution: Word Sense Disambiguation (WSD) and ContextSensitive Spelling Correction (CSSC). Word Sense Disambiguation has a long history as a computational task (Kelly and Stone, 1975), and the field has recently supported large-scale international system evaluation exercises in multiple languages (SENSEVAL-1, Kilgarriff and Palmer (2000), and SENSEVAL-2, Edmonds and Cotton (2001)). General purpose Spelling Correction is also a long-standing task (e.g. McIlroy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous work has addressed the problem of CSSC fro</context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>P. Edmonds and S. Cotton. 2001. SENSEVAL-2 overview. In Proceedings of SENSEVAL-2, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>D Yarowsky</author>
</authors>
<title>Modeling consensus: Classifier combination for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-2002.</booktitle>
<contexts>
<context position="29133" citStr="Florian and Yarowsky (2002)" startWordPosition="4829" endWordPosition="4832">suggesting the advantage of developing a diverse space of models for classifier combination. 5.4 Using MMVC in Classifier Combination The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naïve Bayes (9.6% averaged complementary rate, as defined in Brill and Wu (1998)). Table 5 shows the improvement obtained by adding the MMVC model to empirically the best voting system we had using Bayes, BayesRatio, TBL and Decision Lists (all classifier combination methods tried and their results are presented exhaustively in Florian and Yarowsky (2002)). The improvement is significant in both cases, as measured by a paired McNemar test: for SENSEVAL-1 data, for SENSEVAL-2 data. without with error MMVC MMVC reduction Senseval1 82.26 83.06 4.5% Senseval2 67.53 68.66 3.5% Table 5: The contribution of MMVC in a rank-based classifier combination on SENSEVAL-1 and SENSEVAL-2 English as computed by 5-fold cross validation over training data MMVC is also the top performer of the 5 systems mentioned above on SENSEVAL-2 English test 66 64 62 60 58 56 MMVC MM Sense Classification Accuracy data, with an accuracy of 62.5%. Table 6 contrasts the performa</context>
</contexts>
<marker>Florian, Yarowsky, 2002</marker>
<rawString>R. Florian and D. Yarowsky. 2002. Modeling consensus: Classifier combination for word sense disambiguation. In Proceedings of EMNLP-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>A decision-theoretic generalization of on-line learning and application to boosting.</title>
<date>1997</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>55--119</pages>
<contexts>
<context position="14116" citStr="Freund and Schapire, 1997" startWordPosition="2197" endWordPosition="2200">mputed by the two models. As shown in Table 1, Bayes and mixture models yield comparable results for the given task. However, they capture the properties of the feature space in distinct ways (example applications of the two models on the sentence in Figure 1 are illustrated in Figure 3) and therefore, are very appropriate to be used together in combination (see Section 5.4). 4 Classification Correction and Boosting We first present an original classification correction method based on the variation of posterior probability estimates across data and then the adaptation of the Adaboost method (Freund and Schapire, 1997) to the task of lexical classification. 4.1 The Maximum Variance Correction Method (MVC) One problem arising from the sparseness of training data is that mixture models tend to excessively favor the best represented senses in the training set. A probable cause is that spurious words, which can not be considered general stopwords but do not carry sense-disambiguation information for a particular target word, may occur only by chance both in training and test data.6 Another cause is the fact that mixture models search for decision surfaces linear in the feature space7; therefore, they can not ma</context>
<context position="20706" citStr="Freund and Schapire (1997)" startWordPosition="3455" endWordPosition="3458">d mixture of random variables corresponding to the individual features. In the simplest case, when considering binary features, these variables are Bernoulli trials. Furthermore, if the trials have the same probability-mass function then a component of the posterior distribution will follow a binomial distribution, and therefore would have good statistical properties. In general, the underlying distributions can not be computed, but our experiments show that they usually have good statistical properties as required by MVC. 4.2 AdaBoost AdaBoost is an iterative boosting algorithm introduced by Freund and Schapire (1997) shown to be successful for several natural language classification tasks. AdaBoost successively builds classifiers based on a weak learner (base learning algorithm) by weighting differently the examples in the training space, and outputs the final classification by mixing the predictions of the iteratively built classifiers. Because sense disambiguation is a multi-class problem, we chose to use version AdaBoost.M2. We could not apply AdaBoost straightforwardly to the problem of sense disambiguation because of the high dimensionality and sparseness of the feature space. Superficial modeling of</context>
</contexts>
<marker>Freund, Schapire, 1997</marker>
<rawString>Y. Freund and R. E. Schapire. 1997. A decision-theoretic generalization of on-line learning and application to boosting. Journal of Computer and System Sciences, 55:119–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="7154" citStr="Gale et al., 1992" startWordPosition="1067" endWordPosition="1070">t) features ObjectTo presents VBZ present/V Modifier problem NN problem/N Bigram collocationalfeatures -1 Bigram another DT another/D +1 Bigram of IN of/I Bigram POS environment POS-2-1 - VBZ+DT - POS+1+2 - IN+DT - Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features 3 Mixture Models (MM) We investigate in this Section a direct statistical model that uses the same starting point as the algorithm presented in Walker (1987). We then compare the functionality and the performance of this model to those of the widely used Naïve Bayes model for the WSD task (Gale et al., 1992; Mooney, 1996; Pedersen, 1998), enhanced with the full richer feature space beyond the traditional unordered bag-ofwords. Algorithm 1 Naïve Bayes Model It is known that Bayes decision rule is optimal if the distribution of the data of each class is known (Duda and Hart, 1973, ch. 2). However, the classconditional distributions of the data are not known and have to be estimated. Both Naïve Bayes and the mixture model we investigated estimate starting from mathematically correct formulations, and thus would be equivalent if the assumptions they make were correct. Naïve Bayes makes the assumptio</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>Applying winnow to contextsensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Machine Learning: Proceedings of the 13th International Conference,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="2427" citStr="Golding and Roth, 1996" startWordPosition="344" endWordPosition="347"> focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997). Generally, both tasks involve the selection between a relatively small set of alternatives per keyword (e.g. sense id’s such as church/BUILDING and church/INSTITUTION or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntactic patterns to resolve between the set of alternatives. Thus both tasks can share a common feature space, data representation and algorithm infrastructure. We present a framework of doing so, while investigating the use of mixture models in conju</context>
<context position="4850" citStr="Golding and Roth, 1996" startWordPosition="726" endWordPosition="729">e/D Context other/JJ JJ other/J Target bar/NN NN bar/N Context with/IN IN with/I Context my/PRP$ PRP$ my/P Context pint/NN NN pint/N Syntactic (predicate-argument) features ObjectTo moved/VBD VBD move/V Modifier other/JJ JJ other/J Bigram collocational features -1 Bigram other/JJ JJ other/J +1 Bigram with/IN IN with/IN Figure 1: Example context for WSD SENSEVAL-2 target word bar (inventory of 21 senses) and extracted features tive approach in Section 4.1. The contexts are represented as a collection of features. Previous work in WSD and CSSC (Golding, 1995; Bruce et al., 1996; Yarowsky, 1996; Golding and Roth, 1996; Pedersen, 1998) has found diverse feature types to be useful, including inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic relationships, including local bigrams and trigrams, predicate-argument relationships, and wide-context bag-of-words associations. Examples of the feature types we employ are illustrated in Figures 1 and 2. The syntactic features are intended to capture the predicate-argument relationships in the syntactic window in which the target word occurs. Different relations are considered depending on the target word’s POS. For nouns, the</context>
<context position="22851" citStr="Golding and Roth (1996)" startWordPosition="3783" endWordPosition="3786">nventory. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 and Senseval-2 test data. The described models were initially trained and tested by performing 5-fold cross-validation on the SENSEVAL-2 English lexical-sample-task training data. When parameters needed to be estimated, jackknife or a 3-1-1 split (training and/or parameter estimation - testing) were used. 5.1 SENSEVAL-2 The English training set for SENSEVAL-2 is composed of 8861 instances representing 73 target words with an average number of 12.5 senses per word.</context>
<context position="26870" citStr="Golding and Roth, 1996" startWordPosition="4441" endWordPosition="4444">.57 75.9 78.79 AdaMixt 76.83 83.39 77.10 80.16 MMVC 78.49 84.79 76.81 81.06 Table 3: Results using 5-fold cross validation on SENSEVAL1 training data (English) 5.3 Spelling Correction Both MM and the enhanced Bayes model obtain virtually the same overall performance9 as the TriBayes system reported in (Golding and Schabes, 1996), which uses a similar feature space. The correction and boosting methods we investigated marginally improve the performance of the mixture model, as can be seen in Table 4 but they do not achieve the performance of RuleS 93.1% (Mangu and Brill, 1997) and Winnow 93.5% (Golding and Roth, 1996; Golding and Roth, 1999), methods that include features more directly specialized for spelling correction. Because of the small size of the test set, the differences in performance are due to only 14 and 20 more incorrectly classified examples respectively. More important than this difference10 may be the fact that the systems built for WSD were able to achieve competitive performance 9All figures reported are for the standard 14 confusion sets; the accuracies for the 18 sets are generally higher. 10We did not have the actual classifications from the other systems to check the significance of</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>A. R. Golding and D. Roth. 1996. Applying winnow to contextsensitive spelling correction. In Machine Learning: Proceedings of the 13th International Conference, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A winnow-based approach to context-sensitive spelling correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="26895" citStr="Golding and Roth, 1999" startWordPosition="4445" endWordPosition="4448">6.83 83.39 77.10 80.16 MMVC 78.49 84.79 76.81 81.06 Table 3: Results using 5-fold cross validation on SENSEVAL1 training data (English) 5.3 Spelling Correction Both MM and the enhanced Bayes model obtain virtually the same overall performance9 as the TriBayes system reported in (Golding and Schabes, 1996), which uses a similar feature space. The correction and boosting methods we investigated marginally improve the performance of the mixture model, as can be seen in Table 4 but they do not achieve the performance of RuleS 93.1% (Mangu and Brill, 1997) and Winnow 93.5% (Golding and Roth, 1996; Golding and Roth, 1999), methods that include features more directly specialized for spelling correction. Because of the small size of the test set, the differences in performance are due to only 14 and 20 more incorrectly classified examples respectively. More important than this difference10 may be the fact that the systems built for WSD were able to achieve competitive performance 9All figures reported are for the standard 14 confusion sets; the accuracies for the 18 sets are generally higher. 10We did not have the actual classifications from the other systems to check the significance of the difference. with lit</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A winnow-based approach to context-sensitive spelling correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>Y Schabes</author>
</authors>
<title>Combining trigram-based and feature-based methods for context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL-1996,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="11554" citStr="Golding and Schabes (1996)" startWordPosition="1794" endWordPosition="1797">ith most of the senses of a word, providing no relevant information about the sense itself, are penalized. Improvements obtained using weighted frequencies and filtering over MLE are shown in Table 1. Bayes Mixture MLE bag-of-words only 55.55 56.31 MLE with syntactic features 61.62 62.27 + Weighting + Filtering 63.28 63.06 + Collocational Senses5 65.70 65.41 Table 1: The increase in performance for successive variants of Bayes and Mixture Model as evaluated by 5-fold cross validation on SENSEVAL-2 English data can be seen as weighting factors in the mixture model formula (4). When is a word, 2Golding and Schabes (1996) show that the most important words for CSSC are contained within a window of . 3The results shown were obtained for with term weights doubled within a context window. Various other functions and parameters values were tried on held-out parameter-optimization data for SENSEVAL-2. 4A normalization step is required to output probability distributions. 5The collocational sense information is specific to the SENSEVAL-2 English task and relies on the given inventory of collocation sense labels (e.g. art_gallery%1:06:00::). . If are set expresses the positional relationship between the occurrences o</context>
<context position="26578" citStr="Golding and Schabes, 1996" startWordPosition="4392" endWordPosition="4395">new test set. Comparative performance is shown in Table 3. The difference between MMVC and enhanced Naïve Bayes is statistically significant (McNemar rejection risk 0.036). Adjectives Nouns Verbs Overall Most Likely 63.43 66.52 57.6 63.09 Naïve Bayes (FE) 75.67 84.15 76.65 80.16 Mixture 76.45 81.57 75.9 78.79 AdaMixt 76.83 83.39 77.10 80.16 MMVC 78.49 84.79 76.81 81.06 Table 3: Results using 5-fold cross validation on SENSEVAL1 training data (English) 5.3 Spelling Correction Both MM and the enhanced Bayes model obtain virtually the same overall performance9 as the TriBayes system reported in (Golding and Schabes, 1996), which uses a similar feature space. The correction and boosting methods we investigated marginally improve the performance of the mixture model, as can be seen in Table 4 but they do not achieve the performance of RuleS 93.1% (Mangu and Brill, 1997) and Winnow 93.5% (Golding and Roth, 1996; Golding and Roth, 1999), methods that include features more directly specialized for spelling correction. Because of the small size of the test set, the differences in performance are due to only 14 and 20 more incorrectly classified examples respectively. More important than this difference10 may be the </context>
</contexts>
<marker>Golding, Schabes, 1996</marker>
<rawString>A. R. Golding and Y. Schabes. 1996. Combining trigram-based and feature-based methods for context-sensitive spelling correction. In Proceedings ofACL-1996, pages 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
</authors>
<title>A Bayesian hybrid method for contextsensitive spelling correction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>39--53</pages>
<contexts>
<context position="2394" citStr="Golding, 1995" startWordPosition="340" endWordPosition="342">oy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997). Generally, both tasks involve the selection between a relatively small set of alternatives per keyword (e.g. sense id’s such as church/BUILDING and church/INSTITUTION or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntactic patterns to resolve between the set of alternatives. Thus both tasks can share a common feature space, data representation and algorithm infrastructure. We present a framework of doing so, while investigating t</context>
<context position="4790" citStr="Golding, 1995" startWordPosition="718" endWordPosition="719">ve/V Context into/IN IN into/I Context the/DT DT the/D Context other/JJ JJ other/J Target bar/NN NN bar/N Context with/IN IN with/I Context my/PRP$ PRP$ my/P Context pint/NN NN pint/N Syntactic (predicate-argument) features ObjectTo moved/VBD VBD move/V Modifier other/JJ JJ other/J Bigram collocational features -1 Bigram other/JJ JJ other/J +1 Bigram with/IN IN with/IN Figure 1: Example context for WSD SENSEVAL-2 target word bar (inventory of 21 senses) and extracted features tive approach in Section 4.1. The contexts are represented as a collection of features. Previous work in WSD and CSSC (Golding, 1995; Bruce et al., 1996; Yarowsky, 1996; Golding and Roth, 1996; Pedersen, 1998) has found diverse feature types to be useful, including inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic relationships, including local bigrams and trigrams, predicate-argument relationships, and wide-context bag-of-words associations. Examples of the feature types we employ are illustrated in Figures 1 and 2. The syntactic features are intended to capture the predicate-argument relationships in the syntactic window in which the target word occurs. Different relations are c</context>
<context position="22826" citStr="Golding (1995)" startWordPosition="3781" endWordPosition="3782">-grained sense inventory. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 and Senseval-2 test data. The described models were initially trained and tested by performing 5-fold cross-validation on the SENSEVAL-2 English lexical-sample-task training data. When parameters needed to be estimated, jackknife or a 3-1-1 split (training and/or parameter estimation - testing) were used. 5.1 SENSEVAL-2 The English training set for SENSEVAL-2 is composed of 8861 instances representing 73 target words with an average number</context>
</contexts>
<marker>Golding, 1995</marker>
<rawString>A. R. Golding. 1995. A Bayesian hybrid method for contextsensitive spelling correction. In Proceedings of the Third Workshop on Very Large Corpora, pages 39–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Kelly</author>
<author>P J Stone</author>
</authors>
<title>Computer Recognition of English Word Senses.</title>
<date>1975</date>
<publisher>North Holland Press.</publisher>
<contexts>
<context position="1502" citStr="Kelly and Stone, 1975" startWordPosition="207" endWordPosition="210">inal correction technique can improve the results of the raw model significantly, achieving stateof-the-art performance on several standard test sets in four languages. With substantially different output to Naïve Bayes and other statistical methods, the investigated models are also shown to be effective participants in classifier combination. 1 Introduction The focus tasks of this paper are two related problems in lexical ambiguity resolution: Word Sense Disambiguation (WSD) and ContextSensitive Spelling Correction (CSSC). Word Sense Disambiguation has a long history as a computational task (Kelly and Stone, 1975), and the field has recently supported large-scale international system evaluation exercises in multiple languages (SENSEVAL-1, Kilgarriff and Palmer (2000), and SENSEVAL-2, Edmonds and Cotton (2001)). General purpose Spelling Correction is also a long-standing task (e.g. McIlroy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context</context>
</contexts>
<marker>Kelly, Stone, 1975</marker>
<rawString>E. F. Kelly and P. J. Stone. 1975. Computer Recognition of English Word Senses. North Holland Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>M Palmer</author>
</authors>
<title>Introduction to the special issue on</title>
<date>2000</date>
<booktitle>SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1658" citStr="Kilgarriff and Palmer (2000)" startWordPosition="228" endWordPosition="231">in four languages. With substantially different output to Naïve Bayes and other statistical methods, the investigated models are also shown to be effective participants in classifier combination. 1 Introduction The focus tasks of this paper are two related problems in lexical ambiguity resolution: Word Sense Disambiguation (WSD) and ContextSensitive Spelling Correction (CSSC). Word Sense Disambiguation has a long history as a computational task (Kelly and Stone, 1975), and the field has recently supported large-scale international system evaluation exercises in multiple languages (SENSEVAL-1, Kilgarriff and Palmer (2000), and SENSEVAL-2, Edmonds and Cotton (2001)). General purpose Spelling Correction is also a long-standing task (e.g. McIlroy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>A. Kilgarriff and M. Palmer. 2000. Introduction to the special issue on SENSEVAL. Computers and the Humanities, 34(1-2):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2007" citStr="Kukich (1992)" startWordPosition="281" endWordPosition="283">rection (CSSC). Word Sense Disambiguation has a long history as a computational task (Kelly and Stone, 1975), and the field has recently supported large-scale international system evaluation exercises in multiple languages (SENSEVAL-1, Kilgarriff and Palmer (2000), and SENSEVAL-2, Edmonds and Cotton (2001)). General purpose Spelling Correction is also a long-standing task (e.g. McIlroy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997). Generally, both tasks involve the selection between a relatively small set of alternatives per keyword (e.g. sense id’s </context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>K. Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys, 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Mangu</author>
<author>E Brill</author>
</authors>
<title>Automatic rule acquisition for spelling correction.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th International Conference on Machine Learning,</booktitle>
<pages>734--741</pages>
<contexts>
<context position="2485" citStr="Mangu and Brill, 1997" startWordPosition="351" endWordPosition="354">sition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, 1995), Winnow (Golding and Roth, 1996) and Transformation-Based Learning (Mangu and Brill, 1997). Generally, both tasks involve the selection between a relatively small set of alternatives per keyword (e.g. sense id’s such as church/BUILDING and church/INSTITUTION or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntactic patterns to resolve between the set of alternatives. Thus both tasks can share a common feature space, data representation and algorithm infrastructure. We present a framework of doing so, while investigating the use of mixture models in conjunction with a new error-correction technique as competitiv</context>
<context position="22878" citStr="Mangu and Brill (1997)" startWordPosition="3788" endWordPosition="3791">edish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). Finally, we present the results obtained by the investigated methods on a single run on the Senseval-1 and Senseval-2 test data. The described models were initially trained and tested by performing 5-fold cross-validation on the SENSEVAL-2 English lexical-sample-task training data. When parameters needed to be estimated, jackknife or a 3-1-1 split (training and/or parameter estimation - testing) were used. 5.1 SENSEVAL-2 The English training set for SENSEVAL-2 is composed of 8861 instances representing 73 target words with an average number of 12.5 senses per word. Table 2 illustrates the pe</context>
<context position="26829" citStr="Mangu and Brill, 1997" startWordPosition="4434" endWordPosition="4437"> 75.67 84.15 76.65 80.16 Mixture 76.45 81.57 75.9 78.79 AdaMixt 76.83 83.39 77.10 80.16 MMVC 78.49 84.79 76.81 81.06 Table 3: Results using 5-fold cross validation on SENSEVAL1 training data (English) 5.3 Spelling Correction Both MM and the enhanced Bayes model obtain virtually the same overall performance9 as the TriBayes system reported in (Golding and Schabes, 1996), which uses a similar feature space. The correction and boosting methods we investigated marginally improve the performance of the mixture model, as can be seen in Table 4 but they do not achieve the performance of RuleS 93.1% (Mangu and Brill, 1997) and Winnow 93.5% (Golding and Roth, 1996; Golding and Roth, 1999), methods that include features more directly specialized for spelling correction. Because of the small size of the test set, the differences in performance are due to only 14 and 20 more incorrectly classified examples respectively. More important than this difference10 may be the fact that the systems built for WSD were able to achieve competitive performance 9All figures reported are for the standard 14 confusion sets; the accuracies for the 18 sets are generally higher. 10We did not have the actual classifications from the o</context>
</contexts>
<marker>Mangu, Brill, 1997</marker>
<rawString>L. Mangu and E. Brill. 1997. Automatic rule acquisition for spelling correction. In Proceedings of the 14th International Conference on Machine Learning, pages 734–741.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9991" citStr="Manning and Schütze, 1999" startWordPosition="1530" endWordPosition="1533">tension to this classical estimation method is to use distance-weighted counts instead of raw counts for the relative frequencies: denotes the training contexts of word and the subset of corresponding to sense. When is a syntactic headword, is computed by raw count. When is a context word, is computed as a function of the positionof the target word inand the positions where occurs in: toregardless of the distance then MLE estimates are obtained. There are various other ways of choosing the weighting measure. One natural way is to transform the distance into a closeness measure by considering (Manning and Schütze, 1999, ch. 14.1). This measure proves to be effective for the spelling correction task, where the words in the immediate vicinity are far more important than the rest of the context words2, but imposes counterproductive differences between the much wider context positions (such as +30 vs. +31) used in WSD, especially when considering large context windows. Experimental results indicate that it is more effective to level out the local positional differences given by a continuous weighting, by instead using weight-equivalent regions which can be described with a simple stepfunction , ( is a constant3</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>C.D. Manning and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D McIlroy</author>
</authors>
<title>Development of a spelling list. j-IEEETRANS-COMM,</title>
<date>1982</date>
<pages>30--1</pages>
<contexts>
<context position="1789" citStr="McIlroy, 1982" startWordPosition="248" endWordPosition="249"> effective participants in classifier combination. 1 Introduction The focus tasks of this paper are two related problems in lexical ambiguity resolution: Word Sense Disambiguation (WSD) and ContextSensitive Spelling Correction (CSSC). Word Sense Disambiguation has a long history as a computational task (Kelly and Stone, 1975), and the field has recently supported large-scale international system evaluation exercises in multiple languages (SENSEVAL-1, Kilgarriff and Palmer (2000), and SENSEVAL-2, Edmonds and Cotton (2001)). General purpose Spelling Correction is also a long-standing task (e.g. McIlroy, 1982), traditionally focusing on resolving typographical errors such as transposition and deletion to find the closest “valid” word (in a dictionary or a morphological variant), typically ignoring context. Yet Kukich (1992) observed that about 25-50% of the spelling errors found in modern documents are either context-inappropriate misuses or substitutions of valid words (such as principal and principle) which are not detected by traditional spelling correctors. Previous work has addressed the problem of CSSC from a machine learning perspective, including Bayesian and Decision List models (Golding, </context>
</contexts>
<marker>McIlroy, 1982</marker>
<rawString>M. D. McIlroy. 1982. Development of a spelling list. j-IEEETRANS-COMM, COM-30(1):91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP-1996,</booktitle>
<pages>82--91</pages>
<contexts>
<context position="7168" citStr="Mooney, 1996" startWordPosition="1071" endWordPosition="1072">o presents VBZ present/V Modifier problem NN problem/N Bigram collocationalfeatures -1 Bigram another DT another/D +1 Bigram of IN of/I Bigram POS environment POS-2-1 - VBZ+DT - POS+1+2 - IN+DT - Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features 3 Mixture Models (MM) We investigate in this Section a direct statistical model that uses the same starting point as the algorithm presented in Walker (1987). We then compare the functionality and the performance of this model to those of the widely used Naïve Bayes model for the WSD task (Gale et al., 1992; Mooney, 1996; Pedersen, 1998), enhanced with the full richer feature space beyond the traditional unordered bag-ofwords. Algorithm 1 Naïve Bayes Model It is known that Bayes decision rule is optimal if the distribution of the data of each class is known (Duda and Hart, 1973, ch. 2). However, the classconditional distributions of the data are not known and have to be estimated. Both Naïve Bayes and the mixture model we investigated estimate starting from mathematically correct formulations, and thus would be equivalent if the assumptions they make were correct. Naïve Bayes makes the assumption (used to tra</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. J. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of EMNLP-1996, pages 82–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>R Florian</author>
</authors>
<title>Transformation-based learning in the fast lane.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL-2001,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="22363" citStr="Ngai and Florian, 2001" startWordPosition="3705" endWordPosition="3708"> are updated at feature level rather than context level. This modified Adaboost algorithm could only be implemented for the mixture model, which “perceives” the contexts as additive mixture of features. The Adaboost-enhanced mixture model is called AdaMixt henceforth. 5 Evaluation We present a comparative study for four languages (English, Swedish, Spanish, and Basque) by performing 5-fold cross-validation on the SENSEVAL-2 lexical-sample training data, using the fine-grained sense inventory. For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. We also present the results obtained by the different algorithms on another WSD standard set, SENSEVAL-1, also by performing 5-fold cross validation on the original training data. For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995), Golding and Roth (1996) and Mangu and Brill (1997). Finally, we present the results obtained by the investigated methods on a single ru</context>
</contexts>
<marker>Ngai, Florian, 2001</marker>
<rawString>G. Ngai and R. Florian. 2001. Transformation-based learning in the fast lane. In Proceedings ofNAACL-2001, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Naïve Bayes as a satisficing model.</title>
<date>1998</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on Satisficing Models,</booktitle>
<pages>60--67</pages>
<contexts>
<context position="4867" citStr="Pedersen, 1998" startWordPosition="730" endWordPosition="731">other/J Target bar/NN NN bar/N Context with/IN IN with/I Context my/PRP$ PRP$ my/P Context pint/NN NN pint/N Syntactic (predicate-argument) features ObjectTo moved/VBD VBD move/V Modifier other/JJ JJ other/J Bigram collocational features -1 Bigram other/JJ JJ other/J +1 Bigram with/IN IN with/IN Figure 1: Example context for WSD SENSEVAL-2 target word bar (inventory of 21 senses) and extracted features tive approach in Section 4.1. The contexts are represented as a collection of features. Previous work in WSD and CSSC (Golding, 1995; Bruce et al., 1996; Yarowsky, 1996; Golding and Roth, 1996; Pedersen, 1998) has found diverse feature types to be useful, including inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic relationships, including local bigrams and trigrams, predicate-argument relationships, and wide-context bag-of-words associations. Examples of the feature types we employ are illustrated in Figures 1 and 2. The syntactic features are intended to capture the predicate-argument relationships in the syntactic window in which the target word occurs. Different relations are considered depending on the target word’s POS. For nouns, these relations are:</context>
<context position="7185" citStr="Pedersen, 1998" startWordPosition="1073" endWordPosition="1074"> present/V Modifier problem NN problem/N Bigram collocationalfeatures -1 Bigram another DT another/D +1 Bigram of IN of/I Bigram POS environment POS-2-1 - VBZ+DT - POS+1+2 - IN+DT - Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features 3 Mixture Models (MM) We investigate in this Section a direct statistical model that uses the same starting point as the algorithm presented in Walker (1987). We then compare the functionality and the performance of this model to those of the widely used Naïve Bayes model for the WSD task (Gale et al., 1992; Mooney, 1996; Pedersen, 1998), enhanced with the full richer feature space beyond the traditional unordered bag-ofwords. Algorithm 1 Naïve Bayes Model It is known that Bayes decision rule is optimal if the distribution of the data of each class is known (Duda and Hart, 1973, ch. 2). However, the classconditional distributions of the data are not known and have to be estimated. Both Naïve Bayes and the mixture model we investigated estimate starting from mathematically correct formulations, and thus would be equivalent if the assumptions they make were correct. Naïve Bayes makes the assumption (used to transform Equation (</context>
</contexts>
<marker>Pedersen, 1998</marker>
<rawString>T. Pedersen. 1998. Naïve Bayes as a satisficing model. In Working Notes of the AAAI Spring Symposium on Satisficing Models, pages 60–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
</authors>
<title>Learning to resolve natural language ambiguities: a unified approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th Conference of the AAAI,</booktitle>
<pages>806--813</pages>
<contexts>
<context position="3242" citStr="Roth, 1998" startWordPosition="469" endWordPosition="470">and church/INSTITUTION or commonly confused spellings such as quiet and quite), and are dependent on local and long-distance collocational and syntactic patterns to resolve between the set of alternatives. Thus both tasks can share a common feature space, data representation and algorithm infrastructure. We present a framework of doing so, while investigating the use of mixture models in conjunction with a new error-correction technique as competitive alternatives to Bayesian models. While several authors have observed the fundamental similarities between CSSC and WSD (e.g. Berleant, 1995 and Roth, 1998), to our knowledge no previous comparative empirical study has tackled these two problems in a single unified framework. 2 Problem Formulation. Feature Space The problem of lexical disambiguation can be modeled as a classification task, in which each instance of the word to be disambiguated (target word, henceforth), identified by its context, has to be labeled with one of the established sense labels .1 The approaches we investigate are statistical methods , outputting conditional probability distributions over the sense set given a context . The classification of a context is generally made </context>
<context position="16539" citStr="Roth (1998)" startWordPosition="2592" endWordPosition="2593">e face again the problem of under-representation in the training data: the expected values in the posterior distributions for the under-represented senses when they express the correct classification can not be accurately estimated. Therefore, we have to look at the problem from another angle. 6For example, assuming that every context contains approximately the same number of such words, then given two senses, one represented in the training set by 20 examples, and the other one by 4, it is five times more likely that a spurious word in a test context co-occurs with the larger sampled sense. 7Roth (1998) shows that Bayes, TBL and Decision Lists also search for a decision surface which is a linear function in the feature space 3 8 2 1 .3 .5 .5 1 - - - 2 - - 2 . . . . . . . . . . . . . . . . . . 0.44 0.41 . . . . . . . . . . . . Ts Ts . . . . . . . . . . . . 0.31 0.24 Senses: c (art) 1 c (art) 2 c (art) 3 Training contexts c (art) 4 c (art) 5 c (art) 6 Test context c (art) 157 Variational Coefficients cs,c P(s|c) s1 . . . s n . . . sk−1 sk s . . . s n 1 . . . sk−1 sk c (art) −0.6 . . . +1.6 . . . . . . . . . 1 −0.4 . . . +1.2 . . . . . . . . . c (art) +1.2 . . . −0.8 . . . +2.3 . . . 2 +2.9 . .</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>D. Roth. 1998. Learning to resolve natural language ambiguities: a unified approach. In Proceedings of the 15th Conference of the AAAI, pages 806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Walker</author>
</authors>
<title>Knowledge resource tools for accessing large text files.</title>
<date>1987</date>
<booktitle>Machine Translation: Theoretical and Methodogical Issues,</booktitle>
<pages>247--261</pages>
<editor>In Sergei Nirenburg, editor,</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7003" citStr="Walker (1987)" startWordPosition="1041" endWordPosition="1042">t another DT another/D Target {peace,piece} NN /N Context of IN of/I Context the DT the/D Context problem NN problem/N Syntactic (predicate-argument) features ObjectTo presents VBZ present/V Modifier problem NN problem/N Bigram collocationalfeatures -1 Bigram another DT another/D +1 Bigram of IN of/I Bigram POS environment POS-2-1 - VBZ+DT - POS+1+2 - IN+DT - Figure 2: Example context for the spelling confusion set {piece,peace} and extracted features 3 Mixture Models (MM) We investigate in this Section a direct statistical model that uses the same starting point as the algorithm presented in Walker (1987). We then compare the functionality and the performance of this model to those of the widely used Naïve Bayes model for the WSD task (Gale et al., 1992; Mooney, 1996; Pedersen, 1998), enhanced with the full richer feature space beyond the traditional unordered bag-ofwords. Algorithm 1 Naïve Bayes Model It is known that Bayes decision rule is optimal if the distribution of the data of each class is known (Duda and Hart, 1973, ch. 2). However, the classconditional distributions of the data are not known and have to be estimated. Both Naïve Bayes and the mixture model we investigated estimate sta</context>
</contexts>
<marker>Walker, 1987</marker>
<rawString>D. E. Walker. 1987. Knowledge resource tools for accessing large text files. In Sergei Nirenburg, editor, Machine Translation: Theoretical and Methodogical Issues, pages 247–261. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Homograph disambiguation in speech synthesis.</title>
<date>1996</date>
<booktitle>Progress in Speech Synthesis,</booktitle>
<pages>159--175</pages>
<editor>In J. Olive J. van Santen, R. Sproat and J. Hirschberg, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="4826" citStr="Yarowsky, 1996" startWordPosition="724" endWordPosition="725">ext the/DT DT the/D Context other/JJ JJ other/J Target bar/NN NN bar/N Context with/IN IN with/I Context my/PRP$ PRP$ my/P Context pint/NN NN pint/N Syntactic (predicate-argument) features ObjectTo moved/VBD VBD move/V Modifier other/JJ JJ other/J Bigram collocational features -1 Bigram other/JJ JJ other/J +1 Bigram with/IN IN with/IN Figure 1: Example context for WSD SENSEVAL-2 target word bar (inventory of 21 senses) and extracted features tive approach in Section 4.1. The contexts are represented as a collection of features. Previous work in WSD and CSSC (Golding, 1995; Bruce et al., 1996; Yarowsky, 1996; Golding and Roth, 1996; Pedersen, 1998) has found diverse feature types to be useful, including inflected words, lemmas and part-of-speech (POS) in a variety of collocational and syntactic relationships, including local bigrams and trigrams, predicate-argument relationships, and wide-context bag-of-words associations. Examples of the feature types we employ are illustrated in Figures 1 and 2. The syntactic features are intended to capture the predicate-argument relationships in the syntactic window in which the target word occurs. Different relations are considered depending on the target wo</context>
</contexts>
<marker>Yarowsky, 1996</marker>
<rawString>D. Yarowsky. 1996. Homograph disambiguation in speech synthesis. In J. Olive J. van Santen, R. Sproat and J. Hirschberg, editors, Progress in Speech Synthesis, pages 159–175. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>