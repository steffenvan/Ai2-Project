<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003857">
<title confidence="0.957011">
SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation
within an End-User Application
</title>
<author confidence="0.873141">
Roberto Navigli and Daniele Vannella
</author>
<affiliation confidence="0.760108">
Dipartimento di Informatica
</affiliation>
<address confidence="0.8207005">
Sapienza Universit`a di Roma
Viale Regina Elena, 295 – 00161 Roma Italy
</address>
<email confidence="0.9972">
{navigli,vannella}@di.uniroma1.it
</email>
<sectionHeader confidence="0.996788" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9929591">
In this paper we describe our Semeval-2013
task on Word Sense Induction and Dis-
ambiguation within an end-user application,
namely Web search result clustering and diver-
sification. Given a target query, induction and
disambiguation systems are requested to clus-
ter and diversify the search results returned by
a search engine for that query. The task en-
ables the end-to-end evaluation and compari-
son of systems.
</bodyText>
<sectionHeader confidence="0.998418" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941425925926">
Word ambiguity is a pervasive issue in Natural Lan-
guage Processing. Two main techniques in compu-
tational lexical semantics, i.e., Word Sense Disam-
biguation (WSD) and Word Sense Induction (WSI)
address this issue from different perspectives: the
former is aimed at assigning word senses from a pre-
defined sense inventory to words in context, whereas
the latter automatically identifies the meanings of a
word of interest by clustering the contexts in which
it occurs (see (Navigli, 2009; Navigli, 2012) for a
survey).
Unfortunately, the paradigms of both WSD and
WSI suffer from significant issues which hamper
their success in real-world applications. In fact, the
performance of WSD systems depends heavily on
which sense inventory is chosen. For instance, the
most popular computational lexicon of English, i.e.,
WordNet (Fellbaum, 1998), provides fine-grained
distinctions which make the disambiguation task
quite difficult even for humans (Edmonds and Kil-
garriff, 2002; Snyder and Palmer, 2004), although
disagreements can be solved to some extent with
graph-based methods (Navigli, 2008). On the other
hand, although WSI overcomes this issue by allow-
ing unrestrained sets of senses, its evaluation is par-
ticularly arduous because there is no easy way of
comparing and ranking different representations of
senses. In fact, all the proposed measures in the lit-
erature tend to favour specific cluster shapes (e.g.,
singletons or all-in-one clusters) of the senses pro-
duced as output. Indeed, WSI evaluation is actually
an instance of the more general and difficult problem
of evaluating clustering algorithms.
Nonetheless, many everyday tasks carried out by
online users would benefit from intelligent systems
able to address the lexical ambiguity issue effec-
tively. A case in point is Web information retrieval, a
task which is becoming increasingly difficult given
the continuously growing pool of Web text of the
most wildly disparate kinds. Recent work has ad-
dressed this issue by proposing a general evaluation
framework for injecting WSI into Web search result
clustering and diversification (Navigli and Crisa-
fulli, 2010; Di Marco and Navigli, 2013). In this
task the search results returned by a search engine
for an input query are grouped into clusters, and di-
versified by providing a reranking which maximizes
the meaning heterogeneity of the top ranking results.
The Semeval-2013 task described in this paper1
adopts the evaluation framework of Di Marco and
Navigli (2013), and extends it to both WSD and WSI
systems. The task is aimed at overcoming the well-
known limitations of in vitro evaluations, such as
those of previous SemEval tasks on the topic (Agirre
</bodyText>
<footnote confidence="0.98428">
1http://www.cs.york.ac.uk/semeval-2013/task11/
</footnote>
<page confidence="0.976812">
193
</page>
<bodyText confidence="0.9194475">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 193–201, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
and Soroa, 2007; Manandhar et al., 2010), and en-
abling a fair comparison between the two disam-
biguation paradigms. Key to our framework is the
assumption that search results grouped into a given
cluster are semantically related to each other and
that each cluster is expected to represent a specific
meaning of the input query (even though it is possi-
ble for more than one cluster to represent the same
meaning). For instance, consider the target query
apple and the following 3 search result snippets:
</bodyText>
<listItem confidence="0.997887">
1. Apple Inc., formerly Apple Computer, Inc., is...
2. The science of apple growing is called pomology...
3. Apple designs and creates iPod and iTunes...
</listItem>
<bodyText confidence="0.9998425">
Participating systems were requested to produce a
clustering that groups snippets conveying the same
meaning of the input query apple, i.e., ideally {1, 3}
and {2} in the above example.
</bodyText>
<sectionHeader confidence="0.978996" genericHeader="method">
2 Task setup
</sectionHeader>
<bodyText confidence="0.999670454545454">
For each ambiguous query the task required partic-
ipating systems to cluster the top ranking snippets
returned by a search engine (we used the Google
Search API). WSI systems were required to iden-
tify the meanings of the input query and cluster the
snippets into semantically-related groups according
to their meanings. Instead, WSD systems were re-
quested to sense-tag the given snippets with the ap-
propriate senses of the input query, thereby implic-
itly determining a clustering of snippets (i.e., one
cluster per sense).
</bodyText>
<subsectionHeader confidence="0.972495">
2.1 Dataset
</subsectionHeader>
<bodyText confidence="0.99907575">
We created a dataset of 100 ambiguous queries.
The queries were randomly sampled from the AOL
search logs so as to ensure that they had been used in
real search sessions. Following previous work on the
topic (Bernardini et al., 2009; Di Marco and Navigli,
2013) we selected those queries for which a sense
inventory exists as a disambiguation page in the En-
glish Wikipedia2. This guaranteed that the selected
queries consisted of either a single word or a multi-
word expression for which we had a collaboratively-
edited list of meanings, including lexicographic and
encyclopedic ones. We discarded all queries made
</bodyText>
<footnote confidence="0.990558">
2http://en.wikipedia.org/wiki/Disambiguation page
</footnote>
<figureCaption confidence="0.9680365">
Figure 1: An example of search result for the apple query,
including: page title, URL and snippet.
</figureCaption>
<table confidence="0.982714333333333">
query length 1 2 3 4
AOL logs 45.89 40.98 10.98 2.32
our dataset 40.00 40.00 15.00 5.00
</table>
<tableCaption confidence="0.670907">
Table 1: Percentage distribution of AOL query lengths
(first row) vs. the queries sampled for our task (second
row).
</tableCaption>
<bodyText confidence="0.997474269230769">
up of &gt; 4 words, since the length of the great ma-
jority of queries lay in the range [1, 4]. In Table
1 we compare the percentage distribution of 1- to
4-word queries in the AOL query logs against our
dataset of queries. Note that we increased the per-
centage of 3- and 4-word queries in order to have
a significant coverage of those lengths. Anyhow,
in both cases most queries contained from 1 to 2
words. Note that the reported percentage distribu-
tions of query length is different from recent statis-
tics for two reasons: first, over the years users have
increased the average number of words per query in
order to refine their searches; second, we selected
only queries which were either single words (e.g.,
apple) or multi-word expressions (e.g., mortal kom-
bat), thereby discarding several long queries com-
posed of different words (such as angelina jolie ac-
tress).
Finally, we submitted each query to Google
search and retrieved the 64 top-ranking results re-
turned for each query. Therefore, overall the dataset
consists of 100 queries and 6,400 results. Each
search result includes the following information:
page title, URL of the page and snippet of the page
text. We show an example of search result for the
apple query in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998253">
2.2 Dataset Annotation
</subsectionHeader>
<bodyText confidence="0.9818835">
For each query q we used Amazon Mechani-
cal Turk3 to annotate each query result with the
</bodyText>
<footnote confidence="0.991547">
3https://www.mturk.com
</footnote>
<page confidence="0.998468">
194
</page>
<bodyText confidence="0.99954984375">
most suitable sense. The sense inventory for q
was obtained by listing the senses available in the
Wikipedia disambiguation page of q augmented
with additional options from the classes obtained
from the section headings of the disambiguation
page plus the OTHER catch-all meaning. For in-
stance, consider the apple query. We show its disam-
biguation page in Figure 2. The sense inventory for
apple was made up of the senses listed in that page
(e.g., MALUS, APPLE INC., APPLE BANK, etc.)
plus the set of generic classes OTHER PLANTS AND
PLANT PARTS, OTHER COMPANIES, OTHER FILMS,
plus OTHER.
For each query we ensured that three annotators
tagged each of the 64 results for that query with
the most suitable sense among those in the sense
inventory (selecting OTHER if no sense was appro-
priate). Specifically, each Turker was provided with
the following instructions: “The goal is annotating
the search result snippets returned by Google for a
given query with the appropriate meaning among
those available (obtained from the Wikipedia disam-
biguation page for the query). You have to select
the meaning that you consider most appropriate”.
No constraint on the age, gender and citizenship of
the annotators was imposed. However, in order to
avoid random tagging of search results, we provided
3 gold-standard result annotations per query, which
could be shown to the Turker more than once during
the annotation process. In the case (s)he failed to
annotate the gold items, the annotator was automat-
ically excluded.
</bodyText>
<subsectionHeader confidence="0.9750495">
2.3 Inter-Annotator Agreement and
Adjudication
</subsectionHeader>
<bodyText confidence="0.99983525">
In order to determine the reliability of the Turkers’
annotations, we calculated the individual values of
Fleiss’ kappa N (Fleiss, 1971) for each query q and
then averaged them:
</bodyText>
<equation confidence="0.987202">
EqEQ Nq
N =|Q |� (1)
</equation>
<bodyText confidence="0.999799">
where Nq is the Fleiss’ kappa agreement of the three
annotators who tagged the 64 snippets returned by
the Google search engine for the query q E Q, and
Q is our set of 100 queries. We obtained an average
value of N = 0.66, which according to Landis and
</bodyText>
<figureCaption confidence="0.99908">
Figure 2: The Wikipedia disambiguation page of Apple.
</figureCaption>
<bodyText confidence="0.999845045454546">
Koch (1977) can be seen as substantial agreement,
with a standard deviation Q = 0.185.
In Table 2 we show the agreement distribution
of our 6400 snippets, distinguishing between full
agreement (3 out of 3), majority agreement (2 out of
3), and no agreement. Most of the items were anno-
tated with full or majority agreement, indicating that
the manual annotation task was generally doable for
the layman. We manually checked all the cases of
majority agreement, correcting only 7.92% of the
majority adjudications, and manually adjudicated
all the snippets for which there was no agreement.
We observed during adjudication that in many cases
the disagreement was due to the existence of sub-
tle sense distinctions, like between MORTAL KOM-
BAT (VIDEO GAME) and MORTAL KOMBAT (2011
VIDEO GAME), or between THE DA VINCI CODE
and INACCURACIES IN THE DA VINCI CODE.
The average number of senses associated with
the search results of each query was 7.69
(higher than in previous datasets, such as AMBI-
ENT4+MORESQUE5, which associates 5.07 senses
</bodyText>
<footnote confidence="0.99991">
4http://credo.fub.it/ambient
5http://lcl.uniroma1.it/moresque
</footnote>
<page confidence="0.991426">
195
</page>
<table confidence="0.9736225">
Full agr. Majority Disagr.
% snippets 66.70 25.85 7.45
</table>
<tableCaption confidence="0.9626765">
Table 2: Percentage of snippets with full agreement, ma-
jority agreement and full disagreement.
</tableCaption>
<bodyText confidence="0.924598">
per query on average).
</bodyText>
<sectionHeader confidence="0.98913" genericHeader="method">
3 Scoring
</sectionHeader>
<bodyText confidence="0.998735230769231">
Following Di Marco and Navigli (2013), we eval-
uated the systems’ outputs in terms of the snippet
clustering quality (Section 3.1) and the snippet di-
versification quality (Section 3.2). Given a query
q ∈ Q and the corresponding set of 64 snippet re-
sults, let C be the clustering output by a given system
and let G be the gold-standard clustering for those
results. Each measure W, G) presented below is
calculated for the query q using these two cluster-
ings. The overall results on the entire set of queries
Q in the dataset is calculated by averaging the val-
ues of W, G) obtained for each single test query
q ∈ Q.
</bodyText>
<subsectionHeader confidence="0.999919">
3.1 Clustering Quality
</subsectionHeader>
<bodyText confidence="0.9991364">
The first evaluation concerned the quality of the
clusters produced by the participating systems.
Since clustering evaluation is a difficult issue, we
calculated four distinct measures available in the lit-
erature, namely:
</bodyText>
<listItem confidence="0.9998534">
• Rand Index (Rand, 1971);
• Adjusted Rand Index (Hubert and Arabie,
1985);
• Jaccard Index (Jaccard, 1901);
• F1 measure (van Rijsbergen, 1979).
</listItem>
<bodyText confidence="0.9979106">
The Rand Index (RI) of a clustering C is a mea-
sure of clustering agreement which determines the
percentage of correctly bucketed snippet pairs across
the two clusterings C and G. RI is calculated as fol-
lows:
</bodyText>
<table confidence="0.987726625">
������ C C1 C2 ··· C. Sums
G
G1 n11 n12 · · · n1m a1
G2 n21 n22 ·· · n2m a2
... ... ... .. .. ...
..
G9 ng1 ng2 ··· ngm ag
Sums b1 b2 ··· bm N
</table>
<tableCaption confidence="0.999932">
Table 3: Contingency table for the clusterings G and C.
</tableCaption>
<bodyText confidence="0.998072944444444">
G, TN is the number of true negatives, i.e., pairs
which are in different clusters in both clusterings,
and FP and FN are, respectively, the number of false
positives and false negatives. RI ranges between 0
and 1, where 1 indicates perfect correspondence.
Adjusted Rand Index (ARI) is a development of
Rand Index which corrects the RI for chance agree-
ment and makes it vary according to expectaction:
ARI(C, G) = maxRI(C, G) − E(RI(C, G))
where E(RI(C, G)) is the expected value of the RI.
Using the contingency table reported in Table 3 we
can quantify the degree of overlap between C and G,
where nij denotes the number of snippets in com-
mon between Gi and Cj (namely, nij = |Gi ∩ Cj|),
ai and bj represent, respectively, the number of snip-
pets in Gi and Cj, and N is the total number of snip-
pets, i.e., N = 64. Now, the above equation can be
reformulated as:
</bodyText>
<equation confidence="0.6758845">
2 [P
1i (ai2)+Pj (bj2 )]−[Pi (ai2)Pj (bj2 )]/(N2).
</equation>
<bodyText confidence="0.999120142857143">
The ARI ranges between −1 and +1 and is 0
when the index equals its expected value.
Jaccard Index (JI) is a measure which takes into
account only the snippet pairs which are in the same
cluster both in C and G, i.e., the true positives (TP),
while neglecting true negatives (TN), which are the
vast majority of cases. JI is calculated as follows:
</bodyText>
<equation confidence="0.992267777777778">
RI(C, G) − E(RI(C, G))
ARI(C,ff&apos;&apos;)_ Pij (n2j)−[Pi ( i2) Pj (b2 )]/( 2 )
TP + TN
RI(C, G) = , (2)
TP + FP + FN + TN
JI(C, G) =
� (5)
TP + FP + FN
TP
</equation>
<bodyText confidence="0.99975">
where TP is the number of true positives, i.e., snip-
pet pairs which are in the same cluster both in C and
Finally, the F1 measure calculates the harmonic
mean of precision (P) and recall (R). Precision de-
termines how accurately the clusters of C represent
</bodyText>
<page confidence="0.995749">
196
</page>
<bodyText confidence="0.9945374">
the query meanings in the gold standard G, whereas
recall measures how accurately the different mean-
ings in G are covered by the clusters in C. We follow
Crabtree et al. (2005) and define the precision of a
cluster Cj E C as follows:
</bodyText>
<equation confidence="0.993675">
|Cs j |
P(Cj) =|Cj|, (6)
</equation>
<bodyText confidence="0.999611">
where Csj is the intersection between Cj E C and the
gold cluster Gs E G which maximizes the cardinal-
ity of the intersection. The recall of a query sense s
is instead calculated as:
</bodyText>
<equation confidence="0.988954">
R(s) =  |UC;EC9 Cs j |
ns , (7)
</equation>
<bodyText confidence="0.998385">
where Cs is the subset of clusters of C whose ma-
jority sense is s, and ns is the number of snippets
tagged with query sense s in the gold standard. The
total precision and recall of the clustering C are then
calculated as:
</bodyText>
<equation confidence="0.995216">
P = EC;EC P(Cj)  |Cj  |; R = EsES R(s)ns
EC;EC |Cj  |EsES ns
</equation>
<bodyText confidence="0.9077152">
(8)
where S is the set of senses in the gold standard G
for the given query (i.e., |S |= |G|). The two values
of P and R are then combined into their harmonic
mean, namely the F1 measure:
</bodyText>
<equation confidence="0.89336">
2P R
F1(C, G) = P � R. (9)
</equation>
<subsectionHeader confidence="0.999174">
3.2 Clustering Diversity
</subsectionHeader>
<bodyText confidence="0.99885044">
Our second evaluation is aimed at determining the
impact of the output clustering on the diversifica-
tion of the top results shown to a Web user. To
this end, we applied an automatic procedure for flat-
tening the clusterings produced by the participating
systems to a list of search results. Given a clus-
tering C = (C1, C2,.. . , Cm), we add to the ini-
tially empty list the first element of each cluster Cj
(j = 1, ... , m); then we iterate the process by se-
lecting the second element of each cluster Cj such
that |Cj |&gt; 2, and so on. The remaining elements re-
turned by the search engine, but not included in any
cluster of C, are appended to the bottom of the list
in their original order. Note that systems were asked
to sort snippets within clusters, as well as clusters
themselves, by relevance.
Since our goal is to determine how many differ-
ent meanings are covered by the top-ranking search
results according to the output clustering, we used
the measures of S-recall@K (Subtopic recall at rank
K) and S-precision@r (Subtopic precision at recall
r) (Zhai et al., 2003).
S-recall@K determines the ratio of different
meanings for a given query q in the top-K results
returned:
</bodyText>
<equation confidence="0.956649666666667">
S-recall@K = |{sense(ri) : i E {1, ... , K}} |,
g
(10)
</equation>
<bodyText confidence="0.999858111111111">
where sense(ri) is the gold-standard sense associ-
ated with the i-th snippet returned by the system,
and g is the total number of distinct senses for the
query q in our gold standard.
S-precision@r instead determines the ratio of dif-
ferent senses retrieved for query q in the first Kr
snippets, where Kr is the minimum number of top
results for which the system achieves recall r. The
measure is defined as follows:
</bodyText>
<equation confidence="0.967951666666667">
 |UKr sense(r•)|
S-precision @ r = i=1 K Z (11)
r
</equation>
<subsectionHeader confidence="0.990909">
3.3 Baselines
</subsectionHeader>
<bodyText confidence="0.9846025">
We compared the participating systems with two
simple baselines:
</bodyText>
<listItem confidence="0.9981175">
• SINGLETONS: each snippet is clustered as a
separate singleton cluster (i.e., |C |= 64).
• ALL-IN-ONE: all snippets are clustered into a
single cluster (i.e., |C |= 1).
</listItem>
<bodyText confidence="0.996547">
These baselines are important in that they make
explicit the preference of certain quality measures
towards clusterings made up with a small or large
number of clusters.
</bodyText>
<sectionHeader confidence="0.998287" genericHeader="method">
4 Systems
</sectionHeader>
<bodyText confidence="0.999910142857143">
5 teams submitted 10 systems, out of which 9 were
WSI systems, while 1 was a WSD system, i.e., us-
ing the Wikipedia sense inventory for performing
the disambiguation task. All systems could exploit
the information provided for each search result, i.e.,
URL, page title and result snippet. WSI systems
were requested to use unannotated corpora only.
</bodyText>
<page confidence="0.994991">
197
</page>
<table confidence="0.999573454545455">
System URLs Snippets Wikipedia YAGO Hierarchy Distr. Thesaurus Other
HDP-CLUSTERS-LEMMA ✓ ✓
HDP-CLUSTERS-NOLEMMA ✓ ✓
DULUTH.SYS 1.PK2 ✓
DULUTH.SYS7.PK2 ✓
WSI DULUTH.SYS9.PK2 Gigaword
UKP-WSI-WP-LLR2 ✓ ✓ ✓ WaCky
UKP-WSI-WP-PMI ✓ ✓ ✓ WaCky
UKP-WSI-WACKY-LLR ✓ ✓ ✓ WaCky
SATTY-APPROACH1 ✓
WSD RAKESH ✓ DBPedia
</table>
<tableCaption confidence="0.999846">
Table 4: Resources used for WSI/WSD.
</tableCaption>
<bodyText confidence="0.999980631578948">
We asked each team to provide information about
their systems. In Table 4 we report the resources
used by each system. The HDP and UKP systems
use Wikipedia as raw text for sampling word counts;
DULUTH-SYS9-PK2 uses the first 10,000 paragraphs
of the Associated Press wire service data from the
English Gigaword Corpus (Graff, 2003, 1st edition),
whereas DULUTH-SYS1-PK2 and DULUTH-SYS7-
PK2 both use the snippets for inducing the query
senses. Finally, the UKP systems were the only ones
to retrieve the Web pages from the corresponding
URLs and exploit them for WSI purposes. They
also use WaCky (Baroni et al., 2009) and a distri-
butional thesaurus obtained from the Leipzig Cor-
pora Collection6 (Biemann et al., 2007). SATTY-
APPROACH1 just uses snippets.
The only participating WSD system, RAKESH,
uses the YAGO hierarchy (Suchanek et al., 2008) to-
gether with DBPedia abstracts (Bizer et al., 2009).
</bodyText>
<sectionHeader confidence="0.999921" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999962384615385">
We show the results of RI and ARI in Table 5. The
best performing systems are those from the HDP
team, with considerably higher RI and ARI. The
next best systems are SATTY-APPROACH1, which
uses only the words in the snippets, and the only
WSD system, i.e., RAKESH. SINGLETONS perform
well with RI, but badly when chance agreement is
taken into account.
As for F1 and JI, whose values are shown in Table
6, the two HDP systems again perform best in terms
of F1, and are on par with UKP-WSI-WACKY-LLR in
terms of JI. The third best approach in terms of F1
is again SATTY-APPROACH1, which however per-
</bodyText>
<footnote confidence="0.963169">
6http://corpora.uni-leipzig.de/
</footnote>
<table confidence="0.999481357142857">
System RI ARI
HDP-CLUSTERS-LEMMA 65.22 21.31
HDP-CLUSTERS-NOLEMMA 64.86 21.49
SATTY-APPROACH1 59.55 7.19
DULUTH.SYS9.PK2 54.63 2.59
DULUTH.SYS1.PK2 52.18 5.74
WSI DULUTH.SYS7.PK2 52.04 6.78
UKP-WSI-WP-LLR2 51.09 3.77
3.64
UKP-WSI-WP-PMI 50.50 2.53
UKP-WSI-WACKY-LLR 50.02
WSD RAKESH 58.76 8.11
L SINGLETONS 60.09 0.00
ALL-IN-ONE 39.90 0.00
</table>
<tableCaption confidence="0.991904">
Table 5: Results for Rand Index (RI) and Adjusted Rand
Index (ARI), sorted by RI.
</tableCaption>
<bodyText confidence="0.995255210526316">
forms badly in terms of JI. The SINGLETONS base-
line clearly obtains the best F1 performance, but the
worst JI results. The ALL-IN-ONE baseline outper-
forms all other systems with the JI measure, because
TN are not considered, which favours large clusters.
To get more insights into the performance of the
various systems, we calculated the average number
of clusters per clustering produced by each system
and compared it with the gold standard average. We
also computed the average cluster size, i.e., the aver-
age number of snippets per cluster. The statistics are
shown in Table 7. Interestingly, the best performing
systems are those with the cluster number and aver-
age number of clusters closest to the gold standard
ones. This finding is also confirmed by Figure 3,
where we draw each system according to its average
values regarding cluster number and size: again the
distance from the gold standard is meaningful.
We now move to the diversification perfor-
</bodyText>
<page confidence="0.993964">
198
</page>
<table confidence="0.999724923076923">
System JI F1
UKP-WSI-WACKY-LLR 33.94 58.26
HDP-CLUSTERS-NOLEMMA 33.75 68.03
HDP-CLUSTERS-LEMMA 33.02 68.30
DULUTH.SYS 1.PK2 31.79 56.83
SI UKP-WSI-WP-LLR2 31.77 58.64
DULUTH.SYS7.PK2 31.03 58.78
UKP-WSI-WP-PMI 29.32 60.48
DULUTH.SYS9.PK2 22.24 57.02
SATTY-APPROACH1 15.05 67.09
WSD RAKESH 30.52 39.49
L SINGLETONS 0.00 100.00
ALL-IN-ONE 39.90 54.42
</table>
<tableCaption confidence="0.781646">
Table 6: Results for Jaccard Index (JI) and F1 measure.
</tableCaption>
<table confidence="0.9999375">
System # cl. ACS
GOLD STANDARD 7.69 11.56
HDP-CLUSTERS-LEMMA 6.63 11.07
HDP-CLUSTERS-NOLEMMA 6.54 11.68
SATTY-APPROACH1 9.90 6.46
UKP-WSI-WP-PMI 5.86 30.30
WSI DULUTH.SYS7.PK2 3.01 25.15
UKP-WSI-WP-LLR2 4.17 21.87
UKP-WSI-WACKY-LLR 3.64 32.34
DULUTH.SYS9.PK2 3.32 19.84
DULUTH.SYS1.PK2 2.53 26.45
WSD RAKESH 9.07 2.94
</table>
<tableCaption confidence="0.9792755">
Table 7: Average number of clusters (# cl.) and average
cluster size (ACS).
</tableCaption>
<figure confidence="0.8074615">
2 4 6 8 10 12
average cluster size (ACS)
</figure>
<figureCaption confidence="0.998857">
Figure 3: Average cluster size (ACS) vs. average number
of clusters.
</figureCaption>
<bodyText confidence="0.954762">
mance, calculated in terms of S-recall@K and S-
precision@r, whose results are shown in Tables 8
</bodyText>
<table confidence="0.9990465">
System K
5 10 20 40
HDP-CL.-NOLEMMA 50.80 63.21 79.26 92.48
HDP-CL.-LEMMA 48.13 65.51 78.86 91.68
UKP-WACKY-LLR 41.19 55.41 68.61 83.90
UKP-WP-LLR2 41.07 53.76 68.87 85.87
SI UKP-WP-PMI 40.45 56.25 68.70 84.92
W SATTY-APPROACH1 38.97 48.90 62.72 82.14
DULUTH.SYS7.PK2 38.88 53.79 70.38 86.23
DULUTH.SYS9.PK2 37.15 49.90 68.91 83.65
DULUTH.SYS1.PK2 37.11 53.29 71.24 88.48
WSD RAKESH 46.48 62.36 78.66 90.72
</table>
<tableCaption confidence="0.976931">
Table 8: S-recall@K.
</tableCaption>
<table confidence="0.999698416666667">
System r
50 60 70 80
HDP-CL.-LEMMA 48.85 42.93 35.19 27.62
HDP-CL.-NOLEMMA 48.18 43.88 34.85 29.30
UKP-WP-PMI 42.83 33.40 26.63 22.92
UKP-WACKY-LLR 42.47 31.73 25.39 22.71
SI UKP-WP-LLR2 42.06 32.04 26.57 22.41
DULUTH.SYS1.PK2 40.08 31.31 26.73 24.51
DULUTH.SYS7.PK2 39.11 30.42 26.54 23.43
DULUTH.SYS9.PK2 35.90 29.72 25.26 21.26
SATTY-APPROACH1 34.94 26.88 23.55 20.40
WSD RAKESH 48.00 39.04 32.72 27.92
</table>
<tableCaption confidence="0.999083">
Table 9: S-precision@r.
</tableCaption>
<bodyText confidence="0.998529777777778">
and 9, respectively. Here we find that, again, the
HDP team obtains the best performance, followed by
RAKESH. We note however that not all systems op-
timized the order of clusters and cluster snippets by
relevance.
We also graph the diversification performance
trend of S-recall@K and S-precision@r in Fig-
ures 4 and 5 for K = 1,...,25 and r E
140,50,..., 1001.
</bodyText>
<sectionHeader confidence="0.987876" genericHeader="conclusions">
6 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999898307692308">
One of the aims of the SemEval-2013 task on Word
Sense Induction &amp; Disambiguation within an End
User Application was to enable an objective compar-
ison of WSI and WSD systems when integrated into
Web search result clustering and diversification. The
task is a hard one, in that it involves clustering, but
provides clear-cut evidence that our end-to-end ap-
plication framework overcomes the limits of previ-
ous in-vitro evaluations. Indeed, the systems which
create good clusters and better diversify search re-
sults, i.e., those from the HDP team, achieve good
performance across all the proposed measures, with
no contradictory evidence.
</bodyText>
<figure confidence="0.9587325">
gold-standard
hdp-lemma
hdp-nolemma
sys1.pk2
sys7.pk2
sys9.pk2
rakesh
satty-approach1
ukp-wsi-wacky-llr
ukp-wsi-wp-llr2
ukp-wsi-wp-pmi
40
35
average number of clusters
30
25
20
15
10
5
199
S-recall-at-K
5 10 15 20 25
K
</figure>
<figureCaption confidence="0.982338">
Figure 4: S-recall@K.
</figureCaption>
<figure confidence="0.9333705">
40 50 60 70 80 90 100
r
</figure>
<figureCaption confidence="0.977373">
Figure 5: S-precision@r.
</figureCaption>
<figure confidence="0.998072297297297">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
hdp-lemma
hdp-nolemma
sys1.pk2
sys7.pk2
sys9.pk2
satty-approach1
ukp-wsi-wacky-llr
ukp-wsi-wp-llr2
ukp-wsi-wp-pmi
rakesh
S-precision-at-r
0.6
0.5
0.4
0.3
0.2
0.1
0.0
hdp-lemma
hdp-nolemma
sys1.pk2
sys7.pk2
sys9.pk2
satty-approach1
ukp-wsi-wacky-llr
ukp-wsi-wp-llr2
ukp-wsi-wp-pmi
rakesh
</figure>
<bodyText confidence="0.999882583333333">
Our annotation experience showed that the
Wikipedia sense inventory, augmented with our
generic classes, is a good choice for semantically
tagging search results, in that it covers most of the
meanings a Web user might be interested in. In fact,
only 20% of the snippets was annotated with the
OTHER class.
Future work might consider large-scale multilin-
gual lexical resources, such as BabelNet (Navigli
and Ponzetto, 2012), both as sense inventory and for
performing the search result clustering and diversi-
fication task.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9940712">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
We thank Antonio Di Marco and David A. Jur-
gens for their help.
</bodyText>
<page confidence="0.991905">
200
</page>
<sectionHeader confidence="0.982765" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999360193877551">
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
pages 7–12, Prague, Czech Republic.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky Wide Web: A
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209–226.
Andrea Bernardini, Claudio Carpineto, and Massimil-
iano D’Amico. 2009. Full-subtopic retrieval with
keyphrase-based search results clustering. In Proceed-
ings of Web Intelligence 2009, volume 1, pages 206–
213, Los Alamitos, CA, USA.
Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and
Matthias Richter. 2007. The Leipzig corpora collec-
tion - monolingual corpora of standard size. In Pro-
ceedings of Corpus Linguistic 2007, Birmingham, UK.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009. Dbpedia - a crystallization point
for the web of data. J. Web Sem., 7(3):154–165.
Daniel Crabtree, Xiaoying Gao, and Peter Andreae.
2005. Improving web clustering by cluster selection.
In Proceedings of the 2005 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 172–
178, Washington, DC, USA.
Antonio Di Marco and Roberto Navigli. 2013. Clus-
tering and diversifying web search results with graph-
based word sense induction. Computational Linguis-
tics, 39(4).
Philip Edmonds and Adam Kilgarriff. 2002. Introduc-
tion to the special issue on evaluating word sense dis-
ambiguation systems. Journal of Natural Language
Engineering, 8(4):279–291.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA, USA.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. In Psychological Bulletin,
volume 76, page 378–382.
David Graff. 2003. English Gigaword. In Technical
Report, LDC2003T05, Linguistic Data Consortium,
Philadelphia, PA, USA.
Lawrence Hubert and Phipps Arabie. 1985. Comparing
Partitions. Journal of Classification, 2(1):193–218.
Paul Jaccard. 1901. ´Etude comparative de la distribution
florale dans une portion des alpes et des jura. In Bul-
letin de la Soci´et´e Vaudoise des Sciences Naturelles,
volume 37, page 547–579.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. SemEval-2010
task 14: Word sense induction &amp; disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63–68, Uppsala, Sweden.
Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing
word senses to improve web search result clustering.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 116–
126, Boston, USA.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.
Roberto Navigli. 2008. A structural approach to the
automatic adjudication of word sense disagreements.
Journal of Natural Language Engineering, 14(4):293–
310.
Roberto Navigli. 2009. Word Sense Disambiguation: a
survey. ACM Computing Surveys, 41(2):1–69.
Roberto Navigli. 2012. A quick tour of word sense
disambiguation, induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115–129.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the American
Statistical association, 66(336):846–850.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the Yd Inter-
national Workshop on the Evaluation of Systems for
the Semantic Analysis of Text (Senseval-3), pages 41–
43, Barcelona, Spain.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. YAGO: A large ontology from
wikipedia and wordnet. Journal of Web Semantics,
6(3):203–217.
Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Butterworths, second edition.
ChengXiang Zhai, William W. Cohen, and John Lafferty.
2003. Beyond independent relevance: Methods and
evaluation metrics for subtopic retrieval. In Proceed-
ings of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, pages 10–17, Toronto, Canada.
</reference>
<page confidence="0.998395">
201
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.351906">
<title confidence="0.956868333333333">SemEval-2013 Task 11: Word Sense Induction &amp; Disambiguation within an End-User Application Navigli</title>
<author confidence="0.637006333333333">Dipartimento di_Sapienza Universit`a di_Viale Regina Elena</author>
<abstract confidence="0.999305181818182">In this paper we describe our Semeval-2013 task on Word Sense Induction and Disambiguation within an end-user application, namely Web search result clustering and diversification. Given a target query, induction and disambiguation systems are requested to cluster and diversify the search results returned by a search engine for that query. The task enables the end-to-end evaluation and comparison of systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic.</location>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 7–12, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="18278" citStr="Baroni et al., 2009" startWordPosition="3093" endWordPosition="3096"> We asked each team to provide information about their systems. In Table 4 we report the resources used by each system. The HDP and UKP systems use Wikipedia as raw text for sampling word counts; DULUTH-SYS9-PK2 uses the first 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7- PK2 both use the snippets for inducing the query senses. Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes. They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection6 (Biemann et al., 2007). SATTYAPPROACH1 just uses snippets. The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009). 5 Results We show the results of RI and ARI in Table 5. The best performing systems are those from the HDP team, with considerably higher RI and ARI. The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH. SINGLETONS perform well with RI, but badly </context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A collection of very large linguistically processed webcrawled corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Bernardini</author>
<author>Claudio Carpineto</author>
<author>Massimiliano D’Amico</author>
</authors>
<title>Full-subtopic retrieval with keyphrase-based search results clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of Web Intelligence</booktitle>
<volume>1</volume>
<pages>206--213</pages>
<location>Los Alamitos, CA, USA.</location>
<marker>Bernardini, Carpineto, D’Amico, 2009</marker>
<rawString>Andrea Bernardini, Claudio Carpineto, and Massimiliano D’Amico. 2009. Full-subtopic retrieval with keyphrase-based search results clustering. In Proceedings of Web Intelligence 2009, volume 1, pages 206– 213, Los Alamitos, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Gerhard Heyer</author>
<author>Uwe Quasthoff</author>
<author>Matthias Richter</author>
</authors>
<title>The Leipzig corpora collection - monolingual corpora of standard size.</title>
<date>2007</date>
<booktitle>In Proceedings of Corpus Linguistic</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="18378" citStr="Biemann et al., 2007" startWordPosition="3109" endWordPosition="3112">used by each system. The HDP and UKP systems use Wikipedia as raw text for sampling word counts; DULUTH-SYS9-PK2 uses the first 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7- PK2 both use the snippets for inducing the query senses. Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes. They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection6 (Biemann et al., 2007). SATTYAPPROACH1 just uses snippets. The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009). 5 Results We show the results of RI and ARI in Table 5. The best performing systems are those from the HDP team, with considerably higher RI and ARI. The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH. SINGLETONS perform well with RI, but badly when chance agreement is taken into account. As for F1 and JI, whose values are shown in Table 6, th</context>
</contexts>
<marker>Biemann, Heyer, Quasthoff, Richter, 2007</marker>
<rawString>Chris Biemann, Gerhard Heyer, Uwe Quasthoff, and Matthias Richter. 2007. The Leipzig corpora collection - monolingual corpora of standard size. In Proceedings of Corpus Linguistic 2007, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Bizer</author>
<author>Jens Lehmann</author>
<author>Georgi Kobilarov</author>
<author>S¨oren Auer</author>
<author>Christian Becker</author>
<author>Richard Cyganiak</author>
<author>Sebastian Hellmann</author>
</authors>
<title>Dbpedia - a crystallization point for the web of data.</title>
<date>2009</date>
<journal>J. Web Sem.,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="18558" citStr="Bizer et al., 2009" startWordPosition="3137" endWordPosition="3140"> data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7- PK2 both use the snippets for inducing the query senses. Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes. They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection6 (Biemann et al., 2007). SATTYAPPROACH1 just uses snippets. The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009). 5 Results We show the results of RI and ARI in Table 5. The best performing systems are those from the HDP team, with considerably higher RI and ARI. The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH. SINGLETONS perform well with RI, but badly when chance agreement is taken into account. As for F1 and JI, whose values are shown in Table 6, the two HDP systems again perform best in terms of F1, and are on par with UKP-WSI-WACKY-LLR in terms of JI. The third best approach in terms of F1 is again SATTY-APPROACH1, which ho</context>
</contexts>
<marker>Bizer, Lehmann, Kobilarov, Auer, Becker, Cyganiak, Hellmann, 2009</marker>
<rawString>Christian Bizer, Jens Lehmann, Georgi Kobilarov, S¨oren Auer, Christian Becker, Richard Cyganiak, and Sebastian Hellmann. 2009. Dbpedia - a crystallization point for the web of data. J. Web Sem., 7(3):154–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Crabtree</author>
<author>Xiaoying Gao</author>
<author>Peter Andreae</author>
</authors>
<title>Improving web clustering by cluster selection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence,</booktitle>
<pages>172--178</pages>
<location>Washington, DC, USA.</location>
<contexts>
<context position="14019" citStr="Crabtree et al. (2005)" startWordPosition="2333" endWordPosition="2336">f cases. JI is calculated as follows: RI(C, G) − E(RI(C, G)) ARI(C,ff&apos;&apos;)_ Pij (n2j)−[Pi ( i2) Pj (b2 )]/( 2 ) TP + TN RI(C, G) = , (2) TP + FP + FN + TN JI(C, G) = � (5) TP + FP + FN TP where TP is the number of true positives, i.e., snippet pairs which are in the same cluster both in C and Finally, the F1 measure calculates the harmonic mean of precision (P) and recall (R). Precision determines how accurately the clusters of C represent 196 the query meanings in the gold standard G, whereas recall measures how accurately the different meanings in G are covered by the clusters in C. We follow Crabtree et al. (2005) and define the precision of a cluster Cj E C as follows: |Cs j | P(Cj) =|Cj|, (6) where Csj is the intersection between Cj E C and the gold cluster Gs E G which maximizes the cardinality of the intersection. The recall of a query sense s is instead calculated as: R(s) = |UC;EC9 Cs j | ns , (7) where Cs is the subset of clusters of C whose majority sense is s, and ns is the number of snippets tagged with query sense s in the gold standard. The total precision and recall of the clustering C are then calculated as: P = EC;EC P(Cj) |Cj |; R = EsES R(s)ns EC;EC |Cj |EsES ns (8) where S is the set </context>
</contexts>
<marker>Crabtree, Gao, Andreae, 2005</marker>
<rawString>Daniel Crabtree, Xiaoying Gao, and Peter Andreae. 2005. Improving web clustering by cluster selection. In Proceedings of the 2005 IEEE/WIC/ACM International Conference on Web Intelligence, pages 172– 178, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Di Marco</author>
<author>Roberto Navigli</author>
</authors>
<title>Clustering and diversifying web search results with graphbased word sense induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<marker>Di Marco, Navigli, 2013</marker>
<rawString>Antonio Di Marco and Roberto Navigli. 2013. Clustering and diversifying web search results with graphbased word sense induction. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Introduction to the special issue on evaluating word sense disambiguation systems.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="1669" citStr="Edmonds and Kilgarriff, 2002" startWordPosition="243" endWordPosition="247">reas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009; Navigli, 2012) for a survey). Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008). On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses. In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output. Indeed, WSI evaluation is actually an instance of the more general and difficult probl</context>
</contexts>
<marker>Edmonds, Kilgarriff, 2002</marker>
<rawString>Philip Edmonds and Adam Kilgarriff. 2002. Introduction to the special issue on evaluating word sense disambiguation systems. Journal of Natural Language Engineering, 8(4):279–291.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>In Psychological Bulletin,</journal>
<volume>76</volume>
<pages>378--382</pages>
<contexts>
<context position="9105" citStr="Fleiss, 1971" startWordPosition="1447" endWordPosition="1448">elect the meaning that you consider most appropriate”. No constraint on the age, gender and citizenship of the annotators was imposed. However, in order to avoid random tagging of search results, we provided 3 gold-standard result annotations per query, which could be shown to the Turker more than once during the annotation process. In the case (s)he failed to annotate the gold items, the annotator was automatically excluded. 2.3 Inter-Annotator Agreement and Adjudication In order to determine the reliability of the Turkers’ annotations, we calculated the individual values of Fleiss’ kappa N (Fleiss, 1971) for each query q and then averaged them: EqEQ Nq N =|Q |� (1) where Nq is the Fleiss’ kappa agreement of the three annotators who tagged the 64 snippets returned by the Google search engine for the query q E Q, and Q is our set of 100 queries. We obtained an average value of N = 0.66, which according to Landis and Figure 2: The Wikipedia disambiguation page of Apple. Koch (1977) can be seen as substantial agreement, with a standard deviation Q = 0.185. In Table 2 we show the agreement distribution of our 6400 snippets, distinguishing between full agreement (3 out of 3), majority agreement (2 </context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. In Psychological Bulletin, volume 76, page 378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword.</title>
<date>2003</date>
<booktitle>In Technical Report, LDC2003T05, Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="17990" citStr="Graff, 2003" startWordPosition="3048" endWordPosition="3049">ther HDP-CLUSTERS-LEMMA ✓ ✓ HDP-CLUSTERS-NOLEMMA ✓ ✓ DULUTH.SYS 1.PK2 ✓ DULUTH.SYS7.PK2 ✓ WSI DULUTH.SYS9.PK2 Gigaword UKP-WSI-WP-LLR2 ✓ ✓ ✓ WaCky UKP-WSI-WP-PMI ✓ ✓ ✓ WaCky UKP-WSI-WACKY-LLR ✓ ✓ ✓ WaCky SATTY-APPROACH1 ✓ WSD RAKESH ✓ DBPedia Table 4: Resources used for WSI/WSD. We asked each team to provide information about their systems. In Table 4 we report the resources used by each system. The HDP and UKP systems use Wikipedia as raw text for sampling word counts; DULUTH-SYS9-PK2 uses the first 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7- PK2 both use the snippets for inducing the query senses. Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes. They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection6 (Biemann et al., 2007). SATTYAPPROACH1 just uses snippets. The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009). 5 Results We show the results </context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword. In Technical Report, LDC2003T05, Linguistic Data Consortium, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hubert</author>
<author>Phipps Arabie</author>
</authors>
<title>Comparing Partitions.</title>
<date>1985</date>
<journal>Journal of Classification,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="11695" citStr="Hubert and Arabie, 1985" startWordPosition="1880" endWordPosition="1883">t G be the gold-standard clustering for those results. Each measure W, G) presented below is calculated for the query q using these two clusterings. The overall results on the entire set of queries Q in the dataset is calculated by averaging the values of W, G) obtained for each single test query q ∈ Q. 3.1 Clustering Quality The first evaluation concerned the quality of the clusters produced by the participating systems. Since clustering evaluation is a difficult issue, we calculated four distinct measures available in the literature, namely: • Rand Index (Rand, 1971); • Adjusted Rand Index (Hubert and Arabie, 1985); • Jaccard Index (Jaccard, 1901); • F1 measure (van Rijsbergen, 1979). The Rand Index (RI) of a clustering C is a measure of clustering agreement which determines the percentage of correctly bucketed snippet pairs across the two clusterings C and G. RI is calculated as follows: ������ C C1 C2 ··· C. Sums G G1 n11 n12 · · · n1m a1 G2 n21 n22 ·· · n2m a2 ... ... ... .. .. ... .. G9 ng1 ng2 ··· ngm ag Sums b1 b2 ··· bm N Table 3: Contingency table for the clusterings G and C. G, TN is the number of true negatives, i.e., pairs which are in different clusters in both clusterings, and FP and FN are</context>
</contexts>
<marker>Hubert, Arabie, 1985</marker>
<rawString>Lawrence Hubert and Phipps Arabie. 1985. Comparing Partitions. Journal of Classification, 2(1):193–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Jaccard</author>
</authors>
<title>Etude comparative de la distribution florale dans une portion des alpes et des jura.</title>
<date>1901</date>
<booktitle>In Bulletin de la Soci´et´e Vaudoise des Sciences Naturelles,</booktitle>
<volume>37</volume>
<pages>547--579</pages>
<contexts>
<context position="11728" citStr="Jaccard, 1901" startWordPosition="1887" endWordPosition="1888">se results. Each measure W, G) presented below is calculated for the query q using these two clusterings. The overall results on the entire set of queries Q in the dataset is calculated by averaging the values of W, G) obtained for each single test query q ∈ Q. 3.1 Clustering Quality The first evaluation concerned the quality of the clusters produced by the participating systems. Since clustering evaluation is a difficult issue, we calculated four distinct measures available in the literature, namely: • Rand Index (Rand, 1971); • Adjusted Rand Index (Hubert and Arabie, 1985); • Jaccard Index (Jaccard, 1901); • F1 measure (van Rijsbergen, 1979). The Rand Index (RI) of a clustering C is a measure of clustering agreement which determines the percentage of correctly bucketed snippet pairs across the two clusterings C and G. RI is calculated as follows: ������ C C1 C2 ··· C. Sums G G1 n11 n12 · · · n1m a1 G2 n21 n22 ·· · n2m a2 ... ... ... .. .. ... .. G9 ng1 ng2 ··· ngm ag Sums b1 b2 ··· bm N Table 3: Contingency table for the clusterings G and C. G, TN is the number of true negatives, i.e., pairs which are in different clusters in both clusterings, and FP and FN are, respectively, the number of fal</context>
</contexts>
<marker>Jaccard, 1901</marker>
<rawString>Paul Jaccard. 1901. ´Etude comparative de la distribution florale dans une portion des alpes et des jura. In Bulletin de la Soci´et´e Vaudoise des Sciences Naturelles, volume 37, page 547–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data. biometrics,</title>
<date>1977</date>
<pages>159--174</pages>
<marker>Landis, Koch, 1977</marker>
<rawString>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<title>SemEval-2010 task 14: Word sense induction &amp; disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>63--68</pages>
<location>Uppsala,</location>
<contexts>
<context position="3715" citStr="Manandhar et al., 2010" startWordPosition="555" endWordPosition="558">ask described in this paper1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems. The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre 1http://www.cs.york.ac.uk/semeval-2013/task11/ 193 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 193–201, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics and Soroa, 2007; Manandhar et al., 2010), and enabling a fair comparison between the two disambiguation paradigms. Key to our framework is the assumption that search results grouped into a given cluster are semantically related to each other and that each cluster is expected to represent a specific meaning of the input query (even though it is possible for more than one cluster to represent the same meaning). For instance, consider the target query apple and the following 3 search result snippets: 1. Apple Inc., formerly Apple Computer, Inc., is... 2. The science of apple growing is called pomology... 3. Apple designs and creates iP</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach, and Sameer S. Pradhan. 2010. SemEval-2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 63–68, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Giuseppe Crisafulli</author>
</authors>
<title>Inducing word senses to improve web search result clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>116--126</pages>
<location>Boston, USA.</location>
<contexts>
<context position="2826" citStr="Navigli and Crisafulli, 2010" startWordPosition="422" endWordPosition="426">luation is actually an instance of the more general and difficult problem of evaluating clustering algorithms. Nonetheless, many everyday tasks carried out by online users would benefit from intelligent systems able to address the lexical ambiguity issue effectively. A case in point is Web information retrieval, a task which is becoming increasingly difficult given the continuously growing pool of Web text of the most wildly disparate kinds. Recent work has addressed this issue by proposing a general evaluation framework for injecting WSI into Web search result clustering and diversification (Navigli and Crisafulli, 2010; Di Marco and Navigli, 2013). In this task the search results returned by a search engine for an input query are grouped into clusters, and diversified by providing a reranking which maximizes the meaning heterogeneity of the top ranking results. The Semeval-2013 task described in this paper1 adopts the evaluation framework of Di Marco and Navigli (2013), and extends it to both WSD and WSI systems. The task is aimed at overcoming the wellknown limitations of in vitro evaluations, such as those of previous SemEval tasks on the topic (Agirre 1http://www.cs.york.ac.uk/semeval-2013/task11/ 193 Se</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 116– 126, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A structural approach to the automatic adjudication of word sense disagreements.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>310</pages>
<contexts>
<context position="1789" citStr="Navigli, 2008" startWordPosition="263" endWordPosition="264">i, 2009; Navigli, 2012) for a survey). Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008). On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses. In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output. Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clustering algorithms. Nonetheless, many everyday tasks carried out by online users would benefit from </context>
</contexts>
<marker>Navigli, 2008</marker>
<rawString>Roberto Navigli. 2008. A structural approach to the automatic adjudication of word sense disagreements. Journal of Natural Language Engineering, 14(4):293– 310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: a survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="1182" citStr="Navigli, 2009" startWordPosition="176" endWordPosition="177">eturned by a search engine for that query. The task enables the end-to-end evaluation and comparison of systems. 1 Introduction Word ambiguity is a pervasive issue in Natural Language Processing. Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009; Navigli, 2012) for a survey). Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: a survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A quick tour of word sense disambiguation, induction and related approaches.</title>
<date>2012</date>
<booktitle>In Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM),</booktitle>
<pages>115--129</pages>
<contexts>
<context position="1198" citStr="Navigli, 2012" startWordPosition="178" endWordPosition="179">arch engine for that query. The task enables the end-to-end evaluation and comparison of systems. 1 Introduction Word ambiguity is a pervasive issue in Natural Language Processing. Two main techniques in computational lexical semantics, i.e., Word Sense Disambiguation (WSD) and Word Sense Induction (WSI) address this issue from different perspectives: the former is aimed at assigning word senses from a predefined sense inventory to words in context, whereas the latter automatically identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009; Navigli, 2012) for a survey). Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008). On the </context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>Roberto Navigli. 2012. A quick tour of word sense disambiguation, induction and related approaches. In Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM), pages 115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>Journal of the American Statistical association,</journal>
<pages>66--336</pages>
<contexts>
<context position="11646" citStr="Rand, 1971" startWordPosition="1874" endWordPosition="1875">ring output by a given system and let G be the gold-standard clustering for those results. Each measure W, G) presented below is calculated for the query q using these two clusterings. The overall results on the entire set of queries Q in the dataset is calculated by averaging the values of W, G) obtained for each single test query q ∈ Q. 3.1 Clustering Quality The first evaluation concerned the quality of the clusters produced by the participating systems. Since clustering evaluation is a difficult issue, we calculated four distinct measures available in the literature, namely: • Rand Index (Rand, 1971); • Adjusted Rand Index (Hubert and Arabie, 1985); • Jaccard Index (Jaccard, 1901); • F1 measure (van Rijsbergen, 1979). The Rand Index (RI) of a clustering C is a measure of clustering agreement which determines the percentage of correctly bucketed snippet pairs across the two clusterings C and G. RI is calculated as follows: ������ C C1 C2 ··· C. Sums G G1 n11 n12 · · · n1m a1 G2 n21 n22 ·· · n2m a2 ... ... ... .. .. ... .. G9 ng1 ng2 ··· ngm ag Sums b1 b2 ··· bm N Table 3: Contingency table for the clusterings G and C. G, TN is the number of true negatives, i.e., pairs which are in differen</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>William M. Rand. 1971. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical association, 66(336):846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the Yd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3),</booktitle>
<pages>41--43</pages>
<location>Barcelona,</location>
<contexts>
<context position="1695" citStr="Snyder and Palmer, 2004" startWordPosition="248" endWordPosition="251">identifies the meanings of a word of interest by clustering the contexts in which it occurs (see (Navigli, 2009; Navigli, 2012) for a survey). Unfortunately, the paradigms of both WSD and WSI suffer from significant issues which hamper their success in real-world applications. In fact, the performance of WSD systems depends heavily on which sense inventory is chosen. For instance, the most popular computational lexicon of English, i.e., WordNet (Fellbaum, 1998), provides fine-grained distinctions which make the disambiguation task quite difficult even for humans (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), although disagreements can be solved to some extent with graph-based methods (Navigli, 2008). On the other hand, although WSI overcomes this issue by allowing unrestrained sets of senses, its evaluation is particularly arduous because there is no easy way of comparing and ranking different representations of senses. In fact, all the proposed measures in the literature tend to favour specific cluster shapes (e.g., singletons or all-in-one clusters) of the senses produced as output. Indeed, WSI evaluation is actually an instance of the more general and difficult problem of evaluating clusterin</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English all-words task. In Proceedings of the Yd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), pages 41– 43, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A large ontology from wikipedia and wordnet.</title>
<date>2008</date>
<journal>Journal of Web Semantics,</journal>
<volume>6</volume>
<issue>3</issue>
<contexts>
<context position="18505" citStr="Suchanek et al., 2008" startWordPosition="3128" endWordPosition="3131">t 10,000 paragraphs of the Associated Press wire service data from the English Gigaword Corpus (Graff, 2003, 1st edition), whereas DULUTH-SYS1-PK2 and DULUTH-SYS7- PK2 both use the snippets for inducing the query senses. Finally, the UKP systems were the only ones to retrieve the Web pages from the corresponding URLs and exploit them for WSI purposes. They also use WaCky (Baroni et al., 2009) and a distributional thesaurus obtained from the Leipzig Corpora Collection6 (Biemann et al., 2007). SATTYAPPROACH1 just uses snippets. The only participating WSD system, RAKESH, uses the YAGO hierarchy (Suchanek et al., 2008) together with DBPedia abstracts (Bizer et al., 2009). 5 Results We show the results of RI and ARI in Table 5. The best performing systems are those from the HDP team, with considerably higher RI and ARI. The next best systems are SATTY-APPROACH1, which uses only the words in the snippets, and the only WSD system, i.e., RAKESH. SINGLETONS perform well with RI, but badly when chance agreement is taken into account. As for F1 and JI, whose values are shown in Table 6, the two HDP systems again perform best in terms of F1, and are on par with UKP-WSI-WACKY-LLR in terms of JI. The third best appro</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from wikipedia and wordnet. Journal of Web Semantics, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelis Joost van Rijsbergen</author>
</authors>
<date>1979</date>
<note>Information Retrieval. Butterworths, second edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Butterworths, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ChengXiang Zhai</author>
<author>William W Cohen</author>
<author>John Lafferty</author>
</authors>
<title>Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>10--17</pages>
<location>Toronto, Canada.</location>
<contexts>
<context position="15908" citStr="Zhai et al., 2003" startWordPosition="2700" endWordPosition="2703">selecting the second element of each cluster Cj such that |Cj |&gt; 2, and so on. The remaining elements returned by the search engine, but not included in any cluster of C, are appended to the bottom of the list in their original order. Note that systems were asked to sort snippets within clusters, as well as clusters themselves, by relevance. Since our goal is to determine how many different meanings are covered by the top-ranking search results according to the output clustering, we used the measures of S-recall@K (Subtopic recall at rank K) and S-precision@r (Subtopic precision at recall r) (Zhai et al., 2003). S-recall@K determines the ratio of different meanings for a given query q in the top-K results returned: S-recall@K = |{sense(ri) : i E {1, ... , K}} |, g (10) where sense(ri) is the gold-standard sense associated with the i-th snippet returned by the system, and g is the total number of distinct senses for the query q in our gold standard. S-precision@r instead determines the ratio of different senses retrieved for query q in the first Kr snippets, where Kr is the minimum number of top results for which the system achieves recall r. The measure is defined as follows: |UKr sense(r•)| S-preci</context>
</contexts>
<marker>Zhai, Cohen, Lafferty, 2003</marker>
<rawString>ChengXiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 10–17, Toronto, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>