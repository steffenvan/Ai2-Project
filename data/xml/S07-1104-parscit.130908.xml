<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.092800">
<title confidence="0.986253">
UVA: Language Modeling Techniques for Web People Search
</title>
<author confidence="0.998532">
Krisztian Balog Leif Azzopardi Maarten de Rijke
</author>
<affiliation confidence="0.999334">
ISLA, University of Amsterdam University of Glasgow ISLA, University of Amsterdam
</affiliation>
<email confidence="0.996178">
kbalog@science.uva.nl leif@dcs.gla.ac.uk mdr@science.uva.nl
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999010111111111">
In this paper we describe our participation in
the SemEval 2007 Web People Search task.
Our main aim in participating was to adapt
language modeling tools for the task, and to
experiment with various document represen-
tations. Our main finding is that single pass
clustering, using title, snippet and body to
represent documents, is the most effective
setting.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999910086956522">
The goal of the Web People Search task at SemEval
2007 was to disambiguate person names in a web
searching scenario (Artiles et al., 2007). Participants
were presented with the following setting: given a
list of documents retrieved from a web search engine
using a person’s name as a query, group documents
that refer to the same individual.
Our aim with the participation was to adapt lan-
guage modeling techniques to this task. To this
end, we employed two methods: single pass cluster-
ing (SPC) and probabilistic latent semantic analysis
(PLSA). Our main finding is that the former leads to
high purity, while the latter leads to high inverse pu-
rity scores. Furthermore, we experimented with var-
ious document representations, based on the snip-
pets and body text. Highest overall performance was
achieved with the combination of both.
The remainder of the paper is organized as fol-
lows. In Section 2 we present the two approaches we
employed for clustering documents. Next, in Sec-
tion 3 we discuss document representation and pre-
processing. Section 4 reports on our experiments.
We conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.99777" genericHeader="introduction">
2 Modeling
</sectionHeader>
<subsectionHeader confidence="0.998107">
2.1 Single Pass Clustering
</subsectionHeader>
<bodyText confidence="0.999977259259259">
We employed single pass clustering (Hill., 1968) to
automatically assign pages to clusters, where we as-
sume that each cluster is a set of pages related to one
particular sense of the person.
The process for assignment was performed as fol-
lows: The first document was taken and assigned
to the first cluster. Then each subsequent document
was compared against each cluster with a similarity
measure based on the log odds ratio (initially, there
was only the initial one created). A document was
assigned to the most likely cluster, as long as the
similarity score was higher than a threshold α; oth-
erwise, the document was assigned to a new cluster,
unless the maximum number of desired clusters q
had been reached; in that case the document was as-
signed to the last cluster (i.e., the left overs).
The similarity measure we employed was the log
odds ratio to decide whether the document was more
likely to be generated from that cluster or not. This
approach follows Kalt (1996)’s work on document
classification using the document likelihood by rep-
resenting the cluster as a multinomial term distribu-
tion (i.e., a cluster language model) and predicting
the probability of a document D, given the cluster
language model, i.e., p(DjBc). It is assumed that
the terms t in a document are sampled independently
and identically, so the log odds ratio is calculated as
</bodyText>
<page confidence="0.991936">
468
</page>
<bodyText confidence="0.859583">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 468–471,
Prague, June 2007. c�2007 Association for Computational Linguistics
follows:
</bodyText>
<equation confidence="0.799094">
11 tED p(t |B�C)n(t,D)
</equation>
<bodyText confidence="0.988026055555555">
where n(t, D) is the number of times a term ap-
pears in a document, and the BC represents the lan-
guage model that represents not being in the cluster.
Note this is similar to a well-known relevance mod-
eling approach, where the clusters are relevance and
non-relevance, except, here, it is applied in the con-
text of classification as done by Kalt (1996).
The cluster language model was estimated by per-
forming a linear interpolation between the empirical
probability of a term occurring in the cluster p(t|C)
and the background model p(t), the probability of
a term occurring at random in the collection, i.e.,
p(t|BC) = A · p(t|C) + (1 − A) · p(t), where A was
set to 0.5.1 The “not in the cluster” language model
was approximated by using the background model
p(t). The similarity threshold above (used for de-
ciding whether to assign a document to an existing
cluster) was set to α = 1, and q was set to 100.
</bodyText>
<subsectionHeader confidence="0.998954">
2.2 Probabilistic Latent Semantic Analysis
</subsectionHeader>
<bodyText confidence="0.999702756097561">
The second method for disambiguation we em-
ployed was probabilistic latent semantic analysis
(PLSA) (Hofmann, 1999). PLSA clusters docu-
ments based on the term-document co-occurrence
which results in semantic decomposition of the term
document matrix into a lower dimensional latent
space. Formally, PLSA can be defined as:
so the document is assigned to one of the person-
topics. Here, we made the assignment based on the
maximum p(z|d), so if p(z|d) = maxp(z|d), then d
was assigned to z.
In order to automatically select the number of
person-topics, we performed the following process
to decide when the appropriate number of person-
topics (defined by k) have been identified: (1) we
set k = 2 and computed the log-likelihood of the de-
composition on a held out sample of data; (2) we in-
cremented k and computed the log-likelihood; if the
log-likelihood had increased over a given threshold
(0.001) then we repeated step 2, else (3) we stopped
as we have maximized the log-likelihood of the de-
compositions, with respect to the number person-
topics. This point was assumed to be the optimal
with respect to the number of person senses. Since,
we are focusing on identifying the true number of
classes, this should result in higher inverse purity,
whereas with the single pass clustering the number
of clusters is not restricted, and so we would expect
single pass clustering to produce more clusters but
with a higher purity.
We used Lemur2 and the PennAspect implemen-
tation of PLSA (Schein et al., 2002) for our exper-
iments, where the parameters for PLSA where set
as follows. For each k we performed 10 initializa-
tions where the best initialization in terms of log-
likelihood was selected. The EM algorithm was
run using tempering with up to 100 EM Steps. For
tempering the setting suggested in (Hofmann, 1999)
were used. The models were estimated on 90% of
the data and 10% of the data was held out in order to
compute the log-likelihood of the decompositions.
</bodyText>
<equation confidence="0.93579">
log O(D,C) = log p(D|BC) (1)
p(D|B �C)
11tED p(t|BC)n(t,D)
= log ,
p(t, d) = p(d) � p(t|z)p(z|d), (2)
z 3 Document Representation
</equation>
<bodyText confidence="0.999967666666667">
where p(t, d) is the probability of term t and doc-
ument d co-occurring, p(t|z) is the probability of a
term given a latent topic z and p(z|d) is the probabil-
ity of a latent topic in a document. The prior prob-
ability of the document, p(d), was assumed to be
uniform. This decomposition can be obtained auto-
matically using the EM algorithm (Hofmann, 1999).
Once estimated, we assumed that each latent topic
represents one of the different senses of the person,
</bodyText>
<footnote confidence="0.6266435">
1This value was not tuned but selected based on best per-
forming range suggested by Lavrenko and Croft (2001).
</footnote>
<bodyText confidence="0.999895">
This section describes the various document repre-
sentations we considered, and preprocessing steps
we applied.
For each document, we considered the title, snip-
pet, and body text. Title and snippet were pro-
vided by the output of the search engine results
(person name.xml files), while the body text
was extracted from the crawled index.html files.
</bodyText>
<footnote confidence="0.991269">
2http://www.lemurproject.org
</footnote>
<page confidence="0.998159">
469
</page>
<table confidence="0.999903125">
Method Pur Title+Snippet F0.2 Pur Body F0.2 Title+Snippet+Body
InvP F0.5 InvP F0.5 Pur InvP F0.5 F0.2
Train data
SPC 0.903 0.298 0.422 0.336 0.776 0.416 0.482 0.434 0.768 0.438 0.506 0.456
PLSA 0.589 0.833 0.636 0.716 0.591 0.656 0.563 0.592 0.579 0.724 0.588 0.641
Test data
SPC 0.867 0.541 0.640 0.575 0.818 0.570 0.647 0.596 0.810 0.607 0.669 0.628
PLSA 0.292 0.892 0.383 0.533 0.311 0.869 0.413 0.563 0.305 0.923 0.405 0.566
</table>
<tableCaption confidence="0.99994">
Table 1: Results of the clustering methods using various document representations.
</tableCaption>
<subsectionHeader confidence="0.998845">
3.1 Acquiring Plain-Text Content from HTML
</subsectionHeader>
<bodyText confidence="0.99998405">
Our aim is to extract the plain-text content from
HTML pages and to leave out blocks or segments
that contain little or no useful textual information
(headers, footers, navigation menus, adverts, etc.).
To this end, we exploit the fact that most web-
pages consist of blocks of text content with rel-
atively little markup, interspersed with navigation
links, images with captions, etc. These segments of
a page are usually separated by block-level HTML
tags. Our extractor first generates a syntax tree from
the HTML document. We then traverse this tree
while bookkeeping the stretch of uninterrupted non-
HTML text we have seen. Each time we encounter a
block-level HTML tag we examine the buffer of text
we have collected, and if it is longer than a threshold,
we output it. The threshold for the minimal length of
buffer text was empirically set to 10. In other words,
we only consider segments of the page, separated
by block-level HTML tags, that contain 10 or more
words.
</bodyText>
<subsectionHeader confidence="0.998667">
3.2 Indexing
</subsectionHeader>
<bodyText confidence="0.999945125">
We used a standard (English) stopword list but we
did not apply stemming. A separate index was built
for each person, using the Lemur toolkit. We created
three index variations: title+snippet, body,
and title+snippet+body.
In our official run we used the
title+snippet+body index; however, in
the next section we report on all three variations.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="background">
4 Results
</sectionHeader>
<bodyText confidence="0.989948">
Table 1 reports on the results of our experiments us-
ing the Single Pass Clustering (SPC) and Probabilis-
tic Latent Semantic Analysis (PLSA) methods with
various document representations. The measures
(purity, inverse purity, and F-score with α = 0.5
and α = 0.2) are presented for both the train and
test data sets.
The results clearly demonstrate the difference in
the behaviors of the two clustering methods. SPC
assigns people to the same cluster with high preci-
sion, as is reflected by the high purity scores. How-
ever, it is overly restrictive, and documents that be-
long to the same person are distributed into a number
of clusters, which should be further merged. This
explains the low inverse purity scores. Further ex-
periments should be performed to evaluate to which
extent this restrictive behavior could be controlled
by the α parameter of the method.
In contrast with SPC, the PLSA method produces
far fewer clusters per person. These clusters may
cover multiple referents of a name, as is witnessed
by the low purity scores. On the other hand, inverse
purity scores are very high, which means referents
are usually not dispersed among clusters.
As to the various document representations, we
found that highest overall performance was achieved
with the combination of title, snippet, and body text.
Since the data was not homogenous, it would be
interesting to see how performance varies on the dif-
ferent names. We leave this analysis to further work.
Our official run employed the SPC method, using
the title+snippet+body index. The results
of our official submission are presented in Table 2.
Our purity score was the highest of all submissions,
and our system was ranked overall 4th, based on the
Fα=0.5 measure.
</bodyText>
<page confidence="0.993769">
470
</page>
<table confidence="0.9948428">
Pur InvP F0.s F0.2
Lowest 0.30 0.60 0.40 0.55
Highest 0.81 0.95 0.78 0.83
Average 0.54 0.82 0.60 0.69
UVA 0.81 0.60 0.67 0.62
</table>
<tableCaption confidence="0.989254">
Table 2: Official submission results and statistics.
</tableCaption>
<sectionHeader confidence="0.999036" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999948916666667">
We have described our participation in the SemEval
2007 Web People Search task. Our main aim in par-
ticipating was to adapt language modeling tools for
the task, and to experiment with various document
representations. Our main finding is that single pass
clustering, using title, snippet and body to represent
documents, is the most effective setting.
We explored the two very different clustering
schemes with contrasting characteristics. Looking
forward, possible improvements might be pursued
by combining the two approaches into a more robust
system.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999548777777778">
Krisztian Balog was supported by the Netherlands
Organization for Scientific Research (NWO) un-
der project number 220-80-001. Maarten de Rijke
was supported by NWO under project numbers
017.001.190, 220-80-001, 264-70-050, 354-20-005,
600.065.120, 612-13-001, 612.000.106, 612.066.-
302, 612.069.006, 640.001.501, 640.002.501, and
by the E.U. IST programme of the 6th FP for RTD
under project MultiMATCH contract IST-033104.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999804035714286">
J. Artiles, J. Gonzalo, and S. Sekine. 2007. The
SemEval-2007 WePS Evaluation: Establishing a
benchmark for the Web People Search Task. In Pro-
ceedings of Semeval 2007, Association for Computa-
tional Linguistics.
D. R. Hill. 1968. A vector clustering technique. In
Samuelson, editor, Mechanised Information Storage,
Retrieval and Dissemination, North-Holland, Amster-
dam.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of Uncertainty in Artificial Intelli-
gence, UAI’99, Stockholm.
T. Kalt. 1996. A new probabilistic model of text classifi-
cation and retrieval. Technical Report CIIR TR98-18,
University of Massachusetts, January 25, 1996.
V. Lavrenko and W. B. Croft. 2001. Relevance-based
language models. In Proceedings of the 24th annual
international ACM SIGIR conference, pages 120–127,
New Orleans, LA. ACM Press.
Andrew I. Schein, Alexandrin Popescul, Lyle H. Un-
gar, and David M. Pennock. 2002. Meth-
ods and metrics for cold-start recommendations.
In SIGIR ’02: Proceedings of the 25th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 253–260, New York, NY, USA. ACM
Press. See http://www.cis.upenn.edu/
datamining/software dist/PennAspect/.
</reference>
<page confidence="0.998726">
471
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.935837">
<title confidence="0.998825">UVA: Language Modeling Techniques for Web People Search</title>
<author confidence="0.995648">Krisztian Balog Leif Azzopardi Maarten de_Rijke</author>
<affiliation confidence="0.999126">ISLA, University of Amsterdam University of Glasgow ISLA, University of Amsterdam</affiliation>
<email confidence="0.964798">kbalog@science.uva.nlleif@dcs.gla.ac.ukmdr@science.uva.nl</email>
<abstract confidence="0.9975342">In this paper we describe our participation in the SemEval 2007 Web People Search task. Our main aim in participating was to adapt language modeling tools for the task, and to experiment with various document representations. Our main finding is that single pass clustering, using title, snippet and body to represent documents, is the most effective setting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>J Gonzalo</author>
<author>S Sekine</author>
</authors>
<title>The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task.</title>
<date>2007</date>
<booktitle>In Proceedings of Semeval 2007, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="768" citStr="Artiles et al., 2007" startWordPosition="111" endWordPosition="114"> Glasgow ISLA, University of Amsterdam kbalog@science.uva.nl leif@dcs.gla.ac.uk mdr@science.uva.nl Abstract In this paper we describe our participation in the SemEval 2007 Web People Search task. Our main aim in participating was to adapt language modeling tools for the task, and to experiment with various document representations. Our main finding is that single pass clustering, using title, snippet and body to represent documents, is the most effective setting. 1 Introduction The goal of the Web People Search task at SemEval 2007 was to disambiguate person names in a web searching scenario (Artiles et al., 2007). Participants were presented with the following setting: given a list of documents retrieved from a web search engine using a person’s name as a query, group documents that refer to the same individual. Our aim with the participation was to adapt language modeling techniques to this task. To this end, we employed two methods: single pass clustering (SPC) and probabilistic latent semantic analysis (PLSA). Our main finding is that the former leads to high purity, while the latter leads to high inverse purity scores. Furthermore, we experimented with various document representations, based on th</context>
</contexts>
<marker>Artiles, Gonzalo, Sekine, 2007</marker>
<rawString>J. Artiles, J. Gonzalo, and S. Sekine. 2007. The SemEval-2007 WePS Evaluation: Establishing a benchmark for the Web People Search Task. In Proceedings of Semeval 2007, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Hill</author>
</authors>
<title>A vector clustering technique.</title>
<date>1968</date>
<booktitle>Mechanised Information Storage, Retrieval and Dissemination, North-Holland,</booktitle>
<editor>In Samuelson, editor,</editor>
<location>Amsterdam.</location>
<marker>Hill, 1968</marker>
<rawString>D. R. Hill. 1968. A vector clustering technique. In Samuelson, editor, Mechanised Information Storage, Retrieval and Dissemination, North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic analysis.</title>
<date>1999</date>
<booktitle>In Proc. of Uncertainty in Artificial Intelligence, UAI’99,</booktitle>
<location>Stockholm.</location>
<contexts>
<context position="4407" citStr="Hofmann, 1999" startWordPosition="722" endWordPosition="723"> probability of a term occurring in the cluster p(t|C) and the background model p(t), the probability of a term occurring at random in the collection, i.e., p(t|BC) = A · p(t|C) + (1 − A) · p(t), where A was set to 0.5.1 The “not in the cluster” language model was approximated by using the background model p(t). The similarity threshold above (used for deciding whether to assign a document to an existing cluster) was set to α = 1, and q was set to 100. 2.2 Probabilistic Latent Semantic Analysis The second method for disambiguation we employed was probabilistic latent semantic analysis (PLSA) (Hofmann, 1999). PLSA clusters documents based on the term-document co-occurrence which results in semantic decomposition of the term document matrix into a lower dimensional latent space. Formally, PLSA can be defined as: so the document is assigned to one of the persontopics. Here, we made the assignment based on the maximum p(z|d), so if p(z|d) = maxp(z|d), then d was assigned to z. In order to automatically select the number of person-topics, we performed the following process to decide when the appropriate number of persontopics (defined by k) have been identified: (1) we set k = 2 and computed the log-</context>
<context position="6100" citStr="Hofmann, 1999" startWordPosition="1011" endWordPosition="1012">ses, this should result in higher inverse purity, whereas with the single pass clustering the number of clusters is not restricted, and so we would expect single pass clustering to produce more clusters but with a higher purity. We used Lemur2 and the PennAspect implementation of PLSA (Schein et al., 2002) for our experiments, where the parameters for PLSA where set as follows. For each k we performed 10 initializations where the best initialization in terms of loglikelihood was selected. The EM algorithm was run using tempering with up to 100 EM Steps. For tempering the setting suggested in (Hofmann, 1999) were used. The models were estimated on 90% of the data and 10% of the data was held out in order to compute the log-likelihood of the decompositions. log O(D,C) = log p(D|BC) (1) p(D|B �C) 11tED p(t|BC)n(t,D) = log , p(t, d) = p(d) � p(t|z)p(z|d), (2) z 3 Document Representation where p(t, d) is the probability of term t and document d co-occurring, p(t|z) is the probability of a term given a latent topic z and p(z|d) is the probability of a latent topic in a document. The prior probability of the document, p(d), was assumed to be uniform. This decomposition can be obtained automatically usi</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proc. of Uncertainty in Artificial Intelligence, UAI’99, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kalt</author>
</authors>
<title>A new probabilistic model of text classification and retrieval.</title>
<date>1996</date>
<tech>Technical Report CIIR TR98-18,</tech>
<institution>University of Massachusetts,</institution>
<contexts>
<context position="2751" citStr="Kalt (1996)" startWordPosition="447" endWordPosition="448">ster with a similarity measure based on the log odds ratio (initially, there was only the initial one created). A document was assigned to the most likely cluster, as long as the similarity score was higher than a threshold α; otherwise, the document was assigned to a new cluster, unless the maximum number of desired clusters q had been reached; in that case the document was assigned to the last cluster (i.e., the left overs). The similarity measure we employed was the log odds ratio to decide whether the document was more likely to be generated from that cluster or not. This approach follows Kalt (1996)’s work on document classification using the document likelihood by representing the cluster as a multinomial term distribution (i.e., a cluster language model) and predicting the probability of a document D, given the cluster language model, i.e., p(DjBc). It is assumed that the terms t in a document are sampled independently and identically, so the log odds ratio is calculated as 468 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 468–471, Prague, June 2007. c�2007 Association for Computational Linguistics follows: 11 tED p(t |B�C)n(t,D) where n(t,</context>
</contexts>
<marker>Kalt, 1996</marker>
<rawString>T. Kalt. 1996. A new probabilistic model of text classification and retrieval. Technical Report CIIR TR98-18, University of Massachusetts, January 25, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lavrenko</author>
<author>W B Croft</author>
</authors>
<title>Relevance-based language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference,</booktitle>
<pages>120--127</pages>
<publisher>ACM Press.</publisher>
<location>New Orleans, LA.</location>
<contexts>
<context position="6949" citStr="Lavrenko and Croft (2001)" startWordPosition="1162" endWordPosition="1165">d) = p(d) � p(t|z)p(z|d), (2) z 3 Document Representation where p(t, d) is the probability of term t and document d co-occurring, p(t|z) is the probability of a term given a latent topic z and p(z|d) is the probability of a latent topic in a document. The prior probability of the document, p(d), was assumed to be uniform. This decomposition can be obtained automatically using the EM algorithm (Hofmann, 1999). Once estimated, we assumed that each latent topic represents one of the different senses of the person, 1This value was not tuned but selected based on best performing range suggested by Lavrenko and Croft (2001). This section describes the various document representations we considered, and preprocessing steps we applied. For each document, we considered the title, snippet, and body text. Title and snippet were provided by the output of the search engine results (person name.xml files), while the body text was extracted from the crawled index.html files. 2http://www.lemurproject.org 469 Method Pur Title+Snippet F0.2 Pur Body F0.2 Title+Snippet+Body InvP F0.5 InvP F0.5 Pur InvP F0.5 F0.2 Train data SPC 0.903 0.298 0.422 0.336 0.776 0.416 0.482 0.434 0.768 0.438 0.506 0.456 PLSA 0.589 0.833 0.636 0.716</context>
</contexts>
<marker>Lavrenko, Croft, 2001</marker>
<rawString>V. Lavrenko and W. B. Croft. 2001. Relevance-based language models. In Proceedings of the 24th annual international ACM SIGIR conference, pages 120–127, New Orleans, LA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew I Schein</author>
<author>Alexandrin Popescul</author>
<author>Lyle H Ungar</author>
<author>David M Pennock</author>
</authors>
<title>Methods and metrics for cold-start recommendations.</title>
<date>2002</date>
<booktitle>In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>253--260</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5793" citStr="Schein et al., 2002" startWordPosition="956" endWordPosition="959">hreshold (0.001) then we repeated step 2, else (3) we stopped as we have maximized the log-likelihood of the decompositions, with respect to the number persontopics. This point was assumed to be the optimal with respect to the number of person senses. Since, we are focusing on identifying the true number of classes, this should result in higher inverse purity, whereas with the single pass clustering the number of clusters is not restricted, and so we would expect single pass clustering to produce more clusters but with a higher purity. We used Lemur2 and the PennAspect implementation of PLSA (Schein et al., 2002) for our experiments, where the parameters for PLSA where set as follows. For each k we performed 10 initializations where the best initialization in terms of loglikelihood was selected. The EM algorithm was run using tempering with up to 100 EM Steps. For tempering the setting suggested in (Hofmann, 1999) were used. The models were estimated on 90% of the data and 10% of the data was held out in order to compute the log-likelihood of the decompositions. log O(D,C) = log p(D|BC) (1) p(D|B �C) 11tED p(t|BC)n(t,D) = log , p(t, d) = p(d) � p(t|z)p(z|d), (2) z 3 Document Representation where p(t, </context>
</contexts>
<marker>Schein, Popescul, Ungar, Pennock, 2002</marker>
<rawString>Andrew I. Schein, Alexandrin Popescul, Lyle H. Ungar, and David M. Pennock. 2002. Methods and metrics for cold-start recommendations. In SIGIR ’02: Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, pages 253–260, New York, NY, USA. ACM</rawString>
</citation>
<citation valid="false">
<authors>
<author>Press</author>
</authors>
<note>See http://www.cis.upenn.edu/ datamining/software dist/PennAspect/.</note>
<marker>Press, </marker>
<rawString>Press. See http://www.cis.upenn.edu/ datamining/software dist/PennAspect/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>