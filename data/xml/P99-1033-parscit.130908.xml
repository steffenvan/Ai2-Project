<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993782">
Dependency Parsing with an Extended Finite State Approach
</title>
<author confidence="0.987823">
Kemal Oflazer
</author>
<affiliation confidence="0.924109333333333">
Department of Computer Engineering Computing Research Laboratory
Bilkent University New Mexico State University
Ankara, 06533,Turkey Las Cruces, NM, 88003 USA
</affiliation>
<email confidence="0.910439">
koecs.bilkent.eduAr koecrl.nmsu.edu
</email>
<sectionHeader confidence="0.990771" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999754416666667">
This paper presents a dependency parsing scheme
using an extended finite state approach. The parser
augments input representation with &amp;quot;channels&amp;quot; so
that links representing syntactic dependency rela-
tions among words can be accommodated, and it-
erates on the input a number of times to arrive at
a fixed point. Intermediate configurations violating
various constraints of projective dependency repre-
sentations such as no crossing links, no independent
items except sentential head, etc, are filtered via fi-
nite state filters. We have applied the parser to de-
pendency parsing of Turkish.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919225806452">
Recent advances in the development of sophisticated
tools for building finite state systems (e.g., XRCE
Finite State Tools (Karttunen et al., 1996), AT&amp;T
Tools (Mohri et al., 1998)) have fostered the develop-
ment of quite complex finite state systems for natu-
ral language processing. In the last several years,
there have been a number of studies on develop-
ing finite state parsing systems, (Koskenniemi, 1990;
Koskenniemi et al., 1992; Grefenstette, 1996; Ait-
Mokhtar and Chanod, 1997). There have also been
a number of approaches to natural language pars-
ing using extended finite state approaches in which
a finite state engine is applied multiple times to the
input, or various derivatives thereof, until some stop-
ping condition is reached. Roche (1997) presents
an approach for parsing in which the input is itera-
tively bracketed using a finite state transducer. Ab-
ney(1996) presents a finite state parsing approach
in which a tagged sentence is parsed by transducers
which progressively transform the input to sequences
of symbols representing phrasal constituents. This
paper presents an approach to dependency parsing
using an extended finite state model resembling the
approaches of Roche and Abney. The parser pro-
duces outputs that encode a labeled dependency tree
representation of the syntactic relations between the
words in the sentence.
We assume that the reader is familiar with the
basic concepts of finite state transducers (FST here-
after), finite state devices that map between two reg-
ular languages U and L (Kaplan and Kay, 1994).
</bodyText>
<sectionHeader confidence="0.990179" genericHeader="method">
2 Dependency Syntax
</sectionHeader>
<bodyText confidence="0.984505222222222">
Dependency approaches to syntactic representation
use the notion of syntactic relation to associate sur-
face lexical items. The book by Mel6uk (1988)
presents a comprehensive exposition of dependency
syntax. Computational approaches to dependency
syntax have recently become quite popular (e.g.,
a workshop dedicated to computational approaches
to dependency grammars has been held at COL-
ING/ACL&apos;98 Conference). Jarvinen and Tapana-
ninen have demonstrated an efficient wide-coverage
dependency parser for English (Tapanainen and
Jarvinen, 1997; Jarvinen and Tapanainen, 1998).
The work of Sleator and Temperley(1991) on link
grammar, an essentially lexicalized variant of depen-
dency grammar, has also proved to be interesting in
a number of aspects. Dependency-based statistical
language modeling and analysis have also become
quite popular in statistical natural language process-
ing (Lafferty et al., 1992; Eisner, 1996; Chelba and
et al., 1997).
Robinson(1970) gives four axioms for well-formed
dependency structures, which have been assumed in
almost all computational approaches. In a depen-
dency structure of a sentence (i) one and only one
word is independent, i.e., not linked to some other
word, (ii) all others depend directly on some word,
(iii) no word depends on more than one other, and,
</bodyText>
<listItem confidence="0.607867818181818">
(iv) if a word A depends directly on B, and some
word C intervenes between them (in linear order),
then C depends directly on A or on B, or on some
other intervening word. This last condition of pro-
jectivity (or various extensions of it; see e.g., Lau
and Huang (1994)) is usually assumed by most com-
putational approaches to dependency grammars as
a constraint for filtering configurations, and has also
been used as a simplifying condition in statistical
approaches for inducing dependencies from corpora
(e.g., Yaret(1998).)
</listItem>
<sectionHeader confidence="0.998752" genericHeader="method">
3 Turkish
</sectionHeader>
<bodyText confidence="0.999604666666667">
Turkish is an agglutinative language where a se-
quence of inflectional and derivational morphemes
get affixed to a root (Oflazer, 1993). Derivations are
very productive, and the syntactic relations that a
word is involved in as a dependent or head element,
are determined by the inflectional properties of the
</bodyText>
<page confidence="0.99657">
254
</page>
<figure confidence="0.99938925">
a) Input sequence of IGs are augmented with symbols to represent Channels.
(IG1) (1G2) (1G3)... (16,) (IQ)
b) Links are embedded in channels.
(1G1) (IGO (1G3)... (IG.4) (IG.)
Links from Dopendonto Link to Head
1G2
IG1 IG3
Word
</figure>
<figureCaption confidence="0.999977">
Figure 1: Links and Inflectional Groups
</figureCaption>
<bodyText confidence="0.996130272727273">
one or more (intermediate) derived forms. In this
work, we assume that a Turkish word is represented
as a sequence of inflectional groups (IGs hereafter),
separated by &amp;quot;DBs denoting derivation boundaries,
in the following general form:
root+Inf1i-DB+Inf12-DB+. • •-DB+Infin
where Inf1i denote relevant inflectional features
including the part-of-speech for the root, or any
of the derived forms. For instance, the derived
determiner saglamlagtirdigimizdakil would be
represented as:2
</bodyText>
<figure confidence="0.76035775">
saglam+Adj-DB+Verb+Become-DB+Verb+Caus+Pos
-DB+Ad j +PastPart+P lsg -DB
+Noun+Zero+A3sg+Pnon+Loc-DB+Det
This word has 6 IGs:
1. saglam+Adj 2. +Verb+Become
3. +Verb+Caus+Pos 4. +Adj+PastPart+Plsg
5. +Noun+Zero+A3sg 6. +Det
+Pnon+Loc
</figure>
<bodyText confidence="0.93785575">
A sentence would then be represented as a sequence
of the IGs making up the words.
An interesting observation that we can make
about Turkish is that, when a word is considered
as a sequence of IGs, syntactic relation links only
emanate from the last IG of a (dependent) word,
and land on one of the IG&apos;s of the (head) word on
the right (with minor exceptions), as exemplified in
</bodyText>
<figureCaption confidence="0.681875">
Figure 1. A second observation is that, with minor
exceptions, the dependency links between the IGs,
when drawn above the IG sequence, do not cross.
Figure 2 shows a dependency tree for a sentence laid
on top of the words segmented along IG boundaries.
</figureCaption>
<sectionHeader confidence="0.991452" genericHeader="method">
4 Finite State Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999942">
The approach relies on augmenting the input with
&amp;quot;channels&amp;quot; that (logically) reside above the IG se-
quence and &amp;quot;laying&amp;quot; links representing dependency
relations in these channels, as depicted Figure 3 a).
The parser operates in a number of iterations: At
each iteration of the parser, an new empty channel
</bodyText>
<footnote confidence="0.8870535">
&apos;Literally, &amp;quot;(the thing existing) at the time we caused
(something) to become strong&amp;quot;. Obviously this is not a word
that one would use everyday. Turkish words found in typical
text average about 3-4 morphemes including the stem.
2 The morphological features other than the obvious POSs
are: +Become: become verb, +Caus: causative verb, PastPart:
Derived past participle, Plsg: lsg possessive agreement,
A3sg: 3sg number-person agreement,+Zero: Zero derivation
with no overt morpheme, +Pnon: No possessive agreement,
+Loc:Locative case, +Pos: Positive Polarity.
</footnote>
<equation confidence="0.955980714285714">
C) New channels are &amp;quot;stacked on top of each other.
(1G1) (IG2) (1G3)... (IQ)
d) So that links that can not be accommodated in lower channels can be established.
4
(IG,) (IG2) (1G3)... aGn (IG„)
4=3.
(IG,) (IG,) (IG,)... (IQ)... (IG„,,) (IG„)
</equation>
<figureCaption confidence="0.999022">
Figure 3: Channels and Links
</figureCaption>
<bodyText confidence="0.9789865">
is &amp;quot;stacked&amp;quot; on top of the input, and any possible
links are established using these channels, until no
new links can be added. An abstract view of this is
presented in parts b) through e) of Figure 3.
</bodyText>
<subsectionHeader confidence="0.9968295">
4.1 Representing Channels and Syntactic
Relations
</subsectionHeader>
<bodyText confidence="0.968151518518518">
The sequence (or the chart) of IGs is produced by
a a morphological analyzer FST, with each IG be-
ing augmented by two pairs of delimiter symbols, as
&lt;(IG)&gt;. Word final IGs, IGs that links will emanate
from, are further augmented with a special marker 0.
Channels are represented by pairs of matching sym-
bols that surround the &lt;...( and the ) ...&gt; pairs.
Symbols for new channels (upper channels in Figure
3) are stacked so that the symbols for the topmost
channels are those closest to the ( ).3 The chan-
nel symbol 0 indicates that the channel segment is
not used while 1 indicates that the channel is used
by a link that starts at some IG on the left and
ends at some IG on the right, that is, the link is
just crossing over the IG. If a link starts from an
IG (ends on an IG), then a start (stop) symbol de-
noting the syntactic relation is used on the right
(left) side of the IG. The syntactic relations (along
with symbols used) that we currently encode in our
parser are the following:4 S (Subject), 0 (Object),
M (Modifier, adv/adj), P (Possessor), C (Classifier),
D (Determiner), T (Dative Adjunct), L ( Locative
Adjunct), A: (Ablative Adjunct) and I (Instrumen-
tal Adjunct). For instance, with three channels, the
two IGs of bakedeki in Figure 2, would be repre-
sented as &lt;MDO (bahge+Noun+A3sg+Pnon+Loc ) 000&gt;
&lt;000(+DetC1)00d&gt;. The M and the D to the left of
</bodyText>
<footnote confidence="0.911927">
3 At any time, the number of channel symbols on both sides
of an IG are the same.
4 We use the lower case symbol to mark the start of the
link and the upper case symbol to encode the end of the link.
</footnote>
<page confidence="0.995114">
255
</page>
<figure confidence="0.9355335">
Det Pos Subj
D ADJ N
D N ADV V
er1—.nces-i
PN ADV V
Last line shows the final POS for each word.
</figure>
<figureCaption confidence="0.999145">
Figure 2: Dependency Links in an example Turkish Sentence
</figureCaption>
<bodyText confidence="0.997938333333333">
the first IG indicate the incoming modifier and de-
terminer links, and the d on the right of the second
IG indicates the outgoing determiner link.
</bodyText>
<subsectionHeader confidence="0.999786">
4.2 Components of a Parser Stage
</subsectionHeader>
<bodyText confidence="0.9919378">
The basic strategy of a parser stage is to recognize by
a rule (encoded as a regular expression) a dependent
IG and a head IG, and link them by modifying the
&amp;quot;topmost&amp;quot; channel between those two. To achieve
this:
</bodyText>
<listItem confidence="0.957071222222222">
1. we put temporary brackets to the left of the
dependent IG and to the right of the head IG,
making sure that (i) the last channel in that
segment is free, and (ii) the dependent is not
already linked (at one of the lower channels),
2. we mark the channels of the start, intermediate
and ending IGs with the appropriate symbols
encoding the relation thus established by the
brackets,
</listItem>
<bodyText confidence="0.859536861111111">
3. we remove the temporary brackets.
A typical linking rule looks like the following:5
[LL IG1 LR] [ML IG2 MR]* [RL IG3 RR] (-&gt;)
This rule says: (optionally) bracket (with IS and
S}), any occurrence of morphological pattern IG1
(dependent), skipping over any number of occur-
rences of pattern IG2, finally ending with a pat-
tern IGS (governor). The symbols L(eft)L(eft),
LB., ML, MB., RI. and RR are regular expressions
that encode constraints on the bounding chan-
nel symbols. For instance, LEL is the pattern
&amp;quot;0&amp;quot; &amp;quot;)&amp;quot; &amp;quot;0&amp;quot; [&amp;quot;0&amp;quot; I 11* &amp;quot;&gt;&amp;quot; which checks that
(i) this is a word-final IG (has a &amp;quot;0&amp;quot;), (ii) the right
side &amp;quot;topmost&amp;quot; channel is empty (channel symbol
nearest to &amp;quot;)&amp;quot;is &amp;quot;0&amp;quot;), and (iii) the IG is not linked
to any other in any of the lower channels (the only
symbols on the right side are Os and is.)
For instance the example rule
[LL NominativeNominalA3p1 Lit] [ML AnyIG MR]*
[RL [FiniteVerbA3sg I FiniteVerbA3p1] RR ]
(-&gt;) &amp;quot;{S&amp;quot; &amp;quot;S}&apos;
5We use the XRCE Regular Expression Language
Syntax; see http://vvw.xrce.xerox.com/research/
mitt/fst/fssyntax.html for details.
is used to bracket a segment starting with a plural
nominative nominal, as subject of a finite verb on
the right with either +A3sg or +A3p1 number-person
agreement (allowed in Turkish.) The regular expres-
sion NominativeNominalA3p1 matches any nomi-
nal IG with nominative case and A3p1 agreement,
while the regular expression [FiniteVerbA3sg
FiniteVerbA3p1] matches any finite verb IG with
either A3sg or A3p1 agreement. The regular expres-
sion AnyIG matches any IG.
All the rules are grouped together into a parallel
bracketing rule defined as follows:
</bodyText>
<equation confidence="0.991849666666667">
Bracket = [
Pattern1 (-&gt;) &amp;quot;Glen&amp;quot; &amp;quot;Re111&amp;quot;,
Pattern2 (-&gt;) &amp;quot;{Re12&amp;quot; &amp;quot;Re12}&amp;quot;,
</equation>
<bodyText confidence="0.988924">
which will produce all possible bracketing of the in-
put IG sequence.6
</bodyText>
<subsectionHeader confidence="0.999667">
4.3 Filtering Crossing Link Configurations
</subsectionHeader>
<bodyText confidence="0.999984363636364">
The bracketings produced by Bracket contain con-
figurations that may have crossing links. This hap-
pens when the left side channel symbols of the IG
immediately right of a open bracket contains the
symbol 1 for one of the lower channels, indicating
a link entering the region, or when the right side
channel symbols of the IG immediately to the left
of a close bracket contains the symbol 1 for one of
the lower channels, indicating a link exiting the seg-
ment, i.e., either or both of the following patterns
appear in the bracketed segment:
</bodyText>
<equation confidence="0.966837">
(i) fs &lt; 1 ... 0 ( )
(ii) ( ) 0 ... 1 ... &gt;S}
</equation>
<bodyText confidence="0.99833">
Configurations generated by bracketing are filtered
by FSTs implementing suitable regular expressions
that reject inputs having crossing links.
A second configuration that may appear is the fol-
lowing: A rule may attempt to put a link in the
topmost channel even though the corresponding seg-
ment is not utilized in a previous channel, e.g., the
corresponding segment one of the previous channels
may be all Os. This constraint filters such cases to
</bodyText>
<footnote confidence="0.9467705">
6 {Reli and Reli} are pairs of brackets; there is a distinct
pair for each syntactic relation to be identified by these rules.
</footnote>
<page confidence="0.994057">
256
</page>
<bodyText confidence="0.876983333333333">
prevent redundant configurations from proliferating
for later iterations of the parser.7 For these two con-
figuration constraints we define FilterConfigs as8
</bodyText>
<equation confidence="0.425419833333333">
FilterConfigs = [ FilterCrossingLinks .o.
FilterEmptySegments];
We can now define one phase (of one iteration) of
the parser as:
Phase = Bracket .o. FilterConfigs .o.
MarkChannels .o. RemoveTempBrackets;
</equation>
<bodyText confidence="0.9541101875">
The transducer MarkChannels modifies the chan-
nel symbols in the bracketed segments to either
the syntactic relation start or end symbol, or a
1, depending on the 1G. Finally, the transducer
RemoveTempBrackets, removes the brackets.9
The formulation up to now does not allow us to
bracket an IG on two consecutive non-overlapping
links in the same channel. We would need a brack-
eting configuration like
but this would not be possible within Bracket, as
patterns check that no other brackets are within
their segment of interest. Simply composing the
Phase transducer with itself without introducing a
new channel solves this problem, giving us a one-
stage parser, i.e.,
Parse = Phase .o. Phase;
</bodyText>
<subsectionHeader confidence="0.990862">
4.4 Enforcing Syntactic Constraints
</subsectionHeader>
<bodyText confidence="0.99992247368421">
The rules linking the IGs are overgenerating in that
they may generate configurations that may vio-
late some general or language specific constraints.
For instance, more than one subject or one ob-
ject may attach to a verb, or more that one deter-
miner or possessor may attach to a nominal, an ob-
ject may attach to a passive verb (conjunctions are
handled in the manner described in Jarvinen and
Tapanainen(1998)), or a nominative pronoun may
be linked as a direct object (which is not possible
in Turkish), etc. Constraints preventing these may
can be encoded in the bracketing patterns, but do-
ing so results in complex and unreadable rules. In-
stead, each can be implemented as a finite state filter
which operate on the outputs of Parse by checking
the symbols denoting the relations. For instance we
can define the following regular expression for fil-
tering out configurations where two determiners are
attached to the same IG:19
</bodyText>
<footnote confidence="0.724276818181818">
7 This constraint is a bit trickier since one has to check that
the same number of channels on both sides are empty; we limit
ourselves to the last 3 channels in the implementation.
8 . 0 . denotes the transducer composition operator. We
also use, for exposition purposes, =, instead of the XRCE
define command.
9 The details of these regular expressions are quite uninter-
esting.
10LeftChannelSymbols and RightChannelSymbols denote
the sets of symbols that can appear on the left and right side
channels.
</footnote>
<equation confidence="0.9846245">
AtMostOneDet =
[ &amp;quot;&lt;&amp;quot; [ -[[$&amp;quot;D&amp;quot;]-1] &amp; LeftChannelSymbols* ]
&amp;quot;(&amp;quot; AnyIG (&amp;quot;0&amp;quot;) &amp;quot;)&amp;quot;
RightChannelSymbols* &amp;quot;&gt;&amp;quot; ]*;
</equation>
<bodyText confidence="0.9224565">
The FST for this regular expression makes sure that
all configurations that are produced have at most
one D symbol among the left channel symbols.&amp;quot;
Many other syntactic constraints (e.g., only one ob-
ject to a verb) can be formulated similar to above.
All such constraints Consl, Cons2 ...ConsN, can
then be composed to give one FST that enforces all
of these:
</bodyText>
<figure confidence="0.848244">
SyntacticFilter = [ Consl .o. Cons2 .o.
Cons3 .o. .o. ConsN]
4.5 Iterative application of the parser
Full parsing consists of iterative applications of the
Parser and SyntacticFilter FSTs. Let Input be
a transducer that represents the word sequence. Let
LastChannelNotEmpty =
[&amp;quot;&lt;&amp;quot; LeftChannelSymbols+
&amp;quot;C&amp;quot; AnyIG (&amp;quot;0&amp;quot;) &amp;quot;)&amp;quot;
RightChannelSymbols+ e&gt;n]* —
1&amp;quot;&lt;&amp;quot; LeftChannelSymbols* o
&amp;quot;(&amp;quot; AnyIG (&amp;quot;0&amp;quot;) &amp;quot;)&amp;quot;
0 RightChannelSymbols* &amp;quot;&gt;&amp;quot;]*;
</figure>
<figureCaption confidence="0.976271363636364">
be a transducer which detects if any configuration
has at least one link established in the last channel
added (i.e., not all of the &amp;quot;topmost&amp;quot; channel sym-
bols are O&apos;s.) Let MorphologicalDisambiguator
be a reductionistic finite state disambiguator which
performs accurate but very conservative local dis-
ambiguation and multi-word construct coalescing, to
reduce morphological ambiguity without making any
errors.
The iterative applications of the parser can now
be given (in pseudo-code) as:
</figureCaption>
<bodyText confidence="0.6287985">
# Map sentence to a transducer representing
a chart of IGs
</bodyText>
<equation confidence="0.998460333333333">
M = [Sentence .o. MorphologicalAnalyzer] .o.
MorphologicalDisambiguator;
repeat
M = M .o. AddChannel .o. Parse .o.
SyntacticFilter;
1
until ( [M .o. LastChannelNotEmpty].1 == 1)
M = M .o. OnlyOneUnlinked ;
Parses = M.1;
</equation>
<bodyText confidence="0.998799">
This procedure iterates until the most recently
added channel of every configuration generated is
unused (i.e., the (lower regular) language recognized
by M .o. LastChannelNotEmpty is empty.)
The step after the loop, M = M .o.
OnlyOneUnlinked, enforces the constraint that
</bodyText>
<footnote confidence="0.972313333333333">
11The crucial portion at the beginning says &amp;quot;For any IG it is
not the case that there is more than one substring containing
D among the left channel symbols of that IG.&amp;quot;
</footnote>
<page confidence="0.996695">
257
</page>
<bodyText confidence="0.9999455">
in a correct dependency parse all except one of
the word final IGs have to link as a dependent
to some head. This transduction filters all those
configurations (and usually there are many of them
due to the optionality in the bracketing step.)
Then, Parses defined as the (lower) language of the
resulting FST has all the strings that encode the
IGs and the links.
</bodyText>
<subsectionHeader confidence="0.974787">
4.6 Robust Parsing
</subsectionHeader>
<bodyText confidence="0.974903256410257">
It is possible that either because of grammar cover-
age, or ungrammatical input, a parse with only one
unlinked word final IG may not be found. In such
cases Parses above would be empty. One may how-
ever opt to accept parses with k &gt; 1 unlinked word
final IGs when there are no parses with &lt; k un-
linked word final IGs (for some small k.) This can be
achieved by using the lenient composition operator
(K art tunen, 1998). Lenient composition, notated
as .0 . , is used with a generator-filter combination.
When a generator transducer G is leniently composed
with a filter transducer, F, the resulting transducer,
G .0. F, has the following behavior when an input
is applied: If any of the outputs of G in response to
the input string satisfies the filter F, then G .0. F
produces just these as output. Otherwise, G .0. F
outputs what G outputs.
Let Unlinked_i denote a regular expression which
accepts parse configurations with less than or equal
i unlinked word final IGs. For instance, for i = 2,
this would be defined as follows:
-[[$[ &amp;quot;&lt;&amp;quot; LeftChannelSymbols* &amp;quot;(&amp;quot; AnyIG &amp;quot;C&amp;quot;
[&amp;quot;0&amp;quot; I 1]* &amp;quot;&gt;&amp;quot;77- &gt; 2 7;
which rejects configurations having more than 2
word final IGs whose right channel symbols contain
only Os and is, i.e., they do not link to some other
IG as a dependent.
Replacing line M = M .o. OnlyOneUnlinked,
with, for instance, M = M .0. Unlinked_i .0.
Unlinked_2 .0. Unlinked_3; will have the parser
produce outputs with up to 3 unlinked word final
IGs, when there are no outputs with a smaller num-
ber of unlinked word final IGs. Thus it is possible to
recover some of the partial dependency structures
when a full dependency structure is not available
for some reason. The caveat would be however that
since Unlinked_i is a very strong constraint, any
relaxation would increase the number of outputs
substantially.
</bodyText>
<sectionHeader confidence="0.946942" genericHeader="method">
5 Experiments with dependency
</sectionHeader>
<subsectionHeader confidence="0.82224">
parsing of Turkish
</subsectionHeader>
<bodyText confidence="0.999920921875">
Our work to date has mainly consisted of developing
and implementing the representation and finite state
techniques involved here, along with a non-trivial
grammar component. We have tested the resulting
system and grammar on a corpus of 50 Turkish sen-
tences, 20 of which were also used for developing and
testing the grammar. These sentences had 4 to 24
words with an average 10 about 12 words.
The grammar has two major components. The
morphological analyzer is a full coverage analyzer
built using XRCE tools, slightly modified to gen-
erate outputs as a sequence of IGs for a sequence
of words. When an input sentence (again repre-
sented as a transducer denoting a sequence of words)
is composed with the morphological analyzer (see
pseudo-code above), a transducer for the chart rep-
resenting all IGs for all morphological ambiguities
(remaining after morphological disambiguation) is
generated. The dependency relations are described
by a set of about 30 patterns much like the ones
exemplified above. The rules are almost all non-
lexical establishing links of the types listed earlier.
Conjunctions are handled by linking the left con-
junct to the conjunction, and linking the conjunction
to the right conjunct (possibly at a different chan-
nel). There are an additional set of about 25 finite
state constraints that impose various syntactic and
configurational constraints. The resulting Parser
transducer has 2707 states 27,713 transitions while
the SyntacticConstraints transducer has 28,894
states and 302,354 transitions. The combined trans-
ducer for morphological analysis and (very limited)
disambiguation has 87,475 states and 218,082 arcs.
Table 1 presents our results for parsing this set of
50 sentences. The number of iterations also count
the last iteration where no new links are added. In-
spired by Lin&apos;s notion of structural complexity (Lin,
1996), measured by the total length of the links in
a dependency parse, we ordered the parses of a sen-
tence using this measure. In 32 out of 50 sentences
(64%), the correct parse was either the top ranked
parse or among the top ranked parses with the same
measure. In 13 out of 50 parses (26%) the correct
parse was not among the top ranked parses, but was
ranked lower. Since smaller structural complexity
requires, for example, verbal adjuncts, etc. to attach
to the nearest verb wherever possible, topicalization
of such items which brings them to the beginning of
the sentence, will generate a long(er) link to the verb
(at the end) increasing complexity. In 5 out of 50
sentences (5%), the correct parse was not available
among the parses generated, mainly due to grammar
coverage. The parses generated in these cases used
other (morphological) ambiguities of certain lexical
items to arrive at some parse within the confines of
the grammar.
The finite state transducers compile in about
2 minutes on Apple Macintosh 250 Mhz Power-
book. Parsing is about a second per iteration in-
cluding lookup in the morphological analyzer. With
completely (and manually) morphologically disam-
biguated input, parsing is instantaneous. Figure 4
presents the input and the output of the parser for a
sample Turkish sentence. Figure 5 shows the output
</bodyText>
<page confidence="0.993642">
258
</page>
<table confidence="0.89674125">
Input Sentence: Diinya BankasiTiirkiye Direktoni
hükümetin izledigi ekonomik programin sonucunda
önemli adimlann atildignu soyledi.
Parser Output after 3 iterations:
</table>
<bodyText confidence="0.806324">
English: World Bank Turkey Director said that as a re-
sult of the economic program followed by the government,
important steps were taken.
</bodyText>
<figure confidence="0.9639704375">
Parsei:
&lt;000(dUnya+Noun+A3sg+Pnon+Nome)00c&gt;&lt;C00(banka+Noun+A3sg+P3sg+HoW0c0&gt; &lt;010(tUrkiye+Houn+Prop+A3sg+Pnon+Nom(0)01c&gt;
&lt;CCO(direktOr+Noun+A3sg+P3sg+NomMs00&gt;&lt;001(hUkUmet+Noun+A3sg+Pnon+Gen(0)108&gt;&lt;S01(izle+Verb+Pos)100&gt;
&lt;001(+Adj+PastPart+P3sWim0&gt;&lt;011(ekonomik+Adj011m&gt;&lt;MMi(program+Noun+A3sg+Pnon+Gen0)10p&gt;
&lt;P01(sonuC+Houn+A3sg+P3sg+Loc(0110&gt;&lt;011(Onem+Noun)110&gt;&lt;011(+Adj+Withe)lim&gt;&lt;M11(adIm+Noun+A3p1+Pnon+Gen(4)11s,
&lt;S11(at+Verb)110&gt;&lt;011(+Verb+Pass+Pos)110&gt;&lt;011(+Noun+PastPart+A3sg+P3sg+Acce)11o&gt;&lt;OLS(sOyle+Verb+Pos+Past+A3s0)000&gt;
***
Parse2:
&lt;000(dUnya+Noun+A3sg+Pnon+Nome)00c&gt;&lt;C00(banka+Noun+A3sg+P3sg+Home)0c0&gt;&lt;010(tUrkiye+Noun+Prop+A3sg+Pnon+Nome)01c&gt;
&lt;CCO(direktOr+Noun+A3sg+P3sg+Nome)800&gt;&lt;001(hUkUmet+Noun+A3sg+Pnon+Gene)10s&gt;&lt;S01(izle+Verb+Pos)100&gt;
&lt;001(+Adj+PastPart+P3s0)1m0&gt;&lt;011(ekonomik+Adj0)11m&gt;&lt;MMi(program+Noun+A3sg+Pnon+Gen0)10p&gt;
&lt;P01(sonuC+Noun+A3sg+P3sg+Loc0)110&gt;&lt;011(Onem+Noun)110&gt;&lt;011(+Adj+Withe)11m&gt;&lt;M11(adIm+Noun+A3p1+Pnon+GeWlis&gt;
&lt;SL1(at+Verb)100&gt;&lt;001(+Verb+Pass+Pos)100&gt;&lt;001(+Noun+PastPart+A3sg+P3sg+Acce)10o&gt;&lt;O0S(sOyle+Verb+Pos+Past+A3s0)000&gt;
* * *
The only difference in the two are parses are in the locative adjunct attachment (to verbs at and soyle,
highlighted with ***).
</figure>
<figureCaption confidence="0.998367">
Figure 4: Sample Input and Output of the parser
</figureCaption>
<table confidence="0.9999095">
Avg. Words/Sentence: 11.7 (4 — 24)
Avg. IGs/Sentence: 16.4 (5 — 36)
Avg. Parser Iterations: 5.2 (3 — 8)
Avg. Parses/Sentence: 23.9 (1 — 132)
</table>
<tableCaption confidence="0.939506">
Table 1: Statistics from Parsing 50 Turkish Sen-
tences
</tableCaption>
<bodyText confidence="0.618963">
of the parser processed with a Perl script to provide
a more human-consumable presentation:
</bodyText>
<sectionHeader confidence="0.997216" genericHeader="discussions">
6 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.99998175">
We have presented the architecture and implemen-
tation of novel extended finite state dependency
parser, with results from Turkish. We have formu-
lated, but not yet implemented at this stage, two
extensions. Crossing dependency links are very rare
in Turkish and almost always occur in Turkish when
an adjunct of a verb cuts in a certain position of a
(discontinuous) noun phrase. We can solve this by
allowing such adjuncts to use a special channel &amp;quot;be-
low&amp;quot; the IG sequence so that limited crossing link
configurations can be allowed. Links where the de-
pendent is to the right of its head, which can happen
with some of the word order variations (with back-
grounding of some dependents of the main verb) can
similarly be handled with a right-to-left version of
Parser which is applied during each iteration, but
these cases are very rare.
In addition to the reductionistic disambiguator
that we have used just prior to parsing, we have im-
plemented a number of heuristics to limit the num-
ber of potentially spurious configurations that re-
sult because of optionality in bracketing, mainly by
enforcing obligatory bracketing for immediately se-
quential dependency configurations (e.g., the com-
plement of a postposition is immediately before it.)
Such heuristics force such dependencies to appear in
the first channel and hence prune many potentially
useless configurations popping up in later stages.
The robust parsing technique has been very instru-
mental during the process mainly in the debugging
of the grammar, but we have not made any substan-
tial experiments with it yet.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999956375">
This work was partially supported by a NATO
Science for Stability Program Project Grant, TU-
LANGUAGE made to Bilkent University. A portion
of this work was done while the author was visit-
ing Computing Research Laboratory at New Mexico
State University. The author thanks Lauri Kart-
tunen of Xerox Research Centre Europe, Grenoble
for making available XRCE Finite State Tools.
</bodyText>
<sectionHeader confidence="0.996991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.903037928571428">
Steven Abney. 1996. Partial parsing via finite state
cascades. In Proceedings of the ESSLLI&apos;96 Robust
Parsing Workshop.
Salah Ait-Mokhtar and Jean-Pierre Chanod. 1997.
Incremental finite-state parsing. In Proceedings of
ANLP&apos;97, pages 72 — 79, April.
Ciprian Chelba and et al. 1997. Structure and esti-
mation of a dependency language model. In Pro-
cessings of Eurospeech&apos;97.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Pro-
ceedings of the 16th International Conference on
Computational Linguistics (COLING-96), pages
340-345, August.
</reference>
<page confidence="0.997103">
259
</page>
<table confidence="0.9386245">
C s
c---C c---CC s s---S m---MM 13-
dUnya banka tUrkiye direktOr hUkUmet izle ekonomik program
Noun Noun Noun Noun Noun Verb Adj Adj0 Noun
A3sg A3sg Prop A3sg A3sg Pos PastPart A3sg
Pnon P3sg A3sg P3sg Pnon P3sg@ Pnon
Nom® Nom@ Pnon Nom@ Gen@ Gen@
Nom@
</table>
<figure confidence="0.983960466666667">
Onem
Noun Adj
With@
adIm at
Noun Verb
A3p1
Pnon
Gen@
o---0 S
sOyle
Verb
Pos
Past
A3sg0
1
--P
m---M s---SL
sonuC
Noun
A3sg
P3sg
Loc@
Verb
Pass
Pos
Noun
PastPart
A3sg
P3sg
Acc@
</figure>
<figureCaption confidence="0.999088">
Figure 5: Dependency tree for the second parse
</figureCaption>
<reference confidence="0.968004943661972">
Gregory Grefenstette. 1996. Light parsing as finite-
state filtering. In ECAI &apos;96 Workshop on Ex-
tended finite state models of language. August.
Timo Jarvinen and Pasi Tapanainen. 1998. Towards
an implementable dependency grammar. In Pro-
ceedings of COLING/ACL&apos;98 Workshop on Pro-
cessing Dependency-based Grammars, pages 1-10.
Ronald M. Kaplan and Martin Kay. 1994. Regular
models of phonological rule systems. Computa-
tional Linguistics, 20(3):331-378, September.
Lauri Karttunen, Jean-Pierre Chanod, Gregory
Grefenstette, and Anne Schiller. 1996. Regu-
lar expressions for language engineering. Natural
Language Engineering, 2(4):305-328.
Lauri Karttunen. 1998. The proper treatment of
optimality theory in computational linguistics. In
Lauri Karttunen and Kemal Oflazer, editors, Pro-
ceedings of the International Workshop on Finite
State Methods in Natural Language Processing-
FSMNLP, June.
Kimmo Koskenniemi, Pasi Tapanainen, and Atro
Voutilainen. 1992. Compiling and using finite-
state syntactic rules. In Proceedings of the 14th
International Conference on Computational Lin-
guistics, COLING-92, pages 156-162.
Kimmo Koskenniemi. 1990. Finite-state parsing
and disambiguation. In Proceedings of the 13th
International Conference on Computational Lin-
guistics, COLING&apos;90, pages 229 - 233.
John Lafferty, Daniel Sleator, and Davy Temper-
ley. 1992. Grammatical trigrams: A probabilis-
tic model of link grammars. In Proceedings of the
1992 AAAI Fall Symposium on Probablistic Ap-
proaches to Natural Language.
Bong Yeung Tom Lai and Changning Huang. 1994.
Dependency grammar and the parsing of Chinese
sentences. In Proceedings of the 1994 Joint Con-
ference of 8th ACLIC and 2nd PaFoCol.
Dekang Lin. 1996. On the structural complexity of
natural language sentences. In Proceedings of the
16th International Conference on Computational
Linguistics (COLING-96).
Igor A. Meleuk. 1988. Dependency Syntax: Theory
and Practice. State University of New York Press.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 1998. A rational design for a weighted finite-
state transducer library. In Lecture Notes in Com-
puter Science, 1436. Springer Verlag.
Kemal Oflazer. 1993. Two-level description of Turk-
ish morphology. In Proceedings of the Sixth Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, April. A full
version appears in Literary and Linguistic Com-
puting, Vol.9 No.2, 1994.
Jane J. Robinson. 1970. Dependency structures and
transformational rules. Language, 46(2):259-284.
Emmanuel Roche. 1997. Parsing with finite state
transducers. In Emmanuel Roche and Yves Sch-
abes, editors, Finite-State Language Processing,
chapter 8. The MIT Press.
Daniel Sleator and Davy Temperley. 1991. Parsing
English with a link grammar. Technical Report
CMU-CS-91-196, Computer Science Department,
Carnegie Mellon University.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of
ANLP&apos;97, pages 64 - 71, April.
Deniz Yiiret. 1998. Discovery of Linguistic Rela-
tions Using Lexical Attraction. Ph.D. thesis, De-
partment of Electrical Engineering and Computer
Science, Massachusetts Institute of Technology.
</reference>
<page confidence="0.997028">
260
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872569">
<title confidence="0.997843">Dependency Parsing with an Extended Finite State Approach</title>
<author confidence="0.989618">Kemal Oflazer</author>
<affiliation confidence="0.999983">Department of Computer Engineering Computing Research Laboratory Bilkent University New Mexico State University</affiliation>
<address confidence="0.9999">Ankara, 06533,Turkey Las Cruces, NM, 88003 USA</address>
<email confidence="0.998899">koecs.bilkent.eduArkoecrl.nmsu.edu</email>
<abstract confidence="0.990766153846154">This paper presents a dependency parsing scheme using an extended finite state approach. The parser augments input representation with &amp;quot;channels&amp;quot; so that links representing syntactic dependency relations among words can be accommodated, and iterates on the input a number of times to arrive at a fixed point. Intermediate configurations violating various constraints of projective dependency representations such as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via finite state cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI&apos;96 Robust Parsing Workshop.</booktitle>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via finite state cascades. In Proceedings of the ESSLLI&apos;96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Salah Ait-Mokhtar</author>
<author>Jean-Pierre Chanod</author>
</authors>
<title>Incremental finite-state parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP&apos;97,</booktitle>
<pages>72--79</pages>
<marker>Ait-Mokhtar, Chanod, 1997</marker>
<rawString>Salah Ait-Mokhtar and Jean-Pierre Chanod. 1997. Incremental finite-state parsing. In Proceedings of ANLP&apos;97, pages 72 — 79, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Structure and estimation of a dependency language model.</title>
<date>1997</date>
<booktitle>In Processings of Eurospeech&apos;97.</booktitle>
<marker>Chelba, 1997</marker>
<rawString>Ciprian Chelba and et al. 1997. Structure and estimation of a dependency language model. In Processings of Eurospeech&apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="3362" citStr="Eisner, 1996" startWordPosition="498" endWordPosition="499">dedicated to computational approaches to dependency grammars has been held at COLING/ACL&apos;98 Conference). Jarvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and Jarvinen, 1997; Jarvinen and Tapanainen, 1998). The work of Sleator and Temperley(1991) on link grammar, an essentially lexicalized variant of dependency grammar, has also proved to be interesting in a number of aspects. Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing (Lafferty et al., 1992; Eisner, 1996; Chelba and et al., 1997). Robinson(1970) gives four axioms for well-formed dependency structures, which have been assumed in almost all computational approaches. In a dependency structure of a sentence (i) one and only one word is independent, i.e., not linked to some other word, (ii) all others depend directly on some word, (iii) no word depends on more than one other, and, (iv) if a word A depends directly on B, and some word C intervenes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. This last condition of projectivity (or various </context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340-345, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Light parsing as finitestate filtering.</title>
<date>1996</date>
<booktitle>In ECAI &apos;96 Workshop on Extended finite state models of language.</booktitle>
<contexts>
<context position="1330" citStr="Grefenstette, 1996" startWordPosition="193" endWordPosition="194">o independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), AT&amp;T Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer. Abney(1996) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal</context>
</contexts>
<marker>Grefenstette, 1996</marker>
<rawString>Gregory Grefenstette. 1996. Light parsing as finitestate filtering. In ECAI &apos;96 Workshop on Extended finite state models of language. August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Jarvinen</author>
<author>Pasi Tapanainen</author>
</authors>
<title>Towards an implementable dependency grammar.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL&apos;98 Workshop on Processing Dependency-based Grammars,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="3017" citStr="Jarvinen and Tapanainen, 1998" startWordPosition="445" endWordPosition="448">ages U and L (Kaplan and Kay, 1994). 2 Dependency Syntax Dependency approaches to syntactic representation use the notion of syntactic relation to associate surface lexical items. The book by Mel6uk (1988) presents a comprehensive exposition of dependency syntax. Computational approaches to dependency syntax have recently become quite popular (e.g., a workshop dedicated to computational approaches to dependency grammars has been held at COLING/ACL&apos;98 Conference). Jarvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and Jarvinen, 1997; Jarvinen and Tapanainen, 1998). The work of Sleator and Temperley(1991) on link grammar, an essentially lexicalized variant of dependency grammar, has also proved to be interesting in a number of aspects. Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing (Lafferty et al., 1992; Eisner, 1996; Chelba and et al., 1997). Robinson(1970) gives four axioms for well-formed dependency structures, which have been assumed in almost all computational approaches. In a dependency structure of a sentence (i) one and only one word is independent, i.e., not</context>
</contexts>
<marker>Jarvinen, Tapanainen, 1998</marker>
<rawString>Timo Jarvinen and Pasi Tapanainen. 1998. Towards an implementable dependency grammar. In Proceedings of COLING/ACL&apos;98 Workshop on Processing Dependency-based Grammars, pages 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--3</pages>
<contexts>
<context position="2422" citStr="Kaplan and Kay, 1994" startWordPosition="365" endWordPosition="368">h a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal constituents. This paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of Roche and Abney. The parser produces outputs that encode a labeled dependency tree representation of the syntactic relations between the words in the sentence. We assume that the reader is familiar with the basic concepts of finite state transducers (FST hereafter), finite state devices that map between two regular languages U and L (Kaplan and Kay, 1994). 2 Dependency Syntax Dependency approaches to syntactic representation use the notion of syntactic relation to associate surface lexical items. The book by Mel6uk (1988) presents a comprehensive exposition of dependency syntax. Computational approaches to dependency syntax have recently become quite popular (e.g., a workshop dedicated to computational approaches to dependency grammars has been held at COLING/ACL&apos;98 Conference). Jarvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and Jarvinen, 1997; Jarvinen and Tapanainen, 1998). The</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald M. Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3):331-378, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
<author>Jean-Pierre Chanod</author>
<author>Gregory Grefenstette</author>
<author>Anne Schiller</author>
</authors>
<title>Regular expressions for language engineering.</title>
<date>1996</date>
<journal>Natural Language Engineering,</journal>
<pages>2--4</pages>
<contexts>
<context position="1023" citStr="Karttunen et al., 1996" startWordPosition="142" endWordPosition="145">ation with &amp;quot;channels&amp;quot; so that links representing syntactic dependency relations among words can be accommodated, and iterates on the input a number of times to arrive at a fixed point. Intermediate configurations violating various constraints of projective dependency representations such as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), AT&amp;T Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche</context>
</contexts>
<marker>Karttunen, Chanod, Grefenstette, Schiller, 1996</marker>
<rawString>Lauri Karttunen, Jean-Pierre Chanod, Gregory Grefenstette, and Anne Schiller. 1996. Regular expressions for language engineering. Natural Language Engineering, 2(4):305-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The proper treatment of optimality theory in computational linguistics.</title>
<date>1998</date>
<booktitle>In Lauri Karttunen and Kemal Oflazer, editors, Proceedings of the International Workshop on Finite State Methods in Natural Language ProcessingFSMNLP,</booktitle>
<marker>Karttunen, 1998</marker>
<rawString>Lauri Karttunen. 1998. The proper treatment of optimality theory in computational linguistics. In Lauri Karttunen and Kemal Oflazer, editors, Proceedings of the International Workshop on Finite State Methods in Natural Language ProcessingFSMNLP, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
<author>Pasi Tapanainen</author>
<author>Atro Voutilainen</author>
</authors>
<title>Compiling and using finitestate syntactic rules.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics, COLING-92,</booktitle>
<pages>156--162</pages>
<contexts>
<context position="1310" citStr="Koskenniemi et al., 1992" startWordPosition="189" endWordPosition="192">ch as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), AT&amp;T Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer. Abney(1996) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols </context>
</contexts>
<marker>Koskenniemi, Tapanainen, Voutilainen, 1992</marker>
<rawString>Kimmo Koskenniemi, Pasi Tapanainen, and Atro Voutilainen. 1992. Compiling and using finitestate syntactic rules. In Proceedings of the 14th International Conference on Computational Linguistics, COLING-92, pages 156-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Finite-state parsing and disambiguation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics, COLING&apos;90,</booktitle>
<pages>229--233</pages>
<contexts>
<context position="1284" citStr="Koskenniemi, 1990" startWordPosition="187" endWordPosition="188"> representations such as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), AT&amp;T Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer. Abney(1996) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the inpu</context>
</contexts>
<marker>Koskenniemi, 1990</marker>
<rawString>Kimmo Koskenniemi. 1990. Finite-state parsing and disambiguation. In Proceedings of the 13th International Conference on Computational Linguistics, COLING&apos;90, pages 229 - 233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Grammatical trigrams: A probabilistic model of link grammars.</title>
<date>1992</date>
<booktitle>In Proceedings of the 1992 AAAI Fall Symposium on Probablistic Approaches to Natural Language.</booktitle>
<contexts>
<context position="3348" citStr="Lafferty et al., 1992" startWordPosition="494" endWordPosition="497">ular (e.g., a workshop dedicated to computational approaches to dependency grammars has been held at COLING/ACL&apos;98 Conference). Jarvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and Jarvinen, 1997; Jarvinen and Tapanainen, 1998). The work of Sleator and Temperley(1991) on link grammar, an essentially lexicalized variant of dependency grammar, has also proved to be interesting in a number of aspects. Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing (Lafferty et al., 1992; Eisner, 1996; Chelba and et al., 1997). Robinson(1970) gives four axioms for well-formed dependency structures, which have been assumed in almost all computational approaches. In a dependency structure of a sentence (i) one and only one word is independent, i.e., not linked to some other word, (ii) all others depend directly on some word, (iii) no word depends on more than one other, and, (iv) if a word A depends directly on B, and some word C intervenes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. This last condition of projectivit</context>
</contexts>
<marker>Lafferty, Sleator, Temperley, 1992</marker>
<rawString>John Lafferty, Daniel Sleator, and Davy Temperley. 1992. Grammatical trigrams: A probabilistic model of link grammars. In Proceedings of the 1992 AAAI Fall Symposium on Probablistic Approaches to Natural Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bong Yeung Tom Lai</author>
<author>Changning Huang</author>
</authors>
<title>Dependency grammar and the parsing of Chinese sentences.</title>
<date>1994</date>
<booktitle>In Proceedings of the 1994 Joint Conference of 8th ACLIC and 2nd PaFoCol.</booktitle>
<marker>Lai, Huang, 1994</marker>
<rawString>Bong Yeung Tom Lai and Changning Huang. 1994. Dependency grammar and the parsing of Chinese sentences. In Proceedings of the 1994 Joint Conference of 8th ACLIC and 2nd PaFoCol.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>On the structural complexity of natural language sentences.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96).</booktitle>
<contexts>
<context position="21850" citStr="Lin, 1996" startWordPosition="3572" endWordPosition="3573">e are an additional set of about 25 finite state constraints that impose various syntactic and configurational constraints. The resulting Parser transducer has 2707 states 27,713 transitions while the SyntacticConstraints transducer has 28,894 states and 302,354 transitions. The combined transducer for morphological analysis and (very limited) disambiguation has 87,475 states and 218,082 arcs. Table 1 presents our results for parsing this set of 50 sentences. The number of iterations also count the last iteration where no new links are added. Inspired by Lin&apos;s notion of structural complexity (Lin, 1996), measured by the total length of the links in a dependency parse, we ordered the parses of a sentence using this measure. In 32 out of 50 sentences (64%), the correct parse was either the top ranked parse or among the top ranked parses with the same measure. In 13 out of 50 parses (26%) the correct parse was not among the top ranked parses, but was ranked lower. Since smaller structural complexity requires, for example, verbal adjuncts, etc. to attach to the nearest verb wherever possible, topicalization of such items which brings them to the beginning of the sentence, will generate a long(er</context>
</contexts>
<marker>Lin, 1996</marker>
<rawString>Dekang Lin. 1996. On the structural complexity of natural language sentences. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Meleuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Meleuk, 1988</marker>
<rawString>Igor A. Meleuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando Pereira</author>
<author>Michael Riley</author>
</authors>
<title>A rational design for a weighted finitestate transducer library.</title>
<date>1998</date>
<booktitle>In Lecture Notes in Computer Science,</booktitle>
<pages>1436</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="1056" citStr="Mohri et al., 1998" startWordPosition="148" endWordPosition="151">epresenting syntactic dependency relations among words can be accommodated, and iterates on the input a number of times to arrive at a fixed point. Intermediate configurations violating various constraints of projective dependency representations such as no crossing links, no independent items except sentential head, etc, are filtered via finite state filters. We have applied the parser to dependency parsing of Turkish. 1 Introduction Recent advances in the development of sophisticated tools for building finite state systems (e.g., XRCE Finite State Tools (Karttunen et al., 1996), AT&amp;T Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for </context>
</contexts>
<marker>Mohri, Pereira, Riley, 1998</marker>
<rawString>Mehryar Mohri, Fernando Pereira, and Michael Riley. 1998. A rational design for a weighted finitestate transducer library. In Lecture Notes in Computer Science, 1436. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Two-level description of Turkish morphology.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics, April. A full version appears in Literary and Linguistic Computing, Vol.9 No.2,</booktitle>
<contexts>
<context position="4413" citStr="Oflazer, 1993" startWordPosition="671" endWordPosition="672">ntervenes between them (in linear order), then C depends directly on A or on B, or on some other intervening word. This last condition of projectivity (or various extensions of it; see e.g., Lau and Huang (1994)) is usually assumed by most computational approaches to dependency grammars as a constraint for filtering configurations, and has also been used as a simplifying condition in statistical approaches for inducing dependencies from corpora (e.g., Yaret(1998).) 3 Turkish Turkish is an agglutinative language where a sequence of inflectional and derivational morphemes get affixed to a root (Oflazer, 1993). Derivations are very productive, and the syntactic relations that a word is involved in as a dependent or head element, are determined by the inflectional properties of the 254 a) Input sequence of IGs are augmented with symbols to represent Channels. (IG1) (1G2) (1G3)... (16,) (IQ) b) Links are embedded in channels. (1G1) (IGO (1G3)... (IG.4) (IG.) Links from Dopendonto Link to Head 1G2 IG1 IG3 Word Figure 1: Links and Inflectional Groups one or more (intermediate) derived forms. In this work, we assume that a Turkish word is represented as a sequence of inflectional groups (IGs hereafter),</context>
</contexts>
<marker>Oflazer, 1993</marker>
<rawString>Kemal Oflazer. 1993. Two-level description of Turkish morphology. In Proceedings of the Sixth Conference of the European Chapter of the Association for Computational Linguistics, April. A full version appears in Literary and Linguistic Computing, Vol.9 No.2, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane J Robinson</author>
</authors>
<title>Dependency structures and transformational rules.</title>
<date>1970</date>
<journal>Language,</journal>
<pages>46--2</pages>
<marker>Robinson, 1970</marker>
<rawString>Jane J. Robinson. 1970. Dependency structures and transformational rules. Language, 46(2):259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
</authors>
<title>Parsing with finite state transducers.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 8. The</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1630" citStr="Roche (1997)" startWordPosition="242" endWordPosition="243">1996), AT&amp;T Tools (Mohri et al., 1998)) have fostered the development of quite complex finite state systems for natural language processing. In the last several years, there have been a number of studies on developing finite state parsing systems, (Koskenniemi, 1990; Koskenniemi et al., 1992; Grefenstette, 1996; AitMokhtar and Chanod, 1997). There have also been a number of approaches to natural language parsing using extended finite state approaches in which a finite state engine is applied multiple times to the input, or various derivatives thereof, until some stopping condition is reached. Roche (1997) presents an approach for parsing in which the input is iteratively bracketed using a finite state transducer. Abney(1996) presents a finite state parsing approach in which a tagged sentence is parsed by transducers which progressively transform the input to sequences of symbols representing phrasal constituents. This paper presents an approach to dependency parsing using an extended finite state model resembling the approaches of Roche and Abney. The parser produces outputs that encode a labeled dependency tree representation of the syntactic relations between the words in the sentence. We as</context>
</contexts>
<marker>Roche, 1997</marker>
<rawString>Emmanuel Roche. 1997. Parsing with finite state transducers. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, chapter 8. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-196,</tech>
<institution>Computer Science Department, Carnegie Mellon University.</institution>
<marker>Sleator, Temperley, 1991</marker>
<rawString>Daniel Sleator and Davy Temperley. 1991. Parsing English with a link grammar. Technical Report CMU-CS-91-196, Computer Science Department, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP&apos;97,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="2985" citStr="Tapanainen and Jarvinen, 1997" startWordPosition="441" endWordPosition="444">t map between two regular languages U and L (Kaplan and Kay, 1994). 2 Dependency Syntax Dependency approaches to syntactic representation use the notion of syntactic relation to associate surface lexical items. The book by Mel6uk (1988) presents a comprehensive exposition of dependency syntax. Computational approaches to dependency syntax have recently become quite popular (e.g., a workshop dedicated to computational approaches to dependency grammars has been held at COLING/ACL&apos;98 Conference). Jarvinen and Tapananinen have demonstrated an efficient wide-coverage dependency parser for English (Tapanainen and Jarvinen, 1997; Jarvinen and Tapanainen, 1998). The work of Sleator and Temperley(1991) on link grammar, an essentially lexicalized variant of dependency grammar, has also proved to be interesting in a number of aspects. Dependency-based statistical language modeling and analysis have also become quite popular in statistical natural language processing (Lafferty et al., 1992; Eisner, 1996; Chelba and et al., 1997). Robinson(1970) gives four axioms for well-formed dependency structures, which have been assumed in almost all computational approaches. In a dependency structure of a sentence (i) one and only on</context>
</contexts>
<marker>Tapanainen, Jarvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo Jarvinen. 1997. A nonprojective dependency parser. In Proceedings of ANLP&apos;97, pages 64 - 71, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yiiret</author>
</authors>
<title>Discovery of Linguistic Relations Using Lexical Attraction.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.</institution>
<marker>Yiiret, 1998</marker>
<rawString>Deniz Yiiret. 1998. Discovery of Linguistic Relations Using Lexical Attraction. Ph.D. thesis, Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>