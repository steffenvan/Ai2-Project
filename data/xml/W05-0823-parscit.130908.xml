<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057815">
<title confidence="0.992587">
Statistical Machine Translation of Euparl Data by using Bilingual N-grams
</title>
<author confidence="0.984736">
Rafael E. Banchs Josep M. Crego Adri`a de Gispert Patrik Lambert Jos´e B. Mari˜no
</author>
<affiliation confidence="0.984825">
Department of Signal Theory and Communications
</affiliation>
<address confidence="0.45605">
Universitat Polit`ecnica de Catalunya, Barcelona 08034, Spain
</address>
<email confidence="0.999245">
{rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.edu
</email>
<sectionHeader confidence="0.995646" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995595">
This work discusses translation results for
the four Euparl data sets which were made
available for the shared task “Exploit-
ing Parallel Texts for Statistical Machine
Translation”. All results presented were
generated by using a statistical machine
translation system which implements a
log-linear combination of feature func-
tions along with a bilingual n-gram trans-
lation model.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947655172414">
During the last decade, statistical machine transla-
tion (SMT) systems have evolved from the orig-
inal word-based approach (Brown et al., 1993)
into phrase-based translation systems (Koehn et al.,
2003). Similarly, the noisy channel approach has
been expanded to a more general maximum entropy
approach in which a log-linear combination of mul-
tiple models is implemented (Och and Ney, 2002).
The SMT approach used in this work implements
a log-linear combination of feature functions along
with a translation model which is based on bilingual
n-grams. This translation model was developed by
de Gispert and Mari˜no (2002), and it differs from the
well known phrase-based translation model in two
basic issues: first, training data is monotonously seg-
mented into bilingual units; and second, the model
considers n-gram probabilities instead of relative
frequencies. This model is described in section 2.
Translation results from the four source languages
made available for the shared task (es: Spanish, fr:
French, de: German, and fi: Finnish) into English
(en) are presented and discussed.
The paper is structured as follows. Section 2 de-
scribes the bilingual n-gram translation model. Sec-
tion 3 presents a brief overview of the whole SMT
procedure. Section 4 presents and discusses the
shared task results and other interesting experimen-
tation. Finally, section 5 presents some conclusions
and further work.
</bodyText>
<sectionHeader confidence="0.97097" genericHeader="method">
2 Bilingual N-gram Translation Model
</sectionHeader>
<bodyText confidence="0.999948142857143">
As already mentioned, the translation model used
here is based on bilingual n-grams. It actually con-
stitutes a language model of bilingual units which
are referred to as tuples (de Gispert and Mari˜no,
2002). This model approximates the joint probabil-
ity between source and target languages by using 3-
grams as it is described in the following equation:
</bodyText>
<equation confidence="0.997576333333333">
N
p(T, S) ≈ H p((t, s)nJ(t, s)n−2, (t, s)n−1) (1)
n=1
</equation>
<bodyText confidence="0.999910615384615">
where t refers to target, s to source and (t, s)n to the
nth tuple of a given bilingual sentence pair.
Tuples are extracted from a word-to-word aligned
corpus according to the following two constraints:
first, tuple extraction should produce a monotonic
segmentation of bilingual sentence pairs; and sec-
ond, the produced segmentation is maximal in the
sense that no smaller tuples can be extracted with-
out violating the previous constraint (Crego et al.,
2004). According to this, tuple extraction provides a
unique segmentation for a given bilingual sentence
pair alignment. Figure 1 illustrates this idea with a
simple example.
</bodyText>
<page confidence="0.985835">
133
</page>
<note confidence="0.8383715">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 133–136,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.7731475">
We would like to achieve perfect translations
NULL quisieramos lograr traducciones perfectas
</bodyText>
<equation confidence="0.791726">
t1 t2 t3 t4
</equation>
<figureCaption confidence="0.9981365">
Figure 1: Example of tuple extraction from an
aligned sentence pair.
</figureCaption>
<bodyText confidence="0.9999786875">
Two important issues regarding this translation
model must be mentioned. First, when extracting
tuples, some words always appear embedded into tu-
ples containing two or more words, so no translation
probability for an independent occurrence of such
words exists. To overcome this problem, the tuple
3-gram model is enhanced by incorporating 1-gram
translation probabilities for all the embedded words
(de Gispert et al., 2004).
Second, some words linked to NULL end up pro-
ducing tuples with NULL source sides. This cannot
be allowed since no NULL is expected to occur in a
translation input. This problem is solved by prepro-
cessing alignments before tuple extraction such that
any target word that is linked to NULL is attached
to either its precedent or its following word.
</bodyText>
<sectionHeader confidence="0.998348" genericHeader="method">
3 SMT Procedure Description
</sectionHeader>
<bodyText confidence="0.999873333333333">
This section describes the procedure followed for
preprocessing the data, training the models and op-
timizing the translation system parameters.
</bodyText>
<subsectionHeader confidence="0.999496">
3.1 Preprocessing and Alignment
</subsectionHeader>
<bodyText confidence="0.999991736842106">
The Euparl data provided for this shared task (Eu-
parl, 2003) was preprocessed for eliminating all sen-
tence pairs with a word ratio larger than 2.4. As a
result of this preprocessing, the number of sentences
in each training set was slightly reduced. However,
no significant reduction was produced.
In the case of French, a re-tokenizing procedure
was performed in which all apostrophes appearing
alone were attached to their corresponding words.
For example, pairs of tokens such as l ’ and qu ’
were reduced to single tokens such as l’ and qu’.
Once the training data was preprocessed, a word-
to-word alignment was performed in both direc-
tions, source-to-target and target-to-source, by us-
ing GIZA++ (Och and Ney, 2000). As an approxi-
mation to the most probable alignment, the Viterbi
alignment was considered. Then, the intersection
and union of alignment sets in both directions were
computed for each training set.
</bodyText>
<subsectionHeader confidence="0.999253">
3.2 Feature Function Computation
</subsectionHeader>
<bodyText confidence="0.978948578947369">
The considered translation system implements a to-
tal of five feature functions. The first of these mod-
els is the tuple 3-gram model, which was already de-
scribed in section 2. Tuples for the translation model
were extracted from the union set of alignments as
shown in Figure 1. Once tuples had been extracted,
the tuple vocabulary was pruned by using histogram
pruning. The same pruning parameter, which was
actually estimated for Spanish-English, was used for
the other three language pairs. After pruning, the
tuple 3-gram model was trained by using the SRI
Language Modeling toolkit (Stolcke, 2002). Finally,
the obtained model was enhanced by incorporating
1-gram probabilities for the embedded word tuples,
which were extracted from the intersection set of
alignments.
Table 1 presents the total number of running
words, distinct tokens and tuples, for each of the four
training data sets.
</bodyText>
<tableCaption confidence="0.998905">
Table 1: Total number of running words, distinct to-
kens and tuples in training.
</tableCaption>
<table confidence="0.9993165">
source running distinct tuple
language words tokens vocabulary
Spanish 15670801 113570 1288770
French 14844465 78408 1173424
German 15207550 204949 1391425
Finnish 11228947 389223 1496417
</table>
<bodyText confidence="0.99700775">
The second feature function considered was a tar-
get language model. This feature actually consisted
of a word 3-gram model, which was trained from the
target side of the bilingual corpus by using the SRI
Language Modeling toolkit.
The third feature function was given by a word
penalty model. This function introduces a sentence
length penalization in order to compensate the sys-
</bodyText>
<page confidence="0.994509">
134
</page>
<bodyText confidence="0.999951666666667">
tem preference for short output sentences. More
specifically, the penalization factor was given by the
total number of words contained in the translation
hypothesis.
Finally, the fourth and fifth feature functions cor-
responded to two lexicon models based on IBM
Model 1 lexical parameters p(t1s) (Brown et al.,
1993). These lexicon models were calculated for
each tuple according to the following equation:
</bodyText>
<equation confidence="0.96322475">
plexicon((t, s)n) = J I
1
��p(ti n|sj n) (2)
(I + 1)J j�� i��
</equation>
<bodyText confidence="0.99987625">
where sjn and tin are the jth and ith words in the
source and target sides of tuple (t, s)n, being J and
I the corresponding total number words in each side
of it.
The forward lexicon model uses IBM Model 1 pa-
rameters obtained from source-to-target alignments,
while the backward lexicon model uses parameters
obtained from target-to-source alignments.
</bodyText>
<subsectionHeader confidence="0.999064">
3.3 Decoding and Optimization
</subsectionHeader>
<bodyText confidence="0.999798">
The search engine for this translation system was
developed by Crego et al. (2005). It implements
a beam-search strategy based on dynamic program-
ming and takes into account all the five feature func-
tions described above simultaneously. It also allows
for three different pruning methods: threshold prun-
ing, histogram pruning, and hypothesis recombina-
tion. For all the results presented in this work the
decoder’s monotonic search modality was used.
An optimization tool, which is based on a simplex
method (Press et al., 2002), was developed and used
for computing log-linear weights for each of the fea-
ture functions described above. This algorithm ad-
justs the log-linear weights so that BLEU (Papineni
et al., 2002) is maximized over a given development
set. One optimization for each language pair was
performed by using the 2000-sentence development
sets made available for the shared task.
</bodyText>
<sectionHeader confidence="0.994137" genericHeader="method">
4 Shared Task Results
</sectionHeader>
<bodyText confidence="0.78534">
Table 2 presents the BLEU scores obtained for the
shared task test data. Each test set consisted of 2000
sentences. The computed BLEU scores were case
insensitive and used one translation reference.
</bodyText>
<tableCaption confidence="0.959201">
Table 2: BLEU scores (shared task test sets).
</tableCaption>
<bodyText confidence="0.950291">
es - en fr - en de - en fi - en
0.3007 0.3020 0.2426 0.2031
As can be seen from Table 2 the best ranked trans-
lations were those obtained for French, followed by
Spanish, German and Finnish. A big difference is
observed between the best and the worst results.
Differences can be observed from translation out-
puts too. Consider, for example, the following seg-
ments taken from one of the test sentences:
</bodyText>
<construct confidence="0.927114666666667">
es-en: We know very well that the present Treaties are not
enough and that, in the future , it will be necessary to develop
a structure better and different for the European Union...
fr-en: We know very well that the Treaties in their current
are not enough and that it will be necessary for the future to
develop a structure more effective and different for the Union...
de-en: We very much aware that the relevant treaties are
inadequate and, in future to another, more efficient structure
for the European Union that must be developed...
fi-en: We know full well that the current Treaties are not
sufficient and that, in the future , it is necessary to develop the
Union better and a different structure...
</construct>
<bodyText confidence="0.999914636363637">
It is evident from these translation outputs that
translation quality decreases when moving from
Spanish and French to German and Finnish. A
detailed observation of translation outputs reveals
that there are basically two problems related to this
degradation in quality. The first has to do with re-
ordering, which seems to be affecting Finnish and,
specially, German translations.
The second problem has to do with vocabulary. It
is well known that large vocabularies produce data
sparseness problems (Koehn, 2002). As can be con-
firmed from Tables 1 and 2, translation quality de-
creases as vocabulary size increases. However, it is
not clear yet, in which degree such degradation is
due to monotonic decoding and/or vocabulary size.
Finally, we also evaluated how much the full fea-
ture function system differs from the baseline tu-
ple 3-gram model alone. In this way, BLEU scores
were computed for translation outputs obtained for
the baseline system and the full system. Since the
English reference for the test set was not available,
we computed translations and BLEU scores over de-
</bodyText>
<page confidence="0.99759">
135
</page>
<bodyText confidence="0.9720355">
velopment sets. Table 3 presents the results for both
the full system and the baseline.1
</bodyText>
<tableCaption confidence="0.9653">
Table 3: Baseline- and full-system BLEU scores
(computed over development sets).
</tableCaption>
<table confidence="0.9977454">
language pair baseline full
es - en 0.2588 0.3004
fr - en 0.2547 0.2938
de - en 0.1844 0.2350
fi - en 0.1526 0.1989
</table>
<bodyText confidence="0.998479">
From Table 3, it is evident that the four additional
feature functions produce important improvements
in translation quality.
</bodyText>
<sectionHeader confidence="0.997491" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999898692307692">
As can be concluded from the presented results, per-
formance of the translation system used is much bet-
ter for French and Spanish than for German and
Finnish. As some results suggest, reordering and
vocabulary size are the most important problems re-
lated to the low translation quality achieved for Ger-
man and Finnish.
It is also evident that the bilingual n-gram model
used requires the additional feature functions to pro-
duce better translations. However, more experimen-
tation is required in order to fully understand each
individual feature’s influence on the overall log-
linear model performance.
</bodyText>
<sectionHeader confidence="0.999437" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999974714285714">
This work has been funded by the European Union
under the integrated project TC-STAR - Technology
and Corpora for Speech to Speech Translation -(IST-
2002-FP6-506738,http://www.tc-star.org).
The authors also want to thank Jos´e A. R. Fonol-
losa and Marta Ruiz Costa-juss`a for their participa-
tion in discussions related to this work.
</bodyText>
<sectionHeader confidence="0.998853" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996744754716982">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. “The mathemat-
1Differently from BLEU scores presented in Table 2, which
are case insensitive, BLEU scores presented in Table 3 are case
sensitive.
ics of statistical machine translation: parameter esti-
mation”. Computational Linguistics, 19(2):263–311.
Josep M. Crego, Jos´e B. Mari˜no, and Adri`a de Gispert.
2004. “Finite-state-based and phrase-based statistical
machine translation”. Proc. of the 8th Int. Conf. on
Spoken Language Processing, :37–40, October.
Josep M. Crego, Jos´e B. Mari˜no, and Adri`a de Gispert.
2005. “A Ngram-based Statistical Machine Transla-
tion Decoder”. Submitted to INTERSPEECH 2005.
Adri`a de Gispert, and Jos´e B. Mari˜no. 2002. “Using X-
grams for speech-to-speech translation”. Proc. of the
7th Int. Conf. on Spoken Language Processing.
Adri`a de Gispert, Jos´e B. Mari˜no, and Josep M. Crego.
2004. “TALP: Xgram-based spoken language transla-
tion system”. Proc. of the Int. Workshop on Spoken
Language Translation, :85–90. Kyoto, Japan, October.
EUPARL: European Parliament Proceedings Parallel
Corpus 1996-2003. Available on-line at: http://
people.csail.mit.edu/people/koehn/public
ations/europarl/
Philipp Koehn. 2002. “Europarl: A Multilingual Cor-
pus for Evaluation of Machine Translation”. Avail-
able on-line at: http://people.csail.mit.edu/
people/koehn/publications/europarl/
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
“Statistical phrase-based translation”. Proc. of the
2003 Meeting of the North American chapter of the
ACL, Edmonton, Alberta.
Franz J. Och and Hermann Ney. 2000. “Improved statis-
tical alignment models”. Proc. of the 38th Ann. Meet-
ing of the ACL, Hong Kong, China, October.
Franz J. Och and Hermann Ney. 2002. “Discriminative
training and maximum entropy models for statistical
machine translation”. Proc. of the 40th Ann. Meeting
of the ACL, :295–302, Philadelphia, PA, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. “Bleu: a method for automatic eval-
uation of machine translation”. Proc. of the 40th Ann.
Conf. of the ACL, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing, Cambridge
University Press.
Andreas Stolcke. 2002. “SRLIM: an extensible language
modeling toolkit”. Proc. of the Int. Conf. on Spoken
Language Processing :901–904, Denver, CO, Septem-
ber. Available on line at: http://www.speech.sr
i.com/projects/srilm/
</reference>
<page confidence="0.998764">
136
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697331">
<title confidence="0.999786">Statistical Machine Translation of Euparl Data by using Bilingual N-grams</title>
<author confidence="0.992551">E Banchs Josep M Crego Adri`a de_Gispert Patrik Lambert Jos´e B</author>
<affiliation confidence="0.861447">Department of Signal Theory and Universitat Polit`ecnica de Catalunya, Barcelona 08034, Spain</affiliation>
<abstract confidence="0.995691636363636">This work discusses translation results for the four Euparl data sets which were made for the shared task “Exploiting Parallel Texts for Statistical Machine All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathemat1Differently from BLEU scores presented in Table 2, which are case insensitive, BLEU scores presented in Table 3 are case sensitive. ics of statistical machine translation: parameter estimation”.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="868" citStr="Brown et al., 1993" startWordPosition="115" endWordPosition="118">na 08034, Spain {rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.edu Abstract This work discusses translation results for the four Euparl data sets which were made available for the shared task “Exploiting Parallel Texts for Statistical Machine Translation”. All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonou</context>
<context position="7337" citStr="Brown et al., 1993" startWordPosition="1136" endWordPosition="1139"> actually consisted of a word 3-gram model, which was trained from the target side of the bilingual corpus by using the SRI Language Modeling toolkit. The third feature function was given by a word penalty model. This function introduces a sentence length penalization in order to compensate the sys134 tem preference for short output sentences. More specifically, the penalization factor was given by the total number of words contained in the translation hypothesis. Finally, the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p(t1s) (Brown et al., 1993). These lexicon models were calculated for each tuple according to the following equation: plexicon((t, s)n) = J I 1 ��p(ti n|sj n) (2) (I + 1)J j�� i�� where sjn and tin are the jth and ith words in the source and target sides of tuple (t, s)n, being J and I the corresponding total number words in each side of it. The forward lexicon model uses IBM Model 1 parameters obtained from source-to-target alignments, while the backward lexicon model uses parameters obtained from target-to-source alignments. 3.3 Decoding and Optimization The search engine for this translation system was developed by C</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. “The mathemat1Differently from BLEU scores presented in Table 2, which are case insensitive, BLEU scores presented in Table 3 are case sensitive. ics of statistical machine translation: parameter estimation”. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
<author>Adri`a de Gispert</author>
</authors>
<title>Finite-state-based and phrase-based statistical machine translation”.</title>
<date>2004</date>
<booktitle>Proc. of the 8th Int. Conf. on Spoken Language Processing,</booktitle>
<pages>37--40</pages>
<marker>Crego, Mari˜no, de Gispert, 2004</marker>
<rawString>Josep M. Crego, Jos´e B. Mari˜no, and Adri`a de Gispert. 2004. “Finite-state-based and phrase-based statistical machine translation”. Proc. of the 8th Int. Conf. on Spoken Language Processing, :37–40, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mari˜no</author>
<author>Adri`a de Gispert</author>
</authors>
<title>A Ngram-based Statistical Machine Translation Decoder”. Submitted to INTERSPEECH</title>
<date>2005</date>
<marker>Crego, Mari˜no, de Gispert, 2005</marker>
<rawString>Josep M. Crego, Jos´e B. Mari˜no, and Adri`a de Gispert. 2005. “A Ngram-based Statistical Machine Translation Decoder”. Submitted to INTERSPEECH 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Jos´e B Mari˜no</author>
</authors>
<title>Using Xgrams for speech-to-speech translation”.</title>
<date>2002</date>
<booktitle>Proc. of the 7th Int. Conf. on Spoken Language Processing.</booktitle>
<marker>de Gispert, Mari˜no, 2002</marker>
<rawString>Adri`a de Gispert, and Jos´e B. Mari˜no. 2002. “Using Xgrams for speech-to-speech translation”. Proc. of the 7th Int. Conf. on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Jos´e B Mari˜no</author>
<author>Josep M Crego</author>
</authors>
<title>TALP: Xgram-based spoken language translation system”.</title>
<date>2004</date>
<booktitle>Proc. of the Int. Workshop on Spoken Language Translation, :85–90. Kyoto,</booktitle>
<location>Japan,</location>
<marker>de Gispert, Mari˜no, Crego, 2004</marker>
<rawString>Adri`a de Gispert, Jos´e B. Mari˜no, and Josep M. Crego. 2004. “TALP: Xgram-based spoken language translation system”. Proc. of the Int. Workshop on Spoken Language Translation, :85–90. Kyoto, Japan, October.</rawString>
</citation>
<citation valid="false">
<authors>
<author>EUPARL European</author>
</authors>
<title>Parliament Proceedings Parallel Corpus 1996-2003. Available on-line at: http:// people.csail.mit.edu/people/koehn/public ations/europarl/</title>
<marker>European, </marker>
<rawString>EUPARL: European Parliament Proceedings Parallel Corpus 1996-2003. Available on-line at: http:// people.csail.mit.edu/people/koehn/public ations/europarl/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation”. Available on-line at:</title>
<date>2002</date>
<note>http://people.csail.mit.edu/ people/koehn/publications/europarl/</note>
<contexts>
<context position="10660" citStr="Koehn, 2002" startWordPosition="1691" endWordPosition="1692">nd that, in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when moving from Spanish and French to German and Finnish. A detailed observation of translation outputs reveals that there are basically two problems related to this degradation in quality. The first has to do with reordering, which seems to be affecting Finnish and, specially, German translations. The second problem has to do with vocabulary. It is well known that large vocabularies produce data sparseness problems (Koehn, 2002). As can be confirmed from Tables 1 and 2, translation quality decreases as vocabulary size increases. However, it is not clear yet, in which degree such degradation is due to monotonic decoding and/or vocabulary size. Finally, we also evaluated how much the full feature function system differs from the baseline tuple 3-gram model alone. In this way, BLEU scores were computed for translation outputs obtained for the baseline system and the full system. Since the English reference for the test set was not available, we computed translations and BLEU scores over de135 velopment sets. Table 3 pre</context>
</contexts>
<marker>Koehn, 2002</marker>
<rawString>Philipp Koehn. 2002. “Europarl: A Multilingual Corpus for Evaluation of Machine Translation”. Available on-line at: http://people.csail.mit.edu/ people/koehn/publications/europarl/</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation”.</title>
<date>2003</date>
<booktitle>Proc. of the 2003 Meeting of the North American chapter of the ACL,</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="927" citStr="Koehn et al., 2003" startWordPosition="123" endWordPosition="126">ps.tsc.upc.edu Abstract This work discusses translation results for the four Euparl data sets which were made available for the shared task “Exploiting Parallel Texts for Statistical Machine Translation”. All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model c</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. “Statistical phrase-based translation”. Proc. of the 2003 Meeting of the North American chapter of the ACL, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models”.</title>
<date>2000</date>
<booktitle>Proc. of the 38th Ann. Meeting of the ACL,</booktitle>
<location>Hong Kong, China,</location>
<contexts>
<context position="5243" citStr="Och and Ney, 2000" startWordPosition="808" endWordPosition="811">tence pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ’ and qu ’ were reduced to single tokens such as l’ and qu’. Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. 3.2 Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1. Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. “Improved statistical alignment models”. Proc. of the 38th Ann. Meeting of the ACL, Hong Kong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation”.</title>
<date>2002</date>
<booktitle>Proc. of the 40th Ann. Meeting of the ACL,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1115" citStr="Och and Ney, 2002" startWordPosition="153" endWordPosition="156">e Translation”. All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. 1 Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003). Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002). The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mari˜no (2002), and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task (</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. “Discriminative training and maximum entropy models for statistical machine translation”. Proc. of the 40th Ann. Meeting of the ACL, :295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation”.</title>
<date>2002</date>
<booktitle>Proc. of the 40th Ann. Conf. of the ACL,</booktitle>
<location>Philadelphia, PA,</location>
<contexts>
<context position="8590" citStr="Papineni et al., 2002" startWordPosition="1341" endWordPosition="1344">ts a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder’s monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002), was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. 4 Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. Table 2: BLEU scores (shared task test sets). es - en fr - en de - en fi - en 0.3007 0.3020 0.2426 0.2031 As can be seen from Table 2 the best ranked translations were those obtained for French, follow</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. “Bleu: a method for automatic evaluation of machine translation”. Proc. of the 40th Ann. Conf. of the ACL, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<date>2002</date>
<booktitle>Numerical Recipes in C++: the Art of Scientific Computing,</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8399" citStr="Press et al., 2002" startWordPosition="1310" endWordPosition="1313">del uses parameters obtained from target-to-source alignments. 3.3 Decoding and Optimization The search engine for this translation system was developed by Crego et al. (2005). It implements a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder’s monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002), was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. 4 Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. Table 2: B</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2002</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2002. Numerical Recipes in C++: the Art of Scientific Computing, Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRLIM: an extensible language modeling toolkit”.</title>
<date>2002</date>
<booktitle>Proc. of the Int. Conf. on Spoken Language Processing :901–904,</booktitle>
<location>Denver, CO,</location>
<note>Available on line at: http://www.speech.sr i.com/projects/srilm/</note>
<contexts>
<context position="6076" citStr="Stolcke, 2002" startWordPosition="943" endWordPosition="944"> Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1. Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning. The same pruning parameter, which was actually estimated for Spanish-English, was used for the other three language pairs. After pruning, the tuple 3-gram model was trained by using the SRI Language Modeling toolkit (Stolcke, 2002). Finally, the obtained model was enhanced by incorporating 1-gram probabilities for the embedded word tuples, which were extracted from the intersection set of alignments. Table 1 presents the total number of running words, distinct tokens and tuples, for each of the four training data sets. Table 1: Total number of running words, distinct tokens and tuples in training. source running distinct tuple language words tokens vocabulary Spanish 15670801 113570 1288770 French 14844465 78408 1173424 German 15207550 204949 1391425 Finnish 11228947 389223 1496417 The second feature function considered</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. “SRLIM: an extensible language modeling toolkit”. Proc. of the Int. Conf. on Spoken Language Processing :901–904, Denver, CO, September. Available on line at: http://www.speech.sr i.com/projects/srilm/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>