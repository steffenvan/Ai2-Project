<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001238">
<title confidence="0.990029">
Extended Topic Model for Word Dependency
</title>
<author confidence="0.999269">
Tong Wang&apos;, Vish Viswanath2 and Ping Chen&apos;
</author>
<affiliation confidence="0.8477895">
&apos;University of Massachusetts Boston, Boston, MA
2Harvard School of Public Health, Boston, MA
</affiliation>
<email confidence="0.925678">
tongwang0001@gmail.com, Ping.Chen@umb.edu
Vish Viswanath@dfci.harvard.edu
</email>
<sectionHeader confidence="0.992513" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999740578947368">
Topic Model such as Latent Dirichlet
Allocation(LDA) makes assumption that
topic assignment of different words are
conditionally independent. In this paper,
we propose a new model Extended Global
Topic Random Field (EGTRF) to model
non-linear dependencies between words.
Specifically, we parse sentences into de-
pendency trees and represent them as a
graph, and assume the topic assignment of
a word is influenced by its adjacent words
and distance-2 words. Word similarity in-
formation learned from large corpus is in-
corporated to enhance word topic assign-
ment. Parameters are estimated efficiently
by variational inference and experimen-
tal results on two datasets show EGTRF
achieves lower perplexity and higher log
predictive probability.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999021509090909">
Probabilistic topic model such as Latent Dirich-
let Allocation(LDA) (Blei et al, 2003) has been
widely used for discovering latent topics from
document collections by capturing words’ co-
occuring relation. However, the “bag of words”
assumption is employed in most existing topic
models, it assumes the order of words can be ig-
nored and topic assignment of each word is condi-
tionally independent given the topic mixture of a
document.
To relax the “bag of words” assumption, many
extended topic models have been proposed to ad-
dress the limitation of conditional independence.
Wallach (Wallach, 2006) explores a hierarchical
generative probabilistic model that incorporates
both n-gram statistics and latent topic variables.
Gruber (Gruber et al, 2007) models the topics of
words in the document as a Markov chain, and as-
sumes all words in the same sentence are more
likely to have the same topic. Zhu (Zhu et al,
2010) incorporates Markov dependency between
topic assignments of neighboring words, and em-
ploys a general structure of the GLM to define
a conditional distribution of latent topic assign-
ments over words. Most of the models above are
limited to model linear topical dependencies be-
tween words, word topical dependencies can also
be modeled by a non-linear way. In Syntactic topic
models (Boyd-Graber et al, 2009), each word of
a sentence is generated by a distribution that com-
bines document-specific topic weights and parse-
tree-specific syntactic transitions.
In Global Topic Random Field(GTRF)
model (Li et al, 2014), sentences of a document
are parsed into dependency trees (Marneffe et
al, 2008) (Manning et al, 2014) (Marneffe et
al, 2006). They show topics of semantically
or syntactically dependent words achieve the
highest similarity and are able to provide more
useful information in topic modeling, which is
also the basic assumption of our model. Then
they propose GTRF to model non-linear topical
dependencies, word topics are sampled based
on graph structure instead of “bag of words”
representation, the conditional independence of
word topic assignment is thus relaxed.
However, GTRF assumes topic assignment of a
word vertex depends on the topic mixture of the
document and its neighboring word vertices, ig-
noring the fact that word vertex can also be influ-
enced by the distance-2 or further word vertices.
In this paper, we extend GTRF model and present
a novel model Extended Global Topic Random
Field (EGTRF) to exploit topical dependency be-
tween words. In EGTRF, the topic assignment of a
</bodyText>
<footnote confidence="0.585741">
word is assumed to depend on both distance-1 and
distance-2 word vertices. An example of a simple
document that has two sentences shows in Figure
1. The two sentences are parsed into dependency
trees respectively, and then merged into a graph.
506
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 506–510,
</footnote>
<page confidence="0.295458">
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</page>
<figure confidence="0.983156">
stands
stands
LDA alloation
(a) Sentence 2 (c) Document
Example document: LDA stands for latent dirichlet allocation. It discovers
latent topcis from corpus.
Example word vertex: allocation
Distance-1 word vertics: {stands, latent, dirichlet}
Distance-2 word vertics: {LDA, topics}
</figure>
<figureCaption confidence="0.999867">
Figure 1: Dependency tree example
</figureCaption>
<subsectionHeader confidence="0.994792">
2.1 Extended Global Random Field
</subsectionHeader>
<bodyText confidence="0.997268071428572">
After representing document to undirected graph
on previous section, we extend Global Random
Field and give the definition of Extended Global
Random Field to model the graph as below:
Given an undirected graph G, word vertex set is
denoted as W = {wi|i = 1, 2, ..n}, where wi is a
word vertex, and n is the number of unique words
in a document. E1 is distance-1 edge set, E1 =
{(wi, wj)|∃path between wi, wj that length is 1}.
E2 is distance-2 edge set, E2 =
{(wi, wj)|∃path between wi, wj that length is 2}.
The state(topic assignment) of a word vertex w is
generated from Z = {zi|i = 1, 2, ..., k}, k is the
number of topics.
</bodyText>
<figure confidence="0.9838216">
latent dirichlet
(a) Sentence 1
discovers
It topics corpus
latent
LDA alloation
latent dirichlet
topics
discovers
it corpus
</figure>
<equation confidence="0.973829">
P(G) = fG(g) = 1 E1  |+  |E2 |wH f(zw)×
</equation>
<bodyText confidence="0.9999712">
Some hidden dependency relations can also be ex-
tracted by merging dependency trees. For exam-
ple, word “allocation” has a new distance-2 word
“topics” after merging. Therefore, EGTRF can
exploit more semantically or syntactically word
dependencies. Theoretically, we can also model
the distance further than 2, however, it leads to
more complicated computation and small increase
of performance.
Another advantage of EGTRF is it incorporates
word features. The word vector representations
are very interesting because the learned vectors
explicitly encode many linguistic regularities and
patterns (Mikolov et al, 2013). We use the pre-
trained model from Google News dataset(about
100 billion words) using word2vec1 tool to repre-
sent each word as a 300-dimensional word vector,
and apply normalized word similarity as a con-
fidence score to indicate how possible two word
vertices share same topic.
We organized the paper as below: EGTRF is
presented in Section 2, variational inference and
parameter estimation are derived in Section 3, ex-
periments on two datasets are showed in Section
4, we conclude the paper in Section 5.
</bodyText>
<sectionHeader confidence="0.757279" genericHeader="method">
2 Extended Global Topic Random Field
</sectionHeader>
<bodyText confidence="0.9996206">
In this section, we first present Extended Global
Random Field(EGRF) in section 2.1, then show
how to model topical dependencies using EGRF
in section 2.2. We incorporate word similarity in-
formation into model in section 2.3.
</bodyText>
<footnote confidence="0.518333">
1https://code.google.com/p/word2vec/
</footnote>
<equation confidence="0.998377625">
s.t. 1.f(z) &gt; 0, f(1)(z0, z00) &gt; 0, f(2)(z0, z00) &gt; 0
X
2.
z∈Z
3. X f(z0)f(z00)f(1)(z0, z00) = 1
z0,z00∈Z
4. X f(z0)f(z00)f(2)(z0, z00) = 1
z0,z00∈Z
</equation>
<bodyText confidence="0.9999312">
In Equation (1), f(z) is the function defined on
word vertex, which is a probability measure be-
cause of the constraints 1 and 2. f(1)(z, z&apos;) and
f(2)(z, z&apos;) are the function defined on edge set E1
and E2. f(1) and f(2) are not necessarily probabil-
ity measure, however, summing over all possible
states of the product of the edge and the linked
word pair should equal to 1, which are from con-
straints 3 and 4. So f(z&apos;)f(z&apos;&apos;)f(1)(z&apos;, z&apos;&apos;) and
f(z&apos;)f(z&apos;&apos;)f(2)(z&apos;, z&apos;&apos;) are probability measure. g
is one sample of word topic assignments from
graph G. If Equation (1) satisfies all the four con-
straints, it is easy to verify P(G) is also a prob-
ability measure since summing over all possible
samples g equals to 1.
We define the random field as in Equation (1)
a Extended Global Random Field (EGRF). And
EGRF does not have normalization factor, which
is much simplier than models with intractable nor-
malizing factor.
</bodyText>
<subsectionHeader confidence="0.782177">
2.2 Topic Model Using EGRF
</subsectionHeader>
<bodyText confidence="0.9997725">
We define Extended Global Topic Random Field
based on EGRF. EGTRF is a generative proba-
</bodyText>
<equation confidence="0.99992075">
( X Xf(1)(zw0 1 , zw00 1 ) + f(2)(zw02 , zw002 ))
(w01,w001 )∈E1 (w02,w002 )∈E2
(1)
f(z) = 1
</equation>
<page confidence="0.977897">
507
</page>
<bodyText confidence="0.9994465">
bilistic model, the basic idea is that documents
are represented as mixtures of topics, words are
generated depending on the topic mixtures and
graph structure of current document. The genera-
tive process for word sequence of a document is
described as below:
For each document d in corpus D:
Transform document d into graph.
Choose θ ∼ Dir(α).
For each of the n words wn in d:
Choose topic zn ∼ Pegrf(z  |θ),
Choose word wn ∼ Multi(Qzn,wn).
Given Dirichlet prior α, word distribution of topics
Q, topic mixture of document θ, topic assignments
z and words w. We obtain the marginal distribu-
tion of a document:
</bodyText>
<equation confidence="0.988701">
Z X
p(w  |α, β) = P (θ  |α)
z
</equation>
<bodyText confidence="0.999731857142857">
We can see the marginal distribution is similar
to LDA except topic assignment of word is sam-
pled by Extended Global Random Field instead
of Multinomial. So the word topic assignment is
no longer conditionally independent. According
to EGRF described in section 2.1, we define the
probability of topic sequence z as below:
</bodyText>
<equation confidence="0.999313">
1 Y
Pegrf(z  |θ) = f(zw)×
 |E1  |+  |E2  |w∈V
X X
θ2 i λ1 + (1 − θ2i )λ2 = 1 i = 1, 2,..|E1 |(7)
X X
θ2 i λ3 + (1 − θ2 i )λ4 = 1 i = 1, 2, ..|E2 |(8)
</equation>
<bodyText confidence="0.9984365">
Lower A2, A4 give higher reward to the edge
that connects two word vertices with same topic.
If (7) and (8) hold true, Equation (3) is an EGRF.
And we define the topic model based on EGRF as
Extended Global Topic Random Field(EGTRF).
If |E2 |= 0, |E1 |=6 0, EGTRF is equivalent to
GTRF. If |E1 |= 0, |E2 |= 0, EGTRF is equiva-
lent to LDA.
</bodyText>
<subsectionHeader confidence="0.998764">
2.3 Word Similarity Information
</subsectionHeader>
<bodyText confidence="0.987069">
The coherent edge is the edge that the two linked
words have same topic. In distance-i edge set,
i= 1, 2. EC, includes all coherent edges, ENC,
contains all non-coherent edges. Then equation
(3) can be represented as below:
</bodyText>
<equation confidence="0.9996408">
Pegrf(z |θ)
1 Y = Multi(zw  |θ)×
 |E1  |+  |E2  |w∈V
( |EC1  |λ1+  |ENC1  |λ2+  |EC2  |λ3+  |ENC2  |λ4)
Q
w∈V
= × ( |EC1  |(1 − λ2)+  |E1  |λ2θT θ+
( |E1  |+  |E2 |)θT θ
 |EC2  |(1 − λ4)+  |E2  |λ4θT θ)
(9)
</equation>
<bodyText confidence="0.98346625">
From the second line to the third line of Equa-
tion (9), we represent A1, A3 as the function of
A2, A4 based on (7) and (8). The expectation of
the number of edges in Ec, can be computed as:
</bodyText>
<equation confidence="0.994616692307693">
Pegrf (z  |θ) Y P(wn  |zwn , β)dθ
n
(2)
Multi(zw  |θ)
X Xf(1)(zw01 , zw001 ) + f(2)(zw02 , zw002 )) X φTw1 φw2 Sw1,w2 (10)
( (w02,w002 )∈E2 (3) E( |ECi |) =
(w01,w00 (w1,w2)∈Ei
1 )∈E1
where f(zw) = Multi(zw|θ) (4)
f(1)(zw01 , zw00 1 ) = σzw0 =zw00 λ1 + σzw0 6=zw00 λ2 (5)
1 1 1 1
f(2)(zw0 2 , zw002 ) = σzw0 =zw00 λ3 + σzw0 6=zw00 λ4 (6)
2 2 2 2
</equation>
<bodyText confidence="0.999782230769231">
σ is an indicator function and equals 1 if the
topic assignments of two words on an edge are
same. In order to model Equation (3) as an EGRF,
it must satisfy all the four constraints in Equation
(1). Equation (4) defines word vertex as multino-
mial distribution, and we assign A1, A2, A3 and A4
nonzero values, then it is clear to verify constraint
1 and 2 are satisfied. To satisfy the constraint 3
and 4, combine with (5), (6), we get the relation
between A1 and A2, A3 and A4.
φ is the K dimensional variational multinomial
parameters and can be thought as the posterior
probability of a word given the topic assignment.
Sw1,w2 is the similarity measure between word w1
and w2.
As we discussed in section 1, word similarity
information Sw1,w2 works as a confidence score to
model how likely two words on an edge have same
topic. And we make assumption that two words
are more likely to have same topic if they have a
higher similarity score. To get the similarity score
between words, we use word2vec tool to learn the
word representation of each word from pre-trained
model. The word representations are computed
using neural networks, and the learned representa-
tions explicitly encode many linguistic regularities
</bodyText>
<page confidence="0.996206">
508
</page>
<figureCaption confidence="0.998583">
Figure 2: Experimental results on NIPS(left) and 20 news(right) data
</figureCaption>
<bodyText confidence="0.999874833333333">
and patterns from the corpus. Normalized similar-
ity between word vectors can be regarded as the
confidence score of how possible two words have
same topic. In this way, knowledge from large cor-
pus other than current document collections is in-
corporated to guide topic modeling.
</bodyText>
<sectionHeader confidence="0.9415095" genericHeader="method">
3 Posterior Inference and Parameter
Estimation
</sectionHeader>
<bodyText confidence="0.9936774">
We derive Variational Inference for posterior in-
ference. The variational function q is same to the
original LDA paper (Blei et al, 2003). All terms
except P(z|O) in likelihood function are also same
to LDA, Based on Equation (9), we obtain:
</bodyText>
<equation confidence="0.936534125">
Eq[log Pegrf (z  |θ)]
r1 Multi(zwn  |θ))]+
≈Eq[log(
n
(  |E1  |λ2+  |E2  |λ4  |E1  |+  |E2 |
− )Eq(θT θ)+
ζ1 ζ2
log ζ1 − log ζ2
</equation>
<bodyText confidence="0.999995125">
We get the approximation in Equation (11)
from Taylor series, where (1 and (2 are Taylor
approximation. EQ( |ECi |) is obtained directly
from (10), EQ(OTO) is from the property of Dirich-
let distribution. The updating rule of α and Q are
same to LDA, -y is updated using Newton method
since we can not obtain the direct updating rule for
-y. 0 can be approximated as:
</bodyText>
<equation confidence="0.998669666666667">
1 −1λ2 +
(wn,wm)∈E1
φwp,i Sp,n) (12)
</equation>
<bodyText confidence="0.920163">
EM algorithm is applied using above updating
rules. At E-step, we estimate the best -y and 0
given current α and Q. At M-step, we update new
α and Q based on obtained -y and 0. We run such
iterations until convergence.
</bodyText>
<sectionHeader confidence="0.996648" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<bodyText confidence="0.997096166666667">
In this section we study the empirical performance
of EGTRF on two datasets. For each dataset, we
remove very short documents, and compute a vo-
cabulary by removing stop words, rare words, fre-
quent words. Eighty percent data are used for
training, others for testing.
</bodyText>
<listItem confidence="0.947193714285714">
• 20 News Groups: After processing, it con-
tains 13706 documents with a vocabulary of
5164 terms.
• NIPS data (Globerson et al, 2004): Span-
ning from 2000 to 2005. After processing,
it contains 843 documents with a vocabulary
of 6098 terms.
</listItem>
<bodyText confidence="0.999977384615385">
We evaluate how well a model fits the data with
held-out perplexity (Blei et al, 2003) and predic-
tive distribution (Hoffman et al, 2013). Lower
perplexity, higher log predictive probability indi-
cate better generalization performance. We im-
plement GTRF without adding self defined edges
from the original paper, and set A2 = 0.2 to give
higher reward to edges from E1 that the two word
vertices have same topic. We set A4 = 1.2 to
give lower(even negative) reward to edges from
E2 that the two word vertices have same topic in
EGTRF, since the distance-1 words are expected
to have greater topical affects than distance-2
</bodyText>
<equation confidence="0.627750636363636">
1 − λ4
1 − λ2
ζ1
Eq( |EC1 |) +
Eq( |EC2 |)+
(11)
ζ1
Y_
×
(wn,wp)∈E2
1 − λ4
</equation>
<page confidence="0.904688">
ζ1
509
</page>
<bodyText confidence="0.999877136363636">
words. Word is represented as vector from pre-
trained Google News dataset, we use the word vec-
tor learned from original corpus when the word
does not exist in pre-trained Google News dataset.
We choose 10, 20, 30, 50 topics for 20 news
dataset, 10, 15, 20, 25 topics for NIPS dataset.
Figure 2 shows the experimental results of four
models: lda, gtrf, egtrf(EGTRF without word
similarity information), and egtrf+s(EGTRF with
word similarity information) on two datasets. The
results show EGTRF outperforms LDA and GTRF
in general, and EGTRF with word similarity infor-
mation achieves best performance.
We believe modeling distance-2 word vertices
can exploit more semantically or syntactically
word dependencies from document, and word sim-
ilarity information obtained from large corpus can
make up the lack of sufficient information from the
original corpus. Therefore, adding the influence of
distance-2 word vertices and word similarity infor-
mation can improve performance of topic model-
ing.
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999936375">
In this paper, we extended Global Topic Random
Field(GTRF) and proposed a novel topic model
Extended Global Topic Random Field(EGTRF)
which can model dependency relation between
adjacent words and distance-2 words. Word
topics are drawed by Extended Global Random
Field(EGRF) instead of Multinomial, the con-
ditional independence of word topic assignment
is thus relaxed. Word similarity information
learned from large corpus is incorporated into the
model. Experiments on two datasets show EGTRF
achieves better performance than GTRF and LDA,
which confirm our assumption that adding topical
dependency of distance-2 words and incorporating
word similarity information can improve model
performance.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946877192982">
Amir Globerson, Gal Chechik, Fernando Pereira, Naf-
tali Tishby Euclidean Embedding of Co-occurrence
Data. In Advances in neural information processing
systems. pp. 497-504. 2004.
Amit Gruber, Michal Rosen-Zvi and Yair Wei. Hid-
den Topic Markov Models. In Proceedings of the
Eleventh International Conference on Artificial In-
telligence and Statistic. pp. 163-170. 2007.
David Blei, Andrew Ng., and Michael Jordan. La-
tent Dirichlet Allocation. The Journal of Machine
Learning Research. 3:993-1022, 2003.
Hanna M Wallach. Topic modeling: Beyond bag-of-
words. In International Conference on Machine
Learning. pp. 977-984. ACM, 2006.
Jordan Boyd-Graber and David Blei. Syntactic topic
models. In Neural Information Processing Systems.
pp. 185-192. 2009.
Jun Zhu and Eric P. Xing. Conditional Topic Random
Fields. In Proceedings of the 27th International
Conference on Machine Learning. 2010.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard and David Mc-
Closky. The Stanford CoreNLP Natural Language
Processing Toolkit. In Proceedings of 52nd Annual
Meeting of the Association for Computational Lin-
guistics: System Demonstrations. pp. 55-60. 2014.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC. Vol. 6, No. 2006, pp. 449-454.
2006.
Marie-Catherine de Marneffe, Christopher D. Man-
ning. The Stanford typed dependencies represen-
tation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.
pp. 1-8. 2008.
Matthew Hoffman, David Blei, Chong Wang, John
Paisley Stochastic Variational Inference The Jour-
nal of Machine Learning Research. 14(1), 1303-
1347. 2013.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efficient Estimation of Word Representations
in Vector Space. In Proceedings of Workshop at
ICLR. arXiv:1301.3781, 2013.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. Distributed Representations
of Words and Phrases and their Compositionality. In
Proceedings of NIPS. pp. 3111-3119. 2013.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
Linguistic Regularities in Continuous Space Word
Representations. In Proceedings of NAACL HLT.
pp. 746-751, 2013.
Zhixing Li, Siqiang Wen, Juanzi Li, Peng Zhang and
Jie Tang. On Modeling Non-linear Topical Depen-
dencies. In Proceedings of the 31th International
Conference on Machine Learning. pp. 458-466,
2014.
</reference>
<page confidence="0.996667">
510
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196013">
<title confidence="0.653974">Extended Topic Model for Word Dependency</title>
<author confidence="0.593583">Vish</author>
<author confidence="0.593583">Ping</author>
<affiliation confidence="0.867309">of Massachusetts Boston, Boston, School of Public Health, Boston,</affiliation>
<email confidence="0.6874295">tongwang0001@gmail.com,VishViswanath@dfci.harvard.edu</email>
<abstract confidence="0.99938">Topic Model such as Latent Dirichlet Allocation(LDA) makes assumption that topic assignment of different words are conditionally independent. In this paper, we propose a new model Extended Global Topic Random Field (EGTRF) to model non-linear dependencies between words. Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words. Word similarity information learned from large corpus is incorporated to enhance word topic assignment. Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amir Globerson</author>
</authors>
<title>Gal Chechik, Fernando Pereira, Naftali Tishby Euclidean Embedding of Co-occurrence Data.</title>
<date>2004</date>
<booktitle>In Advances in neural information processing systems.</booktitle>
<pages>497--504</pages>
<marker>Globerson, 2004</marker>
<rawString>Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby Euclidean Embedding of Co-occurrence Data. In Advances in neural information processing systems. pp. 497-504. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Gruber</author>
<author>Michal Rosen-Zvi</author>
<author>Yair Wei</author>
</authors>
<title>Hidden Topic Markov Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistic.</booktitle>
<pages>163--170</pages>
<contexts>
<context position="1764" citStr="Gruber et al, 2007" startWordPosition="254" endWordPosition="257">topics from document collections by capturing words’ cooccuring relation. However, the “bag of words” assumption is employed in most existing topic models, it assumes the order of words can be ignored and topic assignment of each word is conditionally independent given the topic mixture of a document. To relax the “bag of words” assumption, many extended topic models have been proposed to address the limitation of conditional independence. Wallach (Wallach, 2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. Gruber (Gruber et al, 2007) models the topics of words in the document as a Markov chain, and assumes all words in the same sentence are more likely to have the same topic. Zhu (Zhu et al, 2010) incorporates Markov dependency between topic assignments of neighboring words, and employs a general structure of the GLM to define a conditional distribution of latent topic assignments over words. Most of the models above are limited to model linear topical dependencies between words, word topical dependencies can also be modeled by a non-linear way. In Syntactic topic models (Boyd-Graber et al, 2009), each word of a sentence </context>
</contexts>
<marker>Gruber, Rosen-Zvi, Wei, 2007</marker>
<rawString>Amit Gruber, Michal Rosen-Zvi and Yair Wei. Hidden Topic Markov Models. In Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistic. pp. 163-170. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Ng</author>
<author>Michael Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research.</journal>
<pages>3--993</pages>
<contexts>
<context position="1100" citStr="Blei et al, 2003" startWordPosition="152" endWordPosition="155">to model non-linear dependencies between words. Specifically, we parse sentences into dependency trees and represent them as a graph, and assume the topic assignment of a word is influenced by its adjacent words and distance-2 words. Word similarity information learned from large corpus is incorporated to enhance word topic assignment. Parameters are estimated efficiently by variational inference and experimental results on two datasets show EGTRF achieves lower perplexity and higher log predictive probability. 1 Introduction Probabilistic topic model such as Latent Dirichlet Allocation(LDA) (Blei et al, 2003) has been widely used for discovering latent topics from document collections by capturing words’ cooccuring relation. However, the “bag of words” assumption is employed in most existing topic models, it assumes the order of words can be ignored and topic assignment of each word is conditionally independent given the topic mixture of a document. To relax the “bag of words” assumption, many extended topic models have been proposed to address the limitation of conditional independence. Wallach (Wallach, 2006) explores a hierarchical generative probabilistic model that incorporates both n-gram st</context>
<context position="12065" citStr="Blei et al, 2003" startWordPosition="2059" endWordPosition="2062">ks, and the learned representations explicitly encode many linguistic regularities 508 Figure 2: Experimental results on NIPS(left) and 20 news(right) data and patterns from the corpus. Normalized similarity between word vectors can be regarded as the confidence score of how possible two words have same topic. In this way, knowledge from large corpus other than current document collections is incorporated to guide topic modeling. 3 Posterior Inference and Parameter Estimation We derive Variational Inference for posterior inference. The variational function q is same to the original LDA paper (Blei et al, 2003). All terms except P(z|O) in likelihood function are also same to LDA, Based on Equation (9), we obtain: Eq[log Pegrf (z |θ)] r1 Multi(zwn |θ))]+ ≈Eq[log( n ( |E1 |λ2+ |E2 |λ4 |E1 |+ |E2 | − )Eq(θT θ)+ ζ1 ζ2 log ζ1 − log ζ2 We get the approximation in Equation (11) from Taylor series, where (1 and (2 are Taylor approximation. EQ( |ECi |) is obtained directly from (10), EQ(OTO) is from the property of Dirichlet distribution. The updating rule of α and Q are same to LDA, -y is updated using Newton method since we can not obtain the direct updating rule for -y. 0 can be approximated as: 1 −1λ2 + </context>
<context position="13517" citStr="Blei et al, 2003" startWordPosition="2326" endWordPosition="2329">onvergence. 4 Experiment In this section we study the empirical performance of EGTRF on two datasets. For each dataset, we remove very short documents, and compute a vocabulary by removing stop words, rare words, frequent words. Eighty percent data are used for training, others for testing. • 20 News Groups: After processing, it contains 13706 documents with a vocabulary of 5164 terms. • NIPS data (Globerson et al, 2004): Spanning from 2000 to 2005. After processing, it contains 843 documents with a vocabulary of 6098 terms. We evaluate how well a model fits the data with held-out perplexity (Blei et al, 2003) and predictive distribution (Hoffman et al, 2013). Lower perplexity, higher log predictive probability indicate better generalization performance. We implement GTRF without adding self defined edges from the original paper, and set A2 = 0.2 to give higher reward to edges from E1 that the two word vertices have same topic. We set A4 = 1.2 to give lower(even negative) reward to edges from E2 that the two word vertices have same topic in EGTRF, since the distance-1 words are expected to have greater topical affects than distance-2 1 − λ4 1 − λ2 ζ1 Eq( |EC1 |) + Eq( |EC2 |)+ (11) ζ1 Y_ × (wn,wp)∈</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Ng., and Michael Jordan. Latent Dirichlet Allocation. The Journal of Machine Learning Research. 3:993-1022, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna M Wallach</author>
</authors>
<title>Topic modeling: Beyond bag-ofwords.</title>
<date>2006</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<pages>977--984</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1612" citStr="Wallach, 2006" startWordPosition="236" endWordPosition="237">. 1 Introduction Probabilistic topic model such as Latent Dirichlet Allocation(LDA) (Blei et al, 2003) has been widely used for discovering latent topics from document collections by capturing words’ cooccuring relation. However, the “bag of words” assumption is employed in most existing topic models, it assumes the order of words can be ignored and topic assignment of each word is conditionally independent given the topic mixture of a document. To relax the “bag of words” assumption, many extended topic models have been proposed to address the limitation of conditional independence. Wallach (Wallach, 2006) explores a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables. Gruber (Gruber et al, 2007) models the topics of words in the document as a Markov chain, and assumes all words in the same sentence are more likely to have the same topic. Zhu (Zhu et al, 2010) incorporates Markov dependency between topic assignments of neighboring words, and employs a general structure of the GLM to define a conditional distribution of latent topic assignments over words. Most of the models above are limited to model linear topical dependencies between</context>
</contexts>
<marker>Wallach, 2006</marker>
<rawString>Hanna M Wallach. Topic modeling: Beyond bag-ofwords. In International Conference on Machine Learning. pp. 977-984. ACM, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>David Blei</author>
</authors>
<title>Syntactic topic models.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems.</booktitle>
<pages>185--192</pages>
<marker>Boyd-Graber, Blei, 2009</marker>
<rawString>Jordan Boyd-Graber and David Blei. Syntactic topic models. In Neural Information Processing Systems. pp. 185-192. 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Eric P Xing</author>
</authors>
<title>Conditional Topic Random Fields.</title>
<date>2010</date>
<booktitle>In Proceedings of the 27th International Conference on Machine Learning.</booktitle>
<marker>Zhu, Xing, 2010</marker>
<rawString>Jun Zhu and Eric P. Xing. Conditional Topic Random Fields. In Proceedings of the 27th International Conference on Machine Learning. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations.</booktitle>
<pages>55--60</pages>
<contexts>
<context position="2647" citStr="Manning et al, 2014" startWordPosition="400" endWordPosition="403">neral structure of the GLM to define a conditional distribution of latent topic assignments over words. Most of the models above are limited to model linear topical dependencies between words, word topical dependencies can also be modeled by a non-linear way. In Syntactic topic models (Boyd-Graber et al, 2009), each word of a sentence is generated by a distribution that combines document-specific topic weights and parsetree-specific syntactic transitions. In Global Topic Random Field(GTRF) model (Li et al, 2014), sentences of a document are parsed into dependency trees (Marneffe et al, 2008) (Manning et al, 2014) (Marneffe et al, 2006). They show topics of semantically or syntactically dependent words achieve the highest similarity and are able to provide more useful information in topic modeling, which is also the basic assumption of our model. Then they propose GTRF to model non-linear topical dependencies, word topics are sampled based on graph structure instead of “bag of words” representation, the conditional independence of word topic assignment is thus relaxed. However, GTRF assumes topic assignment of a word vertex depends on the topic mixture of the document and its neighboring word vertices,</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard and David McClosky. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. pp. 55-60. 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC. Vol. 6, No. 2006, pp. 449-454. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation.</booktitle>
<pages>1--8</pages>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, Christopher D. Manning. The Stanford typed dependencies representation. In COLING 2008 Workshop on Crossframework and Cross-domain Parser Evaluation. pp. 1-8. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>David Blei</author>
</authors>
<title>Chong Wang, John Paisley Stochastic Variational Inference The</title>
<date>2013</date>
<journal>Journal of Machine Learning Research.</journal>
<volume>14</volume>
<issue>1</issue>
<pages>1303--1347</pages>
<marker>Hoffman, Blei, 2013</marker>
<rawString>Matthew Hoffman, David Blei, Chong Wang, John Paisley Stochastic Variational Inference The Journal of Machine Learning Research. 14(1), 1303-1347. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR. arXiv:1301.3781,</booktitle>
<contexts>
<context position="5817" citStr="Mikolov et al, 2013" startWordPosition="907" endWordPosition="910">× Some hidden dependency relations can also be extracted by merging dependency trees. For example, word “allocation” has a new distance-2 word “topics” after merging. Therefore, EGTRF can exploit more semantically or syntactically word dependencies. Theoretically, we can also model the distance further than 2, however, it leads to more complicated computation and small increase of performance. Another advantage of EGTRF is it incorporates word features. The word vector representations are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns (Mikolov et al, 2013). We use the pretrained model from Google News dataset(about 100 billion words) using word2vec1 tool to represent each word as a 300-dimensional word vector, and apply normalized word similarity as a confidence score to indicate how possible two word vertices share same topic. We organized the paper as below: EGTRF is presented in Section 2, variational inference and parameter estimation are derived in Section 3, experiments on two datasets are showed in Section 4, we conclude the paper in Section 5. 2 Extended Global Topic Random Field In this section, we first present Extended Global Random </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR. arXiv:1301.3781, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed Representations of Words and Phrases and their Compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="5817" citStr="Mikolov et al, 2013" startWordPosition="907" endWordPosition="910">× Some hidden dependency relations can also be extracted by merging dependency trees. For example, word “allocation” has a new distance-2 word “topics” after merging. Therefore, EGTRF can exploit more semantically or syntactically word dependencies. Theoretically, we can also model the distance further than 2, however, it leads to more complicated computation and small increase of performance. Another advantage of EGTRF is it incorporates word features. The word vector representations are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns (Mikolov et al, 2013). We use the pretrained model from Google News dataset(about 100 billion words) using word2vec1 tool to represent each word as a 300-dimensional word vector, and apply normalized word similarity as a confidence score to indicate how possible two word vertices share same topic. We organized the paper as below: EGTRF is presented in Section 2, variational inference and parameter estimation are derived in Section 3, experiments on two datasets are showed in Section 4, we conclude the paper in Section 5. 2 Extended Global Topic Random Field In this section, we first present Extended Global Random </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS. pp. 3111-3119. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL HLT.</booktitle>
<pages>746--751</pages>
<contexts>
<context position="5817" citStr="Mikolov et al, 2013" startWordPosition="907" endWordPosition="910">× Some hidden dependency relations can also be extracted by merging dependency trees. For example, word “allocation” has a new distance-2 word “topics” after merging. Therefore, EGTRF can exploit more semantically or syntactically word dependencies. Theoretically, we can also model the distance further than 2, however, it leads to more complicated computation and small increase of performance. Another advantage of EGTRF is it incorporates word features. The word vector representations are very interesting because the learned vectors explicitly encode many linguistic regularities and patterns (Mikolov et al, 2013). We use the pretrained model from Google News dataset(about 100 billion words) using word2vec1 tool to represent each word as a 300-dimensional word vector, and apply normalized word similarity as a confidence score to indicate how possible two word vertices share same topic. We organized the paper as below: EGTRF is presented in Section 2, variational inference and parameter estimation are derived in Section 3, experiments on two datasets are showed in Section 4, we conclude the paper in Section 5. 2 Extended Global Topic Random Field In this section, we first present Extended Global Random </context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word Representations. In Proceedings of NAACL HLT. pp. 746-751, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhixing Li</author>
<author>Siqiang Wen</author>
<author>Juanzi Li</author>
</authors>
<title>Peng Zhang and Jie Tang. On Modeling Non-linear Topical Dependencies.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31th International Conference on Machine Learning.</booktitle>
<pages>458--466</pages>
<contexts>
<context position="2544" citStr="Li et al, 2014" startWordPosition="383" endWordPosition="386">0) incorporates Markov dependency between topic assignments of neighboring words, and employs a general structure of the GLM to define a conditional distribution of latent topic assignments over words. Most of the models above are limited to model linear topical dependencies between words, word topical dependencies can also be modeled by a non-linear way. In Syntactic topic models (Boyd-Graber et al, 2009), each word of a sentence is generated by a distribution that combines document-specific topic weights and parsetree-specific syntactic transitions. In Global Topic Random Field(GTRF) model (Li et al, 2014), sentences of a document are parsed into dependency trees (Marneffe et al, 2008) (Manning et al, 2014) (Marneffe et al, 2006). They show topics of semantically or syntactically dependent words achieve the highest similarity and are able to provide more useful information in topic modeling, which is also the basic assumption of our model. Then they propose GTRF to model non-linear topical dependencies, word topics are sampled based on graph structure instead of “bag of words” representation, the conditional independence of word topic assignment is thus relaxed. However, GTRF assumes topic assi</context>
</contexts>
<marker>Li, Wen, Li, 2014</marker>
<rawString>Zhixing Li, Siqiang Wen, Juanzi Li, Peng Zhang and Jie Tang. On Modeling Non-linear Topical Dependencies. In Proceedings of the 31th International Conference on Machine Learning. pp. 458-466, 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>