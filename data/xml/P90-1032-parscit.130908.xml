<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<sectionHeader confidence="0.875616" genericHeader="abstract">
AUTOMATICALLY EXTRACTING AND REPRESENTING
COLLOCATIONS FOR LANGUAGE GENERATION*
</sectionHeader>
<note confidence="0.4110775">
Frank A. Smadjat
and
</note>
<author confidence="0.996668">
Kathleen R. McKeown
</author>
<affiliation confidence="0.996557">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.964975">
New York, NY 10027
</address>
<sectionHeader confidence="0.518914" genericHeader="keywords">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999802785714286">
Collocational knowledge is necessary for language gener-
ation. The problem is that collocations come in a large
variety of forms. They can involve two, three or more
words, these words can be of different syntactic cate-
gories and they can be involved in more or less rigid
ways. This leads to two main difficulties: collocational
knowledge has to be acquired and it must be represented
flexibly so that it can be used for language generation.
We address both problems in this paper, focusing on the
acquisition problem. We describe a program, Xtract,
that automatically acquires a range of collocations from
large textual corpora and we describe how they can be
represented in a flexible lexicon using a unification based
formalism.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.9999696">
Language generation research on lexical choice has fo-
cused on syntactic and semantic constraints on word
choice and word ordering. Collocational constraints,
however, also play a role in how words can co-occur in
the same sentence. Often, the use of one word in a par-
ticular context of meaning will require the use of one or
more other words in the same sentence. While phrasal
lexicons, in which lexical associations are pre-encoded
(e.g., [Kukich 83], [Jacobs 85], [Danlos 87]), allow for the
treatment of certain types of collocations, they also have
problems. Phrasal entries must be compiled by hand
which is both expensive and incomplete. Furthermore,
phrasal entries tend to capture rather rigid, idiomatic
expressions. In contrast, collocations vary tremendously
in the number of words involved, in the syntactic cat-
egories of the words, in the syntactic relations between
the words, and in how rigidly the individual words are
used together. For example, in some cases, the words of
a collocation must be adjacent, while in others they can
be separated by a varying number of other words.
</bodyText>
<footnote confidence="0.9634155">
&apos;The research reported in this paper was partially sup-
ported by DARPA grant N00039-84-C-0165, by NSF grant
IRT-84-51438 and by ONR grant N00014-89-J-1782.
tMost of this work is also done in collaboration with Bell
Communication Research, 445 South Street, Morristown, NJ
07960-1910
</footnote>
<bodyText confidence="0.99992665625">
In this paper, we identify a range of collocations that
are necessary for language generation, including open
compounds of two or more words, predicative relations
(e.g., subject-verb), and phrasal templates represent-
ing more idiomatic expressions. We then describe how
Xtract automatically acquires the full range of colloca-
tions using a two stage statistical analysis of large do-
main specific corpora. Finally, we show how collocations
can be efficiently represented in a flexible lexicon using a
unification based formalism. This is a word based lexicon
that has been macrocoded with collocational knowledge.
Unlike a purely phrasal lexicon, we thus retain the flexi-
bility of word based lexicons which allows for collocations
to be combined and merged in syntactically acceptable
ways with other words or phrases of the sentence. Unlike
pure word based lexicons, we gain the ability to deal with
a variety of phrasal entries. Furthermore, while there has
been work on the automatic retrieval of lexical informa-
tion from text [Garside 87], [Choueka, 88], [Klavans 88],
[Amsler 89], [Boguraev &amp; Briscoe 89], [Church 89], none
of these systems retrieves the entire range of collocations
that we identify and no real effort has been made to use
this information for language generation [Boguraev &amp;
Briscoe 89].
In the following sections, we describe the range of col-
locations that we can handle, the fully implemented ac-
quisition method, results obtained, and the representa-
tion of collocations in Functional Unification Grammars
(FUGs) [Kay 79]. Our application domain is the domain
of stock market reports and the corpus on which our ex-
pertise is based consists of more than 10 million words
taken from the Associated Press news wire.
</bodyText>
<sectionHeader confidence="0.999835333333333" genericHeader="method">
2 SINGLE WORDS TO WHOLE
PHRASES: WHAT KIND OF
LEXICAL UNITS ARE NEEDED?
</sectionHeader>
<bodyText confidence="0.999426142857143">
Collocational knowledge indicates which members of a
set of roughly synonymous words co-occur with other
words and how they combine syntactically. These affini-
ties can not be predicted on the basis of semantic or syn-
tactic rules, but can be observed with some regularity in
text [Cruse 86]. We have found a range of collocations
from word pairs to whole phrases, and as we shall show,
</bodyText>
<page confidence="0.994615">
252
</page>
<bodyText confidence="0.999492">
this range will require a flexible method of representa-
tion.
Open Compounds . Open compounds involve unin-
terrupted sequences of words such as &amp;quot;stock mar-
ket, &apos; &amp;quot;foreign exchange,&amp;quot; &amp;quot;New York Stock Ex-
change,&amp;quot; &amp;quot;The Dow Jones average of 30 industri-
als.&amp;quot; They can include nouns, adjectives, and closed
class words and are similar to the type of colloca-
tions retrieved by [Choueka 88] or [Amsler 89]. An
open compound generally functions as a single con-
stituent of a sentence. More open compound exam-
ples are given in figure 1.1
Predicative Relations consist of two (or several)
words repeatedly used together in a similar syn-
tactic relation. These lexical relations are harder
to identify since they often correspond to inter-
rupted word sequences in the corpus. They are also
the most flexible in their use. This class of col-
locations is related to Mel&apos;Zuk&apos;s Lexical Functions
[Mel&apos;Euk 81], and Benson&apos;s L-type relations [Ben-
son 86]. Within this class, Xtract retrieves subject-
verb, verb-object, noun-adjective, verb-adverb, verb-
verb and verb-particle predicative relations. Church
[Church 89] also retrieves verb-particle associations.
Such collocations require a representation that al-
lows for a lexical function relating two or more
words. Examples of such collocations are given in
figure 2.2
Phrasal templates: consist of idiomatic phrases con-
taining one, several or no empty slots. They are
extremely rigid and long collocations. These almost
complete phrases are quite representative of a given
domain. Due to their slightly idiosyncratic struc-
ture, we propose representing and generating them
by simple template filling. Although some of these
could be generated using a word based lexicon, in
general, their usage gives an impression of fluency
that cannot be equaled with compositional genera-
tion alone. Xtract has retrieved several dozens of
such templates from our stock market corpus, in-
cluding:
</bodyText>
<construct confidence="0.5559682">
&amp;quot;The NYSE&apos;s composite index of all its listed com-
mon stocks rose
*NUMBER* to *NUMBER*&amp;quot;
&amp;quot;On the American Stock Exchange the market value
index was up
*NUMBER* at *NUMBER*&amp;quot;
&amp;quot;The Dow Jones average of 30 industrials fell
*NUMBER* points to *NUMBER*&amp;quot;
&amp;quot;The closely watched index had been down about
*NUMBER* points in
</construct>
<footnote confidence="0.498912888888889">
the first hour of trading&amp;quot;
&amp;quot;The average finished the week with a net loss of
*NUMBER*&amp;quot;
&apos;All the examples related to the stock market domain have
been actually retrieved by Xtract.
In
&apos; the examples, the &amp;quot;a&amp;quot; sign, represents a gap of zero,
one or several words. The &amp;quot;.*&amp;quot; sign means that the two
words can be in any order.
</footnote>
<sectionHeader confidence="0.9741465" genericHeader="method">
3 THE ACQUISITION METHOD:
Xtract
</sectionHeader>
<bodyText confidence="0.996871727272728">
In order to produce sentences containing collocations, a
language generation system must have knowledge about
the possible collocations that occur in a given domain.
In previous language generation work [Danlos 87], [lor-
danskaja 88], [Nirenburg 88], collocations are identified
and encoded by hand, sometimes using the help of lexi-
cographers (e.g., Danlos&apos; [Danlos 87] use of Gross&apos; [Gross
75] work). This is an expensive and time-consuming pro-
cess, and often incomplete. In this section, we describe
how Xtract can automatically produce the full range of
collocations described above.
Xtract has two main components, a concordancing
component, Xconcord, and a statistical component,
Xstat. Given one or several words, Xconcord locates
all sentences in the corpus containing them. Xstat is
the co-occurrence compiler. Given Xconcord&apos;s output,
it makes statistical observations about these words and
other words with which they appear. Only statistically
significant word pairs are retained. In [Smadja 89a], and
[Smadja 88], we detail an earlier version of Xtract and
its output, and in [Smadja 89b] we compare our results
both qualitatively and quantitatively to the lexicon used
in [Kukich 83]. Xtract has also been used for informa-
tion retrieval in [Maarek &amp; Smadja 89]. In the updated
version of Xtract we describe here, statistical signifi-
cance is based on four parameters, instead of just one,
and a second stage of processing has been added that
looks for combinations of word pairs produced in the
first stage, resulting in multiple word collocations.
Stage one: In the first phase, Xconcord is called for a
single open class word and its output is pipelined to
Xstat which then analyzes the distribution of words
in this sample. The output of this first stage is a list
of tuples (w1, w2, distance, strength, spread, height,
type), where (w1, w2) is a lexical relation between
two open-class words (w1 and tu2). Some results
are given in Table 1. &amp;quot;Type&amp;quot; represents the syn-
tactic categories of wi and w2.3. &amp;quot;Distance&amp;quot; is the
relative distance between the two words, wi and w2
(e.g., a distance of 1 means w2 occurs immediately
after wi and a distance of -1 means it occurs imme-
diately before it). A different tuple is produced for
each statistically significant word pair and distance.
Thus, if the same two words occur equally often sep-
arated by two different distances, they will appear
twice in the list. &amp;quot;Strength&amp;quot; (also computed in the
earlier version of Xtract) indicates how strongly the
two words are related (see [Smadja 89a]). &amp;quot;Spread&amp;quot;
is the distribution of the relative distance between
the two words; thus, the larger the &amp;quot;spread&amp;quot; the
more rigidly they are used in combination to one
another. &amp;quot;Height&amp;quot; combines the factors of &amp;quot;spread&amp;quot;
&apos;In order to get part of speech information we use a
stochastic word tagger developed at AT&amp;T Bell Laborato-
ries by Ken Church [Church 88]
</bodyText>
<page confidence="0.999219">
253
</page>
<tableCaption confidence="0.999746">
Table 1: Some binary lexical relations.
</tableCaption>
<bodyText confidence="0.717168333333333">
word1 -worcl2 distance strength spread height Type
stock market 1 47.018 28.5 11457.1 NN
president vice -1 40.6496 29.7 10757 NN
trade deficit 1 30.3384 28.4361 7358.87 NN
directors board -2 22.6038 28.7682 5611.84 NN
merger agreement 1 20.62 28.7682 5119.32 NN
attempt takeover -1 21.1464 28.407 5118.02 NN
average industrial -1 13.1674 29.3682 3406.85 NJ
index composite -1 12.3874 29.0682 3139.89 NJ
chip blue -1 10.078 30 2721.06 NJ
shares totaled -4 20.7815 29.3682 5376.87 NV
price closing -1 23.0465 25.9415 4615.48 NV
stocks listed -2 27.354 23.8696 4583.57 NV
volume totaled 1 16.8724 29.7 4464.89 NV
takeover bid -1 19.3312 28.1071 4580.39 NN
takeovers hostile 1 13.5184 29.3682 3497.67 NJ
takeover offer -1 5.43739 25.7917 1084.05 &apos; NN
takeovers thwart 2 2.61206 _ 30 705.256 NV
</bodyText>
<tableCaption confidence="0.769406">
Table 2: Concordances for &amp;quot;average industrial&amp;quot;
</tableCaption>
<bodyText confidence="0.998726888888889">
On Tuesday the Dow Jones industrial average
The Dow Jones industrial average
a selling spurt that sent the Dow Jones industrial average
On Wednesday the Dow Jones industrial average
The Dow Jones industrial average
The Dow Jones industrial average
... Thursday with the Dow Jones industrial average
.„ swelling the Dow Jones industrial average
The rise in the Dow Jones industrial average
</bodyText>
<table confidence="0.661866888888889">
rose 26.28 points to 2 304.69.
went up 11.36 points today.
down sharply in the first hour of trading.
showed some strength as ...
was down 17.33 points to 2,287.36
had the biggest one day gain of its history
soaring a record 69.89 points to ...
by more than 475 points in the process ...
was the biggest since a 54.14 point jump on ...
</table>
<tableCaption confidence="0.983393">
Table 3: Concordances for &amp;quot;composite index&amp;quot;
</tableCaption>
<bodyText confidence="0.999428444444444">
The NYSE s composite index
The NYSE a composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE a composite index
The NYSE s composite index
The NYSE s composite index
</bodyText>
<figureCaption confidence="0.998165333333333">
of all its listed common stocks fell 1.76 to 164.13.
of all its listed common stocks fell 0.98 to 164.91.
of all its listed common stocks fell 0.96 to 164.93.
of all its listed common stocks fell 0.91 to 164.98.
of all its listed common stocks rose 1.04 to 167.08.
of all its listed common stocks rose 0.76
of all its listed common stocks rose 0.50 to 166.54.
of all its listed common stocks rose 0.69 to 166.73.
of all its listed common stocks fell 0.33 to 170.63.
</figureCaption>
<page confidence="0.898499">
254
</page>
<bodyText confidence="0.9080420625">
type of collocation examples
open compound &apos;leading industrialized countries&amp;quot;
open compound &amp;quot;the Dow Jones average of SO industrials&amp;quot;
open compound &amp;quot;bear/buil market&amp;quot;
open compound &amp;quot;the Dow Jones industrial average&amp;quot;
open compound &amp;quot;The NYSE a composite index of all its listed common stocks&amp;quot;
open compound &amp;quot;Advancing/winning/losing/declining issues&amp;quot;
open compound &amp;quot;The NASDAQ composite index for the over the counter market&amp;quot;
open compound &amp;quot;stock market&amp;quot;
open compound &amp;quot;central bank
open compound &amp;quot;leveraged buyout&amp;quot;
open compound &amp;quot;the gross national product&amp;quot;
open compound &amp;quot;blue chip stocks&amp;quot;
open compound &amp;quot;White House spokesman Marlin Fitzwater&amp;quot;
open compound &amp;quot;takeover speculation/strategist/target/threat/attempt&amp;quot;
open compound &amp;quot;takeover bid/battle/defense/efforts/fight/law/proposal/rumor&amp;quot;
</bodyText>
<figureCaption confidence="0.920973">
Figure 1: Some examples of open compounds
</figureCaption>
<table confidence="0.992675923076923">
type of collocation examples
noun adjective &amp;quot;heavy/light 0 trading/smoker/traffic&amp;quot;
noun adjective &amp;quot;high/low I] fertility/pressure/bounce&amp;quot;
noun adjective &amp;quot;large/small 0 crowd/retailer/client&amp;quot;
subject verb &amp;quot;index 0 rose
subject verb &amp;quot;stock 0 [rose, fell, closed, jumped, continued, declined, crashed, ...]&amp;quot;
subject verb &amp;quot;advancers U [outnumbered, outpaced, overwhelmed, outstripped]&amp;quot;
verb adverb &amp;quot;trade # actively,&amp;quot; &amp;quot;mix 44, narrowly,&amp;quot; &amp;quot;use # widely,&amp;quot; &amp;quot;watch s&gt; closely&amp;quot;
verb object &amp;quot;posted 0 gain
verb object &amp;quot;momentum 0 [pick up, build, carry over, gather, loose, gain]&amp;quot;
verb particle &amp;quot;take [II from,&amp;quot; &amp;quot;raise 0 by,&amp;quot; &amp;quot;mix I] with&amp;quot;
verb verb &amp;quot;offer to [acquire, buy&amp;quot;]
verb verb &amp;quot;agree to [acquire, buy&amp;quot;]
</table>
<figureCaption confidence="0.973262">
Figure 2: Some examples of predicative collocations
</figureCaption>
<bodyText confidence="0.998986716981132">
and &amp;quot;strength&amp;quot; resulting in a ranking of the two
words for their &amp;quot;distances&amp;quot;. Church [Church 89]
produces results similar to those presented in the
table using a different statistical method. However,
Church&apos;s method is mainly based on the computa-
tion of the &amp;quot;strength&amp;quot; attribute, and it does not take
into account &amp;quot;spread&amp;quot; and &amp;quot;height&amp;quot;. As we shall
see, these additional parameters are crucial for pro-
ducing multiple word collocations and distinguish-
ing between open compounds (words are adjacent)
and predicative relations (words can be separated
by varying distance).
Stage two: In the second phase, Xtra.et first uses the
same components but in a different way. It starts
with the pairwise lexical relations produced in Stage
one to produce multiple word collocations, then
classifies the collocations as one of three classes iden-
tified above, and finally attempts to determine the
syntactic relations between the words of the collo-
cation. To do this, Xtraet studies the lexical re-
lations in context, which is exactly what lexicogra-
phers do. For each entry of Table 1, Xtract calls
Xconcord on the two words wi and w2 to pro-
duce the concordances. Tables 2 and 3 show the
concordances (output of Xconeord) for the input
pairs: &amp;quot;average-industrial&amp;quot; and &amp;quot;index-composite&amp;quot;.
Xstat then compiles information on the words sur-
rounding both wi and w2 in the corpus. This stage
allows us to filter out incorrect associations such
as &amp;quot;blue-stocks&amp;quot; or &amp;quot;advancing-market&amp;quot; and replace
them with the appropriate ones, &amp;quot;blue chip stocks,&amp;quot;
&amp;quot;the broader market in the NYSE advancing is-
sues.&amp;quot; This stage also produces phrasal templates
such as those given in the previous section. In short,
stage two filters inapropriate results and combines
word pairs to produce multiple word combinations.
To make the results directly usable for language gen-
eration we are currently investigating the use of a
bottom-up parser in combination with stage two in
order to classify the collocations according to syn-
tactic criteria. For example if the lexical relation
involves a noun and a verb it determines if it is a
subject-verb or a verb-object collocation. We plan
to do this using a determin&apos;stic bottom up parser
developed at Bell Communication Research [Abney
89] to parse the concordances. The parser would
analyze each sentence of the concordances and the
parse trees would then be passed to Xstat.
Sample results of Stage two are shown in Fig-
ures 1, 2 and 3. Figure 3 shows phrasal templates and
open compounds. Xstat notices that the words &amp;quot;com-
posite and &amp;quot;index&amp;quot; are used very rigidly throughout the
corpus. They almost always appear in one of the two
</bodyText>
<page confidence="0.996116">
255
</page>
<table confidence="0.987277076923077">
&apos; lexical relation collocation
composite-index &amp;quot;The NYSE&apos;s composite index of all its listed common
stocks fell *NUMBER* to *NUMBER*&amp;quot;
composite-index &amp;quot;the NYSE&apos;s composite index of all its listed common
stocks rose *NUMBER* to *NUMBER*.&amp;quot;
&amp;quot;close-industrial&amp;quot; &amp;quot;Five minutes before the close the Dow Jones average of SO industrials
was up/down *NUMBER* to/from *NUMBER*&amp;quot;
&amp;quot;average industrial&amp;quot; &amp;quot;the Dow Jones industrial average.&amp;quot;
&amp;quot;advancing-market&amp;quot; &amp;quot;the broader market in the NYSE advancing issues&amp;quot;
&amp;quot;block-trading&amp;quot; &amp;quot;Jack Baker head of block trading in Shearson Lehman Brothers Inc.;3
&amp;quot;blue-stocks&amp;quot; &amp;quot;Nue chip stocks&amp;quot;
&amp;quot;cable-television&amp;quot; &amp;quot;cable television&amp;quot;
&amp;quot;consumer index&amp;quot; &amp;quot;The consumer price index&amp;quot;
</table>
<figureCaption confidence="0.995172">
Figure 3: Example collocations output of stage two.
</figureCaption>
<bodyText confidence="0.999225125">
sentences. The lexical relation composite-index thus pro-
duces two phrasal templates. For the lexical relation
average-industrial Xtract produces an open compound
collocation as illustrated in figure 3. Stage two also con-
firms pairwise relations. Some examples are given in
figure 2. By examining the parsed concordances and
extracting recurring patterns, Xstat produces all three
types of collocations.
</bodyText>
<sectionHeader confidence="0.9886925" genericHeader="method">
4 HOW TO REPRESENT THEM
FOR LANGUAGE GENERATION?
</sectionHeader>
<bodyText confidence="0.9799796">
Such a wide variety of lexical associations would be dif-
ficult to use with any of the existing lexicon formalisms.
We need a flexible lexicon capable of using single word
entries, multiple word entries as well as phrasal tem-
plates and a mechanism that would be able to gracefully
merge and combine them with other types of constraints.
The idea of a flexible lexicon is not novel in itself. The
lexical representation used in [Jacobs 85] and later re-
fined in [Desemer &amp; Jabobs 87] could also represent a
wide range of expressions. However, in this language,
collocational, syntactic and selectional constraints are
mixed together into phrasal entries. This makes the lex-
icon both difficult to use and difficult to compile. In the
following we briefly show how FUGs can be successfully
used as they offer a flexible declarative language as well
as a powerful mechanism for sentence generation.
We have implemented a first version of Cook, a sur-
face generator that uses a flexible lexicon for express-
ing co-occurrence constraints. Cook uses FUF [Elhaclad
90], an extended implementation of FUGs, to uniformly
represent the lexicon and the syntax as originally sug-
gested by Halliday [Halliday 66]. Generating a sentence
is equivalent to unifying a semantic structure (Logical
Form) with the grammar. The grammar we use is di-
vided into three zones, the &amp;quot;sentential,&amp;quot; the &amp;quot;lericai&amp;quot;
and &amp;quot;the syntactic zone.&amp;quot; Each zone contains constraints
pertaining to a given domain and the input logical form
is unified in turn with the three zones. As it is, full
backtracking across the three zones is allowed.
• The sentential zone contains the phrasal templates
against which the logical form is unified first. A
sentential entry is a whole sentence that should be
used in a given context. This context is specified by
subparts of the logical form given as input. When
there is a match at this point, unification succeeds
and generation is reduced to simple template filling.
• The lexical zone contains the information used to
lexicalize the input. It contains collocational infor-
mation along with the semantic context in which
to use it. This zone contains predicative and open
compound collocations. Its role is to trigger phrases
or words in the presence of other words or phrases.
Figure 5 is a portion of the lexical grammar used
in Cook. It illustrates the choice of the verb to be
used when &amp;quot;advancers&amp;quot; is the subject. (See below
for more detail).
• The syntactic zone contains the syntactic grammar.
It is used last as it is the part of the grammar en-
suring the correctness of the produced sentences.
An example input logical form is given in Figure 4. In
this example, the logical form represents the fact that on
the New York stock exchange, the advancing issues (se-
mantic representation or sem—R: carinners) were ahead
(predicate c:lead) of the losing ones (sem-R: c:losers) and
that there were 3 times more winning issues than losing
ones ratio). In addition, it also says that this ratio is
of degree 2. A degree of I is considered as a slim lead
whereas a degree of 5 is a commanding margin. When
unified with the grammar, this logical form produces the
sentences given in Figure 6.
As an example of how Cook uses and merges co-
occurrence information with other kind of knowledge
consider Figure 5. The figure is an edited portion of
the lexical zone. It only includes the parts that are rel-
evant to the choice of the verb when &amp;quot;advancers&amp;quot; is the
subject. The lex and sen-ft attributes specify the lex-
eme we are considering (&amp;quot;advancers&amp;quot;) and its semantic
representation (cminners).
The semantic context (sem-con-text) which points to
the logical form and its features will then be used in order
</bodyText>
<page confidence="0.992518">
256
</page>
<figure confidence="0.9646844">
logical-form = predicate-name =
leaders =
trailers
degree
p: lead
sem-ft = c : winners
[ratio 3
sem-R = C: losers
ratio = 1
2
</figure>
<figureCaption confidence="0.9999765">
Figure 4: LF: An example logical form used by Cook
Figure 5: A portion of the lexical grammar showing the verbal collocates of &amp;quot;advancers&amp;quot;.
</figureCaption>
<tableCaption confidence="0.585511666666667">
&amp;quot;Advancers outnumbered declining issues by a margin of 3 to 1.&amp;quot;
&amp;quot;Advancers had a slim lead over losing issues with a margin of 3 to 1.&amp;quot;
&amp;quot;Advancers kept a slim lead over decliners with a margin of 3 to 1&amp;quot;
</tableCaption>
<figureCaption confidence="0.974971">
Figure 6: Example sentences that can be generated with the logical form LF
</figureCaption>
<page confidence="0.783223">
257
</page>
<equation confidence="0.896857666666667">
-
lex = &amp;quot;advancer&amp;quot;
sam-R = c:winners
sem-context = &lt;logical-form&gt;
[predicate-name = p : lead
degree = 2
lex = &amp;quot;outnumber&amp;quot;
lex = &amp;quot;lead&amp;quot;
lex = &amp;quot;finish&amp;quot;
lex = &amp;quot;hold&amp;quot;
lox = &amp;quot;keep&amp;quot;
lex = &amp;quot;have&amp;quot;
</equation>
<figure confidence="0.683681916666667">
[p predicate-name =
: lead ]
degree = 4
lex = &amp;quot;overpower&amp;quot;
lex = &amp;quot;outstrip&amp;quot;
lex = &amp;quot;hold&amp;quot;
lex = &amp;quot;keep&amp;quot;
sem-context
SV-collocates =
•••
cern-context
Mr-collocates
</figure>
<bodyText confidence="0.99979112">
to select among the alternatives classes of verbs. La the
figure we only included two alternatives. Both are rela-
tive to the predicate p: lead but they are used with dif-
ferent values of the degree attribute. When the degree is
2 then the first alternative containing the verbs listed un-
der 5V-collocates (e.g. &amp;quot;outnumber&amp;quot;) will be selected.
When the degree is 4 the second alternative contain-
ing the verbs listed under SV-coilocates (e.g. &amp;quot;over-
power&amp;quot;) will be selected. All the verbal collocates shown
in this figure have actually been retrieved by Xtract at
a preceding stage.
The unification of the logical form of Figure 4 with
the lexical grammar and then with the syntactic gram-
mar will ultimately produce the sentences shown in Fig-
ure 6 among others. In this example, the sentential zone
was not used since no phrasal template expresses its
semantics. The verbs selected are all listed under the
5V-collocates of the first alternative in Figure 5.
We have been able to use Cook to generate several
sentences in the domain of stock market reports using
this method. However, this is still on-going research and
the scope of the system is currently limited. We are
working on extending Cook&apos;s lexicon as well as on de-
veloping extensions that will allow flexible interaction
among collocations.
</bodyText>
<sectionHeader confidence="0.999107" genericHeader="conclusions">
5 CONCLUSION
</sectionHeader>
<bodyText confidence="0.999969615384615">
In summary, we have shown in this paper that there
are many different types of collocations needed for lan-
guage generation. Collocations are flexible and they can
involve two, three or more words in various ways. We
have described a fully implemented program, Xtract,
that automatically acquires such collocations from large
textual corpora and we have shown how they can be
represented in a flexible lexicon using FUF. In FUF, co-
occurrence constraints are expressed uniformly with syn-
tactic and semantic constraints. The grammar&apos;s function
is to satisfy these multiple constraints. We are currently
working on extending Cook as well as developing a full
sized from Xtract&apos;s output.
</bodyText>
<sectionHeader confidence="0.999127" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.998928666666667">
We would like to thank Karen Kukich and the Computer
Systems Research Division at Bell Communication Re-
search for their help on the acquisition part of this work.
</bodyText>
<sectionHeader confidence="0.996794" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995010659793814">
[Abney 89] S. Abney, &amp;quot;Parsing by Chunks&amp;quot; in C. Tenny,
ed., The MIT Parsing Volume, 1989, to appear.
[Amsler 89] R. Amsler, &amp;quot;Research Towards the Devel-
opment of a Lexical Knowledge Base for Natural
Language Processing&amp;quot; Proceedings of the 1989 SI-
MR Conference, Association for Computing Ma-
chinery. Cambridge, Ma, June 1989.
[Benson 86] M. Benson, E. Benson and R. Ilson, Lexi-
cographic Description of English, John Benjamins
Publishing Company, Philadelphia, 1986.
[Boguraev &amp; Briscoe 89] B. Boguraev &amp; T. Briscoe, in
Computational Lexicography for natural language
processing. B. Boguraev and T. Briscoe editors.
Longmans, NY 1989.
[Choueka 88] Y. Choueka, Looking for Needles in, a
Haystack. In Proceedings of the RIAO, p:609-623,
1988.
[Church 88] K. Church, A Stochastic Parts Program and
Noun Phrase Parser for Unrestricted Text In Pro-
ceedings of the Second Conference on Applied Nat-
ural Language Processing, Austin, Texas, 1988.
[Church 89] K. Church &amp; K. Hanks, Word Association
Norms, Mutual Information, and Lexicography. In
Proceedings of the 27th meeting of the Associ-
ation for Computational Linguistics, Vancouver,
B.C, 1989.
[Cruse 86] D.A. Cruse, Lexical Semantics. Cambridge
University Press, 1986.
[Danlos 87] L. Danlos, The Linguistic Basis of Text
Generation. Cambridge University Press, 1987.
[Desemer &amp; Jabobs 87] D. Desemer &amp; P. Jacobs,
FLUSH: A Flexible Lexicon Design. In proceedings
of the 25th Annual Meeting of the ACL, Stanford
University, CA, 1987.
[Elhadad 90] M. Elhadad, Types in Functional Unifica-
tion Grammars, Proceedings of the 28th meeting
of the Association for Computational Linguistics,
Pittsburgh, PA, 1990.
[Garside 87] R. Garside, G. Leech &amp; G. Sampson, edi-
tors, The computational Analysis of English., a cor-
pus based approach. Longmans, NY 1987.
[Gross 75] M. Gross, Methodes en Synt axe. Hermann,
Paris, France, 1975.
[Halliday 66] M.A.K. Halliday, Lexis as a Linguistic
Level. In C.E. Basel, J.C. Catford, M.A.K Hal-
liday and R.H. Robins (eds.), In memory of
Firth, London: Longmans Linguistics fia Library,
1966, pp: 148-162.
[Iorclanskaja 88] L. lordanskaja, R. Kittredge, A.
Polguere, Lexical Selection and Paraphrase in a
Meaning-Text Generation Model. Presented at the
fourth International Workshop on Language Gen-
eration, Catalina Island, CA, 1988.
[Jacobs 85] P. Jacobs, PIIRED: a generator for natu-
ral language interfaces, Computational Linguis-
tics, volume 11-4, 1985
[Kay 79] M. Kay, Functional Grammar, in Proceedings
of the 5th Meeting of the Berkeley Linguistic So-
ciety, Berkeley Linguistic Society, 1979.
[Mayans 88] J. Klavans, &amp;quot;COMPLEX: a computational
lexicon for natural language systems.&amp;quot; In proceed-
ing of the 12th International Conference on Com-
258 putational Linguistics, Budapest, Hungary, 1988.
[Kukich 83] K. Kukich, Knowledge-Based Report Gen-
eration: A Technique for Automatically Gener-
ating Natural Language Reports from Databases.
Proceedings of the 6th International ACM SIGIR
Conference, Washington, DC, 1983.
[Maarek te Smadja 89] Y.S Maarek &amp; F.A. Smadja, Full
Test Indexing Based on Lexical Relations, An Ap-
plication: Software Libraries. Proceedings of the
12th International ACM SIGIR Conference, Cam-
bridge, Ma, June 1989.
[Mel&apos;euk 81] LA Mel&apos;Euk, Meaning-Text Models: a Re-
cent Trend in Soviet Linguistics. The annual re-
view of anthropology, 1981.
[Nirenburg 88] S. Nirenburg et. al., Lexicon building in
natural language processing. In program and ab-
stracts of the 15th International ALLC, Confer-
ence of the Association for Literary and Linguistic
Computing, Jerusalem, Israel, 1988.
[Smadja 88] F.A. Smadja, Lexical Co-occurrence: The
Missing link. In program and abstracts of the
15&apos;4 International ALLC, Conference of the As-
sociation for Literary and Linguistic Computing,
Jerusalem, Israel, 1988. Also in the Journal for
Literary and Linguistic computin.g, Vol. 4, No. 3,
1989, Oxford University Press.
[Smadja 89a] F.A. Smadja, Microcoding the Lexicon for
Language Generation, First International Work-
shop on Lexical Acquisition, IJCAI&apos;89, Detroit,
Mi, August 89. Also in &amp;quot;Lezical Acquisition: Using
on-line resources to build a lexicon&amp;quot;, MIT press,
Un Zenfik editor, to appear.
[Smadja 89b] F.A. Smadja, On the Use of Flexible Col-
locations for Language Generation. Columbia Uni-
versity, technical report, TR# CUCS-507-89.
</reference>
<page confidence="0.998458">
259
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439810">
<title confidence="0.9953105">AUTOMATICALLY EXTRACTING AND REPRESENTING COLLOCATIONS FOR LANGUAGE GENERATION*</title>
<author confidence="0.857946333333333">Frank A Smadjat</author>
<author confidence="0.857946333333333">Kathleen R McKeown</author>
<affiliation confidence="0.9999625">Department of Computer Science Columbia University</affiliation>
<address confidence="0.97667">York, NY</address>
<abstract confidence="0.9859372">Collocational knowledge is necessary for language gener- The problem collocations come in a large variety of forms. They can involve two, three or more words, these words can be of different syntactic categories and they can be involved in more or less rigid ways. This leads to two main difficulties: collocational knowledge has to be acquired and it must be represented flexibly so that it can be used for language generation. We address both problems in this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be represented in a flexible lexicon using a unification based formalism.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks&amp;quot;</title>
<date>1989</date>
<booktitle>The MIT Parsing Volume,</booktitle>
<editor>in C. Tenny, ed.,</editor>
<note>to appear.</note>
<marker>[Abney 89]</marker>
<rawString>S. Abney, &amp;quot;Parsing by Chunks&amp;quot; in C. Tenny, ed., The MIT Parsing Volume, 1989, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Amsler</author>
</authors>
<title>Research Towards the Development of a Lexical Knowledge Base for Natural Language Processing&amp;quot;</title>
<date>1989</date>
<booktitle>Proceedings of the 1989 SIMR Conference, Association for Computing Machinery.</booktitle>
<location>Cambridge, Ma,</location>
<marker>[Amsler 89]</marker>
<rawString>R. Amsler, &amp;quot;Research Towards the Development of a Lexical Knowledge Base for Natural Language Processing&amp;quot; Proceedings of the 1989 SIMR Conference, Association for Computing Machinery. Cambridge, Ma, June 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
<author>E Benson</author>
<author>R Ilson</author>
</authors>
<title>Lexicographic Description of English,</title>
<date>1986</date>
<publisher>John Benjamins Publishing Company,</publisher>
<location>Philadelphia,</location>
<marker>[Benson 86]</marker>
<rawString>M. Benson, E. Benson and R. Ilson, Lexicographic Description of English, John Benjamins Publishing Company, Philadelphia, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>T Briscoe</author>
</authors>
<date>1989</date>
<booktitle>in Computational Lexicography for natural language</booktitle>
<editor>processing. B. Boguraev and T. Briscoe editors. Longmans,</editor>
<location>NY</location>
<marker>[Boguraev &amp; Briscoe 89]</marker>
<rawString>B. Boguraev &amp; T. Briscoe, in Computational Lexicography for natural language processing. B. Boguraev and T. Briscoe editors. Longmans, NY 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
</authors>
<title>Looking for Needles in, a Haystack.</title>
<date>1988</date>
<booktitle>In Proceedings of the RIAO,</booktitle>
<pages>609--623</pages>
<marker>[Choueka 88]</marker>
<rawString>Y. Choueka, Looking for Needles in, a Haystack. In Proceedings of the RIAO, p:609-623, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<location>Austin, Texas,</location>
<marker>[Church 88]</marker>
<rawString>K. Church, A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text In Proceedings of the Second Conference on Applied Natural Language Processing, Austin, Texas, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>K Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography.</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver, B.C,</location>
<marker>[Church 89]</marker>
<rawString>K. Church &amp; K. Hanks, Word Association Norms, Mutual Information, and Lexicography. In Proceedings of the 27th meeting of the Association for Computational Linguistics, Vancouver, B.C, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press,</publisher>
<marker>[Cruse 86]</marker>
<rawString>D.A. Cruse, Lexical Semantics. Cambridge University Press, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Danlos</author>
</authors>
<title>The Linguistic Basis of Text Generation.</title>
<date>1987</date>
<publisher>Cambridge University Press,</publisher>
<marker>[Danlos 87]</marker>
<rawString>L. Danlos, The Linguistic Basis of Text Generation. Cambridge University Press, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Desemer</author>
<author>P Jacobs</author>
</authors>
<title>FLUSH: A Flexible Lexicon Design.</title>
<date>1987</date>
<booktitle>In proceedings of the 25th Annual Meeting of the ACL,</booktitle>
<location>Stanford University, CA,</location>
<marker>[Desemer &amp; Jabobs 87]</marker>
<rawString>D. Desemer &amp; P. Jacobs, FLUSH: A Flexible Lexicon Design. In proceedings of the 25th Annual Meeting of the ACL, Stanford University, CA, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>Types in Functional Unification Grammars,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th meeting of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh, PA,</location>
<marker>[Elhadad 90]</marker>
<rawString>M. Elhadad, Types in Functional Unification Grammars, Proceedings of the 28th meeting of the Association for Computational Linguistics, Pittsburgh, PA, 1990.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<booktitle>The computational Analysis of English., a corpus based approach.</booktitle>
<editor>R. Garside, G. Leech &amp; G. Sampson, editors,</editor>
<location>Longmans, NY</location>
<marker>[Garside 87]</marker>
<rawString>R. Garside, G. Leech &amp; G. Sampson, editors, The computational Analysis of English., a corpus based approach. Longmans, NY 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gross</author>
</authors>
<title>Methodes en Synt axe.</title>
<date>1975</date>
<location>Hermann, Paris, France,</location>
<marker>[Gross 75]</marker>
<rawString>M. Gross, Methodes en Synt axe. Hermann, Paris, France, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Lexis as a Linguistic Level. In</title>
<date>1966</date>
<booktitle>In memory of Firth, London: Longmans Linguistics fia Library,</booktitle>
<pages>148--162</pages>
<editor>C.E. Basel, J.C. Catford, M.A.K Halliday and R.H. Robins (eds.),</editor>
<marker>[Halliday 66]</marker>
<rawString>M.A.K. Halliday, Lexis as a Linguistic Level. In C.E. Basel, J.C. Catford, M.A.K Halliday and R.H. Robins (eds.), In memory of Firth, London: Longmans Linguistics fia Library, 1966, pp: 148-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L lordanskaja</author>
<author>R Kittredge</author>
<author>A Polguere</author>
</authors>
<title>Lexical Selection and Paraphrase in a Meaning-Text Generation Model. Presented at the fourth International Workshop on Language Generation,</title>
<date>1988</date>
<location>Catalina Island, CA,</location>
<marker>[Iorclanskaja 88]</marker>
<rawString>L. lordanskaja, R. Kittredge, A. Polguere, Lexical Selection and Paraphrase in a Meaning-Text Generation Model. Presented at the fourth International Workshop on Language Generation, Catalina Island, CA, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
</authors>
<title>PIIRED: a generator for natural language interfaces,</title>
<date>1985</date>
<journal>Computational Linguistics,</journal>
<volume>volume</volume>
<pages>11--4</pages>
<marker>[Jacobs 85]</marker>
<rawString>P. Jacobs, PIIRED: a generator for natural language interfaces, Computational Linguistics, volume 11-4, 1985</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional Grammar,</title>
<date>1979</date>
<booktitle>in Proceedings of the 5th Meeting of the</booktitle>
<institution>Berkeley Linguistic Society, Berkeley Linguistic Society,</institution>
<marker>[Kay 79]</marker>
<rawString>M. Kay, Functional Grammar, in Proceedings of the 5th Meeting of the Berkeley Linguistic Society, Berkeley Linguistic Society, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
</authors>
<title>COMPLEX: a computational lexicon for natural language systems.&amp;quot;</title>
<date>1988</date>
<booktitle>In proceeding of the 12th International Conference on Com258 putational Linguistics,</booktitle>
<location>Budapest, Hungary,</location>
<marker>[Mayans 88]</marker>
<rawString>J. Klavans, &amp;quot;COMPLEX: a computational lexicon for natural language systems.&amp;quot; In proceeding of the 12th International Conference on Com258 putational Linguistics, Budapest, Hungary, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Knowledge-Based Report Generation: A Technique for Automatically Generating Natural Language Reports from Databases.</title>
<date>1983</date>
<booktitle>Proceedings of the 6th International ACM SIGIR Conference,</booktitle>
<location>Washington, DC,</location>
<marker>[Kukich 83]</marker>
<rawString>K. Kukich, Knowledge-Based Report Generation: A Technique for Automatically Generating Natural Language Reports from Databases. Proceedings of the 6th International ACM SIGIR Conference, Washington, DC, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Maarek</author>
<author>F A Smadja</author>
</authors>
<title>Full Test Indexing Based on Lexical Relations, An Application: Software Libraries.</title>
<date>1989</date>
<booktitle>Proceedings of the 12th International ACM SIGIR Conference,</booktitle>
<location>Cambridge, Ma,</location>
<marker>[Maarek te Smadja 89]</marker>
<rawString>Y.S Maarek &amp; F.A. Smadja, Full Test Indexing Based on Lexical Relations, An Application: Software Libraries. Proceedings of the 12th International ACM SIGIR Conference, Cambridge, Ma, June 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LA Mel&apos;Euk</author>
</authors>
<title>Meaning-Text Models: a Recent Trend in Soviet Linguistics. The annual review of anthropology,</title>
<date>1981</date>
<marker>[Mel&apos;euk 81]</marker>
<rawString>LA Mel&apos;Euk, Meaning-Text Models: a Recent Trend in Soviet Linguistics. The annual review of anthropology, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenburg</author>
</authors>
<title>Lexicon building in natural language processing.</title>
<date>1988</date>
<booktitle>In program and abstracts of the 15th International ALLC, Conference of the Association for Literary and Linguistic Computing,</booktitle>
<location>Jerusalem, Israel,</location>
<marker>[Nirenburg 88]</marker>
<rawString>S. Nirenburg et. al., Lexicon building in natural language processing. In program and abstracts of the 15th International ALLC, Conference of the Association for Literary and Linguistic Computing, Jerusalem, Israel, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F A Smadja</author>
</authors>
<title>Lexical Co-occurrence: The Missing link. In program and abstracts of the 15&apos;4</title>
<date>1988</date>
<booktitle>International ALLC, Conference of the Association for Literary and Linguistic Computing,</booktitle>
<volume>4</volume>
<publisher>University Press.</publisher>
<location>Jerusalem, Israel,</location>
<marker>[Smadja 88]</marker>
<rawString>F.A. Smadja, Lexical Co-occurrence: The Missing link. In program and abstracts of the 15&apos;4 International ALLC, Conference of the Association for Literary and Linguistic Computing, Jerusalem, Israel, 1988. Also in the Journal for Literary and Linguistic computin.g, Vol. 4, No. 3, 1989, Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F A Smadja</author>
</authors>
<title>Microcoding the Lexicon for Language Generation,</title>
<date></date>
<booktitle>First International Workshop on Lexical Acquisition, IJCAI&apos;89,</booktitle>
<location>Detroit, Mi,</location>
<note>to appear.</note>
<marker>[Smadja 89a]</marker>
<rawString>F.A. Smadja, Microcoding the Lexicon for Language Generation, First International Workshop on Lexical Acquisition, IJCAI&apos;89, Detroit, Mi, August 89. Also in &amp;quot;Lezical Acquisition: Using on-line resources to build a lexicon&amp;quot;, MIT press, Un Zenfik editor, to appear.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F A Smadja</author>
</authors>
<title>On the Use of Flexible Collocations for Language Generation. Columbia University, technical report,</title>
<journal>TR#</journal>
<pages>507--89</pages>
<marker>[Smadja 89b]</marker>
<rawString>F.A. Smadja, On the Use of Flexible Collocations for Language Generation. Columbia University, technical report, TR# CUCS-507-89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>