<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.997893">
A Generalized Alignment-Free Phrase Extraction
</title>
<author confidence="0.88673">
Bing Zhao
</author>
<affiliation confidence="0.717025">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.631231">
Pittsburgh, PA-15213
</address>
<email confidence="0.990921">
bzhao@cs.cmu.edu
</email>
<sectionHeader confidence="0.994633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999685470588235">
In this paper, we present a phrase ex-
traction algorithm using a translation lex-
icon, a fertility model, and a simple dis-
tortion model. Except these models, we
do not need explicit word alignments for
phrase extraction. For each phrase pair (a
block), a bilingual lexicon based score is
computed to estimate the translation qual-
ity between the source and target phrase
pairs; a fertility score is computed to es-
timate how good the lengths are matched
between phrase pairs; a center distortion
score is computed to estimate the relative
position divergence between the phrase
pairs. We presented the results and our
experience in the shared tasks on French-
English.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999904052631579">
Phrase extraction becomes a key component in to-
day’s state-of-the-art statistical machine translation
systems. With a longer context than unigram, phrase
translation models have flexibilities of modelling lo-
cal word-reordering, and are less sensitive to the er-
rors made from preprocessing steps including word
segmentations and tokenization. However, most of
the phrase extraction algorithms rely on good word
alignments. A widely practiced approach explained
in details in (Koehn, 2004), (Och and Ney, 2003)
and (Tillmann, 2003) is to get word alignments from
two directions: source to target and target to source;
the intersection or union operation is applied to get
refined word alignment with pre-designed heuristics
fixing the unaligned words. With this refined word
alignment, the phrase extraction for a given source
phrase is essentially to extract the target candidate
phrases in the target sentence by searching the left
and right projected boundaries.
</bodyText>
<page confidence="0.970521">
141
</page>
<note confidence="0.9887775">
Stephan Vogel
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA-15213
</note>
<email confidence="0.861464">
vogel+@cs.cmu.edu
</email>
<bodyText confidence="0.99971784">
In (Vogel et al., 2004), they treat phrase align-
ment as a sentence splitting problem: given a source
phrase, find the boundaries of the target phrase such
that the overall sentence alignment lexicon probabil-
ity is optimal. We generalize it in various ways, esp.
by using a fertility model to get a better estimation of
phrase lengths, and a phrase level distortion model.
In our proposed algorithm, we do not need ex-
plicit word alignment for phrase extraction. Thereby
it avoids the burden of testing and comparing differ-
ent heuristics especially for some language specific
ones. On the other hand, the algorithm has such flex-
ibilities that one can incorporate word alignment and
heuristics in several possible stages within this pro-
posed framework to further improve the quality of
phrase pairs. In this way, our proposed algorithm
is more generalized than the usual word alignment
based phrase extraction algorithms.
The paper is structured as follows: in section 2,
The concept of blocks is explained; in section 3, a
dynamic programming approach is model the width
of the block; in section 4, a simple center distortion
of the block; in section 5, the lexicon model; the
complete algorithm is in section 6; in section 7, our
experience and results using the proposed approach.
</bodyText>
<sectionHeader confidence="0.975772" genericHeader="introduction">
2 Blocks
</sectionHeader>
<bodyText confidence="0.999795923076923">
We consider each phrase pair as a block within a
given parallel sentence pair, as shown in Figure 1.
The y-axis is the source sentence, indexed word
by word from bottom to top; the x-axis is the target
sentence, indexed word by word from left to right.
The block is defined by the source phrase and its pro-
jection. The source phrase is bounded by the start
and the end positions in the source sentence. The
projection of the source phrase is defined as the left
and right boundaries in the target sentence. Usually,
the boundaries can be inferred according to word
alignment as the left most and right most aligned
positions from the words in the source phrase. In
</bodyText>
<note confidence="0.8650995">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 141–144,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<figureCaption confidence="0.998885">
Figure 1: Blocks with “width” and “centers”
</figureCaption>
<bodyText confidence="0.999967875">
this paper, we provide another view of the block,
which is defined by the centers of source and target
phrases, and the width of the target phrase.
Phrase extraction algorithms in general search
for the left and right projected boundaries of each
source phrase according to some score metric com-
puted for the given parallel sentence pairs. We
present here three models: a phrase level fertility
model score for phrase pairs’ length mismatch, a
simple center-based distortion model score for the
divergence of phrase pairs’ relative positions, and
a phrase level translation score to approximate the
phrase pairs’ translational equivalence. Given a
source phrase, we can search for the best possible
block with the highest combined scores from the
three models.
</bodyText>
<sectionHeader confidence="0.998312" genericHeader="method">
3 Length Model: Dynamic Programming
</sectionHeader>
<bodyText confidence="0.974106913043478">
Given the word fertility definitions in IBM Mod-
els (Brown et al., 1993), we can compute a prob-
ability to predict phrase length: given the candi-
date target phrase (English) eI1, and a source phrase
(French) of length J, the model gives the estima-
tion of P(J|eI1) via a dynamic programming algo-
rithm using the source word fertilities. Figure 2
shows an example fertility trellis of an English tri-
gram. Each edge between two nodes represents one
English word ei. The arc between two nodes rep-
resents one candidate non-zero fertility for ei. The
fertility of zero (i.e. generating a NULL word) cor-
responds to the direct edge between two nodes, and
in this way, the NULL word is naturally incorpo-
rated into this model’s representation. Each arc is
Figure 2: An example of fertility trellis for dynamic
programming
associated with a English word fertility probability
P(φi|ei). A path φI1 through the trellis represents
the number of French words φi generated by each
English word ei. Thus, the probability of generating
J words from the English phrase along the Viterbi
path is:
</bodyText>
<equation confidence="0.998355">
P(J|eI1) = max
1O1,.1=E i=1 Oil
</equation>
<bodyText confidence="0.995069466666667">
The Viterbi path is inferred via dynamic program-
ming in the trellis of the lower panel in Figure 2:
{ φ[j, i − 1] + log PNULL(0|ei)
φ[j − 1, i − 1] + log PO(1|ei)
φ[j − 2, i − 1] + log PO(2|ei)
φ[j − 3, i − 1] + log PO(3|ei)
where PNULL(0|ei) is the probability of generating
a NULL word from ei; PO(k = 1|ei) is the usual
word fertility model of generating one French word
from the word ei; φ[j, i] is the cost so far for gener-
ating j words from i English words ei1 : e1, , ei.
After computing the cost of φ[J, I], we can trace
back the Viterbi path, along which the probability
P(J|eI1) of generating J French words from the En-
glish phrase eI1 as shown in Eqn. 1.
</bodyText>
<figure confidence="0.974477833333333">
Start
End
Left boundary
src center
Width
tgt center
Right boundary
3 3 3
e1 e2 e3
e1 e2 e3
4
3
2
1
....
0 0
2
1
2
1
0
2
1
I
</figure>
<equation confidence="0.871111333333333">
P(φi|ei) (1)
i=1
φ[j, i] = max
</equation>
<page confidence="0.984639">
142
</page>
<bodyText confidence="0.999896">
With this phrase length model, for every candidate
block, we can compute a phrase level fertility score
to estimate to how good the phrase pairs are match
in their lengthes.
</bodyText>
<sectionHeader confidence="0.92384" genericHeader="method">
4 Distortion of Centers
</sectionHeader>
<bodyText confidence="0.9615265">
The centers of source and target phrases are both il-
lustrated in Figure 1. We compute a simple distor-
tion score to estimate how far away the two centers
are in a parallel sentence pair in a sense the block is
close to the diagonal.
In our algorithm, the source center �fj+l
j of the
phrase fj+l
j with length l + 1 is simply a normalized
relative position defined as follows:
</bodyText>
<equation confidence="0.9108355">
Ofj+l =
j
</equation>
<bodyText confidence="0.977001">
where |F |is the French sentence length.
For the center of English phrase ei+k
i in the target
sentence, we first define the expected corresponding
relative center for every French word fj0 using the
lexicalized position score as follows:
</bodyText>
<equation confidence="0.998078666666667">
(i+k)
1 i0=i i0
· P(fj0|ei0)
�ei+k
i (fj0) = |E |· (i+k) (3)
i0=i P(fj0|ei0)
</equation>
<bodyText confidence="0.992450444444445">
where |E |is the English sentence length. P(fj0|ei)
is the word translation lexicon estimated in IBM
Models. i is the position index, which is weighted
by the word level translation probabilities; the term
of Ii=1 P(fj0|ei) provides a normalization so that
the expected center is within the range of target sen-
tence length. The expected center for ei+k
i is simply
a average of Oei+k (fj0):
</bodyText>
<equation confidence="0.962765">
i
Oei+k (fj0) (4)
</equation>
<bodyText confidence="0.922396625">
This is a general framework, and one can certainly
plug in other kinds of score schemes or even word
alignments to get better estimations.
Given the estimated centers of �fj+l
j and
�ei+k
i , we can compute how close they are by
the probability of P(�ei+k
</bodyText>
<equation confidence="0.5886455">
i |�fj+l
j ). To estimate
P(O i+k  |(D j+l ), one can start with a flat gaussian
ei fj
</equation>
<bodyText confidence="0.881534857142857">
model to enforce the point of (�ei+k
i , �fj+l
j ) not too
far off the diagonal and build an initial list of phrase
pairs, and then compute the histogram to approxi-
mate P(O i+k |O j+l).
ei fj
</bodyText>
<sectionHeader confidence="0.985266" genericHeader="method">
5 Lexicon Model
</sectionHeader>
<bodyText confidence="0.986754666666667">
Similar to (Vogel et al., 2004), we compute for each
candidate block a score within a given sentence pair
using a word level lexicon P(f|e) as follows:
</bodyText>
<equation confidence="0.771594">
P (fj+l
j |ei+k
i )
</equation>
<sectionHeader confidence="0.992426" genericHeader="method">
6 Algorithm
</sectionHeader>
<bodyText confidence="0.95984215">
Our phrase extraction is described in Algorithm
1. The input parameters are essentially from IBM
Model-4: the word level lexicon P(f|e), the English
word level fertility Pφ(φe = k|e), and the center
based distortion P((D i+k |O j+l).
ei fj
Overall, for each source phrase fj+l
j , the algo-
rithm first estimates its normalized relative center
in the source sentence, its projected relative cen-
ter in the target sentence. The scores of the phrase
length, center-based distortion, and a lexicon based
score are computed for each candidate block A lo-
cal greedy search is carried out for the best scored
phrase pair (fj+l
j , ei+k
i ).
In our submitted system, we computed the
following seven base scores for phrase pairs:
Pef(fj+l
</bodyText>
<equation confidence="0.971289888888889">
j |ei+k
i ), Pfe(ei+k
i |fj+l
j ), sharing similar
function form in Eqn. 5.
P(fj0|ei0)P(ei0|ei+k
i )
P(fj0|ei0) (5)
k + 1
</equation>
<bodyText confidence="0.82828675">
We compute phrase level relative frequency in both
directions: Prf(fj+l
j |ei+k
i ) and Prf(ei+k
</bodyText>
<equation confidence="0.5526265">
i |fj+l
j ). We
</equation>
<bodyText confidence="0.832012444444444">
compute two other lexicon scores which were also
used in (Vogel et al., 2004): S1(fj+l
j |ei+k
i ) and
S2(ei+k
i |fj+l
j ) using the similar function in Eqn. 6:

S(fj+l
</bodyText>
<equation confidence="0.823990931034483">
j |ei+k
i ) =
j0
1 j0=j+l j0
|F | (2)
j0=j l + 1
1
j+l

j0=j
l + 1
Oei+k =
i
=  P(fj0|ei0)
j0∈[j,j+l] i0∈[i,i+k] k + 1
·
j0 /∈[j,j+l]

i0 /∈[i,i+k]
P(fj0|ei0)
|E |− k − 1
Pef(fj+l = 
j |ei+k j0 i0
i )
=
j0

i0
 P(fj0|ei0) (6)
</equation>
<page confidence="0.889631">
i0
143
</page>
<bodyText confidence="0.856288">
In addition, we put the phrase level fertility score
computed in section 3 via dynamic programming to
be as one additional score for decoding.
Algorithm 1 A Generalized Alignment-free Phrase
Extraction
</bodyText>
<listItem confidence="0.975327928571429">
1: Input: Pre-trained models: PO(Oe = k|e) ,
P(OE|OF) , and P(f|e).
2: Output: PhraseSet: Phrase pair collections.
3: Loop over the next sentence pair
4: for j : 0 → |F |− 1,
5: for l : 0 → MaxLength,
6: foreach f�+l
�
7: compute (Df and OE
8: left = OE · |E|-MaxLength,
9: right= OE · |E|+MaxLength,
10: for i : left → right,
11: for k : 0 → right,
12: compute Oe of e�+k
</listItem>
<bodyText confidence="0.841963">
� ,
13: score the phrase pair (f�+l
</bodyText>
<equation confidence="0.893226333333333">
� , e�+k
� ), where
score = P(Oe|Of)P(l|ez+k)P( fj+l|ei+k)
</equation>
<bodyText confidence="0.845565666666667">
14: add top-n {(f�+l
� , e�+k
� )} into PhraseSet.
</bodyText>
<sectionHeader confidence="0.998198" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.99296456">
Our system is based on the IBM Model-4 param-
eters. We train IBM Model 4 with a scheme of
1720h73043 using GIZA++ (Och and Ney, 2003).
The maximum fertility for an English word is 3. All
the data is used as given, i.e. we do not have any
preprocessing of the English-French data. The word
alignment provided in the workshop is not used in
our evaluations. The language model is provided
by the workshop, and we do not use other language
models.
The French phrases up to 8-gram in the devel-
opment and test sets are extracted with top-3 can-
didate English phrases. There are in total 2.6 mil-
lion phrase pairs 1 extracted for both development
set and the unseen test set. We did minimal tuning
of the parameters in the pharaoh decoder (Koehn,
2004) settings, simply to balance the length penalty
for Bleu score. Most of the weights are left as they
are given: [ttable-limit]=20, [ttable-threshold]=0.01,
&apos;Our phrase table is to be released to public in this workshop
[stack]=100, [beam-threshold]=0.01, [distortion-
limit]=4, [weight-d]=0.5, [weight-l]=1.0, [weight-
w]=-0.5. Table 1 shows the algorithm’s performance
on several settings for the seven basic scores pro-
vided in section 6.
</bodyText>
<table confidence="0.933283666666667">
settings Dev.Bleu Tst.Bleu
s1 27.44 27.65
s2 27.62 28.25
</table>
<tableCaption confidence="0.998488">
Table 1: Pharaoh Decoder Settings
</tableCaption>
<bodyText confidence="0.7971978">
In Table 1, setting s1 was our submission
without using the inverse relative frequency of
Prf(e�+k
� |f�+l
� ). s2 is using all the seven scores.
</bodyText>
<sectionHeader confidence="0.998071" genericHeader="conclusions">
8 Discussions
</sectionHeader>
<bodyText confidence="0.999871555555556">
In this paper, we propose a generalized phrase ex-
traction algorithm towards word alignment-free uti-
lizing the fertility model to predict the width of the
block, a distortion model to predict how close the
centers of source and target phrases are, and a lex-
icon model for translational equivalence. The algo-
rithm is a general framework, in which one could
plug in other scores and word alignment to get bet-
ter results.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999845904761905">
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263–331.
Philip Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based smt. In Proceedings of the Confer-
ence of the Association for Machine Translation in the
Americans (AMTA).
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19–51.
Christoph Tillmann. 2003. A projection extension algo-
rithm for statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
Stephan Vogel, Sanjika Hewavitharana, Muntsin Kolss,
and Alex Waibel. 2004. The ISL statistical translation
system for spoken language translation. In Proc. of the
International Workshop on Spoken Language Transla-
tion, pages 65–72, Kyoto, Japan.
</reference>
<page confidence="0.998596">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.198003">
<title confidence="0.999889">A Generalized Alignment-Free Phrase Extraction</title>
<author confidence="0.609551">Bing</author>
<affiliation confidence="0.786901333333333">Language Technologies Carnegie Mellon Pittsburgh,</affiliation>
<email confidence="0.999915">bzhao@cs.cmu.edu</email>
<abstract confidence="0.998639294117647">In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model. Except these models, we do not need explicit word alignments for phrase extraction. For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs. We presented the results and our experience in the shared tasks on French-</abstract>
<intro confidence="0.437492">English.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>263--331</pages>
<marker>Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, Stephen A. Della Pietra, Vincent. J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, volume 19(2), pages 263–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based smt.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americans (AMTA).</booktitle>
<contexts>
<context position="1328" citStr="Koehn, 2004" startWordPosition="196" endWordPosition="197">e between the phrase pairs. We presented the results and our experience in the shared tasks on FrenchEnglish. 1 Introduction Phrase extraction becomes a key component in today’s state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. 141 Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vog</context>
<context position="11714" citStr="Koehn, 2004" startWordPosition="2067" endWordPosition="2068">). The maximum fertility for an English word is 3. All the data is used as given, i.e. we do not have any preprocessing of the English-French data. The word alignment provided in the workshop is not used in our evaluations. The language model is provided by the workshop, and we do not use other language models. The French phrases up to 8-gram in the development and test sets are extracted with top-3 candidate English phrases. There are in total 2.6 million phrase pairs 1 extracted for both development set and the unseen test set. We did minimal tuning of the parameters in the pharaoh decoder (Koehn, 2004) settings, simply to balance the length penalty for Bleu score. Most of the weights are left as they are given: [ttable-limit]=20, [ttable-threshold]=0.01, &apos;Our phrase table is to be released to public in this workshop [stack]=100, [beam-threshold]=0.01, [distortionlimit]=4, [weight-d]=0.5, [weight-l]=1.0, [weightw]=-0.5. Table 1 shows the algorithm’s performance on several settings for the seven basic scores provided in section 6. settings Dev.Bleu Tst.Bleu s1 27.44 27.65 s2 27.62 28.25 Table 1: Pharaoh Decoder Settings In Table 1, setting s1 was our submission without using the inverse relat</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philip Koehn. 2004. Pharaoh: a beam search decoder for phrase-based smt. In Proceedings of the Conference of the Association for Machine Translation in the Americans (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<pages>pages</pages>
<contexts>
<context position="1349" citStr="Och and Ney, 2003" startWordPosition="198" endWordPosition="201">hrase pairs. We presented the results and our experience in the shared tasks on FrenchEnglish. 1 Introduction Phrase extraction becomes a key component in today’s state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. 141 Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vogel et al., 2004), the</context>
<context position="11103" citStr="Och and Ney, 2003" startWordPosition="1955" endWordPosition="1958">and P(f|e). 2: Output: PhraseSet: Phrase pair collections. 3: Loop over the next sentence pair 4: for j : 0 → |F |− 1, 5: for l : 0 → MaxLength, 6: foreach f�+l � 7: compute (Df and OE 8: left = OE · |E|-MaxLength, 9: right= OE · |E|+MaxLength, 10: for i : left → right, 11: for k : 0 → right, 12: compute Oe of e�+k � , 13: score the phrase pair (f�+l � , e�+k � ), where score = P(Oe|Of)P(l|ez+k)P( fj+l|ei+k) 14: add top-n {(f�+l � , e�+k � )} into PhraseSet. 7 Experimental Results Our system is based on the IBM Model-4 parameters. We train IBM Model 4 with a scheme of 1720h73043 using GIZA++ (Och and Ney, 2003). The maximum fertility for an English word is 3. All the data is used as given, i.e. we do not have any preprocessing of the English-French data. The word alignment provided in the workshop is not used in our evaluations. The language model is provided by the workshop, and we do not use other language models. The French phrases up to 8-gram in the development and test sets are extracted with top-3 candidate English phrases. There are in total 2.6 million phrase pairs 1 extracted for both development set and the unseen test set. We did minimal tuning of the parameters in the pharaoh decoder (K</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29, pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
</authors>
<title>A projection extension algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1370" citStr="Tillmann, 2003" startWordPosition="203" endWordPosition="204">d the results and our experience in the shared tasks on FrenchEnglish. 1 Introduction Phrase extraction becomes a key component in today’s state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. 141 Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vogel et al., 2004), they treat phrase alignm</context>
</contexts>
<marker>Tillmann, 2003</marker>
<rawString>Christoph Tillmann. 2003. A projection extension algorithm for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Sanjika Hewavitharana</author>
<author>Muntsin Kolss</author>
<author>Alex Waibel</author>
</authors>
<title>The ISL statistical translation system for spoken language translation.</title>
<date>2004</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation,</booktitle>
<pages>65--72</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="1944" citStr="Vogel et al., 2004" startWordPosition="285" endWordPosition="288">04), (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. 141 Stephan Vogel Language Technologies Institute Carnegie Mellon University Pittsburgh, PA-15213 vogel+@cs.cmu.edu In (Vogel et al., 2004), they treat phrase alignment as a sentence splitting problem: given a source phrase, find the boundaries of the target phrase such that the overall sentence alignment lexicon probability is optimal. We generalize it in various ways, esp. by using a fertility model to get a better estimation of phrase lengths, and a phrase level distortion model. In our proposed algorithm, we do not need explicit word alignment for phrase extraction. Thereby it avoids the burden of testing and comparing different heuristics especially for some language specific ones. On the other hand, the algorithm has such f</context>
<context position="8698" citStr="Vogel et al., 2004" startWordPosition="1490" endWordPosition="1493">age of Oei+k (fj0): i Oei+k (fj0) (4) This is a general framework, and one can certainly plug in other kinds of score schemes or even word alignments to get better estimations. Given the estimated centers of �fj+l j and �ei+k i , we can compute how close they are by the probability of P(�ei+k i |�fj+l j ). To estimate P(O i+k |(D j+l ), one can start with a flat gaussian ei fj model to enforce the point of (�ei+k i , �fj+l j ) not too far off the diagonal and build an initial list of phrase pairs, and then compute the histogram to approximate P(O i+k |O j+l). ei fj 5 Lexicon Model Similar to (Vogel et al., 2004), we compute for each candidate block a score within a given sentence pair using a word level lexicon P(f|e) as follows: P (fj+l j |ei+k i ) 6 Algorithm Our phrase extraction is described in Algorithm 1. The input parameters are essentially from IBM Model-4: the word level lexicon P(f|e), the English word level fertility Pφ(φe = k|e), and the center based distortion P((D i+k |O j+l). ei fj Overall, for each source phrase fj+l j , the algorithm first estimates its normalized relative center in the source sentence, its projected relative center in the target sentence. The scores of the phrase le</context>
</contexts>
<marker>Vogel, Hewavitharana, Kolss, Waibel, 2004</marker>
<rawString>Stephan Vogel, Sanjika Hewavitharana, Muntsin Kolss, and Alex Waibel. 2004. The ISL statistical translation system for spoken language translation. In Proc. of the International Workshop on Spoken Language Translation, pages 65–72, Kyoto, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>