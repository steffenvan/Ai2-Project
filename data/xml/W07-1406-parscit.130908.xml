<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000343">
<title confidence="0.998615">
Recognizing Textual Entailment Using Sentence Similarity based on
Dependency Tree Skeletons
</title>
<author confidence="0.887127">
Rui Wang and Günter Neumann
</author>
<affiliation confidence="0.590198">
LT-lab, DFKI
</affiliation>
<address confidence="0.905314">
Stuhlsatzenhausweg 3, 66123 Saarbrücken, Germany
</address>
<email confidence="0.996899">
{wang.rui,Neumann}@dfki.de
</email>
<sectionHeader confidence="0.997352" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888214285714">
We present a novel approach to RTE that
exploits a structure-oriented sentence rep-
resentation followed by a similarity func-
tion. The structural features are automati-
cally acquired from tree skeletons that are
extracted and generalized from dependency
trees. Our method makes use of a limited
size of training data without any external
knowledge bases (e.g. WordNet) or hand-
crafted inference rules. We have achieved
an accuracy of 71.1% on the RTE-3 devel-
opment set performing a 10-fold cross
validation and 66.9% on the RTE-3 test
data.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999695745454546">
Textual entailment has been introduced as a rela-
tion between text expressions, capturing the fact
that the meaning of one expression can be inferred
from the other (Dagan and Glickman, 2004). More
precisely, textual entailment is defined as “... a
relationship between a coherent text T and a lan-
guage expression, which is considered as a hy-
pothesis, H. We say that T entails H (H is a conse-
quent of T), denoted by T ⇒ H, if the meaning of
H, as interpreted in the context of T, can be in-
ferred from the meaning of T.”
Table 1 displays several examples from the
RTE-3 development set. For the third pair (id=410)
the key knowledge needed to decide whether the
entailment relation holds is that “[PN1]’s wife,
[PN2]” entails “The name of [PN1]’s wife is
[PN2]”, although T contains much more (irrelevant)
information. On the other hand, the first pair (id=1)
requires an understanding of concepts with oppo-
site meanings (i.e. “buy” and “sell”), which is a
case of semantic entailment.
The different sources of possible entailments
motivated us to consider the development of spe-
cialized entailment strategies for different NLP
tasks. In particular, we want to find out the poten-
tial connections between entailment relations be-
longing to different linguistic layers for different
applications.
In this paper, we propose a novel approach to-
wards structure-oriented entailment based on our
empirical discoveries from the RTE corpora: 1) H
is usually textually shorter than T; 2) not all infor-
mation in T is relevant to make decisions for the
entailment; 3) the dissimilarity of relations among
the same topics between T and H are of great im-
portance.
Based on the observations, our primary method
starts from H to T (i.e. in the opposite direction of
the entailment relation) so as to exclude irrelevant
information from T. Then corresponding key top-
ics and predicates of both elements are extracted.
We then represent the structural differences be-
tween T and H by means of a set of Closed-Class
Symbols. Finally, these acquired representations
(named Entailment Patterns - EPs) are classified by
means of a subsequence kernel.
The Structure Similarity Function is combined
with two robust backup strategies, which are re-
sponsible for cases that are not handled by the EPs.
One is a Triple Similarity Function applied on top
of the local dependency relations of T and H; the
other is a simple Bag-of-Words (BoW) approach
that calculates the overlapping ratio of H and T.
Together, these three methods deal with different
entailment cases in practice.
</bodyText>
<page confidence="0.983436">
36
</page>
<note confidence="0.4416045">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 36–41,
Prague, June 2007. @2007 Association for Computational Linguistics
</note>
<table confidence="0.997257">
Id Task Text Hypothesis Entails?
1 IE The sale was made to pay Yukos&apos; US$ 27.5 billion tax bill, Yuganskneftegaz Baikalfinansgroup YES
was originally sold for US$ 9.4 billion to a little known company was sold to
Baikalfinansgroup which was later bought by the Russian state-owned oil Rosneft.
390 IR Typhoon Xangsane lashed the Philippine capital on Thursday, A typhoon batters YES
grounding flights, halting vessels and closing schools and markets after the Philippines .
triggering fatal flash floods in the centre of the country.
410 QA The name of YES
(Sentence 1 ...). Along with the first lady&apos;s mother, Jenna Welch, the George H.W.
weekend gathering includes the president&apos;s parents, form er President Bush&apos;s wife is
George H .W. Bush and his wife, Barbara ; his sister Doro Koch and her Barbara.
husband, Bobby; and his brother, Marvin, and his wife, Margaret.
739 SUM The FDA NO
The FDA would not say in which states the pills had been sold, but provided a list of
instead recom m ended that custom ers determ ine whether products they states in which the
bought are being recalled by checking the store list on the FDA Web site, pills have been
and the batch list. The batch numbers appear on the container&apos;s label.
</table>
<tableCaption confidence="0.997865">
Table 1 Examples from RTE-3
</tableCaption>
<sectionHeader confidence="0.999739" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99989576">
Conventional methods for RTE define measures for
the similarity between T and H either by assuming
an independence between words (Corley and Mi-
halcea, 2005) in a BoW fashion or by exploiting
syntactic interpretations. (Kouylekov and Magnini,
2006) explore a syntactic tree editing distance to
detect entailment relations. Since they calculate the
similarity between the two dependency trees of T
and H directly, the noisy information may decrease
accuracy. This observation actually motivated us to
start from H towards the most relevant information
in T.
Logic rules (as proposed by (Bos and Markert,
2005)) or sequences of allowed rewrite rules (as in
(de Salvo Braz et al., 2005)) are another fashion of
tackling RTE. One the best two teams in RTE-2
(Tatu et al., 2006) proposed a knowledge represen-
tation model which achieved about 10% better per-
formance than the third (Zanzotto and Moschitti,
2006) based on their logic prover. The other best
team in RTE-2 (Hickl et al., 2006) automatically
acquired extra training data, enabling them to
achieve about 10% better accuracy than the third as
well. Consequently, obtaining more training data
and embedding deeper knowledge were expected
</bodyText>
<figureCaption confidence="0.955058">
Figure 1 Overview of RTE
</figureCaption>
<bodyText confidence="0.99996319047619">
to be the two main directions pointed out for future
research in the RTE-2 summary statement. How-
ever, except for the positive cases of SUM, T-H
pairs are normally not very easy to collect auto-
matically. Multi-annotator agreement is difficult to
reach on most of the cases as well. The knowledge-
based approach also has its caveats since logical
rules are usually implemented manually and there-
fore require a high amount of specialized human
expertise in different NLP areas.
Another group (Zanzotto and Moschitti, 2006)
utilized a tree kernel method for cross-pair similar-
ity, which showed an improvement, and this has
motivated us to investigate kernel-based methods.
The main difference in our method is that we apply
subsequence kernels on patterns extracted from the
dependency trees of T and H, instead of applying
tree kernels on complete parsing trees. On the one
hand, this allows us to discover essential parts in-
dicating an entailment relationship, and on the
other hand, computational complexity is reduced.
</bodyText>
<sectionHeader confidence="0.946902" genericHeader="method">
3 An Overview of RTE
</sectionHeader>
<bodyText confidence="0.999807230769231">
Figure 1 shows the different processing techniques
and depths applied to the RTE task. Our work fo-
cuses on constructing a similarity function operat-
ing between sentences. In detail, it consists of sev-
eral similarity scores with different domains of
locality on top of the dependency structure. Figure
2 gives out the workflow of our system. The main
part of the sentence similarity function is the Struc-
ture Similarity Function; two other similarity
scores are calculated by our backup strategies. The
first backup strategy is a straightforward BoW
method that we will not present in this paper (see
more details in (Corley and Mihalcea, 2005));
</bodyText>
<page confidence="0.99875">
37
</page>
<figureCaption confidence="0.986719">
Figure 2 Workflow of the System
</figureCaption>
<bodyText confidence="0.99994775">
while the second one is based on a triple set repre-
sentation of sentences that expresses the local de-
pendency relations found by a parser1.
A dependency structure consists of a set of triple
relations (TRs). A TR is of the form &lt;node1, rela-
tion, node2&gt;, where node1 represents the head,
node2 the modifier and relation the dependency
relation. Chief requirements for the backup system
are robustness and simplicity. Accordingly, we
construct a similarity function, the Triple Similar-
ity Function (TSF), which operates on two triple
sets and determines how many triples of H2 are
contained in T. The core assumption here is that
the higher the number of matching triple elements,
the more similar both sets are, and the more likely
it is that T entails H.
TSF uses an approximate matching function.
Different cases (i.e. ignoring either the parent node
or the child node, or the relation between nodes)
might provide different indications for the similar-
ity of T and H. In all cases, a successful match be-
tween two nodes means that they have the same
lemma and POS. We then sum them up using dif-
ferent weights and divide the result by the cardinal-
ity of H for normalization. The different weights
learned from the corpus indicate that the “amount
of missing linguistic information” affect entailment
decisions differently.
</bodyText>
<sectionHeader confidence="0.779463" genericHeader="method">
4 Workflow of the Main Approach
</sectionHeader>
<bodyText confidence="0.9974384">
Our Structure Similarity Function is based on the
hypothesis that some particular differences be-
tween T and H will block or change the entailment
relationship. Initially we assume when judging the
entailment relation that it holds for each T-H pair
</bodyText>
<footnote confidence="0.72947725">
1 We are using Minipar (Lin, 1998) and Stanford Parser (Klein
and Manning, 2003) as preprocessors, see also sec. 5.2.
2 Note that henceforth T and H will represent either the origi-
nal texts or the dependency structures.
</footnote>
<bodyText confidence="0.999369">
(using the default value “YES”). The major steps
are as follows (see also Figure 2):
</bodyText>
<subsectionHeader confidence="0.986287">
4.1 Tree Skeleton Extractor
</subsectionHeader>
<bodyText confidence="0.9938173">
Since we assume that H indicates how to extract
relevant parts in T for the entailment relation, we
start from the Tree Skeleton of H (TSH). First, we
construct a set of keyword pairs using all the nouns
that appear in both T and H. In order to increase
the hits of keyword pairs, we have applied a partial
search using stemming and some word variation
techniques on the substring level. For instance, the
pair (id=390) in Table 1 has the following list of
keyword pairs,
</bodyText>
<equation confidence="0.9399185">
&lt;Typhoon_Xangsane ## typhoon,
Philippine ## Philippines&gt;
</equation>
<bodyText confidence="0.999922714285714">
Then we mark the keywords in the dependency
trees of T and H and extract the sub-trees by ignor-
ing the inner yields. Usually, the Root Node of H
(RNH) is the main verb; all the keywords are con-
tained in the two spines of TSH (see Figure 3).
Note that in the Tree Skeleton of T (TST), 1) the
Root Node (RNT) can either be a verb, a noun or
even a dependency relation, and 2) if the two Foot
Nodes (FNs) belong to two sentences, a dummy
node is created that connects the two spines.
Thus, the prerequisite for this algorithm is that
TSH has two spines containing all keywords in H,
and T satisfies this as well. For the RTE-3 devel-
opment set, we successfully extracted tree skele-
</bodyText>
<figureCaption confidence="0.596599">
Figure 3 Example of a Tree Skeleton
</figureCaption>
<page confidence="0.995329">
38
</page>
<bodyText confidence="0.997615294117647">
tons from 254 pairs, i.e., 32% of the data is cov-
ered by this step, see also sec. 5.2.
Next, we collapse some of the dependency rela-
tion names from the parsers to more generalized
tag names, e.g., collapsing &lt;OBJ2&gt; and &lt;DESC&gt;
to &lt;OBJ&gt;. We group together all nodes that have
relation labels like &lt;CONJ&gt; or &lt;NN&gt;, since they
are assumed to refer to the same entity or belong to
one class of entities sharing some common charac-
teristics. Lemmas are removed except for the key-
words. Finally, we add all the tags to the CCS set.
Since a tree skeleton TS consists of spines con-
nected via the same root node, TS can be trans-
formed into a sequence. Figure 4 displays an ex-
ample corresponding to the second pair (id=390) of
Table 1. Thus, the general form of a sequential rep-
resentation of a tree skeleton is:
</bodyText>
<sectionHeader confidence="0.680083" genericHeader="method">
LSP #RN# RSP
</sectionHeader>
<bodyText confidence="0.998912692307692">
where LSP represents the Left Spine, RSP repre-
sents the Right Spine, and RN is the Root Node.
On basis of this representation, a comparison of the
two tree skeletons is straightforward: 1) merge the
two LSPs by excluding the longest common prefix,
and 2) merge the two RSPs by excluding the long-
est common suffix. Then the Spine Difference (SD)
is defined as the remaining infixes, which consists
of two parts, SDT and SDH. Each part can be either
empty (i.e. E) or a CCS sequence. For instance, the
two SDs of the example in Figure 4 (id=390) are
(LSD – Left SD; RSD – Right SD; ## is a separa-
tor sign):
</bodyText>
<equation confidence="0.9922255">
LSDT(N) ## LSDH(E)
RSDT(E) ## RSDH(E)
</equation>
<bodyText confidence="0.999963625">
We have observed that two neighboring depend-
ency relations of the root node of a tree skeleton
(&lt;SUBJ&gt; or &lt;OBJ&gt;) can play important roles in
predicting the entailment relation as well. There-
fore, we assign them two extra features named
Verb Consistence (VC) and Verb Relation Con-
sistence (VRC). The former indicates whether two
root nodes have a similar meaning, and the latter
</bodyText>
<figureCaption confidence="0.804406">
Figure 4 Spine Merging
</figureCaption>
<bodyText confidence="0.999076153846154">
indicates whether the relations are contradictive
(e.g. &lt;SUBJ&gt; and &lt;OBJ&gt; are contradictive).
We represent the differences between TST and
TSH by means of an Entailment Pattern (EP),
which is a quadruple &lt;LSD, RSD, VC, VRC&gt;. VC
is either true or false, meaning that the two RNs
are either consistent or not. VRC has ternary value,
whereby 1 means that both relations are consistent,
-1 means at least one pair of corresponding rela-
tions is inconsistent, and 0 means RNT is not a
verb.3 The set of EPs defines the feature space for
the subsequence kernels in our Structure Similarity
Function.
</bodyText>
<subsectionHeader confidence="0.994287">
4.2 Structure Similarity Function
</subsectionHeader>
<bodyText confidence="0.999821238095238">
We define the function by constructing two basic
kernels to process the LSD and RSD part of an EP,
and two trivial kernels for VC and VRC. The four
kernels are combined linearly by a composite ker-
nel that performs binary classification on them.
Since all spine differences SDs are either empty
or CCS sequences, we can utilize subsequence
kernel methods to represent features implicitly, cf.
(Bunescu and Mooney, 2006). Our subsequence
kernel function is:
whereby T and H refers to all spine differences
SDs from T and H, and |T |and |H |represent the
cardinalities of SDs. The function KCCS(CCS,CCS’)
checks whether its arguments are equal.
Since the RTE task checks the relationship be-
tween T and H, we need to consider collocations
of some CCS subsequences between T and H as
well. Essentially, this kernel evaluates the similar-
ity of T and H by means of those CCS subse-
quences appearing in both elements. The kernel
function is as follows:
</bodyText>
<equation confidence="0.998014666666667">
K ( , , &apos; , &apos; )
&lt;T H T H
&gt; &lt; &gt;
collocatio n
 ||
T  |&apos;|
T  ||
H  |&apos;|
H
= ∑∑∑∑ KCCS (CCSi, CCSi&apos;) ⋅
i= = = =
1 i&apos; 1 j 1 j&apos; 1
</equation>
<bodyText confidence="0.732509666666667">
On top of the two simple kernels, KVC, and KVRC,
we use a composite kernel to combine them line-
arly with different weights:
</bodyText>
<figure confidence="0.916165428571429">
Kcomposite αKsubsequence +YKcollocation + γ KVC + δKVRC ,
3 Note that RNH is guaranteed to be a verb, because otherwise
the pair would have been delegated to the backup strategies.
)
e
( &lt;
Ksubsequenc
T,H, &apos;, &apos;
&gt; &lt; T H &gt;
|
|
|
&apos;
&apos;
</figure>
<equation confidence="0.959013285714286">
T
 ||
T
 ||
H  |H
&apos;
)
+ ∑ ∑
j=1j &apos;=
KCCS (CCSi,CCSi
1
K ( ,
CCS CCS j
1 CCS j
=
∑ ∑
= 1i &apos;=
i
&apos;
)
KCCS (CCSj, CCSj&apos;)
</equation>
<page confidence="0.995237">
39
</page>
<bodyText confidence="0.9989855">
where y and S are learned from the training corpus;
α=R=1.
</bodyText>
<sectionHeader confidence="0.995307" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999667222222222">
We have evaluated four methods: the two backup
systems as baselines (BoW and TSM, the Triple
Set Matcher) and the kernel method combined with
the backup strategies using different parsers, Mini-
par (Mi+SK+BS) and the Stanford Parser
(SP+SK+BS). The experiments are based on RTE-
3 Data4. For the kernel-based classification, we
used the classifier SMO from the WEKA toolkit
(Witten and Frank, 1999).
</bodyText>
<subsectionHeader confidence="0.993678">
5.1 Experiment Results
</subsectionHeader>
<bodyText confidence="0.905221">
RTE-3 data include the Dev Data (800 T-H pairs,
each task has 200 pairs) and the Test Data (same
size). Experiment A performs a 10-fold cross-
validation on Dev Data; Experiment B uses Dev
Data for training and Test Data for testing cf. Table
2 (the numbers denote accuracies):
</bodyText>
<table confidence="0.9994037">
Systems\Tasks IE IR QA SUM All
Exp A: 10-fold Cross Validation on RTE-3 Dev Data
BoW 54.5 70 76.5 68.5 67.4
TSM 53.5 60 68 62.5 61.0
Mi+SK+BS 63 74 79 68.5 71.1
SP+SK+BS 60.5 70 81.5 68.5 70.1
Exp B: Train: Dev Data; Test: Test Data
BoW 54.5 66.5 76.5 56 63.4
TSM 54.5 62.5 66 54.5 59.4
Mi+SP+SK+BS 58.5 70.5 79.5 59 66.9
</table>
<tableCaption confidence="0.995764">
Table 2 Results on RTE-3 Data
</tableCaption>
<bodyText confidence="0.950339538461539">
For the IE task, Mi+SK+BS obtained the highest
improvement over the baseline systems, suggesting
that the kernel method seems to be more appropri-
ate if the underlying task conveys a more “rela-
tional nature.” Improvements in the other tasks are
less convincing as compared to the baselines. Nev-
ertheless, the overall result obtained in experiment
B would have been among the top 3 of the RTE-2
challenge. We utilize the system description table
of (Bar-Haim et al., 2006) to compare our system
with the best two systems of RTE-2 in Table 35:
4 See (Wang and Neumann, 2007) for details concerning the
experiments of our method on RTE-2 data.
</bodyText>
<footnote confidence="0.876493833333333">
5 Following the notation in (Bar-Haim et al., 2006): Lx: Lexi-
cal Relation DB; Ng: N-Gram / Subsequence overlap; Sy:
Syntactic Matching / Alignment; Se: Semantic Role Labeling;
LI: Logical Inference; C: Corpus/Web; M: ML Classification;
B: Paraphrase Technology / Background Knowledge; L: Ac-
quisition of Entailment Corpora.
</footnote>
<note confidence="0.949137">
Systems Lx Ng Sy Se LI C M B L
Hickl et al. X X X X X X X
Tatu et al. X X X
Ours X X X
</note>
<tableCaption confidence="0.994149">
Table 3 Comparison with the top 2 systems in
</tableCaption>
<bodyText confidence="0.9655982">
RTE-2.
Note that the best system (Hickl et al., 2006) ap-
plies both shallow and deep techniques, especially
in acquiring extra entailment corpora. The second
best system (Tatu et al., 2006) contains many
manually designed logical inference rules and
background knowledge. On the contrary, we ex-
ploit no additional knowledge sources besides the
dependency trees computed by the parsers, nor any
extra training corpora.
</bodyText>
<subsectionHeader confidence="0.864953">
5.2 Discussions
</subsectionHeader>
<tableCaption confidence="0.5796305">
Table 4 shows how our method performs for the
task-specific pairs matched by our patterns:
</tableCaption>
<table confidence="0.9983786">
Tasks IE IR QA SUM ALL
ExpA:Matched 53 19 23.5 31.5 31.8
ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8
ExpB:Matched 58.5 16 27.5 42 36
ExpB:Accuracy 57.2 81.5 90.9 65.5 68.8
</table>
<tableCaption confidence="0.968517">
Table 4 Performances of our method
</tableCaption>
<bodyText confidence="0.999713923076923">
For IE pairs, we find good coverage, whereas
for IR and QA pairs the coverage is low, though it
achieves good accuracy. According to the experi-
ments, BoW has already achieved the best per-
formance for SUM pairs cf. Table 2.
As a whole, developing task specific entailment
operators is a promising direction. As we men-
tioned in the first section, the RTE task is neither a
one-level nor a one-case task. The experimental
results uncovered differences among pairs of dif-
ferent tasks with respect to accuracy and coverage.
On the one hand, our method works successfully
on structure-oriented T-H pairs, most of which are
from IE. If both TST and TSH can be transformed
into CCS sequences, the comparison performs well,
as in the case of the last example (id=410) in Table
1. Here, the relation between “wife”, “name”, and
“Barbara” is conveyed by the punctuation “,”, the
verb “is”, and the preposition “of”. Other cases like
the “work for” relation of a person and a company
or the “is located in” relation between two location
names are normally conveyed by the preposition
“of”. Based on these findings, taking into account
more carefully the lexical semantics based on in-
ference rules of functional words might be helpful
in improving RTE.
</bodyText>
<page confidence="0.994415">
40
</page>
<bodyText confidence="0.999986">
On the other hand, accuracy varies with T-H
pairs from different tasks. Since our method is
mainly structure-oriented, differences in modifiers
may change the results and would not be caught
under the current version of our tree skeleton. For
instance, “a commercial company” will not entail
“a military company”, even though they are struc-
turally equivalent.
Most IE pairs are constructed from a binary rela-
tion, and so meet the prerequisite of our algorithm
(see sec. 4.1). However, our method still has rather
low coverage. T-H pairs from other tasks, for ex-
ample like IR and SUM, usually contain more in-
formation, i.e. more nouns, the dependency trees of
which are more complex. For instance, the pair
(id=739) in Table 1 contains four keyword pairs
which we cannot handle by our current method.
This is one reason why we have constructed extra
T-H pairs from MUC, TREC, and news articles
following the methods of (Bar-Haim et al., 2006).
Still, the overall performance does not improve.
All extra training data only serves to improve the
matched pairs (about 32% of the data set) for
which we already have high accuracy (see Table 4).
Thus, extending coverage by machine learning
methods for lexical semantics will be the main fo-
cus of our future work.
</bodyText>
<sectionHeader confidence="0.999487" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999167">
Applying different RTE strategies for different
NLP tasks is a reasonable solution. We have util-
ized a structure similarity function to deal with the
structure-oriented pairs, and applied backup strate-
gies for the rest. The results show the advantage of
our method and direct our future work as well. In
particular, we will extend the tree skeleton extrac-
tion by integrating lexical semantics based on in-
ference rules for functional words in order to get
larger domains of locality.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99849925">
The work presented here was partially supported
by a research grant from BMBF to the DFKI pro-
ject HyLaP (FKZ: 01 IW F02) and the EC-funded
project QALL-ME.
</bodyText>
<sectionHeader confidence="0.999588" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999924645833333">
Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-
colo, D., Magnini, B. and Szpektor, I. 2006. The Sec-
ond PASCAL Recognising Textual Entailment Chal-
lenge. In Proc. of the PASCAL RTE-2 Challenge.
Bos, J. and Markert, K. 2005. Combining Shallow and
Deep NLP Methods for Recognizing Textual Entail-
ment. In Proc. of the PASCAL RTE Challenge.
Bunescu, R. and Mooney, R. 2006. Subsequence Ker-
nels for Relation Extraction. In Advances in Neural
Information Processing Systems 18. MIT Press.
Corley, C. and Mihalcea, R. 2005. Measuring the Se-
mantic Similarity of Texts. In Proc. of the ACL
Workshop on Empirical Modeling of Semantic
Equivalence and Entailment.
Dagan, R., Glickman, O. 2004. Probabilistic textual
entailment: Generic applied modelling of language
variability. In PASCAL Workshop on Text Under-
standing and Mining.
de Salvo Braz, R., Girju, R., Punyaka-nok, V., Roth, D.,
and Sammons, M. 2005. An Inference Model for Se-
mantic Entailment in Natural Language. In Proc. of
the PASCAL RTE Challenge.
Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink,
B. and Shi, Y. 2006. Recognizing Textual Entailment
with LCC’s GROUNDHOG System. In Proc. of the
PASCAL RTE-2 Challenge.
Klein, D. and Manning, C. 2003. Accurate Unlexical-
ized Parsing. In Proc. of ACL 2003.
Kouylekov, M. and Magnini, B. 2006. Tree Edit Dis-
tance for Recognizing Textual Entailment: Estimat-
ing the Cost of Insertion. In Proc. of the PASCAL
RTE-2 Challenge.
Lin, D. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems.
Tatu, M., Iles, B., Slavik, J., Novischi, A. and Moldo-
van, D. 2006. COGEX at the Second Recognizing
Textual Entailment Challenge. In Proc. of the PAS-
CAL RTE-2 Challenge.
Wang, R. and Neumann, G. 2007. Recognizing Textual
Entailment Using a Subsequence Kernel Method. In
Proc. of AAAI 2007.
Witten, I. H. and Frank, E. Weka: Practical Machine
Learning Tools and Techniques with Java Implemen-
tations. Morgan Kaufmann, 1999.
Zanzotto, F.M. and Moschitti, A. 2006. Automatic
Learning of Textual Entailments with Cross-pair
Similarities. In Proc. of ACL 2006.
</reference>
<page confidence="0.999448">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399361">
<title confidence="0.999127">Recognizing Textual Entailment Using Sentence Similarity based Dependency Tree Skeletons</title>
<author confidence="0.990154">Wang</author>
<email confidence="0.620657">LT-lab,</email>
<address confidence="0.732347">Stuhlsatzenhausweg 3, 66123 Saarbrücken,</address>
<email confidence="0.997314">wang.rui@dfki.de</email>
<email confidence="0.997314">Neumann@dfki.de</email>
<abstract confidence="0.997438066666667">We present a novel approach to RTE that exploits a structure-oriented sentence representation followed by a similarity function. The structural features are automatically acquired from tree skeletons that are extracted and generalized from dependency trees. Our method makes use of a limited size of training data without any external knowledge bases (e.g. WordNet) or handcrafted inference rules. We have achieved an accuracy of 71.1% on the RTE-3 development set performing a 10-fold cross validation and 66.9% on the RTE-3 test data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Szpektor</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proc. of the PASCAL RTE-2 Challenge.</booktitle>
<contexts>
<context position="16464" citStr="Bar-Haim et al., 2006" startWordPosition="2838" endWordPosition="2841">Train: Dev Data; Test: Test Data BoW 54.5 66.5 76.5 56 63.4 TSM 54.5 62.5 66 54.5 59.4 Mi+SP+SK+BS 58.5 70.5 79.5 59 66.9 Table 2 Results on RTE-3 Data For the IE task, Mi+SK+BS obtained the highest improvement over the baseline systems, suggesting that the kernel method seems to be more appropriate if the underlying task conveys a more “relational nature.” Improvements in the other tasks are less convincing as compared to the baselines. Nevertheless, the overall result obtained in experiment B would have been among the top 3 of the RTE-2 challenge. We utilize the system description table of (Bar-Haim et al., 2006) to compare our system with the best two systems of RTE-2 in Table 35: 4 See (Wang and Neumann, 2007) for details concerning the experiments of our method on RTE-2 data. 5 Following the notation in (Bar-Haim et al., 2006): Lx: Lexical Relation DB; Ng: N-Gram / Subsequence overlap; Sy: Syntactic Matching / Alignment; Se: Semantic Role Labeling; LI: Logical Inference; C: Corpus/Web; M: ML Classification; B: Paraphrase Technology / Background Knowledge; L: Acquisition of Entailment Corpora. Systems Lx Ng Sy Se LI C M B L Hickl et al. X X X X X X X Tatu et al. X X X Ours X X X Table 3 Comparison w</context>
<context position="19999" citStr="Bar-Haim et al., 2006" startWordPosition="3443" endWordPosition="3446">though they are structurally equivalent. Most IE pairs are constructed from a binary relation, and so meet the prerequisite of our algorithm (see sec. 4.1). However, our method still has rather low coverage. T-H pairs from other tasks, for example like IR and SUM, usually contain more information, i.e. more nouns, the dependency trees of which are more complex. For instance, the pair (id=739) in Table 1 contains four keyword pairs which we cannot handle by our current method. This is one reason why we have constructed extra T-H pairs from MUC, TREC, and news articles following the methods of (Bar-Haim et al., 2006). Still, the overall performance does not improve. All extra training data only serves to improve the matched pairs (about 32% of the data set) for which we already have high accuracy (see Table 4). Thus, extending coverage by machine learning methods for lexical semantics will be the main focus of our future work. 6 Conclusions and Future Work Applying different RTE strategies for different NLP tasks is a reasonable solution. We have utilized a structure similarity function to deal with the structure-oriented pairs, and applied backup strategies for the rest. The results show the advantage of</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B. and Szpektor, I. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proc. of the PASCAL RTE-2 Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>K Markert</author>
</authors>
<title>Combining Shallow and Deep NLP Methods for Recognizing Textual Entailment.</title>
<date>2005</date>
<booktitle>In Proc. of the PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="5345" citStr="Bos and Markert, 2005" startWordPosition="867" endWordPosition="870">2 Related Work Conventional methods for RTE define measures for the similarity between T and H either by assuming an independence between words (Corley and Mihalcea, 2005) in a BoW fashion or by exploiting syntactic interpretations. (Kouylekov and Magnini, 2006) explore a syntactic tree editing distance to detect entailment relations. Since they calculate the similarity between the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Tatu et al., 2006) proposed a knowledge representation model which achieved about 10% better performance than the third (Zanzotto and Moschitti, 2006) based on their logic prover. The other best team in RTE-2 (Hickl et al., 2006) automatically acquired extra training data, enabling them to achieve about 10% better accuracy than the third as well. Consequently, obtaining more training data and embedding deeper knowledge were expected Figure 1 Overvie</context>
</contexts>
<marker>Bos, Markert, 2005</marker>
<rawString>Bos, J. and Markert, K. 2005. Combining Shallow and Deep NLP Methods for Recognizing Textual Entailment. In Proc. of the PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Subsequence Kernels for Relation Extraction.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13757" citStr="Bunescu and Mooney, 2006" startWordPosition="2317" endWordPosition="2320">onding relations is inconsistent, and 0 means RNT is not a verb.3 The set of EPs defines the feature space for the subsequence kernels in our Structure Similarity Function. 4.2 Structure Similarity Function We define the function by constructing two basic kernels to process the LSD and RSD part of an EP, and two trivial kernels for VC and VRC. The four kernels are combined linearly by a composite kernel that performs binary classification on them. Since all spine differences SDs are either empty or CCS sequences, we can utilize subsequence kernel methods to represent features implicitly, cf. (Bunescu and Mooney, 2006). Our subsequence kernel function is: whereby T and H refers to all spine differences SDs from T and H, and |T |and |H |represent the cardinalities of SDs. The function KCCS(CCS,CCS’) checks whether its arguments are equal. Since the RTE task checks the relationship between T and H, we need to consider collocations of some CCS subsequences between T and H as well. Essentially, this kernel evaluates the similarity of T and H by means of those CCS subsequences appearing in both elements. The kernel function is as follows: K ( , , &apos; , &apos; ) &lt;T H T H &gt; &lt; &gt; collocatio n || T |&apos;| T || H |&apos;| H = ∑∑∑∑ K</context>
</contexts>
<marker>Bunescu, Mooney, 2006</marker>
<rawString>Bunescu, R. and Mooney, R. 2006. Subsequence Kernels for Relation Extraction. In Advances in Neural Information Processing Systems 18. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Corley</author>
<author>R Mihalcea</author>
</authors>
<title>Measuring the Semantic Similarity of Texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</booktitle>
<contexts>
<context position="4894" citStr="Corley and Mihalcea, 2005" startWordPosition="797" endWordPosition="801">ara. husband, Bobby; and his brother, Marvin, and his wife, Margaret. 739 SUM The FDA NO The FDA would not say in which states the pills had been sold, but provided a list of instead recom m ended that custom ers determ ine whether products they states in which the bought are being recalled by checking the store list on the FDA Web site, pills have been and the batch list. The batch numbers appear on the container&apos;s label. Table 1 Examples from RTE-3 2 Related Work Conventional methods for RTE define measures for the similarity between T and H either by assuming an independence between words (Corley and Mihalcea, 2005) in a BoW fashion or by exploiting syntactic interpretations. (Kouylekov and Magnini, 2006) explore a syntactic tree editing distance to detect entailment relations. Since they calculate the similarity between the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Ta</context>
<context position="7637" citStr="Corley and Mihalcea, 2005" startWordPosition="1241" endWordPosition="1244">e different processing techniques and depths applied to the RTE task. Our work focuses on constructing a similarity function operating between sentences. In detail, it consists of several similarity scores with different domains of locality on top of the dependency structure. Figure 2 gives out the workflow of our system. The main part of the sentence similarity function is the Structure Similarity Function; two other similarity scores are calculated by our backup strategies. The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)); 37 Figure 2 Workflow of the System while the second one is based on a triple set representation of sentences that expresses the local dependency relations found by a parser1. A dependency structure consists of a set of triple relations (TRs). A TR is of the form &lt;node1, relation, node2&gt;, where node1 represents the head, node2 the modifier and relation the dependency relation. Chief requirements for the backup system are robustness and simplicity. Accordingly, we construct a similarity function, the Triple Similarity Function (TSF), which operates on two triple sets and determines how many t</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Corley, C. and Mihalcea, R. 2005. Measuring the Semantic Similarity of Texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dagan</author>
<author>O Glickman</author>
</authors>
<title>Probabilistic textual entailment: Generic applied modelling of language variability.</title>
<date>2004</date>
<booktitle>In PASCAL Workshop on Text Understanding and Mining.</booktitle>
<contexts>
<context position="960" citStr="Dagan and Glickman, 2004" startWordPosition="140" endWordPosition="143">y function. The structural features are automatically acquired from tree skeletons that are extracted and generalized from dependency trees. Our method makes use of a limited size of training data without any external knowledge bases (e.g. WordNet) or handcrafted inference rules. We have achieved an accuracy of 71.1% on the RTE-3 development set performing a 10-fold cross validation and 66.9% on the RTE-3 test data. 1 Introduction Textual entailment has been introduced as a relation between text expressions, capturing the fact that the meaning of one expression can be inferred from the other (Dagan and Glickman, 2004). More precisely, textual entailment is defined as “... a relationship between a coherent text T and a language expression, which is considered as a hypothesis, H. We say that T entails H (H is a consequent of T), denoted by T ⇒ H, if the meaning of H, as interpreted in the context of T, can be inferred from the meaning of T.” Table 1 displays several examples from the RTE-3 development set. For the third pair (id=410) the key knowledge needed to decide whether the entailment relation holds is that “[PN1]’s wife, [PN2]” entails “The name of [PN1]’s wife is [PN2]”, although T contains much more</context>
</contexts>
<marker>Dagan, Glickman, 2004</marker>
<rawString>Dagan, R., Glickman, O. 2004. Probabilistic textual entailment: Generic applied modelling of language variability. In PASCAL Workshop on Text Understanding and Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de Salvo Braz</author>
<author>R Girju</author>
<author>R Punyaka-nok</author>
<author>V Roth</author>
<author>D</author>
<author>M Sammons</author>
</authors>
<title>An Inference Model for Semantic Entailment in Natural Language.</title>
<date>2005</date>
<booktitle>In Proc. of the PASCAL RTE Challenge.</booktitle>
<contexts>
<context position="5420" citStr="Braz et al., 2005" startWordPosition="881" endWordPosition="884">between T and H either by assuming an independence between words (Corley and Mihalcea, 2005) in a BoW fashion or by exploiting syntactic interpretations. (Kouylekov and Magnini, 2006) explore a syntactic tree editing distance to detect entailment relations. Since they calculate the similarity between the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Tatu et al., 2006) proposed a knowledge representation model which achieved about 10% better performance than the third (Zanzotto and Moschitti, 2006) based on their logic prover. The other best team in RTE-2 (Hickl et al., 2006) automatically acquired extra training data, enabling them to achieve about 10% better accuracy than the third as well. Consequently, obtaining more training data and embedding deeper knowledge were expected Figure 1 Overview of RTE to be the two main directions pointed out for future research in t</context>
</contexts>
<marker>Braz, Girju, Punyaka-nok, Roth, D, Sammons, 2005</marker>
<rawString>de Salvo Braz, R., Girju, R., Punyaka-nok, V., Roth, D., and Sammons, M. 2005. An Inference Model for Semantic Entailment in Natural Language. In Proc. of the PASCAL RTE Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hickl</author>
<author>J Williams</author>
<author>J Bensley</author>
<author>K Roberts</author>
<author>B Rink</author>
<author>Y Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s GROUNDHOG System.</title>
<date>2006</date>
<booktitle>In Proc. of the PASCAL RTE-2 Challenge.</booktitle>
<contexts>
<context position="5721" citStr="Hickl et al., 2006" startWordPosition="933" endWordPosition="936">n the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Tatu et al., 2006) proposed a knowledge representation model which achieved about 10% better performance than the third (Zanzotto and Moschitti, 2006) based on their logic prover. The other best team in RTE-2 (Hickl et al., 2006) automatically acquired extra training data, enabling them to achieve about 10% better accuracy than the third as well. Consequently, obtaining more training data and embedding deeper knowledge were expected Figure 1 Overview of RTE to be the two main directions pointed out for future research in the RTE-2 summary statement. However, except for the positive cases of SUM, T-H pairs are normally not very easy to collect automatically. Multi-annotator agreement is difficult to reach on most of the cases as well. The knowledgebased approach also has its caveats since logical rules are usually impl</context>
<context position="17142" citStr="Hickl et al., 2006" startWordPosition="2966" endWordPosition="2969">n Table 35: 4 See (Wang and Neumann, 2007) for details concerning the experiments of our method on RTE-2 data. 5 Following the notation in (Bar-Haim et al., 2006): Lx: Lexical Relation DB; Ng: N-Gram / Subsequence overlap; Sy: Syntactic Matching / Alignment; Se: Semantic Role Labeling; LI: Logical Inference; C: Corpus/Web; M: ML Classification; B: Paraphrase Technology / Background Knowledge; L: Acquisition of Entailment Corpora. Systems Lx Ng Sy Se LI C M B L Hickl et al. X X X X X X X Tatu et al. X X X Ours X X X Table 3 Comparison with the top 2 systems in RTE-2. Note that the best system (Hickl et al., 2006) applies both shallow and deep techniques, especially in acquiring extra entailment corpora. The second best system (Tatu et al., 2006) contains many manually designed logical inference rules and background knowledge. On the contrary, we exploit no additional knowledge sources besides the dependency trees computed by the parsers, nor any extra training corpora. 5.2 Discussions Table 4 shows how our method performs for the task-specific pairs matched by our patterns: Tasks IE IR QA SUM ALL ExpA:Matched 53 19 23.5 31.5 31.8 ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8 ExpB:Matched 58.5 16 27.5 42 36 E</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink, B. and Shi, Y. 2006. Recognizing Textual Entailment with LCC’s GROUNDHOG System. In Proc. of the PASCAL RTE-2 Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="9358" citStr="Klein and Manning, 2003" startWordPosition="1530" endWordPosition="1533"> POS. We then sum them up using different weights and divide the result by the cardinality of H for normalization. The different weights learned from the corpus indicate that the “amount of missing linguistic information” affect entailment decisions differently. 4 Workflow of the Main Approach Our Structure Similarity Function is based on the hypothesis that some particular differences between T and H will block or change the entailment relationship. Initially we assume when judging the entailment relation that it holds for each T-H pair 1 We are using Minipar (Lin, 1998) and Stanford Parser (Klein and Manning, 2003) as preprocessors, see also sec. 5.2. 2 Note that henceforth T and H will represent either the original texts or the dependency structures. (using the default value “YES”). The major steps are as follows (see also Figure 2): 4.1 Tree Skeleton Extractor Since we assume that H indicates how to extract relevant parts in T for the entailment relation, we start from the Tree Skeleton of H (TSH). First, we construct a set of keyword pairs using all the nouns that appear in both T and H. In order to increase the hits of keyword pairs, we have applied a partial search using stemming and some word vari</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and Manning, C. 2003. Accurate Unlexicalized Parsing. In Proc. of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Tree Edit Distance for Recognizing Textual Entailment: Estimating the Cost of Insertion.</title>
<date>2006</date>
<booktitle>In Proc. of the PASCAL RTE-2 Challenge.</booktitle>
<contexts>
<context position="4985" citStr="Kouylekov and Magnini, 2006" startWordPosition="811" endWordPosition="814">The FDA would not say in which states the pills had been sold, but provided a list of instead recom m ended that custom ers determ ine whether products they states in which the bought are being recalled by checking the store list on the FDA Web site, pills have been and the batch list. The batch numbers appear on the container&apos;s label. Table 1 Examples from RTE-3 2 Related Work Conventional methods for RTE define measures for the similarity between T and H either by assuming an independence between words (Corley and Mihalcea, 2005) in a BoW fashion or by exploiting syntactic interpretations. (Kouylekov and Magnini, 2006) explore a syntactic tree editing distance to detect entailment relations. Since they calculate the similarity between the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Tatu et al., 2006) proposed a knowledge representation model which achieved about 10% better </context>
</contexts>
<marker>Kouylekov, Magnini, 2006</marker>
<rawString>Kouylekov, M. and Magnini, B. 2006. Tree Edit Distance for Recognizing Textual Entailment: Estimating the Cost of Insertion. In Proc. of the PASCAL RTE-2 Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems.</booktitle>
<contexts>
<context position="9312" citStr="Lin, 1998" startWordPosition="1525" endWordPosition="1526">hat they have the same lemma and POS. We then sum them up using different weights and divide the result by the cardinality of H for normalization. The different weights learned from the corpus indicate that the “amount of missing linguistic information” affect entailment decisions differently. 4 Workflow of the Main Approach Our Structure Similarity Function is based on the hypothesis that some particular differences between T and H will block or change the entailment relationship. Initially we assume when judging the entailment relation that it holds for each T-H pair 1 We are using Minipar (Lin, 1998) and Stanford Parser (Klein and Manning, 2003) as preprocessors, see also sec. 5.2. 2 Note that henceforth T and H will represent either the original texts or the dependency structures. (using the default value “YES”). The major steps are as follows (see also Figure 2): 4.1 Tree Skeleton Extractor Since we assume that H indicates how to extract relevant parts in T for the entailment relation, we start from the Tree Skeleton of H (TSH). First, we construct a set of keyword pairs using all the nouns that appear in both T and H. In order to increase the hits of keyword pairs, we have applied a pa</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>B Iles</author>
<author>J Slavik</author>
<author>A Novischi</author>
<author>D Moldovan</author>
</authors>
<title>COGEX at the Second Recognizing Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Proc. of the PASCAL RTE-2 Challenge.</booktitle>
<contexts>
<context position="5510" citStr="Tatu et al., 2006" startWordPosition="898" endWordPosition="901">5) in a BoW fashion or by exploiting syntactic interpretations. (Kouylekov and Magnini, 2006) explore a syntactic tree editing distance to detect entailment relations. Since they calculate the similarity between the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Tatu et al., 2006) proposed a knowledge representation model which achieved about 10% better performance than the third (Zanzotto and Moschitti, 2006) based on their logic prover. The other best team in RTE-2 (Hickl et al., 2006) automatically acquired extra training data, enabling them to achieve about 10% better accuracy than the third as well. Consequently, obtaining more training data and embedding deeper knowledge were expected Figure 1 Overview of RTE to be the two main directions pointed out for future research in the RTE-2 summary statement. However, except for the positive cases of SUM, T-H pairs are n</context>
<context position="17277" citStr="Tatu et al., 2006" startWordPosition="2987" endWordPosition="2990">in (Bar-Haim et al., 2006): Lx: Lexical Relation DB; Ng: N-Gram / Subsequence overlap; Sy: Syntactic Matching / Alignment; Se: Semantic Role Labeling; LI: Logical Inference; C: Corpus/Web; M: ML Classification; B: Paraphrase Technology / Background Knowledge; L: Acquisition of Entailment Corpora. Systems Lx Ng Sy Se LI C M B L Hickl et al. X X X X X X X Tatu et al. X X X Ours X X X Table 3 Comparison with the top 2 systems in RTE-2. Note that the best system (Hickl et al., 2006) applies both shallow and deep techniques, especially in acquiring extra entailment corpora. The second best system (Tatu et al., 2006) contains many manually designed logical inference rules and background knowledge. On the contrary, we exploit no additional knowledge sources besides the dependency trees computed by the parsers, nor any extra training corpora. 5.2 Discussions Table 4 shows how our method performs for the task-specific pairs matched by our patterns: Tasks IE IR QA SUM ALL ExpA:Matched 53 19 23.5 31.5 31.8 ExpA:Accuracy 67.9 78.9 91.5 71.4 74.8 ExpB:Matched 58.5 16 27.5 42 36 ExpB:Accuracy 57.2 81.5 90.9 65.5 68.8 Table 4 Performances of our method For IE pairs, we find good coverage, whereas for IR and QA pai</context>
</contexts>
<marker>Tatu, Iles, Slavik, Novischi, Moldovan, 2006</marker>
<rawString>Tatu, M., Iles, B., Slavik, J., Novischi, A. and Moldovan, D. 2006. COGEX at the Second Recognizing Textual Entailment Challenge. In Proc. of the PASCAL RTE-2 Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Wang</author>
<author>G Neumann</author>
</authors>
<title>Recognizing Textual Entailment Using a Subsequence Kernel Method.</title>
<date>2007</date>
<booktitle>In Proc. of AAAI</booktitle>
<contexts>
<context position="16565" citStr="Wang and Neumann, 2007" startWordPosition="2858" endWordPosition="2861">8.5 70.5 79.5 59 66.9 Table 2 Results on RTE-3 Data For the IE task, Mi+SK+BS obtained the highest improvement over the baseline systems, suggesting that the kernel method seems to be more appropriate if the underlying task conveys a more “relational nature.” Improvements in the other tasks are less convincing as compared to the baselines. Nevertheless, the overall result obtained in experiment B would have been among the top 3 of the RTE-2 challenge. We utilize the system description table of (Bar-Haim et al., 2006) to compare our system with the best two systems of RTE-2 in Table 35: 4 See (Wang and Neumann, 2007) for details concerning the experiments of our method on RTE-2 data. 5 Following the notation in (Bar-Haim et al., 2006): Lx: Lexical Relation DB; Ng: N-Gram / Subsequence overlap; Sy: Syntactic Matching / Alignment; Se: Semantic Role Labeling; LI: Logical Inference; C: Corpus/Web; M: ML Classification; B: Paraphrase Technology / Background Knowledge; L: Acquisition of Entailment Corpora. Systems Lx Ng Sy Se LI C M B L Hickl et al. X X X X X X X Tatu et al. X X X Ours X X X Table 3 Comparison with the top 2 systems in RTE-2. Note that the best system (Hickl et al., 2006) applies both shallow a</context>
</contexts>
<marker>Wang, Neumann, 2007</marker>
<rawString>Wang, R. and Neumann, G. 2007. Recognizing Textual Entailment Using a Subsequence Kernel Method. In Proc. of AAAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Weka Frank</author>
</authors>
<date>1999</date>
<booktitle>Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="15341" citStr="Witten and Frank, 1999" startWordPosition="2637" endWordPosition="2640">bsequenc T,H, &apos;, &apos; &gt; &lt; T H &gt; | | | &apos; &apos; T || T || H |H &apos; ) + ∑ ∑ j=1j &apos;= KCCS (CCSi,CCSi 1 K ( , CCS CCS j 1 CCS j = ∑ ∑ = 1i &apos;= i &apos; ) KCCS (CCSj, CCSj&apos;) 39 where y and S are learned from the training corpus; α=R=1. 5 Evaluation We have evaluated four methods: the two backup systems as baselines (BoW and TSM, the Triple Set Matcher) and the kernel method combined with the backup strategies using different parsers, Minipar (Mi+SK+BS) and the Stanford Parser (SP+SK+BS). The experiments are based on RTE3 Data4. For the kernel-based classification, we used the classifier SMO from the WEKA toolkit (Witten and Frank, 1999). 5.1 Experiment Results RTE-3 data include the Dev Data (800 T-H pairs, each task has 200 pairs) and the Test Data (same size). Experiment A performs a 10-fold crossvalidation on Dev Data; Experiment B uses Dev Data for training and Test Data for testing cf. Table 2 (the numbers denote accuracies): Systems\Tasks IE IR QA SUM All Exp A: 10-fold Cross Validation on RTE-3 Dev Data BoW 54.5 70 76.5 68.5 67.4 TSM 53.5 60 68 62.5 61.0 Mi+SK+BS 63 74 79 68.5 71.1 SP+SK+BS 60.5 70 81.5 68.5 70.1 Exp B: Train: Dev Data; Test: Test Data BoW 54.5 66.5 76.5 56 63.4 TSM 54.5 62.5 66 54.5 59.4 Mi+SP+SK+BS </context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Witten, I. H. and Frank, E. Weka: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>A Moschitti</author>
</authors>
<title>Automatic Learning of Textual Entailments with Cross-pair Similarities.</title>
<date>2006</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5642" citStr="Zanzotto and Moschitti, 2006" startWordPosition="918" endWordPosition="921">iting distance to detect entailment relations. Since they calculate the similarity between the two dependency trees of T and H directly, the noisy information may decrease accuracy. This observation actually motivated us to start from H towards the most relevant information in T. Logic rules (as proposed by (Bos and Markert, 2005)) or sequences of allowed rewrite rules (as in (de Salvo Braz et al., 2005)) are another fashion of tackling RTE. One the best two teams in RTE-2 (Tatu et al., 2006) proposed a knowledge representation model which achieved about 10% better performance than the third (Zanzotto and Moschitti, 2006) based on their logic prover. The other best team in RTE-2 (Hickl et al., 2006) automatically acquired extra training data, enabling them to achieve about 10% better accuracy than the third as well. Consequently, obtaining more training data and embedding deeper knowledge were expected Figure 1 Overview of RTE to be the two main directions pointed out for future research in the RTE-2 summary statement. However, except for the positive cases of SUM, T-H pairs are normally not very easy to collect automatically. Multi-annotator agreement is difficult to reach on most of the cases as well. The kn</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Zanzotto, F.M. and Moschitti, A. 2006. Automatic Learning of Textual Entailments with Cross-pair Similarities. In Proc. of ACL 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>