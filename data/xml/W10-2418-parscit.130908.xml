<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.994728">
Think Globally, Apply Locally: Using Distributional Characteristics for
Hindi Named Entity Identification
</title>
<author confidence="0.993196">
Shalini Gupta Pushpak Bhattacharyya
</author>
<affiliation confidence="0.879713333333333">
Department of Computer Science and Engineering
IIT Bombay
Mumbai, India.
</affiliation>
<email confidence="0.994299">
{shalini, pb}@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.994657" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999871444444445">
In this paper, we present a novel ap-
proach for Hindi Named Entity Identifica-
tion (NEI) in a large corpus. The key idea
is to harness the global distributional char-
acteristics of the words in the corpus. We
show that combining the global distribu-
tional characteristics along with the local
context information improves the NEI per-
formance over statistical baseline systems
that employ only local context. The im-
provement is very significant (about 10%)
in scenarios where the test and train cor-
pus belong to different genres. We also
propose a novel measure for NEI based
on term informativeness and show that it
is competitive with the best measure and
better than other well known information
measures.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884787234043">
NER is the task of identifying and classifying
words in a document into predefined classes like
person, location, organization, etc. It has many ap-
plications in Natural Language Processing (NLP)
NER can be divided into two sub-tasks, Named
Entity Identification (NEI) and Named Entity
Classification (NEC). In this paper, we focus on
the first step, i.e., Named Entity Identification.
NEI is useful in applications where a list of Named
Entities (NEs) is required. Machine Translation
needs identification of named entities, so that they
can be transliterated.
For Indian languages, it is tough to identify
named entities because of the lack of capitaliza-
tion. Many approaches based on MEMM (Saha et
al., 2008b), CRFs (Li and McCallum, 2003) and
hybrid models have been tried for Hindi Named
Entity Recognition. These approaches use only
the local context for tagging the text. Many ap-
plications need entity identification in large cor-
pora. When such a large corpus is to be tagged,
one can use the global distributional characteris-
tics of the words to identify the named entities.
The state-of-the-art methods do not take advantage
of these characteristics. Also, the performance
of these systems degrades when the training and
test corpus are from different domain or different
genre. We present here our approach-Combined
Local and Global Information for Named Entity
Identification (CLGIN) which combines the global
characteristics with the local context for Hindi
Named Entity Identification. The approach com-
prises of two steps: (i) Named Entity Identifica-
tion using Global Information (NGI) which uses
the global distributional characteristics along with
the language cues to identify NEs and (ii) Com-
bining the tagging from step 1 with the MEMM
based statistical system. We consider the MEMM
based statistical system (S-MEMM) as the Base-
line. Results show that the CLGIN approach out-
performs the baseline S-MEMM system by a mar-
gin of about 10% when the training and test corpus
belong to different genre and by a margin of about
2% when both, training and test corpus are similar.
NGI also outperforms the baseline, in the former
case, when training and test corpus are from dif-
ferent genre. Our contributions in this paper are:
</bodyText>
<listItem confidence="0.998000636363636">
• Developing an approach of harnessing the
global characteristics of the corpus for Hindi
Named Entity Identification using informa-
tion measures, distributional similarity, lex-
icon, term co-occurrence and language cues
• Demonstrating that combining the global
characteristics with the local contexts im-
proves the accuracy; and with a very signif-
icant amount when the train and test corpus
are not from same domain or similar genre
• Demonstrating that the system using only the
</listItem>
<page confidence="0.984496">
116
</page>
<note confidence="0.979402">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 116–125,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99682875">
global characteristics is also quite compara-
ble with the existing systems and performs
better than them, when train and test corpus
are unrelated
</bodyText>
<listItem confidence="0.98899325">
• Introducing a new scoring function, which
is quite competitive with the best measure
and better than other well known information
measures
</listItem>
<table confidence="0.46446225">
Approach Description
S-MEMM MEMM based statistical system without
(Baseline) inserting global information
NGI Uses global distributional characteristics
</table>
<tableCaption confidence="0.6127804">
along with language information for NE
Identification
CLGIN Combines the global characteristics de-
rived using NGI with S-MEMM
Table 1: Summary of Approaches
</tableCaption>
<sectionHeader confidence="0.999209" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998827931034483">
There is a plethora of work on NER for En-
glish ranging from supervised approaches like
HMMs(Bikel et al., 1999), Maximum Entropy
(Borthwick, 1999) (Borthwick et al., 1998), CRF
(Lafferty et al., 2001) and SVMs to unsupervised
(Alfonseca and Manandhar, 2002), (Volker, 2005)
and semi-supervised approaches (Li and Mccal-
lum, 2005). However, these approaches do not
perform well for Indian languages mainly due to
lack of capitalization and unavailability of good
gazetteer lists. The best F Score reported for Hindi
NER using these approaches on a standard cor-
pus (IJCNLP) is 65.13% ((Saha et al., 2008a)).
Higher accuracies have been reported (81%) (Saha
et al., 2008b), albeit, on a non-standard corpus us-
ing rules and comprehensive gazetteers.
Current state-of-the-art systems (Li and McCal-
lum, 2003) (Saha et al., 2008b) use various lan-
guage independent and language specific features,
like, context word information, POS tags, suffix
and prefix information, gazetteer lists, common
preceding and following words, etc. The perfor-
mance of these systems is significantly hampered
when the test corpus is not similar to the training
corpus. Few studies (Guo et al., 2009), (Poibeau
and Kosseim, 2001) have been performed towards
genre/domain adaptation. But this still remains an
open area. Moreover, no work has been done to-
wards this for Indian languages.
</bodyText>
<figureCaption confidence="0.998089">
Figure 1: Block diagram of CLGIN Approach
</figureCaption>
<bodyText confidence="0.999958892857143">
One shortcoming of current approaches is that
they do not leverage on global distributional char-
acteristics of words (e.g., Information Content,
Term Co-occurrence statistics, etc.) when a large
corpus needs NEI. Rennie and Jaakkola (2005)
introduced a new information measure and used
it for NE detection. They used this approach
only on uncapitalized and ungrammatical English
text, like blogs where spellings and POS tags are
not correct. Some semi-supervised approaches
(Collins and Singer, 1999), (Riloff and Jones,
1999), (Pas¸ca, 2007) have also used large available
corpora to generate context patterns for named en-
tities or for generating gazetteer lists and entity
expansion using seed entities. Klementiev and
Roth (2006) use cooccurrence of sets of terms
within documents to boost the certainty (in a
cross-lingual setting) that the terms in question
were really transliterations of each other.
In this paper, we contend that using such global
distributional characteristics improves the perfor-
mance of Hindi NEI when applied to a large cor-
pus. Further, we show that the performance of
such systems which use global distribution charac-
teristics is better than current state-of-the-art sys-
tems when the training and test corpus are not sim-
ilar (different domain/genre) thereby being more
suitable for domain adaptation.
</bodyText>
<sectionHeader confidence="0.982467" genericHeader="method">
3 MEMM based Statistical System
(S-MEMM)
</sectionHeader>
<bodyText confidence="0.999961833333333">
We implemented the Maximum Entropy Markov
Model based system(Saha et al., 2008b) for NE
Identification. We use this system as our Base-
line and compare our approaches NGI and CLGIN
with this baseline. We used various language de-
pendent and independent features. An important
</bodyText>
<figure confidence="0.993603693877551">
DataSetto be Tagged
Threshold (Set using
Development Set
Trained Model
Corpus
Added as a feature
Select words
based on
Information
Measure
NEIG Tagged
DataSet
Final Tagged
DataSet
Statistical
System
(MEMM)
Features
(Context Words,
POS Tags, Suffix
Info, Gazetteers,
Lexicon, etc.)
Applying
Augmenting
Heuristics
Applying
Pruning
Heuristics
MEMM Based
Statistical
System
(S-MEMM)
Step 1
Tagging
using Global
Distribution
(NEIG)
Step 2
117
Input Text:
Translitera
English T
Apply
Augmenting
Heuristics
(Term Co‐occurre
Output
Transliterati
English Trans
</figure>
<figureCaption confidence="0.987659">
Figure 2: An Example explaining the NGI approach
</figureCaption>
<subsectionHeader confidence="0.493149">
Input Text: ि
</subsectionHeader>
<bodyText confidence="0.998982444444445">
modification was the use of lexicon along with tra-
ditionally used gazetteers. Gazetteers just improve
the recall whereas including the lexicon improves
the precision. The state-of-art Hindi NER sys-
tems do not use lexicon of general words but we
found that using lexicons significantly improves
the performance. Unlike English, NEs in Hindi are
not capitalized and hence it becomes important to
know, if a word is a common word or not.
</bodyText>
<listItem confidence="0.9721155">
Features used in S-MEMM were:
• Context Words: Preceding and succeeding two
words of the current word
• Word suffix and prefix: Fixed length (size: 2)
suffix information was used. Besides, suffix
list of common location suffixes was created
• First word and last word information
• Previous NE Tag information
• Digit information
• Gazetteer Lists: Person and Location names,
Frequent words after and before person, orga-
nization and location names, list of common
initials, stopwords, etc.
• POS Tag Information
• Lexicons: If the stemmed word was present in
the lexicon, this feature was true.
</listItem>
<sectionHeader confidence="0.992802" genericHeader="method">
4 Our Approach-CLGIN
</sectionHeader>
<bodyText confidence="0.9994344375">
In this section, we describe our approach, CLGIN
in detail. It combines the global information from
the corpus with the local context. Figure 1 gives
the block diagram of the system while tagging a
corpus and Figure 2 explains the approach using
an example. This approach involves two steps.
Step 1 of CLGIN is NGI which creates a list
of probable NEs (both uni-word and multi-word)
from the given corpus and uses it to tag the whole
corpus. Sections 4.1 and 4.2 explain this step in
detail. Later, in step 2, it combines the tagging
obtained from step 1, as a feature in the MEMM
based statistical system. Output thus obtained
from the MEMM system is the final output of the
CLGIN approach. The creation of list in step 1,
involves the following steps
</bodyText>
<listItem confidence="0.986123909090909">
• A list of all words which appeared as a noun at
least once in the the corpus is extracted.
• List is ordered on the basis of the information
content derived using the whole corpus. Words
above the threshold (set during training using
the development set) are selected as NEs.
• Heuristics are applied for pruning and aug-
menting the list.
• Multi-word NEs derived using term co-
occurrence statistics along with language char-
acteristics are added to the NE list.
</listItem>
<bodyText confidence="0.995350285714286">
The above process generates a list of NEs (uni-
word and multi-word). In the second step, we pro-
vide this tagging to the S-MEMM along with other
set of features described in Section 3
During training, the cutoff threshold is set for
selecting NEs (in bullet 2) above. Also the tagging
obtained from the step 1 is added as a feature to
</bodyText>
<page confidence="0.992842">
118
</page>
<bodyText confidence="0.999730666666667">
S-MEMM and a model is trained during the train-
ing phase. The following sections describe this ap-
proach in detail.
</bodyText>
<subsectionHeader confidence="0.98273">
4.1 Information Measures/Scoring Functions
</subsectionHeader>
<bodyText confidence="0.99994575">
Various measures have been introduced for de-
termining the information content of the words.
These include, IDF (Inverse Document Fre-
quency) (Jones, 1972) , Residual IDF (Church and
Gale, 1995), xI- measure (Bookstein and Swan-
son, 1974), Gain (Papineni, 2001), etc. We intro-
duced our own information measure, RF (Ratio of
Frequencies).
</bodyText>
<subsubsectionHeader confidence="0.581748">
4.1.1 RF (Ratio of Frequencies)
</subsubsectionHeader>
<bodyText confidence="0.999914166666667">
NEs are highly relevant words in a document
(Clifton et al., 2002) and are expected to have high
information content (Rennie and Jaakkola, 2005).
It has been found that words that appear frequently
in a set of documents and not so frequently in the
rest of the documents are important with respect to
that set of documents where they are frequent.
We expected the NEs to be concentrated in few
documents. We defined a new criteria which mea-
sures the ratio of the total number of times the
word appears in the corpus to the number of doc-
uments containing a word.
</bodyText>
<equation confidence="0.929801">
RF (w) = d (w)
</equation>
<bodyText confidence="0.999440461538462">
where cf(w) (w
wheretotal frequency of a word in
the whole corpus and df(w) is the document fre-
quency. This measure is different from the TF-IDF
measure in terms of the term frequency. TF-IDF
considers the frequency of the word in the docu-
ment. RF considers it over the whole corpus.
We use the scoring function (information mea-
sure) to score all the words. During training, we
fix a threshold using the development set. Dur-
ing testing, we pick words above the threshold as
NEs. We then apply heuristics to augment this list
as well as to exclude terms from the generated list.
</bodyText>
<subsectionHeader confidence="0.866086">
4.2 Heuristics for Pruning and Augmenting
NE List
</subsectionHeader>
<bodyText confidence="0.97922915">
Distributional Similarity: The underlying idea
of Distributional Similarity is that a word is char-
acterized by the company it keeps (Firth, 1957).
Two words are said to be distributionally similar
if they appear in similar contexts. From the previ-
ous step (Sect. 4.1), we get a list of words having
high score. Say, top t, words were selected. In
this step, we take t more words and then cluster
together these words. The purpose at this phase is
primarily to remove the false positives and to in-
troduce more words which are expected to be NEs.
For each distinct word, w in the corpus, we cre-
ate a vector of the size of the number of distinct
words in the corpus. Each term in the vector rep-
resents the frequency with which it appears in the
context (context window: size 3) of word, w. It
was observed that the NEs were clustered in some
clusters and general words in other clusters. We
tag a cluster as a NE cluster if most of the words
in the cluster are good words. We define a word
as good if it has high information content. If the
sum of the ranks of 50% of the top ranked word is
low, we tag the cluster as NE and add the words
in that set as NEs. Also, if most of the words in
the cluster have higher rank i.e. lower information
content, we remove it from the NE set.
This heuristic is used for both augmenting the
list as well to exclude terms from the list.
Lexicon: We used this as a list for excluding
terms. Terms present in the lexicon have a high
chance of not being NEs. When used alone, the
lexicon is not very effective (explained in Sec-
tion 5.2). But, when used with other approaches,
it helps in improving the precision of the sys-
tem significantly. State-of-art Hindi NER systems
use lists of gazetteers for Person names, location
names, organization names, etc. (Sangal et al.,
2008), but lexicon of general words has not been
used. Unlike English, for Indian languages, it is
important to know, if a word is a general word
or not. Lexicons as opposed to gazetteers are
generic and can be applied to any domain. Un-
like gazetteers, the words would be quite common
and would appear in any text irrespective of the
domain.
Suffixes: NEs in Hindi are open class words and
appear as free morphemes. Unlike nouns, NEs,
usually do not take any suffixes (attached to them).
However, there are few exceptions like, cTc-f N*
4 TF�T (laal kile ke baahar, (outside Red Fort))
or when NEs are used as common nouns, kw 4?r
T~ ift WF� t (desh ko gandhiyon ki za-
roorat hai, The country needs Gandhis.) etc. We
remove words appearing with common suffixes
like &apos;t (ein), 31-t (on), 4�- (yenge), etc. from the
NE list.
Term Co-occurrence: We use the term co-
occurrence statistics to detect multi-word NEs. A
word may be a NE in some context but not in an-
other. E.g. �T (mahatma “saint”) when ap-
</bodyText>
<page confidence="0.99624">
119
</page>
<bodyText confidence="0.9992262">
pearing with gA\DF (Gandhi “Gandhi”) is a NE,
but may not be, otherwise. To identify such multi-
words NEs, we use this heuristic. Such words can
be identified using Term Co-occurrence. We use
the given set of documents to find all word pairs.
We then calculate Pointwise Mutual Information
(PMI) (Church and Hanks, 1990) for each of these
word pairs and order the pairs in descending order
of their PMI values. Most of the word pairs belong
to the following categories:
</bodyText>
<listItem confidence="0.9999506">
• Adjective Noun combination (Adjectives fol-
lowed by noun): This was the most frequent
combination. E.g. BFnF g\D (bheeni gandh
“sweet smell”)
• Noun Verb combination: Edl DwknA (dil
dhadakna, “heart beating”)
• Adverb verb combination: EKlEKlAkr
h\snA (khilkhilakar hansna, “merrily laugh”)
• Cardinal/Ordinal Noun Combination: TowF
d�r (thodi der, “some time”)
• Named Entities
• Hindi Idioms: uSl� sFDA (ullu seedha)
• Noun Noun Combination: HyAtF aEj�t (khy-
ati arjit, “earn fame”)
• Hindi Multiwords: jof Krof (josh kharosh)
</listItem>
<bodyText confidence="0.9999175">
We need to extract NEs from these word pairs. The
first four combinations can be easily excluded be-
cause of the presence of a verb, cardinals and ad-
jectives. Sometimes both words in the NEs appear
as nouns. So, we cannot reject the Noun Noun
combination. We handle rest of the cases by look-
ing at the neighbours (context) of the word pairs.
We noticed three important things here:
</bodyText>
<listItem confidence="0.980876125">
• Multiwords which are followed (alteast once)
by m�\ (mein), s~ (se), n� (ne), k� (ke), ko
(ko) (Hindi Case Markers) are usually NEs.
We did not include kF (ki) in the list be-
cause many words in the noun-noun combi-
nation are frequently followed by ki in the
sense of EkyA/ krnA (kiya/karna, “do/did”)
e.g. HyAtF aEj�t kF (khyati arjit ki, “earned
fame”), prF&amp;quot;A u•FZ kF (pariksha uttirand
ki, “cleared the exam”), etc.
• There were word pairs which were followed
by a single word most of the time. E.g I-V
i\EXyA (East India, “East India”) was followed
by k\pnF (Company, “Company”) in almost all
the cases. When Company appears alone, it
may not be a NE, but when it appears with East
</listItem>
<note confidence="0.8496855">
Corpus No. of Tagged No. of No. of Source Genre
Documents Words NEs
</note>
<table confidence="0.5573605">
Gyaan 1570 569K 21K Essay, Biography,
Nidhi History and Story
</table>
<tableCaption confidence="0.990403">
Table 2: Corpus Statistics
</tableCaption>
<bodyText confidence="0.99286175">
India, it appears as a NE. Other examples of
such word pairs were: KA iNn~ (Khan Ibnu,
“Khan Ibnu”) followed by alFsm (Alisam,
“Alisam”)
</bodyText>
<listItem confidence="0.553415636363636">
• There were word pairs which were followed
by uncommon words were not common words
but were different words each time, it ap-
peared. i.e. Most of the words following the
word pair were not part of lexicon. gvn�r
jnrl (governor general, “Governor Gen-
eral”) followed by [ dlhOsF, bhd� r, solbrF,
m{VkA&apos;, lOX� ((dalhousie, bahadur, solbari,
metkaf, lord), “Dalhousie, Bahadur, Solbari,
Metkaf, Lord”)] Such words are multi word
NEs.
</listItem>
<subsectionHeader confidence="0.952843">
4.3 Step 2: Combining NGI with S-MEMM
</subsectionHeader>
<bodyText confidence="0.999907166666667">
The tagging obtained as the result of the step 1
(NGI), is given as input to the MEMM based sta-
tistical system (S-MEMM). This feature is intro-
duced as a binary feature OldTag=NE. If a word is
tagged as NE in the previous step, this feature is
turned on, otherwise OldTag=O is turned on.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.992780941176471">
We have used Gyaan Nidhi Corpus for eval-
uation which is a collection of various books
in Hindi. It contains about 75000 documents.
The details of the corpus are given in Table
2. Names of persons, locations, organizations,
books, plays, etc. were tagged as NE and other
general words were tagged as O (others). The
tagged documents are publicly made available at
http://www.cfilt.iitb.ac.in/ner.tar.gz.
We use the following metrics for evaluation:
Precision, Recall and F-Score. Precision is the
ratio of the number of words correctly tagged as
NEs to the total number of words tagged as NEs.
Recall is the ratio of the number of words cor-
rectly tagged as NEs to the total number of NEs
present in the data set. F Score is defined as
(F=2*P*R/(P+R))
</bodyText>
<page confidence="0.986949">
120
</page>
<subsectionHeader confidence="0.998013">
5.1 Comparison of Information Measures
</subsectionHeader>
<bodyText confidence="0.999536125">
We compare the performance of the various
term informativeness measures for NEI which are
Residual IDF1, IDF 2, Gain3 and x&apos; measure 4
and the measure defined in Section 4.1.1. Table
3 shows the results averaged after five-fold cross
validation. The graphs in the Figure 3 to Figure
7 show the distribution of words (nouns) over the
range of values of each information measure.
</bodyText>
<table confidence="0.998791333333333">
Scoring Function Prec. Recall F Score
Residual IDF 0.476 0.537 0.504
IDF 0.321 0.488 0.387
x-dash Measure 0.125 0.969 0.217
RF (Our Measure) 0.624 0.396 0.484
Gain 0.12 0.887 0.211
</table>
<tableCaption confidence="0.811011">
Table 3: Comparison of performance of various
information measures
The best results were obtained using Residual
IDF followed by Ratio of Frequencies (RF).
</tableCaption>
<table confidence="0.9997463">
Method Prec Recall F Score
S -MEMM (Baseline) 0.871 0.762 0.812
Res. IDF 0.476 0.537 0.504
Res. IDF + Dist Sim (DS) 0.588 0.522 0.553
Res. IDF + Lexicon (Lex) 0.586 0.569 0.572
Res. IDF + DS + Suffix 0.611 0.524 0.563
Res. IDF + Lex + Suffix 0.752 0.576 0.65
Res. IDF + Lex + Suffix + Term 0.757 0.62 0.68
Cooccur (NGI)
CLGIN 0.879 0.784 0.829
</table>
<tableCaption confidence="0.9914705">
Table 4: Performance of various Approaches
(Here, train and test are similar)
</tableCaption>
<subsectionHeader confidence="0.993286">
5.2 NGI and CLGIN Approaches (Training
and Test Set from Similar Genre)
</subsectionHeader>
<bodyText confidence="0.9992091">
Table 4 compares the results of S-MEMM, NGI
approach and CLGIN. Besides, it also shows the
step wise improvement of NGI approach. The
final F-Score achieved using NGI approach was
68%. The F-Score of the Baseline system im-
plemented using the MaxEnt package1 from the
OpenNLP community was 81.2%.
Using the lexicon alone gives an F-Score of
only 11% (Precision: 5.97 Recall: 59.7 F-Score:
10.8562). But, when used with Residual IDF, the
</bodyText>
<equation confidence="0.9067956">
1Observed IDF - Expected IDF
2IDF = -log df(w)
D
3Gain = d_D (d_D − 1 − log d_D )
4x�(w) = df(w) − cf(w)
</equation>
<footnote confidence="0.878314">
1http://maxent.sourceforge.net/index.html
</footnote>
<figureCaption confidence="0.984145">
Figure 3: Distribution of Residual IDF values over
the nouns in the corpus
</figureCaption>
<bodyText confidence="0.999317766666667">
performance of the overall system improves sig-
nificantly to about 57%. Note that, the use of lexi-
con resulted in an increase in precision (0.5860)
which was accompanied by improvement in re-
call (0.5693) also. The cutoff thresholds in both
cases (Rows 2 and 4 of Table 4) were different.
Suffix information improved the systems perfor-
mance to 65%. As words were removed, more
words from the initial ordered list (ordered on the
basis of score/information content) were added.
Hence, there was a small improvement in recall,
too. Improvement by distributional similarity was
eclipsed after the pruning by lexicon and suffix in-
formation. But, in the absence of lexicon; distri-
butional similarity and suffix information can be
used as the pruning heuristics. Adding the multi-
word NEs to the list as explained in the section 4.2
using term co-occurrence statistics, improved the
accuracy significantly by 3%. Word pairs were ar-
ranged in the decreasing order of their PMI values
and a list was created. We found that 50% of the
NE word pairs in the whole tagged corpus lied in
the top 1% of this word pairs list and about 70%
of NE word pairs were covered in just top 2% of
the list.
CLGIN which combines the global informa-
tion obtained through NGI with the Baseline S-
MEMM system gives an improvement of about
2%. After including this feature, the F-Score in-
creased to 82.8%.
</bodyText>
<subsectionHeader confidence="0.789321">
5.3 Performance Comparison of Baseline,
</subsectionHeader>
<bodyText confidence="0.8831588">
NGI and CLGIN (Training and Test Data
from different genre)
In the above experiments, documents were ran-
domly placed into different splits. Gyaan Nidhi
is a collection of various books on several top-
</bodyText>
<page confidence="0.996954">
121
</page>
<figureCaption confidence="0.99754675">
Figure 4: Distribution of Gain values over the
nouns in the corpus
Figure 7: Distribution of xi measure values over
the nouns in the corpus
</figureCaption>
<figure confidence="0.82818">
IDF
</figure>
<figureCaption confidence="0.780843">
Figure 5: Distribution of IDF values over the
nouns in the corpus
</figureCaption>
<figure confidence="0.8847815">
Ratio Of Frequencies
60
50
Ratio of Frequencies
</figure>
<figureCaption confidence="0.9862065">
Figure 6: Distribution of Ratio of Frequencies(RF)
values over the nouns in the corpus
</figureCaption>
<bodyText confidence="0.999633923076923">
ics. Random picking resulted into the mixing of
the documents, with each split containing docu-
ments from all books. But, in this experiment,
we divided documents into two groups such that
documents from few books (genre: Story and His-
tory) were placed into one group and rest into an-
other group (Genre: Biography and Essay). Table
5 compares the NGI and CLGIN approaches with
S-MEMM and shows that the CLGIN results are
significantly better than the Baseline System,
when the training and test sets belong to different
genre. The results were obtained after 2-fold cross
validation.
</bodyText>
<table confidence="0.878828">
Method Prec. Recall F Score
S-MEMM 0.842 0.479 0.610
NGI 0.744 0.609 0.67
CLGIN 0.867 0.622 0.723
</table>
<tableCaption confidence="0.9382295">
Table 5: Performance of various Approaches
(Here, train and test are from different genre)
</tableCaption>
<bodyText confidence="0.987860571428572">
Similar improvements were seen when the sets
were divided into (Story and Biography) and (Es-
say and History) (The proportions of train and test
sets in this division were uneven). The F Score
of NGI system was 0.6576 and S-MEMM was
0.4766. The F Score of the combined system
(CLGIN) was 0.6524.
</bodyText>
<sectionHeader confidence="0.997222" genericHeader="discussions">
6 Discussion and Error Analysis
</sectionHeader>
<subsectionHeader confidence="0.999489">
6.1 RF and other information measures
</subsectionHeader>
<bodyText confidence="0.999024384615385">
As can be seen from the graphs in Figures 3 to 7,
Residual IDF best separates the NEs from the gen-
eral words. The measure introduced by us, Ratio
of Frequencies is also a good measure, although
not as good as Residual IDF but performs better
than other measures. The words having RF value
greater than 2.5 can be picked up as NEs, giving a
high recall and precision. It is evident that IDF is
better than both, Gain and xi measure, as most of
the general words have low IDF and NEs lie in the
high IDF zone. But, the general words and NEs
are not very clearly separated. As the number of
nouns is about 7-8 times the number of NEs, the
</bodyText>
<figure confidence="0.997134875">
IDF
25
20
General
Named E
15
10
5
0
40
General Words
Named Entities
30
20
10
0
</figure>
<page confidence="0.99216">
122
</page>
<bodyText confidence="0.999934333333333">
words having high IDF cannot be picked up. This
would result in a low precision, as a large num-
ber of non-NEs would get mixed with the general
words. Gain and xi measure do not demarcate the
NEs from the general words clearly. We observed
that they are not good scoring functions for NEs.
Information Gain doesn’t consider the fre-
quency of the terms within the document itself. It
only takes into account the document frequency
for each word. xi measure considers the fre-
quency within document but it is highly biased
towards high frequency words and hence doesn’t
perform well. Hence, common words like smy
(samay, “time”), Gr (ghar, “home”), etc. have
higher scores compared to NEs like BArt(bharat,
“India”), klk•A (kalkatta, “Calcutta”), etc. Our
measure on the other hand, overcomes this draw-
back, by considering the ratio. We could have
combined the measures, instead of using only the
best measure “Residual IDF”, but the performance
of “Gain”, “IDF” and “x’-measure” was not good.
Also, results of “RF” and “Residual IDF” were
quite similar. Hence, we did not see any gain in
combining the measures.
</bodyText>
<subsectionHeader confidence="0.998742">
6.2 S-MEMM, NGI and CLGIN
</subsectionHeader>
<bodyText confidence="0.999982547619048">
The results in Section 5 show that adding the
global information with the local context helps im-
prove the tagging accuracy especially when the
train and test data are from different genre. Sev-
eral times, the local context is not sufficient to
determine the word as a NE. For example, when
the NEs are not followed by post positions or
case markers, it becomes difficult for S-MEMM to
identify NEs, e.g., V{gor ek apvAd h{\, (tagore ek
apvaad hain,“Tagore is an exception”) or when the
NEs are separated by commas, e.g. s k mArF d•,
c àFlAl.. (Sukumari Dutt, Chunnilal ... “Suku-
mari Dutt, Chunnilal ..”). In such cases, because
of the frequency statistics, the NGI approach is
able to detect the words V{gor (Tagore, “Tagore”),
d• (Dutt, “Dutt”), etc. as NEs and frequently the
CLGIN approach is able to detect such words as
NEs.
The false positives in NEIG are words which
are not present in the lexicon (uncommon words,
words absent due to spelling variations e.g.
sA\p/sA p (sanp “snake”)) but have high informa-
tiveness. Using the context words of these words
is a possible way of eliminating these false pos-
itives. Many of the organization names having
common words (m\Xl (mandal, “board”)) and
person names (like þkAf (prakash,“light”)) are
present in the lexicon are not tagged by NEIG.
Some errors were introduced because of the re-
moval of morphed words. NEs like g SbAno\, Vop�
(Gulbano, Tope) were excluded.
Many of the errors using CLGIN are because of
the presence of the words in the lexicon. This ef-
fect also gets passed on to the neighbouring words.
But, the precision of CLGIN is significantly high
compared to NGI because CLGIN uses context, as
well.
The statistical system (S-MEMM) provides the
context and the global system(NGI) provides a
strong indication that the word is a NE and the
performance of the combined approach(CLGIN)
improves significantly.
</bodyText>
<sectionHeader confidence="0.992777" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999963125">
We presented an novel approach for Hindi NEI
which combines the global distributional charac-
teristics with local context. Results show that the
proposed approach improves performance of NEI
significantly, especially, when the train and test
corpus belong to different genres. We also pro-
posed a new measure for NEI which is based on
term informativeness. The proposed measure per-
forms quite competitively with the best known in-
formation measure in literature.
Future direction of the work will be to study
the distributional characteristics of individual tags
and move towards classification of identified enti-
ties. We also plan to extend the above approach
to other Indian languages and other domains. We
also expect further improvements in accuracy by
replacing the MEMM model by CRF. Currently,
we use a tagged corpus as development set to tune
the cut-off threshold in NGI. To overcome this de-
pendence and to make the approach unsupervised,
a way out can be to find an approximation to the
ratio of the number of nouns which are NEs to the
number of nouns and then use this to decide the
cut-off threshold.
</bodyText>
<sectionHeader confidence="0.998026" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999845666666667">
We would like to acknowledge the efforts of Mr.
Prabhakar Pandey and Mr. Devendra Kairwan for
tagging the data with NE tags.
</bodyText>
<page confidence="0.998347">
123
</page>
<sectionHeader confidence="0.927893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996018706422018">
Enrique Alfonseca and Suresh Manandhar. 2002. An
Unsupervised Method For General Named Entity
Recognition and Automated Concept Discovery. In
Proceedings of the 1 st International Conference on
General WordNet.
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns
What’s In A Name.
A. Bookstein and D. R. Swanson. 1974. Probabilis-
tic Models for Automatic Indexing. Journal of the
American Society for Information Science, 25:312–
318.
Andrew Borthwick, John Sterling, Eugene Agichtein,
and Ralph Grishman. 1998. Nyu: Description of
the MENE Named Entity System as used in MUC-
7. In In Proceedings of the Seventh Message Under-
standing Conference (MUC-7.
Andrew Eliot Borthwick. 1999. A Maximum En-
tropy Approach to Named Entity Recognition. Ph.D.
thesis, New York, NY, USA. Adviser-Grishman,
Ralph.
Kenneth Church and William Gale. 1995. Inverse
Document Frequency (IDF): A Measure of Devi-
ations from Poisson. In Third Workshop on Very
Large Corpora, pages 121–130.
Kenneth Ward Church and Patrick Hanks. 1990. Word
Association Norms, Mutual Information, and Lexi-
cography.
Chris Clifton, Robert Cooley, and Jason Rennie. 2002.
Topcat: Data mining for Topic Identification in a
Text Corpus.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In
In Proceedings of the Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 100–110.
J.R. Firth. 1957. A Synopsis of Linguistic Theory
1930-1955. In In Studies in Linguistic Analysis,
pages 1–32.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain Adaptation
with Latent Semantic Association for Named Entity
Recognition. In NAACL ’09, pages 281–289, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Karen Sprck Jones. 1972. A Statistical Interpretation
of Term Specificity and its Application in Retrieval.
Journal of Documentation, 28:11–21.
Alexandre Klementiev and Dan Roth. 2006. Named
Entity Transliteration and Discovery from Multi-
lingual Comparable Corpora. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of
the Association of Computational Linguistics, pages
82–88, Morristown, NJ, USA. Association for Com-
putational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML ’01: Proceedings of the
Eighteenth International Conference on Machine
Learning, pages 282–289, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Wei Li and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition using
Conditional Random Fields and Feature Induction.
ACM Transactions on Asian Language Information
Processing (TALIP), 2(3):290–294.
Wei Li and Andrew Mccallum. 2005. Semi-supervised
Sequence Modeling with Syntactic Topic Models.
In AAAI-05, The Twentieth National Conference on
Artificial Intelligence.
Marius Pas¸ca. 2007. Organizing and Searching the
World Wide Web of facts – Step Two: Harnessing
the Wisdom of the Crowds. In WWW ’07: Proceed-
ings of the 16th international conference on World
Wide Web, pages 101–110, New York, NY, USA.
ACM.
Kishore Papineni. 2001. Why Inverse Document
Frequency? In NAACL ’01: Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language technolo-
gies 2001, pages 1–8, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Thierry Poibeau and Leila Kosseim. 2001. Proper
Name Extraction from Non-Journalistic Texts. In In
Computational Linguistics in the Netherlands, pages
144–157.
Jason D. M. Rennie and Tommi Jaakkola. 2005. Using
Term Informativeness for Named Entity Detection.
In SIGIR ’05: Proceedings of the 28th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 353–
360, New York, NY, USA. ACM.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. In AAAI ’99/IAAI ’99: Proceedings
of the sixteenth national conference on Artificial in-
telligence and the eleventh Innovative applications
of artificial intelligence conference innovative ap-
plications of artificial intelligence, pages 474–479,
Menlo Park, CA, USA. American Association for
Artificial Intelligence.
Sujan Kumar Saha, Sanjay Chatterji, Sandipan Danda-
pat, Sudeshna Sarkar, and Pabitra Mitra. 2008a. A
Hybrid Named Entity Recognition System for South
and South East Asian Languages. In Proceedings of
the IJCNLP-08 Workshop on Named Entity Recog-
nition for South and South East Asian Languages,
</reference>
<page confidence="0.983051">
124
</page>
<reference confidence="0.999117588235294">
pages 17–24, Hyderabad, India, January. Asian Fed-
eration of Natural Language Processing.
Sujan Kumar Saha, Sudeshna Sarkar, and Pabitra Mi-
tra. 2008b. A Hybrid Feature Set Based Maximum
Entropy Hindi Named Entity Recognition. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing, Kharagpur, India.
Rajeev Sangal, Dipti Sharma, and Anil Singh, editors.
2008. Proceedings of the IJCNLP-08 Workshop on
Named Entity Recognition for South and South East
Asian Languages. Asian Federation of Natural Lan-
guage Processing, Hyderabad, India, January.
Johanna Volker. 2005. Towards Large-Scale, Open-
Domain and Ontology-Based Named Entity Classi-
fication. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP’05, pages 166–172. INCOMA Ltd.
</reference>
<page confidence="0.998491">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521806">
<title confidence="0.956283">Think Globally, Apply Locally: Using Distributional Characteristics Hindi Named Entity Identification</title>
<author confidence="0.999839">Shalini Gupta Pushpak</author>
<affiliation confidence="0.9945085">Department of Computer Science and IIT</affiliation>
<address confidence="0.615107">Mumbai,</address>
<abstract confidence="0.99649747368421">In this paper, we present a novel approach for Hindi Named Entity Identification (NEI) in a large corpus. The key idea is to harness the global distributional characteristics of the words in the corpus. We show that combining the global distributional characteristics along with the local context information improves the NEI performance over statistical baseline systems that employ only local context. The improvement is very significant (about 10%) in scenarios where the test and train corpus belong to different genres. We also propose a novel measure for NEI based on term informativeness and show that it is competitive with the best measure and better than other well known information measures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
<author>Suresh Manandhar</author>
</authors>
<title>An Unsupervised Method For General Named Entity Recognition and Automated Concept Discovery.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1 st International Conference on General WordNet.</booktitle>
<contexts>
<context position="4715" citStr="Alfonseca and Manandhar, 2002" startWordPosition="727" endWordPosition="730">tter than other well known information measures Approach Description S-MEMM MEMM based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent a</context>
</contexts>
<marker>Alfonseca, Manandhar, 2002</marker>
<rawString>Enrique Alfonseca and Suresh Manandhar. 2002. An Unsupervised Method For General Named Entity Recognition and Automated Concept Discovery. In Proceedings of the 1 st International Conference on General WordNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An Algorithm that Learns What’s In A Name.</title>
<date>1999</date>
<contexts>
<context position="4569" citStr="Bikel et al., 1999" startWordPosition="706" endWordPosition="709"> when train and test corpus are unrelated • Introducing a new scoring function, which is quite competitive with the best measure and better than other well known information measures Approach Description S-MEMM MEMM based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using ru</context>
</contexts>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. 1999. An Algorithm that Learns What’s In A Name.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bookstein</author>
<author>D R Swanson</author>
</authors>
<title>Probabilistic Models for Automatic Indexing.</title>
<date>1974</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>25</volume>
<pages>318</pages>
<contexts>
<context position="11113" citStr="Bookstein and Swanson, 1974" startWordPosition="1752" endWordPosition="1756">is tagging to the S-MEMM along with other set of features described in Section 3 During training, the cutoff threshold is set for selecting NEs (in bullet 2) above. Also the tagging obtained from the step 1 is added as a feature to 118 S-MEMM and a model is trained during the training phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI- measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated in few documents. We defined a new criteria which measures the ratio of the total</context>
</contexts>
<marker>Bookstein, Swanson, 1974</marker>
<rawString>A. Bookstein and D. R. Swanson. 1974. Probabilistic Models for Automatic Indexing. Journal of the American Society for Information Science, 25:312– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
<author>John Sterling</author>
<author>Eugene Agichtein</author>
<author>Ralph Grishman</author>
</authors>
<title>Nyu: Description of the MENE Named Entity System as used in MUC7. In</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7.</booktitle>
<contexts>
<context position="4629" citStr="Borthwick et al., 1998" startWordPosition="714" endWordPosition="717">a new scoring function, which is quite competitive with the best measure and better than other well known information measures Approach Description S-MEMM MEMM based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art s</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>Andrew Borthwick, John Sterling, Eugene Agichtein, and Ralph Grishman. 1998. Nyu: Description of the MENE Named Entity System as used in MUC7. In In Proceedings of the Seventh Message Understanding Conference (MUC-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Eliot Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<location>New York, NY, USA. Adviser-Grishman, Ralph.</location>
<contexts>
<context position="4604" citStr="Borthwick, 1999" startWordPosition="712" endWordPosition="713">ted • Introducing a new scoring function, which is quite competitive with the best measure and better than other well known information measures Approach Description S-MEMM MEMM based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. C</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Eliot Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York, NY, USA. Adviser-Grishman, Ralph.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Inverse Document Frequency (IDF): A Measure of Deviations from Poisson.</title>
<date>1995</date>
<booktitle>In Third Workshop on Very Large Corpora,</booktitle>
<pages>121--130</pages>
<contexts>
<context position="11070" citStr="Church and Gale, 1995" startWordPosition="1746" endWordPosition="1749">d). In the second step, we provide this tagging to the S-MEMM along with other set of features described in Section 3 During training, the cutoff threshold is set for selecting NEs (in bullet 2) above. Also the tagging obtained from the step 1 is added as a feature to 118 S-MEMM and a model is trained during the training phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI- measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated in few documents. We defined a new cri</context>
</contexts>
<marker>Church, Gale, 1995</marker>
<rawString>Kenneth Church and William Gale. 1995. Inverse Document Frequency (IDF): A Measure of Deviations from Poisson. In Third Workshop on Very Large Corpora, pages 121–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<date>1990</date>
<journal>Word Association Norms, Mutual Information, and Lexicography.</journal>
<contexts>
<context position="15569" citStr="Church and Hanks, 1990" startWordPosition="2563" endWordPosition="2566">s Gandhis.) etc. We remove words appearing with common suffixes like &apos;t (ein), 31-t (on), 4�- (yenge), etc. from the NE list. Term Co-occurrence: We use the term cooccurrence statistics to detect multi-word NEs. A word may be a NE in some context but not in another. E.g. �T (mahatma “saint”) when ap119 pearing with gA\DF (Gandhi “Gandhi”) is a NE, but may not be, otherwise. To identify such multiwords NEs, we use this heuristic. Such words can be identified using Term Co-occurrence. We use the given set of documents to find all word pairs. We then calculate Pointwise Mutual Information (PMI) (Church and Hanks, 1990) for each of these word pairs and order the pairs in descending order of their PMI values. Most of the word pairs belong to the following categories: • Adjective Noun combination (Adjectives followed by noun): This was the most frequent combination. E.g. BFnF g\D (bheeni gandh “sweet smell”) • Noun Verb combination: Edl DwknA (dil dhadakna, “heart beating”) • Adverb verb combination: EKlEKlAkr h\snA (khilkhilakar hansna, “merrily laugh”) • Cardinal/Ordinal Noun Combination: TowF d�r (thodi der, “some time”) • Named Entities • Hindi Idioms: uSl� sFDA (ullu seedha) • Noun Noun Combination: HyAtF</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word Association Norms, Mutual Information, and Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Clifton</author>
<author>Robert Cooley</author>
<author>Jason Rennie</author>
</authors>
<title>Topcat: Data mining for Topic Identification in a Text Corpus.</title>
<date>2002</date>
<contexts>
<context position="11311" citStr="Clifton et al., 2002" startWordPosition="1785" endWordPosition="1788">1 is added as a feature to 118 S-MEMM and a model is trained during the training phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI- measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated in few documents. We defined a new criteria which measures the ratio of the total number of times the word appears in the corpus to the number of documents containing a word. RF (w) = d (w) where cf(w) (w wheretotal frequency of a word in the whole corpus and df(w) is the docume</context>
</contexts>
<marker>Clifton, Cooley, Rennie, 2002</marker>
<rawString>Chris Clifton, Robert Cooley, and Jason Rennie. 2002. Topcat: Data mining for Topic Identification in a Text Corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised Models for Named Entity Classification. In</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="6357" citStr="Collins and Singer, 1999" startWordPosition="978" endWordPosition="981">s an open area. Moreover, no work has been done towards this for Indian languages. Figure 1: Block diagram of CLGIN Approach One shortcoming of current approaches is that they do not leverage on global distributional characteristics of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical English text, like blogs where spellings and POS tags are not correct. Some semi-supervised approaches (Collins and Singer, 1999), (Riloff and Jones, 1999), (Pas¸ca, 2007) have also used large available corpora to generate context patterns for named entities or for generating gazetteer lists and entity expansion using seed entities. Klementiev and Roth (2006) use cooccurrence of sets of terms within documents to boost the certainty (in a cross-lingual setting) that the terms in question were really transliterations of each other. In this paper, we contend that using such global distributional characteristics improves the performance of Hindi NEI when applied to a large corpus. Further, we show that the performance of su</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised Models for Named Entity Classification. In In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A Synopsis of Linguistic Theory 1930-1955. In</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<contexts>
<context position="12595" citStr="Firth, 1957" startWordPosition="2015" endWordPosition="2016"> of the term frequency. TF-IDF considers the frequency of the word in the document. RF considers it over the whole corpus. We use the scoring function (information measure) to score all the words. During training, we fix a threshold using the development set. During testing, we pick words above the threshold as NEs. We then apply heuristics to augment this list as well as to exclude terms from the generated list. 4.2 Heuristics for Pruning and Augmenting NE List Distributional Similarity: The underlying idea of Distributional Similarity is that a word is characterized by the company it keeps (Firth, 1957). Two words are said to be distributionally similar if they appear in similar contexts. From the previous step (Sect. 4.1), we get a list of words having high score. Say, top t, words were selected. In this step, we take t more words and then cluster together these words. The purpose at this phase is primarily to remove the false positives and to introduce more words which are expected to be NEs. For each distinct word, w in the corpus, we create a vector of the size of the number of distinct words in the corpus. Each term in the vector represents the frequency with which it appears in the con</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J.R. Firth. 1957. A Synopsis of Linguistic Theory 1930-1955. In In Studies in Linguistic Analysis, pages 1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>Xiaoxun Zhang</author>
<author>Xian Wu</author>
<author>Zhong Su</author>
</authors>
<title>Domain Adaptation with Latent Semantic Association for Named Entity Recognition.</title>
<date>2009</date>
<booktitle>In NAACL ’09,</booktitle>
<pages>281--289</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5628" citStr="Guo et al., 2009" startWordPosition="868" endWordPosition="871">d corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language specific features, like, context word information, POS tags, suffix and prefix information, gazetteer lists, common preceding and following words, etc. The performance of these systems is significantly hampered when the test corpus is not similar to the training corpus. Few studies (Guo et al., 2009), (Poibeau and Kosseim, 2001) have been performed towards genre/domain adaptation. But this still remains an open area. Moreover, no work has been done towards this for Indian languages. Figure 1: Block diagram of CLGIN Approach One shortcoming of current approaches is that they do not leverage on global distributional characteristics of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical </context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang, Xian Wu, and Zhong Su. 2009. Domain Adaptation with Latent Semantic Association for Named Entity Recognition. In NAACL ’09, pages 281–289, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sprck Jones</author>
</authors>
<title>A Statistical Interpretation of Term Specificity and its Application in Retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--11</pages>
<contexts>
<context position="11031" citStr="Jones, 1972" startWordPosition="1741" endWordPosition="1742">of NEs (uniword and multi-word). In the second step, we provide this tagging to the S-MEMM along with other set of features described in Section 3 During training, the cutoff threshold is set for selecting NEs (in bullet 2) above. Also the tagging obtained from the step 1 is added as a feature to 118 S-MEMM and a model is trained during the training phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI- measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Karen Sprck Jones. 1972. A Statistical Interpretation of Term Specificity and its Application in Retrieval. Journal of Documentation, 28:11–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Named Entity Transliteration and Discovery from Multilingual Comparable Corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>82--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6589" citStr="Klementiev and Roth (2006)" startWordPosition="1013" endWordPosition="1016">s of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical English text, like blogs where spellings and POS tags are not correct. Some semi-supervised approaches (Collins and Singer, 1999), (Riloff and Jones, 1999), (Pas¸ca, 2007) have also used large available corpora to generate context patterns for named entities or for generating gazetteer lists and entity expansion using seed entities. Klementiev and Roth (2006) use cooccurrence of sets of terms within documents to boost the certainty (in a cross-lingual setting) that the terms in question were really transliterations of each other. In this paper, we contend that using such global distributional characteristics improves the performance of Hindi NEI when applied to a large corpus. Further, we show that the performance of such systems which use global distribution characteristics is better than current state-of-the-art systems when the training and test corpus are not similar (different domain/genre) thereby being more suitable for domain adaptation. 3</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Named Entity Transliteration and Discovery from Multilingual Comparable Corpora. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 82–88, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In</title>
<date>2001</date>
<booktitle>ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4658" citStr="Lafferty et al., 2001" startWordPosition="719" endWordPosition="722">is quite competitive with the best measure and better than other well known information measures Approach Description S-MEMM MEMM based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="1713" citStr="Li and McCallum, 2003" startWordPosition="261" endWordPosition="264">ization, etc. It has many applications in Natural Language Processing (NLP) NER can be divided into two sub-tasks, Named Entity Identification (NEI) and Named Entity Classification (NEC). In this paper, we focus on the first step, i.e., Named Entity Identification. NEI is useful in applications where a list of Named Entities (NEs) is required. Machine Translation needs identification of named entities, so that they can be transliterated. For Indian languages, it is tough to identify named entities because of the lack of capitalization. Many approaches based on MEMM (Saha et al., 2008b), CRFs (Li and McCallum, 2003) and hybrid models have been tried for Hindi Named Entity Recognition. These approaches use only the local context for tagging the text. Many applications need entity identification in large corpora. When such a large corpus is to be tagged, one can use the global distributional characteristics of the words to identify the named entities. The state-of-the-art methods do not take advantage of these characteristics. Also, the performance of these systems degrades when the training and test corpus are from different domain or different genre. We present here our approach-Combined Local and Global</context>
<context position="5259" citStr="Li and McCallum, 2003" startWordPosition="810" endWordPosition="814">afferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language specific features, like, context word information, POS tags, suffix and prefix information, gazetteer lists, common preceding and following words, etc. The performance of these systems is significantly hampered when the test corpus is not similar to the training corpus. Few studies (Guo et al., 2009), (Poibeau and Kosseim, 2001) have been performed towards genre/domain adaptation. But this still remains an open area. Moreover, no work has been done towards this for Indian languages. Figure 1: Block diagram of CLGIN Approach On</context>
</contexts>
<marker>Li, McCallum, 2003</marker>
<rawString>Wei Li and Andrew McCallum. 2003. Rapid Development of Hindi Named Entity Recognition using Conditional Random Fields and Feature Induction. ACM Transactions on Asian Language Information Processing (TALIP), 2(3):290–294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew Mccallum</author>
</authors>
<title>Semi-supervised Sequence Modeling with Syntactic Topic Models.</title>
<date>2005</date>
<booktitle>In AAAI-05, The Twentieth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="4786" citStr="Li and Mccallum, 2005" startWordPosition="736" endWordPosition="740">M based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language specific features, like, context word information, POS tags</context>
</contexts>
<marker>Li, Mccallum, 2005</marker>
<rawString>Wei Li and Andrew Mccallum. 2005. Semi-supervised Sequence Modeling with Syntactic Topic Models. In AAAI-05, The Twentieth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marius Pas¸ca</author>
</authors>
<title>Organizing and Searching the World Wide Web of facts – Step Two: Harnessing the Wisdom of the Crowds.</title>
<date>2007</date>
<booktitle>In WWW ’07: Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>101--110</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Pas¸ca, 2007</marker>
<rawString>Marius Pas¸ca. 2007. Organizing and Searching the World Wide Web of facts – Step Two: Harnessing the Wisdom of the Crowds. In WWW ’07: Proceedings of the 16th international conference on World Wide Web, pages 101–110, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
</authors>
<title>Why Inverse Document Frequency? In</title>
<date>2001</date>
<booktitle>NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11136" citStr="Papineni, 2001" startWordPosition="1758" endWordPosition="1759">other set of features described in Section 3 During training, the cutoff threshold is set for selecting NEs (in bullet 2) above. Also the tagging obtained from the step 1 is added as a feature to 118 S-MEMM and a model is trained during the training phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI- measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated in few documents. We defined a new criteria which measures the ratio of the total number of times the wo</context>
</contexts>
<marker>Papineni, 2001</marker>
<rawString>Kishore Papineni. 2001. Why Inverse Document Frequency? In NAACL ’01: Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, pages 1–8, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Poibeau</author>
<author>Leila Kosseim</author>
</authors>
<title>Proper Name Extraction from Non-Journalistic Texts. In</title>
<date>2001</date>
<booktitle>In Computational Linguistics in the Netherlands,</booktitle>
<pages>144--157</pages>
<contexts>
<context position="5657" citStr="Poibeau and Kosseim, 2001" startWordPosition="872" endWordPosition="875"> 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language specific features, like, context word information, POS tags, suffix and prefix information, gazetteer lists, common preceding and following words, etc. The performance of these systems is significantly hampered when the test corpus is not similar to the training corpus. Few studies (Guo et al., 2009), (Poibeau and Kosseim, 2001) have been performed towards genre/domain adaptation. But this still remains an open area. Moreover, no work has been done towards this for Indian languages. Figure 1: Block diagram of CLGIN Approach One shortcoming of current approaches is that they do not leverage on global distributional characteristics of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical English text, like blogs wher</context>
</contexts>
<marker>Poibeau, Kosseim, 2001</marker>
<rawString>Thierry Poibeau and Leila Kosseim. 2001. Proper Name Extraction from Non-Journalistic Texts. In In Computational Linguistics in the Netherlands, pages 144–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D M Rennie</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Using Term Informativeness for Named Entity Detection.</title>
<date>2005</date>
<booktitle>In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>353--360</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="6096" citStr="Rennie and Jaakkola (2005)" startWordPosition="939" endWordPosition="942">ds, etc. The performance of these systems is significantly hampered when the test corpus is not similar to the training corpus. Few studies (Guo et al., 2009), (Poibeau and Kosseim, 2001) have been performed towards genre/domain adaptation. But this still remains an open area. Moreover, no work has been done towards this for Indian languages. Figure 1: Block diagram of CLGIN Approach One shortcoming of current approaches is that they do not leverage on global distributional characteristics of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical English text, like blogs where spellings and POS tags are not correct. Some semi-supervised approaches (Collins and Singer, 1999), (Riloff and Jones, 1999), (Pas¸ca, 2007) have also used large available corpora to generate context patterns for named entities or for generating gazetteer lists and entity expansion using seed entities. Klementiev and Roth (2006) use cooccurrence of sets of terms within documents to boost the certainty (in a cross-lingual setting) tha</context>
<context position="11389" citStr="Rennie and Jaakkola, 2005" startWordPosition="1797" endWordPosition="1800">raining phase. The following sections describe this approach in detail. 4.1 Information Measures/Scoring Functions Various measures have been introduced for determining the information content of the words. These include, IDF (Inverse Document Frequency) (Jones, 1972) , Residual IDF (Church and Gale, 1995), xI- measure (Bookstein and Swanson, 1974), Gain (Papineni, 2001), etc. We introduced our own information measure, RF (Ratio of Frequencies). 4.1.1 RF (Ratio of Frequencies) NEs are highly relevant words in a document (Clifton et al., 2002) and are expected to have high information content (Rennie and Jaakkola, 2005). It has been found that words that appear frequently in a set of documents and not so frequently in the rest of the documents are important with respect to that set of documents where they are frequent. We expected the NEs to be concentrated in few documents. We defined a new criteria which measures the ratio of the total number of times the word appears in the corpus to the number of documents containing a word. RF (w) = d (w) where cf(w) (w wheretotal frequency of a word in the whole corpus and df(w) is the document frequency. This measure is different from the TF-IDF measure in terms of th</context>
</contexts>
<marker>Rennie, Jaakkola, 2005</marker>
<rawString>Jason D. M. Rennie and Tommi Jaakkola. 2005. Using Term Informativeness for Named Entity Detection. In SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 353– 360, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<journal>Artificial Intelligence.</journal>
<booktitle>In AAAI ’99/IAAI ’99: Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence,</booktitle>
<pages>474--479</pages>
<publisher>American Association for</publisher>
<location>Menlo Park, CA, USA.</location>
<contexts>
<context position="6383" citStr="Riloff and Jones, 1999" startWordPosition="982" endWordPosition="985"> work has been done towards this for Indian languages. Figure 1: Block diagram of CLGIN Approach One shortcoming of current approaches is that they do not leverage on global distributional characteristics of words (e.g., Information Content, Term Co-occurrence statistics, etc.) when a large corpus needs NEI. Rennie and Jaakkola (2005) introduced a new information measure and used it for NE detection. They used this approach only on uncapitalized and ungrammatical English text, like blogs where spellings and POS tags are not correct. Some semi-supervised approaches (Collins and Singer, 1999), (Riloff and Jones, 1999), (Pas¸ca, 2007) have also used large available corpora to generate context patterns for named entities or for generating gazetteer lists and entity expansion using seed entities. Klementiev and Roth (2006) use cooccurrence of sets of terms within documents to boost the certainty (in a cross-lingual setting) that the terms in question were really transliterations of each other. In this paper, we contend that using such global distributional characteristics improves the performance of Hindi NEI when applied to a large corpus. Further, we show that the performance of such systems which use globa</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In AAAI ’99/IAAI ’99: Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence, pages 474–479, Menlo Park, CA, USA. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujan Kumar Saha</author>
<author>Sanjay Chatterji</author>
<author>Sandipan Dandapat</author>
<author>Sudeshna Sarkar</author>
<author>Pabitra Mitra</author>
</authors>
<title>A Hybrid Named Entity Recognition System for South and South East Asian Languages.</title>
<date>2008</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages,</booktitle>
<pages>17--24</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="1681" citStr="Saha et al., 2008" startWordPosition="256" endWordPosition="259">ike person, location, organization, etc. It has many applications in Natural Language Processing (NLP) NER can be divided into two sub-tasks, Named Entity Identification (NEI) and Named Entity Classification (NEC). In this paper, we focus on the first step, i.e., Named Entity Identification. NEI is useful in applications where a list of Named Entities (NEs) is required. Machine Translation needs identification of named entities, so that they can be transliterated. For Indian languages, it is tough to identify named entities because of the lack of capitalization. Many approaches based on MEMM (Saha et al., 2008b), CRFs (Li and McCallum, 2003) and hybrid models have been tried for Hindi Named Entity Recognition. These approaches use only the local context for tagging the text. Many applications need entity identification in large corpora. When such a large corpus is to be tagged, one can use the global distributional characteristics of the words to identify the named entities. The state-of-the-art methods do not take advantage of these characteristics. Also, the performance of these systems degrades when the training and test corpus are from different domain or different genre. We present here our ap</context>
<context position="5058" citStr="Saha et al., 2008" startWordPosition="782" endWordPosition="785">s 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language specific features, like, context word information, POS tags, suffix and prefix information, gazetteer lists, common preceding and following words, etc. The performance of these systems is significantly hampered when the test corpus is not similar to the training corpus. Few studies (Guo et al., 2009), (Poibeau and Kosseim, 2001) </context>
<context position="7307" citStr="Saha et al., 2008" startWordPosition="1124" endWordPosition="1127">g) that the terms in question were really transliterations of each other. In this paper, we contend that using such global distributional characteristics improves the performance of Hindi NEI when applied to a large corpus. Further, we show that the performance of such systems which use global distribution characteristics is better than current state-of-the-art systems when the training and test corpus are not similar (different domain/genre) thereby being more suitable for domain adaptation. 3 MEMM based Statistical System (S-MEMM) We implemented the Maximum Entropy Markov Model based system(Saha et al., 2008b) for NE Identification. We use this system as our Baseline and compare our approaches NGI and CLGIN with this baseline. We used various language dependent and independent features. An important DataSetto be Tagged Threshold (Set using Development Set Trained Model Corpus Added as a feature Select words based on Information Measure NEIG Tagged DataSet Final Tagged DataSet Statistical System (MEMM) Features (Context Words, POS Tags, Suffix Info, Gazetteers, Lexicon, etc.) Applying Augmenting Heuristics Applying Pruning Heuristics MEMM Based Statistical System (S-MEMM) Step 1 Tagging using Glob</context>
</contexts>
<marker>Saha, Chatterji, Dandapat, Sarkar, Mitra, 2008</marker>
<rawString>Sujan Kumar Saha, Sanjay Chatterji, Sandipan Dandapat, Sudeshna Sarkar, and Pabitra Mitra. 2008a. A Hybrid Named Entity Recognition System for South and South East Asian Languages. In Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages, pages 17–24, Hyderabad, India, January. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujan Kumar Saha</author>
<author>Sudeshna Sarkar</author>
<author>Pabitra Mitra</author>
</authors>
<title>A Hybrid Feature Set Based Maximum Entropy Hindi Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<location>Kharagpur, India.</location>
<contexts>
<context position="1681" citStr="Saha et al., 2008" startWordPosition="256" endWordPosition="259">ike person, location, organization, etc. It has many applications in Natural Language Processing (NLP) NER can be divided into two sub-tasks, Named Entity Identification (NEI) and Named Entity Classification (NEC). In this paper, we focus on the first step, i.e., Named Entity Identification. NEI is useful in applications where a list of Named Entities (NEs) is required. Machine Translation needs identification of named entities, so that they can be transliterated. For Indian languages, it is tough to identify named entities because of the lack of capitalization. Many approaches based on MEMM (Saha et al., 2008b), CRFs (Li and McCallum, 2003) and hybrid models have been tried for Hindi Named Entity Recognition. These approaches use only the local context for tagging the text. Many applications need entity identification in large corpora. When such a large corpus is to be tagged, one can use the global distributional characteristics of the words to identify the named entities. The state-of-the-art methods do not take advantage of these characteristics. Also, the performance of these systems degrades when the training and test corpus are from different domain or different genre. We present here our ap</context>
<context position="5058" citStr="Saha et al., 2008" startWordPosition="782" endWordPosition="785">s 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language specific features, like, context word information, POS tags, suffix and prefix information, gazetteer lists, common preceding and following words, etc. The performance of these systems is significantly hampered when the test corpus is not similar to the training corpus. Few studies (Guo et al., 2009), (Poibeau and Kosseim, 2001) </context>
<context position="7307" citStr="Saha et al., 2008" startWordPosition="1124" endWordPosition="1127">g) that the terms in question were really transliterations of each other. In this paper, we contend that using such global distributional characteristics improves the performance of Hindi NEI when applied to a large corpus. Further, we show that the performance of such systems which use global distribution characteristics is better than current state-of-the-art systems when the training and test corpus are not similar (different domain/genre) thereby being more suitable for domain adaptation. 3 MEMM based Statistical System (S-MEMM) We implemented the Maximum Entropy Markov Model based system(Saha et al., 2008b) for NE Identification. We use this system as our Baseline and compare our approaches NGI and CLGIN with this baseline. We used various language dependent and independent features. An important DataSetto be Tagged Threshold (Set using Development Set Trained Model Corpus Added as a feature Select words based on Information Measure NEIG Tagged DataSet Final Tagged DataSet Statistical System (MEMM) Features (Context Words, POS Tags, Suffix Info, Gazetteers, Lexicon, etc.) Applying Augmenting Heuristics Applying Pruning Heuristics MEMM Based Statistical System (S-MEMM) Step 1 Tagging using Glob</context>
</contexts>
<marker>Saha, Sarkar, Mitra, 2008</marker>
<rawString>Sujan Kumar Saha, Sudeshna Sarkar, and Pabitra Mitra. 2008b. A Hybrid Feature Set Based Maximum Entropy Hindi Named Entity Recognition. In Proceedings of the Third International Joint Conference on Natural Language Processing, Kharagpur, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajeev Sangal</author>
<author>Dipti Sharma</author>
<author>Anil Singh</author>
<author>editors</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages. Asian Federation of Natural Language Processing,</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="14255" citStr="Sangal et al., 2008" startWordPosition="2327" endWordPosition="2330"> the cluster have higher rank i.e. lower information content, we remove it from the NE set. This heuristic is used for both augmenting the list as well to exclude terms from the list. Lexicon: We used this as a list for excluding terms. Terms present in the lexicon have a high chance of not being NEs. When used alone, the lexicon is not very effective (explained in Section 5.2). But, when used with other approaches, it helps in improving the precision of the system significantly. State-of-art Hindi NER systems use lists of gazetteers for Person names, location names, organization names, etc. (Sangal et al., 2008), but lexicon of general words has not been used. Unlike English, for Indian languages, it is important to know, if a word is a general word or not. Lexicons as opposed to gazetteers are generic and can be applied to any domain. Unlike gazetteers, the words would be quite common and would appear in any text irrespective of the domain. Suffixes: NEs in Hindi are open class words and appear as free morphemes. Unlike nouns, NEs, usually do not take any suffixes (attached to them). However, there are few exceptions like, cTc-f N* 4 TF�T (laal kile ke baahar, (outside Red Fort)) or when NEs are use</context>
</contexts>
<marker>Sangal, Sharma, Singh, editors, 2008</marker>
<rawString>Rajeev Sangal, Dipti Sharma, and Anil Singh, editors. 2008. Proceedings of the IJCNLP-08 Workshop on Named Entity Recognition for South and South East Asian Languages. Asian Federation of Natural Language Processing, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Volker</author>
</authors>
<title>Towards Large-Scale, OpenDomain and Ontology-Based Named Entity Classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP’05,</booktitle>
<pages>166--172</pages>
<publisher>INCOMA Ltd.</publisher>
<contexts>
<context position="4731" citStr="Volker, 2005" startWordPosition="731" endWordPosition="732">ation measures Approach Description S-MEMM MEMM based statistical system without (Baseline) inserting global information NGI Uses global distributional characteristics along with language information for NE Identification CLGIN Combines the global characteristics derived using NGI with S-MEMM Table 1: Summary of Approaches 2 Related Work There is a plethora of work on NER for English ranging from supervised approaches like HMMs(Bikel et al., 1999), Maximum Entropy (Borthwick, 1999) (Borthwick et al., 1998), CRF (Lafferty et al., 2001) and SVMs to unsupervised (Alfonseca and Manandhar, 2002), (Volker, 2005) and semi-supervised approaches (Li and Mccallum, 2005). However, these approaches do not perform well for Indian languages mainly due to lack of capitalization and unavailability of good gazetteer lists. The best F Score reported for Hindi NER using these approaches on a standard corpus (IJCNLP) is 65.13% ((Saha et al., 2008a)). Higher accuracies have been reported (81%) (Saha et al., 2008b), albeit, on a non-standard corpus using rules and comprehensive gazetteers. Current state-of-the-art systems (Li and McCallum, 2003) (Saha et al., 2008b) use various language independent and language spec</context>
</contexts>
<marker>Volker, 2005</marker>
<rawString>Johanna Volker. 2005. Towards Large-Scale, OpenDomain and Ontology-Based Named Entity Classification. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP’05, pages 166–172. INCOMA Ltd.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>