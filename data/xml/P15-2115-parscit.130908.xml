<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000147">
<title confidence="0.994042">
Machine Comprehension with Syntax, Frames, and Semantics
</title>
<author confidence="0.994401">
Hai Wang Mohit Bansal Kevin Gimpel David McAllester
</author>
<affiliation confidence="0.98164">
Toyota Technological Institute at Chicago, Chicago, IL, 60637, USA
</affiliation>
<email confidence="0.994624">
{haiwang,mbansal,kgimpel,mcallester}@ttic.edu
</email>
<sectionHeader confidence="0.993767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952615384616">
We demonstrate significant improvement
on the MCTest question answering task
(Richardson et al., 2013) by augmenting
baseline features with features based on
syntax, frame semantics, coreference, and
word embeddings, and combining them in
a max-margin learning framework. We
achieve the best results we are aware of on
this dataset, outperforming concurrently-
published results. These results demon-
strate a significant performance gradient
for the use of linguistic structure in ma-
chine comprehension.
</bodyText>
<sectionHeader confidence="0.998789" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999398075471698">
Recent question answering (QA) systems (Fer-
rucci et al., 2010; Berant et al., 2013; Bordes et
al., 2014) have focused on open-domain factoid
questions, relying on knowledge bases like Free-
base (Bollacker et al., 2008) or large corpora of
unstructured text. While clearly useful, this type
of QA may not be the best way to evaluate natu-
ral language understanding capability. Due to the
redundancy of facts expressed on the web, many
questions are answerable with shallow techniques
from information extraction (Yao et al., 2014).
There is also recent work on QA based on syn-
thetic text describing events in
adventure games (Weston et al., 2015;
Sukhbaatar et al., 2015). Synthetic text provides
a cleanroom environment for evaluating QA
systems, and has spurred development of power-
ful neural architectures for complex reasoning.
However, the formulaic semantics underlying
these synthetic texts allows for the construction
of perfect rule-based question answering sys-
tems, and may not reflect the patterns of natural
linguistic expression.
In this paper, we focus on machine compre-
hension, which is QA in which the answer is con-
tained within a provided passage. Several compre-
hension tasks have been developed, including Re-
media (Hirschman et al., 1999), CBC4kids (Breck
et al., 2001), and the QA4MRE textual question
answering tasks in the CLEF evaluations (Pe˜nas et
al., 2011; Pe˜nas et al., 2013; Clark et al., 2012;
Bhaskar et al., 2012).
We consider the Machine Comprehension of
Text dataset (MCTest; Richardson et al., 2013),
a set of human-authored fictional stories with as-
sociated multiple-choice questions. Knowledge
bases and web corpora are not useful for this task,
and answers are typically expressed just once in
each story. While simple baselines presented by
Richardson et al. answer over 60% of questions
correctly, many of the remaining questions require
deeper analysis.
In this paper, we explore the use of depen-
dency syntax, frame semantics, word embeddings,
and coreference for improving performance on
MCTest. Syntax, frame semantics, and coref-
erence are essential for understanding who did
what to whom. Word embeddings address varia-
tion in word choice between the stories and ques-
tions. Our added features achieve the best results
we are aware of on this dataset, outperforming
concurrently-published results (Narasimhan and
Barzilay, 2015; Sachan et al., 2015).
</bodyText>
<sectionHeader confidence="0.991637" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999863181818182">
We use a simple latent-variable classifier trained
with a max-margin criterion. Let P denote the
passage, q denote the question of interest, and A
denote the set of candidate answers for q, where
each a E A denotes one candidate answer. We
want to learn a function h : (P, q) → A that, given
a passage and a question, outputs a legal a E A.
We use a linear model for h that uses a latent vari-
able w to identify the sentence in the passage in
which the answer can be found.
Let W denote the set of sentences within the
</bodyText>
<page confidence="0.819662">
700
</page>
<bodyText confidence="0.825071625">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 700–706,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
passage, where a particular w ∈ W denotes one
sentence.
Given a feature vector f(P, w, q, a) and a
weight vector B with an entry for each feature, the
prediction aˆ for a new P and q is given by:
</bodyText>
<equation confidence="0.6156776">
BTf(P, w, q, a)
Given triples {hPi, qi, aii}ni=1, we minimize an
E2-regularized max-margin loss function:
+ max max BT f (Pi, w&apos;, qi, a) + Δ(a, ai)
aEA �&amp;EW 11
</equation>
<bodyText confidence="0.996068666666667">
where A is the weight of the E2 term and
Δ(a, ai) = 1 if a =6 ai and 0 otherwise. The latent
variable w makes the loss function non-convex.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.999980416666667">
We start with two features from Richardson et al.
(2013). Our first feature corresponds to their slid-
ing window similarity baseline, which measures
weighted word overlap between the bag of words
constructed from the question/answer and the bag
of words in the window. We call this feature B.
The second feature corresponds to their word dis-
tance baseline, and is the minimal distance be-
tween two word occurrences in the passage that
are also contained in the question/answer pair. We
call this feature D. Space does not permit a de-
tailed description.
</bodyText>
<subsectionHeader confidence="0.999775">
3.1 Frame Semantic Features
</subsectionHeader>
<bodyText confidence="0.997782941176471">
Frame semantic parsing (Das et al., 2014)
is the problem of extracting frame-specific
predicate-argument structures from sentences,
where the frames come from an inventory such as
FrameNet (Baker et al., 1998). This task can be
decomposed into three subproblems: target iden-
tification, in which frame-evoking predicates are
marked; frame label identification, in which the
evoked frame is selected for each predicate; and
argument identification, in which arguments to
each frame are identified and labeled with a role
from the frame. An example output of the SE-
MAFOR frame semantic parser (Das et al., 2014)
is given in Figure 1.
Three frames are identified. The target words
pulled, all, and shelves have respective frame la-
bels CAUSE MOTION, QUANTITY, and NATU-
</bodyText>
<figureCaption confidence="0.998414">
Figure 1: Example output from SEMAFOR.
</figureCaption>
<bodyText confidence="0.997702681818182">
RAL FEATURES. Each frame has its own set of ar-
guments; e.g., the CAUSE MOTION frame has the
labeled Agent, Theme, and Goal arguments. Fea-
tures from these parses have been shown to be use-
ful for NLP tasks such as slot filling in spoken dia-
logue systems (Chen et al., 2013). We expect that
the passage sentence containing the answer will
overlap with the question and correct answer in
terms of predicates, frames evoked, and predicted
argument labels, and we design features to capture
this intuition. Given the frame semantic parse for a
sentence, let T be the bag of frame-evoking target
words/phrases.1 We define the bag of frame labels
in the parse as F. For each target t ∈ T, there is an
associated frame label denoted Ft ∈ F. Let R be
the bag of phrases assigned with an argument label
in the parse. We denote the bag of argument labels
in the parse by L. For each phrase r ∈ R, there is
an argument label denoted Lr ∈ L. We define a
frame semantic parse as a tuple hT, F, R, Li. We
define six features based on two parsed sentences
hT1, F1, R1, L1i and hT2, F2, R2, L2i:
</bodyText>
<listItem confidence="0.999314083333333">
• f1: # frame label matches: |{hs,ti : s ∈
F1, t ∈ F2, s = t}|
• f2: # argument label matches: |{hs, ti : s ∈
L1, t ∈ L2, s = t}|.
• f3: # target matches, ignoring frame labels:
|{hs, ti : s ∈ T1, t ∈ T2, s = t}|.
• f4: # argument matches, ignoring arg. labels:
|{hs,ti : s ∈ R1,t ∈ R2,s = t}|.
• f5: # target matches, using frame labels:
|{hs,ti:s∈T1,t∈T2,s=t, Fs1= Ft2}|.
• f6: # argument matches, using arg. labels:
|{hs,ti : s ∈ R1,t ∈ R2,s = t,L1s = L2 t }|.
</listItem>
<bodyText confidence="0.999866">
We use two versions of each of these six features:
one version for the passage sentence w and the
question q, and an additional version for w and the
candidate answer a.
</bodyText>
<subsectionHeader confidence="0.999591">
3.2 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999515">
If two sentences refer to the same event, then it is
likely that they have some overlapping dependen-
</bodyText>
<footnote confidence="0.79432">
1By bag, we mean here a set with possible replicates.
</footnote>
<equation confidence="0.996877545454545">
min A||B||2 +
θ
��
n
− max BT f (Pi, w, qi, ai)
wEW
i=1
aˆ = arg max
aEA
max
wEW
</equation>
<page confidence="0.98966">
701
</page>
<figureCaption confidence="0.998787">
Figure 2: Transforming the question to a statement.
</figureCaption>
<bodyText confidence="0.999819866666667">
cies. To compare a Q/A pair to a sentence in the
passage, we first use rules to transform the ques-
tion into a statement and insert the candidate an-
swer into the trace position. Our simple rule set
is inspired by the rich history of QA research into
modeling syntactic transformations between ques-
tions and answers (Moschitti et al., 2007; Wang et
al., 2007; Heilman and Smith, 2010). Given Stan-
ford dependency tree and part-of-speech (POS)
tags for the question, let arc(u, v) be the label of
the dependency between child word u and head
word v, let POS(u) be the POS tag of u, let c be
the wh-word in the question, let r be the root word
in the question’s dependency tree, and let a be the
candidate answer. We use the following rules:2
</bodyText>
<listItem confidence="0.994584705882353">
• c = what, POS(r) = VB, and arc(c, r) = dobj.
Insert a after word u where arc(u, r) = nsubj.
Delete c and the word after c.
• c = what, POS(r) = NN, and arc(c, r) =
nsubj. Replace c by a.
• c = where, POS(r) = VB, and arc(c, r) = ad-
vmod. Delete c and the word after c. If r has a
child u such that arc(u, r) = dobj, insert a after
u; else, insert a after r and delete r.
• c = where, r = is, POS(r) = VBZ, and arc(c,
r) = advmod. Delete c. Find r’s child u such
that arc(u, r) = nsubj, move r to be right after
u. Insert a after r.
• c = who, POS(r) = NN, and arc(c, r) =
nsubj. Replace c by a.
• c = who, POS(r) E {VB, VBD}, and arc(c, r)
= nsubj. Replace c by a.
</listItem>
<bodyText confidence="0.989906428571429">
We use other rules in addition to those above:
change “why x?” to “the reason x is a”, and
change “how many x”, “how much x”, or “when
x” to “x a”.
Given each candidate answer, we attempt to
transform the question to a statement using the
2There are existing rule-based approaches to transforming
statements to questions (Heilman, 2011); our rules reverse
this process.
rules above.3 An example of the transformation
is given in Figure 2. In the parse, pull is the root
word and What is attached as a dobj. This matches
the first rule, so we delete did and insert the can-
didate answer pudding after pull, making the final
transformed sentence: James pull pudding off.
After this transformation of the question (and
a candidate answer) to a statement, we mea-
sure its similarity to the sentence in the window
using simple dependency-based similarity fea-
tures. Denoting a dependency as (u, v, arc(u, v)),
then two dependencies (u1, v1, arc(u1, v1)) and
(u2, v2, arc(u2, v2)) match if and only if u1 = u2,
v1 = v2, and arc(u1, v1) = arc(u2, v2). One
feature simply counts the number of dependency
matches between the transformed question and the
passage sentence. We include three additional
count features that each consider a subset of de-
pendencies from the following three categories:
</bodyText>
<listItem confidence="0.7853815">
(1) v = r and u = a; (2) v = r but u =�a; and
(3) v =� r. In Figure 2, the triples
</listItem>
<bodyText confidence="0.913964666666667">
(James, pull, nsubj) and (off, pull, prt) belong to
the second category while (pudding, pull, dobj)
belongs to the first.
</bodyText>
<subsectionHeader confidence="0.99848">
3.3 Word Embeddings
</subsectionHeader>
<bodyText confidence="0.999956363636364">
Word embeddings (Mikolov et al., 2013) repre-
sent each word as a low-dimensional vector where
the similarity of vectors captures some aspect of
semantic similarity of words. They have been
used for many tasks, including semantic role label-
ing (Collobert et al., 2011), named entity recogni-
tion (Turian et al., 2010), parsing (Bansal et al.,
2014), and for the Facebook QA tasks (Weston et
al., 2015; Sukhbaatar et al., 2015). We first de-
fine the vector f+w as the vector summation of all
words inside sentence w and f×w as the element-
wise multiplication of the vectors in w. To define
vectors for answer a for question q, we concate-
nate q and a, then calculate f+qa and f×qa. For the
bag-of-words feature B, instead of merely count-
ing matches of the two bags of words, we also use
cos(f+qa, f+w ) and cos(f×qa, f×w ) as features, where
cos is cosine similarity. For syntactic features,
where Tw is the bag of dependencies of w and
Tqa is the bag of dependencies for the transformed
question for candidate answer a, we use a feature
function that returns the following:
</bodyText>
<equation confidence="0.391183">
� � ✶`=`/ cos(u, u0) cos(v, v0)
(u,v,`)∈τw (u/,v/,`/)∈τqa
</equation>
<footnote confidence="0.910259">
3If no rule applies, we return 0 for all syntactic features.
</footnote>
<page confidence="0.991607">
702
</page>
<bodyText confidence="0.995204">
where E is short for arc(u, v).4
</bodyText>
<subsectionHeader confidence="0.969251">
3.4 Coreference Resolution
</subsectionHeader>
<bodyText confidence="0.999899538461538">
Coreference resolution systems aim to identify
chains of mentions (within and across sentences)
that refer to the same entity. We integrate coref-
erence information into the bag-of-words, frame
semantic, and syntactic features. We run a coref-
erence resolution system on each passage, then for
these three sets of features, we replace exact string
match with a check for membership in the same
coreference chain.
When using features augmented by word em-
beddings or coreference, we create new versions
of the features that use the new information, con-
catenating them with the original features.
</bodyText>
<sectionHeader confidence="0.999797" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999970785714286">
MCTest splits its stories into train, development,
and test sets. The original MCtest DEV is too
small, to choose the best feature set, we merged
the train and development sets in MC160 and
MC500 and split them randomly into a 250-story
training set (TRAIN) and a 200-story development
set (DEV). We optimize the max-margin training
criteria on TRAIN and use DEV to tune the regular-
izer A and choose the best feature set. We report
final performance on the original two test sets (for
comparability) from MCTest, named MC160 and
MC500.
We use SEMAFOR (Das et al., 2010; Das et
al., 2014) for frame semantic parsing and the lat-
est Stanford dependency parser (Chen and Man-
ning, 2014) as our dependency parser. We use
the Stanford rule-based system for coreference
resolution (Lee et al., 2013). We use the pre-
trained 300-dimensional word embeddings down-
loadable from the word2vec site.5 We denote
the frame semantic features by F and the syntac-
tic features by S. We use superscripts &apos;&apos; and &apos; to
indicate the use of embeddings and coreference
for a particular feature set. To minimize the loss,
we use the miniFunc package in MATLAB with
LBFGS (Nocedal, 1980; Liu and Nocedal, 1989).
The accuracy of different feature sets on DEV is
given in Table 1.6 The boldface results correspond
</bodyText>
<footnote confidence="0.981764833333333">
4Similar to the original syntactic features (see end of Sec-
tion 3.2), we also have 3 additional features for the three sub-
set categories.
5https://code.google.com/p/word2vec/
6All accuracies are computed with tie-breaking partial
credit (similar to previous work), i.e., if we have the same
</footnote>
<bodyText confidence="0.999590733333333">
to the best feature set combination chosen by eval-
uating on DEV. In this case, the feature dimen-
sionality is 29, which includes 4 bag-of-words fea-
tures, 1 distance feature, 12 frame semantic fea-
tures, and with the remaining being syntactic fea-
tures. After choosing the best feature set on DEV,
we then evaluate our system on TEST.
Negations: in preliminary experiments, we
found that our system suffered with negation ques-
tions, so we developed a simple heuristic to deal
with them. We identify a question as negation if it
contains “not” or “n’t” and does not begin with
“how” or “why”. If a question is identified as
negation, we then negate the final score for each
candidate answer.
</bodyText>
<table confidence="0.90673025">
Features DEV Accuracy (%)
B + D + F 64.18
B+D+F+S 66.24
Bwc + D + Fc + Swc 69.87
</table>
<tableCaption confidence="0.997944">
Table 1: Accuracy on DEV.
</tableCaption>
<bodyText confidence="0.999936846153846">
The final test results are shown in Table 2. We
first compare to results from prior work (Richard-
son et al., 2013). Their first result uses a slid-
ing window with the bag-of-words feature B de-
scribed in Sec. 3; this system is called “Base-
line 1” (B1). They then add the distance feature
D, also described in Sec. 3. The combined sys-
tem, which uses B and D, is called “Baseline 2”
(B2). Their third result adds a rich textual entail-
ment system to B2; it is referred to as B2+RTE.7
We also compare to concurrently-published re-
sults (Narasimhan and Barzilay, 2015; Sachan et
al., 2015).
We report accuracies for all questions as well
as separately for the two types: those that are
answerable with a single sentence from the pas-
sage (“Single”) and those that require multiple
sentences (“Multiple”). We see gains in accuracy
of 6% absolute compared to the B2+RTE base-
line and also outperform concurrently-published
results (Narasimhan and Barzilay, 2015; Sachan
et al., 2015). Even though our system only ex-
plicitly uses a single sentence from the passage
when choosing an answer, we improve baseline
accuracy for both single-sentence and multiple-
sentence questions. 8
</bodyText>
<footnote confidence="0.811769">
score for all four candidate answers, then we get partial credit
of 0.25 for this question.
7These three results are obtained from files at
http://research.microsoft.com/en-us/
um/redmond/projects/mctest/results.html.
8However, we inspected these question annotations and
</footnote>
<page confidence="0.99519">
703
</page>
<table confidence="0.999394625">
System MC160 MC500
Single (112) Multiple (128) All Single (272) Multiple (328) All
B1 64.73 56.64 60.41 58.21 56.17 57.09
Richardson et al. (2013) B2 75.89 60.15 67.50 64.00 57.46 60.43
B2+RTE 76.78 62.50 69.16 68.01 59.45 63.33
Narasimhan and Barzilay (2015) 82.36 65.23 73.23 68.38 59.90 63.75
Sachan et al. (2015) - - - 67.65 67.99 67.83
our system 84.22 67.85 75.27 72.05 67.94 69.94
</table>
<tableCaption confidence="0.967395">
Table 2: Accuracy comparison of published results on test sets.
</tableCaption>
<table confidence="0.9998556">
Features DEV Accuracy (%)
full (Bwc+D+Fc+Swc) 69.87
− Bwc (D + Fc+Swc) 58.46
− D (Bwc+Fc+Swc) 65.89
− Bwc, − D (Fc+Swc) 54.19
− embeddings (Bc+D+Fc+Sc) 68.28
− coreference (Bw+D+F+Sw) 68.43
− frame semantics (Bwc+D+Swc) 67.89
− syntax (Bwc+D+Fc) 67.64
− negation (Bwc+D+Fc+Swc) 68.72
</table>
<tableCaption confidence="0.999973">
Table 3: Ablation study of feature types on the dev set.
</tableCaption>
<bodyText confidence="0.999972454545454">
We also measure the contribution of each fea-
ture set by deleting it from the full feature set.
These ablation results are shown in Table 3. We
find that frame semantic and syntax features con-
tribute almost equally, and using word embed-
dings contributes slightly more than coreference
information. If we delete the bag-of-words and
distance features, then accuracy drops signifi-
cantly, which suggests that in MCTest, simple
surface-level similarity features suffice to answer
a large portion of questions.
</bodyText>
<sectionHeader confidence="0.977576" genericHeader="evaluation">
5 Analysis
</sectionHeader>
<bodyText confidence="0.9998022">
Successes To show the effects of different fea-
tures, we show cases where the full system gives
the correct prediction (marked with *) but ablat-
ing the named features causes the incorrect answer
(marked with †) to be predicted:
</bodyText>
<figureCaption confidence="0.88661">
Ex. 1: effect of embeddings: we find the soft similarity
between ‘noodle’ and ‘spaghetti’.
</figureCaption>
<figure confidence="0.524608428571429">
clue: Marsha’s favorite dinner was spaghetti.
q: What is Marsha’s noodle made out of? *A) Spaghetti;
†C) mom;
Ex. 2: coreference resolves She to Hannah Harvey.
Hannah Harvey was a ten year old. She lived in New York.
q: Where does Hannah Harvey live? *A) New York; †C)
Kenya;
</figure>
<figureCaption confidence="0.212991333333333">
Ex. 4: effect of syntax: by inserting answer C, the trans-
formed statement is: Todd say there’s no place like home
when he got home from the city.
</figureCaption>
<bodyText confidence="0.84393">
occasionally found them to be noisy, which may cloud these
comparisons.
When his mom asked him about his trip to the city Todd
said, “There’s no place like home.”
q: What did Todd say when he got home from the city? †B)
There were so many people in cars; *C) There’s no place
like home;
Errors To give insight into our system’s perfor-
mance and reveal future research directions, we
also analyzed the errors made by our system. We
found that many required inferential reasoning,
counting, set enumeration, multiple sentences,
time manipulation, and comparisons. Some ran-
domly sampled examples are given below, with the
correct answer starred (*):
</bodyText>
<figureCaption confidence="0.849811666666667">
Ex. 1: requires inference across multiple sentences:
One day Fritz got a splinter in his foot. Stephen did not
believe him. Fritz showed him the picture. Then Stephen
believed him. q: What made Stephen believe Fritz? *A)
the picture of the splinter in his foot; †C) the picture of the
cereal with milk;
</figureCaption>
<figure confidence="0.6502434">
Ex. 2: requires temporal reasoning and world knowledge:
Ashley woke up bright and early on Friday morning. Her
birthday was only a day away. q: What day of the week was
Ashley’s birthday? *A) Saturday; †C) Friday;
Ex. 3: requires comparative reasoning:
</figure>
<figureCaption confidence="0.856599666666667">
Tommy has an old bicycle now. He is getting too big for
it. q: What’s wrong with Tommy’s old bicycle? *B) it’s too
small; †C) it’s old;
</figureCaption>
<sectionHeader confidence="0.991987" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999395">
We proposed several novel features for machine
comprehension, including those based on frame
semantics, dependency syntax, word embeddings,
and coreference resolution. Empirical results
demonstrate substantial improvements over sev-
eral strong baselines, achieving new state-of-the-
art results on MCTest. Our error analysis sug-
gests that deeper linguistic analysis and inferential
reasoning can yield further improvements on this
task.
</bodyText>
<sectionHeader confidence="0.989804" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.977106">
We thank Dekang Lin, Nathan Schneider and the
anonymous reviewers for helpful comments.
</bodyText>
<page confidence="0.997692">
704
</page>
<sectionHeader confidence="0.951518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999328733944954">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of the 17th International Conference on Com-
putational Linguistics-Volume 1, pages 86–90. As-
sociation for Computational Linguistics.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
809–815, Baltimore, Maryland, June. Association
for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of EMNLP.
Pinaki Bhaskar, Partha Pakray, Somnath Banerjee,
Samadrita Banerjee, Sivaji Bandyopadhyay, and
Alexander F Gelbukh. 2012. Question answering
system for QA4MRE@CLEF 2012. In CLEF (On-
line Working Notes/Labs/Workshop).
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.
Antoine Bordes, Sumit Chopra, and Jason Weston.
2014. Question answering with subgraph embed-
dings. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 615–620, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Eric Breck, Marc Light, Gideon S Mann, Ellen Riloff,
Brianne Brown, Pranav Anand, Mats Rooth, and
Michael Thelen. 2001. Looking under the hood:
Tools for diagnosing your question answering en-
gine. In Proceedings of the workshop on Open-
domain question answering-Volume 12, pages 1–8.
Association for Computational Linguistics.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 740–750, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Yun-Nung Chen, William Yang Wang, and Alexander I
Rudnicky. 2013. Unsupervised induction and fill-
ing of semantic slots for spoken dialogue systems
using frame-semantic parsing. In Automatic Speech
Recognition and Understanding (ASRU), 2013 IEEE
Workshop on, pages 120–125. IEEE.
Peter Clark, Philip Harrison, and Xuchen Yao.
2012. An entailment-based approach to the
QA4MRE challenge. In CLEF (Online Working
Notes/Labs/Workshop).
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 948–956, Los Angeles, California,
June. Association for Computational Linguistics.
Dipanjan Das, Desai Chen, Andr´e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics, 40(1):9–56.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John
Prager, et al. 2010. Building Watson: An overview
of the DeepQA project. AI magazine, 31(3):59–79.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011–1019,
Los Angeles, California, June. Association for Com-
putational Linguistics.
M. Heilman. 2011. Automatic factual question gener-
ation from text. Ph.D. thesis, Carnegie Mellon Uni-
versity.
Lynette Hirschman, Marc Light, Eric Breck, and
John D Burger. 1999. Deep read: A read-
ing comprehension system. In Proceedings of the
37th annual meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
pages 325–332. Association for Computational Lin-
guistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Comput. Linguist., 39(4):885–916, December.
D. C. Liu and J. Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Math. Programming, 45:503–528.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their composition-
ality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.
</reference>
<page confidence="0.980863">
705
</page>
<reference confidence="0.999767666666666">
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 776–783, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Karthik Narasimhan and Regina Barzilay. 2015. Ma-
chine comprehension with discourse relations. In
53rd Annual Meeting of the Association for Com-
putational Linguistics.
Jorge Nocedal. 1980. Updating quasi-newton matrices
with limited storage. Mathematics of Computation,
35:773–782.
Anselmo Pe˜nas, Eduard H Hovy, Pamela Forner,
´Alvaro Rodrigo, Richard Sutcliffe, Corina Forascu,
and Caroline Sporleder. 2011. Overview of
QA4MRE at CLEF 2011: Question answering for
machine reading evaluation. In CLEF (Notebook
Papers/Labs/Workshop), pages 1–20.
Anselmo Pe˜nas, Eduard Hovy, Pamela Forner, ´Alvaro
Rodrigo, Richard Sutcliffe, and Roser Morante.
2013. QA4MRE 2011-2013: Overview of ques-
tion answering for machine reading evaluation.
In Information Access Evaluation. Multilinguality,
Multimodality, and Visualization, pages 303–320.
Springer.
Matthew Richardson, Christopher J.C. Burges, and
Erin Renshaw. 2013. MCTest: A challenge dataset
for the open-domain machine comprehension of
text. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 193–203, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Mrinmaya Sachan, Avinava Dubey, Eric P. Xing, and
Matthew Richardson. 2015. Learning answer-
entailing structures for machine comprehension. In
53rd Annual Meeting of the Association for Compu-
tational Linguistics.
S. Sukhbaatar, A. Szlam, J. Weston, and R. Fer-
gus. 2015. Weakly supervised memory networks.
March.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ’10, pages 384–394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
is the Jeopardy model? a quasi-synchronous gram-
mar for QA. In Proc. of EMNLP-CoNLL.
Jason Weston, Antoine Bordes, Sumit Chopra, and
Tomas Mikolov. 2015. Towards ai-complete ques-
tion answering: A set of prerequisite toy tasks.
April.
X. Yao, J. Berant, and B. Van Durme. 2014. Freebase
QA: Information extraction or semantic parsing? In
Workshop on Semantic Parsing.
</reference>
<page confidence="0.998556">
706
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343911">
<title confidence="0.998705">Machine Comprehension with Syntax, Frames, and Semantics</title>
<author confidence="0.950299">Hai Wang Mohit Bansal Kevin Gimpel David</author>
<address confidence="0.421436">Toyota Technological Institute at Chicago, Chicago, IL, 60637,</address>
<abstract confidence="0.992938714285714">We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>86--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5238" citStr="Baker et al., 1998" startWordPosition="841" endWordPosition="844"> between the bag of words constructed from the question/answer and the bag of words in the window. We call this feature B. The second feature corresponds to their word distance baseline, and is the minimal distance between two word occurrences in the passage that are also contained in the question/answer pair. We call this feature D. Space does not permit a detailed description. 3.1 Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet (Baker et al., 1998). This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three frames are identified. The target words pulled, all, and shelves have respective frame labels CAUSE MOTION, QUANTITY, and NATUFigure 1: Example output from SEMAFOR. RAL FEA</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>809--815</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="11115" citStr="Bansal et al., 2014" startWordPosition="1961" endWordPosition="1964">dependencies from the following three categories: (1) v = r and u = a; (2) v = r but u =�a; and (3) v =� r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector f+w as the vector summation of all words inside sentence w and f×w as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concatenate q and a, then calculate f+qa and f×qa. For the bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use cos(f+qa, f+w ) and cos(f×qa, f×w ) as features, where cos is cosine similarity. For syntactic features, where Tw is the bag of dependencies of w and Tqa is the bag </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 809–815, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="829" citStr="Berant et al., 2013" startWordPosition="109" endWordPosition="112">r}@ttic.edu Abstract We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic te</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pinaki Bhaskar</author>
<author>Partha Pakray</author>
<author>Somnath Banerjee</author>
<author>Samadrita Banerjee</author>
<author>Sivaji Bandyopadhyay</author>
<author>Alexander F Gelbukh</author>
</authors>
<title>Question answering system for QA4MRE@CLEF 2012. In</title>
<date>2012</date>
<journal>CLEF (Online Working Notes/Labs/Workshop).</journal>
<contexts>
<context position="2189" citStr="Bhaskar et al., 2012" startWordPosition="325" endWordPosition="328">soning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Breck et al., 2001), and the QA4MRE textual question answering tasks in the CLEF evaluations (Pe˜nas et al., 2011; Pe˜nas et al., 2013; Clark et al., 2012; Bhaskar et al., 2012). We consider the Machine Comprehension of Text dataset (MCTest; Richardson et al., 2013), a set of human-authored fictional stories with associated multiple-choice questions. Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper analysis. In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving performance on MCTest. Syntax,</context>
</contexts>
<marker>Bhaskar, Pakray, Banerjee, Banerjee, Bandyopadhyay, Gelbukh, 2012</marker>
<rawString>Pinaki Bhaskar, Partha Pakray, Somnath Banerjee, Samadrita Banerjee, Sivaji Bandyopadhyay, and Alexander F Gelbukh. 2012. Question answering system for QA4MRE@CLEF 2012. In CLEF (Online Working Notes/Labs/Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="964" citStr="Bollacker et al., 2008" startWordPosition="130" endWordPosition="133">enting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroom environment for evaluating QA systems, and has spurred development of powerful neural architectures for complex</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Jason Weston</author>
</authors>
<title>Question answering with subgraph embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>615--620</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="851" citStr="Bordes et al., 2014" startWordPosition="113" endWordPosition="116">We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroo</context>
</contexts>
<marker>Bordes, Chopra, Weston, 2014</marker>
<rawString>Antoine Bordes, Sumit Chopra, and Jason Weston. 2014. Question answering with subgraph embeddings. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Marc Light</author>
<author>Gideon S Mann</author>
<author>Ellen Riloff</author>
<author>Brianne Brown</author>
<author>Pranav Anand</author>
<author>Mats Rooth</author>
<author>Michael Thelen</author>
</authors>
<title>Looking under the hood: Tools for diagnosing your question answering engine.</title>
<date>2001</date>
<booktitle>In Proceedings of the workshop on Opendomain question answering-Volume 12,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2031" citStr="Breck et al., 2001" startWordPosition="298" endWordPosition="301">15). Synthetic text provides a cleanroom environment for evaluating QA systems, and has spurred development of powerful neural architectures for complex reasoning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Breck et al., 2001), and the QA4MRE textual question answering tasks in the CLEF evaluations (Pe˜nas et al., 2011; Pe˜nas et al., 2013; Clark et al., 2012; Bhaskar et al., 2012). We consider the Machine Comprehension of Text dataset (MCTest; Richardson et al., 2013), a set of human-authored fictional stories with associated multiple-choice questions. Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper an</context>
</contexts>
<marker>Breck, Light, Mann, Riloff, Brown, Anand, Rooth, Thelen, 2001</marker>
<rawString>Eric Breck, Marc Light, Gideon S Mann, Ellen Riloff, Brianne Brown, Pranav Anand, Mats Rooth, and Michael Thelen. 2001. Looking under the hood: Tools for diagnosing your question answering engine. In Proceedings of the workshop on Opendomain question answering-Volume 12, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="13308" citStr="Chen and Manning, 2014" startWordPosition="2337" endWordPosition="2341">t sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer A and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts &apos;&apos; and &apos; to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The accuracy of different feature sets on DEV is given in Table 1.6 The boldface results correspond </context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Nung Chen</author>
<author>William Yang Wang</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing.</title>
<date>2013</date>
<booktitle>In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on,</booktitle>
<pages>120--125</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6102" citStr="Chen et al., 2013" startWordPosition="988" endWordPosition="991"> arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three frames are identified. The target words pulled, all, and shelves have respective frame labels CAUSE MOTION, QUANTITY, and NATUFigure 1: Example output from SEMAFOR. RAL FEATURES. Each frame has its own set of arguments; e.g., the CAUSE MOTION frame has the labeled Agent, Theme, and Goal arguments. Features from these parses have been shown to be useful for NLP tasks such as slot filling in spoken dialogue systems (Chen et al., 2013). We expect that the passage sentence containing the answer will overlap with the question and correct answer in terms of predicates, frames evoked, and predicted argument labels, and we design features to capture this intuition. Given the frame semantic parse for a sentence, let T be the bag of frame-evoking target words/phrases.1 We define the bag of frame labels in the parse as F. For each target t ∈ T, there is an associated frame label denoted Ft ∈ F. Let R be the bag of phrases assigned with an argument label in the parse. We denote the bag of argument labels in the parse by L. For each </context>
</contexts>
<marker>Chen, Wang, Rudnicky, 2013</marker>
<rawString>Yun-Nung Chen, William Yang Wang, and Alexander I Rudnicky. 2013. Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 120–125. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Philip Harrison</author>
<author>Xuchen Yao</author>
</authors>
<title>An entailment-based approach to the QA4MRE challenge.</title>
<date>2012</date>
<booktitle>In CLEF (Online Working Notes/Labs/Workshop).</booktitle>
<contexts>
<context position="2166" citStr="Clark et al., 2012" startWordPosition="321" endWordPosition="324">ures for complex reasoning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Breck et al., 2001), and the QA4MRE textual question answering tasks in the CLEF evaluations (Pe˜nas et al., 2011; Pe˜nas et al., 2013; Clark et al., 2012; Bhaskar et al., 2012). We consider the Machine Comprehension of Text dataset (MCTest; Richardson et al., 2013), a set of human-authored fictional stories with associated multiple-choice questions. Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper analysis. In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving perform</context>
</contexts>
<marker>Clark, Harrison, Yao, 2012</marker>
<rawString>Peter Clark, Philip Harrison, and Xuchen Yao. 2012. An entailment-based approach to the QA4MRE challenge. In CLEF (Online Working Notes/Labs/Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="11036" citStr="Collobert et al., 2011" startWordPosition="1948" endWordPosition="1951">ntence. We include three additional count features that each consider a subset of dependencies from the following three categories: (1) v = r and u = a; (2) v = r but u =�a; and (3) v =� r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector f+w as the vector summation of all words inside sentence w and f×w as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concatenate q and a, then calculate f+qa and f×qa. For the bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use cos(f+qa, f+w ) and cos(f×qa, f×w ) as features, where cos is cosine similarity. For s</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Nathan Schneider</author>
<author>Desai Chen</author>
<author>Noah A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>948--956</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="13195" citStr="Das et al., 2010" startWordPosition="2318" endWordPosition="2321"> them with the original features. 4 Experiments MCTest splits its stories into train, development, and test sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer A and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts &apos;&apos; and &apos; to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Noc</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith. 2010. Probabilistic frame-semantic parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 948–956, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>Andr´e F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="5070" citStr="Das et al., 2014" startWordPosition="817" endWordPosition="820">start with two features from Richardson et al. (2013). Our first feature corresponds to their sliding window similarity baseline, which measures weighted word overlap between the bag of words constructed from the question/answer and the bag of words in the window. We call this feature B. The second feature corresponds to their word distance baseline, and is the minimal distance between two word occurrences in the passage that are also contained in the question/answer pair. We call this feature D. Space does not permit a detailed description. 3.1 Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet (Baker et al., 1998). This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame. An example output of the SEMAFOR frame semantic parser (Das et al., 2014) is given in Figure 1. Three fram</context>
<context position="13214" citStr="Das et al., 2014" startWordPosition="2322" endWordPosition="2325">ginal features. 4 Experiments MCTest splits its stories into train, development, and test sets. The original MCtest DEV is too small, to choose the best feature set, we merged the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer A and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts &apos;&apos; and &apos; to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The ac</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, Andr´e F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic parsing. Computational Linguistics, 40(1):9–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Ferrucci</author>
<author>Eric Brown</author>
<author>Jennifer Chu-Carroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya A Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John Prager</author>
</authors>
<title>Building Watson: An overview of the DeepQA project.</title>
<date>2010</date>
<journal>AI magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="808" citStr="Ferrucci et al., 2010" startWordPosition="104" endWordPosition="108">ansal,kgimpel,mcallester}@ttic.edu Abstract We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension. 1 Introduction Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al.</context>
</contexts>
<marker>Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, 2010</marker>
<rawString>David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur, Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. 2010. Building Watson: An overview of the DeepQA project. AI magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>1011--1019</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="8239" citStr="Heilman and Smith, 2010" startWordPosition="1408" endWordPosition="1411"> is likely that they have some overlapping dependen1By bag, we mean here a set with possible replicates. min A||B||2 + θ �� n − max BT f (Pi, w, qi, ai) wEW i=1 aˆ = arg max aEA max wEW 701 Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS(u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS(r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. • c = what, POS(r) = NN, and arc(c, r) = nsubj. Replace c by a. • c = where, POS(r) = VB, and arc(c, r) = advmod. Delet</context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 1011–1019, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
</authors>
<title>Automatic factual question generation from text.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="9597" citStr="Heilman, 2011" startWordPosition="1697" endWordPosition="1698"> is, POS(r) = VBZ, and arc(c, r) = advmod. Delete c. Find r’s child u such that arc(u, r) = nsubj, move r to be right after u. Insert a after r. • c = who, POS(r) = NN, and arc(c, r) = nsubj. Replace c by a. • c = who, POS(r) E {VB, VBD}, and arc(c, r) = nsubj. Replace c by a. We use other rules in addition to those above: change “why x?” to “the reason x is a”, and change “how many x”, “how much x”, or “when x” to “x a”. Given each candidate answer, we attempt to transform the question to a statement using the 2There are existing rule-based approaches to transforming statements to questions (Heilman, 2011); our rules reverse this process. rules above.3 An example of the transformation is given in Figure 2. In the parse, pull is the root word and What is attached as a dobj. This matches the first rule, so we delete did and insert the candidate answer pudding after pull, making the final transformed sentence: James pull pudding off. After this transformation of the question (and a candidate answer) to a statement, we measure its similarity to the sentence in the window using simple dependency-based similarity features. Denoting a dependency as (u, v, arc(u, v)), then two dependencies (u1, v1, arc</context>
</contexts>
<marker>Heilman, 2011</marker>
<rawString>M. Heilman. 2011. Automatic factual question generation from text. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Marc Light</author>
<author>Eric Breck</author>
<author>John D Burger</author>
</authors>
<title>Deep read: A reading comprehension system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>325--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2000" citStr="Hirschman et al., 1999" startWordPosition="293" endWordPosition="296">et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroom environment for evaluating QA systems, and has spurred development of powerful neural architectures for complex reasoning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Breck et al., 2001), and the QA4MRE textual question answering tasks in the CLEF evaluations (Pe˜nas et al., 2011; Pe˜nas et al., 2013; Clark et al., 2012; Bhaskar et al., 2012). We consider the Machine Comprehension of Text dataset (MCTest; Richardson et al., 2013), a set of human-authored fictional stories with associated multiple-choice questions. Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remain</context>
</contexts>
<marker>Hirschman, Light, Breck, Burger, 1999</marker>
<rawString>Lynette Hirschman, Marc Light, Eric Breck, and John D Burger. 1999. Deep read: A reading comprehension system. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 325–332. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Comput. Linguist.,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="13418" citStr="Lee et al., 2013" startWordPosition="2355" endWordPosition="2358">ts in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV). We optimize the max-margin training criteria on TRAIN and use DEV to tune the regularizer A and choose the best feature set. We report final performance on the original two test sets (for comparability) from MCTest, named MC160 and MC500. We use SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts &apos;&apos; and &apos; to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The accuracy of different feature sets on DEV is given in Table 1.6 The boldface results correspond 4Similar to the original syntactic features (see end of Section 3.2), we also have 3 additional features for t</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Comput. Linguist., 39(4):885–916, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D C Liu</author>
<author>J Nocedal</author>
</authors>
<title>On the limited memory BFGS method for large scale optimization.</title>
<date>1989</date>
<journal>Math. Programming,</journal>
<pages>45--503</pages>
<contexts>
<context position="13806" citStr="Liu and Nocedal, 1989" startWordPosition="2422" endWordPosition="2425">t al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts &apos;&apos; and &apos; to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The accuracy of different feature sets on DEV is given in Table 1.6 The boldface results correspond 4Similar to the original syntactic features (see end of Section 3.2), we also have 3 additional features for the three subset categories. 5https://code.google.com/p/word2vec/ 6All accuracies are computed with tie-breaking partial credit (similar to previous work), i.e., if we have the same to the best feature set combination chosen by evaluating on DEV. In this case, the feature dimensionality is 29, which includes 4 bag-of-words features, 1 distance feature, 12 frame semantic features, and wi</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>D. C. Liu and J. Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45:503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="10808" citStr="Mikolov et al., 2013" startWordPosition="1911" endWordPosition="1914">1, v1, arc(u1, v1)) and (u2, v2, arc(u2, v2)) match if and only if u1 = u2, v1 = v2, and arc(u1, v1) = arc(u2, v2). One feature simply counts the number of dependency matches between the transformed question and the passage sentence. We include three additional count features that each consider a subset of dependencies from the following three categories: (1) v = r and u = a; (2) v = r but u =�a; and (3) v =� r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector f+w as the vector summation of all words inside sentence w and f×w as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concatenate q </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>776--783</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="8194" citStr="Moschitti et al., 2007" startWordPosition="1400" endWordPosition="1403"> sentences refer to the same event, then it is likely that they have some overlapping dependen1By bag, we mean here a set with possible replicates. min A||B||2 + θ �� n − max BT f (Pi, w, qi, ai) wEW i=1 aˆ = arg max aEA max wEW 701 Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS(u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS(r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. • c = what, POS(r) = NN, and arc(c, r) = nsubj. Replace c by a. • c = wher</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Narasimhan</author>
<author>Regina Barzilay</author>
</authors>
<title>Machine comprehension with discourse relations.</title>
<date>2015</date>
<booktitle>In 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3113" citStr="Narasimhan and Barzilay, 2015" startWordPosition="464" endWordPosition="467">ry. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper analysis. In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving performance on MCTest. Syntax, frame semantics, and coreference are essential for understanding who did what to whom. Word embeddings address variation in word choice between the stories and questions. Our added features achieve the best results we are aware of on this dataset, outperforming concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). 2 Model We use a simple latent-variable classifier trained with a max-margin criterion. Let P denote the passage, q denote the question of interest, and A denote the set of candidate answers for q, where each a E A denotes one candidate answer. We want to learn a function h : (P, q) → A that, given a passage and a question, outputs a legal a E A. We use a linear model for h that uses a latent variable w to identify the sentence in the passage in which the answer can be found. Let W denote the set of sentences within the 700 Proceedings of the 53rd Annual Meeting of the </context>
<context position="15552" citStr="Narasimhan and Barzilay, 2015" startWordPosition="2729" endWordPosition="2732">4.18 B+D+F+S 66.24 Bwc + D + Fc + Swc 69.87 Table 1: Accuracy on DEV. The final test results are shown in Table 2. We first compare to results from prior work (Richardson et al., 2013). Their first result uses a sliding window with the bag-of-words feature B described in Sec. 3; this system is called “Baseline 1” (B1). They then add the distance feature D, also described in Sec. 3. The combined system, which uses B and D, is called “Baseline 2” (B2). Their third result adds a rich textual entailment system to B2; it is referred to as B2+RTE.7 We also compare to concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). We report accuracies for all questions as well as separately for the two types: those that are answerable with a single sentence from the passage (“Single”) and those that require multiple sentences (“Multiple”). We see gains in accuracy of 6% absolute compared to the B2+RTE baseline and also outperform concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). Even though our system only explicitly uses a single sentence from the passage when choosing an answer, we improve baseline accuracy for both single-sentence and multiplesentence questio</context>
</contexts>
<marker>Narasimhan, Barzilay, 2015</marker>
<rawString>Karthik Narasimhan and Regina Barzilay. 2015. Machine comprehension with discourse relations. In 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating quasi-newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<pages>35--773</pages>
<contexts>
<context position="13782" citStr="Nocedal, 1980" startWordPosition="2420" endWordPosition="2421"> SEMAFOR (Das et al., 2010; Das et al., 2014) for frame semantic parsing and the latest Stanford dependency parser (Chen and Manning, 2014) as our dependency parser. We use the Stanford rule-based system for coreference resolution (Lee et al., 2013). We use the pretrained 300-dimensional word embeddings downloadable from the word2vec site.5 We denote the frame semantic features by F and the syntactic features by S. We use superscripts &apos;&apos; and &apos; to indicate the use of embeddings and coreference for a particular feature set. To minimize the loss, we use the miniFunc package in MATLAB with LBFGS (Nocedal, 1980; Liu and Nocedal, 1989). The accuracy of different feature sets on DEV is given in Table 1.6 The boldface results correspond 4Similar to the original syntactic features (see end of Section 3.2), we also have 3 additional features for the three subset categories. 5https://code.google.com/p/word2vec/ 6All accuracies are computed with tie-breaking partial credit (similar to previous work), i.e., if we have the same to the best feature set combination chosen by evaluating on DEV. In this case, the feature dimensionality is 29, which includes 4 bag-of-words features, 1 distance feature, 12 frame s</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Jorge Nocedal. 1980. Updating quasi-newton matrices with limited storage. Mathematics of Computation, 35:773–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Pe˜nas</author>
<author>Eduard H Hovy</author>
<author>Pamela Forner</author>
<author>´Alvaro Rodrigo</author>
<author>Richard Sutcliffe</author>
<author>Corina Forascu</author>
<author>Caroline Sporleder</author>
</authors>
<title>Overview of QA4MRE at CLEF 2011: Question answering for machine reading evaluation.</title>
<date>2011</date>
<booktitle>In CLEF (Notebook Papers/Labs/Workshop),</booktitle>
<pages>1--20</pages>
<marker>Pe˜nas, Hovy, Forner, Rodrigo, Sutcliffe, Forascu, Sporleder, 2011</marker>
<rawString>Anselmo Pe˜nas, Eduard H Hovy, Pamela Forner, ´Alvaro Rodrigo, Richard Sutcliffe, Corina Forascu, and Caroline Sporleder. 2011. Overview of QA4MRE at CLEF 2011: Question answering for machine reading evaluation. In CLEF (Notebook Papers/Labs/Workshop), pages 1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Pe˜nas</author>
<author>Eduard Hovy</author>
<author>Pamela Forner</author>
<author>´Alvaro Rodrigo</author>
<author>Richard Sutcliffe</author>
<author>Roser Morante</author>
</authors>
<title>QA4MRE 2011-2013: Overview of question answering for machine reading evaluation.</title>
<date>2013</date>
<booktitle>In Information Access Evaluation. Multilinguality, Multimodality, and Visualization,</booktitle>
<pages>303--320</pages>
<publisher>Springer.</publisher>
<marker>Pe˜nas, Hovy, Forner, Rodrigo, Sutcliffe, Morante, 2013</marker>
<rawString>Anselmo Pe˜nas, Eduard Hovy, Pamela Forner, ´Alvaro Rodrigo, Richard Sutcliffe, and Roser Morante. 2013. QA4MRE 2011-2013: Overview of question answering for machine reading evaluation. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization, pages 303–320. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Richardson</author>
<author>Christopher J C Burges</author>
<author>Erin Renshaw</author>
</authors>
<title>MCTest: A challenge dataset for the open-domain machine comprehension of text.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>193--203</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="2278" citStr="Richardson et al., 2013" startWordPosition="338" endWordPosition="341">he construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Breck et al., 2001), and the QA4MRE textual question answering tasks in the CLEF evaluations (Pe˜nas et al., 2011; Pe˜nas et al., 2013; Clark et al., 2012; Bhaskar et al., 2012). We consider the Machine Comprehension of Text dataset (MCTest; Richardson et al., 2013), a set of human-authored fictional stories with associated multiple-choice questions. Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper analysis. In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving performance on MCTest. Syntax, frame semantics, and coreference are essential for understanding who did what to whom. W</context>
<context position="4506" citStr="Richardson et al. (2013)" startWordPosition="724" endWordPosition="727">6-31, 2015. c�2015 Association for Computational Linguistics passage, where a particular w ∈ W denotes one sentence. Given a feature vector f(P, w, q, a) and a weight vector B with an entry for each feature, the prediction aˆ for a new P and q is given by: BTf(P, w, q, a) Given triples {hPi, qi, aii}ni=1, we minimize an E2-regularized max-margin loss function: + max max BT f (Pi, w&apos;, qi, a) + Δ(a, ai) aEA �&amp;EW 11 where A is the weight of the E2 term and Δ(a, ai) = 1 if a =6 ai and 0 otherwise. The latent variable w makes the loss function non-convex. 3 Features We start with two features from Richardson et al. (2013). Our first feature corresponds to their sliding window similarity baseline, which measures weighted word overlap between the bag of words constructed from the question/answer and the bag of words in the window. We call this feature B. The second feature corresponds to their word distance baseline, and is the minimal distance between two word occurrences in the passage that are also contained in the question/answer pair. We call this feature D. Space does not permit a detailed description. 3.1 Frame Semantic Features Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-</context>
<context position="15107" citStr="Richardson et al., 2013" startWordPosition="2647" endWordPosition="2651">t on DEV, we then evaluate our system on TEST. Negations: in preliminary experiments, we found that our system suffered with negation questions, so we developed a simple heuristic to deal with them. We identify a question as negation if it contains “not” or “n’t” and does not begin with “how” or “why”. If a question is identified as negation, we then negate the final score for each candidate answer. Features DEV Accuracy (%) B + D + F 64.18 B+D+F+S 66.24 Bwc + D + Fc + Swc 69.87 Table 1: Accuracy on DEV. The final test results are shown in Table 2. We first compare to results from prior work (Richardson et al., 2013). Their first result uses a sliding window with the bag-of-words feature B described in Sec. 3; this system is called “Baseline 1” (B1). They then add the distance feature D, also described in Sec. 3. The combined system, which uses B and D, is called “Baseline 2” (B2). Their third result adds a rich textual entailment system to B2; it is referred to as B2+RTE.7 We also compare to concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). We report accuracies for all questions as well as separately for the two types: those that are answerable with a single sentence fr</context>
<context position="16580" citStr="Richardson et al. (2013)" startWordPosition="2882" endWordPosition="2885">., 2015). Even though our system only explicitly uses a single sentence from the passage when choosing an answer, we improve baseline accuracy for both single-sentence and multiplesentence questions. 8 score for all four candidate answers, then we get partial credit of 0.25 for this question. 7These three results are obtained from files at http://research.microsoft.com/en-us/ um/redmond/projects/mctest/results.html. 8However, we inspected these question annotations and 703 System MC160 MC500 Single (112) Multiple (128) All Single (272) Multiple (328) All B1 64.73 56.64 60.41 58.21 56.17 57.09 Richardson et al. (2013) B2 75.89 60.15 67.50 64.00 57.46 60.43 B2+RTE 76.78 62.50 69.16 68.01 59.45 63.33 Narasimhan and Barzilay (2015) 82.36 65.23 73.23 68.38 59.90 63.75 Sachan et al. (2015) - - - 67.65 67.99 67.83 our system 84.22 67.85 75.27 72.05 67.94 69.94 Table 2: Accuracy comparison of published results on test sets. Features DEV Accuracy (%) full (Bwc+D+Fc+Swc) 69.87 − Bwc (D + Fc+Swc) 58.46 − D (Bwc+Fc+Swc) 65.89 − Bwc, − D (Fc+Swc) 54.19 − embeddings (Bc+D+Fc+Sc) 68.28 − coreference (Bw+D+F+Sw) 68.43 − frame semantics (Bwc+D+Swc) 67.89 − syntax (Bwc+D+Fc) 67.64 − negation (Bwc+D+Fc+Swc) 68.72 Table 3: A</context>
</contexts>
<marker>Richardson, Burges, Renshaw, 2013</marker>
<rawString>Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 193–203, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mrinmaya Sachan</author>
<author>Avinava Dubey</author>
<author>Eric P Xing</author>
<author>Matthew Richardson</author>
</authors>
<title>Learning answerentailing structures for machine comprehension.</title>
<date>2015</date>
<booktitle>In 53rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3135" citStr="Sachan et al., 2015" startWordPosition="468" endWordPosition="471">ented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper analysis. In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving performance on MCTest. Syntax, frame semantics, and coreference are essential for understanding who did what to whom. Word embeddings address variation in word choice between the stories and questions. Our added features achieve the best results we are aware of on this dataset, outperforming concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). 2 Model We use a simple latent-variable classifier trained with a max-margin criterion. Let P denote the passage, q denote the question of interest, and A denote the set of candidate answers for q, where each a E A denotes one candidate answer. We want to learn a function h : (P, q) → A that, given a passage and a question, outputs a legal a E A. We use a linear model for h that uses a latent variable w to identify the sentence in the passage in which the answer can be found. Let W denote the set of sentences within the 700 Proceedings of the 53rd Annual Meeting of the Association for Comput</context>
<context position="15574" citStr="Sachan et al., 2015" startWordPosition="2733" endWordPosition="2736"> + Swc 69.87 Table 1: Accuracy on DEV. The final test results are shown in Table 2. We first compare to results from prior work (Richardson et al., 2013). Their first result uses a sliding window with the bag-of-words feature B described in Sec. 3; this system is called “Baseline 1” (B1). They then add the distance feature D, also described in Sec. 3. The combined system, which uses B and D, is called “Baseline 2” (B2). Their third result adds a rich textual entailment system to B2; it is referred to as B2+RTE.7 We also compare to concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). We report accuracies for all questions as well as separately for the two types: those that are answerable with a single sentence from the passage (“Single”) and those that require multiple sentences (“Multiple”). We see gains in accuracy of 6% absolute compared to the B2+RTE baseline and also outperform concurrently-published results (Narasimhan and Barzilay, 2015; Sachan et al., 2015). Even though our system only explicitly uses a single sentence from the passage when choosing an answer, we improve baseline accuracy for both single-sentence and multiplesentence questions. 8 score for all fo</context>
</contexts>
<marker>Sachan, Dubey, Xing, Richardson, 2015</marker>
<rawString>Mrinmaya Sachan, Avinava Dubey, Eric P. Xing, and Matthew Richardson. 2015. Learning answerentailing structures for machine comprehension. In 53rd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sukhbaatar</author>
<author>A Szlam</author>
<author>J Weston</author>
<author>R Fergus</author>
</authors>
<title>Weakly supervised memory networks.</title>
<date>2015</date>
<contexts>
<context position="1415" citStr="Sukhbaatar et al., 2015" startWordPosition="205" endWordPosition="208">ucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroom environment for evaluating QA systems, and has spurred development of powerful neural architectures for complex reasoning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Bre</context>
<context position="11193" citStr="Sukhbaatar et al., 2015" startWordPosition="1975" endWordPosition="1978"> v = r but u =�a; and (3) v =� r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector f+w as the vector summation of all words inside sentence w and f×w as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concatenate q and a, then calculate f+qa and f×qa. For the bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use cos(f+qa, f+w ) and cos(f×qa, f×w ) as features, where cos is cosine similarity. For syntactic features, where Tw is the bag of dependencies of w and Tqa is the bag of dependencies for the transformed question for candidate answer a, we use a </context>
</contexts>
<marker>Sukhbaatar, Szlam, Weston, Fergus, 2015</marker>
<rawString>S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. 2015. Weakly supervised memory networks. March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11084" citStr="Turian et al., 2010" startWordPosition="1956" endWordPosition="1959">that each consider a subset of dependencies from the following three categories: (1) v = r and u = a; (2) v = r but u =�a; and (3) v =� r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector f+w as the vector summation of all words inside sentence w and f×w as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concatenate q and a, then calculate f+qa and f×qa. For the bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use cos(f+qa, f+w ) and cos(f×qa, f×w ) as features, where cos is cosine similarity. For syntactic features, where Tw is the bag of depend</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wang</author>
<author>N A Smith</author>
<author>T Mitamura</author>
</authors>
<title>What is the Jeopardy model? a quasi-synchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="8213" citStr="Wang et al., 2007" startWordPosition="1404" endWordPosition="1407">same event, then it is likely that they have some overlapping dependen1By bag, we mean here a set with possible replicates. min A||B||2 + θ �� n − max BT f (Pi, w, qi, ai) wEW i=1 aˆ = arg max aEA max wEW 701 Figure 2: Transforming the question to a statement. cies. To compare a Q/A pair to a sentence in the passage, we first use rules to transform the question into a statement and insert the candidate answer into the trace position. Our simple rule set is inspired by the rich history of QA research into modeling syntactic transformations between questions and answers (Moschitti et al., 2007; Wang et al., 2007; Heilman and Smith, 2010). Given Stanford dependency tree and part-of-speech (POS) tags for the question, let arc(u, v) be the label of the dependency between child word u and head word v, let POS(u) be the POS tag of u, let c be the wh-word in the question, let r be the root word in the question’s dependency tree, and let a be the candidate answer. We use the following rules:2 • c = what, POS(r) = VB, and arc(c, r) = dobj. Insert a after word u where arc(u, r) = nsubj. Delete c and the word after c. • c = what, POS(r) = NN, and arc(c, r) = nsubj. Replace c by a. • c = where, POS(r) = VB, and</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>M. Wang, N. A. Smith, and T. Mitamura. 2007. What is the Jeopardy model? a quasi-synchronous grammar for QA. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Antoine Bordes</author>
<author>Sumit Chopra</author>
<author>Tomas Mikolov</author>
</authors>
<title>Towards ai-complete question answering: A set of prerequisite toy tasks.</title>
<date>2015</date>
<contexts>
<context position="1389" citStr="Weston et al., 2015" startWordPosition="201" endWordPosition="204">ng (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable with shallow techniques from information extraction (Yao et al., 2014). There is also recent work on QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroom environment for evaluating QA systems, and has spurred development of powerful neural architectures for complex reasoning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression. In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et</context>
<context position="11167" citStr="Weston et al., 2015" startWordPosition="1971" endWordPosition="1974"> v = r and u = a; (2) v = r but u =�a; and (3) v =� r. In Figure 2, the triples (James, pull, nsubj) and (off, pull, prt) belong to the second category while (pudding, pull, dobj) belongs to the first. 3.3 Word Embeddings Word embeddings (Mikolov et al., 2013) represent each word as a low-dimensional vector where the similarity of vectors captures some aspect of semantic similarity of words. They have been used for many tasks, including semantic role labeling (Collobert et al., 2011), named entity recognition (Turian et al., 2010), parsing (Bansal et al., 2014), and for the Facebook QA tasks (Weston et al., 2015; Sukhbaatar et al., 2015). We first define the vector f+w as the vector summation of all words inside sentence w and f×w as the elementwise multiplication of the vectors in w. To define vectors for answer a for question q, we concatenate q and a, then calculate f+qa and f×qa. For the bag-of-words feature B, instead of merely counting matches of the two bags of words, we also use cos(f+qa, f+w ) and cos(f×qa, f×w ) as features, where cos is cosine similarity. For syntactic features, where Tw is the bag of dependencies of w and Tqa is the bag of dependencies for the transformed question for can</context>
</contexts>
<marker>Weston, Bordes, Chopra, Mikolov, 2015</marker>
<rawString>Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yao</author>
<author>J Berant</author>
<author>B Van Durme</author>
</authors>
<title>Freebase QA: Information extraction or semantic parsing?</title>
<date>2014</date>
<booktitle>In Workshop on Semantic Parsing.</booktitle>
<marker>Yao, Berant, Van Durme, 2014</marker>
<rawString>X. Yao, J. Berant, and B. Van Durme. 2014. Freebase QA: Information extraction or semantic parsing? In Workshop on Semantic Parsing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>