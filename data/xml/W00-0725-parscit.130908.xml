<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005676">
<note confidence="0.835254">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 123-125, Lisbon, Portugal, 2000.
</note>
<title confidence="0.849433">
A Comparison of PCFG Models*
</title>
<note confidence="0.620107">
Jose Luis Verdli-Mas and Jorge Calera-Rubio and Rafael C. Carrasco
Departament de Llenguatges i Sistemes Informatics
Universitat d&apos;Alacant, E-03071 Alacant (Spain)
</note>
<email confidence="0.97952">
{verdu, calera, carrasco}@dlsi.ua.es
</email>
<sectionHeader confidence="0.988777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99799025">
In this paper, we compare three different ap-
proaches to build a probabilistic context-free
grammar for natural language parsing from a
tree bank corpus: 1) a model that simply ex-
tracts the rules contained in the corpus and
counts the number of occurrences of each rule
2) a model that also stores information about
the parent node&apos;s category and, 3) a model that
estimates the probabilities according to a gen-
eralized k-gram scheme with k = 3. The last
one allows for a faster parsing and decreases the
perplexity of test samples.
</bodyText>
<sectionHeader confidence="0.995474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987085">
Recent work (Johnson, 1998) has explored the
performance of parsers based on a probabilistic
context-free grammar (PCFG) extracted from a
training corpus. The results show that the type
of tree representation used in the corpus can
have a substantial effect in the estimated like-
lihood of each sentence or parse tree. Accord-
ing to (Johnson, 1998), weaker independence as-
sumptions —such as decreasing the number of
nodes or increasing the number of node labels—
improve the efficiency of the parser. The best
results were obtained with parent-annotated la-
bels where each node stores contextual informa-
tion in the form of the category of the node&apos;s
parent. This fact is in agreement with the
observation put forward by Charniak (Char-
niak, 1996) that simple PCFGs, directly ob-
tained from a corpus, largely overgeneralize.
This property suggests that, in these models,
a large probability mass is assigned to incorrect
</bodyText>
<footnote confidence="0.83721">
* Work partially supported by the Spanish CICYT un-
der grant TIC97-0941.
</footnote>
<bodyText confidence="0.988680714285714">
parses and, therefore, any procedure that con-
centrates the probability on the correct parses
will increase the likelihood of the samples.
In this spirit, we introduce a generalization of
the classic k-gram models, widely used for string
processing (Brown et al., 1992; Ney et al., 1995),
to the case of trees. The PCFG obtained in this
way consists of rules that include information
about the context where the rule is applied.
The experiments were performed using the
Wall Street Journal (WSJ) corpus of the Uni-
versity of Pennsylvania (Marcus et al., 1993)
modified as described in (Charniak, 1996)
and (Johnson, 1998).
</bodyText>
<sectionHeader confidence="0.665379" genericHeader="method">
2 A generalized k-gram model
</sectionHeader>
<bodyText confidence="0.984565">
Recall that k-gram models are stochastic mod-
els for the generation of sequences 81,82, •••
based on conditional probabilities, that is:
</bodyText>
<listItem confidence="0.887493333333333">
1. the probability P(sis2 . . . stIM) of a se-
quence in the model M is computed as a
product
</listItem>
<equation confidence="0.968195">
pm(si)Pm(s2Is1) • • •pm(stIs1s2... St-1),
</equation>
<bodyText confidence="0.973113">
and
2. the dependence of the probabilities pm
on previous history is assumed to be re-
stricted to the immediate preceding con-
text, in particular, the last k - 1 words:
Pm(stisi • • • St-i) = Pm(stist-k+1 - - - St-i)•
Note that in this kind of models, the probability
that the observation st is generated at time t is
computed as a function of the subsequence of
length k - 1 that immediately precedes st (this
is called a state). However, in the case of trees,
it is not obvious what context should be taken in
to account. Indeed, there is a natural preference
when processing strings (the usual left-to-right
</bodyText>
<page confidence="0.990238">
123
</page>
<figure confidence="0.79091">
VP
</figure>
<figureCaption confidence="0.999908">
Figure 1: A sample parse tree of depth 3.
</figureCaption>
<bodyText confidence="0.998895352941177">
order) but there are at least two standard ways
of processing trees: ascending (or bottom-up)
analysis and descending (or top-down) analysis.
Ascending tree automata recognize a wider class
of languages (Nivat and Podelski, 1997; Gecseg
and Steinby, 1984) and, therefore, they allow for
richer descriptions.
Thus, our model will compute the expansion
probability for a given node as a function of the
subtree of depth k —2 that the node generatesl,
i.e., every state stores a subtree of depth k — 2.
In the particular case k = 2, only the label of the
node is taken into account (this is analogous to
the standard bigram model for strings) and the
model coincides with the simple rule-counting
approach. For instance, for the tree depicted in
Fig. 1, the following rules are obtained:
</bodyText>
<equation confidence="0.724298666666667">
VP—* V NP PP
NP Det N
PP PNP
</equation>
<bodyText confidence="0.999929">
However, in case k = 3, the expansion proba-
bilities depend on the states that are defined by
the node label, the number of descendents the
node and the sequence of labels in the descen-
dents (if any). Therefore, for the same tree the
following rules are obtained in this case:
</bodyText>
<equation confidence="0.901106333333333">
VP(V NP PP) —&gt; V NP(Det N) PP(P NP)
NP(Det N) Det N
PP (P NP) P NP(Det N)
</equation>
<bodyText confidence="0.9921153">
where each state has the form X(Zi
This is equivalent to a relabeling of the parse
tree before extracting the rules.
Finally, in the parent annotated model (PA)
described in (Johnson, 1998) the states depend
&apos;Note that in our notation a single node tree has
depth 0. This is in contrast to strings, where a single
symbol has length 1.
on both the node label and the node&apos;s parent
label:
</bodyText>
<equation confidence="0.92144875">
VPS V NPvP PPvP
NPvP Det N
PPvP P NPPP
NP PP Det N
</equation>
<bodyText confidence="0.999743846153846">
It is obvious that the k = 3 and PA models
incorporate contextual information that is not
present in the case k = 2 and, then, a higher
number of rules for a fixed number of categories
is possible. In practice, due to the finite size
of the training corpus, the number of rules is
always moderate. However, as higher values of
k lead to an enormous number of possible rules,
huge data sets would be necessary in order to
have a reliable estimate of the probabilities for
values above k = 3. A detailed mathematical
description of these type of models can be found
in (Rico-Juan et al., 2000)
</bodyText>
<sectionHeader confidence="0.998185" genericHeader="method">
3 Experimental results
</sectionHeader>
<bodyText confidence="0.9261305">
The following table shows some data obtained
with the three different models and the WSJ
corpus. The second column contains the num-
ber of rules in the grammar obtained from a
training subset of the corpus (24500 sentences,
about the first half in the corpus) and the last
one contains the percentage of sentences in a
test set (2000 sentences) that cannot be parsed
by the grammar.
Model number of rules % unparsed sent.
k = 2 11377 0
k = 3 64892 24
PA 18022 0.2
As expected, the number of rules obtained in-
creases as more information is conveyed by the
node label, although this increase is not ex-
treme. On the other hand, as the generaliza-
tion power decreases, some sentences in the test
set become unparsable, that is, they cannot be
generated by the grammar. The number of un-
parsed sentences is very small for the parent an-
notated model but cannot be neglected for the
k = 3 model.
As we will use the perplexity of a test sample
S = {w1, wisi} as an indication of the quality
of the model,
</bodyText>
<equation confidence="0.701428">
Is&apos;
1
PP = — log2p(wk
isl k=1
NP
Det N
PP
P NP
Det N
M)
</equation>
<page confidence="0.994853">
124
</page>
<bodyText confidence="0.946782428571429">
, unparsable sentences would produce an infinite
perplexity. Therefore, we studied the perplex-
ity of the test set for a linear combination of
two models Mi and Mi with P(wklMi
AP(wkiMi) ± (1 — A)P(wklMj). The mixing pa-
rameter A was chosen in order to minimize the
perplexity. Figure 2 shows that there is always
</bodyText>
<figureCaption confidence="0.94064">
Figure 2: Test set perplexity as a function of the
mixture parameter A. Upper line: k = 2 and PA.
Lower line: k = 2 and k = 3.
</figureCaption>
<bodyText confidence="0.987097461538462">
a minimum perplexity for an intermediate value
of A. The best results were obtained with a mix-
ture of the k-gram models for k = 2 and k = 3
with a heavier component (73%) of the last one.
The minimum perplexity PPm and the corre-
sponding value of A obtained are shown in the
following table:
Mixture model PPm Am
k = 2 and PA 107.9 0.58
k = 2 and k = 3 91.0 0.27
It is also worth to remark that the model k = 3
is the less ambiguous model and, then, parsing
of sentences becomes much faster.
</bodyText>
<sectionHeader confidence="0.998708" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999912909090909">
We have investigated the applicability of a
PCFG model based on the extension of k-gram
models described in (Rico-Juan et al., 2000).
The perplexity of the test sample decreases
when a combination of models with k = 2 and
k = 3 is used to predict string probabilities. We
are at present checking that the behavior also
holds for other quality measures as the precision
and recall of parses of sentences that express
strong equivalence between the model and the
data.
</bodyText>
<sectionHeader confidence="0.968823" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999519068965517">
Peter F. Brown, Vincent J. Della Pietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479.
Eugene Charniak. 1996. Tree-bank grammars. In
Proceedings of the Thirteenth National Confer-
ence on Artificial Intelligence and the Eighth
Innovative Applications of Artificial Intelligence
Conference, pages 1031-1036, Menlo Park. AAAI
Press/MIT Press.
Ferenc Gecseg and Magnus Steinby. 1984. Tree Au-
tomata. Akademiai Kiado, Budapest.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613-632.
Mitchell P. Marcus, Beatrice Santorini, and Mary
Ann Marcinkiewicz. 1993. Building a large anno-
tated corpus of english: the penn treebank. Com-
putational Linguistics, 19:313-330.
H. Ney, U. Essen, and R. Kneser. 1995. On the esti-
mation of small probabilities by leaving-one-out.
IEEE Trans. on Pattern Analysis and Machine
Intelligence, 17(12)1202-1212.
Maurice Nivat and Andreas Podelski. 1997. Min-
imal ascending and descending tree automata.
SIAM Journal on Computing, 26(1):39-58.
Juan Ram6n Rico-Juan, Jorge Calera-Rubio, and
Rafael C. Carrasco. 2000. Probabilistic k-testable
tree language. In ICGI-2000, to appear.
</reference>
<page confidence="0.998488">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584332">
<note confidence="0.891555">of CoNLL-2000 and LLL-2000, 123-125, Lisbon, Portugal, 2000.</note>
<title confidence="0.990525">A Comparison of PCFG Models*</title>
<author confidence="0.997877">Jose Luis Verdli-Mas</author>
<author confidence="0.997877">Jorge Calera-Rubio</author>
<author confidence="0.997877">C Rafael</author>
<affiliation confidence="0.879286">Departament de Llenguatges i Sistemes Universitat d&apos;Alacant, E-03071 Alacant</affiliation>
<email confidence="0.889793">verdu@dlsi.ua.es</email>
<email confidence="0.889793">calera@dlsi.ua.es</email>
<email confidence="0.889793">carrasco@dlsi.ua.es</email>
<abstract confidence="0.999814307692308">In this paper, we compare three different approaches to build a probabilistic context-free grammar for natural language parsing from a tree bank corpus: 1) a model that simply extracts the rules contained in the corpus and counts the number of occurrences of each rule 2) a model that also stores information about the parent node&apos;s category and, 3) a model that estimates the probabilities according to a genk-gram scheme with = The last one allows for a faster parsing and decreases the perplexity of test samples.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="2122" citStr="Brown et al., 1992" startWordPosition="332" endWordPosition="335">the node&apos;s parent. This fact is in agreement with the observation put forward by Charniak (Charniak, 1996) that simple PCFGs, directly obtained from a corpus, largely overgeneralize. This property suggests that, in these models, a large probability mass is assigned to incorrect * Work partially supported by the Spanish CICYT under grant TIC97-0941. parses and, therefore, any procedure that concentrates the probability on the correct parses will increase the likelihood of the samples. In this spirit, we introduce a generalization of the classic k-gram models, widely used for string processing (Brown et al., 1992; Ney et al., 1995), to the case of trees. The PCFG obtained in this way consists of rules that include information about the context where the rule is applied. The experiments were performed using the Wall Street Journal (WSJ) corpus of the University of Pennsylvania (Marcus et al., 1993) modified as described in (Charniak, 1996) and (Johnson, 1998). 2 A generalized k-gram model Recall that k-gram models are stochastic models for the generation of sequences 81,82, ••• based on conditional probabilities, that is: 1. the probability P(sis2 . . . stIM) of a sequence in the model M is computed as</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference,</booktitle>
<pages>1031--1036</pages>
<publisher>AAAI Press/MIT Press.</publisher>
<location>Menlo Park.</location>
<contexts>
<context position="1610" citStr="Charniak, 1996" startWordPosition="253" endWordPosition="255">ted from a training corpus. The results show that the type of tree representation used in the corpus can have a substantial effect in the estimated likelihood of each sentence or parse tree. According to (Johnson, 1998), weaker independence assumptions —such as decreasing the number of nodes or increasing the number of node labels— improve the efficiency of the parser. The best results were obtained with parent-annotated labels where each node stores contextual information in the form of the category of the node&apos;s parent. This fact is in agreement with the observation put forward by Charniak (Charniak, 1996) that simple PCFGs, directly obtained from a corpus, largely overgeneralize. This property suggests that, in these models, a large probability mass is assigned to incorrect * Work partially supported by the Spanish CICYT under grant TIC97-0941. parses and, therefore, any procedure that concentrates the probability on the correct parses will increase the likelihood of the samples. In this spirit, we introduce a generalization of the classic k-gram models, widely used for string processing (Brown et al., 1992; Ney et al., 1995), to the case of trees. The PCFG obtained in this way consists of rul</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Eugene Charniak. 1996. Tree-bank grammars. In Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference, pages 1031-1036, Menlo Park. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc Gecseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree Automata. Akademiai Kiado,</title>
<date>1984</date>
<location>Budapest.</location>
<contexts>
<context position="3688" citStr="Gecseg and Steinby, 1984" startWordPosition="604" endWordPosition="607">t is generated at time t is computed as a function of the subsequence of length k - 1 that immediately precedes st (this is called a state). However, in the case of trees, it is not obvious what context should be taken in to account. Indeed, there is a natural preference when processing strings (the usual left-to-right 123 VP Figure 1: A sample parse tree of depth 3. order) but there are at least two standard ways of processing trees: ascending (or bottom-up) analysis and descending (or top-down) analysis. Ascending tree automata recognize a wider class of languages (Nivat and Podelski, 1997; Gecseg and Steinby, 1984) and, therefore, they allow for richer descriptions. Thus, our model will compute the expansion probability for a given node as a function of the subtree of depth k —2 that the node generatesl, i.e., every state stores a subtree of depth k — 2. In the particular case k = 2, only the label of the node is taken into account (this is analogous to the standard bigram model for strings) and the model coincides with the simple rule-counting approach. For instance, for the tree depicted in Fig. 1, the following rules are obtained: VP—* V NP PP NP Det N PP PNP However, in case k = 3, the expansion pro</context>
</contexts>
<marker>Gecseg, Steinby, 1984</marker>
<rawString>Ferenc Gecseg and Magnus Steinby. 1984. Tree Automata. Akademiai Kiado, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4</pages>
<contexts>
<context position="895" citStr="Johnson, 1998" startWordPosition="137" endWordPosition="138">u, calera, carrasco}@dlsi.ua.es Abstract In this paper, we compare three different approaches to build a probabilistic context-free grammar for natural language parsing from a tree bank corpus: 1) a model that simply extracts the rules contained in the corpus and counts the number of occurrences of each rule 2) a model that also stores information about the parent node&apos;s category and, 3) a model that estimates the probabilities according to a generalized k-gram scheme with k = 3. The last one allows for a faster parsing and decreases the perplexity of test samples. 1 Introduction Recent work (Johnson, 1998) has explored the performance of parsers based on a probabilistic context-free grammar (PCFG) extracted from a training corpus. The results show that the type of tree representation used in the corpus can have a substantial effect in the estimated likelihood of each sentence or parse tree. According to (Johnson, 1998), weaker independence assumptions —such as decreasing the number of nodes or increasing the number of node labels— improve the efficiency of the parser. The best results were obtained with parent-annotated labels where each node stores contextual information in the form of the cat</context>
<context position="2474" citStr="Johnson, 1998" startWordPosition="393" endWordPosition="394">s and, therefore, any procedure that concentrates the probability on the correct parses will increase the likelihood of the samples. In this spirit, we introduce a generalization of the classic k-gram models, widely used for string processing (Brown et al., 1992; Ney et al., 1995), to the case of trees. The PCFG obtained in this way consists of rules that include information about the context where the rule is applied. The experiments were performed using the Wall Street Journal (WSJ) corpus of the University of Pennsylvania (Marcus et al., 1993) modified as described in (Charniak, 1996) and (Johnson, 1998). 2 A generalized k-gram model Recall that k-gram models are stochastic models for the generation of sequences 81,82, ••• based on conditional probabilities, that is: 1. the probability P(sis2 . . . stIM) of a sequence in the model M is computed as a product pm(si)Pm(s2Is1) • • •pm(stIs1s2... St-1), and 2. the dependence of the probabilities pm on previous history is assumed to be restricted to the immediate preceding context, in particular, the last k - 1 words: Pm(stisi • • • St-i) = Pm(stist-k+1 - - - St-i)• Note that in this kind of models, the probability that the observation st is genera</context>
<context position="4786" citStr="Johnson, 1998" startWordPosition="811" endWordPosition="812"> in Fig. 1, the following rules are obtained: VP—* V NP PP NP Det N PP PNP However, in case k = 3, the expansion probabilities depend on the states that are defined by the node label, the number of descendents the node and the sequence of labels in the descendents (if any). Therefore, for the same tree the following rules are obtained in this case: VP(V NP PP) —&gt; V NP(Det N) PP(P NP) NP(Det N) Det N PP (P NP) P NP(Det N) where each state has the form X(Zi This is equivalent to a relabeling of the parse tree before extracting the rules. Finally, in the parent annotated model (PA) described in (Johnson, 1998) the states depend &apos;Note that in our notation a single node tree has depth 0. This is in contrast to strings, where a single symbol has length 1. on both the node label and the node&apos;s parent label: VPS V NPvP PPvP NPvP Det N PPvP P NPPP NP PP Det N It is obvious that the k = 3 and PA models incorporate contextual information that is not present in the case k = 2 and, then, a higher number of rules for a fixed number of categories is possible. In practice, due to the finite size of the training corpus, the number of rules is always moderate. However, as higher values of k lead to an enormous nu</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--313</pages>
<contexts>
<context position="2412" citStr="Marcus et al., 1993" startWordPosition="382" endWordPosition="385">rtially supported by the Spanish CICYT under grant TIC97-0941. parses and, therefore, any procedure that concentrates the probability on the correct parses will increase the likelihood of the samples. In this spirit, we introduce a generalization of the classic k-gram models, widely used for string processing (Brown et al., 1992; Ney et al., 1995), to the case of trees. The PCFG obtained in this way consists of rules that include information about the context where the rule is applied. The experiments were performed using the Wall Street Journal (WSJ) corpus of the University of Pennsylvania (Marcus et al., 1993) modified as described in (Charniak, 1996) and (Johnson, 1998). 2 A generalized k-gram model Recall that k-gram models are stochastic models for the generation of sequences 81,82, ••• based on conditional probabilities, that is: 1. the probability P(sis2 . . . stIM) of a sequence in the model M is computed as a product pm(si)Pm(s2Is1) • • •pm(stIs1s2... St-1), and 2. the dependence of the probabilities pm on previous history is assumed to be restricted to the immediate preceding context, in particular, the last k - 1 words: Pm(stisi • • • St-i) = Pm(stist-k+1 - - - St-i)• Note that in this kin</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: the penn treebank. Computational Linguistics, 19:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<title>On the estimation of small probabilities by leaving-one-out.</title>
<date>1995</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence,</journal>
<pages>17--12</pages>
<contexts>
<context position="2141" citStr="Ney et al., 1995" startWordPosition="336" endWordPosition="339">his fact is in agreement with the observation put forward by Charniak (Charniak, 1996) that simple PCFGs, directly obtained from a corpus, largely overgeneralize. This property suggests that, in these models, a large probability mass is assigned to incorrect * Work partially supported by the Spanish CICYT under grant TIC97-0941. parses and, therefore, any procedure that concentrates the probability on the correct parses will increase the likelihood of the samples. In this spirit, we introduce a generalization of the classic k-gram models, widely used for string processing (Brown et al., 1992; Ney et al., 1995), to the case of trees. The PCFG obtained in this way consists of rules that include information about the context where the rule is applied. The experiments were performed using the Wall Street Journal (WSJ) corpus of the University of Pennsylvania (Marcus et al., 1993) modified as described in (Charniak, 1996) and (Johnson, 1998). 2 A generalized k-gram model Recall that k-gram models are stochastic models for the generation of sequences 81,82, ••• based on conditional probabilities, that is: 1. the probability P(sis2 . . . stIM) of a sequence in the model M is computed as a product pm(si)Pm</context>
</contexts>
<marker>Ney, Essen, Kneser, 1995</marker>
<rawString>H. Ney, U. Essen, and R. Kneser. 1995. On the estimation of small probabilities by leaving-one-out. IEEE Trans. on Pattern Analysis and Machine Intelligence, 17(12)1202-1212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Nivat</author>
<author>Andreas Podelski</author>
</authors>
<title>Minimal ascending and descending tree automata.</title>
<date>1997</date>
<journal>SIAM Journal on Computing,</journal>
<pages>26--1</pages>
<contexts>
<context position="3661" citStr="Nivat and Podelski, 1997" startWordPosition="600" endWordPosition="603">ity that the observation st is generated at time t is computed as a function of the subsequence of length k - 1 that immediately precedes st (this is called a state). However, in the case of trees, it is not obvious what context should be taken in to account. Indeed, there is a natural preference when processing strings (the usual left-to-right 123 VP Figure 1: A sample parse tree of depth 3. order) but there are at least two standard ways of processing trees: ascending (or bottom-up) analysis and descending (or top-down) analysis. Ascending tree automata recognize a wider class of languages (Nivat and Podelski, 1997; Gecseg and Steinby, 1984) and, therefore, they allow for richer descriptions. Thus, our model will compute the expansion probability for a given node as a function of the subtree of depth k —2 that the node generatesl, i.e., every state stores a subtree of depth k — 2. In the particular case k = 2, only the label of the node is taken into account (this is analogous to the standard bigram model for strings) and the model coincides with the simple rule-counting approach. For instance, for the tree depicted in Fig. 1, the following rules are obtained: VP—* V NP PP NP Det N PP PNP However, in ca</context>
</contexts>
<marker>Nivat, Podelski, 1997</marker>
<rawString>Maurice Nivat and Andreas Podelski. 1997. Minimal ascending and descending tree automata. SIAM Journal on Computing, 26(1):39-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Ram6n Rico-Juan</author>
<author>Jorge Calera-Rubio</author>
<author>Rafael C Carrasco</author>
</authors>
<title>Probabilistic k-testable tree language. In</title>
<date>2000</date>
<note>ICGI-2000, to appear.</note>
<contexts>
<context position="5626" citStr="Rico-Juan et al., 2000" startWordPosition="971" endWordPosition="974">P Det N PPvP P NPPP NP PP Det N It is obvious that the k = 3 and PA models incorporate contextual information that is not present in the case k = 2 and, then, a higher number of rules for a fixed number of categories is possible. In practice, due to the finite size of the training corpus, the number of rules is always moderate. However, as higher values of k lead to an enormous number of possible rules, huge data sets would be necessary in order to have a reliable estimate of the probabilities for values above k = 3. A detailed mathematical description of these type of models can be found in (Rico-Juan et al., 2000) 3 Experimental results The following table shows some data obtained with the three different models and the WSJ corpus. The second column contains the number of rules in the grammar obtained from a training subset of the corpus (24500 sentences, about the first half in the corpus) and the last one contains the percentage of sentences in a test set (2000 sentences) that cannot be parsed by the grammar. Model number of rules % unparsed sent. k = 2 11377 0 k = 3 64892 24 PA 18022 0.2 As expected, the number of rules obtained increases as more information is conveyed by the node label, although t</context>
</contexts>
<marker>Rico-Juan, Calera-Rubio, Carrasco, 2000</marker>
<rawString>Juan Ram6n Rico-Juan, Jorge Calera-Rubio, and Rafael C. Carrasco. 2000. Probabilistic k-testable tree language. In ICGI-2000, to appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>