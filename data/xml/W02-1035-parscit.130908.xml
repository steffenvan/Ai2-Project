<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000551">
<note confidence="0.568577">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 273-280.
Association for Computational Linguistics.
</note>
<bodyText confidence="0.999903517241379">
cus of spoken language parsing research for sev-
eral years (Bear et al., 1992; Seneff, 1992; Hee-
man, 1997; Ruland et al., 1998; Core and Schu-
bert, 1999). Most of the previous approaches
cope with dysfluencies and speech repairs in
the parser by providing ways for the parser to
skip over syntactically ill-formed parts of an
utterance. In more recent work (Stolcke and
Shriberg, 1996; Charniak and Johnson, 2001),
the problem of speech parsing is viewed as a
two step process. A preprocessing step is used
to identify speech repairs before parsing begins.
The approach we present in this paper, is simi-
lar to (Charniak and Johnson, 2001) with a few
differences. We do not constrain speech edits
and restarts to conform to a particular struc-
ture. Further, we also segment the utterance
into clauses which we believe would be easier
to interpret. Finally, we use plain word strings
with no punctuation marks since this is typically
the output of a speech recognizer.
The layout of the paper is as follows. In Sec-
tion 2, we will define the task and illustrate with
an example the encoding of the task that makes
it suitable for training models for annotation.
In Section 3, we discuss the two approaches for
clausification - n-gram approach and discrimina-
tive approach. The experiments and evaluation
results are presented in Section 4.
</bodyText>
<sectionHeader confidence="0.980934" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999759285714286">
Clausifier takes as input the result of recognition
of a user&apos;s utterance and generates clauses as its
output. The clausifier annotates its input with
tags that help in segmenting it into clauses. The
&lt;s&gt; tag is used to indicate sentence boundaries,
strings within [ and ] are to be edited out and
strings between {c and } indicate coordinating
conjunctions. These tags are then interpreted to
retrieve the set of clauses. This interpretation
involves deleting the words within [ and ] and
replacing &lt;s&gt; and the words enclosed within { c
and } with a line feed. An example&apos; illustrating
the input, the annotated output and the set of
clauses resulting from interpreting the output is
</bodyText>
<footnote confidence="0.939507">
1Stime_amount in this example is a named entity.
</footnote>
<note confidence="0.847642">
shown below. 2
</note>
<author confidence="0.279027">
Clausifier Input:
</author>
<bodyText confidence="0.9997535">
yes I got the bill and and eh I have
a question about I was surprised I
got a phone call with in I mean for
er $time_amount is what the the bill
said and you know you charged me eh
$time_amount plus tax so eh ...
</bodyText>
<subsectionHeader confidence="0.400353">
Clausifier annotated Output:
</subsectionHeader>
<bodyText confidence="0.93206375">
yes &lt;s&gt; I got the bill [ and ] {c and
} [ eh ] I have a question &lt;s&gt; [ about
] I was surprised &lt;s&gt; I got a phone
call [ with in ] [ I mean ] for [ er ]
$time_amount is what [ the ] the bill
said fc and 1 [ you know ] you charged
me [ eh ] $time_amount plus tax [ so eh
Clausifier parsed Output:
</bodyText>
<listItem confidence="0.999827571428571">
• yes
• I got the bill
• I have a question
• I was surprised
• I got a phone call for $time_amount
is what the bill said
• you charged me $time_amount plus tax
</listItem>
<bodyText confidence="0.971193285714286">
In order to train models for the clausifier,
we have encoded the sentence boundary, edit
and conjunction information as tags following
a word. If there is sentence boundary before
a word, it is tagged as &amp;quot;Segment&amp;quot;. Edit and
conjunction tags also contain span information;
&lt;Editl&gt; is for edit of one word to the left,
</bodyText>
<footnote confidence="0.994986">
2The example input is taken from a telephone con-
versation of a customer speaking to an operator. Due to
the unavailability of annotated data for this domain, we
report experimental results in this paper for the Switch-
board corpus.
</footnote>
<listItem confidence="0.9637495">
&lt;Edit2&gt; is for edit of two words to the left and
so on. A similar encoding is used for coordi-
nating conjunctions. A word boundary that has
neither of these tags is tagged as &amp;quot;No Action&amp;quot; .3
</listItem>
<subsectionHeader confidence="0.765318">
Encoding the problem:
</subsectionHeader>
<construct confidence="0.9300231">
yes &lt;s&gt; I got the bill and &lt;Edit1&gt;
and &lt;Conj1&gt; eh &lt;Edit1&gt; I have a
question &lt;s&gt; about &lt;Edit1&gt; I was
surprised &lt;s&gt; I got a phone call
with in &lt;Edit2&gt; I mean &lt;Edit2&gt; for
er &lt;Edit1&gt; $time_amount is what the
&lt;Edit1&gt; the bill said and &lt;Conj1&gt;
you know &lt;Edit2&gt; you charged me eh
&lt;Edit1&gt; $time_amount plus tax so eh
&lt;Edit2&gt;
</construct>
<sectionHeader confidence="0.988276" genericHeader="method">
3 Clausifier Method
</sectionHeader>
<bodyText confidence="0.999892416666667">
The task of annotating the input can be viewed
as a tagging problem. Each word of the input
is tagged with one of a few tags that indicate
the type of annotation following the word. In
particular, we consider the presence of sentence
boundary tag &lt;s&gt; and its absence &lt;nos&gt; as
two possible tags to associate with each word.
We can then use an n-gram tagging model (sim-
ilar to (Church, 1988)) as shown in equation 1
to retrieve the best tag sequence for a given in-
put sentence. We follow the same notation as
in (Church, 1988).
</bodyText>
<equation confidence="0.999916">
P(T) = argmaxTP(wiiti) * P(tiiti-1, ti-2) (1)
</equation>
<bodyText confidence="0.9789239375">
Such an n-gram based sentence boundary de-
tection method was presented in (Stolcke and
Shriberg, 1996) who also point out that the
advantage of n-gram based method is its nat-
ural integration within the language model of
the speech recognizer. However, increasing the
conditioning context in an n-gram increases the
number of parameters combinatorially and es-
timating these parameters reliably becomes an
issue. We present a discriminative classification
approach to clausifier which allows us to add
larger number of features, in contrast to the gen-
erative n-gram model.
3An alternate way of encoding is to use the &amp;quot;Inside-
Outside-Boundary&amp;quot; tags for each word as is typically
done for chunking of noun groups.
</bodyText>
<subsectionHeader confidence="0.994959">
3.1 Classifier
</subsectionHeader>
<bodyText confidence="0.999915911764706">
We used a machine-learning tool called Boos-
texter, which is based on the boosting family
of algorithms first proposed in (Schapire, 1999).
The basic idea of boosting is to build a highly
accurate classifier by combining many &amp;quot;weak&amp;quot; or
&amp;quot;simple&amp;quot; base classifiers, each one of which may
only be moderately accurate. To obtain these
base classifiers, it is assumed that a base learn-
ing algorithm is available that can be used as
a black-box subroutine. The collection of base
classifiers is iteratively constructed. On each it-
eration t, the base learner is used to generate
a base classifier ht. Besides supplying the base
learner with training data, the boosting algo-
rithm also provides a set of nonnegative weights
wt over the training examples. Intuitively, the
weights encode how important it is that ht cor-
rectly classifies each training example. Gener-
ally, the examples that were most often misclas-
sified by the preceding base classifiers will be
given the most weight so as to force the base
learner to focus on the &amp;quot;hardest&amp;quot; examples. As
described in (Schapire and Singer, 1999), Boos-
texter uses confidence rated classifiers h that,
rather than providing a binary decision of -1
or +1, output a real number h(x) whose sign
(-1 or +1) is interpreted as a prediction, and
whose magnitude Ih(x)1 is a measure of &amp;quot;con-
fidence.&amp;quot; The output of the final classifier f is
f (x) = ht(x), i.e. the sum of confidence of
all classifiers ht. The real-valued predictions of
the final classifier f can be converted into prob-
abilities by passing them through a logistic func-
tion; that is, we can regard the quantity
</bodyText>
<equation confidence="0.9509795">
1
1+ e-f(x)
</equation>
<bodyText confidence="0.9994344">
as an estimate of the probability that x belongs
to class +1. In fact, the boosting procedure is
designed to minimize the negative conditional
log likelihood of the data under this model,
namely:
</bodyText>
<equation confidence="0.96931">
E + e-Yif(xi))
</equation>
<bodyText confidence="0.9874195">
The extension of Boostexter to the multiclass
problem is described in (Schapire and Singer,
</bodyText>
<figure confidence="0.761084833333333">
Sequence of Words
Sentence Boundary
Detector
Sentences
Edit Detector
Edited Sentences
Conjunction Detector
Clauses
word_z i E {1, 2, 3} words to the left. &apos;bos&apos; if there is no word
wordz i E {1,2,3} words to the right. &apos;eos&apos; if there is no word
pos_z i E {1, 2, 3} parts of speech of three words to the left
posz i E {1, 2, 3} parts of speech of three words to the right
wl 1 if wordi=word_i otherwise 0
w2 # of words common in 2 left and 2 right words
w3 # of words common in 3 left and 3 right words
p1 1 if posi=pos_i otherwise 0
p2 # of pos common in 2 left and 2 right pos
A3 # of pos common in 3 left and 3 right pos
</figure>
<tableCaption confidence="0.998403">
Table 1: Features used for the classifiers
</tableCaption>
<bodyText confidence="0.999933736842105">
individual tags as well as the total error rate
and the baseline error rate for each tagging task.
This baseline error rate is calculated by using a
classifier that assigns each example the tag that
occurs most frequently in the data.
Since we are eventually interested in parsing
and understanding the resulting clauses, we also
report recall and precision after each of the an-
notations are interpreted (i.e. after utterances
are split at sentence boundaries, after edits are
deleted and after utterances are split at con-
junctions.). These scores are reported under the
&amp;quot;Sentence&amp;quot; column of each model&apos;s performance
table. Like other recall and precision numbers
sentence level recall indicates the proportion of
clauses in the input that are correctly identified
in the clausifier output, and precision indicates
the proportion of the output clauses that are in
the input.
</bodyText>
<subsectionHeader confidence="0.960784">
4.3 N-gram model: Baseline Model
</subsectionHeader>
<bodyText confidence="0.999824625">
Table 2 shows the results of using a trigram
model, similar to (Stolcke and Shriberg, 1996)
for sentence boundary detection on the data de-
scribed above. In our experiments, instead of
using the true part-of-speech tags as was done
in (Stolcke and Shriberg, 1996), we used the
result of tagging from an n-gram part-of-speech
tagger (similar to (Church, 1988)). In addition
to providing recall and precision scores on the in-
dividual segmentation decision, we also provide
sentence level performance. Notice that segmen-
tation precision and recall of approximately 80%
and 52% turn into sentence level precision and
recall of 50% and 32% respectively. We also no-
ticed that including POS improves the perfor-
mance by approximately 1%.
</bodyText>
<subsectionHeader confidence="0.873944">
4.4 Classifier Models
</subsectionHeader>
<bodyText confidence="0.999991333333333">
Training data for the classifiers was prepared by
labeling each word boundary with one of the
tags described in Section 2 and features shown
in Table 1. Apart from training individual clas-
sifiers for sentence boundary, edit and conjunc-
tion classification, we also trained a combined
classifier which performs all the three tasks in
one step and does not make any independence
assumptions as shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.970193">
4.5 Combined Classifier
</subsectionHeader>
<bodyText confidence="0.999933230769231">
Table 3 shows the performance of a combined
classifier that predicts a combined tag for each
of the components. The tagset for the combined
classifier is the cross-product of the tagsets for
segmentation,edits and conjunctions. Since this
classifier makes all the decisions, the output of
this classifier can be directly used to generate
clauses from the input strings of words. As ex-
pected this classifier outperforms the N-gram
based classifier both at segmentation decision
level and at sentence level (compare column 8
and 9 of table 3 with column 3 and 4 of table 2
respectively) .
</bodyText>
<subsectionHeader confidence="0.653923">
4.6 Individual Classifiers
</subsectionHeader>
<bodyText confidence="0.998125">
Tables 4,5,6 show the performance of the three
classifiers used in the cascade shown in Figure 1.
In these tables sentence level performances are
with respect to their own inputs and outputs.
Over all sentence level performance is shown in
</bodyText>
<table confidence="0.999460285714286">
No Segment Sentence
Action
Counts 57454 10284 10654
Recall (%) 98.13(98.02) 52.79(52.26) 32.55(31.53)
Precision (%) 92.07(91.98) 83.47(79.36) 50.94(49.29)
Total Error (%) 9.23(9.93)
Baseline Error (%) 15.18
</table>
<tableCaption confidence="0.9976775">
Table 2: Segmentation Performance Using Trigram Model. Performance without part-of-speech
information is shown in parenthesis.
</tableCaption>
<table confidence="0.999817285714286">
No Edit Edit Edit One Two Segment Sentence
Action One Two Three Conj Conjs
Counts 51126 3704 895 130 2391 258 9756 10789
Recall(%) 96.41 69.68 49.94 8.46 79.26 48.06 77.95 51.2
Prec(%) 93.10 78.69 67.12 50.00 80.71 73.37 85.18 50.06
Total Error 9.38%
Baseline Error 25.22%
</table>
<tableCaption confidence="0.893639">
Table 3: Performance of a classifier that predicts a combined tag
Table 7. These tables show that cascaded clas-
</tableCaption>
<bodyText confidence="0.82386175">
sifiers are significantly more accurate at mak-
ing individual decisions which results in higher
recall and precision at sentence level (compare
column of table 7 with column 9 of table 3).
</bodyText>
<sectionHeader confidence="0.948142" genericHeader="method">
5 Sensitivity Analysis
</sectionHeader>
<bodyText confidence="0.999924659090909">
In this section, we investigate the effect of vari-
ous features on the performance of the Segmen-
tation, Edit and Conjunction classifiers. Table 8
shows the results for each classifier using only
words, only parts-of-speech (POS) tags, words
and POS tags and words, POS tags combined
with the similarity measure. It is not surpris-
ing to note that using only POS tags to predict
sentence boundaries, edits and conjunctions re-
sults in a higher error rate compared to using
only words. Adding POS features to the words-
only model does not improve the performance of
these classifiers. This is to be expected since the
generalization provided by the POS tags is not
really needed for these tasks, as there are not
many unseen contexts even when using words
contextual features. However, we suspect su-
pertags (Bangalore and Joshi, 1999) can cap-
ture long-distance effects (eg. subcategorization
frame of preceding verb) which could improve
the segmentation performance.
It is however surprising to note that the sim-
ilarity features, which had been designed to
specifically capture patterns in speech repairs
does not contribute as much to the performance
(Words+POS+Similarity=3.5% as compared to
Words Only=3.9%). By separating discourse
markers (eg. you know, well, so), explicit edit
terms (eg. I mean, sorry, excuse me), and fillers
(eg. um, uh) from the set of Edit tags, we are
left only with restarts and repairs. We trained
a classifier for identifying only these tags and
the sensitivity results are shown in the fourth
column of Table 8. Note that the baseline per-
formance of Restarts and Repairs is much lower
than that of Edits indicating that it is an easier
task than identifying Edits. Incorporating the
similarity feature reduces the error rate for iden-
tifying restarts by 37% over a model which uses
only words. (Words+POS+Similarity=1.7% as
compared to Words Only=2.7%). This suggests
that an additional classifier to identify discourse
markers and explicit edit terms would be bene-
ficial.
</bodyText>
<table confidence="0.999524166666667">
No Action Segment Sentence
Counts 57980 10371 10561
Recall (%) 97.37 77.57 56.22
Precision (%) 96.04 84.07 60.83
Total Error 5.63%
Baseline Error 15.05%
</table>
<tableCaption confidence="0.994442">
Table 4: Performance of classifier for sentence boundary detection
</tableCaption>
<table confidence="0.999813428571429">
No Edit Edit Edit Edit Sentence
Action One Two Three Four
Counts 53936 4074 1042 125 58 10400
Recall(%) 99.09 76.12 62.57 9.60 1.72 81.58
Precision(%) 97.23 88.50 84.13 48.00 100.0 80.34
Total Error 3.48%
Baseline Error 9.02%
</table>
<tableCaption confidence="0.987521">
Table 5: Performance of the classifier to identify presence and the span of an edit.
</tableCaption>
<table confidence="0.999902285714286">
No One Two Sentence
Action Conj Conj
Counts 49682 1997 121 10789
Recall (%) 99.70 91.69 88.43 95.13
Precision (%) 99.66 92.94 84.25 94.46
Total Error 0.64%
Baseline Error 4.10%
</table>
<tableCaption confidence="0.970567">
Table 6: Performance of a classifier to identify presence and the span of a conjunction.
</tableCaption>
<table confidence="0.999541">
Sentence Level
Counts 10789
Recall (%) 53.87
Precision (%) 55.85
</table>
<tableCaption confidence="0.996701">
Table 7: End to End Performance
</tableCaption>
<table confidence="0.998435666666667">
Segment Edit Conjunction Restarts and Repairs
Baseline (%) 15.0 9.0 4.1 2.8
Words Only 5.6 3.9 0.6 2.7
Pos Only 9.9 7.3 1.6 2.8
Words-EPOS 5.6 3.9 0.6 2.7
Words+Pos+Similarity 5.6 3.5 0.6 1.7
</table>
<tableCaption confidence="0.987376">
Table 8: Sensitivity analysis of the features used for classification
</tableCaption>
<sectionHeader confidence="0.9907" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999989653846154">
In this paper, we have presented a clausifier
which would be used as a preprocessor in the
context of speech parsing and understanding
system. The clausifier contains three classifiers
that are trained to detect sentence boundaries,
speech repairs and coordinating conjunctions.
These models have been trained and tested on
Switchboard corpus and provide an end-to-end
recall and precision of 54% and 56% respec-
tively for the task of clause identification. We
have shown that classifier models clearly out-
perform the n-gram models, and that a com-
bined model does not perform as well as a model
that makes individual predictions. We believe
that the sentence level performance can be im-
proved further by improving the training data
quantity and quality. In the Switchboard cor-
pus we found that average turn length is 6, and
that the turn boundaries are very strong indi-
cator of the sentence boundaries. This makes
it hard for the classifier to learn other discrimi-
nating features. We plan to use this system to
iteratively annotate additional data with longer
turn lengths (customer-operator telephone con-
versations), manually correct it and retrain the
models described in this paper.
</bodyText>
<sectionHeader confidence="0.998765" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999489377777778">
Srinivas Bangalore and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
J. Bear, J. Dowding, and E. Shriberg. 1992. Inte-
grating multiple knowledge sources for detection
and correction of repairs in human-computer dia-
log. In Proceedings of ACL, pages 56-63.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of NAACL, Pittsburgh, PA.
Kenneth Ward Church. 1988. A Stochastic Parts
Program and Noun Phrase Parser for Unrestricted
Text. In 2nd Applied Natural Language Processing
Conference, Austin, Texas.
M.G. Core and L.K. Schubert. 1999. A syntactic
framework for speech repairs and other disrup-
tions. In Proceedings of ACL, pages 413-420.
A. L. Gorin, G. Riccardi, and J. H Wright. 1997.
How May I Help You? Speech Communication,
23:113-127.
Peter Heeman. 1997. Speech Repairs, Intona-
tion Boundaries and Discourse Markers: Model-
ing Speakers&apos; Utterances. Ph.D. thesis, University
of Rochester.
M. Meteer et al. 1995. Dysfluency annotation style-
book for the switchboard corpus. In Distributed
by LDC.
T. Ruland, C.J. Rupp, J. Spilker, H. Weber, and
K.L. Worm. 1998. Making the most of multiplic-
ity: A multi-parser multi-strategy architecture for
the robust processing of spoken language. Techni-
cal report, DFKI, Verbmobil report 230.
R.E. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence-rated predictions.
Machine Learning, 37(3):297-336, December.
R.E. Schapire. 1999. A brief introduction to boost-
ing. In Proceedings of the Sixteenth International
Joint Conference on Artificial Intelligence.
Stephanie Seneff. 1992. A relaxation method for
understanding spontaneous speech utterances. In
Proceedings, Speech and Natural Language Work-
shop, San Mateo, CA.
A. Stolcke and E. Shriberg. 1996. Statistical lan-
guage modeling for speech disfluencies. In Pro-
ceedings of ICASSP, Atlanta, GA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000026">
<note confidence="0.9002295">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 273-280. Association for Computational Linguistics. cus of spoken language parsing research for several years (Bear et al., 1992; Seneff, 1992; Heeman, 1997; Ruland et al., 1998; Core and Schu-</note>
<abstract confidence="0.996389592814372">bert, 1999). Most of the previous approaches cope with dysfluencies and speech repairs in the parser by providing ways for the parser to skip over syntactically ill-formed parts of an utterance. In more recent work (Stolcke and Shriberg, 1996; Charniak and Johnson, 2001), the problem of speech parsing is viewed as a two step process. A preprocessing step is used to identify speech repairs before parsing begins. The approach we present in this paper, is similar to (Charniak and Johnson, 2001) with a few differences. We do not constrain speech edits and restarts to conform to a particular structure. Further, we also segment the utterance into clauses which we believe would be easier to interpret. Finally, we use plain word strings with no punctuation marks since this is typically the output of a speech recognizer. The layout of the paper is as follows. In Section 2, we will define the task and illustrate with an example the encoding of the task that makes it suitable for training models for annotation. In Section 3, we discuss the two approaches for clausification n-gram approach and discriminative approach. The experiments and evaluation results are presented in Section 4. 2 Task Definition Clausifier takes as input the result of recognition of a user&apos;s utterance and generates clauses as its output. The clausifier annotates its input with tags that help in segmenting it into clauses. The &lt;s&gt; tag is used to indicate sentence boundaries, strings within [ and ] are to be edited out and strings between {c and } indicate coordinating conjunctions. These tags are then interpreted to retrieve the set of clauses. This interpretation involves deleting the words within [ and ] and replacing &lt;s&gt; and the words enclosed within { c and } with a line feed. An example&apos; illustrating the input, the annotated output and the set of clauses resulting from interpreting the output is in this example is a named entity. below. 2 Clausifier Input: yes I got the bill and and eh I have a question about I was surprised I got a phone call with in I mean for er $time_amount is what the the bill said and you know you charged me eh $time_amount plus tax so eh ... Clausifier annotated Output: &lt;s&gt; I got bill [ and {c } [ eh ] I have a question &lt;s&gt; [ about ] I was surprised &lt;s&gt; I got a phone [ with in ] [ ] for [ er ] $time_amount is what [ the ] the bill 1 [ you know ] you charged me [ eh ] $time_amount plus tax [ so eh Clausifier parsed Output: • yes • I got the bill • I have a question • I was surprised • I got a phone call for $time_amount is what the bill said • you charged me $time_amount plus tax In order to train models for the clausifier, we have encoded the sentence boundary, edit and conjunction information as tags following a word. If there is sentence boundary before a word, it is tagged as &amp;quot;Segment&amp;quot;. Edit and conjunction tags also contain span information; &lt;Editl&gt; is for edit of one word to the left, example input is taken from a telephone conversation of a customer speaking to an operator. Due to the unavailability of annotated data for this domain, we report experimental results in this paper for the Switchboard corpus. &lt;Edit2&gt; is for edit of two words to the left and so on. A similar encoding is used for coordinating conjunctions. A word boundary that has of these tags is tagged as &amp;quot;No Action&amp;quot; Encoding the problem: yes &lt;s&gt; I got the bill and &lt;Edit1&gt; and &lt;Conj1&gt; eh &lt;Edit1&gt; I have a question &lt;s&gt; about &lt;Edit1&gt; I was surprised &lt;s&gt; I got a phone call with in &lt;Edit2&gt; I mean &lt;Edit2&gt; for er &lt;Edit1&gt; $time_amount is what the &lt;Edit1&gt; the bill said and &lt;Conj1&gt; you know &lt;Edit2&gt; you charged me eh &lt;Edit1&gt; $time_amount plus tax so eh &lt;Edit2&gt; 3 Clausifier Method The task of annotating the input can be viewed as a tagging problem. Each word of the input is tagged with one of a few tags that indicate the type of annotation following the word. In particular, we consider the presence of sentence boundary tag &lt;s&gt; and its absence &lt;nos&gt; as two possible tags to associate with each word. We can then use an n-gram tagging model (similar to (Church, 1988)) as shown in equation 1 to retrieve the best tag sequence for a given input sentence. We follow the same notation as in (Church, 1988). = argmaxTP(wiiti) * P(tiiti-1, ti-2) Such an n-gram based sentence boundary detection method was presented in (Stolcke and Shriberg, 1996) who also point out that the advantage of n-gram based method is its natural integration within the language model of the speech recognizer. However, increasing the conditioning context in an n-gram increases the number of parameters combinatorially and estimating these parameters reliably becomes an issue. We present a discriminative classification approach to clausifier which allows us to add larger number of features, in contrast to the generative n-gram model. alternate way of encoding is to use the &amp;quot;Inside- Outside-Boundary&amp;quot; tags for each word as is typically done for chunking of noun groups. 3.1 Classifier We used a machine-learning tool called Boostexter, which is based on the boosting family of algorithms first proposed in (Schapire, 1999). The basic idea of boosting is to build a highly accurate classifier by combining many &amp;quot;weak&amp;quot; or &amp;quot;simple&amp;quot; base classifiers, each one of which may only be moderately accurate. To obtain these base classifiers, it is assumed that a base learning algorithm is available that can be used as a black-box subroutine. The collection of base classifiers is iteratively constructed. On each itbase learner is used to generate base classifier supplying the base learner with training data, the boosting algorithm also provides a set of nonnegative weights over the training examples. Intuitively, the encode how important it is that correctly classifies each training example. Generally, the examples that were most often misclassified by the preceding base classifiers will be given the most weight so as to force the base learner to focus on the &amp;quot;hardest&amp;quot; examples. As described in (Schapire and Singer, 1999), Boosuses rated rather than providing a binary decision of -1 +1, output a real number sign (-1 or +1) is interpreted as a prediction, and whose magnitude Ih(x)1 is a measure of &amp;quot;con- The output of the final classifier (x) = the sum of confidence classifiers real-valued predictions of final classifier be converted into probabilities by passing them through a logistic function; that is, we can regard the quantity 1 an estimate of the probability that to class +1. In fact, the boosting procedure is designed to minimize the negative conditional log likelihood of the data under this model, namely: E + e-Yif(xi)) The extension of Boostexter to the multiclass problem is described in (Schapire and Singer,</abstract>
<title confidence="0.965250875">Sequence of Words Sentence Boundary Detector Sentences Edit Detector Edited Sentences Conjunction Detector Clauses</title>
<abstract confidence="0.99522776">E 2, 3} words to the left. &apos;bos&apos; if there is no word E {1,2,3} to the right. &apos;eos&apos; if there is no word E 2, 3} parts of speech of three words to the left E 2, 3} parts of speech of three words to the right 1 if otherwise 0 w2 # of words common in 2 left and 2 right words w3 # of words common in 3 left and 3 right words 1 if 0 p2 # of pos common in 2 left and 2 right pos Table 1: Features used for the classifiers individual tags as well as the total error rate and the baseline error rate for each tagging task. This baseline error rate is calculated by using a classifier that assigns each example the tag that occurs most frequently in the data. Since we are eventually interested in parsing and understanding the resulting clauses, we also report recall and precision after each of the annotations are interpreted (i.e. after utterances are split at sentence boundaries, after edits are deleted and after utterances are split at conjunctions.). These scores are reported under the &amp;quot;Sentence&amp;quot; column of each model&apos;s performance table. Like other recall and precision numbers sentence level recall indicates the proportion of clauses in the input that are correctly identified in the clausifier output, and precision indicates the proportion of the output clauses that are in the input. 4.3 N-gram model: Baseline Model Table 2 shows the results of using a trigram model, similar to (Stolcke and Shriberg, 1996) for sentence boundary detection on the data described above. In our experiments, instead of using the true part-of-speech tags as was done in (Stolcke and Shriberg, 1996), we used the result of tagging from an n-gram part-of-speech tagger (similar to (Church, 1988)). In addition to providing recall and precision scores on the individual segmentation decision, we also provide sentence level performance. Notice that segmentation precision and recall of approximately 80% and 52% turn into sentence level precision and of 50% and 32% respectively. We also noticed that including POS improves the performance by approximately 1%. 4.4 Classifier Models Training data for the classifiers was prepared by labeling each word boundary with one of the tags described in Section 2 and features shown in Table 1. Apart from training individual classifiers for sentence boundary, edit and conjunction classification, we also trained a combined classifier which performs all the three tasks in one step and does not make any independence assumptions as shown in Figure 1. 4.5 Combined Classifier Table 3 shows the performance of a combined classifier that predicts a combined tag for each of the components. The tagset for the combined classifier is the cross-product of the tagsets for segmentation,edits and conjunctions. Since this classifier makes all the decisions, the output of this classifier can be directly used to generate clauses from the input strings of words. As expected this classifier outperforms the N-gram based classifier both at segmentation decision level and at sentence level (compare column 8 and 9 of table 3 with column 3 and 4 of table 2 respectively) . 4.6 Individual Classifiers Tables 4,5,6 show the performance of the three classifiers used in the cascade shown in Figure 1. In these tables sentence level performances are with respect to their own inputs and outputs.</abstract>
<note confidence="0.790722333333333">Over all sentence level performance is shown in No Action Segment Sentence Counts 57454 10284 10654 Recall (%) 98.13(98.02) 52.79(52.26) 32.55(31.53) Precision (%) 92.07(91.98) 83.47(79.36) 50.94(49.29) Total Error (%) 9.23(9.93) Baseline Error (%) 15.18 Table 2: Segmentation Performance Using Trigram Model. Performance without part-of-speech information is shown in parenthesis. No Action One Edit Two Edit Three One Conj Two Conjs Segment Sentence Counts 51126 3704 895 130 2391 258 9756 10789 Recall(%) 96.41 69.68 49.94 8.46 79.26 48.06 77.95 51.2 Prec(%) 93.10 78.69 67.12 50.00 80.71 73.37 85.18 50.06 Total Error 9.38% Baseline Error 25.22%</note>
<abstract confidence="0.995548156862745">Table 3: Performance of a classifier that predicts a combined tag Table 7. These tables show that cascaded classifiers are significantly more accurate at making individual decisions which results in higher recall and precision at sentence level (compare column of table 7 with column 9 of table 3). 5 Sensitivity Analysis In this section, we investigate the effect of various features on the performance of the Segmentation, Edit and Conjunction classifiers. Table 8 shows the results for each classifier using only words, only parts-of-speech (POS) tags, words and POS tags and words, POS tags combined with the similarity measure. It is not surprising to note that using only POS tags to predict sentence boundaries, edits and conjunctions results in a higher error rate compared to using only words. Adding POS features to the wordsonly model does not improve the performance of these classifiers. This is to be expected since the generalization provided by the POS tags is not really needed for these tasks, as there are not many unseen contexts even when using words contextual features. However, we suspect supertags (Bangalore and Joshi, 1999) can capture long-distance effects (eg. subcategorization frame of preceding verb) which could improve the segmentation performance. It is however surprising to note that the similarity features, which had been designed to specifically capture patterns in speech repairs does not contribute as much to the performance (Words+POS+Similarity=3.5% as compared to Words Only=3.9%). By separating discourse markers (eg. you know, well, so), explicit edit terms (eg. I mean, sorry, excuse me), and fillers (eg. um, uh) from the set of Edit tags, we are left only with restarts and repairs. We trained a classifier for identifying only these tags and the sensitivity results are shown in the fourth column of Table 8. Note that the baseline performance of Restarts and Repairs is much lower than that of Edits indicating that it is an easier task than identifying Edits. Incorporating the similarity feature reduces the error rate for identifying restarts by 37% over a model which uses only words. (Words+POS+Similarity=1.7% as compared to Words Only=2.7%). This suggests that an additional classifier to identify discourse markers and explicit edit terms would be beneficial.</abstract>
<note confidence="0.869868970588235">No Action Segment Sentence Counts 57980 10371 10561 Recall (%) 97.37 77.57 56.22 Precision (%) 96.04 84.07 60.83 Total Error 5.63% Baseline Error 15.05% Table 4: Performance of classifier for sentence boundary detection No Action Edit One Edit Two Edit Three Edit Four Sentence Counts 53936 4074 1042 125 58 10400 Recall(%) 99.09 76.12 62.57 9.60 1.72 81.58 Precision(%) 97.23 88.50 84.13 48.00 100.0 80.34 Total Error 3.48% Baseline Error 9.02% Table 5: Performance of the classifier to identify presence and the span of an edit. No Action Conj Two Conj Sentence Counts 49682 1997 121 10789 Recall (%) 99.70 91.69 88.43 95.13 Precision (%) 99.66 92.94 84.25 94.46 Total Error 0.64% Baseline Error 4.10% Table 6: Performance of a classifier to identify presence and the span of a conjunction. Sentence Level Counts 10789 Recall (%) 53.87 Precision (%) 55.85 Table 7: End to End Performance Segment Edit Conjunction Restarts and Repairs Baseline (%) 15.0 9.0 4.1 2.8 Words Only 5.6 3.9 0.6 2.7 Pos Only 9.9 7.3 1.6 2.8 Words-EPOS 5.6 3.9 0.6 2.7 Words+Pos+Similarity 5.6 3.5 0.6 1.7 Table 8: Sensitivity analysis of the features used for classification 6 Conclusions</note>
<abstract confidence="0.997540230769231">In this paper, we have presented a clausifier which would be used as a preprocessor in the context of speech parsing and understanding system. The clausifier contains three classifiers that are trained to detect sentence boundaries, speech repairs and coordinating conjunctions. These models have been trained and tested on Switchboard corpus and provide an end-to-end recall and precision of 54% and 56% respectively for the task of clause identification. We have shown that classifier models clearly outperform the n-gram models, and that a combined model does not perform as well as a model that makes individual predictions. We believe that the sentence level performance can be improved further by improving the training data quantity and quality. In the Switchboard corpus we found that average turn length is 6, and that the turn boundaries are very strong indicator of the sentence boundaries. This makes it hard for the classifier to learn other discriminating features. We plan to use this system to iteratively annotate additional data with longer turn lengths (customer-operator telephone conversations), manually correct it and retrain the models described in this paper.</abstract>
<note confidence="0.763110608695652">References Srinivas Bangalore and Aravind Joshi. 1999. Su- An approach to almost parsing. Com- Linguistics, J. Bear, J. Dowding, and E. Shriberg. 1992. Integrating multiple knowledge sources for detection and correction of repairs in human-computer dia- In of ACL, 56-63. E. Charniak and M. Johnson. 2001. Edit detection parsing for transcribed speech. In NAACL, PA. Kenneth Ward Church. 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted In Applied Natural Language Processing Texas. M.G. Core and L.K. Schubert. 1999. A syntactic framework for speech repairs and other disrup- In of ACL, 413-420. L. Gorin, G. Riccardi, and Wright. 1997. May I Help You? Communication, 23:113-127. Heeman. 1997. Repairs, Intonation Boundaries and Discourse Markers: Model-</note>
<affiliation confidence="0.762926">Speakers&apos; Utterances. thesis, University</affiliation>
<abstract confidence="0.928522090909091">of Rochester. M. Meteer et al. 1995. Dysfluency annotation stylefor the switchboard corpus. In by LDC. T. Ruland, C.J. Rupp, J. Spilker, H. Weber, and K.L. Worm. 1998. Making the most of multiplicity: A multi-parser multi-strategy architecture for the robust processing of spoken language. Technical report, DFKI, Verbmobil report 230. R.E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions.</abstract>
<intro confidence="0.872765">Learning, December.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="12665" citStr="Bangalore and Joshi, 1999" startWordPosition="2174" endWordPosition="2177">nly parts-of-speech (POS) tags, words and POS tags and words, POS tags combined with the similarity measure. It is not surprising to note that using only POS tags to predict sentence boundaries, edits and conjunctions results in a higher error rate compared to using only words. Adding POS features to the wordsonly model does not improve the performance of these classifiers. This is to be expected since the generalization provided by the POS tags is not really needed for these tasks, as there are not many unseen contexts even when using words contextual features. However, we suspect supertags (Bangalore and Joshi, 1999) can capture long-distance effects (eg. subcategorization frame of preceding verb) which could improve the segmentation performance. It is however surprising to note that the similarity features, which had been designed to specifically capture patterns in speech repairs does not contribute as much to the performance (Words+POS+Similarity=3.5% as compared to Words Only=3.9%). By separating discourse markers (eg. you know, well, so), explicit edit terms (eg. I mean, sorry, excuse me), and fillers (eg. um, uh) from the set of Edit tags, we are left only with restarts and repairs. We trained a cla</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bear</author>
<author>J Dowding</author>
<author>E Shriberg</author>
</authors>
<title>Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog.</title>
<date>1992</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>56--63</pages>
<marker>Bear, Dowding, Shriberg, 1992</marker>
<rawString>J. Bear, J. Dowding, and E. Shriberg. 1992. Integrating multiple knowledge sources for detection and correction of repairs in human-computer dialog. In Proceedings of ACL, pages 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="808" citStr="Charniak and Johnson, 2001" startWordPosition="125" endWordPosition="128">oken language parsing research for several years (Bear et al., 1992; Seneff, 1992; Heeman, 1997; Ruland et al., 1998; Core and Schubert, 1999). Most of the previous approaches cope with dysfluencies and speech repairs in the parser by providing ways for the parser to skip over syntactically ill-formed parts of an utterance. In more recent work (Stolcke and Shriberg, 1996; Charniak and Johnson, 2001), the problem of speech parsing is viewed as a two step process. A preprocessing step is used to identify speech repairs before parsing begins. The approach we present in this paper, is similar to (Charniak and Johnson, 2001) with a few differences. We do not constrain speech edits and restarts to conform to a particular structure. Further, we also segment the utterance into clauses which we believe would be easier to interpret. Finally, we use plain word strings with no punctuation marks since this is typically the output of a speech recognizer. The layout of the paper is as follows. In Section 2, we will define the task and illustrate with an example the encoding of the task that makes it suitable for training models for annotation. In Section 3, we discuss the two approaches for clausification - n-gram approach</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>E. Charniak and M. Johnson. 2001. Edit detection and parsing for transcribed speech. In Proceedings of NAACL, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
</authors>
<title>A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.</title>
<date>1988</date>
<booktitle>In 2nd Applied Natural Language Processing Conference,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="4471" citStr="Church, 1988" startWordPosition="807" endWordPosition="808"> I got a phone call with in &lt;Edit2&gt; I mean &lt;Edit2&gt; for er &lt;Edit1&gt; $time_amount is what the &lt;Edit1&gt; the bill said and &lt;Conj1&gt; you know &lt;Edit2&gt; you charged me eh &lt;Edit1&gt; $time_amount plus tax so eh &lt;Edit2&gt; 3 Clausifier Method The task of annotating the input can be viewed as a tagging problem. Each word of the input is tagged with one of a few tags that indicate the type of annotation following the word. In particular, we consider the presence of sentence boundary tag &lt;s&gt; and its absence &lt;nos&gt; as two possible tags to associate with each word. We can then use an n-gram tagging model (similar to (Church, 1988)) as shown in equation 1 to retrieve the best tag sequence for a given input sentence. We follow the same notation as in (Church, 1988). P(T) = argmaxTP(wiiti) * P(tiiti-1, ti-2) (1) Such an n-gram based sentence boundary detection method was presented in (Stolcke and Shriberg, 1996) who also point out that the advantage of n-gram based method is its natural integration within the language model of the speech recognizer. However, increasing the conditioning context in an n-gram increases the number of parameters combinatorially and estimating these parameters reliably becomes an issue. We pres</context>
<context position="9226" citStr="Church, 1988" startWordPosition="1624" endWordPosition="1625">ion numbers sentence level recall indicates the proportion of clauses in the input that are correctly identified in the clausifier output, and precision indicates the proportion of the output clauses that are in the input. 4.3 N-gram model: Baseline Model Table 2 shows the results of using a trigram model, similar to (Stolcke and Shriberg, 1996) for sentence boundary detection on the data described above. In our experiments, instead of using the true part-of-speech tags as was done in (Stolcke and Shriberg, 1996), we used the result of tagging from an n-gram part-of-speech tagger (similar to (Church, 1988)). In addition to providing recall and precision scores on the individual segmentation decision, we also provide sentence level performance. Notice that segmentation precision and recall of approximately 80% and 52% turn into sentence level precision and recall of 50% and 32% respectively. We also noticed that including POS improves the performance by approximately 1%. 4.4 Classifier Models Training data for the classifiers was prepared by labeling each word boundary with one of the tags described in Section 2 and features shown in Table 1. Apart from training individual classifiers for senten</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Kenneth Ward Church. 1988. A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In 2nd Applied Natural Language Processing Conference, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Core</author>
<author>L K Schubert</author>
</authors>
<title>A syntactic framework for speech repairs and other disruptions.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>413--420</pages>
<marker>Core, Schubert, 1999</marker>
<rawString>M.G. Core and L.K. Schubert. 1999. A syntactic framework for speech repairs and other disruptions. In Proceedings of ACL, pages 413-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>G Riccardi</author>
<author>J H Wright</author>
</authors>
<title>How May I Help You? Speech Communication,</title>
<date>1997</date>
<pages>23--113</pages>
<marker>Gorin, Riccardi, Wright, 1997</marker>
<rawString>A. L. Gorin, G. Riccardi, and J. H Wright. 1997. How May I Help You? Speech Communication, 23:113-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
</authors>
<title>Speech Repairs, Intonation Boundaries and Discourse Markers: Modeling Speakers&apos; Utterances.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<marker>Heeman, 1997</marker>
<rawString>Peter Heeman. 1997. Speech Repairs, Intonation Boundaries and Discourse Markers: Modeling Speakers&apos; Utterances. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
</authors>
<title>Dysfluency annotation stylebook for the switchboard corpus.</title>
<date>1995</date>
<booktitle>In Distributed by LDC.</booktitle>
<marker>Meteer, 1995</marker>
<rawString>M. Meteer et al. 1995. Dysfluency annotation stylebook for the switchboard corpus. In Distributed by LDC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ruland</author>
<author>C J Rupp</author>
<author>J Spilker</author>
<author>H Weber</author>
<author>K L Worm</author>
</authors>
<title>Making the most of multiplicity: A multi-parser multi-strategy architecture for the robust processing of spoken language.</title>
<date>1998</date>
<tech>Technical report, DFKI, Verbmobil report 230.</tech>
<marker>Ruland, Rupp, Spilker, Weber, Worm, 1998</marker>
<rawString>T. Ruland, C.J. Rupp, J. Spilker, H. Weber, and K.L. Worm. 1998. Making the most of multiplicity: A multi-parser multi-strategy architecture for the robust processing of spoken language. Technical report, DFKI, Verbmobil report 230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Improved boosting algorithms using confidence-rated predictions.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>37--3</pages>
<contexts>
<context position="6459" citStr="Schapire and Singer, 1999" startWordPosition="1132" endWordPosition="1135">lection of base classifiers is iteratively constructed. On each iteration t, the base learner is used to generate a base classifier ht. Besides supplying the base learner with training data, the boosting algorithm also provides a set of nonnegative weights wt over the training examples. Intuitively, the weights encode how important it is that ht correctly classifies each training example. Generally, the examples that were most often misclassified by the preceding base classifiers will be given the most weight so as to force the base learner to focus on the &amp;quot;hardest&amp;quot; examples. As described in (Schapire and Singer, 1999), Boostexter uses confidence rated classifiers h that, rather than providing a binary decision of -1 or +1, output a real number h(x) whose sign (-1 or +1) is interpreted as a prediction, and whose magnitude Ih(x)1 is a measure of &amp;quot;confidence.&amp;quot; The output of the final classifier f is f (x) = ht(x), i.e. the sum of confidence of all classifiers ht. The real-valued predictions of the final classifier f can be converted into probabilities by passing them through a logistic function; that is, we can regard the quantity 1 1+ e-f(x) as an estimate of the probability that x belongs to class +1. In fa</context>
</contexts>
<marker>Schapire, Singer, 1999</marker>
<rawString>R.E. Schapire and Y. Singer. 1999. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297-336, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
</authors>
<title>A brief introduction to boosting.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5514" citStr="Schapire, 1999" startWordPosition="976" endWordPosition="977">ever, increasing the conditioning context in an n-gram increases the number of parameters combinatorially and estimating these parameters reliably becomes an issue. We present a discriminative classification approach to clausifier which allows us to add larger number of features, in contrast to the generative n-gram model. 3An alternate way of encoding is to use the &amp;quot;InsideOutside-Boundary&amp;quot; tags for each word as is typically done for chunking of noun groups. 3.1 Classifier We used a machine-learning tool called Boostexter, which is based on the boosting family of algorithms first proposed in (Schapire, 1999). The basic idea of boosting is to build a highly accurate classifier by combining many &amp;quot;weak&amp;quot; or &amp;quot;simple&amp;quot; base classifiers, each one of which may only be moderately accurate. To obtain these base classifiers, it is assumed that a base learning algorithm is available that can be used as a black-box subroutine. The collection of base classifiers is iteratively constructed. On each iteration t, the base learner is used to generate a base classifier ht. Besides supplying the base learner with training data, the boosting algorithm also provides a set of nonnegative weights wt over the training exa</context>
</contexts>
<marker>Schapire, 1999</marker>
<rawString>R.E. Schapire. 1999. A brief introduction to boosting. In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Seneff</author>
</authors>
<title>A relaxation method for understanding spontaneous speech utterances.</title>
<date>1992</date>
<booktitle>In Proceedings, Speech and Natural Language Workshop,</booktitle>
<location>San Mateo, CA.</location>
<marker>Seneff, 1992</marker>
<rawString>Stephanie Seneff. 1992. A relaxation method for understanding spontaneous speech utterances. In Proceedings, Speech and Natural Language Workshop, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
</authors>
<title>Statistical language modeling for speech disfluencies.</title>
<date>1996</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="4755" citStr="Stolcke and Shriberg, 1996" startWordPosition="854" endWordPosition="857"> as a tagging problem. Each word of the input is tagged with one of a few tags that indicate the type of annotation following the word. In particular, we consider the presence of sentence boundary tag &lt;s&gt; and its absence &lt;nos&gt; as two possible tags to associate with each word. We can then use an n-gram tagging model (similar to (Church, 1988)) as shown in equation 1 to retrieve the best tag sequence for a given input sentence. We follow the same notation as in (Church, 1988). P(T) = argmaxTP(wiiti) * P(tiiti-1, ti-2) (1) Such an n-gram based sentence boundary detection method was presented in (Stolcke and Shriberg, 1996) who also point out that the advantage of n-gram based method is its natural integration within the language model of the speech recognizer. However, increasing the conditioning context in an n-gram increases the number of parameters combinatorially and estimating these parameters reliably becomes an issue. We present a discriminative classification approach to clausifier which allows us to add larger number of features, in contrast to the generative n-gram model. 3An alternate way of encoding is to use the &amp;quot;InsideOutside-Boundary&amp;quot; tags for each word as is typically done for chunking of noun g</context>
<context position="8960" citStr="Stolcke and Shriberg, 1996" startWordPosition="1579" endWordPosition="1582">the annotations are interpreted (i.e. after utterances are split at sentence boundaries, after edits are deleted and after utterances are split at conjunctions.). These scores are reported under the &amp;quot;Sentence&amp;quot; column of each model&apos;s performance table. Like other recall and precision numbers sentence level recall indicates the proportion of clauses in the input that are correctly identified in the clausifier output, and precision indicates the proportion of the output clauses that are in the input. 4.3 N-gram model: Baseline Model Table 2 shows the results of using a trigram model, similar to (Stolcke and Shriberg, 1996) for sentence boundary detection on the data described above. In our experiments, instead of using the true part-of-speech tags as was done in (Stolcke and Shriberg, 1996), we used the result of tagging from an n-gram part-of-speech tagger (similar to (Church, 1988)). In addition to providing recall and precision scores on the individual segmentation decision, we also provide sentence level performance. Notice that segmentation precision and recall of approximately 80% and 52% turn into sentence level precision and recall of 50% and 32% respectively. We also noticed that including POS improves</context>
</contexts>
<marker>Stolcke, Shriberg, 1996</marker>
<rawString>A. Stolcke and E. Shriberg. 1996. Statistical language modeling for speech disfluencies. In Proceedings of ICASSP, Atlanta, GA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>