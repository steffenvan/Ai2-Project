<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000162">
<title confidence="0.876771">
Priberam: A Turbo Semantic Parser with Second Order Features
</title>
<author confidence="0.640629">
Andr´e F. T. Martins*† Mariana S. C. Almeida*†
*Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal
</author>
<affiliation confidence="0.733975">
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal
</affiliation>
<email confidence="0.974441">
{atm,mla}@priberam.pt
</email>
<sectionHeader confidence="0.993071" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838916666667">
This paper presents our contribution to
the SemEval-2014 shared task on Broad-
Coverage Semantic Dependency Parsing.
We employ a feature-rich linear model, in-
cluding scores for first and second-order
dependencies (arcs, siblings, grandparents
and co-parents). Decoding is performed in
a global manner by solving a linear relax-
ation with alternating directions dual de-
composition (AD3). Our system achieved
the top score in the open challenge, and the
second highest score in the closed track.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996295">
The last decade saw a considerable progress in sta-
tistical modeling for dependency syntactic pars-
ing (K¨ubler et al., 2009). Models that incorporate
rich global features are typically more accurate,
even if pruning is necessary or decoding needs to
be approximate (McDonald et al., 2006; Koo and
Collins, 2010; Bohnet and Nivre, 2012; Martins et
al., 2009, 2013). This paper applies the same ratio-
nale to semantic dependency parsing, in which
the output variable is a semantic graph, rather
than a syntactic tree. We extend a recently pro-
posed dependency parser, TurboParser (Martins et
al., 2010, 2013), to be able to perform semantic
parsing using any of the three formalisms consid-
ered in this shared task (DM, PAS, and PCEDT).
The result is TurboSemanticParser, which we re-
lease as open-source software.1
We describe here a second order model for se-
mantic parsing (§2). We follow prior work in se-
mantic role labeling (Toutanova et al., 2005; Jo-
</bodyText>
<footnote confidence="0.892249333333333">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1http://labs.priberam.com/Resources/
TurboSemanticParser
</footnote>
<figureCaption confidence="0.657724">
Figure 1: Example of a semantic graph in the DM
formalism (sentence #22006003). We treat top
nodes as a special semantic role TOP whose predi-
cate is a dummy root symbol.
</figureCaption>
<bodyText confidence="0.997204571428572">
hansson and Nugues, 2008; Das et al., 2012; Flani-
gan et al., 2014), by adding constraints and model-
ing interactions among arguments within the same
frame; however, we go beyond such sibling in-
teractions to consider more complex grandpar-
ent and co-parent structures, effectively correlat-
ing different predicates. We formulate parsing as
a global optimization problem and solve a relax-
ation through AD3, a fast dual decomposition al-
gorithm in which several simple local subprob-
lems are solved iteratively (§3). Through a rich
set of features (§4), we arrive at top accuracies at
parsing speeds around 1,000 tokens per second, as
described in the experimental section (§5).
</bodyText>
<sectionHeader confidence="0.980556" genericHeader="method">
2 A Second Order Model for Parsing
</sectionHeader>
<bodyText confidence="0.999947071428571">
Figure 1 depicts a sentence and its semantic graph.
We cast semantic parsing as a structured predic-
tion problem. Let x be a sentence and Y(x) the
set of possible dependency graphs. We assume
each candidate graph y E Y(x) can be repre-
sented as a set of substructures (called parts) in
an underlying set S (e.g., predicates, arcs, pairs
of adjacent arcs). We design a score function f
which decomposes as a sum over these substruc-
tures, f(x, y) := EsES fs(x, ys). We parametrize
this function using a weight vector w, and write
each atomic function as fs(x, ys) := w·φs(x, ys),
where φs(x, ys) is a vector of local features. The
decoding problem consists in obtaining the best-
</bodyText>
<page confidence="0.982767">
471
</page>
<note confidence="0.7404715">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 471–476,
Dublin, Ireland, August 23-24, 2014.
</note>
<construct confidence="0.325079">
Algorithm 1 Decoding in an Arc-Factored Model
</construct>
<listItem confidence="0.991198">
1: input: Predicate scores vP (p), arc scores vA(p → a),
labeled arc scores vLA(p r→ a).
2: Initialize semantic graph G ← 0
3: for p = 0 to L do
4: Initialize v ← vP (p), frame A(p) ← 0
5: for a = 1 to L do
6: Set r� ← arg maxr vLA(p r→ a)
7: if vA(p → a) + vLA(p r� → a) &gt; 0 then
8: A(p) ← A(p) ∪ {hp, a, r�i}
9: v ← v + vA(p → a) + vLA(p r�→ a)
10: end if
11: end for
12: if v &gt; 0 then set G ← G ∪ {hp, A(p)i}
13: end for
14: output: semantic graph G.
</listItem>
<equation confidence="0.86135">
scored semantic graph y given a sentence x:
y = arg max f(x, y). (1)
y∈Y(x)
</equation>
<bodyText confidence="0.999951103448276">
Our choice of parts is given in Figure 2. The sec-
ond order parts are inspired by prior work in syn-
tactic parsing, modeling interactions for pairs of
(unlabeled) dependency arcs, such as grandpar-
ents (Carreras, 2007) and siblings (Smith and Eis-
ner, 2008; Martins et al., 2009). The main novelty
is co-parent parts, which, to the best of our knowl-
edge, were never considered before, as they only
make sense when multiple parents are allowed.
If all parts were basic, decoding could be done
independently for each predicate p, as illustrated
in Algorithm 1. The total runtime, for a sentence
with L words, is O(L2|9Z|), where 92, is the set
of semantic roles. Adding consecutive siblings
still permits independent decoding for each pred-
icate, but dynamic programming is necessary to
decode the best argument frame, increasing the
runtime to O(L3|9Z|). The addition of consec-
utive co-parents, grandparents, and arbitrary sib-
lings and co-parents breaks this independency and
sets a demand for approximate decoding. Even
without second-order parts, the inclusion of hard
constraints (such as requiring some roles to be
unique, see §3) also makes the problem harder.2
Rather than looking for a model in which exact
decoding is tractable, which could be even more
stringent for parsing semantic graphs than for de-
pendency trees, we embrace approximate decod-
ing strategies. Namely, our approach is based on
</bodyText>
<footnote confidence="0.9957834">
2Albeit the dynamic program could still incorporate con-
straints for unique roles (by appending a bit-string to the state
to mark semantic roles that have been filled), runtime be-
comes exponential in the number of unique roles, only being
feasible when this number is small.
</footnote>
<figureCaption confidence="0.954135">
Figure 2: Parts considered in this paper. The
</figureCaption>
<bodyText confidence="0.99548878125">
top row illustrate the basic parts, representing the
event that a word is a predicate, or the existence of
an arc between a predicate and an argument, even-
tually labeled with a semantic role. Our second-
order model looks at some pairs of arcs: arcs bear-
ing a grandparent relationship, arguments of the
same predicate, predicates sharing the same argu-
ment, and consecutive versions of these two.
dual decomposition, a class of optimization tech-
niques that tackle the dual of combinatorial prob-
lems in a modular and extensible manner (Ko-
modakis et al., 2007; Rush et al., 2010). We em-
ploy alternating directions dual decomposition
(AD3; Martins et al., 2011). Like the subgradi-
ent algorithm of Rush et al. (2010), AD3 splits
the original problem into local subproblems, and
seeks an agreement on the overlapping variables.
The difference is that the AD3 subproblems have
an additional quadratic term to accelerate con-
sensus, achieving a faster convergence rate both
in theory and in practice (Martins et al., 2012,
2013). For several factors (such as logic factors
representing AND, OR and XOR constraints, bud-
get constraints, and binary pairwise factors), these
quadratic subproblems can be solved efficiently.
For dense or structured factors, the quadratic sub-
problems can be solved as a sequence of local
Viterbi decoding steps, via an active set method
(Martins, 2014); this local decoding operation is
the same that needs to be performed in the subgra-
dient algorithm. We describe these subproblems
in detail in the next section.
</bodyText>
<sectionHeader confidence="0.937572" genericHeader="method">
3 Solving the Subproblems
</sectionHeader>
<bodyText confidence="0.994192166666667">
Predicate and Arc-Factored Parts. We capture
all the basic parts with a single component. As
stated in §2, local decoding in this component has
a runtime of O(L2|9Z|), by using Algorithm 1.
Unique Roles. We assume some roles are
unique, i.e., they can occur at most once for the
</bodyText>
<page confidence="0.993216">
472
</page>
<bodyText confidence="0.9802695">
same predicate.3 To cope with unique roles, we
add hard constraints of the kind
Ea I(p r → a E y) ≤ 1, Vp, Vr E Zuniq, (2)
where gZuniq is the set of unique roles. This set is
obtained from the training data by looking at the
roles that never occur multiple times in the gold
argument frames.4 The constraint above corre-
sponds to a ATMOSTONE factor, which is built-in
in AD3 and can be decoded in linear time (ren-
dering the runtime O(L2|9Zuniq|) when aggregat-
ing all such factors). These have also been used
by Das et al. (2012) in frame-semantic parsing.
</bodyText>
<subsectionHeader confidence="0.723184">
Grandparents, Arbitrary Siblings and Co-
</subsectionHeader>
<bodyText confidence="0.999975722222222">
parents. The second-order parts in the middle
row of Figure 2 all involve the simultaneous inclu-
sion of a pair of arcs, without further dependency
on the remaining arcs. We handle each of these
parts using a simple pairwise factor (called PAIR
in the AD3 toolkit). The total runtime to locally
decode these factors is O(L3).
Predicate Automata. To handle consecutive
siblings, we adapt the simple head automaton
model (Alshawi, 1996; Smith and Eisner, 2008;
Koo et al., 2010) to semantic parsing. We in-
troduce one automaton for each predicate p and
attachment direction (left or right). We describe
right-side predicate automata; their left-side coun-
terparts are analogous. Let (a0, a1, ... , ak+1) be
the sequence of right modifiers of p, with a0 =
START and ak+1 = END. Then, we have the fol-
lowing component capturing consecutive siblings:
</bodyText>
<equation confidence="0.993284">
CSIB
fp,→ (p → a1,. . . , p → ak) =
�k+1
j=1 σCSIB(p, aj−1, aj). (3)
</equation>
<bodyText confidence="0.991681111111111">
Maximizing fCSIB p,→ via dynamic programming has
a cost of O(L2), yielding O(L3) total runtime.
Argument Automata. For consecutive co-
parents, we introduce another automaton which is
analogous to the predicate automaton, but where
arrows are reversed. Let (p0, p1, . . . , pk+1) be
the sequence of right predicates that take a as
argument (the left-side case is analagous), with
p0 = START and pk+1 = END. We define:
</bodyText>
<equation confidence="0.97035675">
CCP
f,←(a ← p1, ...,a ← pk) =
�k+1
j=1 σCCP(a,pj−1,pj). (4)
</equation>
<footnote confidence="0.99768875">
3Such roles have been called “deterministic” by Flanigan
et al. (2014).
4For PAS, all 43 roles were found unique; for DM, this
number is 40 out of 52, and for PCEDT only 3 out of 69.
</footnote>
<bodyText confidence="0.831849">
The total runtime is also O(L3).
</bodyText>
<sectionHeader confidence="0.997408" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.93789675">
We define binary features for each part represented
in Figure 2. Most of the features are taken from
TurboParser (Martins et al., 2013), while others
are inspired by the semantic parser of Johansson
and Nugues (2008). Those features marked with †
require information from the dependency syntactic
parser, and are only used in the open track.5
Predicate Features. Our predicate features are:
</bodyText>
<listItem confidence="0.996979454545454">
• PREDWORD, PREDLEMMA, PREDPOS. Lexi-
cal form, lemma, and POS tag of the predicate.
• PREDREL.† Syntactic dependency relation be-
tween the predicate and its head.
• PREDHEADWORD/POS.† Form and POS tag
of the predicate syntactic head, conjoined with
the predicate word and POS tag.
• PREDMODWORD/POS/REL.† Form, POS tag,
and dependency relation of the predicate syn-
tactic dependents, conjoined with the predicate
word and POS tag.
</listItem>
<bodyText confidence="0.551849">
Arc Features. All features above, plus the fol-
lowing (conjoined with arc direction and label):
</bodyText>
<listItem confidence="0.997912619047619">
• ARGWORD, ARGLEMMA, ARGPOS. The lex-
ical form, lemma, and POS tag of the argument.
• ARGREL.† Syntactic dependency relation be-
tween the argument and its head.
• LEFTWORD/POS,† RIGHTWORD/POS.†
Form/POS tag of the leftmost/rightmost de-
pendent of the argument, conjoined with the
predicate word and POS tag.
• LEFTSIBWORD/POS,† RIGHTSIBWORD/POS.†
Form/POS tag of the left/right sibling of the
argument, conjoined with the predicate tag.
• PREDCONTEXTWORD, PREDCONTEXTPOS,
PREDCONTEXTLEMMA. Word, POS, and
lemma on the left and right context of the pred-
icate (context size is 2).
• PREDCONTEXTPOSBIGRAM/TRIGRAM. Bi-
gram and trigram of POS tags on the left and
right side of the predicate.
• PREDVOICE.† Predicate voice: active, passive,
or none. Determined from the syntactic depen-
dency tree as in Johansson and Nugues (2008).
</listItem>
<footnote confidence="0.997763">
5For the open track, the only external information used by
our system were the provided automatic dependency trees.
</footnote>
<page confidence="0.997998">
473
</page>
<listItem confidence="0.982964">
• PREDWORDARGWORD, PREDWORDARG-
POS, PREDPOSARGWORD, PREDPOSARG-
POS. Predicate word/tag conjoined with
argument word/tag.
• PREDARGPOSCONTEXT. Several features
conjoining the POS of words surrounding the
predicate and argument (similar to the contex-
tual features in McDonald et al. (2005)).
• EXACTARCLENGTH, BINNEDARCLENGTH.
Exact and binned arc length (distance between
predicate and argument), conjoined with the
predicate and argument POS tags.
• POSINBETWEEN, WORDINBETWEEN. POS
and forms between the predicate and argument,
conjoined with their own POS tags and forms.
• RELPATH,† POSPATH.† Path in the syntactic
dependency tree between the predicate and the
</listItem>
<bodyText confidence="0.96329775">
argument. The path is formed either by depen-
dency relations or by POS tags.
Second Order Features. These involve a pred-
icate, an argument, and a “companion word”
(which can be a second argument, in the case of
siblings, a second predicate, for co-parents, or the
argument of another argument, for grandparents).
In all cases, features are of the following kind:
</bodyText>
<listItem confidence="0.994888">
• POSTRIPLET. POS tags of the predicate, the
argument, and the companion word.
• UNILEXICAL. One word form (for the predi-
cate/argument/companion) and two POS tags.
• BILEXICAL. One POS tag (for the predi-
cate/argument/companion) and two word forms.
• PAIRWISE. Backed-off pair features for the
companion word form/POS tag and the word
form/POS of the predicate/argument.
</listItem>
<sectionHeader confidence="0.994308" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999890708333334">
All models were trained by running 10 epochs of
max-loss MIRA with C = 0.01 (Crammer et al.,
2006). The cost function takes into account mis-
matches between predicted and gold dependen-
cies, with a cost cP on labeled arcs incorrectly
predicted (false positives) and a cost cR on gold
labeled arcs that were missed (false negatives).
These values were set through cross-validation in
the dev set, yielding cP = 0.4 and cR = 0.6 in all
runs, except for the DM and PCEDT datasets in the
closed track, for which cP = 0.3 and cR = 0.7.
To speed up decoding, we discard arcs whose
posterior probability is below 10−4, according to a
probabilistic unlabeled first-order pruner. Table 1
shows a significant reduction of the search space
with a very small drop in recall.
Table 2 shows our final results in the test set,
for a model trained in the train and development
partitions. Our system achieved the best score in
the open track (an LF score of 86.27%, averaged
over DM, PAS, and PCEDT), and the second best in
the closed track, after the Peking team. Overall,
we observe that the precision and recall in PCEDT
are far below the other two formalisms, but this
difference is much smaller when looking at unla-
beled scores. Comparing the results in the closed
and open tracks, we observe a consistent improve-
ment in the three formalisms of around 1% in F1
from using syntactic information. While this con-
firms previous findings that syntactic features are
important in semantic role labeling (Toutanova et
al., 2005; Johansson and Nugues, 2008), these im-
provements are less striking than expected. We
conjecture this is due to the fact that our model in
the closed track already incorporates a variety of
contextual features which are nearly as informa-
tive as those extracted from the dependency trees.
Finally, to assess the importance of the second
order features, Table 3 reports experiments in the
dev-set that progressively add several groups of
features, along with runtimes. We can see that
siblings, co-parents, and grandparents all provide
valuable information that improves the final scores
(with the exception of the PCEDT labeled scores,
where the difference is negligible). This comes
at only a small cost in terms of runtime, which is
around 1,000 tokens per second for the full mod-
els.
</bodyText>
<table confidence="0.99811325">
UR # UA/tok LR # LA/tok
DM 99.33 3.5 (13.4%) 99.22 34.4 (2.5%)
PAS 99.53 3.3 (12.5%) 99.49 20.8 (1.9%)
PCEDT 99.03 2.1 (8.2%) 98.77 54.5 (3.0%)
</table>
<tableCaption confidence="0.767258333333333">
Table 1: Pruner statistics in the dev-set, for the
open track. Shown are oracle recall scores, consid-
ering both unlabeled (UR) and labeled arcs (LR);
</tableCaption>
<bodyText confidence="0.8716618">
and the averaged number of unlabeled and la-
beled arcs per token that remained after the prun-
ing stage (# UA/tok and # LA/tok). In brackets,
we show the fraction of unlabeled/labeled arcs that
survived the pruning.
</bodyText>
<page confidence="0.996544">
474
</page>
<table confidence="0.998679111111111">
UP UR UF LP LR LF
DM, closed 90.14 88.65 89.39 88.82 87.35 88.08
PAS, closed 93.18 91.12 92.14 91.95 89.92 90.93
PCEDT, closed 90.21 85.51 87.80 78.80 74.70 76.70
average,closed – – 89.77 – – 85.24
DM, open 91.41 89.26 90.32 90.23 88.11 89.16
PAS, open 93.62 92.01 92.81 92.56 90.97 91.76
PCEDT, open 91.58 86.61 89.03 80.14 75.79 77.90
average, open – – 90.72 – – 86.27
</table>
<tableCaption confidence="0.7848624">
Table 2: Submitted results for the closed and open
tracks. For comparison, the best-performing sys-
tem in the closed track (Peking) obtained averaged
UF and LF scores of 91.03% and 85.91%, respec-
tively.
</tableCaption>
<table confidence="0.9999158125">
UF LF Tok/sec
DM, arc-factored 89.90 88.96 1,681
DM, arc-factored, pruned 89.85 88.90 2,642
+siblings 90.34 89.34 1,838
+co-parents 90.80 89.76 1,073
+grandparent (full) 90.95 89.90 955
PAS, arc-factored 92.34 91.40 1,927
PAS, arc-factored, pruned 92.35 91.40 2,914
+siblings 92.45 91.45 2,106
+co-parents 92.71 91.71 1,104
+grandparent (full) 92.87 91.87 1,043
PCEDT, arc-factored 87.90 79.90 1,558
PCEDT, arc-factored, pruned 87.74 79.83 2,906
+siblings 88.46 79.98 2,066
+co-parents 90.17 79.90 1,531
+grandparent (full) 90.18 80.03 1,371
</table>
<tableCaption confidence="0.999372">
Table 3: Results in the dev-set for the open track,
</tableCaption>
<bodyText confidence="0.997996">
progressively adding several groups of features,
until the full model is obtained. We report un-
labeled/labeled F1 and parsing speeds in tokens
per second. Our speeds include the time necessary
for pruning, evaluating features, and decoding, as
measured on a Intel Core i7 processor @3.4 GHz.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999997727272727">
We have described a system for broad-coverage
semantic dependency parsing. Our system, which
is inspired by prior work in syntactic parsing, im-
plements a linear model with second-order fea-
tures, being able to model interactions between
siblings, grandparents and co-parents. We have
shown empirically that second-order features have
an impact in the final scores. Approximate de-
coding was performed via alternating directions
dual decomposition (AD3), yielding fast runtimes
of around 1,000 tokens per second.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998565">
We would like to thank the reviewers for
their helpful comments. This work was par-
tially supported by the EU/FEDER programme,
QREN/POR Lisboa (Portugal), under the Intelligo
project (contract 2012/24803) and by a FCT grant
PTDC/EEI-SII/2312/2012.
</bodyText>
<sectionHeader confidence="0.990005" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99911897826087">
Hiyan Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations. In
Proc. ofAnnual Meeting of the Association for Com-
putational Linguistics, pages 167–176.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc.
of the Empirical Methods in Natural Language Pro-
cessing, pages 1455–1465.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In International
Conference on Natural Language Learning, pages
957–961.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line Passive-Aggressive Algorithms. Journal of Ma-
chine Learning Research, 7:551–585.
Dipanjan Das, Andr´e F. T. Martins, and Noah A. Smith.
2012. An Exact Dual Decomposition Algorithm
for Shallow Semantic Parsing with Constraints. In
Proc. of First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pages 209–217.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A. Smith. 2014. A discrim-
inative graph-based parser for the abstract mean-
ing representation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1426–1436.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis with
PropBank and NomBank. International Conference
on Natural Language Learning, pages 183–187.
Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. MRF optimization via dual decompo-
sition: Message-passing revisited. In Proc. of In-
ternational Conference on Computer Vision, pages
1–8.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1–11.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proc. of Empirical Methods for Natu-
ral Language Processing, pages 1288–1298.
</reference>
<page confidence="0.987745">
475
</page>
<reference confidence="0.999843618181818">
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Morgan &amp; Claypool
Publishers.
Andr´e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise Integer Linear Programming Formu-
lations for Dependency Parsing. In Proc. of Annual
Meeting of the Association for Computational Lin-
guistics, pages 342–350.
Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing,
Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo.
2010. Turbo Parsers: Dependency Parsing by Ap-
proximate Variational Inference. In Proc. of Em-
pirical Methods for Natural Language Processing,
pages 34–44.
Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M´ario A. T. Figueiredo. 2011. Dual De-
composition with Many Overlapping Components.
In Proc. of Empirical Methods for Natural Language
Processing, pages 238–249.
Andr´e F. T. Martins, M´ario A. T. Figueiredo, Pedro
M. Q. Aguiar, Noah A. Smith, and Eric P. Xing.
2012. Alternating directions dual decomposition.
Arxiv preprint arXiv:1212.6550.
Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics, pages 617–622.
Andr´e F. T. Martins. 2014. AD3: A Fast Decoder
for Structured Prediction. In S. Nowozin, P. Gehler,
J. Jancsary, and C. Lampert, editors, Advanced
Structured Prediction. MIT Press, Cambridge, MA,
USA.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. of Annual Meeting of the
Association for Computational Linguistics, pages
91–98.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of International
Conference on Natural Language Learning, pages
216–220.
Alexander M. Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposi-
tion and linear programming relaxations for natural
language processing. In Proc. of Empirical Methods
for Natural Language Processing.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of Empirical
Methods for Natural Language Processing, pages
145–156.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In ACL, pages 589–596.
</reference>
<page confidence="0.999107">
476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.306344">
<title confidence="0.997579">Priberam: A Turbo Semantic Parser with Second Order Features</title>
<author confidence="0.68429">F T S C</author>
<note confidence="0.6264725">Labs, Alameda D. Afonso Henriques, 41, 1000-123 Lisboa, Portugal de Instituto Superior T´ecnico, 1049-001 Lisboa,</note>
<abstract confidence="0.998513076923077">This paper presents our contribution to the SemEval-2014 shared task on Broad- Coverage Semantic Dependency Parsing. We employ a feature-rich linear model, including scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual de- Our system achieved the top score in the open challenge, and the second highest score in the closed track.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
</authors>
<title>Head automata and bilingual tiling: Translation with minimal representations.</title>
<date>1996</date>
<booktitle>In Proc. ofAnnual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>167--176</pages>
<contexts>
<context position="8882" citStr="Alshawi, 1996" startWordPosition="1489" endWordPosition="1490">ering the runtime O(L2|9Zuniq|) when aggregating all such factors). These have also been used by Das et al. (2012) in frame-semantic parsing. Grandparents, Arbitrary Siblings and Coparents. The second-order parts in the middle row of Figure 2 all involve the simultaneous inclusion of a pair of arcs, without further dependency on the remaining arcs. We handle each of these parts using a simple pairwise factor (called PAIR in the AD3 toolkit). The total runtime to locally decode these factors is O(L3). Predicate Automata. To handle consecutive siblings, we adapt the simple head automaton model (Alshawi, 1996; Smith and Eisner, 2008; Koo et al., 2010) to semantic parsing. We introduce one automaton for each predicate p and attachment direction (left or right). We describe right-side predicate automata; their left-side counterparts are analogous. Let (a0, a1, ... , ak+1) be the sequence of right modifiers of p, with a0 = START and ak+1 = END. Then, we have the following component capturing consecutive siblings: CSIB fp,→ (p → a1,. . . , p → ak) = �k+1 j=1 σCSIB(p, aj−1, aj). (3) Maximizing fCSIB p,→ via dynamic programming has a cost of O(L2), yielding O(L3) total runtime. Argument Automata. For co</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>Hiyan Alshawi. 1996. Head automata and bilingual tiling: Translation with minimal representations. In Proc. ofAnnual Meeting of the Association for Computational Linguistics, pages 167–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of the Empirical Methods in Natural Language Processing,</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="1146" citStr="Bohnet and Nivre, 2012" startWordPosition="164" endWordPosition="167"> siblings, grandparents and co-parents). Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition (AD3). Our system achieved the top score in the open challenge, and the second highest score in the closed track. 1 Introduction The last decade saw a considerable progress in statistical modeling for dependency syntactic parsing (K¨ubler et al., 2009). Models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree. We extend a recently proposed dependency parser, TurboParser (Martins et al., 2010, 2013), to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT). The result is TurboSemanticParser, which we release as open-source software.1 We describe here a second order model for semantic parsing (§2). We follow prior work in semantic role labeling (Toutanova e</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proc. of the Empirical Methods in Natural Language Processing, pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In International Conference on Natural Language Learning,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="4512" citStr="Carreras, 2007" startWordPosition="763" endWordPosition="764"> p = 0 to L do 4: Initialize v ← vP (p), frame A(p) ← 0 5: for a = 1 to L do 6: Set r� ← arg maxr vLA(p r→ a) 7: if vA(p → a) + vLA(p r� → a) &gt; 0 then 8: A(p) ← A(p) ∪ {hp, a, r�i} 9: v ← v + vA(p → a) + vLA(p r�→ a) 10: end if 11: end for 12: if v &gt; 0 then set G ← G ∪ {hp, A(p)i} 13: end for 14: output: semantic graph G. scored semantic graph y given a sentence x: y = arg max f(x, y). (1) y∈Y(x) Our choice of parts is given in Figure 2. The second order parts are inspired by prior work in syntactic parsing, modeling interactions for pairs of (unlabeled) dependency arcs, such as grandparents (Carreras, 2007) and siblings (Smith and Eisner, 2008; Martins et al., 2009). The main novelty is co-parent parts, which, to the best of our knowledge, were never considered before, as they only make sense when multiple parents are allowed. If all parts were basic, decoding could be done independently for each predicate p, as illustrated in Algorithm 1. The total runtime, for a sentence with L words, is O(L2|9Z|), where 92, is the set of semantic roles. Adding consecutive siblings still permits independent decoding for each predicate, but dynamic programming is necessary to decode the best argument frame, inc</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In International Conference on Natural Language Learning, pages 957–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online Passive-Aggressive Algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="13435" citStr="Crammer et al., 2006" startWordPosition="2222" endWordPosition="2225">dicate, for co-parents, or the argument of another argument, for grandparents). In all cases, features are of the following kind: • POSTRIPLET. POS tags of the predicate, the argument, and the companion word. • UNILEXICAL. One word form (for the predicate/argument/companion) and two POS tags. • BILEXICAL. One POS tag (for the predicate/argument/companion) and two word forms. • PAIRWISE. Backed-off pair features for the companion word form/POS tag and the word form/POS of the predicate/argument. 5 Experimental Results All models were trained by running 10 epochs of max-loss MIRA with C = 0.01 (Crammer et al., 2006). The cost function takes into account mismatches between predicted and gold dependencies, with a cost cP on labeled arcs incorrectly predicted (false positives) and a cost cR on gold labeled arcs that were missed (false negatives). These values were set through cross-validation in the dev set, yielding cP = 0.4 and cR = 0.6 in all runs, except for the DM and PCEDT datasets in the closed track, for which cP = 0.3 and cR = 0.7. To speed up decoding, we discard arcs whose posterior probability is below 10−4, according to a probabilistic unlabeled first-order pruner. Table 1 shows a significant r</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints.</title>
<date>2012</date>
<booktitle>In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>209--217</pages>
<contexts>
<context position="2242" citStr="Das et al., 2012" startWordPosition="336" endWordPosition="339">escribe here a second order model for semantic parsing (§2). We follow prior work in semantic role labeling (Toutanova et al., 2005; JoThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://labs.priberam.com/Resources/ TurboSemanticParser Figure 1: Example of a semantic graph in the DM formalism (sentence #22006003). We treat top nodes as a special semantic role TOP whose predicate is a dummy root symbol. hansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we go beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. We formulate parsing as a global optimization problem and solve a relaxation through AD3, a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively (§3). Through a rich set of features (§4), we arrive at top accuracies at parsing speeds around 1,000 tokens per second, as described in th</context>
<context position="8383" citStr="Das et al. (2012)" startWordPosition="1408" endWordPosition="1411">ue Roles. We assume some roles are unique, i.e., they can occur at most once for the 472 same predicate.3 To cope with unique roles, we add hard constraints of the kind Ea I(p r → a E y) ≤ 1, Vp, Vr E Zuniq, (2) where gZuniq is the set of unique roles. This set is obtained from the training data by looking at the roles that never occur multiple times in the gold argument frames.4 The constraint above corresponds to a ATMOSTONE factor, which is built-in in AD3 and can be decoded in linear time (rendering the runtime O(L2|9Zuniq|) when aggregating all such factors). These have also been used by Das et al. (2012) in frame-semantic parsing. Grandparents, Arbitrary Siblings and Coparents. The second-order parts in the middle row of Figure 2 all involve the simultaneous inclusion of a pair of arcs, without further dependency on the remaining arcs. We handle each of these parts using a simple pairwise factor (called PAIR in the AD3 toolkit). The total runtime to locally decode these factors is O(L3). Predicate Automata. To handle consecutive siblings, we adapt the simple head automaton model (Alshawi, 1996; Smith and Eisner, 2008; Koo et al., 2010) to semantic parsing. We introduce one automaton for each </context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>Dipanjan Das, Andr´e F. T. Martins, and Noah A. Smith. 2012. An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints. In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM), pages 209–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1426--1436</pages>
<contexts>
<context position="2266" citStr="Flanigan et al., 2014" startWordPosition="340" endWordPosition="344">ond order model for semantic parsing (§2). We follow prior work in semantic role labeling (Toutanova et al., 2005; JoThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://labs.priberam.com/Resources/ TurboSemanticParser Figure 1: Example of a semantic graph in the DM formalism (sentence #22006003). We treat top nodes as a special semantic role TOP whose predicate is a dummy root symbol. hansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we go beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. We formulate parsing as a global optimization problem and solve a relaxation through AD3, a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively (§3). Through a rich set of features (§4), we arrive at top accuracies at parsing speeds around 1,000 tokens per second, as described in the experimental section (</context>
<context position="9907" citStr="Flanigan et al. (2014)" startWordPosition="1668" endWordPosition="1671">lings: CSIB fp,→ (p → a1,. . . , p → ak) = �k+1 j=1 σCSIB(p, aj−1, aj). (3) Maximizing fCSIB p,→ via dynamic programming has a cost of O(L2), yielding O(L3) total runtime. Argument Automata. For consecutive coparents, we introduce another automaton which is analogous to the predicate automaton, but where arrows are reversed. Let (p0, p1, . . . , pk+1) be the sequence of right predicates that take a as argument (the left-side case is analagous), with p0 = START and pk+1 = END. We define: CCP f,←(a ← p1, ...,a ← pk) = �k+1 j=1 σCCP(a,pj−1,pj). (4) 3Such roles have been called “deterministic” by Flanigan et al. (2014). 4For PAS, all 43 roles were found unique; for DM, this number is 40 out of 52, and for PCEDT only 3 out of 69. The total runtime is also O(L3). 4 Features We define binary features for each part represented in Figure 2. Most of the features are taken from TurboParser (Martins et al., 2013), while others are inspired by the semantic parser of Johansson and Nugues (2008). Those features marked with † require information from the dependency syntactic parser, and are only used in the open track.5 Predicate Features. Our predicate features are: • PREDWORD, PREDLEMMA, PREDPOS. Lexical form, lemma,</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 1426–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<date>2008</date>
<booktitle>Dependency-based syntactic–semantic analysis with PropBank and NomBank. International Conference on Natural Language Learning,</booktitle>
<pages>183--187</pages>
<contexts>
<context position="10280" citStr="Johansson and Nugues (2008)" startWordPosition="1737" endWordPosition="1740">e of right predicates that take a as argument (the left-side case is analagous), with p0 = START and pk+1 = END. We define: CCP f,←(a ← p1, ...,a ← pk) = �k+1 j=1 σCCP(a,pj−1,pj). (4) 3Such roles have been called “deterministic” by Flanigan et al. (2014). 4For PAS, all 43 roles were found unique; for DM, this number is 40 out of 52, and for PCEDT only 3 out of 69. The total runtime is also O(L3). 4 Features We define binary features for each part represented in Figure 2. Most of the features are taken from TurboParser (Martins et al., 2013), while others are inspired by the semantic parser of Johansson and Nugues (2008). Those features marked with † require information from the dependency syntactic parser, and are only used in the open track.5 Predicate Features. Our predicate features are: • PREDWORD, PREDLEMMA, PREDPOS. Lexical form, lemma, and POS tag of the predicate. • PREDREL.† Syntactic dependency relation between the predicate and its head. • PREDHEADWORD/POS.† Form and POS tag of the predicate syntactic head, conjoined with the predicate word and POS tag. • PREDMODWORD/POS/REL.† Form, POS tag, and dependency relation of the predicate syntactic dependents, conjoined with the predicate word and POS ta</context>
<context position="11798" citStr="Johansson and Nugues (2008)" startWordPosition="1971" endWordPosition="1974">orm/POS tag of the leftmost/rightmost dependent of the argument, conjoined with the predicate word and POS tag. • LEFTSIBWORD/POS,† RIGHTSIBWORD/POS.† Form/POS tag of the left/right sibling of the argument, conjoined with the predicate tag. • PREDCONTEXTWORD, PREDCONTEXTPOS, PREDCONTEXTLEMMA. Word, POS, and lemma on the left and right context of the predicate (context size is 2). • PREDCONTEXTPOSBIGRAM/TRIGRAM. Bigram and trigram of POS tags on the left and right side of the predicate. • PREDVOICE.† Predicate voice: active, passive, or none. Determined from the syntactic dependency tree as in Johansson and Nugues (2008). 5For the open track, the only external information used by our system were the provided automatic dependency trees. 473 • PREDWORDARGWORD, PREDWORDARGPOS, PREDPOSARGWORD, PREDPOSARGPOS. Predicate word/tag conjoined with argument word/tag. • PREDARGPOSCONTEXT. Several features conjoining the POS of words surrounding the predicate and argument (similar to the contextual features in McDonald et al. (2005)). • EXACTARCLENGTH, BINNEDARCLENGTH. Exact and binned arc length (distance between predicate and argument), conjoined with the predicate and argument POS tags. • POSINBETWEEN, WORDINBETWEEN. P</context>
<context position="14873" citStr="Johansson and Nugues, 2008" startWordPosition="2471" endWordPosition="2474">the open track (an LF score of 86.27%, averaged over DM, PAS, and PCEDT), and the second best in the closed track, after the Peking team. Overall, we observe that the precision and recall in PCEDT are far below the other two formalisms, but this difference is much smaller when looking at unlabeled scores. Comparing the results in the closed and open tracks, we observe a consistent improvement in the three formalisms of around 1% in F1 from using syntactic information. While this confirms previous findings that syntactic features are important in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008), these improvements are less striking than expected. We conjecture this is due to the fact that our model in the closed track already incorporates a variety of contextual features which are nearly as informative as those extracted from the dependency trees. Finally, to assess the importance of the second order features, Table 3 reports experiments in the dev-set that progressively add several groups of features, along with runtimes. We can see that siblings, co-parents, and grandparents all provide valuable information that improves the final scores (with the exception of the PCEDT labeled sc</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. International Conference on Natural Language Learning, pages 183–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
<author>Nikos Paragios</author>
<author>Georgios Tziritas</author>
</authors>
<title>MRF optimization via dual decomposition: Message-passing revisited.</title>
<date>2007</date>
<booktitle>In Proc. of International Conference on Computer Vision,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6569" citStr="Komodakis et al., 2007" startWordPosition="1097" endWordPosition="1101">this number is small. Figure 2: Parts considered in this paper. The top row illustrate the basic parts, representing the event that a word is a predicate, or the existence of an arc between a predicate and an argument, eventually labeled with a semantic role. Our secondorder model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). We employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus, achieving a faster convergence rate both in theory and in practice (Martins et al., 2012, 2013). For several factors (such as logic factors representing AND, OR and XOR constraints, budget constraints, and binary pairwise factors), </context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. MRF optimization via dual decomposition: Message-passing revisited. In Proc. of International Conference on Computer Vision, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1122" citStr="Koo and Collins, 2010" startWordPosition="160" endWordPosition="163">der dependencies (arcs, siblings, grandparents and co-parents). Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition (AD3). Our system achieved the top score in the open challenge, and the second highest score in the closed track. 1 Introduction The last decade saw a considerable progress in statistical modeling for dependency syntactic parsing (K¨ubler et al., 2009). Models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree. We extend a recently proposed dependency parser, TurboParser (Martins et al., 2010, 2013), to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT). The result is TurboSemanticParser, which we release as open-source software.1 We describe here a second order model for semantic parsing (§2). We follow prior work in semantic ro</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proc. of Annual Meeting of the Association for Computational Linguistics, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing,</booktitle>
<pages>1288--1298</pages>
<contexts>
<context position="8925" citStr="Koo et al., 2010" startWordPosition="1495" endWordPosition="1498">gregating all such factors). These have also been used by Das et al. (2012) in frame-semantic parsing. Grandparents, Arbitrary Siblings and Coparents. The second-order parts in the middle row of Figure 2 all involve the simultaneous inclusion of a pair of arcs, without further dependency on the remaining arcs. We handle each of these parts using a simple pairwise factor (called PAIR in the AD3 toolkit). The total runtime to locally decode these factors is O(L3). Predicate Automata. To handle consecutive siblings, we adapt the simple head automaton model (Alshawi, 1996; Smith and Eisner, 2008; Koo et al., 2010) to semantic parsing. We introduce one automaton for each predicate p and attachment direction (left or right). We describe right-side predicate automata; their left-side counterparts are analogous. Let (a0, a1, ... , ak+1) be the sequence of right modifiers of p, with a0 = START and ak+1 = END. Then, we have the following component capturing consecutive siblings: CSIB fp,→ (p → a1,. . . , p → ak) = �k+1 j=1 σCSIB(p, aj−1, aj). (3) Maximizing fCSIB p,→ via dynamic programming has a cost of O(L2), yielding O(L3) total runtime. Argument Automata. For consecutive coparents, we introduce another a</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proc. of Empirical Methods for Natural Language Processing, pages 1288–1298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Dependency parsing.</title>
<date>2009</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency parsing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise Integer Linear Programming Formulations for Dependency Parsing.</title>
<date>2009</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>342--350</pages>
<contexts>
<context position="1168" citStr="Martins et al., 2009" startWordPosition="168" endWordPosition="171">and co-parents). Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition (AD3). Our system achieved the top score in the open challenge, and the second highest score in the closed track. 1 Introduction The last decade saw a considerable progress in statistical modeling for dependency syntactic parsing (K¨ubler et al., 2009). Models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree. We extend a recently proposed dependency parser, TurboParser (Martins et al., 2010, 2013), to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT). The result is TurboSemanticParser, which we release as open-source software.1 We describe here a second order model for semantic parsing (§2). We follow prior work in semantic role labeling (Toutanova et al., 2005; JoThis wo</context>
<context position="4572" citStr="Martins et al., 2009" startWordPosition="772" endWordPosition="775">0 5: for a = 1 to L do 6: Set r� ← arg maxr vLA(p r→ a) 7: if vA(p → a) + vLA(p r� → a) &gt; 0 then 8: A(p) ← A(p) ∪ {hp, a, r�i} 9: v ← v + vA(p → a) + vLA(p r�→ a) 10: end if 11: end for 12: if v &gt; 0 then set G ← G ∪ {hp, A(p)i} 13: end for 14: output: semantic graph G. scored semantic graph y given a sentence x: y = arg max f(x, y). (1) y∈Y(x) Our choice of parts is given in Figure 2. The second order parts are inspired by prior work in syntactic parsing, modeling interactions for pairs of (unlabeled) dependency arcs, such as grandparents (Carreras, 2007) and siblings (Smith and Eisner, 2008; Martins et al., 2009). The main novelty is co-parent parts, which, to the best of our knowledge, were never considered before, as they only make sense when multiple parents are allowed. If all parts were basic, decoding could be done independently for each predicate p, as illustrated in Algorithm 1. The total runtime, for a sentence with L words, is O(L2|9Z|), where 92, is the set of semantic roles. Adding consecutive siblings still permits independent decoding for each predicate, but dynamic programming is necessary to decode the best argument frame, increasing the runtime to O(L3|9Z|). The addition of consecutiv</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise Integer Linear Programming Formulations for Dependency Parsing. In Proc. of Annual Meeting of the Association for Computational Linguistics, pages 342–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Turbo Parsers: Dependency Parsing by Approximate Variational Inference.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing,</booktitle>
<pages>34--44</pages>
<contexts>
<context position="1409" citStr="Martins et al., 2010" startWordPosition="207" endWordPosition="210">d track. 1 Introduction The last decade saw a considerable progress in statistical modeling for dependency syntactic parsing (K¨ubler et al., 2009). Models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree. We extend a recently proposed dependency parser, TurboParser (Martins et al., 2010, 2013), to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT). The result is TurboSemanticParser, which we release as open-source software.1 We describe here a second order model for semantic parsing (§2). We follow prior work in semantic role labeling (Toutanova et al., 2005; JoThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://labs.priberam.com/Resources/</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2010. Turbo Parsers: Dependency Parsing by Approximate Variational Inference. In Proc. of Empirical Methods for Natural Language Processing, pages 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Dual Decomposition with Many Overlapping Components.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing,</booktitle>
<pages>238--249</pages>
<contexts>
<context position="6670" citStr="Martins et al., 2011" startWordPosition="1114" endWordPosition="1117">, representing the event that a word is a predicate, or the existence of an arc between a predicate and an argument, eventually labeled with a semantic role. Our secondorder model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). We employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus, achieving a faster convergence rate both in theory and in practice (Martins et al., 2012, 2013). For several factors (such as logic factors representing AND, OR and XOR constraints, budget constraints, and binary pairwise factors), these quadratic subproblems can be solved efficiently. For dense or structured factors, the quadratic</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2011. Dual Decomposition with Many Overlapping Components. In Proc. of Empirical Methods for Natural Language Processing, pages 238–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>M´ario A T Figueiredo</author>
<author>Pedro M Q Aguiar</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Alternating directions dual decomposition. Arxiv preprint arXiv:1212.6550.</title>
<date>2012</date>
<contexts>
<context position="7025" citStr="Martins et al., 2012" startWordPosition="1171" endWordPosition="1174">wo. dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). We employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus, achieving a faster convergence rate both in theory and in practice (Martins et al., 2012, 2013). For several factors (such as logic factors representing AND, OR and XOR constraints, budget constraints, and binary pairwise factors), these quadratic subproblems can be solved efficiently. For dense or structured factors, the quadratic subproblems can be solved as a sequence of local Viterbi decoding steps, via an active set method (Martins, 2014); this local decoding operation is the same that needs to be performed in the subgradient algorithm. We describe these subproblems in detail in the next section. 3 Solving the Subproblems Predicate and Arc-Factored Parts. We capture all the </context>
</contexts>
<marker>Martins, Figueiredo, Aguiar, Smith, Xing, 2012</marker>
<rawString>Andr´e F. T. Martins, M´ario A. T. Figueiredo, Pedro M. Q. Aguiar, Noah A. Smith, and Eric P. Xing. 2012. Alternating directions dual decomposition. Arxiv preprint arXiv:1212.6550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Miguel B Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>617--622</pages>
<contexts>
<context position="10199" citStr="Martins et al., 2013" startWordPosition="1724" endWordPosition="1727">n, but where arrows are reversed. Let (p0, p1, . . . , pk+1) be the sequence of right predicates that take a as argument (the left-side case is analagous), with p0 = START and pk+1 = END. We define: CCP f,←(a ← p1, ...,a ← pk) = �k+1 j=1 σCCP(a,pj−1,pj). (4) 3Such roles have been called “deterministic” by Flanigan et al. (2014). 4For PAS, all 43 roles were found unique; for DM, this number is 40 out of 52, and for PCEDT only 3 out of 69. The total runtime is also O(L3). 4 Features We define binary features for each part represented in Figure 2. Most of the features are taken from TurboParser (Martins et al., 2013), while others are inspired by the semantic parser of Johansson and Nugues (2008). Those features marked with † require information from the dependency syntactic parser, and are only used in the open track.5 Predicate Features. Our predicate features are: • PREDWORD, PREDLEMMA, PREDPOS. Lexical form, lemma, and POS tag of the predicate. • PREDREL.† Syntactic dependency relation between the predicate and its head. • PREDHEADWORD/POS.† Form and POS tag of the predicate syntactic head, conjoined with the predicate word and POS tag. • PREDMODWORD/POS/REL.† Form, POS tag, and dependency relation of</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e F. T. Martins, Miguel B. Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 617–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
</authors>
<title>AD3: A Fast Decoder for Structured Prediction.</title>
<date>2014</date>
<booktitle>Advanced Structured Prediction.</booktitle>
<editor>In S. Nowozin, P. Gehler, J. Jancsary, and C. Lampert, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="7384" citStr="Martins, 2014" startWordPosition="1228" endWordPosition="1229">ubproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus, achieving a faster convergence rate both in theory and in practice (Martins et al., 2012, 2013). For several factors (such as logic factors representing AND, OR and XOR constraints, budget constraints, and binary pairwise factors), these quadratic subproblems can be solved efficiently. For dense or structured factors, the quadratic subproblems can be solved as a sequence of local Viterbi decoding steps, via an active set method (Martins, 2014); this local decoding operation is the same that needs to be performed in the subgradient algorithm. We describe these subproblems in detail in the next section. 3 Solving the Subproblems Predicate and Arc-Factored Parts. We capture all the basic parts with a single component. As stated in §2, local decoding in this component has a runtime of O(L2|9Z|), by using Algorithm 1. Unique Roles. We assume some roles are unique, i.e., they can occur at most once for the 472 same predicate.3 To cope with unique roles, we add hard constraints of the kind Ea I(p r → a E y) ≤ 1, Vp, Vr E Zuniq, (2) where </context>
</contexts>
<marker>Martins, 2014</marker>
<rawString>Andr´e F. T. Martins. 2014. AD3: A Fast Decoder for Structured Prediction. In S. Nowozin, P. Gehler, J. Jancsary, and C. Lampert, editors, Advanced Structured Prediction. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="12205" citStr="McDonald et al. (2005)" startWordPosition="2028" endWordPosition="2031">M. Bigram and trigram of POS tags on the left and right side of the predicate. • PREDVOICE.† Predicate voice: active, passive, or none. Determined from the syntactic dependency tree as in Johansson and Nugues (2008). 5For the open track, the only external information used by our system were the provided automatic dependency trees. 473 • PREDWORDARGWORD, PREDWORDARGPOS, PREDPOSARGWORD, PREDPOSARGPOS. Predicate word/tag conjoined with argument word/tag. • PREDARGPOSCONTEXT. Several features conjoining the POS of words surrounding the predicate and argument (similar to the contextual features in McDonald et al. (2005)). • EXACTARCLENGTH, BINNEDARCLENGTH. Exact and binned arc length (distance between predicate and argument), conjoined with the predicate and argument POS tags. • POSINBETWEEN, WORDINBETWEEN. POS and forms between the predicate and argument, conjoined with their own POS tags and forms. • RELPATH,† POSPATH.† Path in the syntactic dependency tree between the predicate and the argument. The path is formed either by dependency relations or by POS tags. Second Order Features. These involve a predicate, an argument, and a “companion word” (which can be a second argument, in the case of siblings, a s</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proc. of Annual Meeting of the Association for Computational Linguistics, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proc. of International Conference on Natural Language Learning,</booktitle>
<pages>216--220</pages>
<contexts>
<context position="1099" citStr="McDonald et al., 2006" startWordPosition="156" endWordPosition="159">for first and second-order dependencies (arcs, siblings, grandparents and co-parents). Decoding is performed in a global manner by solving a linear relaxation with alternating directions dual decomposition (AD3). Our system achieved the top score in the open challenge, and the second highest score in the closed track. 1 Introduction The last decade saw a considerable progress in statistical modeling for dependency syntactic parsing (K¨ubler et al., 2009). Models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree. We extend a recently proposed dependency parser, TurboParser (Martins et al., 2010, 2013), to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT). The result is TurboSemanticParser, which we release as open-source software.1 We describe here a second order model for semantic parsing (§2). We follow pr</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proc. of International Conference on Natural Language Learning, pages 216–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="6589" citStr="Rush et al., 2010" startWordPosition="1102" endWordPosition="1105">gure 2: Parts considered in this paper. The top row illustrate the basic parts, representing the event that a word is a predicate, or the existence of an arc between a predicate and an argument, eventually labeled with a semantic role. Our secondorder model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. dual decomposition, a class of optimization techniques that tackle the dual of combinatorial problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). We employ alternating directions dual decomposition (AD3; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus, achieving a faster convergence rate both in theory and in practice (Martins et al., 2012, 2013). For several factors (such as logic factors representing AND, OR and XOR constraints, budget constraints, and binary pairwise factors), these quadratic subp</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proc. of Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing,</booktitle>
<pages>145--156</pages>
<contexts>
<context position="4549" citStr="Smith and Eisner, 2008" startWordPosition="767" endWordPosition="771"> ← vP (p), frame A(p) ← 0 5: for a = 1 to L do 6: Set r� ← arg maxr vLA(p r→ a) 7: if vA(p → a) + vLA(p r� → a) &gt; 0 then 8: A(p) ← A(p) ∪ {hp, a, r�i} 9: v ← v + vA(p → a) + vLA(p r�→ a) 10: end if 11: end for 12: if v &gt; 0 then set G ← G ∪ {hp, A(p)i} 13: end for 14: output: semantic graph G. scored semantic graph y given a sentence x: y = arg max f(x, y). (1) y∈Y(x) Our choice of parts is given in Figure 2. The second order parts are inspired by prior work in syntactic parsing, modeling interactions for pairs of (unlabeled) dependency arcs, such as grandparents (Carreras, 2007) and siblings (Smith and Eisner, 2008; Martins et al., 2009). The main novelty is co-parent parts, which, to the best of our knowledge, were never considered before, as they only make sense when multiple parents are allowed. If all parts were basic, decoding could be done independently for each predicate p, as illustrated in Algorithm 1. The total runtime, for a sentence with L words, is O(L2|9Z|), where 92, is the set of semantic roles. Adding consecutive siblings still permits independent decoding for each predicate, but dynamic programming is necessary to decode the best argument frame, increasing the runtime to O(L3|9Z|). The</context>
<context position="8906" citStr="Smith and Eisner, 2008" startWordPosition="1491" endWordPosition="1494">me O(L2|9Zuniq|) when aggregating all such factors). These have also been used by Das et al. (2012) in frame-semantic parsing. Grandparents, Arbitrary Siblings and Coparents. The second-order parts in the middle row of Figure 2 all involve the simultaneous inclusion of a pair of arcs, without further dependency on the remaining arcs. We handle each of these parts using a simple pairwise factor (called PAIR in the AD3 toolkit). The total runtime to locally decode these factors is O(L3). Predicate Automata. To handle consecutive siblings, we adapt the simple head automaton model (Alshawi, 1996; Smith and Eisner, 2008; Koo et al., 2010) to semantic parsing. We introduce one automaton for each predicate p and attachment direction (left or right). We describe right-side predicate automata; their left-side counterparts are analogous. Let (a0, a1, ... , ak+1) be the sequence of right modifiers of p, with a0 = START and ak+1 = END. Then, we have the following component capturing consecutive siblings: CSIB fp,→ (p → a1,. . . , p → ak) = �k+1 j=1 σCSIB(p, aj−1, aj). (3) Maximizing fCSIB p,→ via dynamic programming has a cost of O(L2), yielding O(L3) total runtime. Argument Automata. For consecutive coparents, we </context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proc. of Empirical Methods for Natural Language Processing, pages 145–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>589--596</pages>
<contexts>
<context position="1757" citStr="Toutanova et al., 2005" startWordPosition="267" endWordPosition="270">Nivre, 2012; Martins et al., 2009, 2013). This paper applies the same rationale to semantic dependency parsing, in which the output variable is a semantic graph, rather than a syntactic tree. We extend a recently proposed dependency parser, TurboParser (Martins et al., 2010, 2013), to be able to perform semantic parsing using any of the three formalisms considered in this shared task (DM, PAS, and PCEDT). The result is TurboSemanticParser, which we release as open-source software.1 We describe here a second order model for semantic parsing (§2). We follow prior work in semantic role labeling (Toutanova et al., 2005; JoThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1http://labs.priberam.com/Resources/ TurboSemanticParser Figure 1: Example of a semantic graph in the DM formalism (sentence #22006003). We treat top nodes as a special semantic role TOP whose predicate is a dummy root symbol. hansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; ho</context>
<context position="14844" citStr="Toutanova et al., 2005" startWordPosition="2467" endWordPosition="2470">ieved the best score in the open track (an LF score of 86.27%, averaged over DM, PAS, and PCEDT), and the second best in the closed track, after the Peking team. Overall, we observe that the precision and recall in PCEDT are far below the other two formalisms, but this difference is much smaller when looking at unlabeled scores. Comparing the results in the closed and open tracks, we observe a consistent improvement in the three formalisms of around 1% in F1 from using syntactic information. While this confirms previous findings that syntactic features are important in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008), these improvements are less striking than expected. We conjecture this is due to the fact that our model in the closed track already incorporates a variety of contextual features which are nearly as informative as those extracted from the dependency trees. Finally, to assess the importance of the second order features, Table 3 reports experiments in the dev-set that progressively add several groups of features, along with runtimes. We can see that siblings, co-parents, and grandparents all provide valuable information that improves the final scores (with the exce</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher Manning. 2005. Joint learning improves semantic role labeling. In ACL, pages 589–596.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>