<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021496">
<title confidence="0.9970655">
Annotating Large Email Datasets for Named Entity Recognition with
Mechanical Turk
</title>
<author confidence="0.994">
Nolan Lawson, Kevin Eustice, Meliha Yetisgen-Yildiz
Mike Perkowitz
</author>
<affiliation confidence="0.990008">
Kiha Software Biomedical and Health Informatics
</affiliation>
<address confidence="0.877601">
100 South King Street, Suite 320 University of Washington
Seattle, WA 98104 Seattle, WA 98101
</address>
<email confidence="0.999678">
{nolan,kevin,mikep}@kiha.com melihay@u.washington.edu
</email>
<sectionHeader confidence="0.994601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.980749384615385">
Amazon&apos;s Mechanical Turk service has been
successfully applied to many natural language
processing tasks. However, the task of named
entity recognition presents unique challenges.
In a large annotation task involving over
20,000 emails, we demonstrate that a compet-
itive bonus system and inter-annotator agree-
ment can be used to improve the quality of
named entity annotations from Mechanical
Turk. We also build several statistical named
entity recognition models trained with these
annotations, which compare favorably to sim-
ilar models trained on expert annotations.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998668615384615">
It is well known that the performance of many
machine learning systems is heavily determined
by the size and quality of the data used as input
to the training algorithms. Additionally, for cer-
tain applications in natural language processing
(NLP), it has been noted that the particular al-
gorithms or feature sets used tend to become ir-
relevant as the size of the corpus increases
(Banko and Brill 2001). It is therefore not sur-
prising that obtaining large annotated datasets is
an issue of great practical importance for the
working researcher. Traditionally, annotated
training data have been provided by experts in
the field or the researchers themselves, often at
great costs in terms of time and money. Re-
cently, however, attempts have been made to
leverage non-expert annotations provided by
Amazon&apos;s Mechanical Turk (MTurk) service to
create large training corpora at a fraction of the
usual costs (Snow et al. 2008). The initial results
seem promising, and a new avenue for enhancing
existing sources of annotated data appears to
have been opened.
Named entity recognition (NER) is one of the
many fields of NLP that rely on machine learn-
ing methods, and therefore large training cor-
</bodyText>
<page confidence="0.994297">
71
</page>
<bodyText confidence="0.999947404761905">
pora. Indeed, it is a field where more is almost
always better, as indicated by the traditional use
of named entity gazetteers (often culled from ex-
ternal sources) to simulate data that would have
been inferred from a larger training set (Minkov
et al. 2005; Mikheev et al. 1999). Therefore, it
appears to be a field that could profit from the
enormous bargain-price workforce available
through MTurk.
It is not immediately obvious, though, that
MTurk is well-suited for the task of NER annota-
tion. Commonly, MTurk has been used for the
classification task (Snow et al. 2008) or for
straightforward data entry. However, NER does
not fit well into either of these formats. As poin-
ted out by Kozareva (2006), NER can be thought
of as a composition of two subtasks: 1) determin-
ing the start and end boundaries of a textual en-
tity, and 2) determining the label of the identified
span. The second task is the well-understood
classification task, but the first task presents
subtler problems. One is that MTurk&apos;s form-
based user interface is inappropriate for the task
of identifying textual spans. Another problem is
that MTurk&apos;s fixed-fee payment system encour-
ages low recall on the part of the annotators,
since they receive the same pay no matter how
many entities they identify.
This paper addresses both of these problems
by describing a custom user interface and com-
petitive payment system that together create a
fluid user experience while encouraging high-
quality annotations. Further, we demonstrate
that MTurk successfully scales to the task of an-
notating a very large set of documents (over
20,000), with each document annotated by mul-
tiple workers. We also present a system for
resolving inter-annotator conflicts to create the
final training corpus, and determine the ideal
agreement threshold to maximize precision and
recall of a statistical named entity recognition
model. Finally, we demonstrate that a model
</bodyText>
<note confidence="0.8941795">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 71–79,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.997492156862745">
trained on our corpus is on par with one trained Previous works have tackled NER within the
from expert annotations, when applied to a biomedical domain (Settles 2004), newswire do-
labeled test set. main (Grishman and Sundheim 1996), and email
2 Related Work domain (Minkov et al. 2005). In this paper, we
Mechanical Turk is a virtual market in which any focus on extracting entities from email text.
requester can post tasks that are simple for hu- It should be noted that email text has many
mans but difficult for computers. MTurk has distinctive features that create a unique challenge
been adopted for a variety of uses both in in- when applying NER. For one, email text tends to
dustry and academia from user studies (Kittur et be more informal than either newswire or bio-
al. 2008) to image labeling (Sorokin and Forsyth medical text, which reduces the usefulness of
2008). In March 2007, Amazon claimed the user learned features that depend on patterns of capit-
base of MTurk consisted of over 100,000 users alization and spelling. Also, the choice of cor-
from 100 countries (Pontin 2007). pora in email text is particularly important. As
In the scope of this paper, we examine the email corpora tend to come from either a single
feasibility of MTurk in creating large-scale cor- company (e.g., the Enron Email Dataset1) or a
pora for training statistical named entity recogni- small group of people (e.g., the Sarah Palin email
tion models. However, our work was not the first set2), it is easy to build a classifier that overfits
application of MTurk in the NLP domain. Snow the data. For instance, a classifier trained to ex-
et al. (2008) examined the quality of labels cre- tract personal names from Enron emails might
ated by MTurk workers for various NLP tasks in- show an especially high preference to words such
cluding word sense disambiguation, word simil- as “White,” “Lay,” and “Germany,” because they
arity, text entailment, and temporal ordering. correspond to the names of Enron employees.
Since the publication of Snow et al.’s paper, Within the newswire and biomedical domains,
MTurk has become increasingly popular as an such overfitting may be benign or actually bene-
annotation tool for NLP research. Examples in- ficial, since documents in those domains tend to
clude Nakov’s work on creating a manually an- deal with a relatively small and pre-determined
notated resource for noun-noun compound inter- set of named entities (e.g., politicians and large
pretation based on paraphrasing verbs by MTurk corporations for newswire text, gene and protein
(Nakov 2008) and Callison-Burch’s machine names for biomedical text). For NER in the
translation evaluation study with MTurk (Callis- email domain, however, such overfitting is unac-
on-Burch 2009). In contrast to the existing re- ceptable. The personal nature of emails ensures
search, we both evaluated the quality of corpora that they will almost always contain references to
generated by MTurk in different named entity re- people, places, and organizations not covered by
cognition tasks and explored ways to motivate the training data. Therefore, for the classifier to
the workers to do higher quality work. We be- be useful on any spontaneous piece of email text,
lieve the experiences we present in this paper a large, heterogeneous training set is desired.
will contribute greatly to other researchers as To achieve this effect, we chose four different
they design similar large-scale annotation tasks. sources of unlabeled email text to be annotated
3 General Problem Definition by the Mechanical Turk workers for input into
Named entity recognition (NER) is a well-known the training algorithms:
subtask of information extraction. Traditionally, 1. The Enron Email Dataset.
the task has been based on identifying words and 2. The 2005 TREC Public Spam Corpus
phrases that refer to various entities of interest, (non-spam only).3
including persons, locations, and organizations, 3. The 20 Newsgroups Dataset.4
(Nadeau and Sekine 2007). The problem is usu- 4. A private mailing list for synthesizer afi-
ally posed as a sequence labeling task similar to cionados called “Analogue Heaven.”
the part-of-speech (POS) tagging or phrase- 4 Mechanical Turk for NER
chunking tasks, where each token in the input As described previously, MTurk is not explicitly
text corresponds to a label in the output, and is designed for NER tasks. Because of this, we de-
solved with sequential classification algorithms
(such as CRF, SVMCMM, or MEMM).
</bodyText>
<page confidence="0.376154">
72
</page>
<footnote confidence="0.948205">
1 http://www.cs.cmu.edu/~enron/
2 http://palinemail.crivellawest.net/
3 http://plg.uwaterloo.ca/~gvcormac/treccorpus/
4 http://people.csail.mit.edu/jrennie/20Newsgroups/
</footnote>
<figureCaption confidence="0.999307">
Fig. 1: Sample of the interface presented to workers.
</figureCaption>
<bodyText confidence="0.999917428571429">
cided to build a custom user interface and bonus
payment system that largely circumvents the de-
fault MTurk web interface and instead performs
its operations through the MTurk Command Line
Tools.5 Additionally, we built a separate set of
tools designed to determine the ideal number of
workers to assign per email.
</bodyText>
<subsectionHeader confidence="0.994675">
4.1 User Interface
</subsectionHeader>
<bodyText confidence="0.998748689655172">
In order to adapt the task of NER annotation to
the Mechanical Turk format, we developed a
web-based graphical user interface using JavaS-
cript that allowed the user to select a span of text
with the mouse cursor and choose different cat-
egories of entities from a dropdown menu. The
interface also used simple tokenization heuristics
to divide the text into highlightable spans and re-
solve partial overlaps or double-clicks into the
next largest span. For instance, highlighting the
word “Mary” from “M” to “r” would result in the
entire word being selected.
Each Human Intelligence Task (or HIT, a unit
of payable work in the Mechanical Turk system)
presented the entire subject and body of an email
from one of the four corpora. To keep the HITs
at a reasonable size, emails with bodies having
less than 60 characters or more than 900 charac-
ters were omitted. The average email length, in-
cluding both subject and body, was 405.39 char-
acters.
For the labeling task, we chose three distinct
entity types to identify: PERSON, LOCATION, and
ORGANIZATION. To reduce potential worker confu-
sion and make the task size smaller, we also
broke up each individual HIT by entity type, so
the user only had to concentrate on one at a time.
For the PERSON and LOCATION entity types, we
noticed during initial tests that there was a user
</bodyText>
<footnote confidence="0.584569">
5 http://mturkclt.sourceforge.net
</footnote>
<bodyText confidence="0.999929405405406">
tendency to conflate unnamed references (such
as “my mom” and “your house”) with true
named references. Because NER is intended to
be limited only to named entities (i.e., references
that contain proper nouns), we asked the users to
distinguish between “named” and “unnamed”
persons and locations, and to tag both separately.
The inclusion of unnamed entities was intended
to keep their named counterparts pure and undi-
luted; the unnamed entities were discarded after
the annotation process was complete. The same
mechanism could have been used for the
ORGANIZATION entity type, but the risk of unnamed
references seemed smaller.
Initially, we ran a small trial with a base rate
of $0.03 for each HIT. However, after compiling
the results we noticed that there was a general
tendency for the workers to under-tag the entit-
ies. Besides outright freeloaders (i.e., workers
who simply clicked “no entities” each time),
there were also many who would highlight the
first one or two entities, and then ignore the rest
of the email.
This may have been due to a misunderstanding
of the HIT instructions, but we conjectured that a
deeper reason was that we were paying a base
rate regardless of the number of entities identi-
fied. Ideally, a HIT with many entities to high-
light should pay more than a HIT with fewer.
However, the default fixed-rate system was pay-
ing the same for both, and the workers were re-
sponding to such an inflexible incentive system
accordingly. To remedy this situation, we set
about to create a payment system that would mo-
tivate higher recall on entity-rich emails, while
still discouraging the opposite extreme of ran-
dom over-tagging.
</bodyText>
<page confidence="0.992801">
73
</page>
<subsectionHeader confidence="0.994477">
4.2 Bonus Payment System
</subsectionHeader>
<bodyText confidence="0.999623263157895">
Mechanical Turk provides two methods for pay-
ing workers: fixed rates on each HIT and bo-
nuses to individual workers for especially good
work. We chose to leverage these bonuses to
form the core of our payment system. Each HIT
would pay a base rate of $0.01, but each tagged
entity could elicit a bonus of $0.01-$0.02.
PERSON entities paid $0.01, while LOCATION and
ORGANIZATION entities paid $0.02 (since they were
rarer).
To ensure quality and discourage over-tagging,
bonuses for each highlighted entity were limited
based on an agreement threshold with other
workers. This threshold was usually set such that
a majority agreement was required, which was an
arbitrary decision we made in order to control
costs. The terms of the bonuses were explained
in detail in the instructions page for each HIT.
Additionally, we decided to leverage this bo-
nus system to encourage improvements in work-
er performance over time. Since the agreed-upon
spans that elicited bonuses were assumed to be
mostly correct, we realized we could give feed-
back to the workers on these entities to encour-
age similar performance in the future.
In general, worker bonuses are a mysterious
and poorly understood motivational mechanism.
Our feedback system attempted to make these
bonuses more predictable and transparent. The
system we built uses Amazon&apos;s “NotifyWorkers”
REST API to send messages directly to the
workers&apos; email accounts. Bonuses were batched
on a daily basis, and the notification emails gave
a summary description of the day&apos;s bonuses.
In recognition of your performance, you
were awarded a bonus of $0.5 ($0.02x25)
for catching the following span(s): [&apos;ve­
gas&apos;, &apos;Mt. Hood&apos;, &apos;Holland&apos;, [...]
</bodyText>
<figureCaption confidence="0.948794">
Fig. 2: Example bonus notification.
</figureCaption>
<bodyText confidence="0.999965764705882">
Both the UI and the bonus/notification system
were works in progress that were continually re-
fined based on comments from the worker com-
munity. We were pleasantly surprised to find
that, throughout the annotation process, the
Mechanical Turk workers were generally enthu-
siastic about the HITs, and also interested in im-
proving the quality of their annotations. Out of
169,156 total HITs, we received 702 comments
from 140 different workers, as well as over 50
email responses and a dedicated thread at Turk-
erNation.com6. Most of the feedback was posit-
ive, and negative feedback was almost solely dir-
ected at the UI. Based on their comments, we
continually tweaked and debugged the UI and
HIT instructions, but kept the basic structure of
the bonus system.
</bodyText>
<subsectionHeader confidence="0.997519">
4.3 Worker Distribution
</subsectionHeader>
<bodyText confidence="0.999741340909091">
With the bonus system in place, it was still ne-
cessary to determine the ideal number of workers
to assign per email. Previously, Snow et al.
(2008) used expert annotations to find how many
Mechanical Turk workers could “equal” an ex-
pert in terms of annotation quality. Because we
lacked expert annotations, we developed an al-
ternative system to determine the ideal number of
workers based purely on inter-annotator agree-
ment.
As described in the previous section, the most
significant problem faced with our HITs was that
of low recall. Low precision was generally not
considered to be a problem, since, with enough
annotators, inter-annotator agreement could al-
ways be set arbitrarily high in order to weed out
false positives. Recall, on the other hand, could
be consistently expected to improve as more an-
notators were added to the worker pool. There-
fore, the only problem that remained was to cal-
culate the marginal utility (in terms of recall) of
each additional annotator assigned to an email.
In order to estimate this marginal recall gain
for each entity type, we first ran small initial tests
with a relatively large number of workers. From
these results, we took all the entities identified by
at least two workers and set those aside as the
gold standard annotations; any overlapping an-
notations were collapsed into the larger one.
Next, for each n number of workers between 2
and the size of the entire worker pool, we ran-
domly sampled n workers from the pool, re-cal-
culated the entities based on agreement from at
least two workers within that group, and calcu-
lated the recall relative to the gold standard an-
notation. The threshold of 2 was chosen arbitrar-
ily for the purpose of this experiment.
From this data we generated a marginal recall
curve for each entity type, which roughly ap-
proximates how many workers are required per
email before recall starts to drop off signific-
antly. As expected, each graph shows a plateau-
like behavior as the number of workers increases,
but some entity types reach their plateau earlier
</bodyText>
<footnote confidence="0.9596255">
6 http://turkers.proboards.com/index.cgi?action
=display&amp;board=everyoneelse&amp;thread=3177
</footnote>
<page confidence="0.988489">
74
</page>
<figure confidence="0.999874476190476">
0.8
0.6
0.4
0.2
0
2 3 4
1
0.8
0.6
0.4
0.2
0
1
2 3 4 5 6 7 8
0.8
0.6
0.4
0.2
0
1
2 3 4 5 6 7 8 9 10
</figure>
<figureCaption confidence="0.9978415">
Fig. 3: Marginal recall curves for PERSON, LOCATION, and ORGANIZATION entity types, from a trial run of
900-1,000 emails. Recall is plotted on the y-axis, the number of annotators on the x-axis.
</figureCaption>
<bodyText confidence="0.993550692307692">
than others. Most saliently, Person entities seem
to require only a few workers to reach a relat-
ively high recall, compared to LOCATION or
ORGANIZATION entities.
Based on the expected diminishing returns for
each entity type, we determined some number of
workers to assign per email that we felt would
maximize entity recall while staying within our
budgetary limits. After some tinkering and ex-
perimentation with marginal recall curves, we ul-
timately settled on 4 assignments for PERSON en-
tities, 6 for LOCATION entities, and 7 for
ORGANIZATION entities.
</bodyText>
<sectionHeader confidence="0.983162" genericHeader="introduction">
5 Corpora and Experiments
</sectionHeader>
<bodyText confidence="0.9999406">
We ran our Mechanical Turk tasks over a period
of about three months, from August 2008 to
November 2008. We typically processed 500-
1,500 documents per day. In the end, the work-
ers annotated 20,609 unique emails which totaled
7.9 megabytes, including both subject and body.
All in all, we were pleasantly surprised by the
speed at which each HIT series was completed.
Out of 39 total HIT series, the average comple-
tion time (i.e. from when the HITs were first pos-
ted to MTurk.com until the last HIT was com-
pleted) was 3.13 hours, with an average of
715.34 emails per HIT series. The fastest com-
pletion time per number of emails was 1.9 hours
for a 1,000-email task, and the slowest was 5.13
hours for a 100-email task. We noticed, that,
paradoxically, larger HIT series were often com-
pleted more quickly – most likely because
Amazon promotes the larger tasks to the front
page.
</bodyText>
<subsectionHeader confidence="0.973853">
5.1 Corpora Annotation
</subsectionHeader>
<bodyText confidence="0.999954933333333">
In Table 1, we present several statistics regard-
ing the annotation tasks, grouped by corpus and
entity type. Here, “Cost” is the sum of all bo-
nuses and base rates for the HITs, “Avg. Cost”
is the average amount we paid in bonuses and
base rates per email, “Avg. # Workers” is the av-
erage number of workers assigned per email,
“Avg. Bonus” is the average bonus per HIT,
“Avg. # Spans” is the average number of entities
highlighted per HIT, and “Avg. Time” is the av-
erage time of completion per HIT in seconds.
Precision and recall are reported relative to the
“gold standards” determined by the bonus agree-
ment thresholds. None of the reported costs in-
clude fees paid to Amazon, which varied based
on how the bonuses were batched.
A few interesting observations emerge from
these data. For one, the average bonus was usu-
ally a bit more than the base rate of $0.01. The
implication is that bonuses actually comprised
the majority of the compensation, somewhat call-
ing into question their role as a “bonus.”
Also noteworthy is that ORGANIZATION entities
took less time per identified span to complete
than either location or person entities. However,
we suspect that this is due to the fact that we ran
the ORGANIZATION tasks last (after PERSON and
LOCATION), and by that time we had ironed out
several bugs in the UI, and our workers had be-
come more adept at using it.
</bodyText>
<subsectionHeader confidence="0.999005">
5.2 Worker Performance
</subsectionHeader>
<bodyText confidence="0.9974976">
In the end, we had 798 unique workers complete
169,156 total HITs. The average number of
HITs per worker was 211.97, but the median was
only 30. Ten workers who tagged no entities
were blocked, and the 1,029 HITs they com-
pleted were rejected without payment.
For the most part, a small number of dedicated
workers completed the majority of the tasks. Out
of all non-rejected HITs, the top 10 most prolific
workers completed 22.51%, the top 25 com-
pleted 38.59%, the top 50 completed 55.39%,
and the top 100 completed 74.93%.
Callison-Burch (2009) found in their own
Mechanical Turk system that the workers who
contributed more tended to show lower quality,
</bodyText>
<page confidence="0.996891">
75
</page>
<table confidence="0.999352923076923">
Corpus Entity Cost #Emails Avg. Cost Avg. #Workers Avg. Bonus Avg. #Spans Avg. Precision Avg. Recall Avg. Time
20N. Loc. 315.68 1999 0.1579 6 0.0163 1.6885 0.5036 0.7993 144.34
A.H. Loc. 412.2 2500 0.1649 6.4 0.0158 1.1924 0.6881 0.8092 105.34
Enron Loc. 323.54 3000 0.1078 6.23 0.0073 1.0832 0.3813 0.7889 105.25
TREC Loc. 274.88 2500 0.1100 6 0.0083 1.1847 0.3794 0.7864 122.97
20N. Org. 438.44 3500 0.1253 7 0.0079 1.2396 0.3274 0.6277 105.68
A.H. Org. 396.48 2500 0.1586 7 0.0127 1.2769 0.4997 0.7062 92.01
Enron Org. 539.19 2500 0.2157 8.6 0.0151 1.3454 0.5590 0.7415 80.55
TREC Org. 179.94 1500 0.1200 7 0.0071 0.8923 0.4414 0.6992 84.23
20N. Per. 282.51 2500 0.1130 4 0.0183 2.8693 0.7267 0.9297 152.77
A.H. Per. 208.78 2500 0.0835 4 0.0109 1.6529 0.7459 0.9308 112.4
Enron Per. 54.11 400 0.1353 6.14 0.0120 2.7360 0.8343 0.8841 111.23
TREC Per. 214.37 2500 0.0857 4 0.0114 1.5918 0.7950 0.9406 103.73
</table>
<tableCaption confidence="0.999951">
Table 1: Statistics by corpus and entity type (omitting rejected work).
</tableCaption>
<bodyText confidence="0.9973495">
as measured by agreement with an expert. We
had hoped that our bonus system, by rewarding
quality work with higher pay, would yield the
opposite effect, and in practice, our most prolific
workers did indeed tend to show the highest en-
tity recall.
</bodyText>
<figure confidence="0.998521666666667">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 1000 2000 3000 4000 5000 6000 7000 8000
</figure>
<figureCaption confidence="0.999384">
Fig. 4: # HITs Completed vs. Recall
</figureCaption>
<bodyText confidence="0.991407444444444">
Figure 4 shows how each of the non-rejected
workers fared in terms of entity recall (relative to
the “gold standard” determined by the bonus
agreement threshold), compared to the number of
HITs completed. As the chart shows, out of the
10 most productive workers, only one had an av-
erage recall score below 60%, and the rest all had
scores above 80%. While there are still quite a
few underperforming workers within the core
group of high-throughput annotators, the general
trend seemed to be that the more HITs a worker
completes, the more likely he/she is to agree with
the other annotators. This chart may be directly
compared to a similar one in Callison-Burch
(2009), where the curve takes largely the oppos-
ite shape. One interpretation of this is that our
bonus system had the desired effect on annotator
quality.
</bodyText>
<subsectionHeader confidence="0.999788">
5.3 Annotation Quality Experiments
</subsectionHeader>
<bodyText confidence="0.999851342105263">
To evaluate the quality of the worker annota-
tions, one would ideally like to have at least a
subset annotated by an expert, and then compare
the expert&apos;s judgments with the Mechanical Turk
workers&apos;. However, in our case we lacked expert
annotations for any of the annotated emails.
Thus, we devised an alternative method to evalu-
ate the annotation quality, using the NER system
built into the open-source MinorThird toolkit.7
MinorThird is a popular machine learning and
natural language processing library that has pre-
viously been applied to the problem of NER with
some success (Downey et al. 2007). For our pur-
poses, we wanted to minimize the irregularity in-
troduced by deviating from the core features and
algorithms available in MinorThird, and there-
fore did not apply any feature selection or feature
engineering in our experiments. We chose to use
MinorThird&apos;s default “CRFLearner,” which is a
module that learns feature weights using the IITB
CRF library8 and then applies them to a condi-
tional Markov model-based extractor. All of the
parameters were set to their default value, includ-
ing the built-in “TokenFE” feature extractor,
which extracts features for the lowercase value of
each token and its capitalization pattern. The
version of MinorThird used was 13.7.10.8.
In order to convert the Mechanical Turk an-
notations to a format that could be input as train-
ing data to the NER system, we had to resolve
the conflicting annotations of the multiple work-
ers into a unified set of labeled documents. Sim-
ilarly to the bonus system, we achieved this using
a simple voting scheme. In contrast to the bonus
system, though, we experimented with multiple
inter-annotator agreement thresholds between 1
and 4. For the PERSON corpora this meant a relat-
ively stricter threshold than for the LOCATION or
</bodyText>
<footnote confidence="0.998662">
7 http://minorthird.sourceforge.net
8 http://crf.sourceforge.net
</footnote>
<page confidence="0.989003">
76
</page>
<bodyText confidence="0.970409275">
ORGANIZATION corpora, since the PERSON corpora tested our statistical recognizers against all three
typically had only 4 annotations per document. divisions combined as well as the test set alone.
Mail subjects and bodies were split into separate 6 Results
documents. The results from these four tests are presented in
Four separate experiments were run with these Tables 2-5. In these tables, “Agr.” refers to in-
corpora. The first was a 5-fold cross-evaluation ter-annotator agreement, “TP” to token precision,
(i.e., a 80%/20% split) train/test experiment on “SP” to span precision, “TR” to token recall,
each of the twelve corpora. Because this test did “SR” to span recall, “TF” to token F-measure,
not rely on any expert annotations in the gold and “SF” to span F-measure. “Span” scores do
standard, our goal here was only to roughly not award partial credit for entities, and are there-
measure the “cohesiveness” of the corpus. Low fore a stricter measure than “token” scores.
precision and recall scores should indicate a Entity Agr. TP TR TF
messy corpus, where annotations in the training Loc. 1 60.07% 54.65% 57.23%
portion do not necessarily help the extractor to 2 75.47% 70.51% 72.90%
discover annotations in the test portion. Con- 3 71.59% 60.99% 65.86%
versely, high precision and recall scores should 4 59.50% 41.40% 48.83%
indicate a more cohesive corpus – one that is at Org. 1 70.79% 49.34% 58.15%
least somewhat internally consistent across the 2 77.98% 55.97% 65.16%
training and test portions. 3 38.96% 57.87% 46.57%
The second test was another train/test experi- 4 64.68% 50.19% 56.52%
ment, but with the entire Mechanical Turk corpus Per. 1 86.67% 68.27% 76.38%
as training data, and with a small set of 182 2 89.97% 77.36% 83.19%
emails, of which 99 were from the W3C Email 3 87.58% 76.19% 81.49%
Corpus9 and 83 were from emails belonging to 4 75.19% 63.76% 69.00%
various Kiha Software employees, as test data. Table 2: Cross-validation test results.
These 182 test emails were hand-annotated for Entity Agr. TP TR TF
the three entity types by the authors. Although Loc. 1 65.90% 37.52% 47.82%
this test data was small, our goal here was to 2 83.33% 56.28% 67.19%
demonstrate how well the trained extractors 3 84.05% 48.12% 61.20%
could fare against email text from a completely 4 84.21% 26.10% 39.85%
different source than the training data. Org. 1 41.03% 35.54% 38.09%
The third test was similar to the second, but 2 62.89% 30.77% 41.32%
used as its test data 3,116 Enron emails annotated 3 66.00% 15.23% 24.75%
for PERSON entities.10 The labels were manually 4 84.21% 9.85% 17.63%
corrected by the authors before testing. The goal Per. 1 85.48% 70.81% 77.45%
here was the same as with the second test, al- 2 69.93% 69.72% 69.83%
though it must be acknowledged that the PERSON 3 86.95% 64.40% 73.99%
training data did make use of 400 Enron emails, 4 95.02% 43.29% 59.49%
and therefore the test data was not from a com- Table 3: Results from the second test.
pletely separate domain.
The fourth test was intended to increase the
comparability of our own results with those that
others have shown in NER on email text. For the
test data, we chose two subsets of the Enron
Email Corpus used in Minkov et al. (2005).11
The first, “Enron-Meetings,” contains 244 train-
ing documents, 242 tuning documents, and 247
test documents. The second, “Enron-Random,”
contains 89 training documents, 82 tuning docu-
ments, and 83 test documents. For each, we
The cross-validation test results seem to indic-
ate that, in general, an inter-annotator agreement
threshold of 2 produces the most cohesive cor-
pora regardless of the number of workers as-
signed per email. In all cases, the F-measure
peaks at 2 and then begins to drop afterwards.
The results from the second test, using the
W3C and Kiha emails as test data, tell a slightly
different story, however. One predictable obser-
vation from these data is that precision tends to
increase as more inter-annotator agreement is re-
quired, while recall decreases. We believe that
9 http://tides.umiacs.umd.edu/webtrec/trecent/parsed
_w3c_corpus.html
10 http://www.cs.cmu.edu/~wcohen/repository.tgz
and http://www.cs.cmu.edu/~einat/datasets.html.
11 http://www.cs.cmu.edu/~einat/datasets.html.
77
this is due to the fact that entities that were con-
firmed by more workers tended to be less contro-
versial or ambiguous than those confirmed by
fewer. Most surprising about these results is
that, although F-measure peaks with the 2-agree-
ment corpora for both LOCATION and ORGANIZATION
entities, PERSON entities actually show the worst
precision when using the 2-agreement corpus. In
the case of PERSON entities, the corpus generated
using no inter-annotator agreement at all, i.e., an-
notator agreement of 1, actually performs the
best in terms of F-measure.
</bodyText>
<table confidence="0.9987564">
Agr. TP TR TF
1 80.56% 62.55% 70.42%
2 85.08% 67.66% 75.37%
3 93.25% 57.13% 70.86%
4 95.61% 39.67% 56.08%
</table>
<tableCaption confidence="0.998504">
Table 4: Results from the third test.
</tableCaption>
<table confidence="0.990374058823529">
Data Agr. TP TR TF SP SR SF
100% 57.16% 72.74% 100% 50.10% 66.75%
100% 64.31% 78.28% 100% 56.11% 71.88%
100% 50.44% 67.06% 100% 45.11% 62.18%
100% 31.41% 47.81% 100% 27.91% 43.64%
100% 62.17% 76.68% 100% 51.30% 67.81%
100% 66.36% 79.78% 100% 54.28% 70.36%
100% 55.72% 71.56% 100% 45.72% 62.76%
100% 42.24% 59.39% 100% 36.06% 53.01%
36.36% 59.91% 45.25% 40.30% 53.75% 46.07%
70.83% 65.32% 67.96% 67.64% 57.68% 62.26%
88.69% 58.63% 70.60% 82.93% 54.38% 65.68%
93.59% 43.68% 59.56% 89.33% 41.22% 56.41%
100% 60.87% 75.68% 100% 54.82% 70.82%
100% 64.70% 78.56% 100% 59.05% 74.26%
100% 63.06% 77.34% 100% 58.38% 73.72%
100% 43.04% 60.18% 100% 40.10% 57.25%
</table>
<tableCaption confidence="0.999844">
Table 5: Results from the fourth test.
</tableCaption>
<bodyText confidence="0.9999824">
With the third test, however, the results are
more in line with those from the cross-validation
tests: F-measure peaks with the 2-agreement cor-
pus and drops off as the threshold increases.
Most likely these results can be considered more
significant than those from the second test, since
this test corpus contains almost 20 times the
number of documents.
For the fourth test, we report both token-level
statistics and span-level statistics (i.e., where
credit for partially correct entity boundaries is
not awarded) in order to increase comparability
with Minkov et al. (2005). With one exception,
these tests seem to show again that the highest F-
measure comes from the annotator created using
an agreement level of 2, confirming results from
the first and third tests.
The fourth test may also be directly compared
to the results in Minkov et al. (2005), which re-
port span F-measure scores of 59.0% on Enron-
Meetings and 68.1% on Enron-Random, for a
CRF-based recognizer using the “Basic” feature
set (which is identical to ours) and using the
“train” division for training and the “test” divi-
sion for testing. In both cases, our best-perform-
ing annotators exceed these scores – an 11.5%
improvement on Enron-Meetings and a 6.16%
improvement on Enron-Random. This is an en-
couraging result, given that our training data
largely come from a different source than the test
data, and that the labels come from non-experts.
We see this as confirmation that very large cor-
pora annotated by Mechanical Turk workers can
surpass the quality of smaller corpora annotated
by experts.
</bodyText>
<sectionHeader confidence="0.995078" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999994838709678">
In order to quickly and economically build a
large annotated dataset for NER, we leveraged
Amazon’s Mechanical Turk. MTurk allowed us
to build a dataset of 20,609 unique emails with
169,156 total annotations in less than four
months. The MTurk worker population respon-
ded well to NER tasks, and in particular respon-
ded well to the bonus and feedback scheme we
put into place to improve annotation quality. The
bonus feedback system was designed to improve
the transparency of the compensation system and
motivate higher quality work over time. Encour-
agingly, our results indicate that the workers who
completed the most documents also had consist-
ently high entity recall, i.e., agreement with other
workers, indicating that the system achieved the
desired effect.
Given a large body of MTurk annotated docu-
ments, we were able to leverage inter-annotator
agreement to control the precision and recall of a
CRF-based recognizer trained on the data. Im-
portantly, we also showed that inter-annotator
agreement can be used to predict the appropriate
number of workers to assign to a given email in
order to maximize entity recall and reduce costs.
Finally, a direct comparison of the entity re-
cognizers generated from MTurk annotations to
those generated from expert annotations was
very promising, suggesting that Mechanical Turk
is appropriate for NER annotation tasks, when
care is taken to manage annotator error.
</bodyText>
<sectionHeader confidence="0.993944" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.985217">
Thanks to Dolores Labs for the initial version of
the UI, and thanks to Amazon and the 798 Mech-
anical Turk workers for making this work pos-
sible.
</bodyText>
<figure confidence="0.996538375">
E-M 1
(All) 2
3
4
E-M 1
(Test) 2
3
4
E-R 1
(All) 2
3
4
E-R 1
(Test) 2
3
4
</figure>
<page confidence="0.980202">
78
</page>
<bodyText confidence="0.9691585">
Alexander Sorokin and David Forsyth. Utility data an-
notation with Amazon MTurk. In Proceedings of
Computer Vision and Pattern Recognition Work-
shop at CVPR’08.
</bodyText>
<sectionHeader confidence="0.907558" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99054756">
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigu-
ation. In ACL &apos;01:26-33.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
evaluating translation quality using Amazon&apos;s
Mechanical Turk. In EMNLP &apos;09:286-295.
Doug Downey, Matthew Broadhead, and Oren Et-
zioni. 2007. Locating complex named entities in
web text. In IJCAI &apos;07.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference-6: a brief history. In
Proceedings of the 16th conference on Computa-
tional Linguistics:466-471.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk.
In Proceedings of CHI 2008.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In Proceedings of the Eleventh Conference of
the European Chapter of the Association for Com-
putational Linguistics:15-21.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of the Ninth Conference of the
European chapter of the Association for Computa-
tional Linguistics:1-8.
Einat Minkov, Richard C. Wang, and William W. Co-
hen. 2005. Extracting personal names from email:
applying named entity recognition to informal text.
In HTL &apos;05:443-450.
David Nadeau and Satoshi Sekine. 2007. A survey of
named entity recognition and classification. Lin-
guisticae Investigationes, 30(1):3-26.
Preslav Nakov. 2008. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In Pro-
ceedings of the 13th international conference on
Artificial Intelligence: Methodology, Systems and
Applications (AIMSA 2008):103–117.
Jason Pontin. 2007. Artificial Intelligence, With Help
From the Humans. In New York Times (March 25,
2007).
Burr Settles. 2004. Biomedical named entity recogni-
tion using conditional random fields and rich fea-
ture sets. In Proceedings of the COLING 2004 In-
ternational Joint Workshop on Natural Language
Processing in Biomedicine and its Applications.
Rion Snow, Brendan O&apos;Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast – but is it
good?: evaluating non-expert annotations for natur-
al language tasks. In EMNLP &apos;08:254-263.
</reference>
<page confidence="0.999048">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747531">
<title confidence="0.9536">Annotating Large Email Datasets for Named Entity Recognition Mechanical Turk</title>
<author confidence="0.957367">Nolan Lawson</author>
<author confidence="0.957367">Kevin Eustice</author>
<author confidence="0.957367">Meliha Yetisgen-Yildiz Mike Perkowitz</author>
<affiliation confidence="0.986545">Kiha Software Biomedical and Health Informatics</affiliation>
<address confidence="0.9681955">100 South King Street, Suite 320 University of Washington Seattle, WA 98104 Seattle, WA 98101</address>
<email confidence="0.999605">nolan@kiha.commelihay@u.washington.edu</email>
<email confidence="0.999605">kevin@kiha.commelihay@u.washington.edu</email>
<email confidence="0.999605">mikep@kiha.commelihay@u.washington.edu</email>
<abstract confidence="0.998510142857143">Amazon&apos;s Mechanical Turk service has been successfully applied to many natural language processing tasks. However, the task of named entity recognition presents unique challenges. In a large annotation task involving over 20,000 emails, we demonstrate that a competitive bonus system and inter-annotator agreement can be used to improve the quality of named entity annotations from Mechanical Turk. We also build several statistical named entity recognition models trained with these annotations, which compare favorably to similar models trained on expert annotations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In ACL</booktitle>
<pages>01--26</pages>
<contexts>
<context position="1341" citStr="Banko and Brill 2001" startWordPosition="193" endWordPosition="196">nnotations from Mechanical Turk. We also build several statistical named entity recognition models trained with these annotations, which compare favorably to similar models trained on expert annotations. 1 Introduction It is well known that the performance of many machine learning systems is heavily determined by the size and quality of the data used as input to the training algorithms. Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). It is therefore not surprising that obtaining large annotated datasets is an issue of great practical importance for the working researcher. Traditionally, annotated training data have been provided by experts in the field or the researchers themselves, often at great costs in terms of time and money. Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon&apos;s Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al. 2008). The initial results seem promising, and a new avenue for enhancing existing so</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In ACL &apos;01:26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using Amazon&apos;s Mechanical Turk.</title>
<date>2009</date>
<booktitle>In EMNLP</booktitle>
<pages>09--286</pages>
<contexts>
<context position="20710" citStr="Callison-Burch (2009)" startWordPosition="3393" endWordPosition="3394">, and our workers had become more adept at using it. 5.2 Worker Performance In the end, we had 798 unique workers complete 169,156 total HITs. The average number of HITs per worker was 211.97, but the median was only 30. Ten workers who tagged no entities were blocked, and the 1,029 HITs they completed were rejected without payment. For the most part, a small number of dedicated workers completed the majority of the tasks. Out of all non-rejected HITs, the top 10 most prolific workers completed 22.51%, the top 25 completed 38.59%, the top 50 completed 55.39%, and the top 100 completed 74.93%. Callison-Burch (2009) found in their own Mechanical Turk system that the workers who contributed more tended to show lower quality, 75 Corpus Entity Cost #Emails Avg. Cost Avg. #Workers Avg. Bonus Avg. #Spans Avg. Precision Avg. Recall Avg. Time 20N. Loc. 315.68 1999 0.1579 6 0.0163 1.6885 0.5036 0.7993 144.34 A.H. Loc. 412.2 2500 0.1649 6.4 0.0158 1.1924 0.6881 0.8092 105.34 Enron Loc. 323.54 3000 0.1078 6.23 0.0073 1.0832 0.3813 0.7889 105.25 TREC Loc. 274.88 2500 0.1100 6 0.0083 1.1847 0.3794 0.7864 122.97 20N. Org. 438.44 3500 0.1253 7 0.0079 1.2396 0.3274 0.6277 105.68 A.H. Org. 396.48 2500 0.1586 7 0.0127 1.</context>
<context position="22841" citStr="Callison-Burch (2009)" startWordPosition="3758" endWordPosition="3759">s fared in terms of entity recall (relative to the “gold standard” determined by the bonus agreement threshold), compared to the number of HITs completed. As the chart shows, out of the 10 most productive workers, only one had an average recall score below 60%, and the rest all had scores above 80%. While there are still quite a few underperforming workers within the core group of high-throughput annotators, the general trend seemed to be that the more HITs a worker completes, the more likely he/she is to agree with the other annotators. This chart may be directly compared to a similar one in Callison-Burch (2009), where the curve takes largely the opposite shape. One interpretation of this is that our bonus system had the desired effect on annotator quality. 5.3 Annotation Quality Experiments To evaluate the quality of the worker annotations, one would ideally like to have at least a subset annotated by an expert, and then compare the expert&apos;s judgments with the Mechanical Turk workers&apos;. However, in our case we lacked expert annotations for any of the annotated emails. Thus, we devised an alternative method to evaluate the annotation quality, using the NER system built into the open-source MinorThird </context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using Amazon&apos;s Mechanical Turk. In EMNLP &apos;09:286-295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Locating complex named entities in web text.</title>
<date>2007</date>
<booktitle>In IJCAI &apos;07.</booktitle>
<contexts>
<context position="23625" citStr="Downey et al. 2007" startWordPosition="3884" endWordPosition="3887">uality Experiments To evaluate the quality of the worker annotations, one would ideally like to have at least a subset annotated by an expert, and then compare the expert&apos;s judgments with the Mechanical Turk workers&apos;. However, in our case we lacked expert annotations for any of the annotated emails. Thus, we devised an alternative method to evaluate the annotation quality, using the NER system built into the open-source MinorThird toolkit.7 MinorThird is a popular machine learning and natural language processing library that has previously been applied to the problem of NER with some success (Downey et al. 2007). For our purposes, we wanted to minimize the irregularity introduced by deviating from the core features and algorithms available in MinorThird, and therefore did not apply any feature selection or feature engineering in our experiments. We chose to use MinorThird&apos;s default “CRFLearner,” which is a module that learns feature weights using the IITB CRF library8 and then applies them to a conditional Markov model-based extractor. All of the parameters were set to their default value, including the built-in “TokenFE” feature extractor, which extracts features for the lowercase value of each toke</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007. Locating complex named entities in web text. In IJCAI &apos;07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message Understanding Conference-6: a brief history.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational Linguistics:466-471.</booktitle>
<contexts>
<context position="4491" citStr="Grishman and Sundheim 1996" startWordPosition="702" endWordPosition="705"> final training corpus, and determine the ideal agreement threshold to maximize precision and recall of a statistical named entity recognition model. Finally, we demonstrate that a model Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 71–79, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics trained on our corpus is on par with one trained Previous works have tackled NER within the from expert annotations, when applied to a biomedical domain (Settles 2004), newswire dolabeled test set. main (Grishman and Sundheim 1996), and email 2 Related Work domain (Minkov et al. 2005). In this paper, we Mechanical Turk is a virtual market in which any focus on extracting entities from email text. requester can post tasks that are simple for hu- It should be noted that email text has many mans but difficult for computers. MTurk has distinctive features that create a unique challenge been adopted for a variety of uses both in in- when applying NER. For one, email text tends to dustry and academia from user studies (Kittur et be more informal than either newswire or bioal. 2008) to image labeling (Sorokin and Forsyth medic</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message Understanding Conference-6: a brief history. In Proceedings of the 16th conference on Computational Linguistics:466-471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aniket Kittur</author>
<author>Ed H Chi</author>
<author>Bongwon Suh</author>
</authors>
<title>Crowdsourcing user studies with Mechanical Turk.</title>
<date>2008</date>
<booktitle>In Proceedings of CHI</booktitle>
<marker>Kittur, Chi, Suh, 2008</marker>
<rawString>Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008. Crowdsourcing user studies with Mechanical Turk. In Proceedings of CHI 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
</authors>
<title>Bootstrapping named entity recognition with automatically generated gazetteer lists.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics:15-21.</booktitle>
<contexts>
<context position="2837" citStr="Kozareva (2006)" startWordPosition="443" endWordPosition="444">ional use of named entity gazetteers (often culled from external sources) to simulate data that would have been inferred from a larger training set (Minkov et al. 2005; Mikheev et al. 1999). Therefore, it appears to be a field that could profit from the enormous bargain-price workforce available through MTurk. It is not immediately obvious, though, that MTurk is well-suited for the task of NER annotation. Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. However, NER does not fit well into either of these formats. As pointed out by Kozareva (2006), NER can be thought of as a composition of two subtasks: 1) determining the start and end boundaries of a textual entity, and 2) determining the label of the identified span. The second task is the well-understood classification task, but the first task presents subtler problems. One is that MTurk&apos;s formbased user interface is inappropriate for the task of identifying textual spans. Another problem is that MTurk&apos;s fixed-fee payment system encourages low recall on the part of the annotators, since they receive the same pay no matter how many entities they identify. This paper addresses both of</context>
</contexts>
<marker>Kozareva, 2006</marker>
<rawString>Zornitsa Kozareva. 2006. Bootstrapping named entity recognition with automatically generated gazetteer lists. In Proceedings of the Eleventh Conference of the European Chapter of the Association for Computational Linguistics:15-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
<author>Marc Moens</author>
<author>Claire Grover</author>
</authors>
<title>Named entity recognition without gazetteers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European chapter of the Association for Computational Linguistics:1-8.</booktitle>
<contexts>
<context position="2411" citStr="Mikheev et al. 1999" startWordPosition="370" endWordPosition="373">e training corpora at a fraction of the usual costs (Snow et al. 2008). The initial results seem promising, and a new avenue for enhancing existing sources of annotated data appears to have been opened. Named entity recognition (NER) is one of the many fields of NLP that rely on machine learning methods, and therefore large training cor71 pora. Indeed, it is a field where more is almost always better, as indicated by the traditional use of named entity gazetteers (often culled from external sources) to simulate data that would have been inferred from a larger training set (Minkov et al. 2005; Mikheev et al. 1999). Therefore, it appears to be a field that could profit from the enormous bargain-price workforce available through MTurk. It is not immediately obvious, though, that MTurk is well-suited for the task of NER annotation. Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. However, NER does not fit well into either of these formats. As pointed out by Kozareva (2006), NER can be thought of as a composition of two subtasks: 1) determining the start and end boundaries of a textual entity, and 2) determining the label of the identified span</context>
</contexts>
<marker>Mikheev, Moens, Grover, 1999</marker>
<rawString>Andrei Mikheev, Marc Moens, and Claire Grover. 1999. Named entity recognition without gazetteers. In Proceedings of the Ninth Conference of the European chapter of the Association for Computational Linguistics:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Extracting personal names from email: applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In HTL</booktitle>
<pages>05--443</pages>
<contexts>
<context position="2389" citStr="Minkov et al. 2005" startWordPosition="366" endWordPosition="369">rvice to create large training corpora at a fraction of the usual costs (Snow et al. 2008). The initial results seem promising, and a new avenue for enhancing existing sources of annotated data appears to have been opened. Named entity recognition (NER) is one of the many fields of NLP that rely on machine learning methods, and therefore large training cor71 pora. Indeed, it is a field where more is almost always better, as indicated by the traditional use of named entity gazetteers (often culled from external sources) to simulate data that would have been inferred from a larger training set (Minkov et al. 2005; Mikheev et al. 1999). Therefore, it appears to be a field that could profit from the enormous bargain-price workforce available through MTurk. It is not immediately obvious, though, that MTurk is well-suited for the task of NER annotation. Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. However, NER does not fit well into either of these formats. As pointed out by Kozareva (2006), NER can be thought of as a composition of two subtasks: 1) determining the start and end boundaries of a textual entity, and 2) determining the label </context>
<context position="4545" citStr="Minkov et al. 2005" startWordPosition="712" endWordPosition="715">shold to maximize precision and recall of a statistical named entity recognition model. Finally, we demonstrate that a model Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 71–79, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics trained on our corpus is on par with one trained Previous works have tackled NER within the from expert annotations, when applied to a biomedical domain (Settles 2004), newswire dolabeled test set. main (Grishman and Sundheim 1996), and email 2 Related Work domain (Minkov et al. 2005). In this paper, we Mechanical Turk is a virtual market in which any focus on extracting entities from email text. requester can post tasks that are simple for hu- It should be noted that email text has many mans but difficult for computers. MTurk has distinctive features that create a unique challenge been adopted for a variety of uses both in in- when applying NER. For one, email text tends to dustry and academia from user studies (Kittur et be more informal than either newswire or bioal. 2008) to image labeling (Sorokin and Forsyth medical text, which reduces the usefulness of 2008). In Mar</context>
<context position="28136" citStr="Minkov et al. (2005)" startWordPosition="4635" endWordPosition="4638"> by the authors before testing. The goal Per. 1 85.48% 70.81% 77.45% here was the same as with the second test, al- 2 69.93% 69.72% 69.83% though it must be acknowledged that the PERSON 3 86.95% 64.40% 73.99% training data did make use of 400 Enron emails, 4 95.02% 43.29% 59.49% and therefore the test data was not from a com- Table 3: Results from the second test. pletely separate domain. The fourth test was intended to increase the comparability of our own results with those that others have shown in NER on email text. For the test data, we chose two subsets of the Enron Email Corpus used in Minkov et al. (2005).11 The first, “Enron-Meetings,” contains 244 training documents, 242 tuning documents, and 247 test documents. The second, “Enron-Random,” contains 89 training documents, 82 tuning documents, and 83 test documents. For each, we The cross-validation test results seem to indicate that, in general, an inter-annotator agreement threshold of 2 produces the most cohesive corpora regardless of the number of workers assigned per email. In all cases, the F-measure peaks at 2 and then begins to drop afterwards. The results from the second test, using the W3C and Kiha emails as test data, tell a slightl</context>
<context position="31116" citStr="Minkov et al. (2005)" startWordPosition="5091" endWordPosition="5094">25% Table 5: Results from the fourth test. With the third test, however, the results are more in line with those from the cross-validation tests: F-measure peaks with the 2-agreement corpus and drops off as the threshold increases. Most likely these results can be considered more significant than those from the second test, since this test corpus contains almost 20 times the number of documents. For the fourth test, we report both token-level statistics and span-level statistics (i.e., where credit for partially correct entity boundaries is not awarded) in order to increase comparability with Minkov et al. (2005). With one exception, these tests seem to show again that the highest Fmeasure comes from the annotator created using an agreement level of 2, confirming results from the first and third tests. The fourth test may also be directly compared to the results in Minkov et al. (2005), which report span F-measure scores of 59.0% on EnronMeetings and 68.1% on Enron-Random, for a CRF-based recognizer using the “Basic” feature set (which is identical to ours) and using the “train” division for training and the “test” division for testing. In both cases, our best-performing annotators exceed these scores</context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>Einat Minkov, Richard C. Wang, and William W. Cohen. 2005. Extracting personal names from email: applying named entity recognition to informal text. In HTL &apos;05:443-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<booktitle>Linguisticae Investigationes,</booktitle>
<pages>30--1</pages>
<contexts>
<context position="8275" citStr="Nadeau and Sekine 2007" startWordPosition="1311" endWordPosition="1314">eve this effect, we chose four different they design similar large-scale annotation tasks. sources of unlabeled email text to be annotated 3 General Problem Definition by the Mechanical Turk workers for input into Named entity recognition (NER) is a well-known the training algorithms: subtask of information extraction. Traditionally, 1. The Enron Email Dataset. the task has been based on identifying words and 2. The 2005 TREC Public Spam Corpus phrases that refer to various entities of interest, (non-spam only).3 including persons, locations, and organizations, 3. The 20 Newsgroups Dataset.4 (Nadeau and Sekine 2007). The problem is usu- 4. A private mailing list for synthesizer afially posed as a sequence labeling task similar to cionados called “Analogue Heaven.” the part-of-speech (POS) tagging or phrase- 4 Mechanical Turk for NER chunking tasks, where each token in the input As described previously, MTurk is not explicitly text corresponds to a label in the output, and is designed for NER tasks. Because of this, we desolved with sequential classification algorithms (such as CRF, SVMCMM, or MEMM). 72 1 http://www.cs.cmu.edu/~enron/ 2 http://palinemail.crivellawest.net/ 3 http://plg.uwaterloo.ca/~gvcorm</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30(1):3-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
</authors>
<title>Noun compound interpretation using paraphrasing verbs: Feasibility study.</title>
<date>2008</date>
<booktitle>In Proceedings of the 13th international conference on Artificial Intelligence: Methodology, Systems and Applications (AIMSA</booktitle>
<pages>2008--103</pages>
<contexts>
<context position="6845" citStr="Nakov 2008" startWordPosition="1094" endWordPosition="1095"> to the names of Enron employees. Since the publication of Snow et al.’s paper, Within the newswire and biomedical domains, MTurk has become increasingly popular as an such overfitting may be benign or actually beneannotation tool for NLP research. Examples in- ficial, since documents in those domains tend to clude Nakov’s work on creating a manually an- deal with a relatively small and pre-determined notated resource for noun-noun compound inter- set of named entities (e.g., politicians and large pretation based on paraphrasing verbs by MTurk corporations for newswire text, gene and protein (Nakov 2008) and Callison-Burch’s machine names for biomedical text). For NER in the translation evaluation study with MTurk (Callis- email domain, however, such overfitting is unacon-Burch 2009). In contrast to the existing re- ceptable. The personal nature of emails ensures search, we both evaluated the quality of corpora that they will almost always contain references to generated by MTurk in different named entity re- people, places, and organizations not covered by cognition tasks and explored ways to motivate the training data. Therefore, for the classifier to the workers to do higher quality work. </context>
</contexts>
<marker>Nakov, 2008</marker>
<rawString>Preslav Nakov. 2008. Noun compound interpretation using paraphrasing verbs: Feasibility study. In Proceedings of the 13th international conference on Artificial Intelligence: Methodology, Systems and Applications (AIMSA 2008):103–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Pontin</author>
</authors>
<title>Artificial Intelligence, With Help From the Humans. In</title>
<date>2007</date>
<location>New York Times</location>
<contexts>
<context position="5352" citStr="Pontin 2007" startWordPosition="853" endWordPosition="854">ext has many mans but difficult for computers. MTurk has distinctive features that create a unique challenge been adopted for a variety of uses both in in- when applying NER. For one, email text tends to dustry and academia from user studies (Kittur et be more informal than either newswire or bioal. 2008) to image labeling (Sorokin and Forsyth medical text, which reduces the usefulness of 2008). In March 2007, Amazon claimed the user learned features that depend on patterns of capitbase of MTurk consisted of over 100,000 users alization and spelling. Also, the choice of corfrom 100 countries (Pontin 2007). pora in email text is particularly important. As In the scope of this paper, we examine the email corpora tend to come from either a single feasibility of MTurk in creating large-scale cor- company (e.g., the Enron Email Dataset1) or a pora for training statistical named entity recogni- small group of people (e.g., the Sarah Palin email tion models. However, our work was not the first set2), it is easy to build a classifier that overfits application of MTurk in the NLP domain. Snow the data. For instance, a classifier trained to exet al. (2008) examined the quality of labels cre- tract perso</context>
</contexts>
<marker>Pontin, 2007</marker>
<rawString>Jason Pontin. 2007. Artificial Intelligence, With Help From the Humans. In New York Times (March 25, 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields and rich feature sets.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING 2004 International Joint Workshop on Natural Language Processing in Biomedicine and its Applications.</booktitle>
<contexts>
<context position="4427" citStr="Settles 2004" startWordPosition="694" endWordPosition="695"> resolving inter-annotator conflicts to create the final training corpus, and determine the ideal agreement threshold to maximize precision and recall of a statistical named entity recognition model. Finally, we demonstrate that a model Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 71–79, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics trained on our corpus is on par with one trained Previous works have tackled NER within the from expert annotations, when applied to a biomedical domain (Settles 2004), newswire dolabeled test set. main (Grishman and Sundheim 1996), and email 2 Related Work domain (Minkov et al. 2005). In this paper, we Mechanical Turk is a virtual market in which any focus on extracting entities from email text. requester can post tasks that are simple for hu- It should be noted that email text has many mans but difficult for computers. MTurk has distinctive features that create a unique challenge been adopted for a variety of uses both in in- when applying NER. For one, email text tends to dustry and academia from user studies (Kittur et be more informal than either newsw</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Burr Settles. 2004. Biomedical named entity recognition using conditional random fields and rich feature sets. In Proceedings of the COLING 2004 International Joint Workshop on Natural Language Processing in Biomedicine and its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O&apos;Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast – but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<pages>08--254</pages>
<contexts>
<context position="1861" citStr="Snow et al. 2008" startWordPosition="275" endWordPosition="278">re sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). It is therefore not surprising that obtaining large annotated datasets is an issue of great practical importance for the working researcher. Traditionally, annotated training data have been provided by experts in the field or the researchers themselves, often at great costs in terms of time and money. Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon&apos;s Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al. 2008). The initial results seem promising, and a new avenue for enhancing existing sources of annotated data appears to have been opened. Named entity recognition (NER) is one of the many fields of NLP that rely on machine learning methods, and therefore large training cor71 pora. Indeed, it is a field where more is almost always better, as indicated by the traditional use of named entity gazetteers (often culled from external sources) to simulate data that would have been inferred from a larger training set (Minkov et al. 2005; Mikheev et al. 1999). Therefore, it appears to be a field that could p</context>
<context position="15015" citStr="Snow et al. (2008)" startWordPosition="2405" endWordPosition="2408">sted in improving the quality of their annotations. Out of 169,156 total HITs, we received 702 comments from 140 different workers, as well as over 50 email responses and a dedicated thread at TurkerNation.com6. Most of the feedback was positive, and negative feedback was almost solely directed at the UI. Based on their comments, we continually tweaked and debugged the UI and HIT instructions, but kept the basic structure of the bonus system. 4.3 Worker Distribution With the bonus system in place, it was still necessary to determine the ideal number of workers to assign per email. Previously, Snow et al. (2008) used expert annotations to find how many Mechanical Turk workers could “equal” an expert in terms of annotation quality. Because we lacked expert annotations, we developed an alternative system to determine the ideal number of workers based purely on inter-annotator agreement. As described in the previous section, the most significant problem faced with our HITs was that of low recall. Low precision was generally not considered to be a problem, since, with enough annotators, inter-annotator agreement could always be set arbitrarily high in order to weed out false positives. Recall, on the oth</context>
</contexts>
<marker>Snow, O&apos;Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O&apos;Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast – but is it good?: evaluating non-expert annotations for natural language tasks. In EMNLP &apos;08:254-263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>