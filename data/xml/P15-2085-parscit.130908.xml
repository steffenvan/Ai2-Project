<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000920">
<title confidence="0.996718">
Point Process Modelling of Rumour Dynamics in Social Media
</title>
<author confidence="0.997221">
Michal Lukasik&apos;, Trevor Cohn2 and Kalina Bontcheva&apos;
</author>
<affiliation confidence="0.99810275">
&apos;Department of Computer Science,
The University of Sheffield
2Department of Computing and Information Systems,
The University of Melbourne
</affiliation>
<email confidence="0.987799">
{m.lukasik, k.bontcheva}@shef.ac.uk
t.cohn@unimelb.edu.au
</email>
<sectionHeader confidence="0.993784" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997835">
Rumours on social media exhibit complex
temporal patterns. This paper develops a
model of rumour prevalence using a point
process, namely a log-Gaussian Cox pro-
cess, to infer an underlying continuous
temporal probabilistic model of post fre-
quencies. To generalize over different ru-
mours, we present a multi-task learning
method parametrized by the text in posts
which allows data statistics to be shared
between groups of similar rumours. Our
experiments demonstrate that our model
outperforms several strong baseline meth-
ods for rumour frequency prediction eval-
uated on tweets from the 2014 Ferguson
riots.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998973015873">
The ability to model rumour dynamics helps with
identifying those, which, if not debunked early,
will likely spread very fast. One such example is
the false rumour of rioters breaking into McDon-
ald’s during the 2011 England riots. An effective
early warning system of this kind is of interest to
government bodies and news outlets, who struggle
with monitoring and verifying social media posts
during emergencies and social unrests. Another
application of modelling rumour dynamics could
be to predict the prevalence of a rumour through-
out its lifespan, based on occasional spot checks
by journalists.
The challenge comes from the observation that
different rumours exhibit different trajectories.
Figure 1 shows two example rumours from our
dataset (see Section 3): online discussion of ru-
mour #10 quickly drops away, whereas rumour
#37 takes a lot longer to die out. Two charac-
teristics can help determine if a rumour will con-
tinue to be discussed. One is the dynamics of post
occurrences, e.g. if the frequency profile decays
quickly, chances are it would not attract further
attention. A second factor is text from the posts
themselves, where phrases such as not true, un-
confirmed, or debunk help users judge veracity and
thus limit rumour spread (Zhao et al., 2015).
This paper considers the problem of modelling
temporal frequency profiles of rumours by taking
into account both the temporal and textual infor-
mation. Since posts occur at continuous times-
tamps, and their density is typically a smooth func-
tion of time, we base our model on point pro-
cesses, which have been shown to model well such
data in epidemiology and conflict mapping (Brix
and Diggle, 2001; Zammit-Mangion et al., 2012).
This framework models count data in a continuous
time through the underlying intensity of a Poisson
distribution. The posterior distribution can then
be used for several inference problems, e.g. to
query the expected count of posts, or to find the
probability of a count of posts occurring during
an arbitrary time interval. We model frequency
profiles using a log-Gaussian Cox process (Møller
and Syversveen, 1998), a point process where the
log-intensity of the Poisson distribution is mod-
elled via a Gaussian Process (GP). GP is a non-
parametric model which allows for powerful mod-
elling of the underlying intensity function.
Modelling the frequency profile of a rumour
based on posts is extremely challenging, since
many rumours consist of only a small number of
posts and exhibit complex patterns. To overcome
this difficulty we propose a multi-task learning ap-
proach, where patterns are correlated across mul-
tiple rumours. In this way statistics over a larger
training set are shared, enabling more reliable pre-
dictions for distant time periods, in which no posts
from the target rumour have been observed. We
demonstrate how text from observed posts can be
used to weight influence across rumours. Using a
set of Twitter rumours from the 2014 Ferguson un-
rest, we demonstrate that our models provide good
</bodyText>
<footnote confidence="0.556007">
518
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 518–523,
</footnote>
<page confidence="0.31823">
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</page>
<figure confidence="0.998494925925926">
0.0 0.2 0.4 0.6 0.8 1.0
(a) rumour #37
0.0 0.2 0.4 0.6 0.8 1.0
(b) rumour #10
13
15
11
9
3
7
5
1
LGCP
LGCPICM
LGCPTXT
17
15
13
11
9
7
5
3
1
LGCP
LGCPICM
LGCPTXT
</figure>
<figureCaption confidence="0.996159333333333">
Figure 1: Predicted frequency profiles for example rumours. Black bars denote training intervals, white bars denote test
intervals. Dark-coloured lines correspond to mean predictions by the models, light shaded areas denote the 95% confidence
interval, µ ± 2σ. This figure is best viewed in colour.
</figureCaption>
<bodyText confidence="0.995284875">
prediction of rumour popularity.
This paper makes the following contributions:
1. Introduces the problem of modelling rumour
frequency profiles, and presents a method based
on a log-Gaussian Cox process; 2. Incorporates
multi-task learning to generalize across disparate
rumours; and 3. Demonstrates how incorporating
text into multi-task learning improves results.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999937444444445">
There have been several descriptive studies of ru-
mours in social media, e.g. Procter et al. (2013)
analyzed rumours in tweets about the 2011 Lon-
don riots and showed that they follow similar life-
cycles. Friggeri et al. (2014) showed how Face-
book constitutes a rich source of rumours and con-
versation threads on the topic. However, none of
these studies tried to model rumour dynamics.
The problem of modelling the temporal nature
of social media explicitly has received little at-
tention. The work most closely related modelled
hash tag frequency time-series in Twitter using
GP (Preotiuc-Pietro and Cohn, 2013). It made
several simplifications, including discretising time
and treating the problem of modelling counts as
regression, which are both inappropriate. In con-
trast we take a more principled approach, using
a point process. We use the proposed GP-based
method as a baseline to demonstrate the benefit of
using our approaches.
The log-Gaussian Cox process has been applied
for disease and conflict mapping, e.g. Zammit-
Mangion et al. (2012) developed a spatio-temporal
model of conflict events in Afghanistan. In
contrast here we deal with temporal text data,
and model several correlated outputs rather than
their single output. Related also is the extensive
work done in spatio-temporal modelling of meme
spread. One example is application of Hawkes
processes (Yang and Zha, 2013), a probabilistic
framework for modelling self-excitatory phenom-
ena. However, these models were mainly used for
network modelling rather than revealing complex
temporal patterns, which may emerge only implic-
itly, and are more limited in the kinds of temporal
patterns that may be represented.
</bodyText>
<sectionHeader confidence="0.957453" genericHeader="method">
3 Data &amp; Problem
</sectionHeader>
<bodyText confidence="0.999315620689656">
In this section we describe the data and we formal-
ize the problem of modelling rumour popularity.
Data We use the Ferguson rumour data set (Zu-
biaga et al., 2015), consisting of tweets collected
in August and September 2014 during the Fergu-
son unrest. It contains both source tweets and the
conversational threads around these (where avail-
able). All source tweets are categorized as ru-
mour vs non-rumour, other tweets from the same
thread are assigned automatically as belonging to
the same event as the source tweet. Since some
rumours have few posts, we consider only those
with at least 15 posts in the first hour as rumours
of particular interest. This results in 114 rumours
consisting of a total of 4098 tweets.
Problem Definition Let us consider a time in-
terval [0, l] of length l=2 hours, a set of n rumours
R = {Ei}n i=1, where rumour Ei consists of a
set of mi posts Ei = {pi j}mi
j=1. Posts are tuples
pij = (xij, tij), where xij is text (in our case a bag
of words text representation) and tij is a timestamp
describing post pij, measured in time elapsed since
the first post on rumour Ei.
Posts occur at different timestamps, yielding
varying density of posts over time, which we are
interested in estimating. To evaluate the predicted
density for a given rumour Ei we leave out posts
from a set of intervals Tte = {[si k, ei k]}Ki
</bodyText>
<equation confidence="0.361101">
k=1 (where
</equation>
<bodyText confidence="0.8591">
sik and eik are respectively start and end points of
</bodyText>
<page confidence="0.989208">
519
</page>
<bodyText confidence="0.993230035714286">
interval k for rumour i) and estimate performance
at predicting counts in them by the trained model.
The problem is considered in supervised
settings, where posts on this rumour out-
side of these intervals form the training set
j �∈UKi
EO i ={pi j :ti k=1[si k, eik]}. Let the number of
elements in EOi be mOi . We also consider a do-
main adaptation setting, where additionally posts
from other rumours are observed RO i =R\Ei.
Two instantiations of this problem formulation
are considered. The first is interpolation, where
the test intervals are not ordered in any particular
way. This corresponds to a situation, e.g., when
a journalist analyses a rumour during short spot
checks, but wants to know the prevalence of the
rumour at other times, thus limiting the need for
constant attention. The second formulation is that
of extrapolation, where all observed posts occur
before the test intervals. This corresponds to a
scenario where the user seeks to predict the future
profile of the rumour, e.g., to identify rumours that
will attract further attention or wither away.
Although our focus here is on rumours, our
model is more widely applicable. For example,
one could use it to predict whether an advertise-
ment campaign would be successful or how a po-
litical campaign would proceed.
</bodyText>
<sectionHeader confidence="0.997421" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.999197095238095">
We consider a log-Gaussian Cox process (LGCP)
(Møller and Syversveen, 1998), a generalization
of inhomogeneous Poisson process. In LGCP
the intensity function is assumed to be a stochas-
tic process which varies over time. In fact, the
intensity function λ(t) is modelled using a la-
tent function f(t) sampled from a Gaussian pro-
cess (Rasmussen and Williams, 2005), such that
λ(t) = exp (f(t)) (exponent ensures positivity).
This provides a non-parametric approach to model
the intensity function. The intensity function can
be automatically learned from the data set and its
complexity depends on the data points.
We model the occurrence of posts in a rumour
Ei to follow log-Gaussian Cox process (LGCP)
with intensity λi(t), where λi(t) = exp(fi(t)).
We associate a distinct intensity function with
each rumour as they have varying temporal pro-
files. LGCP models the likelihood that a single
tweet occurs at time t in the interval [s, t] for a ru-
mour Ei given the latent function fi(t) as
</bodyText>
<equation confidence="0.9168135">
/t
p(y = 1|fi) = exp(fi(t)) exp(J exp(fi(u)) du)
.
s
</equation>
<bodyText confidence="0.6972315">
Then, the likelihood of posts EOi in time interval
T given a latent function fi can be obtained as
</bodyText>
<equation confidence="0.969171333333333">
mOi
p(EOi  |fi) =exp(−jT_Tte exp (fi(u)) du + 1:fi(tij) j=1
(1)
</equation>
<bodyText confidence="0.999837526315789">
The likelihood of posts in the rumour data is
obtained by taking the product of the likelihoods
over individual rumours. The likelihood (1) is
commonly approximated by considering sub-
regions of T and assuming constant intensities
in sub-regions of T (Møller and Syversveen,
1998; Vanhatalo et al., 2013) to overcome com-
putational difficulties arising due to integration.
Following this, we approximate the likelihood as
p(EOi |fi) = HSs=1 Poisson(ys  |lsexp (fi(˙ts))).
Here, time is divided into S intervals indexed
by s, ˙ts is the centre of the sth interval, ls is the
length of the sth interval and ys is number of
tweets posted during this interval.
The latent function f is modelled via a Gaussian
process (GP) (Rasmussen and Williams, 2005):
f(t) ∼ GP(m(t), k(t, t&apos;)), where m is the mean
function (equal 0) and k is the kernel specifying
how outputs covary as a function of the inputs.
We use a Radial Basis Function (RBF) kernel,
k(t, t&apos;) = a exp(−(t − t&apos;)2/l), where lengthscale
l controls the extent to which nearby points influ-
ence one another and a controls the scale of the
function.
The distribution of the posterior p(fi(t)|EOi ) at
an arbitrary timestamp t is calculated based on the
specified prior and the Poisson likelihood. It is
intractable and approximation techniques are re-
quired. There exist various methods to deal with
calculating the posterior; here we use the Laplace
approximation, where the posterior is approxi-
mated by a Gaussian distribution based on the first
2 moments. For more details about the model and
inference we refer the reader to (Rasmussen and
Williams, 2005). The predictive distribution over
time t* is obtained using the approximated poste-
rior. This predictive distribution is then used to
obtain the intensity function value at the point t*:
</bodyText>
<equation confidence="0.644174">
λi(t* |E°) = f exp (fi(t)) p (fi(t) |Eo) dfi
</equation>
<bodyText confidence="0.960371">
The predictive distribution over counts at a par-
ticular time interval of length w with a mid-point
t* for rumour Ei is Poisson distributed with rate
wλi(t*|EOi ).
</bodyText>
<page confidence="0.988087">
520
</page>
<bodyText confidence="0.999340538461538">
Multi-task learning and incorporating text In
order to exploit similarities across rumours we
propose a multi-task approach where each rumour
represents a task. We consider two approaches.
First, we employ a multiple output GP based
on the Intrinsic Coregionalization Model (ICM)
(´Alvarez et al., 2012). It is a method which has
been successfully applied to a range of NLP tasks
(Beck et al., 2014; Cohn and Specia, 2013). ICM
parametrizes the kernel by a matrix representing
similarities between pairs of tasks. We expect it to
find correlations between rumours exhibiting sim-
ilar temporal patterns. The kernel takes the form
</bodyText>
<equation confidence="0.481723">
kICM((t, i), (t&apos;, i&apos;))=ktime(t, t&apos;)Bi,i,,
</equation>
<bodyText confidence="0.99992725">
where B is a square coregionalization matrix (rank
1, B = κI + vvT ), i and i&apos; denote the tasks of the
two inputs, ktime is a kernel for comparing inputs
t and t&apos; (here RBF) and κ is a vector of values
modulating the extent of each task independence.
In a second approach, we parametrize the inter-
task similarity measures by incorporating text of
the posts. The full multi-task kernel takes form
</bodyText>
<equation confidence="0.99155375">
kTXT((t, i), (t&apos;, i&apos;)) = ktime(t, t&apos;) ×
�Xij,
p�,
� EES,
</equation>
<bodyText confidence="0.998233545454546">
We compare text vectors using cosine similar-
ity, ktext(X,Y) = b + cx�y
llxllllyll, where the hyper-
parameters b &gt; 0 and c &gt; 0 modulate between
text similarity and a global constant similarity. We
also consider combining both multi-task kernels,
yielding kICM+TXT = kICM + kTXT.
Optimization All hyperparameters are opti-
mized by maximizing the marginal likelihood of
the data L(EOi |0), where 0 = (a, l, κ, v, b, c) or a
subset thereof, depending on the choice of kernel.
</bodyText>
<sectionHeader confidence="0.998217" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999913636363636">
Evaluation metric We use mean squared error
(MSE) to measure the difference between true
counts and predicted counts in the test intervals.
Since probabilistic models (GP, LGCP) return dis-
tributions over possible outputs, we also evalu-
ate them via the log-likelihood (LL) of the true
counts under the returned distributions (respec-
tively Gaussian and Poisson distribution).
Baselines We use the following baselines. The
first is the Homogenous Poisson Process (HPP)
trained on the training set of the rumour. We se-
lect its intensity A using maximum likelihood esti-
mate, which equals to the mean frequency of posts
in the training intervals. The second baseline is
Gaussian Process (GP) used for predicting hash-
tag frequencies in Twitter by Preotiuc-Pietro and
Cohn (2013). Authors considered various kernels
in their experiments, most notably periodic ker-
nels. In our case it is not apparent that rumours
exhibit periodic characteristics, as can be seen in
Figure 1. We restrict our focus to RBF kernel and
leave inspection of other kernels such as periodic
ones for both GP and LGCP models for future.
The third baseline is to always predict 0 posts in
all intervals. The fourth baseline is tailored for the
interpolation setting, and uses simple interpolation
by averaging over the frequencies of the closest
left and right intervals, or the frequency of the
closest interval for test intervals on a boundary.
Data preprocessing In our experiments, we
consider the first two hours of each rumour lifes-
pan, which we split into 20 evenly spaced inter-
vals. This way, our dataset consists in total of 2280
intervals. We iterate over rumours using a form
of folded cross-validation, where in each iteration
we exclude some (but not all) time intervals for a
single target rumour. The excluded time intervals
form the test set: either by selecting half at random
(interpolation); or by taking only the second half
for testing (extrapolation). To ameliorate the prob-
lems of data sparsity, we replace words with their
Brown cluster ids, using 1000 clusters acquired on
a large scale Twitter corpus (Owoputi et al., 2013).
The mean function for the underlying GP in
LGCP methods is assumed to be 0, which results
in intensity function to be around 1 in the absence
of nearby observations. This prevents our method
from predicting 0 counts in these regions. We
add 1 to the counts in the intervals to deal with
this problem as a preprocessing step. The original
counts can be obtained by decrementing 1 from
the predicted counts. Instead, one could use a GP
with a non-zero mean function and learn the mean
function, a more elegant way of approaching this
problem, which we leave for future work.
</bodyText>
<sectionHeader confidence="0.999171" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9980246">
The left columns of Table 1 report the results
for the extrapolation experiments, showing the
mean and variance of results across the 114 ru-
mours. According to log likelihood evaluation
metric, GP is the worst from the probabilistic ap-
</bodyText>
<equation confidence="0.791027333333333">
ktext 1
\ p��EES�
X� I .
</equation>
<page confidence="0.992659">
521
</page>
<table confidence="0.9998795">
Extrapolation Interpolation
MSE LL MSE LL
HPP 7.14±10.1* -23.5±10.1* 7.66±7.55* -25.8±11.0*
GP 4.58±11.0* -34.6±8.78* 6.13±6.57* -90.1±198 *
Interpolate 4.90±13.1* - 5.29±6.06* -
0 2.76±7.81* - 7.65±11.0* -
LGCP 3.44±9.99* -15.8±11.6†* 6.01±6.29* -21.0±8.77†*
LGCP ICM 2.46±7.82†* -14.8±11.2†* 8.59±19.9* -20.7±9.87†*
LGCP TXT 2.32±7.06† -14.7±9.12† 3.66±5.67† -16.9±5.91†
LGCP ICM+TXT 2.31±7.80† -14.6±10.8† 3.92±5.20† -16.8±5.34†
</table>
<tableCaption confidence="0.9660802">
Table 1: MSE between the true counts and the predicted counts (lower is better) and predictive log likelihood of the true
counts from probabilistic models (higher is better) for test intervals over the 114 Ferguson rumours for extrapolation (left) and
interpolation (right) settings, showing mean ± std. dev. Baselines are shown above the line, with LGCP models below. Key:
† denotes significantly better than the best baseline; * denotes significantly worse than LGCP TXT, according to one-sided
Wilcoxon signed rank test P &lt; 0.05.
</tableCaption>
<bodyText confidence="0.999935520833333">
proaches. This is due to GP modelling a dis-
tribution with continuous support, which is inap-
propriate for modelling discrete counts. Chang-
ing the model from a GP to a better fitting to the
modelling temporal count data LGCP gives a big
improvement, even when a point estimate of the
prediction is considered (MSE). The 0 baseline is
very strong, since many rumours have compara-
tively little discussion in the second hour of their
lifespan relative to the first hour. Incorporating in-
formation about other rumours helps outperform
this method. ICM, TXT and ICM+TXT multi-
task learning approaches achieve the best scores
and significantly outperform all baselines. TXT
turns out to be a good approach to multi-task learn-
ing and outperforms ICM. In Figure 1a we show
an example rumour frequency profile for the ex-
trapolation setting. TXT makes a lower error than
LGCP and LGCPICM, both of which underesti-
mate the counts in the second hour.
Next, we move to the interpolation setting. Un-
surprisingly, Interpolate is the strongest baseline,
and outperforms the raw LGCP method. Again,
HPP and GP are outperformed by LGCP in terms
of both MSE and LL. Considering the output dis-
tributions (LL) the difference in performance be-
tween the Poisson Process based approaches and
GP is especially big, demonstrating how well the
principled models handle uncertainty in the pre-
dictive distributions. As for the multi-task meth-
ods, we notice that text is particularly useful, with
TXT achieving the highest MSE score out of all
considered models. ICM turns out to be not very
helpful in this setting. For example, ICM (just as
LGCP) does not learn there should be a peak at the
beginning of a rumour frequency profile depicted
in Figure 1b. TXT manages to make a signifi-
cantly smaller error by predicting a large posting
frequency there. We also found, that for a few ru-
mours ICM made a big error by predicting a high
frequency at the start of a rumour lifespan when
there was no such peak. We hypothesize ICM per-
forms poorly because it is hard to learn correct cor-
relations between frequency profiles when training
intervals do not form continuous segments of sig-
nificant sizes. ICM manages to learn correlations
more properly in extrapolation setting, where the
first hour is fully observed.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99996225">
This paper introduced the problem of modelling
frequency profiles of rumours in social media.
We demonstrated that joint modelling of collec-
tive data over multiple rumours using multi-task
learning resulted in more accurate models that are
able to recognise and predict commonly occurring
temporal patterns. We showed how text data from
social media posts added important information
about similarities between different rumours. Our
method is generalizable to problems other than
modelling rumour popularity, such as predicting
success of advertisement campaigns.
</bodyText>
<sectionHeader confidence="0.995035" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.54011775">
We would like to thank Srijith P. K. for helpful
comments. This work was funded by the PHEME
FP7 project (grant No. 611233) and partially sup-
ported by the Australian Research Council.
</bodyText>
<page confidence="0.993655">
522
</page>
<sectionHeader confidence="0.995819" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888402985075">
Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D.
Lawrence. 2012. Kernels for vector-valued func-
tions: A review. Found. Trends Mach. Learn.,
4(3):195–266.
The GPy authors. 2012–2015. GPy: A Gaussian
process framework in Python. http://github.
com/SheffieldML/GPy.
Daniel Beck, Trevor Cohn, and Lucia Specia. 2014.
Joint emotion analysis via multi-task Gaussian pro-
cesses. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’14, pages 1798–1803.
Anders Brix and Peter J. Diggle. 2001. Spatiotemporal
prediction for log-gaussian cox processes. Journal
of the Royal Statistical Society Series B, 63(4):823–
841.
Trevor Cohn and Lucia Specia. 2013. Modelling an-
notator bias with multi-task Gaussian processes: An
application to machine translation quality estima-
tion. In 51st Annual Meeting of the Association for
Computational Linguistics, ACL ’13, pages 32–42.
Adrien Friggeri, Lada Adamic, Dean Eckles, and Justin
Cheng. 2014. Rumor cascades. In International
AAAI Conference on Weblogs and Social Media.
Jesper Møller and Anne Randi Syversveen. 1998. Log
Gaussian Cox processes. Scandinavian Journal of
Statistics, pages 451–482.
Olutobi Owoputi, Chris Dyer, Kevin Gimpel, Nathan
Schneider, and Noah A. Smith. 2013. Improved
part-of-speech tagging for online conversational text
with word clusters. In Proceedings of NAACL, pages
380–390.
Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A tem-
poral model of text periodicities using Gaussian pro-
cesses. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’13, pages 977–988.
Rob Procter, Jeremy Crump, Susanne Karstedt, Alex
Voss, and Marta Cantijoch. 2013. Reading the riots:
What were the police doing on twitter? Policing and
society, 23(4):413–436.
Carl Edward Rasmussen and Christopher K. I.
Williams. 2005. Gaussian Processes for Ma-
chine Learning (Adaptive Computation and Ma-
chine Learning). The MIT Press.
Jarno Vanhatalo, Jaakko Riihim¨aki, Jouni Hartikainen,
Pasi Jyl¨anki, Ville Tolvanen, and Aki Vehtari. 2013.
Gpstuff: Bayesian modeling with Gaussian pro-
cesses. J. Mach. Learn. Res., 14(1):1175–1179.
Shuang-Hong Yang and Hongyuan Zha. 2013. Mix-
ture of mutually exciting processes for viral diffu-
sion. In ICML (2), volume 28 of JMLR Proceedings,
pages 1–9.
Andrew Zammit-Mangion, Michael Dewar, Visakan
Kadirkamanathan, and Guido Sanguinetti. 2012.
Point process modelling of the Afghan War Diary.
Proceedings of the National Academy of Sciences
of the United States of America, 109(31):12414–
12419.
Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015.
Early detection of rumors in social media from en-
quiry posts. In International World Wide Web Con-
ference Committee (IW3C2).
Arkaitz Zubiaga, Maria Liakata, Rob Procter, Kalina
Bontcheva, and Peter Tolmie. 2015. Towards de-
tecting rumours in social media. In AAAI Workshop
on AI for Cities.
</reference>
<page confidence="0.998923">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593109">
<title confidence="0.995513">Point Process Modelling of Rumour Dynamics in Social Media</title>
<author confidence="0.993923">Trevor</author>
<affiliation confidence="0.97849425">of Computer The University of of Computing and Information The University of</affiliation>
<email confidence="0.97363">t.cohn@unimelb.edu.au</email>
<abstract confidence="0.966352470588235">Rumours on social media exhibit complex temporal patterns. This paper develops a model of rumour prevalence using a point process, namely a log-Gaussian Cox process, to infer an underlying continuous temporal probabilistic model of post frequencies. To generalize over different rumours, we present a multi-task learning method parametrized by the text in posts which allows data statistics to be shared between groups of similar rumours. Our experiments demonstrate that our model outperforms several strong baseline methods for rumour frequency prediction evaluated on tweets from the 2014 Ferguson riots.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mauricio A ´Alvarez</author>
<author>Lorenzo Rosasco</author>
<author>Neil D Lawrence</author>
</authors>
<title>Kernels for vector-valued functions: A review.</title>
<date>2012</date>
<journal>Found. Trends Mach. Learn.,</journal>
<volume>4</volume>
<issue>3</issue>
<marker>´Alvarez, Rosasco, Lawrence, 2012</marker>
<rawString>Mauricio A. ´Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. 2012. Kernels for vector-valued functions: A review. Found. Trends Mach. Learn., 4(3):195–266.</rawString>
</citation>
<citation valid="false">
<title>The GPy authors. 2012–2015. GPy: A Gaussian process framework in Python.</title>
<note>http://github. com/SheffieldML/GPy.</note>
<marker></marker>
<rawString>The GPy authors. 2012–2015. GPy: A Gaussian process framework in Python. http://github. com/SheffieldML/GPy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Beck</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Joint emotion analysis via multi-task Gaussian processes.</title>
<date>2014</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’14,</booktitle>
<pages>1798--1803</pages>
<contexts>
<context position="13083" citStr="Beck et al., 2014" startWordPosition="2141" endWordPosition="2144">nt t*: λi(t* |E°) = f exp (fi(t)) p (fi(t) |Eo) dfi The predictive distribution over counts at a particular time interval of length w with a mid-point t* for rumour Ei is Poisson distributed with rate wλi(t*|EOi ). 520 Multi-task learning and incorporating text In order to exploit similarities across rumours we propose a multi-task approach where each rumour represents a task. We consider two approaches. First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) (´Alvarez et al., 2012). It is a method which has been successfully applied to a range of NLP tasks (Beck et al., 2014; Cohn and Specia, 2013). ICM parametrizes the kernel by a matrix representing similarities between pairs of tasks. We expect it to find correlations between rumours exhibiting similar temporal patterns. The kernel takes the form kICM((t, i), (t&apos;, i&apos;))=ktime(t, t&apos;)Bi,i,, where B is a square coregionalization matrix (rank 1, B = κI + vvT ), i and i&apos; denote the tasks of the two inputs, ktime is a kernel for comparing inputs t and t&apos; (here RBF) and κ is a vector of values modulating the extent of each task independence. In a second approach, we parametrize the intertask similarity measures by inc</context>
</contexts>
<marker>Beck, Cohn, Specia, 2014</marker>
<rawString>Daniel Beck, Trevor Cohn, and Lucia Specia. 2014. Joint emotion analysis via multi-task Gaussian processes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’14, pages 1798–1803.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Brix</author>
<author>Peter J Diggle</author>
</authors>
<title>Spatiotemporal prediction for log-gaussian cox processes.</title>
<date>2001</date>
<journal>Journal of the Royal Statistical Society Series B,</journal>
<volume>63</volume>
<issue>4</issue>
<pages>841</pages>
<contexts>
<context position="2608" citStr="Brix and Diggle, 2001" startWordPosition="403" endWordPosition="406">re it would not attract further attention. A second factor is text from the posts themselves, where phrases such as not true, unconfirmed, or debunk help users judge veracity and thus limit rumour spread (Zhao et al., 2015). This paper considers the problem of modelling temporal frequency profiles of rumours by taking into account both the temporal and textual information. Since posts occur at continuous timestamps, and their density is typically a smooth function of time, we base our model on point processes, which have been shown to model well such data in epidemiology and conflict mapping (Brix and Diggle, 2001; Zammit-Mangion et al., 2012). This framework models count data in a continuous time through the underlying intensity of a Poisson distribution. The posterior distribution can then be used for several inference problems, e.g. to query the expected count of posts, or to find the probability of a count of posts occurring during an arbitrary time interval. We model frequency profiles using a log-Gaussian Cox process (Møller and Syversveen, 1998), a point process where the log-intensity of the Poisson distribution is modelled via a Gaussian Process (GP). GP is a nonparametric model which allows f</context>
</contexts>
<marker>Brix, Diggle, 2001</marker>
<rawString>Anders Brix and Peter J. Diggle. 2001. Spatiotemporal prediction for log-gaussian cox processes. Journal of the Royal Statistical Society Series B, 63(4):823– 841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>Modelling annotator bias with multi-task Gaussian processes: An application to machine translation quality estimation.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13,</booktitle>
<pages>32--42</pages>
<contexts>
<context position="13107" citStr="Cohn and Specia, 2013" startWordPosition="2145" endWordPosition="2148"> f exp (fi(t)) p (fi(t) |Eo) dfi The predictive distribution over counts at a particular time interval of length w with a mid-point t* for rumour Ei is Poisson distributed with rate wλi(t*|EOi ). 520 Multi-task learning and incorporating text In order to exploit similarities across rumours we propose a multi-task approach where each rumour represents a task. We consider two approaches. First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) (´Alvarez et al., 2012). It is a method which has been successfully applied to a range of NLP tasks (Beck et al., 2014; Cohn and Specia, 2013). ICM parametrizes the kernel by a matrix representing similarities between pairs of tasks. We expect it to find correlations between rumours exhibiting similar temporal patterns. The kernel takes the form kICM((t, i), (t&apos;, i&apos;))=ktime(t, t&apos;)Bi,i,, where B is a square coregionalization matrix (rank 1, B = κI + vvT ), i and i&apos; denote the tasks of the two inputs, ktime is a kernel for comparing inputs t and t&apos; (here RBF) and κ is a vector of values modulating the extent of each task independence. In a second approach, we parametrize the intertask similarity measures by incorporating text of the p</context>
</contexts>
<marker>Cohn, Specia, 2013</marker>
<rawString>Trevor Cohn and Lucia Specia. 2013. Modelling annotator bias with multi-task Gaussian processes: An application to machine translation quality estimation. In 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13, pages 32–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrien Friggeri</author>
<author>Lada Adamic</author>
<author>Dean Eckles</author>
<author>Justin Cheng</author>
</authors>
<title>Rumor cascades.</title>
<date>2014</date>
<booktitle>In International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="5295" citStr="Friggeri et al. (2014)" startWordPosition="833" endWordPosition="836">iewed in colour. prediction of rumour popularity. This paper makes the following contributions: 1. Introduces the problem of modelling rumour frequency profiles, and presents a method based on a log-Gaussian Cox process; 2. Incorporates multi-task learning to generalize across disparate rumours; and 3. Demonstrates how incorporating text into multi-task learning improves results. 2 Related Work There have been several descriptive studies of rumours in social media, e.g. Procter et al. (2013) analyzed rumours in tweets about the 2011 London riots and showed that they follow similar lifecycles. Friggeri et al. (2014) showed how Facebook constitutes a rich source of rumours and conversation threads on the topic. However, none of these studies tried to model rumour dynamics. The problem of modelling the temporal nature of social media explicitly has received little attention. The work most closely related modelled hash tag frequency time-series in Twitter using GP (Preotiuc-Pietro and Cohn, 2013). It made several simplifications, including discretising time and treating the problem of modelling counts as regression, which are both inappropriate. In contrast we take a more principled approach, using a point </context>
</contexts>
<marker>Friggeri, Adamic, Eckles, Cheng, 2014</marker>
<rawString>Adrien Friggeri, Lada Adamic, Dean Eckles, and Justin Cheng. 2014. Rumor cascades. In International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesper Møller</author>
<author>Anne Randi Syversveen</author>
</authors>
<title>Log Gaussian Cox processes.</title>
<date>1998</date>
<journal>Scandinavian Journal of Statistics,</journal>
<pages>451--482</pages>
<contexts>
<context position="3055" citStr="Møller and Syversveen, 1998" startWordPosition="472" endWordPosition="475"> typically a smooth function of time, we base our model on point processes, which have been shown to model well such data in epidemiology and conflict mapping (Brix and Diggle, 2001; Zammit-Mangion et al., 2012). This framework models count data in a continuous time through the underlying intensity of a Poisson distribution. The posterior distribution can then be used for several inference problems, e.g. to query the expected count of posts, or to find the probability of a count of posts occurring during an arbitrary time interval. We model frequency profiles using a log-Gaussian Cox process (Møller and Syversveen, 1998), a point process where the log-intensity of the Poisson distribution is modelled via a Gaussian Process (GP). GP is a nonparametric model which allows for powerful modelling of the underlying intensity function. Modelling the frequency profile of a rumour based on posts is extremely challenging, since many rumours consist of only a small number of posts and exhibit complex patterns. To overcome this difficulty we propose a multi-task learning approach, where patterns are correlated across multiple rumours. In this way statistics over a larger training set are shared, enabling more reliable pr</context>
<context position="9553" citStr="Møller and Syversveen, 1998" startWordPosition="1548" endWordPosition="1551">, thus limiting the need for constant attention. The second formulation is that of extrapolation, where all observed posts occur before the test intervals. This corresponds to a scenario where the user seeks to predict the future profile of the rumour, e.g., to identify rumours that will attract further attention or wither away. Although our focus here is on rumours, our model is more widely applicable. For example, one could use it to predict whether an advertisement campaign would be successful or how a political campaign would proceed. 4 Model We consider a log-Gaussian Cox process (LGCP) (Møller and Syversveen, 1998), a generalization of inhomogeneous Poisson process. In LGCP the intensity function is assumed to be a stochastic process which varies over time. In fact, the intensity function λ(t) is modelled using a latent function f(t) sampled from a Gaussian process (Rasmussen and Williams, 2005), such that λ(t) = exp (f(t)) (exponent ensures positivity). This provides a non-parametric approach to model the intensity function. The intensity function can be automatically learned from the data set and its complexity depends on the data points. We model the occurrence of posts in a rumour Ei to follow log-G</context>
<context position="10959" citStr="Møller and Syversveen, 1998" startWordPosition="1785" endWordPosition="1788"> models the likelihood that a single tweet occurs at time t in the interval [s, t] for a rumour Ei given the latent function fi(t) as /t p(y = 1|fi) = exp(fi(t)) exp(J exp(fi(u)) du) . s Then, the likelihood of posts EOi in time interval T given a latent function fi can be obtained as mOi p(EOi |fi) =exp(−jT_Tte exp (fi(u)) du + 1:fi(tij) j=1 (1) The likelihood of posts in the rumour data is obtained by taking the product of the likelihoods over individual rumours. The likelihood (1) is commonly approximated by considering subregions of T and assuming constant intensities in sub-regions of T (Møller and Syversveen, 1998; Vanhatalo et al., 2013) to overcome computational difficulties arising due to integration. Following this, we approximate the likelihood as p(EOi |fi) = HSs=1 Poisson(ys |lsexp (fi(˙ts))). Here, time is divided into S intervals indexed by s, ˙ts is the centre of the sth interval, ls is the length of the sth interval and ys is number of tweets posted during this interval. The latent function f is modelled via a Gaussian process (GP) (Rasmussen and Williams, 2005): f(t) ∼ GP(m(t), k(t, t&apos;)), where m is the mean function (equal 0) and k is the kernel specifying how outputs covary as a function </context>
</contexts>
<marker>Møller, Syversveen, 1998</marker>
<rawString>Jesper Møller and Anne Randi Syversveen. 1998. Log Gaussian Cox processes. Scandinavian Journal of Statistics, pages 451–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>380--390</pages>
<contexts>
<context position="16413" citStr="Owoputi et al., 2013" startWordPosition="2696" endWordPosition="2699">mour lifespan, which we split into 20 evenly spaced intervals. This way, our dataset consists in total of 2280 intervals. We iterate over rumours using a form of folded cross-validation, where in each iteration we exclude some (but not all) time intervals for a single target rumour. The excluded time intervals form the test set: either by selecting half at random (interpolation); or by taking only the second half for testing (extrapolation). To ameliorate the problems of data sparsity, we replace words with their Brown cluster ids, using 1000 clusters acquired on a large scale Twitter corpus (Owoputi et al., 2013). The mean function for the underlying GP in LGCP methods is assumed to be 0, which results in intensity function to be around 1 in the absence of nearby observations. This prevents our method from predicting 0 counts in these regions. We add 1 to the counts in the intervals to deal with this problem as a preprocessing step. The original counts can be obtained by decrementing 1 from the predicted counts. Instead, one could use a GP with a non-zero mean function and learn the mean function, a more elegant way of approaching this problem, which we leave for future work. 6 Experiments The left co</context>
</contexts>
<marker>Owoputi, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Preotiuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A temporal model of text periodicities using Gaussian processes.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>977--988</pages>
<contexts>
<context position="5680" citStr="Preotiuc-Pietro and Cohn, 2013" startWordPosition="894" endWordPosition="897">sults. 2 Related Work There have been several descriptive studies of rumours in social media, e.g. Procter et al. (2013) analyzed rumours in tweets about the 2011 London riots and showed that they follow similar lifecycles. Friggeri et al. (2014) showed how Facebook constitutes a rich source of rumours and conversation threads on the topic. However, none of these studies tried to model rumour dynamics. The problem of modelling the temporal nature of social media explicitly has received little attention. The work most closely related modelled hash tag frequency time-series in Twitter using GP (Preotiuc-Pietro and Cohn, 2013). It made several simplifications, including discretising time and treating the problem of modelling counts as regression, which are both inappropriate. In contrast we take a more principled approach, using a point process. We use the proposed GP-based method as a baseline to demonstrate the benefit of using our approaches. The log-Gaussian Cox process has been applied for disease and conflict mapping, e.g. ZammitMangion et al. (2012) developed a spatio-temporal model of conflict events in Afghanistan. In contrast here we deal with temporal text data, and model several correlated outputs rathe</context>
<context position="15070" citStr="Preotiuc-Pietro and Cohn (2013)" startWordPosition="2473" endWordPosition="2476">Since probabilistic models (GP, LGCP) return distributions over possible outputs, we also evaluate them via the log-likelihood (LL) of the true counts under the returned distributions (respectively Gaussian and Poisson distribution). Baselines We use the following baselines. The first is the Homogenous Poisson Process (HPP) trained on the training set of the rumour. We select its intensity A using maximum likelihood estimate, which equals to the mean frequency of posts in the training intervals. The second baseline is Gaussian Process (GP) used for predicting hashtag frequencies in Twitter by Preotiuc-Pietro and Cohn (2013). Authors considered various kernels in their experiments, most notably periodic kernels. In our case it is not apparent that rumours exhibit periodic characteristics, as can be seen in Figure 1. We restrict our focus to RBF kernel and leave inspection of other kernels such as periodic ones for both GP and LGCP models for future. The third baseline is to always predict 0 posts in all intervals. The fourth baseline is tailored for the interpolation setting, and uses simple interpolation by averaging over the frequencies of the closest left and right intervals, or the frequency of the closest in</context>
</contexts>
<marker>Preotiuc-Pietro, Cohn, 2013</marker>
<rawString>Daniel Preotiuc-Pietro and Trevor Cohn. 2013. A temporal model of text periodicities using Gaussian processes. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 977–988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Procter</author>
<author>Jeremy Crump</author>
<author>Susanne Karstedt</author>
<author>Alex Voss</author>
<author>Marta Cantijoch</author>
</authors>
<title>Reading the riots: What were the police doing on twitter? Policing and society,</title>
<date>2013</date>
<pages>23--4</pages>
<contexts>
<context position="5169" citStr="Procter et al. (2013)" startWordPosition="811" endWordPosition="814">spond to mean predictions by the models, light shaded areas denote the 95% confidence interval, µ ± 2σ. This figure is best viewed in colour. prediction of rumour popularity. This paper makes the following contributions: 1. Introduces the problem of modelling rumour frequency profiles, and presents a method based on a log-Gaussian Cox process; 2. Incorporates multi-task learning to generalize across disparate rumours; and 3. Demonstrates how incorporating text into multi-task learning improves results. 2 Related Work There have been several descriptive studies of rumours in social media, e.g. Procter et al. (2013) analyzed rumours in tweets about the 2011 London riots and showed that they follow similar lifecycles. Friggeri et al. (2014) showed how Facebook constitutes a rich source of rumours and conversation threads on the topic. However, none of these studies tried to model rumour dynamics. The problem of modelling the temporal nature of social media explicitly has received little attention. The work most closely related modelled hash tag frequency time-series in Twitter using GP (Preotiuc-Pietro and Cohn, 2013). It made several simplifications, including discretising time and treating the problem o</context>
</contexts>
<marker>Procter, Crump, Karstedt, Voss, Cantijoch, 2013</marker>
<rawString>Rob Procter, Jeremy Crump, Susanne Karstedt, Alex Voss, and Marta Cantijoch. 2013. Reading the riots: What were the police doing on twitter? Policing and society, 23(4):413–436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Edward Rasmussen</author>
<author>Christopher K I Williams</author>
</authors>
<date>2005</date>
<booktitle>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning).</booktitle>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9839" citStr="Rasmussen and Williams, 2005" startWordPosition="1595" endWordPosition="1598">ill attract further attention or wither away. Although our focus here is on rumours, our model is more widely applicable. For example, one could use it to predict whether an advertisement campaign would be successful or how a political campaign would proceed. 4 Model We consider a log-Gaussian Cox process (LGCP) (Møller and Syversveen, 1998), a generalization of inhomogeneous Poisson process. In LGCP the intensity function is assumed to be a stochastic process which varies over time. In fact, the intensity function λ(t) is modelled using a latent function f(t) sampled from a Gaussian process (Rasmussen and Williams, 2005), such that λ(t) = exp (f(t)) (exponent ensures positivity). This provides a non-parametric approach to model the intensity function. The intensity function can be automatically learned from the data set and its complexity depends on the data points. We model the occurrence of posts in a rumour Ei to follow log-Gaussian Cox process (LGCP) with intensity λi(t), where λi(t) = exp(fi(t)). We associate a distinct intensity function with each rumour as they have varying temporal profiles. LGCP models the likelihood that a single tweet occurs at time t in the interval [s, t] for a rumour Ei given th</context>
<context position="11427" citStr="Rasmussen and Williams, 2005" startWordPosition="1863" endWordPosition="1866">rs. The likelihood (1) is commonly approximated by considering subregions of T and assuming constant intensities in sub-regions of T (Møller and Syversveen, 1998; Vanhatalo et al., 2013) to overcome computational difficulties arising due to integration. Following this, we approximate the likelihood as p(EOi |fi) = HSs=1 Poisson(ys |lsexp (fi(˙ts))). Here, time is divided into S intervals indexed by s, ˙ts is the centre of the sth interval, ls is the length of the sth interval and ys is number of tweets posted during this interval. The latent function f is modelled via a Gaussian process (GP) (Rasmussen and Williams, 2005): f(t) ∼ GP(m(t), k(t, t&apos;)), where m is the mean function (equal 0) and k is the kernel specifying how outputs covary as a function of the inputs. We use a Radial Basis Function (RBF) kernel, k(t, t&apos;) = a exp(−(t − t&apos;)2/l), where lengthscale l controls the extent to which nearby points influence one another and a controls the scale of the function. The distribution of the posterior p(fi(t)|EOi ) at an arbitrary timestamp t is calculated based on the specified prior and the Poisson likelihood. It is intractable and approximation techniques are required. There exist various methods to deal with </context>
</contexts>
<marker>Rasmussen, Williams, 2005</marker>
<rawString>Carl Edward Rasmussen and Christopher K. I. Williams. 2005. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jarno Vanhatalo</author>
<author>Jaakko Riihim¨aki</author>
<author>Jouni Hartikainen</author>
<author>Pasi Jyl¨anki</author>
<author>Ville Tolvanen</author>
<author>Aki Vehtari</author>
</authors>
<title>Gpstuff: Bayesian modeling with Gaussian processes.</title>
<date>2013</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>14</volume>
<issue>1</issue>
<marker>Vanhatalo, Riihim¨aki, Hartikainen, Jyl¨anki, Tolvanen, Vehtari, 2013</marker>
<rawString>Jarno Vanhatalo, Jaakko Riihim¨aki, Jouni Hartikainen, Pasi Jyl¨anki, Ville Tolvanen, and Aki Vehtari. 2013. Gpstuff: Bayesian modeling with Gaussian processes. J. Mach. Learn. Res., 14(1):1175–1179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuang-Hong Yang</author>
<author>Hongyuan Zha</author>
</authors>
<title>Mixture of mutually exciting processes for viral diffusion.</title>
<date>2013</date>
<booktitle>In ICML (2), volume 28 of JMLR Proceedings,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="6460" citStr="Yang and Zha, 2013" startWordPosition="1014" endWordPosition="1017">ast we take a more principled approach, using a point process. We use the proposed GP-based method as a baseline to demonstrate the benefit of using our approaches. The log-Gaussian Cox process has been applied for disease and conflict mapping, e.g. ZammitMangion et al. (2012) developed a spatio-temporal model of conflict events in Afghanistan. In contrast here we deal with temporal text data, and model several correlated outputs rather than their single output. Related also is the extensive work done in spatio-temporal modelling of meme spread. One example is application of Hawkes processes (Yang and Zha, 2013), a probabilistic framework for modelling self-excitatory phenomena. However, these models were mainly used for network modelling rather than revealing complex temporal patterns, which may emerge only implicitly, and are more limited in the kinds of temporal patterns that may be represented. 3 Data &amp; Problem In this section we describe the data and we formalize the problem of modelling rumour popularity. Data We use the Ferguson rumour data set (Zubiaga et al., 2015), consisting of tweets collected in August and September 2014 during the Ferguson unrest. It contains both source tweets and the </context>
</contexts>
<marker>Yang, Zha, 2013</marker>
<rawString>Shuang-Hong Yang and Hongyuan Zha. 2013. Mixture of mutually exciting processes for viral diffusion. In ICML (2), volume 28 of JMLR Proceedings, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Zammit-Mangion</author>
<author>Michael Dewar</author>
<author>Visakan Kadirkamanathan</author>
<author>Guido Sanguinetti</author>
</authors>
<title>Point process modelling of the Afghan War Diary.</title>
<date>2012</date>
<booktitle>Proceedings of the National Academy of Sciences of the United States of America,</booktitle>
<volume>109</volume>
<issue>31</issue>
<pages>12419</pages>
<contexts>
<context position="2638" citStr="Zammit-Mangion et al., 2012" startWordPosition="407" endWordPosition="410"> further attention. A second factor is text from the posts themselves, where phrases such as not true, unconfirmed, or debunk help users judge veracity and thus limit rumour spread (Zhao et al., 2015). This paper considers the problem of modelling temporal frequency profiles of rumours by taking into account both the temporal and textual information. Since posts occur at continuous timestamps, and their density is typically a smooth function of time, we base our model on point processes, which have been shown to model well such data in epidemiology and conflict mapping (Brix and Diggle, 2001; Zammit-Mangion et al., 2012). This framework models count data in a continuous time through the underlying intensity of a Poisson distribution. The posterior distribution can then be used for several inference problems, e.g. to query the expected count of posts, or to find the probability of a count of posts occurring during an arbitrary time interval. We model frequency profiles using a log-Gaussian Cox process (Møller and Syversveen, 1998), a point process where the log-intensity of the Poisson distribution is modelled via a Gaussian Process (GP). GP is a nonparametric model which allows for powerful modelling of the u</context>
</contexts>
<marker>Zammit-Mangion, Dewar, Kadirkamanathan, Sanguinetti, 2012</marker>
<rawString>Andrew Zammit-Mangion, Michael Dewar, Visakan Kadirkamanathan, and Guido Sanguinetti. 2012. Point process modelling of the Afghan War Diary. Proceedings of the National Academy of Sciences of the United States of America, 109(31):12414– 12419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhe Zhao</author>
<author>Paul Resnick</author>
<author>Qiaozhu Mei</author>
</authors>
<title>Early detection of rumors in social media from enquiry posts.</title>
<date>2015</date>
<booktitle>In International World Wide Web Conference Committee (IW3C2).</booktitle>
<contexts>
<context position="2210" citStr="Zhao et al., 2015" startWordPosition="336" endWordPosition="339">erent rumours exhibit different trajectories. Figure 1 shows two example rumours from our dataset (see Section 3): online discussion of rumour #10 quickly drops away, whereas rumour #37 takes a lot longer to die out. Two characteristics can help determine if a rumour will continue to be discussed. One is the dynamics of post occurrences, e.g. if the frequency profile decays quickly, chances are it would not attract further attention. A second factor is text from the posts themselves, where phrases such as not true, unconfirmed, or debunk help users judge veracity and thus limit rumour spread (Zhao et al., 2015). This paper considers the problem of modelling temporal frequency profiles of rumours by taking into account both the temporal and textual information. Since posts occur at continuous timestamps, and their density is typically a smooth function of time, we base our model on point processes, which have been shown to model well such data in epidemiology and conflict mapping (Brix and Diggle, 2001; Zammit-Mangion et al., 2012). This framework models count data in a continuous time through the underlying intensity of a Poisson distribution. The posterior distribution can then be used for several </context>
</contexts>
<marker>Zhao, Resnick, Mei, 2015</marker>
<rawString>Zhe Zhao, Paul Resnick, and Qiaozhu Mei. 2015. Early detection of rumors in social media from enquiry posts. In International World Wide Web Conference Committee (IW3C2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arkaitz Zubiaga</author>
<author>Maria Liakata</author>
<author>Rob Procter</author>
<author>Kalina Bontcheva</author>
<author>Peter Tolmie</author>
</authors>
<title>Towards detecting rumours in social media.</title>
<date>2015</date>
<booktitle>In AAAI Workshop on AI for Cities.</booktitle>
<contexts>
<context position="6931" citStr="Zubiaga et al., 2015" startWordPosition="1090" endWordPosition="1094">lated also is the extensive work done in spatio-temporal modelling of meme spread. One example is application of Hawkes processes (Yang and Zha, 2013), a probabilistic framework for modelling self-excitatory phenomena. However, these models were mainly used for network modelling rather than revealing complex temporal patterns, which may emerge only implicitly, and are more limited in the kinds of temporal patterns that may be represented. 3 Data &amp; Problem In this section we describe the data and we formalize the problem of modelling rumour popularity. Data We use the Ferguson rumour data set (Zubiaga et al., 2015), consisting of tweets collected in August and September 2014 during the Ferguson unrest. It contains both source tweets and the conversational threads around these (where available). All source tweets are categorized as rumour vs non-rumour, other tweets from the same thread are assigned automatically as belonging to the same event as the source tweet. Since some rumours have few posts, we consider only those with at least 15 posts in the first hour as rumours of particular interest. This results in 114 rumours consisting of a total of 4098 tweets. Problem Definition Let us consider a time in</context>
</contexts>
<marker>Zubiaga, Liakata, Procter, Bontcheva, Tolmie, 2015</marker>
<rawString>Arkaitz Zubiaga, Maria Liakata, Rob Procter, Kalina Bontcheva, and Peter Tolmie. 2015. Towards detecting rumours in social media. In AAAI Workshop on AI for Cities.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>