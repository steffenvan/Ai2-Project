<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013246">
<note confidence="0.938174">
SONE ISSUES IN PARSING AND NATURAL LANGUAGE UNDERSTANDING
Robert J. Bobrow
Bolt Beranek and Newman Inc.
Bonnie L. Webber
</note>
<affiliation confidence="0.4653995">
Department of Computer &amp; Information Science
University of Pennsylvania
</affiliation>
<subsectionHeader confidence="0.572716">
Language is a system for encoding and
</subsectionHeader>
<bodyText confidence="0.9810245">
transmitting ideas. A theory that seeks to
explain linguistic phenomena in terms of this
fact is a Panrtinnal, theory. One that does not
misses the point. [101
</bodyText>
<sectionHeader confidence="0.758658" genericHeader="abstract">
PREAMBLE
</sectionHeader>
<bodyText confidence="0.998746063829787">
Our response to the questions posed to this panel is
influenced by a number of beliefs (or biases!) which we
have developed in the course of building and analyzing
the operation of several natural language understanding
(NLU) systems. [1, 2, 3, 12] While the emphasis of the
panel is on parsing, we feel that the recovery of the
syntactic structure of a natural language utterance
must be viewed as part of a larger process of
recovering the meaning, intentions and goals underlying
its generation. Hence it is inappropriate to consider
designing or evaluating natural language parsers or
grammars without taking into account the architecture
of the whole NLU system of which they&apos;re a part.1 This
is the premise from which our beliefs arise, beliefs
which concern two things:
o the distribution of various types of
knowledge, in particular syntactic knowledge,
among the modules of an NLU system
o the information and control flow among those
modules.
As to the first belief, in the NLU systems we have
worked on, moat syntactic information is localized in a
&amp;quot;syntactic module&amp;quot;, although that module does not
produce a reified data structure representing the
syntactic description of an utterance. Thus, if
&amp;quot;parsing&amp;quot; is taken as requiring the production of such
a reified structure, then we do not believe in its
necessity. However we la believe in the existence of a
module which provides syntactic information to those
other parts of the system whose decisions ride on it.
As to the second belief, we feel that syntax, semantics
and pragmatics effectively constitute parallel but
interacting processors, and that information such as
local syntactic relations is determined by Joint
decisions among them. Our experience shows that with
minimal loss of efficiency, one can design these
processors to interface cleanly with one another, 30 as
to allow independent design, implementation and
modification. We spell out these beliefs in slightly
more detail below, and at greater length in (4].
1We are not claiming that the only factors shaping a
parser or a grammar, beyond syntactic considerations,
are things like meaning, intention, etc. There are
clearly mechanical and memory factors, as well as
laziness - a speaker&apos;s penchant for trying to get away
with the minimal level of effort needed to accomplish
the tank!
</bodyText>
<sectionHeader confidence="0.996637" genericHeader="acknowledgments">
RESPONSES
</sectionHeader>
<subsectionHeader confidence="0.999108">
The Comoutational Persoectiyffi
</subsectionHeader>
<bodyText confidence="0.987397534482759">
The first set of questions to this panel concern the
computational perspective, and the useful purposes
served by distinguishing parsing from interpretation.
We believe that syntactic knowledge plays an important
role in NLU. In particular, we believe that there is a
significant type of utterance description that can be
determined on purely syntactic grounds2, albeit not
necessarily uniquely. This description can be used to
guide semantic and discourse level structure recovery
processes such as interpretation, anaphoric resolution,
focus tracking, given/new distinctions, ellipsis
resolution, etc. in a manner that is independent of the
lexical and conceptual content of the utterance. There
are several advantages to factoring out such knowledge
from the remainder of the NLU system and providing a
&amp;quot;syntactic module&amp;quot; whose interactions with the rest of
the system provide information on the syntactic
structure of an utterance. The first advantage is to
simplify system building, as we know from
experience [1, 2, 3, 4, 5, 12]. Once the pattern of
communication between processors is settled, it is
easier to attach a new semantics to the hooks already
provided in the grammar than to build a new semantic
processor. In addition, because each module has only
to consider a portion of the constraints implicit in
the data (e.g. syntactic constraints, semantic
constraints and discourse context), each module can be
designed to optimize its own processing and provide an
efficient system.
The panel has also been charged with considering
. _
parallel processing as a challenge to its views on
parsing. This touches on our beliefs about the
interaction among the modules that comprise the NLU
system. To respond to this issue, we first want to
distinguish between two types of parallelism: one, in
which many instances of the same thing are done at once
(as in an array of parallel adders) and another, in
which the many things done simultaneously can be
different. Supporting this latter type of parallelism
doesn&apos;t change our view of parsing, but rather
underlies it. We believe that the interconnected
processes involved in NLU must support a basic
operating principle that Norman and Bobrow [14] have
called &amp;quot;The Principle of Continually Available Output&amp;quot;
(CAO). This states that the interacting processes must
begin to provide output over a wide range of resource
allocations, even before their analyzes are complete,
and even before all input data is available. We take
this position for two reasons: one, it facilitates
computational efficiency, and two, it seems to be
closer to human parsing processes (a point which we
will get to in answering the next question).
The requirement that syntactic analysis, semantic
interpretation and discourse processing must be able to
operate in (pseudo-)parallel, obeying the CAO
2that is, solely on the basis of syntactic
categories/features and ordering information
</bodyText>
<page confidence="0.998478">
97
</page>
<bodyText confidence="0.976123438016529">
principle, has sparked our interest in the design of
pairs of processes which can pass forward and backward
useful information/advice/questions as soon as
possible. The added potential for interaction of such
processors can increase the capability and efficiency
of the overall NLU process. Thus, for example, if the
syntactic module makes its intermediate decisions
available to semantics and/or pragmatics, then those
processors can evaluate those decisions, guide syntax&apos;s
future behavior and, in addition, develop in parallel
their own analyses. Having sent on its latest
assertion/advice/question, whether syntax then decides
to continue on with something else or wait for a
response will depend on the particular kind of message
sent. Thus, the parsers and grammars that concern us
are 0003 able to work with other appropriately designed
components to support CAO. While the equipment we are
using to implement and test our ideas is serial, we
take very seriously the notion of parallelism.
Finally under the heading of &amp;quot;Computational
Perspective&amp;quot;, we are asked about what might motivate
our trying to make parsing procedures simulate what we
suspect human parsing processes to be like. One
motivation for U3 is the belief that natural language
is so tuned to the part extraordinary, part banal
cognitive capabilities of human beings that only by
simulating human parsing processes can we cover all and
only the language phenomena that we are called upon to
process. A particular (extraordinary) aspect of human
cognitive (and hence, parsing) behavior that we want to
explore and eventually simulate is people&apos;s ability to
respond even under degraded data or resource
limitations. There are examples of listeners
initiating reasonable responses to an utterance even
before the utterance is complete, and in some case even
before a complete syntactic unit has been heard.
Simultaneous translation is one notable example [8],
and another is provided by the performance of subjects
in a verbally guided assembly task reported by P. Cohen
[6]. Such an ability to produce output before all
input data is available (or before enough processing
resources have been made available to produce the best
possible response) is what led Norman and Bobrow to
formulate their CAO Principle. Our interest is in
architectures for NLU systems which support CAO and in
search strategies through such architectures for an
optimal interpretation.
The Linguistic Persnect4ve
We have been asked to comment on legitimate inferences
about human linguistic competence and performance that
we can draw from our experiences with mechanical
parsing of formal grammar. Our response is that
whatever parsing is for natural languages, it is still
only part of a larger process. Just because we know
what parsing is in formal language systems, we do not
necessarily know what role it plays is in the context
of total communication. Simply put, formal notions of
parsing underconatrain the goals of the syntactic
component of an NLU system. Efficiency measures, based
on the resources required for generation of one or all
complete parses for a sentence, without semantic or
pragmatic interaction, do not necessarily specify
desirable properties of .a natural language syntactic
analysis component.
As for whether the efficiency of parsing algorithms for
CF or regular grammars suggest that the core of NL
:grammars is CF or regular, we want to distinguish that
part of perception (and hence, syntactic analysis)
which groups the stimulus into recognizable units from
that part which fills in gaps in information
(inferentially) on the basis of such groups. Results
in CF grammar theory says that grouping is not best
done purely bottom-up, that there are advantages to
using predictive mechanisms as well (9, 77. This
suggests two things for parsing natural language:
1. There is a level of evidence and a process
for using it that is working to suggest
groups.
2. There is another filtering, inferencing
mechanism that makes predictions and
diagnoses on the basis of those groups.
It is possible that the grouping mechanism may make use
of strategies applicable to CF parsing, such as well-
formed substring tables or charts, without requiring
the overall language specification be CF. In our
current RUS/PSI -CLONE system, grouping is a function of
the syntactic module: its output consists of suggested
groupings. These suggestions may be at abstract,
specific or disjunctive. For example, an abstract
description might be &amp;quot;this is the head of an NP,
everything to its left is a pre-modifier&amp;quot;. Here there
is no comment about exactly how these pre-modifiers
group. A disjunctive description would consist of an
explicit enumeration of all the possibilities at some
point (e.g., &apos;this is either a time prepositional
phrase (PP) or an agentive PP or a locative PP, etc.&amp;quot;).
Disjunctive descriptions allow us to prune
possibilities via case analysis.
In short, we believe in using as much evidence from
formal systems as seems understandable and reasonable,
to constrain what the system should be doing.
The Interactioan
Finally, we have been asked about the nature of the
relationship between a grammar and a procedure for
applying it. On the systems building side, our feeling
is that while one should be able to take a grammar and
convert it to a recognition or generation
procedure (10], it is likely that such procedures will
embody a whole set of principles that are control
structure related, and mar, part of the grammar. For
example, a grammar need not specify in what order to
look for things or in what order decisions should be
made. Thus, one may not be able to reconstruct the
grammar unirmolv from a procedure for applying it.
On the other hand, on the human parsing side, we
definitely feel that natural language is strongly tuned
to both people&apos;s means of production and their means of
recognition, and that principles like McDonalds&apos;
Indelibility Principle [13] or Marcus&apos; Determinism
Hypothesis (11) shape what are (and are not) seen as
sentences of the language.
</bodyText>
<sectionHeader confidence="0.998898" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999893272727273">
1. Bobrow, R. J. The RUE System. BBN Report 3878,
Bolt Beranek and Newman Inc., 1978.
2. Bobrow, R. J. &amp; Webber, B. L. PSI-CLONE - Parsing
and Semantic Interpretation in the BBC Natural Language
Understanding System. CSCSI/CSEIO Annual Conference,
CSCSI/CSEIO, 1980.
3. Bobrow, R. J. &amp; Webber, B. L. Knowledge
Representation for Syntactic/Semantic Processing.
Proceedings of The First Annual National Conference on
Artificial Intelligence, American Association for
Artificial Intelligence, 1980.
</reference>
<page confidence="0.988885">
98
</page>
<reference confidence="0.999872447368421">
4. Bobrow, R.J. &amp; Webber, B.L. Parsing and Semantic
Interpretation as an Incremental Recognition Process.
Proceedings of a Symposium on Modelling Human Parsing
Strategies, Center for Cognitive Science, University of
Texas, Austin TX, 1981.
5. Bobrow, R.J. &amp; Webber, B.L. Systems Considerations
for Search by Cooperating Processes: Providing
Continually Available Output. Proceedings of the Sixth
IJCAI, International Joint Conference on Artificial
Intelligence, 1981.
6. Cohen, P. personal communication. videotape of
experimental task
7. Earley, J. An efficient context-free parsing
algorithm. faamigusasIgAA Aim Am 13 (February
1970), 94-102.
8. Goldman-Eisler, F. Psychological Mechanisms of
Speech Production as Studied through the Analysis of
Simultaneous Translation. In B. Butterworth, Ed.,
jsflguaew Production, Academic Press, 1980.
9. Graham, S., Harrison, M. and Ruzzo, W. An Improved
Context-Free Recognizer. AGM TransactlonA
greiczawang Litumagga ma „Systems (July 1980), 416 -
463.
10. Kay, M. An Algorithm for Compiling Parsing Tables
from a Grammar. Proceedings of a Symposium on
Modelling Human Parsing Strategies, Center for
Cognitive Science, University of Texas, Austin TX,
1981.
11. Marcus, H. AlbAtora.s2LlizIagtialtsraisaitliant2E
Natural Igingaagg. MIT Press, 1980.
12. Mark, W. S. &amp; Barton, G. E. The RUSGrammar
Parsing System. OR 3243, General Motors Research
Laboratories, 1980.
13. McDonald, D. ???. Ph.D. Th., Massachusetts
Institute of Technology, 1980.
14. Norman, D. &amp; Bobrow, D. On Data-limited and
Resource-limited Processes. C5I.74 -2, Xerox PARC, May,
1974.
</reference>
<page confidence="0.998968">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.022409">
<title confidence="0.999435">SONE ISSUES IN PARSING AND NATURAL LANGUAGE UNDERSTANDING</title>
<author confidence="1">Robert J Bobrow</author>
<affiliation confidence="0.584184">Bolt Beranek and Newman Inc.</affiliation>
<author confidence="0.994396">Bonnie L Webber</author>
<affiliation confidence="0.999525">Department of Computer &amp; Information Science University of Pennsylvania</affiliation>
<abstract confidence="0.994569746835444">Language is a system for encoding and transmitting ideas. A theory that seeks to explain linguistic phenomena in terms of this fact is a Panrtinnal, theory. One that does not misses the point. [101 PREAMBLE Our response to the questions posed to this panel is influenced by a number of beliefs (or biases!) which we have developed in the course of building and analyzing the operation of several natural language understanding systems. [1, 2, While the emphasis of the panel is on parsing, we feel that the recovery of the syntactic structure of a natural language utterance must be viewed as part of a larger process of recovering the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluating natural language parsers or grammars without taking into account the architecture the whole NLU system of which they&apos;re a This premise from which our beliefs arise, beliefs which concern two things: o the distribution of various types of knowledge, in particular syntactic knowledge, among the modules of an NLU system o the information and control flow among those modules. As to the first belief, in the NLU systems we have on, moat syntactic information in a &amp;quot;syntactic module&amp;quot;, although that module does not produce a reified data structure representing the syntactic description of an utterance. Thus, if &amp;quot;parsing&amp;quot; is taken as requiring the production of such a reified structure, then we do not believe in its necessity. However we la believe in the existence of a module which provides syntactic information to those other parts of the system whose decisions ride on it. As to the second belief, we feel that syntax, semantics and pragmatics effectively constitute parallel but interacting processors, and that information such as syntactic relations by decisions among them. Our experience shows that with minimal loss of efficiency, one can design these to interface cleanly with one another, to allow independent design, implementation and modification. We spell out these beliefs in slightly detail below, and at greater length are not claiming that the only factors shaping a parser or a grammar, beyond syntactic considerations, are things like meaning, intention, etc. There are clearly mechanical and memory factors, as well as a speaker&apos;s penchant to get away with the minimal level of effort needed to accomplish RESPONSES The Comoutational Persoectiyffi The first set of questions to this panel concern the computational perspective, and the useful purposes served by distinguishing parsing from interpretation. We believe that syntactic knowledge plays an important role in NLU. In particular, we believe that there is a significant type of utterance description that can be on purely syntactic albeit not necessarily uniquely. This description can be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from [1, 2, 5, 12]. Once the pattern of between processors it is attach a new semantics to the hooks already provided in the grammar than to build a new semantic because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our beliefs about the interaction among the modules that comprise the NLU To respond to this first want to distinguish between two types of parallelism: one, in which many instances of the same thing are done at once array of parallel adders) and another, in which the many things done simultaneously can be different. Supporting this latter type of parallelism doesn&apos;t change our view of parsing, but rather underlies it. We believe that the interconnected processes involved in NLU must support a basic operating principle that Norman and Bobrow [14] have called &amp;quot;The Principle of Continually Available Output&amp;quot; (CAO). This states that the interacting processes must begin to provide output over a wide range of resource allocations, even before their analyzes are complete, and even before all input data is available. We take this position for two reasons: one, it facilitates computational efficiency, and two, it seems to be closer to human parsing processes (a point which we will get to in answering the next question). The requirement that syntactic analysis, semantic and discourse processing must to operate in (pseudo-)parallel, obeying the CAO is, solely on the basis of syntactic categories/features and ordering information 97 principle, has sparked our interest in the design of pairsof processes which can pass forward and backward useful information/advice/questions as soon as possible. The added potential for interaction of such processors can increase the capability and efficiency the overall NLU process. Thus, for example, syntactic module makes its intermediate decisions available to semantics and/or pragmatics, then those processors can evaluate those decisions, guide syntax&apos;s future behavior and, in addition, develop in parallel their own analyses. Having sent on its latest assertion/advice/question, whether syntax then decides to continue on with something else or wait for a response will depend on the particular kind of message sent. Thus, the parsers and grammars that concern us to work with other appropriately designed components to support CAO. While the equipment we are to implement our ideas is serial, we take very seriously the notion of parallelism. Finally under the heading of &amp;quot;Computational Perspective&amp;quot;, we are asked about what might motivate our trying to make parsing procedures simulate what we suspect human parsing processes to be like. One for the belief that natural language is so tuned to the part extraordinary, part banal cognitive capabilities of human beings that only by simulating human parsing processes can we cover all and only the language phenomena that we are called upon to process. A particular (extraordinary) aspect of human cognitive (and hence, parsing) behavior that we want to and eventually simulate ability to respond even under degraded data or resource limitations. There are examples of listeners initiating reasonable responses to an utterance even before the utterance is complete, and in some case even before a complete syntactic unit has been heard. Simultaneous translation is one notable example [8], and another is provided by the performance of subjects in a verbally guided assembly task reported by P. Cohen an ability to produce output before all data (or before enough processing resources have been made available to produce the best possible response) is what led Norman and Bobrow to their CAO Principle. Our interest in architectures for NLU systems which support CAO and in search strategies through such architectures for an optimal interpretation. have to comment on legitimate inferences about human linguistic competence and performance that we can draw from our experiences with mechanical of formal grammar. Our response that parsing natural languages, it only part of a larger process. Just because we know what parsing is in formal language systems, we do not necessarily know what role it plays is in the context of total communication. Simply put, formal notions of parsing underconatrain the goals of the syntactic component of an NLU system. Efficiency measures, based on the resources required for generation of one or all complete parses for a sentence, without semantic or pragmatic interaction, do not necessarily specify desirable properties of .a natural language syntactic analysis component. As for whether the efficiency of parsing algorithms for regular grammars suggest that the core of NL is regular, we want to distinguish that part of perception (and hence, syntactic analysis) which groups the stimulus into recognizable units from part which fills in gaps (inferentially) on the basis of such groups. Results in CF grammar theory says that grouping is not best done purely bottom-up, that there are advantages to predictive mechanisms as well 77. suggests two things for parsing natural language: 1. There is a level of evidence and a process for using it that is working to suggest groups. 2. There is another filtering, inferencing mechanism that makes predictions and diagnoses on the basis of those groups. It is possible that the grouping mechanism may make use of strategies applicable to CF parsing, such as wellformed substring tables or charts, without requiring overall language specification be our current RUS/PSI -CLONE system, grouping is a function of the syntactic module: its output consists of suggested groupings. These suggestions may be at abstract, specific or disjunctive. For example, an abstract description might be &amp;quot;this is the head of an NP, to its left pre-modifier&amp;quot;. Here there comment about exactly how these pre-modifiers group. A disjunctive description would consist of an explicit enumeration of all the possibilities at some (e.g., &apos;this a time prepositional phrase (PP) or an agentive PP or a locative PP, etc.&amp;quot;). Disjunctive descriptions allow us to prune possibilities via case analysis. In short, we believe in using as much evidence from formal systems as seems understandable and reasonable, to constrain what the system should be doing. The Interactioan we have about the nature of the relationship between a grammar and a procedure for applying it. On the systems building side, our feeling is that while one should be able to take a grammar and convert it to a recognition or generation procedure (10], it is likely that such procedures will embody a whole set of principles that are control related, and of the grammar. For example, a grammar need not specify in what order to look for things or in what order decisions should be made. Thus, one may not be able to reconstruct the unirmolvfrom a procedure for applying it. On the other hand, on the human parsing side, we definitely feel that natural language is strongly tuned to both people&apos;s means of production and their means of recognition, and that principles like McDonalds&apos; Indelibility Principle [13] or Marcus&apos; Determinism Hypothesis (11) shape what are (and are not) seen as sentences of the language. REFERENCES 1. Bobrow, R. J. The RUE System. BBN Report 3878, Beranek and Newman 2. Bobrow, R. J. &amp; Webber, B. L. PSI-CLONE - Parsing</abstract>
<affiliation confidence="0.5944865">and Semantic Interpretation in the BBC Natural Language Understanding System. CSCSI/CSEIO Annual Conference,</affiliation>
<note confidence="0.966349909090909">Bobrow, R. &amp; B. L. Knowledge Representation for Syntactic/Semantic Processing. Proceedings of The First Annual National Conference on Artificial Intelligence, American Association for Intelligence, 98 4. Bobrow, R.J. &amp; Webber, B.L. Parsing and Semantic Interpretation as an Incremental Recognition Process. Proceedings of a Symposium on Modelling Human Parsing Strategies, Center for Cognitive Science, University of Texas, Austin TX, 1981. 5. Bobrow, R.J. &amp; Webber, B.L. Systems Considerations for Search by Cooperating Processes: Providing Continually Available Output. Proceedings of the Sixth IJCAI, International Joint Conference on Artificial Intelligence, 1981. 6. Cohen, P. personal communication. videotape of experimental task 7. Earley, J. An efficient context-free parsing Aim 13 1970), 94-102. 8. Goldman-Eisler, F. Psychological Mechanisms of Speech Production as Studied through the Analysis of Simultaneous Translation. In B. Butterworth, Ed., Production,Academic Press, 1980. 9. Graham, S., Harrison, M. and Ruzzo, W. An Improved Recognizer. Litumagga „Systems(July 1980), 416 - 463. 10. Kay, M. An Algorithm for Compiling Parsing Tables from a Grammar. Proceedings of a Symposium on Modelling Human Parsing Strategies, Center for Cognitive Science, University of Texas, Austin TX, 1981. Marcus, H. NaturalIgingaagg. Press, 1980. 12. Mark, W. S. &amp; Barton, G. E. The RUSGrammar System. OR Motors Research Laboratories, 1980. 13. McDonald, D. ???. Ph.D. Th., Massachusetts Institute of Technology, 1980. 14. Norman, D. &amp; Bobrow, D. On Data-limited and Resource-limited Processes. C5I.74 -2, Xerox PARC, May, 1974.</note>
<intro confidence="0.559256">99</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
</authors>
<title>The RUE System.</title>
<date>1978</date>
<tech>BBN Report 3878,</tech>
<institution>Bolt Beranek and Newman Inc.,</institution>
<contexts>
<context position="648" citStr="[1, 2, 3, 12]" startWordPosition="101" endWordPosition="104">LANGUAGE UNDERSTANDING Robert J. Bobrow Bolt Beranek and Newman Inc. Bonnie L. Webber Department of Computer &amp; Information Science University of Pennsylvania Language is a system for encoding and transmitting ideas. A theory that seeks to explain linguistic phenomena in terms of this fact is a Panrtinnal, theory. One that does not misses the point. [101 PREAMBLE Our response to the questions posed to this panel is influenced by a number of beliefs (or biases!) which we have developed in the course of building and analyzing the operation of several natural language understanding (NLU) systems. [1, 2, 3, 12] While the emphasis of the panel is on parsing, we feel that the recovery of the syntactic structure of a natural language utterance must be viewed as part of a larger process of recovering the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluating natural language parsers or grammars without taking into account the architecture of the whole NLU system of which they&apos;re a part.1 This is the premise from which our beliefs arise, beliefs which concern two things: o the distribution of various types of knowledge, in particular syntact</context>
<context position="3796" citStr="[1, 2, 3, 4, 5, 12]" startWordPosition="592" endWordPosition="597">n be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from experience [1, 2, 3, 4, 5, 12]. Once the pattern of communication between processors is settled, it is easier to attach a new semantics to the hooks already provided in the grammar than to build a new semantic processor. In addition, because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our belief</context>
</contexts>
<marker>1.</marker>
<rawString>Bobrow, R. J. The RUE System. BBN Report 3878, Bolt Beranek and Newman Inc., 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
<author>B L Webber</author>
</authors>
<title>PSI-CLONE - Parsing and Semantic Interpretation in the BBC Natural Language Understanding System.</title>
<date>1980</date>
<booktitle>CSCSI/CSEIO Annual Conference, CSCSI/CSEIO,</booktitle>
<contexts>
<context position="648" citStr="[1, 2, 3, 12]" startWordPosition="101" endWordPosition="104">LANGUAGE UNDERSTANDING Robert J. Bobrow Bolt Beranek and Newman Inc. Bonnie L. Webber Department of Computer &amp; Information Science University of Pennsylvania Language is a system for encoding and transmitting ideas. A theory that seeks to explain linguistic phenomena in terms of this fact is a Panrtinnal, theory. One that does not misses the point. [101 PREAMBLE Our response to the questions posed to this panel is influenced by a number of beliefs (or biases!) which we have developed in the course of building and analyzing the operation of several natural language understanding (NLU) systems. [1, 2, 3, 12] While the emphasis of the panel is on parsing, we feel that the recovery of the syntactic structure of a natural language utterance must be viewed as part of a larger process of recovering the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluating natural language parsers or grammars without taking into account the architecture of the whole NLU system of which they&apos;re a part.1 This is the premise from which our beliefs arise, beliefs which concern two things: o the distribution of various types of knowledge, in particular syntact</context>
<context position="3796" citStr="[1, 2, 3, 4, 5, 12]" startWordPosition="592" endWordPosition="597">n be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from experience [1, 2, 3, 4, 5, 12]. Once the pattern of communication between processors is settled, it is easier to attach a new semantics to the hooks already provided in the grammar than to build a new semantic processor. In addition, because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our belief</context>
</contexts>
<marker>2.</marker>
<rawString>Bobrow, R. J. &amp; Webber, B. L. PSI-CLONE - Parsing and Semantic Interpretation in the BBC Natural Language Understanding System. CSCSI/CSEIO Annual Conference, CSCSI/CSEIO, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
<author>B L Webber</author>
</authors>
<title>Knowledge Representation for Syntactic/Semantic Processing.</title>
<date>1980</date>
<booktitle>Proceedings of The First Annual National Conference on Artificial Intelligence, American Association for Artificial Intelligence,</booktitle>
<contexts>
<context position="648" citStr="[1, 2, 3, 12]" startWordPosition="101" endWordPosition="104">LANGUAGE UNDERSTANDING Robert J. Bobrow Bolt Beranek and Newman Inc. Bonnie L. Webber Department of Computer &amp; Information Science University of Pennsylvania Language is a system for encoding and transmitting ideas. A theory that seeks to explain linguistic phenomena in terms of this fact is a Panrtinnal, theory. One that does not misses the point. [101 PREAMBLE Our response to the questions posed to this panel is influenced by a number of beliefs (or biases!) which we have developed in the course of building and analyzing the operation of several natural language understanding (NLU) systems. [1, 2, 3, 12] While the emphasis of the panel is on parsing, we feel that the recovery of the syntactic structure of a natural language utterance must be viewed as part of a larger process of recovering the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluating natural language parsers or grammars without taking into account the architecture of the whole NLU system of which they&apos;re a part.1 This is the premise from which our beliefs arise, beliefs which concern two things: o the distribution of various types of knowledge, in particular syntact</context>
<context position="3796" citStr="[1, 2, 3, 4, 5, 12]" startWordPosition="592" endWordPosition="597">n be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from experience [1, 2, 3, 4, 5, 12]. Once the pattern of communication between processors is settled, it is easier to attach a new semantics to the hooks already provided in the grammar than to build a new semantic processor. In addition, because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our belief</context>
</contexts>
<marker>3.</marker>
<rawString>Bobrow, R. J. &amp; Webber, B. L. Knowledge Representation for Syntactic/Semantic Processing. Proceedings of The First Annual National Conference on Artificial Intelligence, American Association for Artificial Intelligence, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
<author>B L Webber</author>
</authors>
<title>Parsing and Semantic Interpretation as an Incremental Recognition Process. Proceedings of a Symposium on Modelling Human Parsing Strategies,</title>
<date>1981</date>
<institution>Center for Cognitive Science, University of Texas,</institution>
<location>Austin TX,</location>
<contexts>
<context position="3796" citStr="[1, 2, 3, 4, 5, 12]" startWordPosition="592" endWordPosition="597">n be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from experience [1, 2, 3, 4, 5, 12]. Once the pattern of communication between processors is settled, it is easier to attach a new semantics to the hooks already provided in the grammar than to build a new semantic processor. In addition, because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our belief</context>
</contexts>
<marker>4.</marker>
<rawString>Bobrow, R.J. &amp; Webber, B.L. Parsing and Semantic Interpretation as an Incremental Recognition Process. Proceedings of a Symposium on Modelling Human Parsing Strategies, Center for Cognitive Science, University of Texas, Austin TX, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Bobrow</author>
<author>B L Webber</author>
</authors>
<title>Systems Considerations for Search by Cooperating Processes: Providing Continually Available Output.</title>
<date>1981</date>
<booktitle>Proceedings of the Sixth IJCAI, International Joint Conference on Artificial Intelligence,</booktitle>
<contexts>
<context position="3796" citStr="[1, 2, 3, 4, 5, 12]" startWordPosition="592" endWordPosition="597">n be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from experience [1, 2, 3, 4, 5, 12]. Once the pattern of communication between processors is settled, it is easier to attach a new semantics to the hooks already provided in the grammar than to build a new semantic processor. In addition, because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our belief</context>
</contexts>
<marker>5.</marker>
<rawString>Bobrow, R.J. &amp; Webber, B.L. Systems Considerations for Search by Cooperating Processes: Providing Continually Available Output. Proceedings of the Sixth IJCAI, International Joint Conference on Artificial Intelligence, 1981.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Cohen</author>
</authors>
<title>personal communication. videotape of experimental task</title>
<contexts>
<context position="7728" citStr="[6]" startWordPosition="1215" endWordPosition="1215"> we are called upon to process. A particular (extraordinary) aspect of human cognitive (and hence, parsing) behavior that we want to explore and eventually simulate is people&apos;s ability to respond even under degraded data or resource limitations. There are examples of listeners initiating reasonable responses to an utterance even before the utterance is complete, and in some case even before a complete syntactic unit has been heard. Simultaneous translation is one notable example [8], and another is provided by the performance of subjects in a verbally guided assembly task reported by P. Cohen [6]. Such an ability to produce output before all input data is available (or before enough processing resources have been made available to produce the best possible response) is what led Norman and Bobrow to formulate their CAO Principle. Our interest is in architectures for NLU systems which support CAO and in search strategies through such architectures for an optimal interpretation. The Linguistic Persnect4ve We have been asked to comment on legitimate inferences about human linguistic competence and performance that we can draw from our experiences with mechanical parsing of formal grammar.</context>
</contexts>
<marker>6.</marker>
<rawString>Cohen, P. personal communication. videotape of experimental task</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>faamigusasIgAA Aim Am</journal>
<volume>13</volume>
<pages>94--102</pages>
<marker>7.</marker>
<rawString>Earley, J. An efficient context-free parsing algorithm. faamigusasIgAA Aim Am 13 (February 1970), 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Goldman-Eisler</author>
</authors>
<title>Psychological Mechanisms of Speech Production as Studied through the Analysis of Simultaneous Translation.</title>
<date>1980</date>
<booktitle>In B. Butterworth, Ed., jsflguaew Production,</booktitle>
<publisher>Academic Press,</publisher>
<contexts>
<context position="7612" citStr="[8]" startWordPosition="1195" endWordPosition="1195">f human beings that only by simulating human parsing processes can we cover all and only the language phenomena that we are called upon to process. A particular (extraordinary) aspect of human cognitive (and hence, parsing) behavior that we want to explore and eventually simulate is people&apos;s ability to respond even under degraded data or resource limitations. There are examples of listeners initiating reasonable responses to an utterance even before the utterance is complete, and in some case even before a complete syntactic unit has been heard. Simultaneous translation is one notable example [8], and another is provided by the performance of subjects in a verbally guided assembly task reported by P. Cohen [6]. Such an ability to produce output before all input data is available (or before enough processing resources have been made available to produce the best possible response) is what led Norman and Bobrow to formulate their CAO Principle. Our interest is in architectures for NLU systems which support CAO and in search strategies through such architectures for an optimal interpretation. The Linguistic Persnect4ve We have been asked to comment on legitimate inferences about human li</context>
</contexts>
<marker>8.</marker>
<rawString>Goldman-Eisler, F. Psychological Mechanisms of Speech Production as Studied through the Analysis of Simultaneous Translation. In B. Butterworth, Ed., jsflguaew Production, Academic Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Graham</author>
<author>M Harrison</author>
<author>W Ruzzo</author>
</authors>
<title>An Improved Context-Free Recognizer.</title>
<date>1980</date>
<booktitle>AGM TransactlonA greiczawang Litumagga ma „Systems</booktitle>
<pages>416</pages>
<marker>9.</marker>
<rawString>Graham, S., Harrison, M. and Ruzzo, W. An Improved Context-Free Recognizer. AGM TransactlonA greiczawang Litumagga ma „Systems (July 1980), 416 -</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Kay</author>
</authors>
<title>An Algorithm for Compiling Parsing Tables from a Grammar. Proceedings of a Symposium on Modelling Human Parsing Strategies,</title>
<institution>Center for Cognitive Science, University of Texas,</institution>
<location>Austin TX,</location>
<marker>10.</marker>
<rawString>Kay, M. An Algorithm for Compiling Parsing Tables from a Grammar. Proceedings of a Symposium on Modelling Human Parsing Strategies, Center for Cognitive Science, University of Texas, Austin TX,</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Marcus</author>
</authors>
<title>AlbAtora.s2LlizIagtialtsraisaitliant2E Natural Igingaagg.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<marker>11.</marker>
<rawString>Marcus, H. AlbAtora.s2LlizIagtialtsraisaitliant2E Natural Igingaagg. MIT Press, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Mark</author>
<author>G E Barton</author>
</authors>
<title>The RUSGrammar Parsing System. OR 3243, General Motors Research Laboratories,</title>
<date>1980</date>
<contexts>
<context position="648" citStr="[1, 2, 3, 12]" startWordPosition="101" endWordPosition="104">LANGUAGE UNDERSTANDING Robert J. Bobrow Bolt Beranek and Newman Inc. Bonnie L. Webber Department of Computer &amp; Information Science University of Pennsylvania Language is a system for encoding and transmitting ideas. A theory that seeks to explain linguistic phenomena in terms of this fact is a Panrtinnal, theory. One that does not misses the point. [101 PREAMBLE Our response to the questions posed to this panel is influenced by a number of beliefs (or biases!) which we have developed in the course of building and analyzing the operation of several natural language understanding (NLU) systems. [1, 2, 3, 12] While the emphasis of the panel is on parsing, we feel that the recovery of the syntactic structure of a natural language utterance must be viewed as part of a larger process of recovering the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluating natural language parsers or grammars without taking into account the architecture of the whole NLU system of which they&apos;re a part.1 This is the premise from which our beliefs arise, beliefs which concern two things: o the distribution of various types of knowledge, in particular syntact</context>
<context position="3796" citStr="[1, 2, 3, 4, 5, 12]" startWordPosition="592" endWordPosition="597">n be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the remainder of the NLU system and providing a &amp;quot;syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, as we know from experience [1, 2, 3, 4, 5, 12]. Once the pattern of communication between processors is settled, it is easier to attach a new semantics to the hooks already provided in the grammar than to build a new semantic processor. In addition, because each module has only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system. The panel has also been charged with considering . _ parallel processing as a challenge to its views on parsing. This touches on our belief</context>
</contexts>
<marker>12.</marker>
<rawString>Mark, W. S. &amp; Barton, G. E. The RUSGrammar Parsing System. OR 3243, General Motors Research Laboratories, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<date>1980</date>
<institution>Massachusetts Institute of Technology,</institution>
<marker>13.</marker>
<rawString>McDonald, D. ???. Ph.D. Th., Massachusetts Institute of Technology, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Norman</author>
<author>D Bobrow</author>
</authors>
<title>On Data-limited and Resource-limited Processes.</title>
<date></date>
<booktitle>C5I.74 -2, Xerox PARC,</booktitle>
<contexts>
<context position="4966" citStr="[14]" startWordPosition="789" endWordPosition="789">arsing. This touches on our beliefs about the interaction among the modules that comprise the NLU system. To respond to this issue, we first want to distinguish between two types of parallelism: one, in which many instances of the same thing are done at once (as in an array of parallel adders) and another, in which the many things done simultaneously can be different. Supporting this latter type of parallelism doesn&apos;t change our view of parsing, but rather underlies it. We believe that the interconnected processes involved in NLU must support a basic operating principle that Norman and Bobrow [14] have called &amp;quot;The Principle of Continually Available Output&amp;quot; (CAO). This states that the interacting processes must begin to provide output over a wide range of resource allocations, even before their analyzes are complete, and even before all input data is available. We take this position for two reasons: one, it facilitates computational efficiency, and two, it seems to be closer to human parsing processes (a point which we will get to in answering the next question). The requirement that syntactic analysis, semantic interpretation and discourse processing must be able to operate in (pseudo-</context>
</contexts>
<marker>14.</marker>
<rawString>Norman, D. &amp; Bobrow, D. On Data-limited and Resource-limited Processes. C5I.74 -2, Xerox PARC, May,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>