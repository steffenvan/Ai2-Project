<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000464">
<note confidence="0.794625333333333">
SemEval-2014 Task 5: L2 Writing Assistant
Maarten van Gompel, Iris Hendrickx,
Antal van den Bosch
</note>
<affiliation confidence="0.772391">
Centre for Language Studies,
Radboud University Nijmegen,
The Netherlands
</affiliation>
<email confidence="0.956781333333333">
proycon@anaproy.nl,
i.hendrickx@let.ru.nl,
a.vandenbosch@let.ru.nl
</email>
<note confidence="0.9442254">
Els Lefever and V´eronique Hoste
LT3,
Language and Translation Technology Team,
Ghent University,
Belgium
</note>
<email confidence="0.6966805">
els.lefever@ugent.be,
veronique.hoste@ugent.be
</email>
<sectionHeader confidence="0.995583" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994757785714286">
We present a new cross-lingual task for
SemEval concerning the translation of
L1 fragments in an L2 context. The
task is at the boundary of Cross-Lingual
Word Sense Disambiguation and Machine
Translation. It finds its application in the
field of computer-assisted translation, par-
ticularly in the context of second language
learning. Translating L1 fragments in an
L2 context allows language learners when
writing in a target language (L2) to fall
back to their native language (L1) when-
ever they are uncertain of the right word
or phrase.
</bodyText>
<sectionHeader confidence="0.999382" genericHeader="categories and subject descriptors">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99890805">
We present a new cross-lingual and application-
oriented task for SemEval that is situated in the
area where Word Sense Disambiguation and Ma-
chine Translation meet. Finding the proper trans-
lation of a word or phrase in a given context is
much like the problem of disambiguating between
multiple senses.
In this task participants are asked to build a
translation/writing assistance system that trans-
lates specifically marked L1 fragments in an L2
context to their proper L2 translation. This type
of translation can be applied in writing assistance
systems for language learners in which users write
in a target language, but are allowed to occasion-
ally back off to their native L1 when they are un-
certain of the proper lexical or grammatical form
in L2. The task concerns the NLP back-end rather
than any user interface.
Full-on machine translation typically concerns
the translation of complete sentences or texts from
</bodyText>
<footnote confidence="0.913351666666667">
This work is licensed under a Creative
Commons Attribution 4.0 International Licence:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.997343625">
L1 to L2. This task, in contrast, focuses on smaller
fragments, side-tracking the problem of full word
reordering.
We focus on the following language combi-
nations of L1 and L2 pairs: English-German,
English-Spanish, French-English and Dutch-
English. Task participants could participate for all
language pairs or any subset thereof.
</bodyText>
<sectionHeader confidence="0.996418" genericHeader="general terms">
2 Task Description
</sectionHeader>
<bodyText confidence="0.99998035483871">
We frame the task in the context of second lan-
guage learning, yielding a specific practical appli-
cation.
Participants build a translation assistance sys-
tem rather than a full machine translation system.
The L1 expression, a word or phrase, is translated
by the system to L2, given the L2 context already
present, including right-side context if available.
The aim here, as in all translation, is to carry the
semantics of the L1 fragment over to L2 and find
the most suitable L2 expression given the already
present L2 context.
Other than a limit on length (6 words), we do
not pose explicit constraints on the kinds of L1
fragments allowed. The number of L1 fragments
is limited to one fragment per sentence.
The task addresses both a core problem of
WSD, with cross-lingual context, and a sub-
problem of Phrase-based Statistical Machine
Translation; that of finding the most suitable trans-
lation of a word or phrase. In MT this would be
modelled by the translation model. In our task
the full complexity of full-sentential translation
is bypassed, putting the emphasis on the seman-
tic aspect of translation. Our task has specific
practical applications and a specific intended au-
dience, namely intermediate and advanced second
language learners, whom one generally wants to
encourage to use their target language as much as
possible, but who may often feel the need to fall
back to their native language.
</bodyText>
<page confidence="0.983059">
36
</page>
<note confidence="0.9805685">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 36–44,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9998874">
Currently, language learners are forced to fall
back to a bilingual dictionary when in doubt. Such
dictionaries do not take the L2 context into ac-
count and are generally more constrained to single
words or short expressions. The proposed applica-
tion would allow more flexible context-dependent
lookups as writing progresses. The task tests how
effectively participating systems accomplish this.
The following examples illustrate the task for
the four language pairs we offer:
</bodyText>
<listItem confidence="0.917175285714286">
• Input (L1=English,L2=Spanish): “Todo ello,
in accordance con los principios que siempre
hemos apoyado.”
Desired output: “Todo ello, de conformidad
con los principios que siempre hemos apoy-
ado.”
• Input (L1-English, L2=German): “Das,
was wir heute machen, is essentially ein
¨Argernis.”
Desired output: “Das, was wir heute machen,
ist im Grunde genommen ein ¨Argernis.”
• Input (L1=French,L2=English): “I rentre a`
la maison because I am tired.”
Desired output: “I return home because I am
tired.”
• Input (L1=Dutch, L2=English): “Workers
are facing a massive aanval op their employ-
ment and social rights.”
Desired output: “Workers are facing a mas-
sive attack on their employment and social
rights.”
</listItem>
<bodyText confidence="0.999831363636363">
The task can be related to two tasks that were
offered in previous years of SemEval: Lexical
Substitution (Mihalcea et al., 2010) and most no-
tably Cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2013).
When comparing our task to the Cross-Lingual
Word-Sense Disambiguation task, one notable dif-
ference is the fact that our task concerns not just
words, but also phrases. Another essential differ-
ence is the nature of the context; our context is in
L2 instead of L1. Unlike the Cross-Lingual Word
Sense Disambiguation task, we do not constrain
the L1 words or phrases that may be used for trans-
lation, except for a maximum length which we set
to 6 tokens, whereas Lefever and Hoste (2013)
only tested a select number of nouns. Our task
emphasizes a correct meaning-preserving choice
of words in which translations have to fit in the
L2 context. There is thus a clear morphosyntactic
aspect to the task, although less prominent than
in full machine translation, as the remainder of
the sentence, already in L2, does not need to be
changed. In the Cross-Lingual Word Sense Dis-
ambiguation tasks, the translations/senses were
lemmatised. We deliberately chose a different path
that allows for the envisioned application to func-
tion directly as a translation assistance system.
A pilot study was conducted to test the feasibil-
ity of the proposed translation system (van Gom-
pel and van den Bosch, 2014). It shows that L2
context information can be a useful cue in transla-
tion of L1 fragments to L2, improving over a non-
context-informed baseline.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="keywords">
3 Data
</sectionHeader>
<bodyText confidence="0.999970939393939">
We did not provide training data for this task, as
we did not want to bias participating systems by
favouring a particular sort of material and method-
ology. Moreover, it would be a prohibitively large
task to manually collect enough training data of
the task itself. Participants were therefore free to
use any suitable training material such as parallel
corpora, wordnets, or bilingual lexica.
Trial and test data has been collected for the
task, both delivered in a simple XML format that
explicitly marks the fragments. System output of
participants adheres to the same format. The trial
set, released early on in the task, was used by par-
ticipants to develop and tune their systems on. The
test set corresponds to the final data released for
the evaluation period; the final evaluation was con-
ducted on this data.
The trial data was constructed in an automated
fashion in the way described in our pilot study
(van Gompel and van den Bosch, 2014). First a
phrase-translation table is constructed from a par-
allel corpus. We used the Europarl parallel corpus
(Koehn, 2005) and the Moses tools (Koehn et al.,
2007), which in turn makes use of GIZA++ (Och
and Ney, 2000). Only strong phrase pairs (ex-
ceeding a set threshold) were retained and weaker
ones were pruned. This phrase-translation table
was then used to create input sentences in which
the L2 fragments are swapped for their L1 coun-
terparts, effectively mimicking a fall-back to L1 in
an L2 context. The full L2 sentence acts as refer-
ence sentence. Finally, to ensure all fragments are
correct and sensible, a manual selection from this
</bodyText>
<page confidence="0.998892">
37
</page>
<bodyText confidence="0.999570428571429">
automatically generated corpus constituted the fi-
nal trial set.
In our pilot study, such a data set, even with-
out the manual selection stage, proved adequate to
demonstrate the feasibility of translating L1 frag-
ments in an L2 context (van Gompel and van den
Bosch, 2014). One can, however, rightfully argue
whether such data is sufficiently representative for
the task and whether it would adequately cover in-
stances where L2 language learners might experi-
ence difficulties and be inclined to fall back to L1.
We therefore created a more representative test set
for the task.
The actual test set conforms to much more
stringent constraints and was composed entirely
by hand from a wide variety of written sources.
Amongst these sources are study books and gram-
mar books for language learners, short bilingual
on-line stories aimed at language learners, gap-
exercises and cloze tests, and contemporary writ-
ten resources such as newspapers, novels, and
Wikipedia. We aimed for actual learner corpora,
but finding suitable learner corpora with sufficient
data proved hard. For German we could use the
the Merlin corpus (Abel et al., 2013). In example
(a) we see a real example of a fragment in a fall-
back language in an L2 context from the Merlin
corpus.
</bodyText>
<listItem confidence="0.74686725">
(a) Input: Das Klima hier ist Tropical und wir haben fast
keinen Winter
Reference: Das Klima hier ist tropisch und wir haben
fast keinen Winter.
</listItem>
<bodyText confidence="0.999921348837209">
For various sources bilingual data was avail-
able. For the ones that were monolingual (L2)
we resorted to manual translation. To ensure our
translations were correct, these were later indepen-
dently verified, and where necessary corrected by
native speakers.
A large portion of the test set comes from off-
line resources because we wanted to make sure
that a substantial portion of the test set could not
be found verbatim on-line. This was done to pre-
vent systems from solving the actual problem by
just attempting to just look up the sources through
the available context information.
Note that in general we aimed for the European
varieties of the different languages. However, for
English we did add the US spelling variants as al-
ternatives. A complete list of all sources used in
establishing the test set is available on our web-
site1.
We created a trial set and test set/gold standard
of 500 sentence pairs per language pair. Due to
the detection of some errors at a later stage, some
of which were caused by the tokenisation pro-
cess, we were forced to remove some sentences
from the test set and found ourselves slightly be-
low our aim for some of the language pairs. The
test set was delivered in both tokenised2 and unto-
kenised form. The trial set was delivered only in
tokenised form. Evaluation was conducted against
the tokenised version, but our evaluation script
was designed to be as lenient as possible regard-
ing differences in tokenisation. We explicitly took
cases into account where participant’s tokenisers
split contractions (such as Spanish “del” to “de”
+ “el”), whereas our tokeniser did not.
For a given input fragment, it may well be possi-
ble that there are multiple correct translations pos-
sible. In establishing our test set, we therefore paid
special attention to adding alternatives. To ensure
no alternatives were missed, all participant output
was aggregated in one set, effectively anonymis-
ing the systems, and valid but previously missed
alternatives were added to the gold standard.
</bodyText>
<sectionHeader confidence="0.998444" genericHeader="introduction">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999975428571429">
Several metrics are available for automatic eval-
uation. First, we measure the absolute accuracy
a = c/n, where c is the number of fragment
translations from the system output that precisely
match the corresponding fragments in the refer-
ence translation, and n is the total number of trans-
latable fragments, including those for which no
translation was found. We also introduce a word-
based accuracy, which unlike the absolute accu-
racy gives some credits to mismatches that show
partial overlap with the reference translation. It as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment and is computed as follows:
</bodyText>
<equation confidence="0.8960794">
wac
longestsubmatch(output, ref erence)
=
max(|output|, |reference|)
(1)
</equation>
<bodyText confidence="0.999335666666667">
The system with the highest word-based accu-
racy wins the competition. All matching is case-
sensitive.
</bodyText>
<footnote confidence="0.998949">
1https://github.com/proycon/semeval2014task5
2Using ucto, available at https://github.com/proycon/ucto
</footnote>
<page confidence="0.999008">
38
</page>
<bodyText confidence="0.999095483870968">
Systems may decide not to translate fragments
if they cannot find a suitable translation. A recall
metric simply measures the number of fragments
for which the system generated a translation, re-
gardless of whether that translation is correct or
not, as a proportion of the total number of frag-
ments.
In addition to these task-specific metrics, stan-
dard MT metrics such as BLEU, NIST, METEOR
and error rates such as WER, PER and TER, are
included in the evaluation script as well. Scores
such as BLEU will generally be high (&gt; 0.95)
when computed on the full sentence, as a large
portion of the sentence is already translated and
only a specific fragment remains to be evaluated.
Nevertheless, these generic metrics are proven in
our pilot study to follow the same trend as the
more task-specific evaluation metrics, and will be
omitted in the result section for brevity.
It regularly occurs that multiple translations are
possible. As stated, in the creation of the test set
we have taken this into account by explicitly en-
coding valid alternatives. A match with any alter-
native in the reference counts as a valid match. For
word accuracy, the highest word accuracy amongst
all possible alternatives in the reference is taken.
Likewise, participant system output may contain
multiple alternatives as well, as we allowed two
different types of runs, following the example of
the Cross-Lingual Lexical Substitution and Cross-
Lingual Word Sense Disambiguation tasks:
</bodyText>
<listItem confidence="0.983326428571429">
• Best - The system may only output one, its
best, translation;
• Out of Five - The system may output up
to five alternatives, effectively allowing 5
guesses. Only the best match is counted. This
metric does not count how many of the five
are valid.
</listItem>
<bodyText confidence="0.9077435">
Participants could submit up to three runs per
language pair and evaluation type.
</bodyText>
<sectionHeader confidence="0.999214" genericHeader="method">
5 Participants
</sectionHeader>
<bodyText confidence="0.999535333333333">
Six teams submitted systems, three of which par-
ticipated for all language pairs. In alphabetic or-
der, these are:
</bodyText>
<listItem confidence="0.987758947368421">
1. CNRC - Cyril Goutte, Michel Simard, Ma-
rine Carpuat - National Research Council
Canada – All language pairs
2. IUCL - Alex Rudnick, Liu Can, Levi King,
Sandra K¨ubler, Markus Dickinson - Indiana
University (US) – all language pairs
3. UEdin - Eva Hasler - University of Ed-
inburgh (UK) – all language pairs except
English-German
4. UNAL - Sergio Jim´enez, Emilio Silva - Uni-
versidad Nacional de Colombia – English-
Spanish
5. Sensible - Liling Tan - Universit¨at des Saar-
landes (Germany) and Nanyang Technolog-
ical University (Singapore) – all language
pairs
6. TeamZ - Anubhav Gupta - Universit´e de
Franche-Comt´e (France) – English-Spanish,
English-German
</listItem>
<bodyText confidence="0.999213878787879">
Participants implemented distinct methodolo-
gies and implementations. One obvious avenue of
tackling the problem is through standard Statisti-
cal Machine Translation (SMT). The CNRC team
takes a pure SMT approach with few modifica-
tions. They employ their own Portage decoder and
directly send an L1 fragment in an L2 context, cor-
responding to a partial translation hypothesis with
only one fragment left to decode, to their decoder
(Goutte et al., 2014). The UEdin team applies a
similar method using the Moses decoder, marking
the L2 context so that the decoder leaves this con-
text as is. In addition they add a context similarity
feature for every phrase pair in the phrase transla-
tion table, which expresses topical similarity with
the test context. In order to properly decode, the
phrase table is filtered per test sentence (Hasler,
2014). The IUCL and UNAL teams do make use
of the information from word alignments or phrase
translation tables, but do not use a standard SMT
decoder. The IUCL system combines various in-
formation sources in a log-linear model: phrase
table, L2 Language Model, Multilingual Dictio-
nary, and a dependency-based collocation model,
although this latter source was not finished in time
for the system submission (Rudnick et al., 2014).
The UNAL system extracts syntactic features as a
means to relate L1 fragments with L2 context to
their L2 fragment translations, and uses memory-
based classifiers to achieve this (Silva-Schlenker
et al., 2014). The two systems on the lower end of
the result spectrum use different techniques alto-
gether. The Sensible team approaches the problem
</bodyText>
<page confidence="0.997161">
39
</page>
<bodyText confidence="0.99993">
by attempting to emulate the manual post-editing
process human translators employ to correct MT
output (Tan et al., 2014), whereas TeamZ relies on
Wiktionary as the sole source (Gupta, 2014).
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999639714285714">
The results of the six participating teams can be
viewed in consensed form in Table 1. This table
shows the highest word accuracy achieved by the
participants, in which multiple system runs have
been aggregated. A ranking can quickly be dis-
tilled from this, as the best score is marked in
bold. The system by the University of Edinburgh
emerges as the clear winner of the task. The full
results of the various system runs by the six par-
ticipants are shown in Tables 2 and 3, two pages
down, all three aforementioned evaluation metrics
are reported there and the systems are sorted by
word accuracy per language pair and evaluation
type.
</bodyText>
<table confidence="0.998759357142857">
Team en-es oof en-de oof
CNRC 0.745 0.887 0.717 0.868
IUCL 0.720 0.847 0.722 0.857
UEdin 0.827 0.949 - -
UNAL 0.809 0.880 - -
Sensible 0.351 0.231 0.233 0.306
TeamZ 0.333 0.386 0.293 0.385
fr-en oof nl-en oof
CNRC 0.694 0.839 0.610 0.723
IUCL 0.682 0.800 0.679 0.753
UEdin 0.824 0.939 0.692 0.811
UNAL - - - -
Sensible 0.116 0.14 0.152 0.171
TeamZ - - - -
</table>
<tableCaption confidence="0.999422">
Table 1: Highest word accuracy per team, per lan-
</tableCaption>
<bodyText confidence="0.985030714285714">
guage pair, and per evaluation type (out-of-five is
include in the “oof” column). The best score in
each column is marked in bold.
For the lowest-ranking participants, the score is
negatively impacted by the low recall; their sys-
tems could not find translations for a large number
of fragments.
Figures 1 (next page) and 2 (last page) show the
results for the best evaluation type for each sys-
tem run. Three bars are shown; from left to right
these represent accuracy (blue), word-accuracy
(green) and recall (red). Graphs for out-of-five
evaluation were omitted for brevity, but tend to fol-
low the same trend with scores that are somewhat
higher. These scores can be viewed on the result
website at http://github.com/proycon/
semeval2014task5/. The result website also
holds the system output and evaluation scripts with
which all graphs and tables can be reproduced.
We observe that the best scoring team in the
task (UEdin), as well as the CNRC team, both em-
ploy standard Statistical Machine Translation and
achieve high results. From this we can conclude
that standard SMT techniques are suitable for this
task. Teams IUCL and UNAL achieve similarly
good results, building on word and phrase align-
ment data as does SMT, yet not using a traditional
SMT decoder. TeamZ and Sensible, the two sys-
tems ranked lowest do not rely on any techniques
from SMT. To what extent the context-informed
measures of the various participants are effective
can not be judged from this comparison, but can
only be assessed in comparison to their own base-
lines. For this we refer to the system papers of the
participants.
</bodyText>
<sectionHeader confidence="0.999307" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999738">
We did not specify any training data for the task.
The advantage of this is that participants were free
to build a wider variety of systems from various
sources, rather than introducing a bias towards for
instances statistical systems. The disadvantage,
however, is that a comparison of the various sys-
tems does not yield conclusive results regarding
the merit of their methodologies. Discrepancies
might at least be partly due to differences in train-
ing data, as it is generally well understood in MT
that more training data improves results. The base-
lines various participants describe in their system
papers provide more insight to the merit of their
approaches than a comparison between them.
In the creation of the test set, we aimed to mimic
intermediate to high-level language learners. We
also aimed at a fair distribution of different part-
of-speech categories and phrasal length. The dif-
ficulty of the task differs between language pairs,
though not intentionally so. We observe that the
Dutch-English set is the hardest and the Spanish-
English is the easiest in the task. One of the par-
ticipants implicitly observes this through measure-
ment of the number of Out-of-Vocabulary words
(Goutte et al., 2014). This implies that when com-
paring system performance between different lan-
guage pairs, one can not simply ascribe a lower
result to a system having more difficulty with said
</bodyText>
<page confidence="0.987131">
40
</page>
<figure confidence="0.996080461538461">
10
0.8
0.6
0 4 -
0 2
0 0CNRC
run1
1 0
En lish (L1) - Spanish (L2), Best
CNRC ILJCL ILJCL Sensib e Sensib e Sensible TeamZ UEdin UEdin UEdin LJNAL LJNAL
run2 run1 run2 wtm vvtmxlingArtmxlingyu run1 run1 run2 run3 run1 run2
English (L1 - German (L2), Best
0.8
0.6
0.4
0.2
00
ILJCL
runl
CNRC
ijnl
CNRC
run2
IUCL
run2
M.
Sensible Sensible Sensible TeamZ
wtm wtmxling wtmxlingyu run1
0
co
0
or)
0
c.`ti
10 French L1 - English (L2), Best
0.8
0.6
0.4
0.2
UEdin UEdin UEdin
run1 run2 run3
CNRC IUCL IUCL
run2 11 run2
0 0
CNRC
run1
Sensible
win)
Sensible
wtmxlIng
Sensible
w1mx ingyu
</figure>
<figureCaption confidence="0.9825545">
Figure 1: English to Spanish (top), English to German (middle) and French to English (bottom). The
three bars, left-to-right, represent Accuracy (blue), Word Accuracy (green) and Recall (red).
</figureCaption>
<page confidence="0.990859">
41
</page>
<table confidence="0.999976608695653">
System Acc W.Acc. Recall
English-Spanish (best)
UEdin-run2 0.755 0.827 1.0
UEdin-run1 0.753 0.827 1.0
UEdin-run3 0.745 0.82 1.0
UNAL-run2 0.733 0.809 0.994
UNAL-run1 0.721 0.794 0.994
CNRC-run1 0.667 0.745 1.0
CNRC-run2 0.651 0.735 1.0
IUCL-run1 0.633 0.72 1.0
IUCL-run2 0.633 0.72 1.0
Sensible-wtmxlingyu 0.239 0.351 0.819
TeamZ-run1 0.223 0.333 0.751
Sensible-wtm 0.145 0.175 0.470
Sensible-wtmxling 0.141 0.171 0.470
English-Spanish (out-of-five)
UEdin-run3 0.928 0.949 1.0
UEdin-run1 0.924 0.946 1.0
UEdin-run2 0.92 0.944 1.0
CNRC-run1 0.843 0.887 1.0
CNRC-run2 0.837 0.884 1.0
UNAL-run1 0.823 0.88 0.994
IUCL-run1 0.781 0.847 1.0
IUCL-run2 0.781 0.847 1.0
Sensible-wtmxlingyu 0.263 0.416 0.819
TeamZ-run1 0.277 0.386 0.751
Sensible-wtm 0.173 0.231 0.470
Sensible-wtmxling 0.169 0.228 0.470
English-German (best)
IUCL-run2 0.665 0.722 1.0
CNRC-run1 0.657 0.717 1.0
CNRC-run2 0.645 0.702 1.0
TeamZ-run1 0.218 0.293 0.852
IUCL-run1 0.198 0.252 1.0
Sensible-wtmxlingyu 0.162 0.233 0.878
Sensible-wtm 0.16 0.184 0.647
Sensible-wtmxling 0.152 0.178 0.647
English-German (out-of-five)
CNRC-run1 0.834 0.868 1.0
CNRC-run2 0.828 0.865 1.0
IUCL-run2 0.806 0.857 1.0
TeamZ-run1 0.307 0.385 0.852
IUCL-run1 0.228 0.317 1.0
Sensible-wtmxlingyu 0.18 0.306 0.878
Sensible-wtm 0.182 0.256 0.647
Sensible-wtmxling 0.174 0.25 0.647
</table>
<tableCaption confidence="0.8981165">
Table 2: Full results for English-Spanish and
English-German.
</tableCaption>
<table confidence="0.999985711111111">
System Acc W.Acc. Recall
French-English (best)
UEdin-run1 0.733 0.824 1.0
UEdin-run2 0.731 0.821 1.0
UEdin-run3 0.723 0.816 1.0
CNRC-run1 0.556 0.694 1.0
CNRC-run2 0.533 0.686 1.0
IUCL-run1 0.545 0.682 1.0
IUCL-run2 0.545 0.682 1.0
Sensible-wtmxlingyu 0.081 0.116 0.321
Sensible-wtm 0.055 0.067 0.210
Sensible-wtmxling 0.055 0.067 0.210
French-English (out-of-five)
UEdin-run2 0.909 0.939 1.0
UEdin-run1 0.905 0.938 1.0
UEdin-run3 0.907 0.937 1.0
CNRC-run1 0.739 0.839 1.0
CNRC-run2 0.731 0.834 1.0
IUCL-run1 0.691 0.8 1.0
IUCL-run2 0.691 0.8 1.0
Sensible-wtmxlingyu 0.085 0.14 0.321
Sensible-wtmxling 0.061 0.09 0.210
Sensible-wtm 0.061 0.089 0.210
Dutch-English (best)
UEdin-run1 0.575 0.692 1.0
UEdin-run2 0.567 0.688 1.0
UEdin-run3 0.565 0.688 1.0
IUCL-run1 0.544 0.679 1.0
IUCL-run2 0.544 0.679 1.0
CNRC-run1 0.45 0.61 1.0
CNRC-run2 0.444 0.609 1.0
Sensible-wtmxlingyu 0.115 0.152 0.335
Sensible-wtm 0.092 0.099 0.214
Sensible-wtmxling 0.088 0.095 0.214
Dutch-English (out-of-five)
UEdin-run1 0.733 0.811 1.0
UEdin-run3 0.727 0.808 1.0
UEdin-run2 0.725 0.808 1.0
IUCL-run1 0.634 0.753 1.0
IUCL-run2 0.634 0.753 1.0
CNRC-run1 0.606 0.723 1.0
CNRC-run2 0.602 0.721 1.0
Sensible-wtmxlingyu 0.123 0.171 0.335
Sensible-wtm 0.099 0.115 0.214
Sensible-wtmxling 0.096 0.112 0.214
</table>
<tableCaption confidence="0.971137">
Table 3: Full results for French-English and
Dutch-English.
</tableCaption>
<bodyText confidence="0.99994247826087">
language pair. This could rather be an intrinsic
property of the test set or the distance between the
languages.
Distance in syntactic structure between lan-
guages also defines the limits of this task. Dur-
ing composition of the test set it became clear that
backing off to L1 was not always possible when
syntax diverged to much. An example of this is
separable verbs in Dutch and German. Consider
the German sentence “Er ruft seine Mutter an”
(translation: “He calls his mother”). Imagine
a German language learner wanting to compose
such a sentence but wanting to fall back to En-
glish for the verb “to call”, which would translate
to German as “anrufen”. The possible input sen-
tence may still be easy to construe: “Er calls seine
Mutter”, but the solution to this problem would
require insertion at two different points, whereas
the task currently only deals with a substitution of
a single fragment. The reverse is arguably even
more complex and may stray too far from what
a language learner may do. Consider an English
language learner wanting to fall back to her na-
</bodyText>
<page confidence="0.996994">
42
</page>
<bodyText confidence="0.997849777777778">
tive German, struggling with the English transla-
tion for “anrufen”. She may compose a sentence
such as “He ruft his mother an”, which would
require translating two dependent fragments into
one.
We already have interesting examples in the
gold standard, such as example (b), showing syn-
tactic word-order changes confined to a single
fragment.
</bodyText>
<figure confidence="0.4821985">
(b) Input: I always wanted iemand te zijn , but now I
realize I should have been more specific.
</figure>
<bodyText confidence="0.958700784615385">
Reference: I always wanted to be somebody, but
now I realize I should have been more specific.
Participant output (aggregated): to be a person; it to
be; someone to his; to be somebody; person to be;
someone to; someone to be; to be anybody; to anyone;
to be someone; a person to have any; to be someone
else
Another question we can ask, but have not in-
vestigated, is whether a language learner would
insert the proper morphosyntactic form of an L1
word given the L2 context, or whether she may
be inclined to fall back to a normal form such
as an infinitive. Especially in the above case of
separable verbs someone may be more inclined to
circumvent the double fragments and provide the
input: “He anrufen his mother“, but in simpler
cases the same issue arises as well. Consider an
English learner falling back to her native Croatian,
a Slavic language which heavily declines nouns.
If she did not know the English word “book” and
wanted to write “He gave the book to him”, she
could use either the Croatian word “knjigu” in its
accusative declension or fall back to the normal
form “knjiga”. A proper writing assistant system
would have to account for both options.
We can analyse which of the sentences in the
test data participants struggled with most. First
we look at the number of sentences that produce
an average word accuracy of zero, measured per
sentence over all systems and runs in the out-of-
five metric. This means no participant was close
to the correct output. There were 6 such sentences
in English-Spanish, 17 in English-German, 6 in
French-English, and 32 in Dutch-English.
A particularly difficult context from the Span-
ish set is when a subjunctive verb form was re-
quired, but an indicative verb form was submit-
ted by the systems, such as in the sentence: “Es-
pero que los frenos del coche funcionen bien.”.
Though this may be deduced from context (the
word “Espero”, expressing hope yet doubt, be-
ing key here), it is often subtle and hard to cap-
ture. Another problematic case that recurs in the
German and Dutch data sets is compound nouns.
The English fragment “work motivation” should
translate into the German compound “Arbeitsmo-
tivation” or “Arbeitsmoral”, yet participants were
not able to find the actual compound noun. Beside
compound nouns, other less frequent multi-word
expressions are also amongst the difficult cases.
Sparsity or complete absence in training data of
these expressions is why systems struggle here.
Another point of discussion is the fact that we
enriched the test set by adding previously unavail-
able alternative translations from an aggregated
pool of system output. This might draw criticism
for possibly introducing a bias, also considering
the fact that the decision to include a particular al-
ternative for a given context is not always straight-
forward and at times subjective. We, however,
contend that this is the best way to ensure that
valid system output is not discarded and reduce the
number of false negatives. The effect of this mea-
sure has been an increase in (word) accuracy for
all systems, without significant impact on ranking.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999980277777778">
In this SemEval task we showed that systems can
translate L1 fragments in an L2 context, a task
that finds application in computer-assisted trans-
lation and computer-assisted language learning.
The localised translation of a fragment in a cross-
lingual context makes it a novel task in the field.
Though the task has its limits, we argue for its
practical application in a language-learning set-
ting: as a writing assistant and dictionary replace-
ment. Six contestants participated in the task,
and used an ensemble of techniques from Statis-
tical Machine Translation and Word Sense Disam-
biguation. Most of the task organizers’ time went
into manually establishing a gold standard based
on a wide variety of sources, most aimed at lan-
guage learners, for each of the four language pairs
in the task. We have been positively surprised by
the good results of the highest ranking systems.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.884894666666667">
We would like to thank Andreu van Hooft and
Sarah Schulz for their manual correction work,
and Sean Banville, Geert Joris, Bernard De Clerck,
Rogier Crijns, Adriane Boyd, Detmar Meurers,
Guillermo Sanz Gallego and Nils Smeuninx for
helping us with the data collection.
</bodyText>
<page confidence="0.999838">
43
</page>
<figureCaption confidence="0.996103">
Figure 2: Dutch to English.
</figureCaption>
<sectionHeader confidence="0.998788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997699557142857">
Andrea Abel, Lionel Nicolas, Jirka Hana, Barbora
ˇStindlov´a, Katrin Wisniewski, Claudia Woldt, Det-
mar Meurers, and Serhiy Bykh. 2013. A trilingual
learner corpus illustrating european reference lev-
els. In Proceedings of the Learner Corpus Research
Conference, Bergen, Norway, 27-29 September.
Cyril Goutte, Michel Simard, and Marine Carpuat.
2014. CNRC-TMT: Second language writing as-
sistant system description. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Anubhav Gupta. 2014. Team Z: Wiktionary as L2
writing assistant. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Eva Hasler. 2014. UEdin: Translating L1 phrases in
L2 context using context-sensitive smt. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation (SemEval-2014), Dublin, Ireland.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ohttp://www.aclweb.org/anthology/P/P07/P07
2045ndrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177–180, Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In In Proceedings
of the Machine Translation Summit X ([MT]’05).,
pages 79–86.
Els Lefever and Veronique Hoste. 2013. SemEval-
2013 Task 10: Cross-Lingual Word Sense Disam-
biguation. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013),
in conjunction with the Second Joint Conference on
Lexical and Computational Semantics.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval 2010 task 2: Cross-lingual lex-
ical substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models. Technical
report, RWTH Aachen, University of Technology.
Alex Rudnick, Levi King, Can Liu, Markus Dickinson,
and Sandra K¨ubler. 2014. IUCL: Combining infor-
mation sources for semeval task 5. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Emilio Silva-Schlenker, Sergio Jimenez, and Julia Ba-
quero. 2014. UNAL-NLP: Cross-lingual phrase
sense disambiguation with syntactic dependency
trees. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Liling Tan, Anne Schumann, Jos´e Martinez, and Fran-
cis Bond. 2014. Sensible: L2 translation assistance
by emulating the manual post-editing process. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Maarten van Gompel and Antal van den Bosch. 2014.
Translation assistance by translation of L1 frag-
ments in an L2 context. In To appear in Proceedings
ofACL 2014.
</reference>
<page confidence="0.999283">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.082448">
<title confidence="0.7971415">SemEval-2014 Task 5: L2 Writing Assistant Maarten van Gompel, Iris</title>
<author confidence="0.998628">Antal van_den</author>
<affiliation confidence="0.9983135">Centre for Language Radboud University</affiliation>
<title confidence="0.44961175">The a.vandenbosch@let.ru.nl Els Lefever and V´eronique Language and Translation Technology</title>
<author confidence="0.50128">Ghent</author>
<email confidence="0.831527">veronique.hoste@ugent.be</email>
<abstract confidence="0.9954676">We present a new cross-lingual task SemEval concerning the translation of L1 fragments in an L2 context. The task is at the boundary of Cross-Lingual Word Sense Disambiguation and Machine Translation. It finds its application in the field of computer-assisted translation, particularly in the context of second language learning. Translating L1 fragments in an L2 context allows language learners when writing in a target language (L2) to fall back to their native language (L1) whenever they are uncertain of the right word or phrase.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrea Abel</author>
<author>Lionel Nicolas</author>
<author>Jirka Hana</author>
<author>Barbora ˇStindlov´a</author>
<author>Katrin Wisniewski</author>
<author>Claudia Woldt</author>
<author>Detmar Meurers</author>
<author>Serhiy Bykh</author>
</authors>
<title>A trilingual learner corpus illustrating european reference levels.</title>
<date>2013</date>
<booktitle>In Proceedings of the Learner Corpus Research Conference,</booktitle>
<location>Bergen,</location>
<marker>Abel, Nicolas, Hana, ˇStindlov´a, Wisniewski, Woldt, Meurers, Bykh, 2013</marker>
<rawString>Andrea Abel, Lionel Nicolas, Jirka Hana, Barbora ˇStindlov´a, Katrin Wisniewski, Claudia Woldt, Detmar Meurers, and Serhiy Bykh. 2013. A trilingual learner corpus illustrating european reference levels. In Proceedings of the Learner Corpus Research Conference, Bergen, Norway, 27-29 September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Goutte</author>
<author>Michel Simard</author>
<author>Marine Carpuat</author>
</authors>
<title>CNRC-TMT: Second language writing assistant system description.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="15642" citStr="Goutte et al., 2014" startWordPosition="2517" endWordPosition="2520">nd Nanyang Technological University (Singapore) – all language pairs 6. TeamZ - Anubhav Gupta - Universit´e de Franche-Comt´e (France) – English-Spanish, English-German Participants implemented distinct methodologies and implementations. One obvious avenue of tackling the problem is through standard Statistical Machine Translation (SMT). The CNRC team takes a pure SMT approach with few modifications. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-l</context>
<context position="20878" citStr="Goutte et al., 2014" startWordPosition="3397" endWordPosition="3400">heir system papers provide more insight to the merit of their approaches than a comparison between them. In the creation of the test set, we aimed to mimic intermediate to high-level language learners. We also aimed at a fair distribution of different partof-speech categories and phrasal length. The difficulty of the task differs between language pairs, though not intentionally so. We observe that the Dutch-English set is the hardest and the SpanishEnglish is the easiest in the task. One of the participants implicitly observes this through measurement of the number of Out-of-Vocabulary words (Goutte et al., 2014). This implies that when comparing system performance between different language pairs, one can not simply ascribe a lower result to a system having more difficulty with said 40 10 0.8 0.6 0 4 - 0 2 0 0CNRC run1 1 0 En lish (L1) - Spanish (L2), Best CNRC ILJCL ILJCL Sensib e Sensib e Sensible TeamZ UEdin UEdin UEdin LJNAL LJNAL run2 run1 run2 wtm vvtmxlingArtmxlingyu run1 run1 run2 run3 run1 run2 English (L1 - German (L2), Best 0.8 0.6 0.4 0.2 00 ILJCL runl CNRC ijnl CNRC run2 IUCL run2 M. Sensible Sensible Sensible TeamZ wtm wtmxling wtmxlingyu run1 0 co 0 or) 0 c.`ti 10 French L1 - English (</context>
</contexts>
<marker>Goutte, Simard, Carpuat, 2014</marker>
<rawString>Cyril Goutte, Michel Simard, and Marine Carpuat. 2014. CNRC-TMT: Second language writing assistant system description. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anubhav Gupta</author>
</authors>
<title>Team Z: Wiktionary as L2 writing assistant.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="16995" citStr="Gupta, 2014" startWordPosition="2739" endWordPosition="2740">was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 Results The results of the six participating teams can be viewed in consensed form in Table 1. This table shows the highest word accuracy achieved by the participants, in which multiple system runs have been aggregated. A ranking can quickly be distilled from this, as the best score is marked in bold. The system by the University of Edinburgh emerges as the clear winner of the task. The full results of the various system runs by the six participants are shown in Tables 2 and 3, two pages down, all three aforementioned evaluation metrics are reported there and the systems are sorted by word</context>
</contexts>
<marker>Gupta, 2014</marker>
<rawString>Anubhav Gupta. 2014. Team Z: Wiktionary as L2 writing assistant. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
</authors>
<title>UEdin: Translating L1 phrases in L2 context using context-sensitive smt.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="16032" citStr="Hasler, 2014" startWordPosition="2585" endWordPosition="2586">tions. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve t</context>
</contexts>
<marker>Hasler, 2014</marker>
<rawString>Eva Hasler. 2014. UEdin: Translating L1 phrases in L2 context using context-sensitive smt. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<location>Christine Moran, Richard Zens, Chris Dyer, Ohttp://www.aclweb.org/anthology/P/P07/P07</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ohttp://www.aclweb.org/anthology/P/P07/P07</rawString>
</citation>
<citation valid="true">
<authors>
<author>2045ndrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<marker>Bojar, Constantin, Herbst, 2007</marker>
<rawString>2045ndrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation. In</title>
<date>2005</date>
<booktitle>In Proceedings of the Machine Translation Summit X ([MT]’05).,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7706" citStr="Koehn, 2005" startWordPosition="1220" endWordPosition="1221">d in a simple XML format that explicitly marks the fragments. System output of participants adheres to the same format. The trial set, released early on in the task, was used by participants to develop and tune their systems on. The test set corresponds to the final data released for the evaluation period; the final evaluation was conducted on this data. The trial data was constructed in an automated fashion in the way described in our pilot study (van Gompel and van den Bosch, 2014). First a phrase-translation table is constructed from a parallel corpus. We used the Europarl parallel corpus (Koehn, 2005) and the Moses tools (Koehn et al., 2007), which in turn makes use of GIZA++ (Och and Ney, 2000). Only strong phrase pairs (exceeding a set threshold) were retained and weaker ones were pruned. This phrase-translation table was then used to create input sentences in which the L2 fragments are swapped for their L1 counterparts, effectively mimicking a fall-back to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this 37 automatically generated corpus constituted the final trial set. In our pi</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In In Proceedings of the Machine Translation Summit X ([MT]’05)., pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>SemEval2013 Task 10: Cross-Lingual Word Sense Disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="5287" citStr="Lefever and Hoste, 2013" startWordPosition="811" endWordPosition="814">esired output: “Das, was wir heute machen, ist im Grunde genommen ein ¨Argernis.” • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number of nouns. Our task emphasizes a correct meaning-preserving choice of words in which </context>
</contexts>
<marker>Lefever, Hoste, 2013</marker>
<rawString>Els Lefever and Veronique Hoste. 2013. SemEval2013 Task 10: Cross-Lingual Word Sense Disambiguation. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
</authors>
<title>task 2: Cross-lingual lexical substitution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010),</booktitle>
<location>Uppsala,</location>
<note>Semeval</note>
<contexts>
<context position="5204" citStr="Mihalcea et al., 2010" startWordPosition="799" endWordPosition="802">English, L2=German): “Das, was wir heute machen, is essentially ein ¨Argernis.” Desired output: “Das, was wir heute machen, ist im Grunde genommen ein ¨Argernis.” • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number o</context>
</contexts>
<marker>Mihalcea, Sinha, McCarthy, 2010</marker>
<rawString>Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. Semeval 2010 task 2: Cross-lingual lexical substitution. In Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>RWTH Aachen, University of Technology.</institution>
<contexts>
<context position="7802" citStr="Och and Ney, 2000" startWordPosition="1237" endWordPosition="1240">s adheres to the same format. The trial set, released early on in the task, was used by participants to develop and tune their systems on. The test set corresponds to the final data released for the evaluation period; the final evaluation was conducted on this data. The trial data was constructed in an automated fashion in the way described in our pilot study (van Gompel and van den Bosch, 2014). First a phrase-translation table is constructed from a parallel corpus. We used the Europarl parallel corpus (Koehn, 2005) and the Moses tools (Koehn et al., 2007), which in turn makes use of GIZA++ (Och and Ney, 2000). Only strong phrase pairs (exceeding a set threshold) were retained and weaker ones were pruned. This phrase-translation table was then used to create input sentences in which the L2 fragments are swapped for their L1 counterparts, effectively mimicking a fall-back to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this 37 automatically generated corpus constituted the final trial set. In our pilot study, such a data set, even without the manual selection stage, proved adequate to demonstr</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Giza++: Training of statistical translation models. Technical report, RWTH Aachen, University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Rudnick</author>
<author>Levi King</author>
<author>Can Liu</author>
<author>Markus Dickinson</author>
<author>Sandra K¨ubler</author>
</authors>
<title>IUCL: Combining information sources for semeval task 5.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<marker>Rudnick, King, Liu, Dickinson, K¨ubler, 2014</marker>
<rawString>Alex Rudnick, Levi King, Can Liu, Markus Dickinson, and Sandra K¨ubler. 2014. IUCL: Combining information sources for semeval task 5. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emilio Silva-Schlenker</author>
<author>Sergio Jimenez</author>
<author>Julia Baquero</author>
</authors>
<title>UNAL-NLP: Cross-lingual phrase sense disambiguation with syntactic dependency trees.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="16666" citStr="Silva-Schlenker et al., 2014" startWordPosition="2684" endWordPosition="2687"> IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 Results The results of the six participating teams can be viewed in consensed form in Table 1. This table shows the highest word accuracy achieved by the participants, in which multiple system runs have been aggregated. A ranking can quickly be distilled from this, a</context>
</contexts>
<marker>Silva-Schlenker, Jimenez, Baquero, 2014</marker>
<rawString>Emilio Silva-Schlenker, Sergio Jimenez, and Julia Baquero. 2014. UNAL-NLP: Cross-lingual phrase sense disambiguation with syntactic dependency trees. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Anne Schumann</author>
<author>Jos´e Martinez</author>
<author>Francis Bond</author>
</authors>
<title>Sensible: L2 translation assistance by emulating the manual post-editing process.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="16926" citStr="Tan et al., 2014" startWordPosition="2726" endWordPosition="2729">ry, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 Results The results of the six participating teams can be viewed in consensed form in Table 1. This table shows the highest word accuracy achieved by the participants, in which multiple system runs have been aggregated. A ranking can quickly be distilled from this, as the best score is marked in bold. The system by the University of Edinburgh emerges as the clear winner of the task. The full results of the various system runs by the six participants are shown in Tables 2 and 3, two pages down, all three aforementioned eva</context>
</contexts>
<marker>Tan, Schumann, Martinez, Bond, 2014</marker>
<rawString>Liling Tan, Anne Schumann, Jos´e Martinez, and Francis Bond. 2014. Sensible: L2 translation assistance by emulating the manual post-editing process. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten van Gompel</author>
<author>Antal van den Bosch</author>
</authors>
<title>Translation assistance by translation of L1 fragments in an L2 context.</title>
<date>2014</date>
<booktitle>In To appear in Proceedings ofACL</booktitle>
<marker>van Gompel, van den Bosch, 2014</marker>
<rawString>Maarten van Gompel and Antal van den Bosch. 2014. Translation assistance by translation of L1 fragments in an L2 context. In To appear in Proceedings ofACL 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>