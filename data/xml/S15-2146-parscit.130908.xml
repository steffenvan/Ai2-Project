<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004229">
<title confidence="0.971988">
UTD: Ensemble-Based Spatial Relation Extraction
</title>
<author confidence="0.97994">
Jennifer D’Souza and Vincent Ng
</author>
<affiliation confidence="0.9901385">
Human Language Technology Research Institute
University of Texas at Dallas
</affiliation>
<address confidence="0.917964">
Richardson, TX 75083-0688
</address>
<email confidence="0.999641">
{jld082000,vince}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997771933333333">
SpaceEval (SemEval 2015 Task 8), which
concerns spatial information extraction, builds
on the spatial role identification tasks intro-
duced in SemEval 2012 and used in SemEval
2013. Among the host of subtasks presented
in SpaceEval, we participated in subtask 3a,
which focuses solely on spatial relation ex-
traction. To address the complexity of a
MOVELINK, we decompose it into smaller re-
lations so that the roles involved in each rela-
tion can be extracted in a joint fashion with-
out losing computational tractability. Our sys-
tem was ranked first in the official evaluation,
achieving an overall spatial relation extraction
F-score of 84.5%.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912933333333">
SpaceEval1 was organized as a shared task for the
semantic evaluation of spatial information extraction
(IE) systems. The goals of the shared task include
identifying and classifying particular constructions
in natural language for expressing spatial informa-
tion that are conveyed through the spatial concepts
of locations, entities participating in spatial rela-
tions, paths, topological relations, direction and ori-
entation, motion, etc. It presents a wide spectrum of
spatial IE related subtasks for interested participants
to choose from, building on the two previous years
shared tasks on the same topic (Kordjamshidi et al.,
2012; Kolomiyets et al., 2013).
Our goal in this paper is to describe the version
of our spatial relation extraction system that partic-
</bodyText>
<footnote confidence="0.929989">
1http://alt.qcri.org/semeval2015/task8/
</footnote>
<bodyText confidence="0.99990924">
ipated in subtask 3a of SpaceEval. Systems par-
ticipating in this subtask assume as input the spa-
tial elements in a text document. For example,
in the sentence The flower is in the vases and the
vase2 is on the table, the set of spatial elements
{flower, in, vases, vase2, on, table} are given and
subsequently used as candidates for predicting spa-
tial relations. Leveraging the successes of a joint
role-labeling approach to spatial relation extraction
involving stationary objects, we employ it to ex-
tract so-called MOVELINKs, which are spatial re-
lations defined over objects in motion. In partic-
ular, we discuss the adaptations needed to handle
the complexity of MOVELINKs. Experiments on the
SpaceEval corpus demonstrate the effectiveness of
our ensemble-based approach to spatial relation ex-
traction. Among the three teams participating in
subtask 3a, our team was ranked first in the official
evaluation, achieving an overall F-score of 84.5%.
The rest of the paper is organized as follows.
We first give a brief overview of the subtask 3a of
SpaceEval and the corpus (Section 2). After that,
we describes related work (Section 3). Finally, we
present our approach (Section 4), evaluation results
(Section 5), and conclusions (Section 6).
</bodyText>
<sectionHeader confidence="0.996292" genericHeader="introduction">
2 The SpaceEval Task
</sectionHeader>
<subsectionHeader confidence="0.989676">
2.1 Subtask 3a: Task Description
</subsectionHeader>
<bodyText confidence="0.9999188">
Subtask 3a focuses solely on spatial relation extrac-
tion using a specified set of spatial elements for a
given sentence. Specifically, given an n-tuple of
participating entities, the goal is to (1) determine
whether the entities in the n-tuple form a spatial re-
</bodyText>
<page confidence="0.958725">
862
</page>
<note confidence="0.953318">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 862–869,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.9434985">
lation, and if so, (2) classify the roles of each partic-
ipating entity in the relation.
</bodyText>
<subsectionHeader confidence="0.998921">
2.2 Training Corpus
</subsectionHeader>
<bodyText confidence="0.99991728125">
To facilitate system development, 59 travel narra-
tives are marked up with seven types of spatial el-
ements (Table 1) and three types of spatial relations
(Table 2), following the ISO-Space (Pustejovsky et
al., 2013) annotation specifications, and provided as
training data. Note that a spatial-signal entity has
a semantic-type attribute expressing the type of the
relation it triggered. Its semantic-type can be topo-
logical, directional, or both.2
What is missing in Table 2 about spatial rela-
tions is that each entity participating in a relation
has a role. In QSLINKs and OLINKs, an element can
participate as a trajector (i.e., object of interest),
landmark (i.e., the grounding location), or trigger
(i.e., the relation indicator). Thus the QSLINK and
OLINK examples shown in Table 2, are actually rep-
resented as the triplet (flowertrajector, vaselandmark,
intrigger). While QSLINK and OLINK relations can
have only three fixed participants, a MOVELINK re-
lation has two fixed participants and up to six op-
tional participants to capture more precisely the re-
lational information expressed in the sentence. The
two mandatory MOVELINK participants are a mover
(i.e., object in motion), and a trigger (i.e., verb de-
noting motion). The six optional MOVELINK par-
ticipants are: source, midpoint, goal, path, and
landmark, express different aspects of the mover
in space, whereas a motion-signal connects the
spatial aspect to the mover.
Note that all spatial relations are intra-sentential.
In other words, all spatial elements participating in
a relation must appear in the same sentence.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.999797166666667">
Recall from Section 2 that spatial relation extraction
is composed of two subtasks, role labeling and rela-
tion classification of spatial elements. Prior systems
have adopted either a pipeline approach or a joint
approach to these subtasks. Given an n-tuple of dis-
tinct spatial elements in a sentence, a pipeline spatial
</bodyText>
<footnote confidence="0.715447666666667">
2In the ISO-Space scheme (Pustejovsky et al., 2013), dif-
ferent spatial entities have different attributes. We omit their
description here owing to space limitations.
</footnote>
<bodyText confidence="0.999944">
relation extraction system first assigns a role to each
spatial element and then uses a binary classifier to
determine whether the elements form a spatial rela-
tion or not (Kordjamshidi et al., 2011; Bastianelli et
al., 2013; Kordjamshidi and Moens, 2014).
One weakness of pipeline approaches is that er-
rors in role labeling can propagate to the relation
classification component. To address this prob-
lem, joint approaches were investigated (Roberts
and Harabagiu, 2012; Roberts et al., 2013). Given
an n-tuple of distinct spatial elements in a sentence
with an assignment of roles to each element, a joint
spatial relation extraction system uses a binary clas-
sifier to determine whether these elements form a
spatial relation with the roles correctly assigned to
all participating elements. In other words, the clas-
sifier will label the n-tuple as TRUE if and only if (1)
the elements in the n-tuple form a relation and (2)
their roles in the relation are correct.
We conclude this section by noting that virtually
all existing systems were developed on datasets that
adopted different or simpler representations of spa-
tial information than SpaceEval’s ISO-Space (2013)
representation (Mani et al., 2010; Kordjamshidi et
al., 2010; Kordjamshidi et al., 2012; Kolomiyets et
al., 2013). In other words, none of these systems
were designed to identify MOVELINKs.
</bodyText>
<sectionHeader confidence="0.984827" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.999922944444445">
To avoid the error propagation problem, we perform
joint role labeling and relation extraction. Unlike
previous work, where a single classifier was trained,
we employ an ensemble of eight classifiers. Creating
the eight classifiers permits (1) separating the treat-
ment of MOVELINKs from QSLINKs and OLINKs;
and (2) simplifying MOVELINK extraction.
We separate MOVELINKs from QSLINKs and
OLINKs for two reasons. First, MOVELINKs in-
volve objects in motion, whereas the other two
link types involve stationary objects. Second,
MOVELINKs are more complicated than the other
two link types: while QSLINKs and OLINKs have
three fixed participants, trajector, landmark and
trigger, MOVELINKs can have up to eight partic-
ipants, including two mandatory participants (i.e.,
mover and trigger) and six optional participants
(i.e., source, midpoint, goal, path, landmark,
</bodyText>
<page confidence="0.993143">
863
</page>
<table confidence="0.9225985">
place path spatial-entity non-motion event motion event motion-signal spatial-signal
(e.g., Rome) (e.g., road) (e.g., car) (e.g., is “serving”) (e.g., arrived) (e.g., by car) (e.g., north of)
</table>
<tableCaption confidence="0.992988">
Table 1: Seven types of spatial elements in SpaceEval.
</tableCaption>
<table confidence="0.8239839">
Relation Description Total
QSLINK Exists between stationary spatial elements with a regional connection. E.g., in The flower is in the 968
vase, the region of the vase has an internal connection with the region of the flower and hence they
are in a QSLINK.
OLINK Exists between stationary spatial elements expressing their relative or absolute orientations. E.g., 244
in The flower is in the vase, the flower and the vase also have an OLINK relation conveying that the
flower is oriented inside the vase.
MOVELINK Exists between spatial elements in motion. E.g., the sentence He biked from Cambridge to Maine 803
has a MOVELINK between mover He, motion verb biked, source of motion Cambridge, and goal
of motion Maine.
</table>
<tableCaption confidence="0.9597195">
Table 2: Three spatial relation types in SpaceEval. The “Total” column shows the number of instances annotated
with the corresponding relation in the training data.
</tableCaption>
<bodyText confidence="0.999879833333333">
and motion-signal). Given the complexity of a
MOVELINK, we decompose a MOVELINK into a set
of simpler relations that are to be identified by an
ensemble of classifiers.
In the rest of this section, we describe how we
train and test our ensemble.
</bodyText>
<subsectionHeader confidence="0.998811">
4.1 Training the Ensemble
</subsectionHeader>
<bodyText confidence="0.99996225">
We employ one classifier for identifying QSLINK
and OLINK relations (Section 4.1.1) and seven clas-
sifiers for identifying MOVELINK relations (Sec-
tion 4.1.2).
</bodyText>
<subsubsectionHeader confidence="0.533308">
4.1.1 The LINK Classifier
</subsubsectionHeader>
<bodyText confidence="0.999964794871795">
We collapse QSLINKs and OLINKs to a single rela-
tion type, LINK, identifying these two types of links
using the LINK classifier. To understand why we can
do this, first note that in QSLINKs and OLINKs, the
trigger has to be a spatial-signal element having a
semantic-type attribute. If its semantic-type is topo-
logical, it triggers a QSLINK; if it is directional, it
triggers an OLINK; and if it is both it triggers both
relation types. Hence, if a LINK is identified by
our classifier, we can simply use the semantic-type
value of the relation’s trigger element to automati-
cally determine whether the relation is a QSLINK an
OLINK, or both.
We create training instances for training a LINK
classifier as follows. Following the joint approach
described above, we create one training instance for
each possible role labeling of each triplet of dis-
tinct spatial elements in each sentence in a training
document. The role labels assigned to the spatial
elements in each triplet are subject to the follow-
ing constraints: (1) each triplet contains a trajec-
tor, a landmark, and a trigger; (2) neither the tra-
jector nor the landmark are of type spatial-signal
or motion-signal; and (3) the trigger is a spatial-
signal.3 Note that these role constraints are derived
from the data annotation scheme. It is worth noting
that while we enforce such global role constraints
when creating training instances, Kordjamshidi and
Moens (2014) enforce them at inference time using
Integer Linear Programming.
A training instance is labeled as TRUE if and only
if the elements in the triplet form a relation and
their roles in the relation are correct. As an exam-
ple, for the QSLINK and OLINK sentence in Table 2,
exactly one positive instance, LINK(flowertrajector,
vaselandmark, intrigger), will be created.
Each instance is represented using the 31 features
shown in Table 3. These features are modeled af-
ter those employed by state-of-the-art spatial rela-
</bodyText>
<footnote confidence="0.671049428571428">
3LINKs can have at most one implicit spatial ele-
ment. For example, the sentence The balloon went up has
LINK(balloontrajector,ontrigger) with an implicit landmark.
To account for LINKs with implicit trajector, landmark, or
trigger participants, we generate three additional triplets from
each LINK triplet, one for each participant having the value IM-
PLICIT.
</footnote>
<page confidence="0.988065">
864
</page>
<listItem confidence="0.991220967741936">
1. Lexical (6 features)
1. concatenated lemma strings of e1, e2, and e3
2. concatenated word strings of e1, e2, and e3
3. lexical pattern created from e1, e2, and e3 based on their order in the text (e.g.,
Trajector is Trigger Landmark)
4. words between the spatial elements
5. e3’s words
6. whether e2’s phrase was seen in role r3 in the training data
2. Grammatical (5 features)
1. dependency paths from e1 to e3 to e2 obtained using the Stanford Dependency Parser (de Marneffe
et al., 2006)
2. dependency paths from e1 to e2
3. dependency paths from e3 to e2
4. paths from e3 to e2 concatenated with e3’s string
5. whether e1 is a prepositional object of a preposition of an element posited in role r3 in any other
relation
3. Semantic (9 features)
1. WordNet (Miller, 1995) hypernyms and synsets of e1/e2
2. semantic role labels of e1/e2/e3 obtained using SENNA (Collobert et al., 2011)
3. General Inquirer (Stone et al., 1966) categories shared by e1 and e2
4. VerbNet (Kipper et al., 2000) classes shared by e1 and e2
4. Positional (2 features)
1. order of participants in text (e.g., r2-r1-r3)
2. whether the order is r3-r2-r1
5. Distance (3 features)
1. distance in tokens between e1 and e3 and that between e2 and e3
2. using a bin of 5 tokens, the concatenated binned distance between (e1,e2), (e1,e3), and (e2,e3)
6. Entity attributes (3 features)
1. spatial entity type of e1/e2/e3
7. Entity roles (3 features)
1. predicted spatial roles of e1/e2/e3 obtained using our in-house relation role labeler
</listItem>
<tableCaption confidence="0.564041">
Table 3: 31 features for spatial relation extraction. Each training instance corresponds to a triplet (e1,e2,e3),
where e1, e2, and e3 are spatial elements of types t1, t2, and t3, with participating roles r1, r2, and r3, respectively.
</tableCaption>
<bodyText confidence="0.996843142857143">
tion extraction systems. Recall that these systems
were developed on datasets that adopted different or
simpler representations of spatial information than
SpaceEval’s ISO-Space (2013) representation (Mani
et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi
et al., 2012; Kolomiyets et al., 2013). Hence, these
31 features have not been used to train classifiers for
extracting MOVELINKs.
We train the LINK classifier using the SVM learn-
ing algorithm as implemented in the SVMlight soft-
ware package (Joachims, 1999). To optimize classi-
fier performance, we tune two parameters, the reg-
ularization parameter C (which establishes the bal-
ance between generalizing and overfitting the classi-
</bodyText>
<page confidence="0.993168">
865
</page>
<bodyText confidence="0.99996975">
fier model to the training data) and the cost-factor
parameter J (which outweights training errors on
positive examples compared to the negative exam-
ples), to maximize F-score on development data.
</bodyText>
<subsubsectionHeader confidence="0.836015">
4.1.2 The Seven MOVELINK Classifiers
</subsubsectionHeader>
<bodyText confidence="0.999306492537313">
If we adopted the aforementioned joint method
as is for extracting MOVELINKs, each instance
would correspond to an octuple of the form:
MOVELINK(triggeri, moverj, sourcek, mid-pointm,
goaln, landmarko, pathp, motion-signalr), where
each participant in the octuple is either a distinct
spatial element with a role or the NULL element (if
it is not present in the relation). However, gener-
ating role permutations for octuples from all spatial
elements in a sentence is computationally infeasible.
In order to address this tractability problem, we sim-
plify MOVELINK extraction as follows. First, we de-
compose the MOVELINK octuple into seven smaller
tuples including one pair and six triplets. The
seven tuples are: (i) (triggeri, moverj); (ii) (triggeri,
moverj, sourcek); (iii) (triggeri, moverj, mid-
pointm); (iv) (triggeri, moverj, goaln); (v) (triggeri,
moverj, landmarko); (vi) (triggeri, moverj, pathp);
(vii) (triggeri, moverj, motion-signalr). Then, we
create seven separate classifiers for identifying the
seven MOVELINK tuples, respectively.
Using this decomposition for MOVELINK in-
stances, we can generate instances for each classi-
fier using the aforementioned joint approach as is.
For instance, to train classifier (i), we generate can-
didate pairs of the form (triggeri, moverj), where
triggeri and movers are spatial elements proposed
as a candidate trigger and mover, respectively. Pos-
itive training instances are those (triggeri, moverj)
pairs annotated with a relation in the training data,
while the rest of the candidate pairs are negative
training instances. The instances for training the re-
maining six classifiers are generated similarly.
As in the LINK classifier, we enforce global role
constraints when creating training instances for the
MOVELINK classifiers. Specifically, the roles as-
signed to the spatial elements in each training in-
stance of each of the MOVELINK classifiers are sub-
ject to the following constraints: (1) the trigger
has type motion; (2) the mover has type place,
path, spatial-entity or non-motion event; (3) the
source, the goal, and the landmark can be NULL
or has type place, path, spatial-entity, or non-
motion event; (4) the mid-point can be NULL or has
type place, path, or spatial-entity; (5) the path can
be NULL or has type path; and (6) the motion-signal
can be NULL or has type motion-signal.
Our way of decomposing the octuple along roles
can be justified as follows. Since the shared task
evaluates MOVELINKs only based on its mandatory
trigger and mover participants, we have a classi-
fier for classifying this core aspect of a motion re-
lation. The next six classifiers, (ii) to (vii), aim to
improve the core MOVELINK extraction by exploit-
ing the stronger contextual dependencies with each
of its unique spatial aspects namely the source, the
mid-point, the goal, the landmark, the path, and the
motion-signal.
As an example, for the MOVELINK sentence in
Table 2, we will create three positive instances:
(Hetrigger, bikedmover) for classifier (i), (Hetrigger,
bikedmover, Cambridgesource) for classifier (ii), and
(Hetrigger, bikedmover, Mainegoal) for classifier (iv).
We represent each training instance using the 31
features shown in Table 3, and train each of the
MOVELINK classifiers using SVMlight, with the C
and J values tuned on development data.
</bodyText>
<subsectionHeader confidence="0.997203">
4.2 Testing the Ensemble
</subsectionHeader>
<bodyText confidence="0.999995619047619">
After training, we apply the resulting classifiers to
classify the test instances, which are created in the
same manner as the training instances. As noted be-
fore, the LINK spatial relations extracted from a test
document by the LINK classifier are further qualified
as QSLINK, OLINK, or both based on the semantic-
type attribute value of its trigger participant. The
MOVELINK relations are extracted from a test doc-
ument by combining the outputs from the seven
MOVELINK classifiers. We explore three different
ways of combining the outputs. The first way is
simply to combine the outputs from all seven classi-
fiers. However, combining outputs in this way could
produce erroneous MOVELINK results, because it
could result in a spatial element being classified with
more than one role in the same relation since the
classifications are made independently. To address
this problem, we adopt a second way of combining
the seven classifier outputs to generate MOVELINKs.
Our second approach resolves multiple role classi-
fications for the same element in a relation by se-
</bodyText>
<page confidence="0.996037">
866
</page>
<table confidence="0.9986896">
QSLINK OLINK MOVELINK OVERALL
False True False True False True
R P F R P F R P F R P F R P F R P F R P F
Training Data 99.5 99.4 99.5 46.9 48.9 47.9 100 99.4 99.7 50.3 100 66.9 91.3 99.8 95.3 84.8 61.5 71.3 78.8 84.8 80.1
Test Data 99.6 99.3 99.4 55.3 68 61 99.9 99.9 99.9 67.9 76 71.7 98.3 97.3 97.8 72.7 81.6 76.9 82.3 87 84.5
</table>
<tableCaption confidence="0.998288">
Table 4: Results for spatial relation extraction using gold spatial elements.
</tableCaption>
<bodyText confidence="0.9998287">
lecting the role that was predicted with highest con-
fidence by the SVM. Our third approach addresses
this problem, alternatively, by using a predetermined
precedence of roles, decided based on training data
statistics of roles’ frequency, and selecting the role
that appears more frequently in the training data than
the other classified roles. Evaluations of the re-
spective outputs produced by adopting each of these
three ways showed that they all achieved a very sim-
ilar level of performance.
</bodyText>
<sectionHeader confidence="0.998275" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.998496">
In this section, we evaluate our ensemble approach
to spatial relation extraction.
</bodyText>
<subsectionHeader confidence="0.988358">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.910643742857143">
Dataset. We use the 59 travel narratives released
as the SpaceEval challenge training data for system
training and development. For testing, we use the 16
travel narratives released as the SpaceEval challenge
test data.
Evaluation metrics. Evaluation results are ob-
tained using the official SpaceEval challenge scor-
ing program. Results are expressed in terms
of recall (R), precision (P), and F-score (F).
When computing recall and precision, true posi-
tives for QSLINKs and OLINKs are those extracted
(trajector,landmark,trigger) triplets that match
with those in the gold data. True positives for
MOVELINKs are those extracted (trigger,mover)
pairs found in the gold data.4
Parameter tuning. As mentioned in the previous
section, we tune the C and J parameters on de-
velopment data when training each SVM classifier.
4During system development, we observed that
(trigger,mover) extraction can be improved by exploit-
ing its stronger dependencies with the optional MOVELINK
participants. Therefore, we have classifiers (ii) to (vii) in our
ensemble for extracting (trigger,mover) pairs missed by
classifier (i).
More specifically, during system training and devel-
opment, we perform five-fold cross validation. In
each fold experiment, we use three folds for training,
one fold for development, and one fold for testing.
Since joint tuning of these two parameters are
computationally expensive, we tune them as fol-
lows. We first tune C by setting the J parameter to
the default value in SVMlight. After finding the C
parameter that maximizes F-score on the develop-
ment set, we fix C and tune J to maximize F-score
on the development set.5
</bodyText>
<subsectionHeader confidence="0.977548">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99996475">
Table 4 shows the spatial relation extraction results
using gold spatial elements of our classifier ensem-
ble from the official SpaceEval scoring program.
The first row shows results from five-fold cross
validation on the training data. In each fold experi-
ment, we first tune the learning parameters of each
classifier as described in Section 5.1, and then re-
train the classifier on all four folds using the learned
parameters before applying it to the test fold. The
results reported are averaged over the five test folds.
The second row results are obtained from evaluation
on the official test data. Here, we train each classifier
on all of the training data. The learning parameters
of each classifier are tuned based on cross validation
on the training data. Specifically, we select the pa-
rameters that give the best averaged F-score over the
five development folds described in Section 5.1.
The column-wise results in the table show per-
formance on extracting the QSLINK, OLINK, and
MOVELINK spatial relations types, respectively, and
overall. The results under column “False” for each
relation type show performance in rejecting the re-
lation candidates that are not actual relations in the
gold data. And the results under column “True” for
</bodyText>
<footnote confidence="0.67126">
5For parameter tuning, C is chosen from the set
{0.01,0.05,0.1,0.5,1.0,10.0,50.0,100.0} and J is chosen from
the set {0.01,0.05,0.1,0.5,1.0,2.0,4.0,6.0}.
</footnote>
<page confidence="0.9839">
867
</page>
<table confidence="0.999888666666667">
R P F
Training Data 60.7 70.1 62
Test Data 65.3 75.2 69.9
</table>
<tableCaption confidence="0.752991">
Table 5: Overall results for spatial relation extrac-
tion of “True” relations using gold spatial elements.
</tableCaption>
<sectionHeader confidence="0.997394" genericHeader="evaluation">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999983">
We would like to thank the SpaceEval organizers
for creating the corpus and organizing the shared
task. We would also like to thank Daniel Cer and
the anonymous reviewers for their helpful sugges-
tions and comments.
each relation type show performance in extracting
relation candidates that are actual relations in the
gold data.
From Table 4, we see that on both the training
and test data, performance on rejecting the False re-
lation candidates is close to 100%. However, per-
formance on extracting the True relations is rela-
tively much lower. In decreasing order of perfor-
mance, our approach is most effective on extract-
ing MOVELINKs, followed by OLINKs, and then QS-
LINKs. Thus the relation types on which our ap-
proach performs poorly can direct our future efforts
in improving performance on this task. We see close
to 80% overall relation extraction F-score of our sys-
tem on both training and test data. This high perfor-
mance is mainly owing to the high performance of
our approach in rejecting the False relation candi-
dates. To better reflect the overall performance of
our approach, we show in Table 5 our overall re-
sults in extracting True relation types using only the
results in “True” columns of Table 4 for the three
relation types. From these results, we see that our
system performance is in the range of 65-70% F-
score on extracting the “True” spatial relations in
both datasets. Thus we see that there is still more
scope for improvement of our system in order to
make it practically usable for spatial relation extrac-
tion.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999978375">
We employed an ensemble approach to spatial re-
lation extraction. To address the complexity of a
MOVELINK, we decomposed it into smaller relations
so that the roles involved in each relation could be
extracted in a joint fashion without losing computa-
tional tractability. When evaluated on the SpaceEval
official test data for subtask 3a, our approach was
ranked first, achieving an F-score of 84.5%.
</bodyText>
<sectionHeader confidence="0.999432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916255813953">
Emanuele Bastianelli, Danilo Croce, Daniele Nardi, and
Roberto Basili. 2013. Unitor-HMM-TK: Structured
kernel-based learning for spatial role labeling. In Pro-
ceedings of SemEval 2013, pages 573–579.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493–
2537.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation, pages 449–454.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning, pages 44–56. MIT Press.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon. In
Proceedings of the Seventeenth National Conference
on Artificial Intelligence and the Twelfth Conference
on Innovative Applications of Artificial Intelligence,
pages 691–696.
Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven
Bethard, and Marie-Francine Moens. 2013. SemEval-
2013 Task 3: Spatial role labeling. In Proceedings of
SemEval 2013, pages 255–266.
Parisa Kordjamshidi and Marie-Francine Moens. 2014.
Global machine learning for spatial ontology popula-
tion. Web Semantics: Science, Services and Agents on
the World Wide Web.
Parisa Kordjamshidi, Marie-Francine Moens, and Mar-
tijn van Otterlo. 2010. Spatial role labeling: Task def-
inition and annotation scheme. In Proceedings of 7th
International Conference on Language Resources and
Evaluation, pages 413–420.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial role labeling: Towards
extraction of spatial relations from natural language.
ACM Transactions on Speech and Language Process-
ing, 8(3):4.
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012. SemEval-2012 task 3: Spatial
</reference>
<page confidence="0.981547">
868
</page>
<reference confidence="0.99980528">
role labeling. In Proceedings of SemEval 2012, pages
365–373.
Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitze-
man, Rob Quimby, Justin Richer, Ben Wellner, Scott
Mardis, and Seamus Clancy. 2010. SpatialML: an-
notation scheme, resources, and evaluation. Language
Resources and Evaluation, 44(3):263–280.
George A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
James Pustejovsky, Jessica Moszkowicz, and Marc Ver-
hagen. 2013. A linguistically grounded annota-
tion language for spatial information. ATALA: Asso-
ciation pour la Traitment Automatique des Langues,
53(2):87–113.
Kirk Roberts and Sanda M. Harabagiu. 2012. UTD-
SpRL: A joint approach to spatial role labeling. In
Proceedings of SemEval 2012, pages 419–424.
Kirk Roberts, Michael A. Skinner, and
Sanda M. Harabagiu. 2013. Recognizing spatial
containment relations between event mentions. In
Proceedings of the 10th International Conference on
Computational Semantics.
Philip J. Stone, Dexter C. Dunphy, and Marshall S. Smith.
1966. General Inquirer: A Computer Approach to
Content Analysis. The MIT Press.
</reference>
<page confidence="0.998919">
869
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341387">
<title confidence="0.997948">UTD: Ensemble-Based Spatial Relation Extraction</title>
<author confidence="0.88688">D’Souza</author>
<affiliation confidence="0.9854945">Human Language Technology Research University of Texas at</affiliation>
<address confidence="0.548781">Richardson, TX</address>
<abstract confidence="0.9811695625">SpaceEval (SemEval 2015 Task 8), which concerns spatial information extraction, builds on the spatial role identification tasks introduced in SemEval 2012 and used in SemEval 2013. Among the host of subtasks presented in SpaceEval, we participated in subtask 3a, which focuses solely on spatial relation extraction. To address the complexity of a we decompose it into smaller relations so that the roles involved in each relation can be extracted in a joint fashion without losing computational tractability. Our system was ranked first in the official evaluation, achieving an overall spatial relation extraction F-score of 84.5%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Emanuele Bastianelli</author>
<author>Danilo Croce</author>
<author>Daniele Nardi</author>
<author>Roberto Basili</author>
</authors>
<title>Unitor-HMM-TK: Structured kernel-based learning for spatial role labeling.</title>
<date>2013</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>573--579</pages>
<contexts>
<context position="5852" citStr="Bastianelli et al., 2013" startWordPosition="898" endWordPosition="901">ole labeling and relation classification of spatial elements. Prior systems have adopted either a pipeline approach or a joint approach to these subtasks. Given an n-tuple of distinct spatial elements in a sentence, a pipeline spatial 2In the ISO-Space scheme (Pustejovsky et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al., 2013). Given an n-tuple of distinct spatial elements in a sentence with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier w</context>
</contexts>
<marker>Bastianelli, Croce, Nardi, Basili, 2013</marker>
<rawString>Emanuele Bastianelli, Danilo Croce, Daniele Nardi, and Roberto Basili. 2013. Unitor-HMM-TK: Structured kernel-based learning for spatial role labeling. In Proceedings of SemEval 2013, pages 573–579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="12645" citStr="Collobert et al., 2011" startWordPosition="1994" endWordPosition="1997">lements 5. e3’s words 6. whether e2’s phrase was seen in role r3 in the training data 2. Grammatical (5 features) 1. dependency paths from e1 to e3 to e2 obtained using the Stanford Dependency Parser (de Marneffe et al., 2006) 2. dependency paths from e1 to e2 3. dependency paths from e3 to e2 4. paths from e3 to e2 concatenated with e3’s string 5. whether e1 is a prepositional object of a preposition of an element posited in role r3 in any other relation 3. Semantic (9 features) 1. WordNet (Miller, 1995) hypernyms and synsets of e1/e2 2. semantic role labels of e1/e2/e3 obtained using SENNA (Collobert et al., 2011) 3. General Inquirer (Stone et al., 1966) categories shared by e1 and e2 4. VerbNet (Kipper et al., 2000) classes shared by e1 and e2 4. Positional (2 features) 1. order of participants in text (e.g., r2-r1-r3) 2. whether the order is r3-r2-r1 5. Distance (3 features) 1. distance in tokens between e1 and e3 and that between e2 and e3 2. using a bin of 5 tokens, the concatenated binned distance between (e1,e2), (e1,e3), and (e2,e3) 6. Entity attributes (3 features) 1. spatial entity type of e1/e2/e3 7. Entity roles (3 features) 1. predicted spatial roles of e1/e2/e3 obtained using our in-house </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>449--454</pages>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods -Support Vector Learning,</booktitle>
<pages>44--56</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14020" citStr="Joachims, 1999" startWordPosition="2216" endWordPosition="2217"> spatial elements of types t1, t2, and t3, with participating roles r1, r2, and r3, respectively. tion extraction systems. Recall that these systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Hence, these 31 features have not been used to train classifiers for extracting MOVELINKs. We train the LINK classifier using the SVM learning algorithm as implemented in the SVMlight software package (Joachims, 1999). To optimize classifier performance, we tune two parameters, the regularization parameter C (which establishes the balance between generalizing and overfitting the classi865 fier model to the training data) and the cost-factor parameter J (which outweights training errors on positive examples compared to the negative examples), to maximize F-score on development data. 4.1.2 The Seven MOVELINK Classifiers If we adopted the aforementioned joint method as is for extracting MOVELINKs, each instance would correspond to an octuple of the form: MOVELINK(triggeri, moverj, sourcek, mid-pointm, goaln, </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Advances in Kernel Methods -Support Vector Learning, pages 44–56. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>Class-based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and the Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>691--696</pages>
<contexts>
<context position="12750" citStr="Kipper et al., 2000" startWordPosition="2013" endWordPosition="2016">ures) 1. dependency paths from e1 to e3 to e2 obtained using the Stanford Dependency Parser (de Marneffe et al., 2006) 2. dependency paths from e1 to e2 3. dependency paths from e3 to e2 4. paths from e3 to e2 concatenated with e3’s string 5. whether e1 is a prepositional object of a preposition of an element posited in role r3 in any other relation 3. Semantic (9 features) 1. WordNet (Miller, 1995) hypernyms and synsets of e1/e2 2. semantic role labels of e1/e2/e3 obtained using SENNA (Collobert et al., 2011) 3. General Inquirer (Stone et al., 1966) categories shared by e1 and e2 4. VerbNet (Kipper et al., 2000) classes shared by e1 and e2 4. Positional (2 features) 1. order of participants in text (e.g., r2-r1-r3) 2. whether the order is r3-r2-r1 5. Distance (3 features) 1. distance in tokens between e1 and e3 and that between e2 and e3 2. using a bin of 5 tokens, the concatenated binned distance between (e1,e2), (e1,e3), and (e2,e3) 6. Entity attributes (3 features) 1. spatial entity type of e1/e2/e3 7. Entity roles (3 features) 1. predicted spatial roles of e1/e2/e3 obtained using our in-house relation role labeler Table 3: 31 features for spatial relation extraction. Each training instance corres</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Karin Kipper, Hoa Trang Dang, and Martha Palmer. 2000. Class-based construction of a verb lexicon. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and the Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 691–696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oleksandr Kolomiyets</author>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>Marie-Francine Moens</author>
</authors>
<date>2013</date>
<booktitle>SemEval2013 Task 3: Spatial role labeling. In Proceedings of SemEval</booktitle>
<pages>255--266</pages>
<contexts>
<context position="1542" citStr="Kolomiyets et al., 2013" startWordPosition="222" endWordPosition="225"> task for the semantic evaluation of spatial information extraction (IE) systems. The goals of the shared task include identifying and classifying particular constructions in natural language for expressing spatial information that are conveyed through the spatial concepts of locations, entities participating in spatial relations, paths, topological relations, direction and orientation, motion, etc. It presents a wide spectrum of spatial IE related subtasks for interested participants to choose from, building on the two previous years shared tasks on the same topic (Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Our goal in this paper is to describe the version of our spatial relation extraction system that partic1http://alt.qcri.org/semeval2015/task8/ ipated in subtask 3a of SpaceEval. Systems participating in this subtask assume as input the spatial elements in a text document. For example, in the sentence The flower is in the vases and the vase2 is on the table, the set of spatial elements {flower, in, vases, vase2, on, table} are given and subsequently used as candidates for predicting spatial relations. Leveraging the successes of a joint role-labeling approach to spatial relation extraction in</context>
<context position="6912" citStr="Kolomiyets et al., 2013" startWordPosition="1065" endWordPosition="1068">sifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE if and only if (1) the elements in the n-tuple form a relation and (2) their roles in the relation are correct. We conclude this section by noting that virtually all existing systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). In other words, none of these systems were designed to identify MOVELINKs. 4 Our Approach To avoid the error propagation problem, we perform joint role labeling and relation extraction. Unlike previous work, where a single classifier was trained, we employ an ensemble of eight classifiers. Creating the eight classifiers permits (1) separating the treatment of MOVELINKs from QSLINKs and OLINKs; and (2) simplifying MOVELINK extraction. We separate MOVELINKs from QSLINKs and OLINKs for two reasons. First, MOVELINKs involve objects in motion, whereas the other two link types involve stationary o</context>
<context position="13801" citStr="Kolomiyets et al., 2013" startWordPosition="2179" endWordPosition="2182"> 1. predicted spatial roles of e1/e2/e3 obtained using our in-house relation role labeler Table 3: 31 features for spatial relation extraction. Each training instance corresponds to a triplet (e1,e2,e3), where e1, e2, and e3 are spatial elements of types t1, t2, and t3, with participating roles r1, r2, and r3, respectively. tion extraction systems. Recall that these systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Hence, these 31 features have not been used to train classifiers for extracting MOVELINKs. We train the LINK classifier using the SVM learning algorithm as implemented in the SVMlight software package (Joachims, 1999). To optimize classifier performance, we tune two parameters, the regularization parameter C (which establishes the balance between generalizing and overfitting the classi865 fier model to the training data) and the cost-factor parameter J (which outweights training errors on positive examples compared to the negative examples), to maximize F-score on development data. 4.1.2 The</context>
</contexts>
<marker>Kolomiyets, Kordjamshidi, Bethard, Moens, 2013</marker>
<rawString>Oleksandr Kolomiyets, Parisa Kordjamshidi, Steven Bethard, and Marie-Francine Moens. 2013. SemEval2013 Task 3: Spatial role labeling. In Proceedings of SemEval 2013, pages 255–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Global machine learning for spatial ontology population. Web Semantics: Science, Services and Agents on the World Wide Web.</title>
<date>2014</date>
<contexts>
<context position="5883" citStr="Kordjamshidi and Moens, 2014" startWordPosition="902" endWordPosition="905">classification of spatial elements. Prior systems have adopted either a pipeline approach or a joint approach to these subtasks. Given an n-tuple of distinct spatial elements in a sentence, a pipeline spatial 2In the ISO-Space scheme (Pustejovsky et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al., 2013). Given an n-tuple of distinct spatial elements in a sentence with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE i</context>
<context position="10877" citStr="Kordjamshidi and Moens (2014)" startWordPosition="1697" endWordPosition="1700">r each possible role labeling of each triplet of distinct spatial elements in each sentence in a training document. The role labels assigned to the spatial elements in each triplet are subject to the following constraints: (1) each triplet contains a trajector, a landmark, and a trigger; (2) neither the trajector nor the landmark are of type spatial-signal or motion-signal; and (3) the trigger is a spatialsignal.3 Note that these role constraints are derived from the data annotation scheme. It is worth noting that while we enforce such global role constraints when creating training instances, Kordjamshidi and Moens (2014) enforce them at inference time using Integer Linear Programming. A training instance is labeled as TRUE if and only if the elements in the triplet form a relation and their roles in the relation are correct. As an example, for the QSLINK and OLINK sentence in Table 2, exactly one positive instance, LINK(flowertrajector, vaselandmark, intrigger), will be created. Each instance is represented using the 31 features shown in Table 3. These features are modeled after those employed by state-of-the-art spatial rela3LINKs can have at most one implicit spatial element. For example, the sentence The b</context>
</contexts>
<marker>Kordjamshidi, Moens, 2014</marker>
<rawString>Parisa Kordjamshidi and Marie-Francine Moens. 2014. Global machine learning for spatial ontology population. Web Semantics: Science, Services and Agents on the World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Marie-Francine Moens</author>
<author>Martijn van Otterlo</author>
</authors>
<title>Spatial role labeling: Task definition and annotation scheme.</title>
<date>2010</date>
<booktitle>In Proceedings of 7th International Conference on Language Resources and Evaluation,</booktitle>
<pages>413--420</pages>
<marker>Kordjamshidi, Moens, van Otterlo, 2010</marker>
<rawString>Parisa Kordjamshidi, Marie-Francine Moens, and Martijn van Otterlo. 2010. Spatial role labeling: Task definition and annotation scheme. In Proceedings of 7th International Conference on Language Resources and Evaluation, pages 413–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Martijn Van Otterlo</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Spatial role labeling: Towards extraction of spatial relations from natural language.</title>
<date>2011</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>8</volume>
<issue>3</issue>
<marker>Kordjamshidi, Van Otterlo, Moens, 2011</marker>
<rawString>Parisa Kordjamshidi, Martijn Van Otterlo, and MarieFrancine Moens. 2011. Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Transactions on Speech and Language Processing, 8(3):4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Steven Bethard</author>
<author>MarieFrancine Moens</author>
</authors>
<title>SemEval-2012 task 3: Spatial role labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>365--373</pages>
<contexts>
<context position="1516" citStr="Kordjamshidi et al., 2012" startWordPosition="218" endWordPosition="221">1 was organized as a shared task for the semantic evaluation of spatial information extraction (IE) systems. The goals of the shared task include identifying and classifying particular constructions in natural language for expressing spatial information that are conveyed through the spatial concepts of locations, entities participating in spatial relations, paths, topological relations, direction and orientation, motion, etc. It presents a wide spectrum of spatial IE related subtasks for interested participants to choose from, building on the two previous years shared tasks on the same topic (Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Our goal in this paper is to describe the version of our spatial relation extraction system that partic1http://alt.qcri.org/semeval2015/task8/ ipated in subtask 3a of SpaceEval. Systems participating in this subtask assume as input the spatial elements in a text document. For example, in the sentence The flower is in the vases and the vase2 is on the table, the set of spatial elements {flower, in, vases, vase2, on, table} are given and subsequently used as candidates for predicting spatial relations. Leveraging the successes of a joint role-labeling approach to spat</context>
<context position="6886" citStr="Kordjamshidi et al., 2012" startWordPosition="1061" endWordPosition="1064">n system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE if and only if (1) the elements in the n-tuple form a relation and (2) their roles in the relation are correct. We conclude this section by noting that virtually all existing systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). In other words, none of these systems were designed to identify MOVELINKs. 4 Our Approach To avoid the error propagation problem, we perform joint role labeling and relation extraction. Unlike previous work, where a single classifier was trained, we employ an ensemble of eight classifiers. Creating the eight classifiers permits (1) separating the treatment of MOVELINKs from QSLINKs and OLINKs; and (2) simplifying MOVELINK extraction. We separate MOVELINKs from QSLINKs and OLINKs for two reasons. First, MOVELINKs involve objects in motion, whereas the other two link </context>
<context position="13775" citStr="Kordjamshidi et al., 2012" startWordPosition="2175" endWordPosition="2178">. Entity roles (3 features) 1. predicted spatial roles of e1/e2/e3 obtained using our in-house relation role labeler Table 3: 31 features for spatial relation extraction. Each training instance corresponds to a triplet (e1,e2,e3), where e1, e2, and e3 are spatial elements of types t1, t2, and t3, with participating roles r1, r2, and r3, respectively. tion extraction systems. Recall that these systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Hence, these 31 features have not been used to train classifiers for extracting MOVELINKs. We train the LINK classifier using the SVM learning algorithm as implemented in the SVMlight software package (Joachims, 1999). To optimize classifier performance, we tune two parameters, the regularization parameter C (which establishes the balance between generalizing and overfitting the classi865 fier model to the training data) and the cost-factor parameter J (which outweights training errors on positive examples compared to the negative examples), to maximize F-score on d</context>
</contexts>
<marker>Kordjamshidi, Bethard, Moens, 2012</marker>
<rawString>Parisa Kordjamshidi, Steven Bethard, and MarieFrancine Moens. 2012. SemEval-2012 task 3: Spatial role labeling. In Proceedings of SemEval 2012, pages 365–373.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Christy Doran</author>
<author>Dave Harris</author>
<author>Janet Hitzeman</author>
<author>Rob Quimby</author>
<author>Justin Richer</author>
<author>Ben Wellner</author>
<author>Scott Mardis</author>
<author>Seamus Clancy</author>
</authors>
<title>SpatialML: annotation scheme, resources, and evaluation.</title>
<date>2010</date>
<journal>Language Resources and Evaluation,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="6832" citStr="Mani et al., 2010" startWordPosition="1053" endWordPosition="1056">ch element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE if and only if (1) the elements in the n-tuple form a relation and (2) their roles in the relation are correct. We conclude this section by noting that virtually all existing systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). In other words, none of these systems were designed to identify MOVELINKs. 4 Our Approach To avoid the error propagation problem, we perform joint role labeling and relation extraction. Unlike previous work, where a single classifier was trained, we employ an ensemble of eight classifiers. Creating the eight classifiers permits (1) separating the treatment of MOVELINKs from QSLINKs and OLINKs; and (2) simplifying MOVELINK extraction. We separate MOVELINKs from QSLINKs and OLINKs for two reasons. First, MOVELINKs </context>
<context position="13721" citStr="Mani et al., 2010" startWordPosition="2167" endWordPosition="2170">features) 1. spatial entity type of e1/e2/e3 7. Entity roles (3 features) 1. predicted spatial roles of e1/e2/e3 obtained using our in-house relation role labeler Table 3: 31 features for spatial relation extraction. Each training instance corresponds to a triplet (e1,e2,e3), where e1, e2, and e3 are spatial elements of types t1, t2, and t3, with participating roles r1, r2, and r3, respectively. tion extraction systems. Recall that these systems were developed on datasets that adopted different or simpler representations of spatial information than SpaceEval’s ISO-Space (2013) representation (Mani et al., 2010; Kordjamshidi et al., 2010; Kordjamshidi et al., 2012; Kolomiyets et al., 2013). Hence, these 31 features have not been used to train classifiers for extracting MOVELINKs. We train the LINK classifier using the SVM learning algorithm as implemented in the SVMlight software package (Joachims, 1999). To optimize classifier performance, we tune two parameters, the regularization parameter C (which establishes the balance between generalizing and overfitting the classi865 fier model to the training data) and the cost-factor parameter J (which outweights training errors on positive examples compar</context>
</contexts>
<marker>Mani, Doran, Harris, Hitzeman, Quimby, Richer, Wellner, Mardis, Clancy, 2010</marker>
<rawString>Inderjeet Mani, Christy Doran, Dave Harris, Janet Hitzeman, Rob Quimby, Justin Richer, Ben Wellner, Scott Mardis, and Seamus Clancy. 2010. SpatialML: annotation scheme, resources, and evaluation. Language Resources and Evaluation, 44(3):263–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="12532" citStr="Miller, 1995" startWordPosition="1978" endWordPosition="1979">3 based on their order in the text (e.g., Trajector is Trigger Landmark) 4. words between the spatial elements 5. e3’s words 6. whether e2’s phrase was seen in role r3 in the training data 2. Grammatical (5 features) 1. dependency paths from e1 to e3 to e2 obtained using the Stanford Dependency Parser (de Marneffe et al., 2006) 2. dependency paths from e1 to e2 3. dependency paths from e3 to e2 4. paths from e3 to e2 concatenated with e3’s string 5. whether e1 is a prepositional object of a preposition of an element posited in role r3 in any other relation 3. Semantic (9 features) 1. WordNet (Miller, 1995) hypernyms and synsets of e1/e2 2. semantic role labels of e1/e2/e3 obtained using SENNA (Collobert et al., 2011) 3. General Inquirer (Stone et al., 1966) categories shared by e1 and e2 4. VerbNet (Kipper et al., 2000) classes shared by e1 and e2 4. Positional (2 features) 1. order of participants in text (e.g., r2-r1-r3) 2. whether the order is r3-r2-r1 5. Distance (3 features) 1. distance in tokens between e1 and e3 and that between e2 and e3 2. using a bin of 5 tokens, the concatenated binned distance between (e1,e2), (e1,e3), and (e2,e3) 6. Entity attributes (3 features) 1. spatial entity </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jessica Moszkowicz</author>
<author>Marc Verhagen</author>
</authors>
<title>A linguistically grounded annotation language for spatial information. ATALA:</title>
<date>2013</date>
<booktitle>Association pour la Traitment Automatique des Langues,</booktitle>
<volume>53</volume>
<issue>2</issue>
<contexts>
<context position="3755" citStr="Pustejovsky et al., 2013" startWordPosition="571" endWordPosition="574">-tuple of participating entities, the goal is to (1) determine whether the entities in the n-tuple form a spatial re862 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 862–869, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics lation, and if so, (2) classify the roles of each participating entity in the relation. 2.2 Training Corpus To facilitate system development, 59 travel narratives are marked up with seven types of spatial elements (Table 1) and three types of spatial relations (Table 2), following the ISO-Space (Pustejovsky et al., 2013) annotation specifications, and provided as training data. Note that a spatial-signal entity has a semantic-type attribute expressing the type of the relation it triggered. Its semantic-type can be topological, directional, or both.2 What is missing in Table 2 about spatial relations is that each entity participating in a relation has a role. In QSLINKs and OLINKs, an element can participate as a trajector (i.e., object of interest), landmark (i.e., the grounding location), or trigger (i.e., the relation indicator). Thus the QSLINK and OLINK examples shown in Table 2, are actually represented </context>
<context position="5514" citStr="Pustejovsky et al., 2013" startWordPosition="845" endWordPosition="848">the mover in space, whereas a motion-signal connects the spatial aspect to the mover. Note that all spatial relations are intra-sentential. In other words, all spatial elements participating in a relation must appear in the same sentence. 3 Related Work Recall from Section 2 that spatial relation extraction is composed of two subtasks, role labeling and relation classification of spatial elements. Prior systems have adopted either a pipeline approach or a joint approach to these subtasks. Given an n-tuple of distinct spatial elements in a sentence, a pipeline spatial 2In the ISO-Space scheme (Pustejovsky et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al.,</context>
</contexts>
<marker>Pustejovsky, Moszkowicz, Verhagen, 2013</marker>
<rawString>James Pustejovsky, Jessica Moszkowicz, and Marc Verhagen. 2013. A linguistically grounded annotation language for spatial information. ATALA: Association pour la Traitment Automatique des Langues, 53(2):87–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirk Roberts</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>UTDSpRL: A joint approach to spatial role labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of SemEval</booktitle>
<pages>419--424</pages>
<contexts>
<context position="6097" citStr="Roberts and Harabagiu, 2012" startWordPosition="934" endWordPosition="937"> ISO-Space scheme (Pustejovsky et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al., 2013). Given an n-tuple of distinct spatial elements in a sentence with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE if and only if (1) the elements in the n-tuple form a relation and (2) their roles in the relation are correct. We conclude this section by noting that virtually all existing systems were developed on datasets that </context>
</contexts>
<marker>Roberts, Harabagiu, 2012</marker>
<rawString>Kirk Roberts and Sanda M. Harabagiu. 2012. UTDSpRL: A joint approach to spatial role labeling. In Proceedings of SemEval 2012, pages 419–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kirk Roberts</author>
<author>Michael A Skinner</author>
<author>Sanda M Harabagiu</author>
</authors>
<title>Recognizing spatial containment relations between event mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics.</booktitle>
<contexts>
<context position="6120" citStr="Roberts et al., 2013" startWordPosition="938" endWordPosition="941">y et al., 2013), different spatial entities have different attributes. We omit their description here owing to space limitations. relation extraction system first assigns a role to each spatial element and then uses a binary classifier to determine whether the elements form a spatial relation or not (Kordjamshidi et al., 2011; Bastianelli et al., 2013; Kordjamshidi and Moens, 2014). One weakness of pipeline approaches is that errors in role labeling can propagate to the relation classification component. To address this problem, joint approaches were investigated (Roberts and Harabagiu, 2012; Roberts et al., 2013). Given an n-tuple of distinct spatial elements in a sentence with an assignment of roles to each element, a joint spatial relation extraction system uses a binary classifier to determine whether these elements form a spatial relation with the roles correctly assigned to all participating elements. In other words, the classifier will label the n-tuple as TRUE if and only if (1) the elements in the n-tuple form a relation and (2) their roles in the relation are correct. We conclude this section by noting that virtually all existing systems were developed on datasets that adopted different or si</context>
</contexts>
<marker>Roberts, Skinner, Harabagiu, 2013</marker>
<rawString>Kirk Roberts, Michael A. Skinner, and Sanda M. Harabagiu. 2013. Recognizing spatial containment relations between event mentions. In Proceedings of the 10th International Conference on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
</authors>
<title>General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12686" citStr="Stone et al., 1966" startWordPosition="2001" endWordPosition="2004">was seen in role r3 in the training data 2. Grammatical (5 features) 1. dependency paths from e1 to e3 to e2 obtained using the Stanford Dependency Parser (de Marneffe et al., 2006) 2. dependency paths from e1 to e2 3. dependency paths from e3 to e2 4. paths from e3 to e2 concatenated with e3’s string 5. whether e1 is a prepositional object of a preposition of an element posited in role r3 in any other relation 3. Semantic (9 features) 1. WordNet (Miller, 1995) hypernyms and synsets of e1/e2 2. semantic role labels of e1/e2/e3 obtained using SENNA (Collobert et al., 2011) 3. General Inquirer (Stone et al., 1966) categories shared by e1 and e2 4. VerbNet (Kipper et al., 2000) classes shared by e1 and e2 4. Positional (2 features) 1. order of participants in text (e.g., r2-r1-r3) 2. whether the order is r3-r2-r1 5. Distance (3 features) 1. distance in tokens between e1 and e3 and that between e2 and e3 2. using a bin of 5 tokens, the concatenated binned distance between (e1,e2), (e1,e3), and (e2,e3) 6. Entity attributes (3 features) 1. spatial entity type of e1/e2/e3 7. Entity roles (3 features) 1. predicted spatial roles of e1/e2/e3 obtained using our in-house relation role labeler Table 3: 31 feature</context>
</contexts>
<marker>Stone, Dunphy, Smith, 1966</marker>
<rawString>Philip J. Stone, Dexter C. Dunphy, and Marshall S. Smith. 1966. General Inquirer: A Computer Approach to Content Analysis. The MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>