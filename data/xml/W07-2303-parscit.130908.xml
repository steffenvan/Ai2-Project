<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003352">
<title confidence="0.980253">
Stochastic Realisation Ranking for a Free Word Order Language
</title>
<author confidence="0.995745">
Aoife Cahill, Martin Forst and Christian Rohrer
</author>
<affiliation confidence="0.998302">
Institute of Natural Language Processing
University of Stuttgart, Germany
</affiliation>
<email confidence="0.991495">
{cahillae|forst|rohrer}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995488" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965636363637">
We present a log-linear model that is used
for ranking the string realisations produced for
given corpus f-structures by a reversible broad-
coverage LFG for German and compare its re-
sults with the ones achieved by the application
of a language model (LM). Like other authors
that have developed log-linear models for reali-
sation ranking, we use a hybrid model that uses
linguistically motivated learning features and a
LM (whose score is simply integrated into the
log-linear model as an additional feature) for
the task of realisation ranking. We carry out a
large evaluation of the model, training on over
8,600 structures and testing on 323. We ob-
serve that the contribution that the structural
features make to the quality of the output is
slightly greater in the case of a free word order
language like German than it is in the case of
English. The exact match metric improves from
27% to 37% when going from the LM-based re-
alisation ranking to the hybrid model, BLEU
score improves from 0.7306 to 0.7939.
</bodyText>
<sectionHeader confidence="0.998736" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979090909091">
Most traditional approaches to stochastic reali-
sation ranking involve applying language model
n-gram statistics to rank alternatives (Langk-
ilde, 2000; Bangalore and Rambow, 2000;
Langkilde-Geary, 2002). Much work has been
carried out into statistical realisation ranking
for English. However, n-grams alone (even
if they are efficiently implemented) may not
be a good enough measure for ranking candi-
date strings, particularly in free-word order lan-
guages.
Belz (2005) moves away from n-gram mod-
els of generation and trains a generator on a
generation treebank, achieving similar results
to a bigram model but at much lower compu-
tational cost. Cahill and van Genabith (2006)
do not use a language model, but rather rely on
treebank-based automatically derived LFG gen-
eration grammars to determine the most likely
surface order.
Ohkuma (2004) writes an LFG genera-
tion grammar for Japanese separate from the
Japanese LFG parsing grammar in order to en-
force canonical word order by symbolic means.
In another purely symbolic approach, Callaway
(2003; 2004) describes a wide-coverage system
and the author argues that there are several ad-
vantages to a symbolic system over a statistical
one. We argue that a reversible symbolic sys-
tem, which is desirable for maintainability and
modularity reasons, augmented with a statisti-
cal ranking component can produce systemat-
ically ranked, high quality surface realisations
while maintaining the flexibility associated with
hand-crafted systems.
Velldal et al. (2004) and Velldal and Oepen
(2005) present discriminative disambiguation
models using a hand-crafted HPSG grammar
for generation from MRS (Minimal Recursion
Semantics) structures. They describe three sta-
tistical models for realization ranking: The first
is a simple n-gram language model, the second
uses structural features in a maximum entropy
model for disambiguation and a third uses a
combination of the two models. Their results
show that the third model where the n-gram
language model is combined with the struc-
tural features in the maximum entropy disam-
biguation model performs best. Nakanishi et
al. (2005) present similar probabilistic models
for a chart generator using a HPSG grammar
acquired from the Penn-II Treebank (the Enju
HPSG), with the difference that, in their ex-
periments, the model that only uses structural
features outperformed the hybrid model. We
</bodyText>
<page confidence="0.998307">
17
</page>
<bodyText confidence="0.999741">
present a model for realisation ranking similar
to the models just mentioned. The main differ-
ences between our work and theirs is that we are
working within the LFG framework and concen-
trating on a less configurational language: Ger-
man.
</bodyText>
<sectionHeader confidence="0.987679" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.996709">
2.1 Lexical Functional Grammar
</subsectionHeader>
<bodyText confidence="0.97950803125">
The work presented in this paper is couched
in the framework of Lexical Functional Gram-
mar (LFG). LFG is a grammar formalism which
makes use of two representation levels to encode
syntactic properties of sentences: constituent
structure (c-structure) and functional struc-
ture (f-structure). C-structures are context-free
trees that encode constituency and linear order.
F-structures are attribute-value matrices that
encode grammatical relations and morphosyn-
tactic features. While translational equivalents
of sentences may vary considerably across lan-
guages at the c-structure level, it is assumed
that, at the f-structure level, where linear or-
der is abstracted away from, languages behave
much more alike. Also, f-structures are taken as
the interface from syntax to semantics. From a
language-technological perspective, f-structures
are the level of representation various (proto-
types of) question-answering systems, a sen-
tence condensation system and machine trans-
lation systems operate on and generate from.
Figures 1 and 2 illustrate the c-structure and
the f-structure that the German broad-coverage
LFG presented in the next paragraph produces
for (1).
(1) Verheugen habe die Worte des
Verheugen had the words the-GEN
Generalinspekteurs falsch interpretiert.
inspector-general wrongly interpreted.
‘Verheugen had mis-interpreted the words
of the inspector-general.’
</bodyText>
<page confidence="0.7445">
21
</page>
<figure confidence="0.995803357142857">
CS 1: ROOT
CProot[std]
PERIOD
Vaux[haben,fin]
habe
VP[v,part]
VPx[v,part]
DP[std]
DPx[std]
NP
Cbar
.
NAMEP DP[std] VPx[v,part]
Generalinspekteurs falsch interpretiert
</figure>
<figureCaption confidence="0.995327">
Figure 1: C-structure for (1)
</figureCaption>
<figure confidence="0.99614106122449">
&amp;quot;Verheugen habe die Worte des Generalinspekteurs falsch interpretiert
PRED &apos;interpretieren&lt;[1:Verheugen], [106:Wort]&gt;&apos;
PRED &apos;falsch&lt;[279-SUBJ:pro]&gt;&apos;
PRED &apos;pro&apos;
ADJUNCT
SUBJ
NSYN pronoun
NTYPE
PRON-TYPE null
ATYPE adverbial, DEG-DIM pos, DEGREE positive
PRED &apos;Wort&apos;
PRED &apos;Inspekteur&apos;
279
PRED &apos;General&apos;
COMMON count
common
PRED &apos;die&apos;
DET-TYPE def
MOD
-12
ADJ-GEN
NTYPE
OBJ
SPEC
DET
NSEM
NSYN
CASE gen, GEND masc, NUM sg, PERS 3
COMMON count
common
PRED &apos;die&apos;
DET-TYPE
NSEM
NSYN
DET
def
NTYPE
SPEC
CASE acc, GEND neut, NUM pl, PERS 3
PRED &apos;Verheugen&apos;
106
229
SUBJ
NTYPE
NSYN proper
PROPER-TYPE name
1 CASE nom, NUM sg, PERS 3
TNS-ASP MOOD subjunctive, PERF +_, TENSE pres
CLAUSE-TYPE decl, PASSIVE -, STMT-TYPE decl, VTYPE main
</figure>
<figureCaption confidence="0.765568">
Figure 2: F-structure for (1)
</figureCaption>
<figure confidence="0.99974952173913">
NSEM PROPER
ADVP[std]
AP[std,-infl]
APx[std,-infl]
A[-infl]
VPx[v,part]
VC[v,part]
V[v,part]
Vx[v,part]
NAME
DPx[std]
DP[std]
Worte
des
N[comm]
Verheugen
D[std]
DPx[std]
die
N[comm]
NP
NP
D[std]
</figure>
<subsectionHeader confidence="0.84474">
2.2 A broad-coverage LFG for German
2.2 A broad-coverage LFG for German parsing coverage of about 80% in terms of full
</subsectionHeader>
<bodyText confidence="0.995107157894737">
parsing coverage of about 80% in terms of full
For the construction of our data, we use the Ger-
parses on newspaper text, and for sentences out
For the construction of our data, we use the Ger- parses on newspaper text, and for sentences out
man broad-coverage LFG documented in Dip-
of coverage, the robustness techniques described
man broad-coverage LFG documented in Dip- of coverage, the robustness techniques described
per (2003) and Rohrer and Forst (2006). It is
in Riezler et al. (2002) (fragment grammar,
per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar,
‘skimming’) are employed for the construction
a hand-crafted grammar developed in and for ‘skimming’) are employed for the construction
a hand-crafted grammar developed in and for
the LFG grammar development and processing
of partial analyses. The grammar is reversible,
the LFG grammar development and processing of partial analyses. The grammar is reversible,
platform XLE (Crouch et al., 2006). It achieves
platform XLE (Crouch et al., 2006). It achieves which means that the XLE generator can pro-
which means that the XLE generator can pro-
</bodyText>
<page confidence="0.990781">
18
</page>
<bodyText confidence="0.9999417">
duce surface realisations for well-formed input
f-structures.
Recently, the grammar has been comple-
mented with a stochastic disambiguation mod-
ule along the lines of Riezler et al. (2002), con-
sisting of a log-linear model based on structural
features (Forst, 2007). This module makes it
possible to determine one c-/f-structure pair as
the most probable analysis of any given sen-
tence.
</bodyText>
<subsectionHeader confidence="0.998732">
2.3 Surface realisation
</subsectionHeader>
<bodyText confidence="0.956231666666666">
As XLE comes with a fully-fledged generator,
the grammar can be used both for parsing and
for surface realisation. Figure 3 shows an ex-
cerpt of the set of strings (and their LM rank-
ing) that are generated from the f-structure in
Figure 2. Note that all of these (as well as the
the remaining 139 strings in the set) are gram-
matical; however, some of them are clearly more
likely or unmarked than others.
</bodyText>
<listItem confidence="0.9677315">
1. Falsch interpretiert habe die Worte
Wrongly interpreted had the words
des Generalinspekteurs Verheugen.
the-GEN inspector-general Verheugen.
2. Falsch interpretiert habe die Worte
des Generalinspekteures Verheugen.
3. Die Worte des Generalinspekteurs
falsch interpretiert habe Verheugen.
5. Die Worte des Generalinspekteurs habe
Verheugen falsch interpretiert.
7. Verheugen habe die Worte des General-
inspekteurs falsch interpretiert.
</listItem>
<figureCaption confidence="0.760821333333333">
Figure 3: Excerpt of the set of 144 strings gen-
erated from the f-structure in Figure 2, ordered
according to their LM score
</figureCaption>
<bodyText confidence="0.999868761904762">
Just as hand-crafted grammars, when used
for parsing, are only useful for most applica-
tions when they have been complemented with
a disambiguation module, their usefulness as a
means of surface realisation depends on a re-
liable module for realisation ranking. A long
list of arbitrarily ordered output strings is use-
less for practical applications such as summari-
sation, QA, MT etc.
Very regular preferences for certain reali-
sation alternatives over others can be imple-
mented by means of so-called optimality marks
(Frank et al., 2001), which are implemented in
XLE both for the parsing and the generation
direction. For ranking string realisations on the
basis of ‘soft’ and potentially contradictory con-
straints, however, the stochastic approach based
on a log-linear model, as it has previously been
implemented for English HPSGs (Nakanishi et
al., 2005; Velldal and Oepen, 2005), seems more
adequate.
</bodyText>
<sectionHeader confidence="0.995787" genericHeader="method">
3 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.992335">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.999735214285714">
We use the TIGER Treebank (Brants et al.,
2002) to train and test our model. It consists
of just over 50,000 annotated sentences of Ger-
man newspaper text. The sentences have been
annotated with morphological and syntactic in-
formation in the form of functionally labelled
graphs that may contain crossing and secondary
edges.
We split the data into training and test data
using the same data split as in Forst (2007),
i.e. sentences 8,001–10,000 of the TIGER
Treebank are reserved for evaluation. Within
this section, we have 422 TIGER annotation-
compatible f-structures, which are further di-
vided into 86 development and 336 test struc-
tures. We use the development set to tune the
parameters of the log-linear model. Of the 86
heldout sentences and the 336 test sentences, 78
and 323 respectively are of length &gt;3 and are
actually used for our final evaluation.
For training, we build a symmetric treebank
of 8,609 packed c/f-structure representations in
a similar manner to Velldal et al. (2004). We do
not include structures for which only one string
is generated, since the log-linear model for re-
alisation ranking cannot learn anything from
them. The symmetric treebank was established
using the following strategy:
</bodyText>
<listItem confidence="0.9993676">
1. Parse input sentence from TIGER Tree-
bank.
2. Select all of the analyses that are compati-
ble with the TIGER Treebank annotation.
3. Of all the TIGER-compatible analyses,
choose the most likely c/f-structure pair ac-
cording to log-linear model for parse disam-
biguation.
4. Generate from the f-structure part of this
analysis.
</listItem>
<page confidence="0.989926">
19
</page>
<table confidence="0.999777666666667">
String Realisations #str. Avg # words
&gt; 100 1206 18.3
&gt; 50, &lt; 100 709 14.3
&gt; 10, &lt; 50 3029 11.8
&gt; 1, &lt; 10 3665 7.6
Total 8609 11.3
</table>
<tableCaption confidence="0.997386">
Table 1: Number of structures and average sen-
</tableCaption>
<bodyText confidence="0.988624942857143">
tence length according to ambiguity classes in
the training set
5. If the input string is contained in the set
of output strings, add this sentence and
all of its corresponding c/f-structure pairs
to training set. The pair(s) that corre-
spond(s) to the original corpus sentence
is/are marked as the intended structure(s),
while all others are marked as unintended.
Theoretically all strings that can be parsed
should be generated by the system, but for rea-
sons of efficiency, punctuation is often not gen-
erated in all possible positions, therefore result-
ing in an input string not being contained in
the set of output strings. Whenever this is the
case for a given sentence, the c/f-structure pairs
associated with it cannot be used for training.
Evaluation can be carried out regardless of this
problem, but it has to be kept in mind that the
original corpus string cannot be generated for
all input f-structures. In our test set, it is gen-
erated only for 62% of them.
Tables 1 and 2 give information about the
ambiguity of the training and test data. For
example, in the training data there are 1,206
structures with more than 100 string realisa-
tions. Most of the training and test structures
have between 2 and 50 possible (and grammat-
ical) string realisations. The average sentence
length of the training data is 11.3 and it is 12.8
for the test data.1 The tables also show that
the structures with more potential string reali-
sations correspond to longer sentences than the
structures that are less ambiguous when gener-
ating.
</bodyText>
<figureCaption confidence="0.861629142857143">
&apos;This is lower than the overall average sentence length
of roughly 16 in TIGER because of the restriction that
the structure produced by the reversible grammar for
any TIGER sentence be compatible with the original
TIGER graph. As the grammar develops further, we
hope that longer sentences can be included in both train-
ing and test data.
</figureCaption>
<table confidence="0.999790666666667">
String Realisations #str. Avg # words
&gt; 100 61 23.7
&gt; 50, &lt; 100 26 13.5
&gt;10, &lt; 50 120 11.6
&gt; 1, &lt; 10 129 7.8
Total 336 12.8
</table>
<tableCaption confidence="0.986668">
Table 2: Number of structures and average sen-
</tableCaption>
<bodyText confidence="0.5180595">
tence length according to ambiguity classes in
the test set
</bodyText>
<subsectionHeader confidence="0.853073">
3.2 Features for realisation ranking
</subsectionHeader>
<bodyText confidence="0.999975576923077">
Using the feature templates presented in Riezler
et al. (2002), Riezler and Vasserman (2004) and
Forst (2007), we construct a list of 186,731 fea-
tures that can be used for training our log-linear
model. Out of these, only 1,471 actually occur
in our training data. In the feature selection
process of our training regime (see Subsection
3.3), 360 features are chosen as the most dis-
criminating; these are used to rank alternative
solutions when the model is applied. The fea-
tures include c-structure features, features that
take both c- and f-structure information into
account, sentence length and language model
scores. Examples of c-structure features are
the number of times a particular category la-
bel occurs in a given c-structure, the number
of children the nodes of a particular category
have or the number of times one particular cat-
egory label dominates another. Examples of
features that take both c- and f-structure in-
formation into account are the relative order of
functions (e.g. ‘SuBj precedes OBj’). As in
Velldal and Oepen (2005), we incorporate the
language model score associated with the string
realisation for a particular structure as a feature
in our model.
</bodyText>
<subsectionHeader confidence="0.993585">
3.3 Training
</subsectionHeader>
<bodyText confidence="0.999928833333333">
We train a log-linear model that maximises the
conditional probability of the observed corpus
sentence given the corresponding f-structure.
The model is trained in a (semi-)supervised
fashion on the 8,609 (partially) labelled struc-
tures of our training set using the cometc soft-
ware provided with the XLE platform. cometc
performs maximum likelihood estimation on
standardised feature values and offers several
regularisation and/or feature selection tech-
niques. We apply the combined method of in-
cremental feature selection and l1 regularisation
</bodyText>
<page confidence="0.990528">
20
</page>
<table confidence="0.998187666666667">
Exact Match Upper Bound 62%
Exact Matches 27%
BLEU score 0.7306
</table>
<tableCaption confidence="0.999978">
Table 3: Results with the language model
</tableCaption>
<bodyText confidence="0.998494125">
presented in Riezler and Vasserman (2004), the
corresponding parameters being adjusted on our
heldout set.
For technical reasons, the training was carried
out on unpacked structures. However, we hope
to be able to train and test on packed struc-
tures in the future which will greatly increase
efficiency.
</bodyText>
<sectionHeader confidence="0.999602" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998758">
4.1 Evaluating the system’s output
</subsectionHeader>
<bodyText confidence="0.944290742857143">
We evaluate the most likely string produced
by our system in terms of two metrics: ex-
act match and BLEU score (Papineni et al.,
2002). Exact match measures what percentage
of the most probable strings are exactly identi-
cal to the string from which the input structure
was produced. BLEU score is a more relaxed
metric which measures the similarity between
the selected string realisation and the observed
corpus string.
We first rank the generator output with a lan-
guage model trained on the Huge German Cor-
pus (a collection of 200 million words of news-
paper and other text) using the SRILM toolkit.
The results are given in Table 3, achieving ex-
act match of 27% and BLEU score of 0.7306.
In comparison to the results reported by Velldal
and Oepen (2005) for a similar experiment on
English, these results are markedly lower, pre-
sumably because of the relatively free word or-
der of German.
We then rank the output of the generator
with our log-linear model as described above
and give the results in Table 4. There is a no-
ticeable improvement in quality. Exact match
increases from 27% to 37%, which corresponds
to an error reduction of 29%,2 and BLEU score
increases from 0.7306 to 0.7939.
There is very little comparable work on re-
alization ranking for German. Gamon et al.
(2002) present work on learning the contexts
2Remember that the original corpus string is gener-
ated only from 62% of the f-structures of our test set,
which fixes the upper bound for exact match at 62%
rather than 100%.
</bodyText>
<table confidence="0.997820666666667">
Exact Match Upper Bound 62%
Exact Matches 37%
BLEU score 0.7939
</table>
<tableCaption confidence="0.999951">
Table 4: Results with the log-linear model
</tableCaption>
<bodyText confidence="0.997547666666667">
for a particular subset of linguistic operations;
however, no evaluation of the overall system is
given.
</bodyText>
<subsectionHeader confidence="0.993936">
4.2 Evaluating the ranker
</subsectionHeader>
<bodyText confidence="0.999994714285714">
In addition to the exact match and BLEU score
metrics, which give us an indication of the qual-
ity of the strings being chosen by the statistical
ranking system, the quality of the ranking it-
self is interesting for internal evaluation during
development. While exact match tells us how
often the correct string is selected, in all other
cases, it gives the developers no indication of
whether the correct string is close to being se-
lected or not. We propose to evaluate the ranker
by calculating the following ranking score:
where n is the total number of potential string
realisations and r is the rank of the gold stan-
dard string. If the gold standard string is not
among the list of potentials, i.e. r is undefined,
the ranking score is defined as 0.
The ranking scores for the language model
and the hybrid log-linear model are given in
Table 5. Since the original string is not nec-
essarily in the set of candidate strings, we also
provide the upper bound (i.e. if the ranker had
chosen the correct string any time the correct
string was available). The table shows that in
almost 62% of the cases, the original string was
generated by the system. The hybrid model
achieves a ranking score of 0.5437 and the lan-
guage model alone achieves 0.4724. The struc-
tural features in the hybrid model result in an
error reduction of 49% over the baseline lan-
guage model ranking score. The hybrid model
is thus noticeably better at ranking the origi-
nal string (when available) higher in the list of
candidate strings. This error reduction is con-
siderably higher than the error reduction of the
much stricter exact match score.
</bodyText>
<equation confidence="0.918071333333333">
fn−r+1 if r is defined,
s = n
0 if r is not defined,
</equation>
<page confidence="0.997828">
21
</page>
<table confidence="0.991551333333333">
Language Model 0.4724
Hybrid Model 0.5437
Upper Bound 0.6190
</table>
<tableCaption confidence="0.97084">
Table 5: Evaluating the ranking
</tableCaption>
<bodyText confidence="0.9998445">
to appear early in the sentence, for example.
Since the system was not provided with data
from which it could learn this generalisation, it
generated output like the following:
</bodyText>
<sectionHeader confidence="0.996128" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.998948448275862">
We had initially expected the increase in BLEU
score to be greater than 0.0633, since German is
far less configurational than English and there-
fore we thought the syntactic features used in
the log-linear model would play an even greater
role in realisation ranking. However, in our ex-
periments, the improvement was only slighlty
greater than the improvement achieved by Vell-
dal and Oepen (2005). In this section, we
present some of the more common errors that
our system still produces.
Word Choice Often there is more than one
surface realisation for a particular group of mor-
phemes. Sometimes the system chooses an in-
correct form for the sentence context, and some-
times it chooses a valid, though marked or dis-
preferred, form. For example, from the struc-
ture in Figure 2, the system chooses the follow-
ing string as the most probable.
Verheugen habe die W¨orter des
Verheugen had the words of the
Generalinspekteures falsch interpretiert.
inspector-general wrongly interpreted.
There are two mis-matches in this output
string with respect to the original corpus string.
In the first case the system has chosen W¨orter
as the surface realisation for the morpheme
sequence Wort+NN.Neut.NGA.Pl rather than
the, in this case, correct form Worte. In the
second (less critical) case, the system has chosen
to mark the genitive case of Generalinspekteur
with es rather than the s that is in the original
corpus. This is a relatively frequent alternation
that is difficult to predict, and there are other
similar alternations in the dative case, for exam-
ple. In the development set, this type of error
occurs in 6 of the 78 sentences. In order to cor-
rect these errors, the morphological component
of the system needs to be improved.
Placement of adjuncts Currently, there is
no feature that captures the (relative) location
of particular types of adjuncts. In German,
there is a strong tendency for temporal adjuncts
Frauen¨arzte haben die Einschr¨ankung
Gynaecologists have the restriction
umstrittener Antibabypillen wegen
controversial birth control pills because of
erh¨ohter Thrombosegefahr am Dienstag
increased risk of thrombosis on Tuesday
kritisiert.
critisised.
‘Gynaecologists criticised the restriction on
controversial birth control pills due to increased
risk of thrombosis on Tuesday.’
where the temporal adjunct on Tuesday was
generated very late in the sentence, resulting
in an awkward utterance. The most obvious so-
lution is to add more features to the model to
capture generalisations about adjunct positions.
Discourse Information In many cases, the
particular subtleties of an utterance can only
be generated using knowledge of the context in
which it occurs. For example, the following sen-
tence appears in our development corpus:
Friedensprozess nach Rabins
peace process after Rabin’s
Frage
question
‘Israel does not challenge the peace process after
Rabin’s death’
Our system generates the string:
Nach Rabins Tod stellt Israel den
After Rabin’s death puts Israel the
Friedensprozess nicht in Frage.
peace process not in question.
which, taken on its own, gets a BLEU score of
0. The sentence produced by our system is a
perfectly valid sentence and captures essentially
the same information as the original corpus
sentence. However, without knowing anything
about the information structure within which
this sentence is uttered, we have no way of
telling where the emphasis of the sentence
is. The work described in this paper is part
of a much larger project, and future research
is already planned to integrate information
structure into the surface realisation process.
</bodyText>
<figure confidence="0.668717">
den
the
Israel
Israel
stellt
puts
in
in
Tod
death
nicht
not
</figure>
<page confidence="0.979716">
22
</page>
<sectionHeader confidence="0.985685" genericHeader="evaluation">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.99999479661017">
In the work of Callaway (2003; 2004), a purely
symbolic system is presented. Our system
makes use of a symbolic grammar, but we ap-
ply a statistical ranking model to the output.
We believe that this gives us the power to
restrict the output of the generator without
under-generating. The symbolic grammar en-
forces hard constraints and the statistical rank-
ing models (possibly conflicting) soft constraints
to weight the alternatives.
Satisfactory methods for the automatic eval-
uation of surface realisation (as well as machine
translation) have not yet been developed (Belz
and Reiter, 2006). The shortcomings of current
methods, particularly BLEU score, seem to be
even more pronounced for German and other
relatively free word-order languages. Given that
all of the sentences generated by our system
are syntactically valid, one would expect much
higher results. A human inspection of the re-
sults and investigation of other evaluation met-
rics such as NIST will be carried out to get a
clearer idea of the quality of the ranking.
There is a potential bias in the learning algo-
rithm, in that the average length of the training
data sentences is lower than the average length
of the test data. In particular, the length of the
sentences with more than 100 potential string
realisations seems to be considerably longer (cf.
Tables 1 and 2). Therefore, it is possible that
the system has not learnt enough features from
longer sentences to be able to correctly rank the
intended string. We plan to increase the size of
the training set to also include longer sentences
that would hopefully avoid this bias. Moreover,
more data will allow us to investigate in detail
the effect of more or less training data on the
results of a stastical realisation ranking system
trained on them.
Finally, more feature design is clearly neces-
sary for the improvement of the system. We
already have several features in mind, which
have not been implemented for technical rea-
sons. Examples are the distance between a
relative clause and its antecedent as well as
its weight, which will hopefully allow us to
learn which types of relative clauses tend to
appear in extraposed position. Another thing
that features for realization ranking should cap-
ture is the information-structural status of con-
stituents. Information structure is an impor-
tant factor when generating the correct surface
realisation (Kruijff et al., 2001). German is a
language in which word order is largely driven
by information structure rather than grammat-
ical function, which is often marked morpholog-
ically. In future work, we plan to integrate in-
formation structure features into the log-linear
model and hope that results will improve.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999982666666667">
In this paper, we have presented a log-linear
realisation ranking system for a German LFG
system. The reversibility of the grammar en-
sures that all output strings will be grammati-
cal. The task then becomes to choose the most
likely one. We train on over 8,600 partially la-
belled structures and test on 323 sentences of
length &gt;3. To our knowledge, this is the largest
statistical realisation experiment carried out for
German. The number of structures used is also
much greater than the data used in Velldal and
Oepen (2005), although the improvement over a
baseline language model was only slightly bet-
ter. We achieved an increase in exact match
score from 27% to 37% and an increase in BLEU
score from 0.7306 to 0.7939. The fact that our
scores are lower than that of Velldal and Oepen
(2005) suggests that it may be more difficult to
achieve high scores for German data, although
this is not necessarily a reflection of the qual-
ity of the strings chosen. We also show, using
a ranking score, that the log-linear ranking sys-
tem generally ranks the correct solution consid-
erably higher than our baseline system.
</bodyText>
<sectionHeader confidence="0.997282" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999822">
The work described in this paper has been car-
ried out as part of the COINS project of the
linguistic Collaborative Research Centre (SFB
732) at the University of Stuttgart, which is
funded by the German Research Foundation
(DFG). Furthermore, we would like to thank
John Maxwell of the Palo Alto Research Center
for being so responsive to our requests for ex-
tensions of the XLE generator functionalities,
some of which were crucial for our work.
</bodyText>
<sectionHeader confidence="0.996835" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.5270515">
Srinivas Bangalore and Owen Rambow. 2000.
Exploiting a probabilistic hierarchical model
</bodyText>
<page confidence="0.985451">
23
</page>
<reference confidence="0.999728252525252">
for generation. In Proceedings of COLING
2000, pages 42–48, Saarbr¨ucken, Germany.
Anja Belz and Ehud Reiter. 2006. Comparing
automatic and human evaluation of NLG sys-
tems. In Proceedings of EACL 2006, pages
313–320, Trento, Italy.
Anja Belz. 2005. Statistical generation: Three
methods compared and evaluated. In Pro-
ceedings of ENLG 2005, pages 15–23, Ab-
erdeen, Scotland.
Sabine Brants, Stefanie Dipper, Silvia Hansen,
Wolfgang Lezius, and George Smith. 2002.
The TIGER Treebank. In Proceedings of the
Workshop on Treebanks and Linguistic The-
ories, Sozopol, Bulgaria.
Aoife Cahill and Josef van Genabith. 2006.
Robust PCFG-Based Generation using Au-
tomatically Acquired LFG Approximations.
In Proceedings of COLING/ACL 2006, pages
1033–1040, Sydney, Australia.
Charles B. Callaway. 2003. Evaluating coverage
for large symbolic NLG grammars. In Pro-
ceedings of IJCAI 2003, pages 811–817, Aca-
pulco, Mexico.
Charles B. Callaway. 2004. Wide coverage sym-
bolic surface realization. In Proceedings of
ACL 2004, pages 125–128, Barcelona, Spain.
Dick Crouch, Mary Dalrymple, Ron Kaplan,
Tracy King, John Maxwell, and Paula New-
man. 2006. XLE documentation. Technical
report, Palo Alto Research Center, CA.
Stefanie Dipper. 2003. Implementing and
Documenting Large-scale Grammars – Ger-
man LFG. Ph.D. thesis, IMS, University of
Stuttgart.
Martin Forst. 2007. Disambiguation for a Lin-
guistically Precise German Parser. Ph.D.
thesis, University of Stuttgart.
Anette Frank, Tracy Holloway King, Jonas
Kuhn, and John T. Maxwell. 2001. Opti-
mality Theory Style Constraint Ranking in
Large-Scale LFG Grammars. In Peter Sells,
editor, Formal and Empirical Issues in Opti-
mality Theoretic Syntax, pages 367–397. CSLI
Publications, Stanford, CA.
Michael Gamon, Eric Ringger, Simon Corston-
Oliver, and Robert Moore. 2002. Machine-
learned contexts for linguistic operations in
German sentence realization. In Proceedings
of ACL 2002, pages 25–32, Philadelphia, PA.
Geert-Jan M. Kruijff, Ivana Kruijff-Korbayova,
John Bateman, and Elke Teich. 2001. Lin-
ear Order as Higher-Level Decision: Informa-
tion Structure in Strategic and Tactical Gen-
eration. In Proceedings of ENLG 2001, pages
74–83, Toulouse, France.
Irene Langkilde-Geary. 2002. An Empirical
Verification of Coverage and Correctness for
a General-Purpose Sentence Generator. In
Proceedings of INLG 2002, NY.
Irene Langkilde. 2000. Forest-based statisti-
cal sentence generation. In Proceedings of
NAACL 2000, pages 170–177, Seattle, WA.
Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi
Tsujii. 2005. Probabilistic models for disam-
biguation of an HPSG-based chart generator.
In Proceedings of IWPT 2005.
Tomoko Ohkuma. 2004. Japanese generation
grammar. Presentation at the ParGram fall
meeting, Dublin, Ireland.
Kishore Papineni, Salim Roukos, Todd Ward,
and WeiJing Zhu. 2002. BLEU: a Method
for Automatic Evaluation of Machine Trans-
lation. In Proceedings of ACL 2002, pages
311–318, Philadelphia, PA.
Stefan Riezler and Alexander Vasserman. 2004.
Gradient feature testing and li regularization
for maximum entropy parsing. In Proceedings
of EMNLP’04, Barcelona, Spain.
Stefan Riezler, Tracy Holloway King, Ronald M.
Kaplan, Richard Crouch, John T. Maxwell,
and Mark Johnson. 2002. Parsing the Wall
Street Journal using a Lexical-Functional
Grammar and Discriminative Estimation
Techniques. In Proceedings of ACL 2002,
Philadelphia, PA.
Christian Rohrer and Martin Forst. 2006. Im-
proving coverage and parsing quality of a
large-scale LFG for German. In Proceedings
of LREC-2006, Genoa, Italy.
Erik Velldal and Stephan Oepen. 2005. Maxi-
mum entropy models for realization ranking.
In Proceedings of the 10th MT Summit, pages
109–116, Thailand.
Erik Velldal, Stephan Oepen, and Dan
Flickinger. 2004. Paraphrasing treebanks for
stochastic realization ranking. In Proceedings
of TLT Workshop, pages 149–160, T¨ubingen,
Germany.
</reference>
<page confidence="0.999176">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.788079">
<title confidence="0.999659">Stochastic Realisation Ranking for a Free Word Order Language</title>
<author confidence="0.983676">Aoife Cahill</author>
<author confidence="0.983676">Martin Forst</author>
<author confidence="0.983676">Christian</author>
<affiliation confidence="0.999724">Institute of Natural Language University of Stuttgart,</affiliation>
<abstract confidence="0.991021608695652">We present a log-linear model that is used for ranking the string realisations produced for given corpus f-structures by a reversible broadcoverage LFG for German and compare its results with the ones achieved by the application of a language model (LM). Like other authors that have developed log-linear models for realisation ranking, we use a hybrid model that uses motivated learning features LM (whose score is simply integrated into the log-linear model as an additional feature) for the task of realisation ranking. We carry out a large evaluation of the model, training on over 8,600 structures and testing on 323. We observe that the contribution that the structural features make to the quality of the output is slightly greater in the case of a free word order language like German than it is in the case of English. The exact match metric improves from 27% to 37% when going from the LM-based realisation ranking to the hybrid model, BLEU score improves from 0.7306 to 0.7939.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>42--48</pages>
<location>Saarbr¨ucken, Germany.</location>
<marker>2000</marker>
<rawString>for generation. In Proceedings of COLING 2000, pages 42–48, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>313--320</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="24053" citStr="Belz and Reiter, 2006" startWordPosition="3867" endWordPosition="3870">22 6 Discussion and future work In the work of Callaway (2003; 2004), a purely symbolic system is presented. Our system makes use of a symbolic grammar, but we apply a statistical ranking model to the output. We believe that this gives us the power to restrict the output of the generator without under-generating. The symbolic grammar enforces hard constraints and the statistical ranking models (possibly conflicting) soft constraints to weight the alternatives. Satisfactory methods for the automatic evaluation of surface realisation (as well as machine translation) have not yet been developed (Belz and Reiter, 2006). The shortcomings of current methods, particularly BLEU score, seem to be even more pronounced for German and other relatively free word-order languages. Given that all of the sentences generated by our system are syntactically valid, one would expect much higher results. A human inspection of the results and investigation of other evaluation metrics such as NIST will be carried out to get a clearer idea of the quality of the ranking. There is a potential bias in the learning algorithm, in that the average length of the training data sentences is lower than the average length of the test data</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings of EACL 2006, pages 313–320, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Statistical generation: Three methods compared and evaluated.</title>
<date>2005</date>
<booktitle>In Proceedings of ENLG 2005,</booktitle>
<pages>15--23</pages>
<location>Aberdeen, Scotland.</location>
<contexts>
<context position="1731" citStr="Belz (2005)" startWordPosition="267" endWordPosition="268">rom 27% to 37% when going from the LM-based realisation ranking to the hybrid model, BLEU score improves from 0.7306 to 0.7939. 1 Introduction Most traditional approaches to stochastic realisation ranking involve applying language model n-gram statistics to rank alternatives (Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002). Much work has been carried out into statistical realisation ranking for English. However, n-grams alone (even if they are efficiently implemented) may not be a good enough measure for ranking candidate strings, particularly in free-word order languages. Belz (2005) moves away from n-gram models of generation and trains a generator on a generation treebank, achieving similar results to a bigram model but at much lower computational cost. Cahill and van Genabith (2006) do not use a language model, but rather rely on treebank-based automatically derived LFG generation grammars to determine the most likely surface order. Ohkuma (2004) writes an LFG generation grammar for Japanese separate from the Japanese LFG parsing grammar in order to enforce canonical word order by symbolic means. In another purely symbolic approach, Callaway (2003; 2004) describes a wi</context>
</contexts>
<marker>Belz, 2005</marker>
<rawString>Anja Belz. 2005. Statistical generation: Three methods compared and evaluated. In Proceedings of ENLG 2005, pages 15–23, Aberdeen, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<date>2002</date>
<contexts>
<context position="10070" citStr="Brants et al., 2002" startWordPosition="1541" endWordPosition="1544">isation, QA, MT etc. Very regular preferences for certain realisation alternatives over others can be implemented by means of so-called optimality marks (Frank et al., 2001), which are implemented in XLE both for the parsing and the generation direction. For ranking string realisations on the basis of ‘soft’ and potentially contradictory constraints, however, the stochastic approach based on a log-linear model, as it has previously been implemented for English HPSGs (Nakanishi et al., 2005; Velldal and Oepen, 2005), seems more adequate. 3 Experimental setup 3.1 Data We use the TIGER Treebank (Brants et al., 2002) to train and test our model. It consists of just over 50,000 annotated sentences of German newspaper text. The sentences have been annotated with morphological and syntactic information in the form of functionally labelled graphs that may contain crossing and secondary edges. We split the data into training and test data using the same data split as in Forst (2007), i.e. sentences 8,001–10,000 of the TIGER Treebank are reserved for evaluation. Within this section, we have 422 TIGER annotationcompatible f-structures, which are further divided into 86 development and 336 test structures. We use</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002.</rawString>
</citation>
<citation valid="false">
<title>The TIGER Treebank.</title>
<booktitle>In Proceedings of the Workshop on Treebanks and Linguistic Theories,</booktitle>
<location>Sozopol, Bulgaria.</location>
<marker></marker>
<rawString>The TIGER Treebank. In Proceedings of the Workshop on Treebanks and Linguistic Theories, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Josef van Genabith</author>
</authors>
<title>Robust PCFG-Based Generation using Automatically Acquired LFG Approximations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>1033--1040</pages>
<location>Sydney, Australia.</location>
<marker>Cahill, van Genabith, 2006</marker>
<rawString>Aoife Cahill and Josef van Genabith. 2006. Robust PCFG-Based Generation using Automatically Acquired LFG Approximations. In Proceedings of COLING/ACL 2006, pages 1033–1040, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
</authors>
<title>Evaluating coverage for large symbolic NLG grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI 2003,</booktitle>
<pages>811--817</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="2309" citStr="Callaway (2003" startWordPosition="361" endWordPosition="362">ee-word order languages. Belz (2005) moves away from n-gram models of generation and trains a generator on a generation treebank, achieving similar results to a bigram model but at much lower computational cost. Cahill and van Genabith (2006) do not use a language model, but rather rely on treebank-based automatically derived LFG generation grammars to determine the most likely surface order. Ohkuma (2004) writes an LFG generation grammar for Japanese separate from the Japanese LFG parsing grammar in order to enforce canonical word order by symbolic means. In another purely symbolic approach, Callaway (2003; 2004) describes a wide-coverage system and the author argues that there are several advantages to a symbolic system over a statistical one. We argue that a reversible symbolic system, which is desirable for maintainability and modularity reasons, augmented with a statistical ranking component can produce systematically ranked, high quality surface realisations while maintaining the flexibility associated with hand-crafted systems. Velldal et al. (2004) and Velldal and Oepen (2005) present discriminative disambiguation models using a hand-crafted HPSG grammar for generation from MRS (Minimal </context>
<context position="23492" citStr="Callaway (2003" startWordPosition="3781" endWordPosition="3782">he sentence produced by our system is a perfectly valid sentence and captures essentially the same information as the original corpus sentence. However, without knowing anything about the information structure within which this sentence is uttered, we have no way of telling where the emphasis of the sentence is. The work described in this paper is part of a much larger project, and future research is already planned to integrate information structure into the surface realisation process. den the Israel Israel stellt puts in in Tod death nicht not 22 6 Discussion and future work In the work of Callaway (2003; 2004), a purely symbolic system is presented. Our system makes use of a symbolic grammar, but we apply a statistical ranking model to the output. We believe that this gives us the power to restrict the output of the generator without under-generating. The symbolic grammar enforces hard constraints and the statistical ranking models (possibly conflicting) soft constraints to weight the alternatives. Satisfactory methods for the automatic evaluation of surface realisation (as well as machine translation) have not yet been developed (Belz and Reiter, 2006). The shortcomings of current methods, </context>
</contexts>
<marker>Callaway, 2003</marker>
<rawString>Charles B. Callaway. 2003. Evaluating coverage for large symbolic NLG grammars. In Proceedings of IJCAI 2003, pages 811–817, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles B Callaway</author>
</authors>
<title>Wide coverage symbolic surface realization.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004,</booktitle>
<pages>125--128</pages>
<location>Barcelona,</location>
<marker>Callaway, 2004</marker>
<rawString>Charles B. Callaway. 2004. Wide coverage symbolic surface realization. In Proceedings of ACL 2004, pages 125–128, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dick Crouch</author>
<author>Mary Dalrymple</author>
<author>Ron Kaplan</author>
<author>Tracy King</author>
<author>John Maxwell</author>
<author>Paula Newman</author>
</authors>
<title>XLE documentation.</title>
<date>2006</date>
<tech>Technical report,</tech>
<location>Palo Alto Research Center, CA.</location>
<contexts>
<context position="7565" citStr="Crouch et al., 2006" startWordPosition="1143" endWordPosition="1146">verage, the robustness techniques described per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar, per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar, ‘skimming’) are employed for the construction a hand-crafted grammar developed in and for ‘skimming’) are employed for the construction a hand-crafted grammar developed in and for the LFG grammar development and processing of partial analyses. The grammar is reversible, the LFG grammar development and processing of partial analyses. The grammar is reversible, platform XLE (Crouch et al., 2006). It achieves platform XLE (Crouch et al., 2006). It achieves which means that the XLE generator can prowhich means that the XLE generator can pro18 duce surface realisations for well-formed input f-structures. Recently, the grammar has been complemented with a stochastic disambiguation module along the lines of Riezler et al. (2002), consisting of a log-linear model based on structural features (Forst, 2007). This module makes it possible to determine one c-/f-structure pair as the most probable analysis of any given sentence. 2.3 Surface realisation As XLE comes with a fully-fledged generato</context>
</contexts>
<marker>Crouch, Dalrymple, Kaplan, King, Maxwell, Newman, 2006</marker>
<rawString>Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King, John Maxwell, and Paula Newman. 2006. XLE documentation. Technical report, Palo Alto Research Center, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefanie Dipper</author>
</authors>
<title>Implementing and Documenting Large-scale Grammars – German LFG.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>IMS, University of Stuttgart.</institution>
<marker>Dipper, 2003</marker>
<rawString>Stefanie Dipper. 2003. Implementing and Documenting Large-scale Grammars – German LFG. Ph.D. thesis, IMS, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Forst</author>
</authors>
<title>Disambiguation for a Linguistically Precise German Parser.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="7977" citStr="Forst, 2007" startWordPosition="1212" endWordPosition="1213">development and processing of partial analyses. The grammar is reversible, the LFG grammar development and processing of partial analyses. The grammar is reversible, platform XLE (Crouch et al., 2006). It achieves platform XLE (Crouch et al., 2006). It achieves which means that the XLE generator can prowhich means that the XLE generator can pro18 duce surface realisations for well-formed input f-structures. Recently, the grammar has been complemented with a stochastic disambiguation module along the lines of Riezler et al. (2002), consisting of a log-linear model based on structural features (Forst, 2007). This module makes it possible to determine one c-/f-structure pair as the most probable analysis of any given sentence. 2.3 Surface realisation As XLE comes with a fully-fledged generator, the grammar can be used both for parsing and for surface realisation. Figure 3 shows an excerpt of the set of strings (and their LM ranking) that are generated from the f-structure in Figure 2. Note that all of these (as well as the the remaining 139 strings in the set) are grammatical; however, some of them are clearly more likely or unmarked than others. 1. Falsch interpretiert habe die Worte Wrongly int</context>
<context position="10438" citStr="Forst (2007)" startWordPosition="1605" endWordPosition="1606"> approach based on a log-linear model, as it has previously been implemented for English HPSGs (Nakanishi et al., 2005; Velldal and Oepen, 2005), seems more adequate. 3 Experimental setup 3.1 Data We use the TIGER Treebank (Brants et al., 2002) to train and test our model. It consists of just over 50,000 annotated sentences of German newspaper text. The sentences have been annotated with morphological and syntactic information in the form of functionally labelled graphs that may contain crossing and secondary edges. We split the data into training and test data using the same data split as in Forst (2007), i.e. sentences 8,001–10,000 of the TIGER Treebank are reserved for evaluation. Within this section, we have 422 TIGER annotationcompatible f-structures, which are further divided into 86 development and 336 test structures. We use the development set to tune the parameters of the log-linear model. Of the 86 heldout sentences and the 336 test sentences, 78 and 323 respectively are of length &gt;3 and are actually used for our final evaluation. For training, we build a symmetric treebank of 8,609 packed c/f-structure representations in a similar manner to Velldal et al. (2004). We do not include </context>
<context position="13990" citStr="Forst (2007)" startWordPosition="2214" endWordPosition="2215">ion that the structure produced by the reversible grammar for any TIGER sentence be compatible with the original TIGER graph. As the grammar develops further, we hope that longer sentences can be included in both training and test data. String Realisations #str. Avg # words &gt; 100 61 23.7 &gt; 50, &lt; 100 26 13.5 &gt;10, &lt; 50 120 11.6 &gt; 1, &lt; 10 129 7.8 Total 336 12.8 Table 2: Number of structures and average sentence length according to ambiguity classes in the test set 3.2 Features for realisation ranking Using the feature templates presented in Riezler et al. (2002), Riezler and Vasserman (2004) and Forst (2007), we construct a list of 186,731 features that can be used for training our log-linear model. Out of these, only 1,471 actually occur in our training data. In the feature selection process of our training regime (see Subsection 3.3), 360 features are chosen as the most discriminating; these are used to rank alternative solutions when the model is applied. The features include c-structure features, features that take both c- and f-structure information into account, sentence length and language model scores. Examples of c-structure features are the number of times a particular category label oc</context>
</contexts>
<marker>Forst, 2007</marker>
<rawString>Martin Forst. 2007. Disambiguation for a Linguistically Precise German Parser. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Tracy Holloway King</author>
<author>Jonas Kuhn</author>
<author>John T Maxwell</author>
</authors>
<title>Optimality Theory Style Constraint Ranking in Large-Scale LFG Grammars.</title>
<date>2001</date>
<booktitle>Formal and Empirical Issues in Optimality Theoretic Syntax,</booktitle>
<pages>367--397</pages>
<editor>In Peter Sells, editor,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="9623" citStr="Frank et al., 2001" startWordPosition="1471" endWordPosition="1474">ngs generated from the f-structure in Figure 2, ordered according to their LM score Just as hand-crafted grammars, when used for parsing, are only useful for most applications when they have been complemented with a disambiguation module, their usefulness as a means of surface realisation depends on a reliable module for realisation ranking. A long list of arbitrarily ordered output strings is useless for practical applications such as summarisation, QA, MT etc. Very regular preferences for certain realisation alternatives over others can be implemented by means of so-called optimality marks (Frank et al., 2001), which are implemented in XLE both for the parsing and the generation direction. For ranking string realisations on the basis of ‘soft’ and potentially contradictory constraints, however, the stochastic approach based on a log-linear model, as it has previously been implemented for English HPSGs (Nakanishi et al., 2005; Velldal and Oepen, 2005), seems more adequate. 3 Experimental setup 3.1 Data We use the TIGER Treebank (Brants et al., 2002) to train and test our model. It consists of just over 50,000 annotated sentences of German newspaper text. The sentences have been annotated with morpho</context>
</contexts>
<marker>Frank, King, Kuhn, Maxwell, 2001</marker>
<rawString>Anette Frank, Tracy Holloway King, Jonas Kuhn, and John T. Maxwell. 2001. Optimality Theory Style Constraint Ranking in Large-Scale LFG Grammars. In Peter Sells, editor, Formal and Empirical Issues in Optimality Theoretic Syntax, pages 367–397. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Eric Ringger</author>
<author>Simon CorstonOliver</author>
<author>Robert Moore</author>
</authors>
<title>Machinelearned contexts for linguistic operations in German sentence realization.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>25--32</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="17365" citStr="Gamon et al. (2002)" startWordPosition="2767" endWordPosition="2770">and BLEU score of 0.7306. In comparison to the results reported by Velldal and Oepen (2005) for a similar experiment on English, these results are markedly lower, presumably because of the relatively free word order of German. We then rank the output of the generator with our log-linear model as described above and give the results in Table 4. There is a noticeable improvement in quality. Exact match increases from 27% to 37%, which corresponds to an error reduction of 29%,2 and BLEU score increases from 0.7306 to 0.7939. There is very little comparable work on realization ranking for German. Gamon et al. (2002) present work on learning the contexts 2Remember that the original corpus string is generated only from 62% of the f-structures of our test set, which fixes the upper bound for exact match at 62% rather than 100%. Exact Match Upper Bound 62% Exact Matches 37% BLEU score 0.7939 Table 4: Results with the log-linear model for a particular subset of linguistic operations; however, no evaluation of the overall system is given. 4.2 Evaluating the ranker In addition to the exact match and BLEU score metrics, which give us an indication of the quality of the strings being chosen by the statistical ran</context>
</contexts>
<marker>Gamon, Ringger, CorstonOliver, Moore, 2002</marker>
<rawString>Michael Gamon, Eric Ringger, Simon CorstonOliver, and Robert Moore. 2002. Machinelearned contexts for linguistic operations in German sentence realization. In Proceedings of ACL 2002, pages 25–32, Philadelphia, PA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Geert-Jan M Kruijff</author>
</authors>
<location>Ivana Kruijff-Korbayova,</location>
<marker>Kruijff, </marker>
<rawString>Geert-Jan M. Kruijff, Ivana Kruijff-Korbayova,</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
<author>Elke Teich</author>
</authors>
<title>Linear Order as Higher-Level Decision: Information Structure in Strategic and Tactical Generation.</title>
<date>2001</date>
<booktitle>In Proceedings of ENLG</booktitle>
<pages>74--83</pages>
<location>Toulouse, France.</location>
<marker>Bateman, Teich, 2001</marker>
<rawString>John Bateman, and Elke Teich. 2001. Linear Order as Higher-Level Decision: Information Structure in Strategic and Tactical Generation. In Proceedings of ENLG 2001, pages 74–83, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator.</title>
<date>2002</date>
<booktitle>In Proceedings of INLG</booktitle>
<pages>NY.</pages>
<contexts>
<context position="1464" citStr="Langkilde-Geary, 2002" startWordPosition="226" endWordPosition="227">er 8,600 structures and testing on 323. We observe that the contribution that the structural features make to the quality of the output is slightly greater in the case of a free word order language like German than it is in the case of English. The exact match metric improves from 27% to 37% when going from the LM-based realisation ranking to the hybrid model, BLEU score improves from 0.7306 to 0.7939. 1 Introduction Most traditional approaches to stochastic realisation ranking involve applying language model n-gram statistics to rank alternatives (Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002). Much work has been carried out into statistical realisation ranking for English. However, n-grams alone (even if they are efficiently implemented) may not be a good enough measure for ranking candidate strings, particularly in free-word order languages. Belz (2005) moves away from n-gram models of generation and trains a generator on a generation treebank, achieving similar results to a bigram model but at much lower computational cost. Cahill and van Genabith (2006) do not use a language model, but rather rely on treebank-based automatically derived LFG generation grammars to determine the </context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator. In Proceedings of INLG 2002, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>Forest-based statistical sentence generation.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>170--177</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="1412" citStr="Langkilde, 2000" startWordPosition="219" endWordPosition="221">large evaluation of the model, training on over 8,600 structures and testing on 323. We observe that the contribution that the structural features make to the quality of the output is slightly greater in the case of a free word order language like German than it is in the case of English. The exact match metric improves from 27% to 37% when going from the LM-based realisation ranking to the hybrid model, BLEU score improves from 0.7306 to 0.7939. 1 Introduction Most traditional approaches to stochastic realisation ranking involve applying language model n-gram statistics to rank alternatives (Langkilde, 2000; Bangalore and Rambow, 2000; Langkilde-Geary, 2002). Much work has been carried out into statistical realisation ranking for English. However, n-grams alone (even if they are efficiently implemented) may not be a good enough measure for ranking candidate strings, particularly in free-word order languages. Belz (2005) moves away from n-gram models of generation and trains a generator on a generation treebank, achieving similar results to a bigram model but at much lower computational cost. Cahill and van Genabith (2006) do not use a language model, but rather rely on treebank-based automatical</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Irene Langkilde. 2000. Forest-based statistical sentence generation. In Proceedings of NAACL 2000, pages 170–177, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT</booktitle>
<contexts>
<context position="3378" citStr="Nakanishi et al. (2005)" startWordPosition="519" endWordPosition="522">t al. (2004) and Velldal and Oepen (2005) present discriminative disambiguation models using a hand-crafted HPSG grammar for generation from MRS (Minimal Recursion Semantics) structures. They describe three statistical models for realization ranking: The first is a simple n-gram language model, the second uses structural features in a maximum entropy model for disambiguation and a third uses a combination of the two models. Their results show that the third model where the n-gram language model is combined with the structural features in the maximum entropy disambiguation model performs best. Nakanishi et al. (2005) present similar probabilistic models for a chart generator using a HPSG grammar acquired from the Penn-II Treebank (the Enju HPSG), with the difference that, in their experiments, the model that only uses structural features outperformed the hybrid model. We 17 present a model for realisation ranking similar to the models just mentioned. The main differences between our work and theirs is that we are working within the LFG framework and concentrating on a less configurational language: German. 2 Background 2.1 Lexical Functional Grammar The work presented in this paper is couched in the frame</context>
<context position="9944" citStr="Nakanishi et al., 2005" startWordPosition="1520" endWordPosition="1523"> for realisation ranking. A long list of arbitrarily ordered output strings is useless for practical applications such as summarisation, QA, MT etc. Very regular preferences for certain realisation alternatives over others can be implemented by means of so-called optimality marks (Frank et al., 2001), which are implemented in XLE both for the parsing and the generation direction. For ranking string realisations on the basis of ‘soft’ and potentially contradictory constraints, however, the stochastic approach based on a log-linear model, as it has previously been implemented for English HPSGs (Nakanishi et al., 2005; Velldal and Oepen, 2005), seems more adequate. 3 Experimental setup 3.1 Data We use the TIGER Treebank (Brants et al., 2002) to train and test our model. It consists of just over 50,000 annotated sentences of German newspaper text. The sentences have been annotated with morphological and syntactic information in the form of functionally labelled graphs that may contain crossing and secondary edges. We split the data into training and test data using the same data split as in Forst (2007), i.e. sentences 8,001–10,000 of the TIGER Treebank are reserved for evaluation. Within this section, we h</context>
</contexts>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Hiroko Nakanishi, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings of IWPT 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoko Ohkuma</author>
</authors>
<title>Japanese generation grammar. Presentation at the ParGram fall meeting,</title>
<date>2004</date>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2104" citStr="Ohkuma (2004)" startWordPosition="328" endWordPosition="329">d out into statistical realisation ranking for English. However, n-grams alone (even if they are efficiently implemented) may not be a good enough measure for ranking candidate strings, particularly in free-word order languages. Belz (2005) moves away from n-gram models of generation and trains a generator on a generation treebank, achieving similar results to a bigram model but at much lower computational cost. Cahill and van Genabith (2006) do not use a language model, but rather rely on treebank-based automatically derived LFG generation grammars to determine the most likely surface order. Ohkuma (2004) writes an LFG generation grammar for Japanese separate from the Japanese LFG parsing grammar in order to enforce canonical word order by symbolic means. In another purely symbolic approach, Callaway (2003; 2004) describes a wide-coverage system and the author argues that there are several advantages to a symbolic system over a statistical one. We argue that a reversible symbolic system, which is desirable for maintainability and modularity reasons, augmented with a statistical ranking component can produce systematically ranked, high quality surface realisations while maintaining the flexibil</context>
</contexts>
<marker>Ohkuma, 2004</marker>
<rawString>Tomoko Ohkuma. 2004. Japanese generation grammar. Presentation at the ParGram fall meeting, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="16216" citStr="Papineni et al., 2002" startWordPosition="2567" endWordPosition="2570">ection and l1 regularisation 20 Exact Match Upper Bound 62% Exact Matches 27% BLEU score 0.7306 Table 3: Results with the language model presented in Riezler and Vasserman (2004), the corresponding parameters being adjusted on our heldout set. For technical reasons, the training was carried out on unpacked structures. However, we hope to be able to train and test on packed structures in the future which will greatly increase efficiency. 4 Evaluation 4.1 Evaluating the system’s output We evaluate the most likely string produced by our system in terms of two metrics: exact match and BLEU score (Papineni et al., 2002). Exact match measures what percentage of the most probable strings are exactly identical to the string from which the input structure was produced. BLEU score is a more relaxed metric which measures the similarity between the selected string realisation and the observed corpus string. We first rank the generator output with a language model trained on the Huge German Corpus (a collection of 200 million words of newspaper and other text) using the SRILM toolkit. The results are given in Table 3, achieving exact match of 27% and BLEU score of 0.7306. In comparison to the results reported by Vel</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL 2002, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
</authors>
<title>Gradient feature testing and li regularization for maximum entropy parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP’04,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="13973" citStr="Riezler and Vasserman (2004)" startWordPosition="2209" endWordPosition="2212"> in TIGER because of the restriction that the structure produced by the reversible grammar for any TIGER sentence be compatible with the original TIGER graph. As the grammar develops further, we hope that longer sentences can be included in both training and test data. String Realisations #str. Avg # words &gt; 100 61 23.7 &gt; 50, &lt; 100 26 13.5 &gt;10, &lt; 50 120 11.6 &gt; 1, &lt; 10 129 7.8 Total 336 12.8 Table 2: Number of structures and average sentence length according to ambiguity classes in the test set 3.2 Features for realisation ranking Using the feature templates presented in Riezler et al. (2002), Riezler and Vasserman (2004) and Forst (2007), we construct a list of 186,731 features that can be used for training our log-linear model. Out of these, only 1,471 actually occur in our training data. In the feature selection process of our training regime (see Subsection 3.3), 360 features are chosen as the most discriminating; these are used to rank alternative solutions when the model is applied. The features include c-structure features, features that take both c- and f-structure information into account, sentence length and language model scores. Examples of c-structure features are the number of times a particular </context>
<context position="15772" citStr="Riezler and Vasserman (2004)" startWordPosition="2493" endWordPosition="2496">ty of the observed corpus sentence given the corresponding f-structure. The model is trained in a (semi-)supervised fashion on the 8,609 (partially) labelled structures of our training set using the cometc software provided with the XLE platform. cometc performs maximum likelihood estimation on standardised feature values and offers several regularisation and/or feature selection techniques. We apply the combined method of incremental feature selection and l1 regularisation 20 Exact Match Upper Bound 62% Exact Matches 27% BLEU score 0.7306 Table 3: Results with the language model presented in Riezler and Vasserman (2004), the corresponding parameters being adjusted on our heldout set. For technical reasons, the training was carried out on unpacked structures. However, we hope to be able to train and test on packed structures in the future which will greatly increase efficiency. 4 Evaluation 4.1 Evaluating the system’s output We evaluate the most likely string produced by our system in terms of two metrics: exact match and BLEU score (Papineni et al., 2002). Exact match measures what percentage of the most probable strings are exactly identical to the string from which the input structure was produced. BLEU sc</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Stefan Riezler and Alexander Vasserman. 2004. Gradient feature testing and li regularization for maximum entropy parsing. In Proceedings of EMNLP’04, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy Holloway King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7059" citStr="Riezler et al. (2002)" startWordPosition="1067" endWordPosition="1070"> NP D[std] 2.2 A broad-coverage LFG for German 2.2 A broad-coverage LFG for German parsing coverage of about 80% in terms of full parsing coverage of about 80% in terms of full For the construction of our data, we use the Gerparses on newspaper text, and for sentences out For the construction of our data, we use the Ger- parses on newspaper text, and for sentences out man broad-coverage LFG documented in Dipof coverage, the robustness techniques described man broad-coverage LFG documented in Dip- of coverage, the robustness techniques described per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar, per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar, ‘skimming’) are employed for the construction a hand-crafted grammar developed in and for ‘skimming’) are employed for the construction a hand-crafted grammar developed in and for the LFG grammar development and processing of partial analyses. The grammar is reversible, the LFG grammar development and processing of partial analyses. The grammar is reversible, platform XLE (Crouch et al., 2006). It achieves platform XLE (Crouch et al., 2006). It achieves which means that the XLE generat</context>
<context position="13943" citStr="Riezler et al. (2002)" startWordPosition="2205" endWordPosition="2208">ce length of roughly 16 in TIGER because of the restriction that the structure produced by the reversible grammar for any TIGER sentence be compatible with the original TIGER graph. As the grammar develops further, we hope that longer sentences can be included in both training and test data. String Realisations #str. Avg # words &gt; 100 61 23.7 &gt; 50, &lt; 100 26 13.5 &gt;10, &lt; 50 120 11.6 &gt; 1, &lt; 10 129 7.8 Total 336 12.8 Table 2: Number of structures and average sentence length according to ambiguity classes in the test set 3.2 Features for realisation ranking Using the feature templates presented in Riezler et al. (2002), Riezler and Vasserman (2004) and Forst (2007), we construct a list of 186,731 features that can be used for training our log-linear model. Out of these, only 1,471 actually occur in our training data. In the feature selection process of our training regime (see Subsection 3.3), 360 features are chosen as the most discriminating; these are used to rank alternative solutions when the model is applied. The features include c-structure features, features that take both c- and f-structure information into account, sentence length and language model scores. Examples of c-structure features are the</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy Holloway King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques. In Proceedings of ACL 2002, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Rohrer</author>
<author>Martin Forst</author>
</authors>
<title>Improving coverage and parsing quality of a large-scale LFG for German.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="7027" citStr="Rohrer and Forst (2006)" startWordPosition="1060" endWordPosition="1063">gen D[std] DPx[std] die N[comm] NP NP D[std] 2.2 A broad-coverage LFG for German 2.2 A broad-coverage LFG for German parsing coverage of about 80% in terms of full parsing coverage of about 80% in terms of full For the construction of our data, we use the Gerparses on newspaper text, and for sentences out For the construction of our data, we use the Ger- parses on newspaper text, and for sentences out man broad-coverage LFG documented in Dipof coverage, the robustness techniques described man broad-coverage LFG documented in Dip- of coverage, the robustness techniques described per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar, per (2003) and Rohrer and Forst (2006). It is in Riezler et al. (2002) (fragment grammar, ‘skimming’) are employed for the construction a hand-crafted grammar developed in and for ‘skimming’) are employed for the construction a hand-crafted grammar developed in and for the LFG grammar development and processing of partial analyses. The grammar is reversible, the LFG grammar development and processing of partial analyses. The grammar is reversible, platform XLE (Crouch et al., 2006). It achieves platform XLE (Crouch et al., 2006). It achieves </context>
</contexts>
<marker>Rohrer, Forst, 2006</marker>
<rawString>Christian Rohrer and Martin Forst. 2006. Improving coverage and parsing quality of a large-scale LFG for German. In Proceedings of LREC-2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
</authors>
<title>Maximum entropy models for realization ranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th MT Summit,</booktitle>
<pages>109--116</pages>
<contexts>
<context position="2796" citStr="Velldal and Oepen (2005)" startWordPosition="431" endWordPosition="434">anese LFG parsing grammar in order to enforce canonical word order by symbolic means. In another purely symbolic approach, Callaway (2003; 2004) describes a wide-coverage system and the author argues that there are several advantages to a symbolic system over a statistical one. We argue that a reversible symbolic system, which is desirable for maintainability and modularity reasons, augmented with a statistical ranking component can produce systematically ranked, high quality surface realisations while maintaining the flexibility associated with hand-crafted systems. Velldal et al. (2004) and Velldal and Oepen (2005) present discriminative disambiguation models using a hand-crafted HPSG grammar for generation from MRS (Minimal Recursion Semantics) structures. They describe three statistical models for realization ranking: The first is a simple n-gram language model, the second uses structural features in a maximum entropy model for disambiguation and a third uses a combination of the two models. Their results show that the third model where the n-gram language model is combined with the structural features in the maximum entropy disambiguation model performs best. Nakanishi et al. (2005) present similar p</context>
<context position="9970" citStr="Velldal and Oepen, 2005" startWordPosition="1524" endWordPosition="1527">. A long list of arbitrarily ordered output strings is useless for practical applications such as summarisation, QA, MT etc. Very regular preferences for certain realisation alternatives over others can be implemented by means of so-called optimality marks (Frank et al., 2001), which are implemented in XLE both for the parsing and the generation direction. For ranking string realisations on the basis of ‘soft’ and potentially contradictory constraints, however, the stochastic approach based on a log-linear model, as it has previously been implemented for English HPSGs (Nakanishi et al., 2005; Velldal and Oepen, 2005), seems more adequate. 3 Experimental setup 3.1 Data We use the TIGER Treebank (Brants et al., 2002) to train and test our model. It consists of just over 50,000 annotated sentences of German newspaper text. The sentences have been annotated with morphological and syntactic information in the form of functionally labelled graphs that may contain crossing and secondary edges. We split the data into training and test data using the same data split as in Forst (2007), i.e. sentences 8,001–10,000 of the TIGER Treebank are reserved for evaluation. Within this section, we have 422 TIGER annotationco</context>
<context position="14928" citStr="Velldal and Oepen (2005)" startWordPosition="2366" endWordPosition="2369">native solutions when the model is applied. The features include c-structure features, features that take both c- and f-structure information into account, sentence length and language model scores. Examples of c-structure features are the number of times a particular category label occurs in a given c-structure, the number of children the nodes of a particular category have or the number of times one particular category label dominates another. Examples of features that take both c- and f-structure information into account are the relative order of functions (e.g. ‘SuBj precedes OBj’). As in Velldal and Oepen (2005), we incorporate the language model score associated with the string realisation for a particular structure as a feature in our model. 3.3 Training We train a log-linear model that maximises the conditional probability of the observed corpus sentence given the corresponding f-structure. The model is trained in a (semi-)supervised fashion on the 8,609 (partially) labelled structures of our training set using the cometc software provided with the XLE platform. cometc performs maximum likelihood estimation on standardised feature values and offers several regularisation and/or feature selection t</context>
<context position="16837" citStr="Velldal and Oepen (2005)" startWordPosition="2675" endWordPosition="2678">02). Exact match measures what percentage of the most probable strings are exactly identical to the string from which the input structure was produced. BLEU score is a more relaxed metric which measures the similarity between the selected string realisation and the observed corpus string. We first rank the generator output with a language model trained on the Huge German Corpus (a collection of 200 million words of newspaper and other text) using the SRILM toolkit. The results are given in Table 3, achieving exact match of 27% and BLEU score of 0.7306. In comparison to the results reported by Velldal and Oepen (2005) for a similar experiment on English, these results are markedly lower, presumably because of the relatively free word order of German. We then rank the output of the generator with our log-linear model as described above and give the results in Table 4. There is a noticeable improvement in quality. Exact match increases from 27% to 37%, which corresponds to an error reduction of 29%,2 and BLEU score increases from 0.7306 to 0.7939. There is very little comparable work on realization ranking for German. Gamon et al. (2002) present work on learning the contexts 2Remember that the original corpu</context>
<context position="20173" citStr="Velldal and Oepen (2005)" startWordPosition="3253" endWordPosition="3257">90 Table 5: Evaluating the ranking to appear early in the sentence, for example. Since the system was not provided with data from which it could learn this generalisation, it generated output like the following: 5 Error Analysis We had initially expected the increase in BLEU score to be greater than 0.0633, since German is far less configurational than English and therefore we thought the syntactic features used in the log-linear model would play an even greater role in realisation ranking. However, in our experiments, the improvement was only slighlty greater than the improvement achieved by Velldal and Oepen (2005). In this section, we present some of the more common errors that our system still produces. Word Choice Often there is more than one surface realisation for a particular group of morphemes. Sometimes the system chooses an incorrect form for the sentence context, and sometimes it chooses a valid, though marked or dispreferred, form. For example, from the structure in Figure 2, the system chooses the following string as the most probable. Verheugen habe die W¨orter des Verheugen had the words of the Generalinspekteures falsch interpretiert. inspector-general wrongly interpreted. There are two m</context>
<context position="26689" citStr="Velldal and Oepen (2005)" startWordPosition="4305" endWordPosition="4308">tion structure features into the log-linear model and hope that results will improve. 7 Conclusions In this paper, we have presented a log-linear realisation ranking system for a German LFG system. The reversibility of the grammar ensures that all output strings will be grammatical. The task then becomes to choose the most likely one. We train on over 8,600 partially labelled structures and test on 323 sentences of length &gt;3. To our knowledge, this is the largest statistical realisation experiment carried out for German. The number of structures used is also much greater than the data used in Velldal and Oepen (2005), although the improvement over a baseline language model was only slightly better. We achieved an increase in exact match score from 27% to 37% and an increase in BLEU score from 0.7306 to 0.7939. The fact that our scores are lower than that of Velldal and Oepen (2005) suggests that it may be more difficult to achieve high scores for German data, although this is not necessarily a reflection of the quality of the strings chosen. We also show, using a ranking score, that the log-linear ranking system generally ranks the correct solution considerably higher than our baseline system. Acknowledge</context>
</contexts>
<marker>Velldal, Oepen, 2005</marker>
<rawString>Erik Velldal and Stephan Oepen. 2005. Maximum entropy models for realization ranking. In Proceedings of the 10th MT Summit, pages 109–116, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Velldal</author>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
</authors>
<title>Paraphrasing treebanks for stochastic realization ranking.</title>
<date>2004</date>
<booktitle>In Proceedings of TLT Workshop,</booktitle>
<pages>149--160</pages>
<location>T¨ubingen, Germany.</location>
<contexts>
<context position="2767" citStr="Velldal et al. (2004)" startWordPosition="426" endWordPosition="429">nese separate from the Japanese LFG parsing grammar in order to enforce canonical word order by symbolic means. In another purely symbolic approach, Callaway (2003; 2004) describes a wide-coverage system and the author argues that there are several advantages to a symbolic system over a statistical one. We argue that a reversible symbolic system, which is desirable for maintainability and modularity reasons, augmented with a statistical ranking component can produce systematically ranked, high quality surface realisations while maintaining the flexibility associated with hand-crafted systems. Velldal et al. (2004) and Velldal and Oepen (2005) present discriminative disambiguation models using a hand-crafted HPSG grammar for generation from MRS (Minimal Recursion Semantics) structures. They describe three statistical models for realization ranking: The first is a simple n-gram language model, the second uses structural features in a maximum entropy model for disambiguation and a third uses a combination of the two models. Their results show that the third model where the n-gram language model is combined with the structural features in the maximum entropy disambiguation model performs best. Nakanishi et</context>
<context position="11018" citStr="Velldal et al. (2004)" startWordPosition="1697" endWordPosition="1700">ng the same data split as in Forst (2007), i.e. sentences 8,001–10,000 of the TIGER Treebank are reserved for evaluation. Within this section, we have 422 TIGER annotationcompatible f-structures, which are further divided into 86 development and 336 test structures. We use the development set to tune the parameters of the log-linear model. Of the 86 heldout sentences and the 336 test sentences, 78 and 323 respectively are of length &gt;3 and are actually used for our final evaluation. For training, we build a symmetric treebank of 8,609 packed c/f-structure representations in a similar manner to Velldal et al. (2004). We do not include structures for which only one string is generated, since the log-linear model for realisation ranking cannot learn anything from them. The symmetric treebank was established using the following strategy: 1. Parse input sentence from TIGER Treebank. 2. Select all of the analyses that are compatible with the TIGER Treebank annotation. 3. Of all the TIGER-compatible analyses, choose the most likely c/f-structure pair according to log-linear model for parse disambiguation. 4. Generate from the f-structure part of this analysis. 19 String Realisations #str. Avg # words &gt; 100 120</context>
</contexts>
<marker>Velldal, Oepen, Flickinger, 2004</marker>
<rawString>Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004. Paraphrasing treebanks for stochastic realization ranking. In Proceedings of TLT Workshop, pages 149–160, T¨ubingen, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>