<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030393">
<title confidence="0.997412">
Identifying hypernyms in distributional semantic spaces
</title>
<author confidence="0.996271">
Alessandro Lenci Giulia Benotto
</author>
<affiliation confidence="0.998988">
University of Pisa, Dept. of Linguistics University of Pisa, Dept. of Linguistics
</affiliation>
<address confidence="0.6526455">
via S. Maria 36 via S. Maria 36
I-56126, Pisa, Italy I-56126, Pisa, Italy
</address>
<email confidence="0.996533">
alessandro.lenci@ling.unipi.it mezzanine.g@gmail.com
</email>
<sectionHeader confidence="0.998649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.988083166666667">
In this paper we apply existing directional
similarity measures to identify hypernyms
with a state-of-the-art distributional semantic
model. We also propose a new directional
measure that achieves the best performance in
hypernym identification.
</bodyText>
<sectionHeader confidence="0.983024" genericHeader="categories and subject descriptors">
1 Introduction and related works
</sectionHeader>
<bodyText confidence="0.999785966101695">
Distributional Semantic Models (DSMs) measure
the semantic similarity between words with proxim-
ity in distributional space. However, semantically
similar words in turn differ for the type of relation
holding between them: e.g., dog is strongly similar
to both animal and cat, but with different types of re-
lations. Current DSMs accounts for these facts only
partially. While they may correctly place both ani-
mal and cat among the nearest distributional neigh-
bors of dog, they are not able to characterize the
different semantic properties of these relations, for
instance the fact that hypernymy is an asymmetric
semantic relation, since being a dog entails being an
animal, but not the other way round.
The purpose of this paper is to explore the possi-
bility of identifying hypernyms in DSMs with direc-
tional (or asymmetric) similarity measures (Kotler-
man et al., 2010). These measures all rely on some
variation of the Distributional Inclusion Hypothe-
sis, according to which if u is a semantically nar-
rower term than v, then a significant number of
salient distributional features of u is included in the
feature vector of v as well. Since hypernymy is
an asymmetric relation and hypernyms are seman-
tically broader terms than their hyponyms, then we
can predict that directional similarity measures are
better suited to identify terms related by the hyper-
nymy relation.
Automatic identification of hypernyms in corpora
is a long-standing research line, but most meth-
ods have adopted semi-supervised, pattern-based ap-
proaches (Hearst, 1992; Pantel and Pennacchiotti,
2006). Fully unsupervised hypernym identification
with DSMs is still a largely open field. Various mod-
els to represent hypernyms in vector spaces have
recently been proposed (Weeds and Weir, 2003;
Weeds et al., 2004; Clarke, 2009), usually grounded
on the Distributional Inclusion Hypothesis (for a dif-
ferent approach based on representing word mean-
ing as “regions” in vector space, see Erk (2009a;
2009b)). The same hypothesis has been adopted by
Kotlerman et al. (2010) to identify (substitutable)
lexical entailments” . Within the context of the Tex-
tual Entailment (TE) paradigm, Zhitomirsky-Geffet
and Dagan (2005; 2009) define (substitutable) lex-
ical entailment as a relation holding between two
words, if there are some contexts in which one of
the words can be substituted by the other and the
meaning of the original word can be inferred from
the new one. Its relevance for TE notwithstanding,
this notion of lexical entailment is more general and
looser than hypernymy. In fact, it encompasses sev-
eral standard semantic relations such as synonymy,
hypernymy, metonymy, some cases of meronymy,
etc.
Differently from Kotlerman et al. (2010), here we
focus on applying directional, asymmetric similar-
ity measures to identify hypernyms. We assume the
classical definition of a hypernymy, such that Y is
</bodyText>
<page confidence="0.991997">
75
</page>
<note confidence="0.530266">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 75–79,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.989828">
an hypernym of X if and only if X is a kind of Y ,
or equivalently every X is a Y .
</bodyText>
<sectionHeader confidence="0.920052" genericHeader="method">
2 Directional similarity measures
</sectionHeader>
<bodyText confidence="0.999519125">
In the experiments reported in section 3 we have ap-
plied the following directional similarity measures
(Fx is the set of distributional features of a term x,
wx(f) is the weight of the feature f for x):
WeedsPrec (M1) - this is a measure that quantifies
the weighted inclusion of the features of a term u
within the features of a term v (Weeds and Weir,
2003; Weeds et al., 2004; Kotlerman et al., 2010):
</bodyText>
<equation confidence="0.9962295">
WeedsPrec(u, v) = Ef EFunFv wu(f) (1)
EfEFu wu(f)
</equation>
<bodyText confidence="0.66326825">
cosWeeds (M2) - this measure corresponds to the
geometrical average of WeedsPrec and the symmet-
ric similarity between u and v, measured by their
vectors’ cosine:
</bodyText>
<equation confidence="0.888391">
�cosWeeds(u, v) = M1(u, v) * cos(u, v) (2)
</equation>
<bodyText confidence="0.99409225">
This is actually a variation of the balPrec measure
in Kotlerman et al. (2010), the difference being that
cosine is used as a symmetric similarity measure
instead of the LIN measure (Lin, 1998).
</bodyText>
<equation confidence="0.92937925">
ClarkeDE (M3) - a close variation of M1,
proposed by Clarke (2009):
ClarkeDE(u, v) = Ef EFunFv min(wu(f ), w„ (f ))
EfEFu wu(f)
</equation>
<bodyText confidence="0.970714294117647">
invCL (M4) - this a new measure that we introduce
and test here for the first time. It takes into account
not only the inclusion of u in v, but also the non-
inclusion of v in u, both measured with ClarkeDE:
�
invCL(u, v) = M3(u, v) * (1 — M3(v, u))
The intuition behind invCL is that, if v is a seman-
tically broader term of u, then the features of u are
included in the features of v, but crucially the fea-
tures of v are also not included in the features of
u. For instance, if animal is a hypernym of lion,
we can expect i.) that a significant number of the
lion-contexts are also animal-contexts, and ii.) that
a significant number of animal-contexts are not lion-
contexts. In fact, being a semantically broader term
of lion, animal should also be found in contexts in
which animals other than lions occur.
</bodyText>
<sectionHeader confidence="0.999359" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.99996748">
The main purpose of the experiments reported below
is to investigate the ability of the directional similar-
ity measures presented in section 2 to identify the
hypernyms of a given target noun, and to discrim-
inate hypernyms from terms related by symmetric
semantic relations, such as coordinate terms.
We have represented lexical items with distribu-
tional feature vectors extracted from the TypeDM
tensor (Baroni and Lenci, 2010). TypeDM is a par-
ticular instantiation of the Distributional Memory
(DM) framework. In DM, distributional facts are
represented as a weighted tuple structure T, a set
of weighted word-link-word tuples ((w1,l, w2), Q),
such that w1 and w2 are content words (e.g. nouns,
verbs, etc.), l is a syntagmatic co-occurrence links
between words in a text (e.g. syntactic dependen-
cies, etc.), and Q is a weight estimating the statis-
tical salience of that tuple. The TypeDM word set
contains 30,693 lemmas (20,410 nouns, 5,026 verbs
and 5,257 adjectives). The TypeDM link set con-
tains 25,336 direct and inverse links formed by (par-
tially lexicalized) syntactic dependencies and pat-
terns. The weight Q is the Local Mutual Informa-
tion (LMI) (Evert, 2005) computed on link type fre-
quency (negative LMI values are raised to 0).
</bodyText>
<subsectionHeader confidence="0.999955">
3.1 Test set
</subsectionHeader>
<bodyText confidence="0.999027181818182">
We have evaluated the directional similarity mea-
sures on a subset of the BLESS data set (Baroni and
Lenci, 2011), consisting of tuples expressing a re-
lation between a target concept (henceforth referred
to as concept) and a relatum concept (henceforth re-
ferred to as relatum). BLESS includes 200 distinct
English concrete nouns as target concepts, equally
divided between living and non-living entities, and
grouped into 17 broader classes (e.g., BIRD, FRUIT,
FURNITURE, VEHICLE, etc.).
For each concept noun, BLESS includes several
</bodyText>
<page confidence="0.926356">
76
</page>
<bodyText confidence="0.99926932">
relatum words, linked to the concept by one of 5 se-
mantic relations. Here, we have used the BLESS
subset formed by 14,547 tuples with the relatum
attested in the TypeDM word set, and containing
one of these relations: COORD: the relatum is a
noun that is a co-hyponym (coordinate) of the con-
cept: (alligator, coord, lizard); HYPER: the rela-
tum is a noun that is a hypernym of the concept:
(alligator, hyper, animal); MERO: the relatum is
a noun referring to a part/component/organ/member
of the concept, or something that the concept con-
tains or is made of: (alligator, mero, mouth);
RANDOM-N: the relatum is a random noun hold-
ing no semantic relation with the target concept:
(alligator, random — n, message).
Kotlerman et al. (2010) evaluate a set of
directional similarity measure on a data set of
valid and invalid (substitutable) lexical entailments
(Zhitomirsky-Geffet and Dagan, 2009). However,
as we said above, lexical entailment is defined as
an asymmetric relation that covers various types of
classic semantic relations, besides hypernymy . The
choice of BLESS is instead motivated by the fact
that here we focus on the ability of directional simi-
larity measure to identify hypernyms.
</bodyText>
<subsectionHeader confidence="0.998208">
3.2 Evaluation and results
</subsectionHeader>
<bodyText confidence="0.999962869565218">
For each word x in the test set, we represented
x in terms of a set Fx of distributional features
(l, w2), such that in the TypeDM tensor there is a
tuple ((w1,l, w2), Q), w1 = x. The feature weight
wx(f) is equal to the weight a of the original DM
tuple. Then, we applied the 4 directional simi-
larity measures in section 2 to BLESS, with the
goal of evaluating their ability to discriminate hy-
pernyms from other semantic relations, in particular
co-hyponymy. In fact, differently from hypernyms,
coordinate terms are not related by inclusion. There-
fore, we want to test whether directional similarity
measures are able to assign higher scores to hyper-
nyms, as predicted by the Distributional Inclusion
Hypothesis. We used the Cosine as our baseline,
since it is a symmetric similarity measure and it is
commonly used in DSMs.
We adopt two different evaluation methods. The
first is based on the methodology described in Ba-
roni and Lenci (2011). Given the similarity scores
for a concept with all its relata across all relations
in our test set, we pick the relatum with the high-
est score (nearest neighbour) for each relation. In
this way, for each of the 200 BLESS concepts, we
obtain 4 similarity scores, one per relation. In or-
der to factor out concept-specific effects that might
add to the overall score variance, we transform the
8 similarity scores of each concept onto standard-
ized z scores (mean: 0; s.d: 1) by subtracting from
each their mean, and dividing by their standard devi-
ation. After this transformation, we produce a box-
plot summarizing the distribution of scores per rela-
tion across the 200 concepts.
Boxplots for each similarity measure are reported
in Figure 1. They display the median of a distribu-
tion as a thick horizontal line within a box extending
from the first to the third quartile, with whiskers cov-
ering 1.5 of the interquartile range in each direction
from the box, and values outside this extended range
– extreme outliers – plotted as circles (these are the
default boxplotting option of the R statistical pack-
age). To identify significant differences between re-
lation types, we also performed pairwise compar-
isons with the Tukey Honestly Significant Differ-
ence test, using the standard ↵ = 0.05 significance
threshold.
In the boxplots we can observe that all measures
(either symmetric or not) are able to discriminate
truly semantically related pairs from unrelated (i.e.
random) ones. Crucially, Cosine shows a strong
tendency to identify coordinates among the near-
est neighbors of target items. This is actually con-
sistent with its being a symmetric similarity mea-
sure. Instead, directional similarity measures signif-
icantly promote hypernyms over coordinates. The
only exception is represented by cosWeeds, which
again places coordinates at the top, though now the
difference with hypernyms is not significant. This
might be due to the cosine component of this mea-
sure, which reduces the effectiveness of the asym-
metric WeedsPrec. The difference between coor-
dinates and hypernyms is slightly bigger in invCL,
and the former appear to be further downgraded than
with the other directional measures. From the box-
plot analysis, we can therefore conclude that simi-
larity measures based on the Distributional Inclusion
Hypothesis do indeed improve hypernym identifica-
tion in context-feature semantic spaces, with respect
to other types of semantic relations, such as COORD.
</bodyText>
<page confidence="0.998567">
77
</page>
<figureCaption confidence="0.9735045">
Figure 1: Distribution of relata similarity scores across concepts (values on ordinate are similarity scores after concept-
by-concept z-normalization).
</figureCaption>
<figure confidence="0.9953038">
Cosine
caartl hyper mero mntlomn
WeedsPrec
_W hyper mero rentlom-n
cosWeeds
wwtl hyper — .n -n
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
ClarkeDE
was hyper mero ranaom-n
InvCL
coord random-n
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
</figure>
<bodyText confidence="0.999913666666667">
The second type of evaluation we have performed
is based on Kotlerman et al. (2010). The similarity
measures have been evaluated with Average Preci-
sion (AP), a method derived from Information Re-
trieval and combining precision, relevance ranking
and overall recall. For each similarity measure, we
computed AP with respect to the 4 BLESS relations.
The best possible score (AP = 1) for a given rela-
tion (e.g., HYPER) corresponds to the ideal case in
which all the relata belonging to that relation have
higher similarity scores than the relata belonging to
the other relations. For every relation, we calculated
the AP for each of the 200 BLESS target concepts.
In Table 1, we report the AP values averaged over
the 200 concepts. On the one hand, these results
confirm the trend illustrated by the boxplots, in par-
ticular the fact that directional similarity measures
clearly outperform Cosine (or cosine-based mea-
sures such as cosWeeds) in identifying hypernyms,
with no significant differences among them. How-
ever, a different picture emerges by comparing the
</bodyText>
<table confidence="0.9976135">
measure COORD HYPER MERO RANDOM-N
Cosine 0.79 0.23 0.21 0.30
WeedsPrec 0.45 0.40 0.31 0.32
cosWeeds 0.69 0.29 0.23 0.30
ClarkeDE 0.45 0.39 0.28 0.33
invCL 0.38 0.40 0.31 0.34
</table>
<tableCaption confidence="0.991539">
Table 1: Mean AP values for each semantic relation re-
ported by the different similarity scores.
</tableCaption>
<bodyText confidence="0.998518909090909">
AP values for HYPER with those for COORD. since
in this case important distinctions among the di-
rectional measures emerge. In fact, even if Weed-
sPrec and ClarkeDE increase the AP for HYPER,
still they assign even higher AP values to COORD.
Conversely, invCL is the only measure that assigns
to HYPER the top AP score, higher than COORD too.
The new directional similarity measure we have
proposed in this paper, invCL, thus reveals a higher
ability to set apart hypernyms from other relations,
coordinates terms included. The latter are expected
</bodyText>
<page confidence="0.993519">
78
</page>
<bodyText confidence="0.999972764705882">
to share a large number of contexts and this is the
reason why they are strongly favored by symmet-
ric similarity measures, such as Cosine. Asymmet-
ric measures like cosWeeds and ClarkeDE also fall
short of distinguishing hypernyms from coordinates
because the condition of feature inclusion they test
is satisfied by coordinate terms as well. If two sets
share a high number of elements, then many ele-
ments of the former are also included in the latter,
and vice versa. Therefore, coordinate terms too are
expected to have high values of feature inclusions.
Conversely, invCL takes into account not only the
inclusion of u into v, but also the amount of v that
is not included in u. Thus, invCL provides a better
distributional correlate to the central property of hy-
pernyms of having a broader semantic content than
their hyponyms.
</bodyText>
<sectionHeader confidence="0.998459" genericHeader="conclusions">
4 Conclusions and ongoing research
</sectionHeader>
<bodyText confidence="0.999928666666667">
The experiments reported in this paper support the
Distributional Inclusion Hypothesis as a viable ap-
proach to model hypernymy in semantic vector
spaces. We have also proposed a new directional
measure that actually outperforms the state-of-the-
art ones. Focusing on the contexts that broader terms
do not share with their narrower terms thus appear
to be an interesting direction to explore to improve
hypernym identification. Our ongoing research in-
cludes testing invCL to recognize lexical entailments
and comparing it with the balAPinc measured pro-
posed by Kotlerman et al. (2010) for this task, as
well as designing new distributional methods to dis-
criminate between various other types of semantic
relations.
</bodyText>
<sectionHeader confidence="0.998136" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999385">
We thank the reviewers for their useful and insight-
ful comments on the paper.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999861563636364">
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional Memory: A general framework for corpus-
based semantics. Computational Linguistics, 36(4):
673–721.
Marco Baroni and Alessandro Lenci. 2011. How we
BLESSed distributional semantic evaluation. In Pro-
ceedings of the GEMS 2011 Workshop on Geometri-
cal Models of Natural Language Semantics, EMNLP
2011, Edinburgh, Scotland, UK: 1–10.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of the
EACL 2009 Workshop on GEMS: GEometrical Models
ofNatural Language Semantics, Athens, Greece: 112–
119.
Katrin Erk. 2009a. Supporting inferences in semantic
space: representing words as regions. In Proceedings
of the 8th International Conference on Computational
Semantics, Tilburg, January: 104–115.
Katrin Erk. 2009b. Representing words as regions in
vector space. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language Learning
(CoNLL), Boulder, Colorado: 57–65.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D. dissertation, Stuttgart University.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING
1992, Nantes, France: 539–545.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04): 359–389.
Dekang Lin. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the COLING-
ACL 1998, Montreal, Canada: 768–774.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of the
COLING-ACL 2006, Sydney, Australia: 113–120.
Idan Szpektor and Ido Dagan. 2008. Learning Entail-
ment Rules for Unary Templates. In Proceedings of
COLING 2008, Manchester, UK: 849–856.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the EMNLP 2003, Sapporo, Japan: 81–88.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional sim-
ilarity. In Proceedings of COLING 2004, Geneva,
Switzerland: 1015–1021.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The
distributional inclusion hypotheses and lexical entail-
ment. In Proceedings of ACL 2005, Ann Arbor, MI:
107–114.
Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Boot-
strapping distributional feature vector quality. Compu-
tational Linguistics, 35(3): 435-461.
</reference>
<page confidence="0.999048">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.763775">
<title confidence="0.999913">Identifying hypernyms in distributional semantic spaces</title>
<author confidence="0.999976">Alessandro Lenci Giulia Benotto</author>
<affiliation confidence="0.999598">University of Pisa, Dept. of Linguistics University of Pisa, Dept. of Linguistics</affiliation>
<address confidence="0.8993075">via S. Maria 36 via S. Maria 36 I-56126, Pisa, Italy I-56126, Pisa, Italy</address>
<email confidence="0.998215">alessandro.lenci@ling.unipi.itmezzanine.g@gmail.com</email>
<abstract confidence="0.993630714285714">In this paper we apply existing directional similarity measures to identify hypernyms with a state-of-the-art distributional semantic model. We also propose a new directional measure that achieves the best performance in hypernym identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory: A general framework for corpusbased semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<pages>673--721</pages>
<contexts>
<context position="5981" citStr="Baroni and Lenci, 2010" startWordPosition="975" endWordPosition="978">number of animal-contexts are not lioncontexts. In fact, being a semantically broader term of lion, animal should also be found in contexts in which animals other than lions occur. 3 Experiments The main purpose of the experiments reported below is to investigate the ability of the directional similarity measures presented in section 2 to identify the hypernyms of a given target noun, and to discriminate hypernyms from terms related by symmetric semantic relations, such as coordinate terms. We have represented lexical items with distributional feature vectors extracted from the TypeDM tensor (Baroni and Lenci, 2010). TypeDM is a particular instantiation of the Distributional Memory (DM) framework. In DM, distributional facts are represented as a weighted tuple structure T, a set of weighted word-link-word tuples ((w1,l, w2), Q), such that w1 and w2 are content words (e.g. nouns, verbs, etc.), l is a syntagmatic co-occurrence links between words in a text (e.g. syntactic dependencies, etc.), and Q is a weight estimating the statistical salience of that tuple. The TypeDM word set contains 30,693 lemmas (20,410 nouns, 5,026 verbs and 5,257 adjectives). The TypeDM link set contains 25,336 direct and inverse </context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A general framework for corpusbased semantics. Computational Linguistics, 36(4): 673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we BLESSed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011,</booktitle>
<pages>1--10</pages>
<location>Edinburgh, Scotland, UK:</location>
<contexts>
<context position="6919" citStr="Baroni and Lenci, 2011" startWordPosition="1132" endWordPosition="1135"> links between words in a text (e.g. syntactic dependencies, etc.), and Q is a weight estimating the statistical salience of that tuple. The TypeDM word set contains 30,693 lemmas (20,410 nouns, 5,026 verbs and 5,257 adjectives). The TypeDM link set contains 25,336 direct and inverse links formed by (partially lexicalized) syntactic dependencies and patterns. The weight Q is the Local Mutual Information (LMI) (Evert, 2005) computed on link type frequency (negative LMI values are raised to 0). 3.1 Test set We have evaluated the directional similarity measures on a subset of the BLESS data set (Baroni and Lenci, 2011), consisting of tuples expressing a relation between a target concept (henceforth referred to as concept) and a relatum concept (henceforth referred to as relatum). BLESS includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities, and grouped into 17 broader classes (e.g., BIRD, FRUIT, FURNITURE, VEHICLE, etc.). For each concept noun, BLESS includes several 76 relatum words, linked to the concept by one of 5 semantic relations. Here, we have used the BLESS subset formed by 14,547 tuples with the relatum attested in the TypeDM word se</context>
<context position="9511" citStr="Baroni and Lenci (2011)" startWordPosition="1559" endWordPosition="1563"> 2 to BLESS, with the goal of evaluating their ability to discriminate hypernyms from other semantic relations, in particular co-hyponymy. In fact, differently from hypernyms, coordinate terms are not related by inclusion. Therefore, we want to test whether directional similarity measures are able to assign higher scores to hypernyms, as predicted by the Distributional Inclusion Hypothesis. We used the Cosine as our baseline, since it is a symmetric similarity measure and it is commonly used in DSMs. We adopt two different evaluation methods. The first is based on the methodology described in Baroni and Lenci (2011). Given the similarity scores for a concept with all its relata across all relations in our test set, we pick the relatum with the highest score (nearest neighbour) for each relation. In this way, for each of the 200 BLESS concepts, we obtain 4 similarity scores, one per relation. In order to factor out concept-specific effects that might add to the overall score variance, we transform the 8 similarity scores of each concept onto standardized z scores (mean: 0; s.d: 1) by subtracting from each their mean, and dividing by their standard deviation. After this transformation, we produce a boxplot</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, Edinburgh, Scotland, UK: 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models ofNatural Language Semantics,</booktitle>
<pages>112--119</pages>
<location>Athens, Greece:</location>
<contexts>
<context position="2383" citStr="Clarke, 2009" startWordPosition="359" endWordPosition="360">ernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word </context>
<context position="4670" citStr="Clarke (2009)" startWordPosition="741" endWordPosition="742">hin the features of a term v (Weeds and Weir, 2003; Weeds et al., 2004; Kotlerman et al., 2010): WeedsPrec(u, v) = Ef EFunFv wu(f) (1) EfEFu wu(f) cosWeeds (M2) - this measure corresponds to the geometrical average of WeedsPrec and the symmetric similarity between u and v, measured by their vectors’ cosine: �cosWeeds(u, v) = M1(u, v) * cos(u, v) (2) This is actually a variation of the balPrec measure in Kotlerman et al. (2010), the difference being that cosine is used as a symmetric similarity measure instead of the LIN measure (Lin, 1998). ClarkeDE (M3) - a close variation of M1, proposed by Clarke (2009): ClarkeDE(u, v) = Ef EFunFv min(wu(f ), w„ (f )) EfEFu wu(f) invCL (M4) - this a new measure that we introduce and test here for the first time. It takes into account not only the inclusion of u in v, but also the noninclusion of v in u, both measured with ClarkeDE: � invCL(u, v) = M3(u, v) * (1 — M3(v, u)) The intuition behind invCL is that, if v is a semantically broader term of u, then the features of u are included in the features of v, but crucially the features of v are also not included in the features of u. For instance, if animal is a hypernym of lion, we can expect i.) that a signif</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the EACL 2009 Workshop on GEMS: GEometrical Models ofNatural Language Semantics, Athens, Greece: 112– 119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Supporting inferences in semantic space: representing words as regions.</title>
<date>2009</date>
<booktitle>In Proceedings of the 8th International Conference on Computational Semantics,</booktitle>
<pages>104--115</pages>
<location>Tilburg,</location>
<contexts>
<context position="2549" citStr="Erk (2009" startWordPosition="385" endWordPosition="386">ernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encom</context>
</contexts>
<marker>Erk, 2009</marker>
<rawString>Katrin Erk. 2009a. Supporting inferences in semantic space: representing words as regions. In Proceedings of the 8th International Conference on Computational Semantics, Tilburg, January: 104–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Representing words as regions in vector space.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>57--65</pages>
<location>Boulder, Colorado:</location>
<contexts>
<context position="2549" citStr="Erk (2009" startWordPosition="385" endWordPosition="386">ernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encom</context>
</contexts>
<marker>Erk, 2009</marker>
<rawString>Katrin Erk. 2009b. Representing words as regions in vector space. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), Boulder, Colorado: 57–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>The Statistics of Word Cooccurrences.</title>
<date>2005</date>
<institution>Stuttgart University.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="6722" citStr="Evert, 2005" startWordPosition="1098" endWordPosition="1099">s a weighted tuple structure T, a set of weighted word-link-word tuples ((w1,l, w2), Q), such that w1 and w2 are content words (e.g. nouns, verbs, etc.), l is a syntagmatic co-occurrence links between words in a text (e.g. syntactic dependencies, etc.), and Q is a weight estimating the statistical salience of that tuple. The TypeDM word set contains 30,693 lemmas (20,410 nouns, 5,026 verbs and 5,257 adjectives). The TypeDM link set contains 25,336 direct and inverse links formed by (partially lexicalized) syntactic dependencies and patterns. The weight Q is the Local Mutual Information (LMI) (Evert, 2005) computed on link type frequency (negative LMI values are raised to 0). 3.1 Test set We have evaluated the directional similarity measures on a subset of the BLESS data set (Baroni and Lenci, 2011), consisting of tuples expressing a relation between a target concept (henceforth referred to as concept) and a relatum concept (henceforth referred to as relatum). BLESS includes 200 distinct English concrete nouns as target concepts, equally divided between living and non-living entities, and grouped into 17 broader classes (e.g., BIRD, FRUIT, FURNITURE, VEHICLE, etc.). For each concept noun, BLESS</context>
</contexts>
<marker>Evert, 2005</marker>
<rawString>Stefan Evert. 2005. The Statistics of Word Cooccurrences. Ph.D. dissertation, Stuttgart University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING 1992,</booktitle>
<pages>539--545</pages>
<location>Nantes, France:</location>
<contexts>
<context position="2125" citStr="Hearst, 1992" startWordPosition="320" endWordPosition="321">utional Inclusion Hypothesis, according to which if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE)</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING 1992, Nantes, France: 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>04</issue>
<pages>359--389</pages>
<contexts>
<context position="1455" citStr="Kotlerman et al., 2010" startWordPosition="212" endWordPosition="216">ilar to both animal and cat, but with different types of relations. Current DSMs accounts for these facts only partially. While they may correctly place both animal and cat among the nearest distributional neighbors of dog, they are not able to characterize the different semantic properties of these relations, for instance the fact that hypernymy is an asymmetric semantic relation, since being a dog entails being an animal, but not the other way round. The purpose of this paper is to explore the possibility of identifying hypernyms in DSMs with directional (or asymmetric) similarity measures (Kotlerman et al., 2010). These measures all rely on some variation of the Distributional Inclusion Hypothesis, according to which if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most method</context>
<context position="3300" citStr="Kotlerman et al. (2010)" startWordPosition="501" endWordPosition="504">ithin the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encompasses several standard semantic relations such as synonymy, hypernymy, metonymy, some cases of meronymy, etc. Differently from Kotlerman et al. (2010), here we focus on applying directional, asymmetric similarity measures to identify hypernyms. We assume the classical definition of a hypernymy, such that Y is 75 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 75–79, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics an hypernym of X if and only if X is a kind of Y , or equivalently every X is a Y . 2 Directional similarity measures In the experiments reported in section 3 we have applied the following directional similarity measures (Fx is the set of distributional features of a t</context>
<context position="8077" citStr="Kotlerman et al. (2010)" startWordPosition="1321" endWordPosition="1324"> by 14,547 tuples with the relatum attested in the TypeDM word set, and containing one of these relations: COORD: the relatum is a noun that is a co-hyponym (coordinate) of the concept: (alligator, coord, lizard); HYPER: the relatum is a noun that is a hypernym of the concept: (alligator, hyper, animal); MERO: the relatum is a noun referring to a part/component/organ/member of the concept, or something that the concept contains or is made of: (alligator, mero, mouth); RANDOM-N: the relatum is a random noun holding no semantic relation with the target concept: (alligator, random — n, message). Kotlerman et al. (2010) evaluate a set of directional similarity measure on a data set of valid and invalid (substitutable) lexical entailments (Zhitomirsky-Geffet and Dagan, 2009). However, as we said above, lexical entailment is defined as an asymmetric relation that covers various types of classic semantic relations, besides hypernymy . The choice of BLESS is instead motivated by the fact that here we focus on the ability of directional similarity measure to identify hypernyms. 3.2 Evaluation and results For each word x in the test set, we represented x in terms of a set Fx of distributional features (l, w2), suc</context>
<context position="12519" citStr="Kotlerman et al. (2010)" startWordPosition="2051" endWordPosition="2054">emantic spaces, with respect to other types of semantic relations, such as COORD. 77 Figure 1: Distribution of relata similarity scores across concepts (values on ordinate are similarity scores after conceptby-concept z-normalization). Cosine caartl hyper mero mntlomn WeedsPrec _W hyper mero rentlom-n cosWeeds wwtl hyper — .n -n -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 ClarkeDE was hyper mero ranaom-n InvCL coord random-n -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 The second type of evaluation we have performed is based on Kotlerman et al. (2010). The similarity measures have been evaluated with Average Precision (AP), a method derived from Information Retrieval and combining precision, relevance ranking and overall recall. For each similarity measure, we computed AP with respect to the 4 BLESS relations. The best possible score (AP = 1) for a given relation (e.g., HYPER) corresponds to the ideal case in which all the relata belonging to that relation have higher similarity scores than the relata belonging to the other relations. For every relation, we calculated the AP for each of the 200 BLESS target concepts. In Table 1, we report </context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(04): 359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLINGACL</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada:</location>
<contexts>
<context position="4602" citStr="Lin, 1998" startWordPosition="729" endWordPosition="730">quantifies the weighted inclusion of the features of a term u within the features of a term v (Weeds and Weir, 2003; Weeds et al., 2004; Kotlerman et al., 2010): WeedsPrec(u, v) = Ef EFunFv wu(f) (1) EfEFu wu(f) cosWeeds (M2) - this measure corresponds to the geometrical average of WeedsPrec and the symmetric similarity between u and v, measured by their vectors’ cosine: �cosWeeds(u, v) = M1(u, v) * cos(u, v) (2) This is actually a variation of the balPrec measure in Kotlerman et al. (2010), the difference being that cosine is used as a symmetric similarity measure instead of the LIN measure (Lin, 1998). ClarkeDE (M3) - a close variation of M1, proposed by Clarke (2009): ClarkeDE(u, v) = Ef EFunFv min(wu(f ), w„ (f )) EfEFu wu(f) invCL (M4) - this a new measure that we introduce and test here for the first time. It takes into account not only the inclusion of u in v, but also the noninclusion of v in u, both measured with ClarkeDE: � invCL(u, v) = M3(u, v) * (1 — M3(v, u)) The intuition behind invCL is that, if v is a semantically broader term of u, then the features of u are included in the features of v, but crucially the features of v are also not included in the features of u. For instan</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the COLINGACL 1998, Montreal, Canada: 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL 2006,</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia:</location>
<contexts>
<context position="2158" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="322" endWordPosition="325">ion Hypothesis, according to which if u is a semantically narrower term than v, then a significant number of salient distributional features of u is included in the feature vector of v as well. Since hypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the COLING-ACL 2006, Sydney, Australia: 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning Entailment Rules for Unary Templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING 2008,</booktitle>
<pages>849--856</pages>
<location>Manchester, UK:</location>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning Entailment Rules for Unary Templates. In Proceedings of COLING 2008, Manchester, UK: 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>In Proceedings of the EMNLP 2003,</booktitle>
<pages>81--88</pages>
<location>Sapporo, Japan:</location>
<contexts>
<context position="2348" citStr="Weeds and Weir, 2003" startWordPosition="351" endWordPosition="354">ypernymy is an asymmetric relation and hypernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other an</context>
<context position="4107" citStr="Weeds and Weir, 2003" startWordPosition="642" endWordPosition="645">on Lexical and Computational Semantics (*SEM), pages 75–79, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics an hypernym of X if and only if X is a kind of Y , or equivalently every X is a Y . 2 Directional similarity measures In the experiments reported in section 3 we have applied the following directional similarity measures (Fx is the set of distributional features of a term x, wx(f) is the weight of the feature f for x): WeedsPrec (M1) - this is a measure that quantifies the weighted inclusion of the features of a term u within the features of a term v (Weeds and Weir, 2003; Weeds et al., 2004; Kotlerman et al., 2010): WeedsPrec(u, v) = Ef EFunFv wu(f) (1) EfEFu wu(f) cosWeeds (M2) - this measure corresponds to the geometrical average of WeedsPrec and the symmetric similarity between u and v, measured by their vectors’ cosine: �cosWeeds(u, v) = M1(u, v) * cos(u, v) (2) This is actually a variation of the balPrec measure in Kotlerman et al. (2010), the difference being that cosine is used as a symmetric similarity measure instead of the LIN measure (Lin, 1998). ClarkeDE (M3) - a close variation of M1, proposed by Clarke (2009): ClarkeDE(u, v) = Ef EFunFv min(wu(f</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Proceedings of the EMNLP 2003, Sapporo, Japan: 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<pages>1015--1021</pages>
<location>Geneva, Switzerland:</location>
<contexts>
<context position="2368" citStr="Weeds et al., 2004" startWordPosition="355" endWordPosition="358">ric relation and hypernyms are semantically broader terms than their hyponyms, then we can predict that directional similarity measures are better suited to identify terms related by the hypernymy relation. Automatic identification of hypernyms in corpora is a long-standing research line, but most methods have adopted semi-supervised, pattern-based approaches (Hearst, 1992; Pantel and Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the</context>
<context position="4127" citStr="Weeds et al., 2004" startWordPosition="646" endWordPosition="649">tional Semantics (*SEM), pages 75–79, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics an hypernym of X if and only if X is a kind of Y , or equivalently every X is a Y . 2 Directional similarity measures In the experiments reported in section 3 we have applied the following directional similarity measures (Fx is the set of distributional features of a term x, wx(f) is the weight of the feature f for x): WeedsPrec (M1) - this is a measure that quantifies the weighted inclusion of the features of a term u within the features of a term v (Weeds and Weir, 2003; Weeds et al., 2004; Kotlerman et al., 2010): WeedsPrec(u, v) = Ef EFunFv wu(f) (1) EfEFu wu(f) cosWeeds (M2) - this measure corresponds to the geometrical average of WeedsPrec and the symmetric similarity between u and v, measured by their vectors’ cosine: �cosWeeds(u, v) = M1(u, v) * cos(u, v) (2) This is actually a variation of the balPrec measure in Kotlerman et al. (2010), the difference being that cosine is used as a symmetric similarity measure instead of the LIN measure (Lin, 1998). ClarkeDE (M3) - a close variation of M1, proposed by Clarke (2009): ClarkeDE(u, v) = Ef EFunFv min(wu(f ), w„ (f )) EfEFu w</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of COLING 2004, Geneva, Switzerland: 1015–1021.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>The distributional inclusion hypotheses and lexical entailment.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<pages>107--114</pages>
<location>Ann Arbor, MI:</location>
<contexts>
<context position="2770" citStr="Zhitomirsky-Geffet and Dagan (2005" startWordPosition="415" endWordPosition="418">d Pennacchiotti, 2006). Fully unsupervised hypernym identification with DSMs is still a largely open field. Various models to represent hypernyms in vector spaces have recently been proposed (Weeds and Weir, 2003; Weeds et al., 2004; Clarke, 2009), usually grounded on the Distributional Inclusion Hypothesis (for a different approach based on representing word meaning as “regions” in vector space, see Erk (2009a; 2009b)). The same hypothesis has been adopted by Kotlerman et al. (2010) to identify (substitutable) lexical entailments” . Within the context of the Textual Entailment (TE) paradigm, Zhitomirsky-Geffet and Dagan (2005; 2009) define (substitutable) lexical entailment as a relation holding between two words, if there are some contexts in which one of the words can be substituted by the other and the meaning of the original word can be inferred from the new one. Its relevance for TE notwithstanding, this notion of lexical entailment is more general and looser than hypernymy. In fact, it encompasses several standard semantic relations such as synonymy, hypernymy, metonymy, some cases of meronymy, etc. Differently from Kotlerman et al. (2010), here we focus on applying directional, asymmetric similarity measure</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2005</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2005. The distributional inclusion hypotheses and lexical entailment. In Proceedings of ACL 2005, Ann Arbor, MI: 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maayan Zhitomirsky-Geffet</author>
<author>Ido Dagan</author>
</authors>
<title>Bootstrapping distributional feature vector quality.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>435--461</pages>
<contexts>
<context position="8234" citStr="Zhitomirsky-Geffet and Dagan, 2009" startWordPosition="1343" endWordPosition="1346"> co-hyponym (coordinate) of the concept: (alligator, coord, lizard); HYPER: the relatum is a noun that is a hypernym of the concept: (alligator, hyper, animal); MERO: the relatum is a noun referring to a part/component/organ/member of the concept, or something that the concept contains or is made of: (alligator, mero, mouth); RANDOM-N: the relatum is a random noun holding no semantic relation with the target concept: (alligator, random — n, message). Kotlerman et al. (2010) evaluate a set of directional similarity measure on a data set of valid and invalid (substitutable) lexical entailments (Zhitomirsky-Geffet and Dagan, 2009). However, as we said above, lexical entailment is defined as an asymmetric relation that covers various types of classic semantic relations, besides hypernymy . The choice of BLESS is instead motivated by the fact that here we focus on the ability of directional similarity measure to identify hypernyms. 3.2 Evaluation and results For each word x in the test set, we represented x in terms of a set Fx of distributional features (l, w2), such that in the TypeDM tensor there is a tuple ((w1,l, w2), Q), w1 = x. The feature weight wx(f) is equal to the weight a of the original DM tuple. Then, we ap</context>
</contexts>
<marker>Zhitomirsky-Geffet, Dagan, 2009</marker>
<rawString>Maayan Zhitomirsky-Geffet and Ido Dagan. 2009. Bootstrapping distributional feature vector quality. Computational Linguistics, 35(3): 435-461.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>