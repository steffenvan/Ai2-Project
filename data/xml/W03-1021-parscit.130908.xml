<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000661">
<title confidence="0.995086">
Training Connectionist Models for the Structured Language Model
</title>
<author confidence="0.997411">
Peng Xu, Ahmad Emami and Frederick Jelinek
</author>
<affiliation confidence="0.9049855">
Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.923917">
Baltimore, MD 21218
</address>
<email confidence="0.998468">
xp,emami,jelinek @jhu.edu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956115384615">
We investigate the performance of the
Structured Language Model (SLM) in
terms of perplexity (PPL) when its compo-
nents are modeled by connectionist mod-
els. The connectionist models use a dis-
tributed representation of the items in
the history and make much better use of
contexts than currently used interpolated
or back-off models, not only because of
the inherent capability of the connection-
ist model in fighting the data sparseness
problem, but also because of the sub-
linear growth in the model size when the
context length is increased. The connec-
tionist models can be further trained by an
EM procedure, similar to the previously
used procedure for training the SLM. Our
experiments show that the connectionist
models can significantly improve the PPL
over the interpolated and back-off mod-
els on the UPENN Treebank corpora, after
interpolating with a baseline trigram lan-
guage model. The EM training procedure
can improve the connectionist models fur-
ther, by using hidden events obtained by
the SLM parser.
</bodyText>
<sectionHeader confidence="0.994549" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981315481481482">
In many systems dealing with natural speech or lan-
guage such as Automatic Speech Recognition and
✄This work was supported by the National Science Founda-
tion under grants No.IIS-9982329 and No.IIS-0085940.
Statistical Machine Translation, a language model
is a crucial component for searching in the often
prohibitively large hypothesis space. Most of the
state-of-the-art systems use n-gram language mod-
els, which are simple and effective most of the
time. Many smoothing techniques that improve lan-
guage model probability estimation have been pro-
posed and studied in the n-gram literature (Chen and
Goodman, 1998).
Recent efforts have studied various ways of us-
ing information from a longer context span than that
usually captured by normal n-gram language mod-
els, as well as ways of using syntactical informa-
tion that is not available to the word-based n-gram
models (Chelba and Jelinek, 2000; Charniak, 2001;
Roark, 2001; Uystel et al., 2001). All these language
models are based on stochastic parsing techniques
that build up parse trees for the input word sequence
and condition the generation of words on syntactical
and lexical information available in the parse trees.
Since these language models capture useful hierar-
chical characteristics of language, they can improve
the PPL significantly for various tasks. Although
more improvement can be achieved by enriching the
syntactical dependencies in the structured language
model (SLM) (Xu et al., 2002), a severe data sparse-
ness problem was observed in (Xu et al., 2002) when
the number of conditioning features was increased.
There has been recent promising work in us-
ing distributional representation of words and neu-
ral networks for language modeling (Bengio et al.,
2001) and parsing (Henderson, 2003). One great ad-
vantage of this approach is its ability to fight data
sparseness. The model size grows only sub-linearly
with the number of predicting features used. It has
been shown that this method improves significantly
on regular n-gram models in perplexity (Bengio et
al., 2001). The ability of the method to accommo-
date longer contexts is most appealing, since exper-
iments have shown consistent improvements in PPL
when the context of one of the components of the
SLM is increased in length (Emami et al., 2003).
Moreover, because the SLM provides an EM train-
ing procedure for its components, the connectionist
models can also be improved by the EM training.
In this paper, we will study the impact of neural
network modeling on the SLM, when all of its three
components are modeled with this approach. An EM
training procedure will be outlined and applied to
further training of the neural network models.
</bodyText>
<sectionHeader confidence="0.990101" genericHeader="method">
2 A Probabilistic Neural Network Model
</sectionHeader>
<bodyText confidence="0.999967">
Recently, a relatively new type of language model
has been introduced where words are represented
by points in a multi-dimensional feature space and
the probability of a sequence of words is computed
by means of a neural network. The neural network,
having the feature vectors of the preceding words as
its input, estimates the probability of the next word
(Bengio et al., 2001). The main idea behind this
model is to fight the curse of dimensionality by inter-
polating the seen sequences in the training data. The
generalization this model aims at is to assign to an
unseen word sequence a probability similar to that
of a seen word sequence whose words are similar to
those of the unseen word sequence. The similarity
is defined as being close in the multi-dimensional
space mentioned above.
In brief, this model can be described as follows.
A feature vector is associated with each token in the
input vocabulary, that is, the vocabulary of all the
items that can be used for conditioning. Then the
conditional probability of the next word is expressed
as a function of the input feature vectors by means
of a neural network. This probability is produced
for every possible next word from the output vocab-
ulary. In general, there does not need to be any rela-
tionship between the input and output vocabularies.
The feature vectors and the parameters of the neural
network are learned simultaneously during training.
The input to the neural network are the feature vec-
tors for all the inputs concatenated, and the output
is the conditional probability distribution over the
output vocabulary. The idea here is that the words
which are close to each other (close in the sense of
their role in predicting words to follow) would have
similar (close) feature vectors and since the proba-
bility function is a smooth function of these feature
values, a small change in the features should only
lead to a small change in the probability.
</bodyText>
<subsectionHeader confidence="0.988181">
2.1 The Architecture of the Neural Network
Model
</subsectionHeader>
<bodyText confidence="0.99473925">
The conditional probability function
where and are from the
input and output vocabularies and respectively,
is determined in two parts:
</bodyText>
<listItem confidence="0.917326833333333">
1. A mapping that associates with each word in
the input vocabulary a real vector of fixed
length
2. A conditional probability function which takes
as the input the concatenation of the feature
vectors of the input items
</listItem>
<bodyText confidence="0.9799215">
Training is achieved by searching for parameters
of the neural network and the values of feature
vectors that maximize the penalized log-likelihood
of the training corpus:
</bodyText>
<equation confidence="0.923612">
(1)
</equation>
<bodyText confidence="0.906794842105263">
where is the probability of word
(network output at time ), is the training data
size and is a regularization term, sum of the
parameters’ squares in our case.
The model architecture is given in Figure 1. The
neural network is a simple fully connected network
with one hidden layer and sigmoid transfer func-
tions. The input to the function is the concatenation
of the feature vectors of the input items. The out-
put of the output layer is passed though a softmax to
.
The function produces a probability distribu-
tion (a vector) over , the element being
the conditional probability of the member
of . This probability function is realized by
a standard multi-layer neural network. A soft-
max function (Equation 4) is used at the output
of the neural net to make sure probabilities sum
to 1.
</bodyText>
<figureCaption confidence="0.999823">
Figure 1: The neural network architecture
</figureCaption>
<bodyText confidence="0.9896314375">
make sure that the scores are positive and sum up to
one, hence are valid probabilities. More specifically,
the output of the hidden layer is given by:
where is the output of the hidden layer,
is the input of the network, and
are weight and bias elements for the hidden layer
respectively, and is the number of hidden units.
Furthermore, the outputs are given by:
where and are weight and bias elements for
the output layer before the softmax layer. The soft-
max layer (equation 4) ensures that the outputs are
positive and sum to one, hence are valid probabili-
ties. The output of the neural network, corre-
sponding to the item of the output vocab-
ulary, is exactly the sought conditional probability,
that is .
</bodyText>
<subsectionHeader confidence="0.998554">
2.2 Training the Neural Network Model
</subsectionHeader>
<bodyText confidence="0.999989846153846">
Standard back-propagation is used to train the pa-
rameters of the neural network as well as the feature
vectors. See (Haykin, 1999) for details about neural
networks and back-propagation. The function we try
to maximize is the log-likelihood of the training data
given by equation 1. It is straightforward to com-
pute the gradient of the likelihood function for the
feature vectors and the neural network parameters,
and hence compute their updates.
We should note from equation 4 that the neural
network model is similar in functional form to the
maximum entropy model (Berger et al., 1996) ex-
cept that the neural network learns the feature func-
tions by itself from the training data. However,
unlike the G/IIS algorithm for the maximum en-
tropy model, the training algorithm (usually stochas-
tic gradient descent) for the neural network models
is not guaranteed to find even a local maximum of
the objective function.
It is very important to mention that one of the
great advantages of this model is that the number
of inputs can be increased causing only sub-linear
increase in the number of model parameters, as op-
posed to exponential growth in n-gram models. This
makes the parameter estimation more robust, espe-
cially when the input span is long.
</bodyText>
<sectionHeader confidence="0.996758" genericHeader="method">
3 Structured Language Model
</sectionHeader>
<bodyText confidence="0.998571571428571">
An extensive presentation of the SLM can be found
in (Chelba and Jelinek, 2000). The model assigns a
probability to every sentence and ev-
ery possible binary parse . The terminals of
are the words of with POS tags, and the nodes
of are annotated with phrase headwords and non-
terminal labels. Let be a sentence of length
</bodyText>
<equation confidence="0.758264">
(&lt;s&gt;, SB) (w_p, t_p) (w_{p+1}, t_{p+1}) (w_k, t_k) w_{k+1}.... &lt;/s&gt;
</equation>
<figureCaption confidence="0.999226">
Figure 2: A word-parse -prefix
</figureCaption>
<bodyText confidence="0.995823923076923">
words to which we have prepended the sentence be-
ginning marker &lt;s&gt; and appended the sentence end
marker &lt;/s&gt; so that &lt;s&gt; and &lt;/s&gt;.
Let be the word -prefix of the
sentence — the words from the beginning of the
sentence up to the current position — and
the word-parse -prefix. Figure 2 shows a word-
parse -prefix; h_0, .., h_{-m} are the ex-
posed heads, each head being a pair (headword, non-
terminal label), or (word, POS tag) in the case of a
root-only tree. The exposed heads at a given po-
sition in the input sentence are a function of the
word-parse -prefix.
</bodyText>
<subsectionHeader confidence="0.983784">
3.1 Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.9982455">
The joint probability of a word sequence
and a complete parse can be broken up into:
</bodyText>
<figure confidence="0.877986888888889">
input layer
W
hidden layer
output
y
tanh softmax
x1
x2
xn−1
</figure>
<equation confidence="0.878807">
V
h_{-m} = (&lt;s&gt;, SB)
h_{-1} h_0 = (h_0.word, h_0.tag)
</equation>
<bodyText confidence="0.993335851851852">
where:
is the word-parse -prefix
is the word predicted by WORD-PREDICTOR
is the tag assigned to by the TAGGER
is the number of operations the CON-
STRUCTOR executes at sentence position before
passing control to the WORD-PREDICTOR (the
-th operation at position k is the null transi-
tion); is a function of
denotes the -th CONSTRUCTOR operation
carried out at position k in the word string; the op-
erations performed by the CONSTRUCTOR ensure
that all possible binary branching parses, with all
possible headword and non-terminal label assign-
ments for the word sequence, can be gen-
erated. The sequence of CONSTRUC-
TOR operations at position grows the word-parse
-prefix into a word-parse -prefix.
The SLM is based on three probabilities, each
can be specified using various smoothing methods
and parameterized (approximated) by using differ-
ent contexts. The bottom-up nature of the SLM
parser enables us to condition the three probabili-
ties on features related to the identity of any exposed
head and any structure below the exposed head.
Since the number of parses for a given word prefix
grows exponentially with ,
</bodyText>
<equation confidence="0.476458">
(6)
</equation>
<bodyText confidence="0.987895666666667">
which ensures a proper probability normalization
over strings , where is the set of all parses
present in our stacks at the current stage .
</bodyText>
<subsectionHeader confidence="0.990725">
3.2 N-best EM Training of the SLM
</subsectionHeader>
<bodyText confidence="0.999805166666667">
Each model component of the SLM —WORD-
PREDICTOR, TAGGER, CONSTRUCTOR— is
initialized from a set of parsed sentences after under-
going headword percolation and binarization. An N-
best EM (Chelba and Jelinek, 2000) variant is then
employed to jointly reestimate the model parameters
such that the PPL on training data is decreased —
the likelihood of the training data under our model
is increased. The reduction in PPL is shown experi-
mentally to carry over to the test data.
Let denote the joint sequence of with
parse structure . The probability of a se-
quence is, according to Equation 5, the
product of the corresponding elementary events.
This product form makes the three components of
the SLM separable, therefore, we can estimate the
parameters separately. According to the EM algo-
rithm, the auxiliary function can be written as:
</bodyText>
<equation confidence="0.879888">
(7)
</equation>
<bodyText confidence="0.999456833333333">
The E step in the EM algorithm is to find
under the model parameters of the
previous iteration, the M step is to find parame-
ters that maximize the auxiliary function
above. In practice, since the space of , all possi-
ble parses, is huge, we normally use a synchronous
multi-stack search algorithm to sample the most
probable parses and approximate the space by the
N-best parses. (Chelba and Jelinek, 2000) showed
that as long as the N-best parses remain invariant,
the M step will increase the likelihood of the train-
ing data.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="method">
4 Neural Network Models in the SLM
</sectionHeader>
<bodyText confidence="0.999068387755102">
As described in the previous section, the three com-
ponents of the SLM can be parameterized in various
ways. The neural network model, because of its abil-
ity in fighting the data sparseness problem, is a very
natural choice when we want to use longer contexts
to improve the language model performance.
The training criterion for the neural network
model is given by Equation 1 , when we have la-
beled training data for the SLM. The labels —the
parse structure— are used to get the conditioning
variables. In order to take advantage of the ability
of the SLM in generating many hidden parses, we
need to modify the training criterion for the neural
network model. Actually, if we take the EM auxil-
iary function in Equation 7 and find parameters of
the neural network models to maximize ,
the solution will be very simple. When standard
back-propagation is used to optimize Equation 1,
,
the state space of our model is huge even for rela-
tively short sentences, so we have to use a search
strategy that prunes it. One choice is a synchronous
multi-stack search algorithm (Chelba and Jelinek,
2000) which is very similar to a beam search.
The language model probability assignment for
the word at position in the input sentence is
made using:
the derivative of with respect to the parameters
is calculated and used as the direction for the gra-
dient descent algorithm. Since is nothing
but a weighted average of the log-likelihood func-
tions, the derivative of with respect to the param-
eters is then a weighted average of the derivatives of
the log-likelihood functions. In practice, we use the
SLM with all components modeled by neural net-
works to generate N-best parses in the E step, and for
the M step, we use the modified back-propagation
algorithm to estimate the parameters of the neural
network models based on the weights calculated in
the E step.
We should be aware that there is no proof that this
EM procedure can actually increase the likelihood
of the training data. Not only are we using a small
portion of the entire hidden parse space, but we also
use the stochastic gradient descent algorithm that is
not guaranteed to converge, for training the neural
network models. Bearing this in mind, we will show
experimentally that this flawed EM procedure can
still lead to improvements in PPL.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999974789473684">
We have used the UPenn Treebank portion of the
WSJ corpus to carry out our experiments. The
UPenn Treebank contains 24 sections of hand-
parsed sentences. We used section 00-20 for training
our models, section 21-22 for tuning some param-
eters (i.e., estimating discount constant for smooth-
ing, and/or making sure overtraining does not occur)
and section 23-24 to test our models. Before car-
rying out our experiments, we normalized the text
in the following ways: numbers in Arabic form are
replaced by a single token “N”, punctuations are re-
moved, all words are mapped to lower case, extra in-
formation in the parse (such like traces) are ignored.
The word vocabulary contains 10k words including
a special token for unknown words. There are 40
items in the part-of-speech set and 54 items in the
non-terminal set, respectively. All of the experimen-
tal results in this section are based on this corpus and
split, unless otherwise stated.
</bodyText>
<subsectionHeader confidence="0.998172">
5.1 Getting a Better Baseline
</subsectionHeader>
<bodyText confidence="0.999972888888889">
Since better performance of the SLM was reported
recently in (Kim et al., 2001) by using Kneser-Ney
smoothing, we first improved the baseline model by
using a variant of Kneser-Ney smoothing: the in-
terpolated Kneser-Ney smoothing as in (Goodman,
2001), which is also implemented in the SRILM
toolkit (Stolcke, 2002).
There are three notable differences in our imple-
mentation of the interpolated Kneser-Ney smooth-
ing related to that in the SRILM toolkit. First, we
used one discount constant for each n-gram level, in-
stead of three different discount constants. Second,
our discount constant was estimated by maximizing
the log-likelihood of the heldout data (assuming the
discount constant is between 0 and 1), instead of
the Good-Turing estimate. Finally, in order to deal
with the fractional counts we encounter during the
EM training procedure, we developed an approxi-
mate Kneser-Ney smoothing for fractional counts.
For lack of space, we do not go into the details of this
approximation, but our approximation becomes the
exact Kneser-Ney smoothing when the counts are in-
tegers.
In order to test our Kneser-Ney smoothing im-
plementation, we built a trigram language model
and compared the performance with that from the
SRILM. Our PPL was 149.6 and the SRILM PPL
was 148.3, therefore, although there are differences
in the implementation details, we think our result is
close enough to the SRILM.
Having tested the smoothing method, we applied
it to the SLM. We used the Kneser-Ney smooth-
ing to all components with the same parameteriza-
tion as the h-2 scheme in (Xu et al., 2002). Table 1
is the comparison between the deleted-interpolation
(DI) smoothing and the Kneser-Ney (KN) smooth-
ing. The in Table 1 is the interpolation weight
between the SLM and the trigram language model
( =1.0 being the trigram language model). The no-
tation “En” indicates the models were obtained af-
ter “n” iterations of EM training&apos;. Since Kneser-
Ney smoothing is consistently better than deleted-
interpolation, we later on report only the Kneser-
Ney smoothing results when comparing to the neural
network models.
</bodyText>
<table confidence="0.902691833333333">
&apos;In particular, E0 simply means initialization.
Model =0.0 =0.4 =1.0
KN-E0 143.5 132.3 149.6
KN-E3 140.7 131.0 149.6
DI-E0 161.4 149.2 166.6
DI-E3 159.4 148.2 166.6
</table>
<tableCaption confidence="0.999987">
Table 1: Comparison between KN and DI smoothing
</tableCaption>
<subsectionHeader confidence="0.981954">
5.2 Training Neural Network Models with the
Treebank
</subsectionHeader>
<bodyText confidence="0.999836368421053">
We used the neural network models for all of the
three components of the SLM. The neural network
models are exactly as described in Section 2.1. Since
the inputs to the networks are always a mixture of
words and NT/POS tags, while the output probabili-
ties are over words in the PREDICTOR, POS tags in
the TAGGER, and adjoint actions in the PARSER,
we used separate input and output vocabularies in
all cases. In all of our experiments with the neu-
ral network models, we used 30 dimensional feature
vectors as input encoding of the mixed items, 100
hidden units and a starting learning rate of 0.001.
Stochastic gradient descent was used for training the
models for a maximum of 50 iterations. The initial-
ization for the parameters is done randomly with a
uniform distribution centered at zero.
In order to study the behavior of the SLM
when longer context is used for conditioning the
probabilities, we gradually increased the context of
the PREDICTOR model. First, the third exposed
previous head was added. Since the syntactical
head gets the head word from one of the children,
either left or right, the child that does not contain
the head word (hence called opposite child) is never
used later on in predicting. This is particularly not
appropriate for the prepositional phrase because the
preposition is always the head word of the phrase
in the UPenn Treebank annotation. Therefore, we
also added the opposite child of the first exposed
previous head into the context for predicting. Both
Kneser-Ney smoothing and the neural network
model were studied when the context was gradually
increased. The results are shown in Table 2.
In Table 2, “nH” stands for “n” exposed previous
heads are used for conditioning in the PREDICTOR
component, “nOP” stands for “n” opposite children
are used, starting from the most recent one. As we
can see, when the length of the context is increased,
</bodyText>
<table confidence="0.880845428571429">
Model +3gram
KN-2H 143.5 132.3
KN-3H 140.2 128.8
KN-3H-1OP 139.4 129.0
NN-2H 162.4 122.9
NN-3H 156.7 120.3
NN-3H-1OP 151.2 118.4
</table>
<tableCaption confidence="0.999915">
Table 2: Comparison between KN and NN (E0)
</tableCaption>
<bodyText confidence="0.99983535">
Kneser-Ney smoothing saturates quickly and could
not improve the PPL further. On the other hand,
the neural network model can still consistently im-
prove the PPL, as longer context is used for predict-
ing. Overall, the best neural network model (after
interpolation with a trigram) achieved 8% relative
improvement over the best result from Kneser-Ney
smoothing.
Another interesting result is that it seems the neu-
ral network model can learn a probability distribu-
tion that is less correlated to the normal trigram
model. Although before interpolating with the tri-
gram, the PPL results of the neural network models
are not as good as the Kneser-Ney smoothed models,
they become much better when combined with the
trigram. In the results of Table 2, the trigram model
is a Kneser-Ney smoothed model that gave PPL of
149.6 by itself. The interpolation weight with the tri-
gram is 0.4 and 0.5 respectively, for the Kneser-Ney
smoothed SLM and neural network based SLM.
</bodyText>
<figure confidence="0.681282">
14
12
10
8
6
4
2
0
NN−2H NN−3H NN−3H−1OP KN−2H KN−3H KN−3H−1OP
</figure>
<figureCaption confidence="0.998915">
Figure 3: Ratio between test and training PPL
</figureCaption>
<bodyText confidence="0.999741684210527">
To better understand why using the neural net-
work models can result in such behavior, we should
look at the difference between the training PPL and
test PPL. Figure 3 shows the ratio between the test
PPL and train PPL. We can see that for the neural
network models, the ratios are much smaller than
that for the Kneser-Ney smoothed models. Further-
more, as the length of context increases, the ratio for
the Kneser-Ney smoothed model becomes greater
— a clear sign of over-parameterization. However,
the ratio for the neural network model changes very
little even when the length of the context increases
from 4 (2H) to 8 (3H-1OP). The exact reason why
the neural network models are more uncorrelated to
the trigram is not completely understood, but we
conjecture that part of the reason is that the neural
network models can learn a probability distribution
very different from the trigram by putting much less
probability mass on the training examples.
</bodyText>
<subsectionHeader confidence="0.6782985">
5.3 Training the Neural Network Models with
EM
</subsectionHeader>
<bodyText confidence="0.9998262">
After the neural network models were trained from
the labeled data —the UPenn Treebank— we per-
formed one iteration of the EM procedure described
in Section 4. The neural network model based SLM
was used to get N-best parses for each training sen-
tence, via the multi-stack search algorithm. This E
step provided us a bigger collection of parse struc-
tures with weights associated with them. In the next
M step, we used the stochastic gradient descent al-
gorithm (modified to utilize the weights associated
with each parse structure) to train the neural network
models. The modified stochastic gradient descent al-
gorithm was run for a maximum of 30 iterations and
the initial parameter values are those from the the
previous iteration.
</bodyText>
<table confidence="0.9976172">
+3gram
NN-3H-1OP E0 151.2 118.4
NN-3H-1OP E1 147.9 117.9
KN-3H-1OP E0 139.4 129.0
KN-3H-1OP E1 139.2 129.2
</table>
<tableCaption confidence="0.999836">
Table 3: EM training results
</tableCaption>
<bodyText confidence="0.998916111111111">
Table 3 shows the PPL results after one EM train-
ing iteration for both the neural network models
and the approximated Kneser-Ney smoothed mod-
els, compared to the results before EM training.
For the neural network models, the EM training did
improve the PPL further, although not a lot. The
improvement from training is consistent with the
training results showed in (Xu et al., 2002) where
deleted-interpolation smoothing was used for the
SLM components. It is worth noting that the ap-
proximated Kneser-Ney smoothed models could not
improve the PPL after one iteration of EM training.
One possible reason is that in order to apply Kneser-
Ney smoothing to fractional counts, we had to ap-
proximate the discounting. The approximation may
degrade the benefit we could have gotten from the
EM training. Similarly, the M step in the EM proce-
dure for the neural network models also has the same
problem: the stochastic gradient descent algorithm
is not guaranteed to converge. This can be clearly
seen in Figure 4 in which we plot the learning curves
of the 3H-1OP model (PREDICTOR component) on
both training and heldout data at EM iteration 0 and
iteration 1. For EM iteration 0, because we started
from parameters drawn from a uniform distribution,
we only plot the last 30 iterations of the stochastic
gradient descent.
</bodyText>
<figureCaption confidence="0.962382">
Figure 4: Learning curves
</figureCaption>
<bodyText confidence="0.9999216">
As we expected, the learning curve of the train-
ing data in EM iteration 1 is not as smooth as that
in EM iteration 0, and even more so for the heldout
data. However, the general trend is still decreasing.
Although we can not prove that the EM training of
the neural network models via the SLM can improve
the PPL, we observed experimentally a gain that is
favorable comparing to that from the usual Kneser-
Ney smoothed models or deleted interpolation mod-
els.
</bodyText>
<sectionHeader confidence="0.997621" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9998955">
By using connectionist models in the SLM, we
achieved significant improvement in PPL over the
baseline trigram and SLM. The neural network en-
henced SLM resulted in a language model that is
much less correlated with the baseline Kneser-Ney
smoothed trigram than the Kneser-Ney smoothed
</bodyText>
<figure confidence="0.98238045">
120
Heldout
Training
100
90
80
105
100
95
90
85
800 5 10 15 20 25 30
E0
E1
110
25
30
35
40 45
50
</figure>
<bodyText confidence="0.999957945945946">
SLM. Overall, the best studied model gave a 21%
relative reduction in PPL over the trigram and 8.7%
relative reduction over the corresponding Kneser-
Ney smoothed SLM. A new EM training procedure
improved the performance of the SLM even further
when applied to the neural network models.
However, reduction in PPL for a language model
does not always mean improvement in performance
of a real application such as speech recognition.
Therefore, future study on applying the neural net-
work enhenced SLM to real applications needs to be
carried out. A preliminary study in (Emami et al.,
2003) already showed that this approach is promis-
ing in reducing the word error rate of a large vocab-
ulary speech recognizer.
There are still many interesting problems in ap-
plying the neural network enhenced SLM to real ap-
plications. Among those, we think the following are
of most of interest:
Speeding up the stochastic gradient descent
algorithm for neural network training: Since
training the neural network models is very
time-consuming, it is essential to speed up the
training in order to carry out many more inter-
esting experiments.
Interpreting the word representations learned in
this framework: For example, word clustering,
context clustering, etc. In particular, if we use
separate mapping matrices for word/NT/POS at
different positions in the context, we may be
able to learn very different representations of
the same word/NT/POS.
Bearing all the challenges in mind, we think the ap-
proach presented in this paper is potentially very
powerful for using the entire partial parse structure
as the conditioning context and for learning useful
features automatically from the data.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999880578947368">
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2001. A neural probabilistic language model. In Ad-
vances in Neural Information Processing Systems.
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguistics,
22(1):39–72, March.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting and 10th Conference ofthe European Chapter
ofACL, pages 116–123, Toulouse, France, July.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283–332, October.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University, Cambridge, Mas-
sachusetts.
Ahmad Emami, Peng Xu, and Frederick Jelinek. 2003.
Using a connectionist model in a syntactical based lan-
guage model. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech, and Signal
Processing, Hong Kong, China, April.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling. Technical Report MSR-TR-2001-72,
Machine Learning and Applied Statistics Group, Mi-
crosoft Research, Redmond, WA.
Simon Haykin. 1999. Neural Networks, A Comprehen-
sive Foundation. Prentice-Hall, Inc., Upper Saddle
River, NJ, USA.
James Henderson. 2003. Neural network probability es-
timation for broad coverage parsing. In Proceedings
of the 10th Conference of the EACL, pages 131–138.
Budapest, Hungary, April.
Woosung Kim, Sanjeev Khudanpur, and Jun Wu. 2001.
Smoothing issues in the structured language model. In
Proc. 7th European Conf. on Speech Communication
and Technology, pages 717–720, Aalborg, Denmark,
September.
Brian Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models and Applica-
tions. Ph.D. thesis, Brown University, Providence, RI.
Andreas Stolcke. 2002. Srilm – an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken Lan-
guage Processing, pages 901–904, Denver, CO.
Dong Hoon Van Uystel, Dirk Van Compernolle, and
Patrick Wambacq. 2001. Maximum-likelihood train-
ing of the plcg-based language model. In Proceedings
of the Automatic Speech Recognition and Understand-
ing Workshop, Madonna di Campiglio, Trento-Italy,
December.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002.
A study on richer syntactic dependencies for struc-
tured language modeling. In Proceedings of the 40th
Annual Meeting of the ACL, pages 191–198, Philadel-
phia, Pennsylvania, USA, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.294884">
<title confidence="0.998845">Training Connectionist Models for the Structured Language Model</title>
<author confidence="0.996257">Ahmad Emami Xu</author>
<affiliation confidence="0.814421">Center for Language and Speech Johns Hopkins</affiliation>
<address confidence="0.987898">Baltimore, MD</address>
<email confidence="0.993216">xp,emami,jelinek@jhu.edu</email>
<abstract confidence="0.97177662962963">We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models. The connectionist models use a distributed representation of the items in the history and make much better use of contexts than currently used interpolated or back-off models, not only because of the inherent capability of the connectionist model in fighting the data sparseness problem, but also because of the sublinear growth in the model size when the context length is increased. The connectionist models can be further trained by an EM procedure, similar to the previously used procedure for training the SLM. Our experiments show that the connectionist models can significantly improve the PPL over the interpolated and back-off models on the UPENN Treebank corpora, after interpolating with a baseline trigram language model. The EM training procedure can improve the connectionist models further, by using hidden events obtained by the SLM parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="2990" citStr="Bengio et al., 2001" startWordPosition="462" endWordPosition="465">ntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in (Xu et al., 2002) when the number of conditioning features was increased. There has been recent promising work in using distributional representation of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used. It has been shown that this method improves significantly on regular n-gram models in perplexity (Bengio et al., 2001). The ability of the method to accommodate longer contexts is most appealing, since experiments have shown consistent improvements in PPL when the context of one of the components of the SLM is increased in length (Emami et al., 2003). Moreover, because the SLM provides an EM training pr</context>
<context position="4349" citStr="Bengio et al., 2001" startWordPosition="689" endWordPosition="692">ural network modeling on the SLM, when all of its three components are modeled with this approach. An EM training procedure will be outlined and applied to further training of the neural network models. 2 A Probabilistic Neural Network Model Recently, a relatively new type of language model has been introduced where words are represented by points in a multi-dimensional feature space and the probability of a sequence of words is computed by means of a neural network. The neural network, having the feature vectors of the preceding words as its input, estimates the probability of the next word (Bengio et al., 2001). The main idea behind this model is to fight the curse of dimensionality by interpolating the seen sequences in the training data. The generalization this model aims at is to assign to an unseen word sequence a probability similar to that of a seen word sequence whose words are similar to those of the unseen word sequence. The similarity is defined as being close in the multi-dimensional space mentioned above. In brief, this model can be described as follows. A feature vector is associated with each token in the input vocabulary, that is, the vocabulary of all the items that can be used for c</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2001</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2001. A neural probabilistic language model. In Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8648" citStr="Berger et al., 1996" startWordPosition="1422" endWordPosition="1425">ining the Neural Network Model Standard back-propagation is used to train the parameters of the neural network as well as the feature vectors. See (Haykin, 1999) for details about neural networks and back-propagation. The function we try to maximize is the log-likelihood of the training data given by equation 1. It is straightforward to compute the gradient of the likelihood function for the feature vectors and the neural network parameters, and hence compute their updates. We should note from equation 4 that the neural network model is similar in functional form to the maximum entropy model (Berger et al., 1996) except that the neural network learns the feature functions by itself from the training data. However, unlike the G/IIS algorithm for the maximum entropy model, the training algorithm (usually stochastic gradient descent) for the neural network models is not guaranteed to find even a local maximum of the objective function. It is very important to mention that one of the great advantages of this model is that the number of inputs can be increased causing only sub-linear increase in the number of model parameters, as opposed to exponential growth in n-gram models. This makes the parameter esti</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Immediate-head parsing for language models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting and 10th Conference ofthe European Chapter ofACL,</booktitle>
<pages>116--123</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2167" citStr="Charniak, 2001" startWordPosition="336" endWordPosition="337">he often prohibitively large hypothesis space. Most of the state-of-the-art systems use n-gram language models, which are simple and effective most of the time. Many smoothing techniques that improve language model probability estimation have been proposed and studied in the n-gram literature (Chen and Goodman, 1998). Recent efforts have studied various ways of using information from a longer context span than that usually captured by normal n-gram language models, as well as ways of using syntactical information that is not available to the word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Eugene Charniak. 2001. Immediate-head parsing for language models. In Proceedings of the 39th Annual Meeting and 10th Conference ofthe European Chapter ofACL, pages 116–123, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="2151" citStr="Chelba and Jelinek, 2000" startWordPosition="332" endWordPosition="335">mponent for searching in the often prohibitively large hypothesis space. Most of the state-of-the-art systems use n-gram language models, which are simple and effective most of the time. Many smoothing techniques that improve language model probability estimation have been proposed and studied in the n-gram literature (Chen and Goodman, 1998). Recent efforts have studied various ways of using information from a longer context span than that usually captured by normal n-gram language models, as well as ways of using syntactical information that is not available to the word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem</context>
<context position="9415" citStr="Chelba and Jelinek, 2000" startWordPosition="1552" endWordPosition="1555">imum entropy model, the training algorithm (usually stochastic gradient descent) for the neural network models is not guaranteed to find even a local maximum of the objective function. It is very important to mention that one of the great advantages of this model is that the number of inputs can be increased causing only sub-linear increase in the number of model parameters, as opposed to exponential growth in n-gram models. This makes the parameter estimation more robust, especially when the input span is long. 3 Structured Language Model An extensive presentation of the SLM can be found in (Chelba and Jelinek, 2000). The model assigns a probability to every sentence and every possible binary parse . The terminals of are the words of with POS tags, and the nodes of are annotated with phrase headwords and nonterminal labels. Let be a sentence of length (&lt;s&gt;, SB) (w_p, t_p) (w_{p+1}, t_{p+1}) (w_k, t_k) w_{k+1}.... &lt;/s&gt; Figure 2: A word-parse -prefix words to which we have prepended the sentence beginning marker &lt;s&gt; and appended the sentence end marker &lt;/s&gt; so that &lt;s&gt; and &lt;/s&gt;. Let be the word -prefix of the sentence — the words from the beginning of the sentence up to the current position — and the word-p</context>
<context position="12034" citStr="Chelba and Jelinek, 2000" startWordPosition="2005" endWordPosition="2008">he SLM parser enables us to condition the three probabilities on features related to the identity of any exposed head and any structure below the exposed head. Since the number of parses for a given word prefix grows exponentially with , (6) which ensures a proper probability normalization over strings , where is the set of all parses present in our stacks at the current stage . 3.2 N-best EM Training of the SLM Each model component of the SLM —WORDPREDICTOR, TAGGER, CONSTRUCTOR— is initialized from a set of parsed sentences after undergoing headword percolation and binarization. An Nbest EM (Chelba and Jelinek, 2000) variant is then employed to jointly reestimate the model parameters such that the PPL on training data is decreased — the likelihood of the training data under our model is increased. The reduction in PPL is shown experimentally to carry over to the test data. Let denote the joint sequence of with parse structure . The probability of a sequence is, according to Equation 5, the product of the corresponding elementary events. This product form makes the three components of the SLM separable, therefore, we can estimate the parameters separately. According to the EM algorithm, the auxiliary funct</context>
<context position="14317" citStr="Chelba and Jelinek, 2000" startWordPosition="2403" endWordPosition="2406">to get the conditioning variables. In order to take advantage of the ability of the SLM in generating many hidden parses, we need to modify the training criterion for the neural network model. Actually, if we take the EM auxiliary function in Equation 7 and find parameters of the neural network models to maximize , the solution will be very simple. When standard back-propagation is used to optimize Equation 1, , the state space of our model is huge even for relatively short sentences, so we have to use a search strategy that prunes it. One choice is a synchronous multi-stack search algorithm (Chelba and Jelinek, 2000) which is very similar to a beam search. The language model probability assignment for the word at position in the input sentence is made using: the derivative of with respect to the parameters is calculated and used as the direction for the gradient descent algorithm. Since is nothing but a weighted average of the log-likelihood functions, the derivative of with respect to the parameters is then a weighted average of the derivatives of the log-likelihood functions. In practice, we use the SLM with all components modeled by neural networks to generate N-best parses in the E step, and for the M</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>Ciprian Chelba and Frederick Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical Report TR-10-98,</tech>
<institution>Computer Science Group, Harvard University,</institution>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1871" citStr="Chen and Goodman, 1998" startWordPosition="285" endWordPosition="288">oduction In many systems dealing with natural speech or language such as Automatic Speech Recognition and ✄This work was supported by the National Science Foundation under grants No.IIS-9982329 and No.IIS-0085940. Statistical Machine Translation, a language model is a crucial component for searching in the often prohibitively large hypothesis space. Most of the state-of-the-art systems use n-gram language models, which are simple and effective most of the time. Many smoothing techniques that improve language model probability estimation have been proposed and studied in the n-gram literature (Chen and Goodman, 1998). Recent efforts have studied various ways of using information from a longer context span than that usually captured by normal n-gram language models, as well as ways of using syntactical information that is not available to the word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture u</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Computer Science Group, Harvard University, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad Emami</author>
<author>Peng Xu</author>
<author>Frederick Jelinek</author>
</authors>
<title>Using a connectionist model in a syntactical based language model.</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<location>Hong Kong, China,</location>
<contexts>
<context position="3536" citStr="Emami et al., 2003" startWordPosition="553" endWordPosition="556"> of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used. It has been shown that this method improves significantly on regular n-gram models in perplexity (Bengio et al., 2001). The ability of the method to accommodate longer contexts is most appealing, since experiments have shown consistent improvements in PPL when the context of one of the components of the SLM is increased in length (Emami et al., 2003). Moreover, because the SLM provides an EM training procedure for its components, the connectionist models can also be improved by the EM training. In this paper, we will study the impact of neural network modeling on the SLM, when all of its three components are modeled with this approach. An EM training procedure will be outlined and applied to further training of the neural network models. 2 A Probabilistic Neural Network Model Recently, a relatively new type of language model has been introduced where words are represented by points in a multi-dimensional feature space and the probability </context>
<context position="26662" citStr="Emami et al., 2003" startWordPosition="4481" endWordPosition="4484">30 E0 E1 110 25 30 35 40 45 50 SLM. Overall, the best studied model gave a 21% relative reduction in PPL over the trigram and 8.7% relative reduction over the corresponding KneserNey smoothed SLM. A new EM training procedure improved the performance of the SLM even further when applied to the neural network models. However, reduction in PPL for a language model does not always mean improvement in performance of a real application such as speech recognition. Therefore, future study on applying the neural network enhenced SLM to real applications needs to be carried out. A preliminary study in (Emami et al., 2003) already showed that this approach is promising in reducing the word error rate of a large vocabulary speech recognizer. There are still many interesting problems in applying the neural network enhenced SLM to real applications. Among those, we think the following are of most of interest: Speeding up the stochastic gradient descent algorithm for neural network training: Since training the neural network models is very time-consuming, it is essential to speed up the training in order to carry out many more interesting experiments. Interpreting the word representations learned in this framework:</context>
</contexts>
<marker>Emami, Xu, Jelinek, 2003</marker>
<rawString>Ahmad Emami, Peng Xu, and Frederick Jelinek. 2003. Using a connectionist model in a syntactical based language model. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, Hong Kong, China, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<tech>Technical Report MSR-TR-2001-72,</tech>
<institution>Machine Learning and Applied Statistics Group, Microsoft Research,</institution>
<location>Redmond, WA.</location>
<contexts>
<context position="16753" citStr="Goodman, 2001" startWordPosition="2816" endWordPosition="2817"> the parse (such like traces) are ignored. The word vocabulary contains 10k words including a special token for unknown words. There are 40 items in the part-of-speech set and 54 items in the non-terminal set, respectively. All of the experimental results in this section are based on this corpus and split, unless otherwise stated. 5.1 Getting a Better Baseline Since better performance of the SLM was reported recently in (Kim et al., 2001) by using Kneser-Ney smoothing, we first improved the baseline model by using a variant of Kneser-Ney smoothing: the interpolated Kneser-Ney smoothing as in (Goodman, 2001), which is also implemented in the SRILM toolkit (Stolcke, 2002). There are three notable differences in our implementation of the interpolated Kneser-Ney smoothing related to that in the SRILM toolkit. First, we used one discount constant for each n-gram level, instead of three different discount constants. Second, our discount constant was estimated by maximizing the log-likelihood of the heldout data (assuming the discount constant is between 0 and 1), instead of the Good-Turing estimate. Finally, in order to deal with the fractional counts we encounter during the EM training procedure, we </context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001. A bit of progress in language modeling. Technical Report MSR-TR-2001-72, Machine Learning and Applied Statistics Group, Microsoft Research, Redmond, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Haykin</author>
</authors>
<title>Neural Networks, A Comprehensive Foundation.</title>
<date>1999</date>
<publisher>Prentice-Hall, Inc.,</publisher>
<location>Upper Saddle River, NJ, USA.</location>
<contexts>
<context position="8189" citStr="Haykin, 1999" startWordPosition="1349" endWordPosition="1350">idden layer respectively, and is the number of hidden units. Furthermore, the outputs are given by: where and are weight and bias elements for the output layer before the softmax layer. The softmax layer (equation 4) ensures that the outputs are positive and sum to one, hence are valid probabilities. The output of the neural network, corresponding to the item of the output vocabulary, is exactly the sought conditional probability, that is . 2.2 Training the Neural Network Model Standard back-propagation is used to train the parameters of the neural network as well as the feature vectors. See (Haykin, 1999) for details about neural networks and back-propagation. The function we try to maximize is the log-likelihood of the training data given by equation 1. It is straightforward to compute the gradient of the likelihood function for the feature vectors and the neural network parameters, and hence compute their updates. We should note from equation 4 that the neural network model is similar in functional form to the maximum entropy model (Berger et al., 1996) except that the neural network learns the feature functions by itself from the training data. However, unlike the G/IIS algorithm for the ma</context>
</contexts>
<marker>Haykin, 1999</marker>
<rawString>Simon Haykin. 1999. Neural Networks, A Comprehensive Foundation. Prentice-Hall, Inc., Upper Saddle River, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Neural network probability estimation for broad coverage parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 10th Conference of the EACL,</booktitle>
<pages>131--138</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="3020" citStr="Henderson, 2003" startWordPosition="468" endWordPosition="469">available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in (Xu et al., 2002) when the number of conditioning features was increased. There has been recent promising work in using distributional representation of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used. It has been shown that this method improves significantly on regular n-gram models in perplexity (Bengio et al., 2001). The ability of the method to accommodate longer contexts is most appealing, since experiments have shown consistent improvements in PPL when the context of one of the components of the SLM is increased in length (Emami et al., 2003). Moreover, because the SLM provides an EM training procedure for its components, th</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Neural network probability estimation for broad coverage parsing. In Proceedings of the 10th Conference of the EACL, pages 131–138. Budapest, Hungary, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Woosung Kim</author>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>Smoothing issues in the structured language model.</title>
<date>2001</date>
<booktitle>In Proc. 7th European Conf. on Speech Communication and Technology,</booktitle>
<pages>717--720</pages>
<location>Aalborg, Denmark,</location>
<contexts>
<context position="16581" citStr="Kim et al., 2001" startWordPosition="2788" endWordPosition="2791">d the text in the following ways: numbers in Arabic form are replaced by a single token “N”, punctuations are removed, all words are mapped to lower case, extra information in the parse (such like traces) are ignored. The word vocabulary contains 10k words including a special token for unknown words. There are 40 items in the part-of-speech set and 54 items in the non-terminal set, respectively. All of the experimental results in this section are based on this corpus and split, unless otherwise stated. 5.1 Getting a Better Baseline Since better performance of the SLM was reported recently in (Kim et al., 2001) by using Kneser-Ney smoothing, we first improved the baseline model by using a variant of Kneser-Ney smoothing: the interpolated Kneser-Ney smoothing as in (Goodman, 2001), which is also implemented in the SRILM toolkit (Stolcke, 2002). There are three notable differences in our implementation of the interpolated Kneser-Ney smoothing related to that in the SRILM toolkit. First, we used one discount constant for each n-gram level, instead of three different discount constants. Second, our discount constant was estimated by maximizing the log-likelihood of the heldout data (assuming the discoun</context>
</contexts>
<marker>Kim, Khudanpur, Wu, 2001</marker>
<rawString>Woosung Kim, Sanjeev Khudanpur, and Jun Wu. 2001. Smoothing issues in the structured language model. In Proc. 7th European Conf. on Speech Communication and Technology, pages 717–720, Aalborg, Denmark, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Robust Probabilistic Predictive Syntactic Processing: Motivations, Models and Applications.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University,</institution>
<location>Providence, RI.</location>
<contexts>
<context position="2180" citStr="Roark, 2001" startWordPosition="338" endWordPosition="339">tively large hypothesis space. Most of the state-of-the-art systems use n-gram language models, which are simple and effective most of the time. Many smoothing techniques that improve language model probability estimation have been proposed and studied in the n-gram literature (Chen and Goodman, 1998). Recent efforts have studied various ways of using information from a longer context span than that usually captured by normal n-gram language models, as well as ways of using syntactical information that is not available to the word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in (Xu et al., </context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Robust Probabilistic Predictive Syntactic Processing: Motivations, Models and Applications. Ph.D. thesis, Brown University, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="16817" citStr="Stolcke, 2002" startWordPosition="2826" endWordPosition="2827">ontains 10k words including a special token for unknown words. There are 40 items in the part-of-speech set and 54 items in the non-terminal set, respectively. All of the experimental results in this section are based on this corpus and split, unless otherwise stated. 5.1 Getting a Better Baseline Since better performance of the SLM was reported recently in (Kim et al., 2001) by using Kneser-Ney smoothing, we first improved the baseline model by using a variant of Kneser-Ney smoothing: the interpolated Kneser-Ney smoothing as in (Goodman, 2001), which is also implemented in the SRILM toolkit (Stolcke, 2002). There are three notable differences in our implementation of the interpolated Kneser-Ney smoothing related to that in the SRILM toolkit. First, we used one discount constant for each n-gram level, instead of three different discount constants. Second, our discount constant was estimated by maximizing the log-likelihood of the heldout data (assuming the discount constant is between 0 and 1), instead of the Good-Turing estimate. Finally, in order to deal with the fractional counts we encounter during the EM training procedure, we developed an approximate Kneser-Ney smoothing for fractional cou</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Hoon Van Uystel</author>
<author>Dirk Van Compernolle</author>
<author>Patrick Wambacq</author>
</authors>
<title>Maximum-likelihood training of the plcg-based language model.</title>
<date>2001</date>
<booktitle>In Proceedings of the Automatic Speech Recognition and Understanding Workshop, Madonna di Campiglio,</booktitle>
<location>Trento-Italy,</location>
<marker>Van Uystel, Van Compernolle, Wambacq, 2001</marker>
<rawString>Dong Hoon Van Uystel, Dirk Van Compernolle, and Patrick Wambacq. 2001. Maximum-likelihood training of the plcg-based language model. In Proceedings of the Automatic Speech Recognition and Understanding Workshop, Madonna di Campiglio, Trento-Italy, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>A study on richer syntactic dependencies for structured language modeling.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>191--198</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="2717" citStr="Xu et al., 2002" startWordPosition="417" endWordPosition="420">word-based n-gram models (Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001; Uystel et al., 2001). All these language models are based on stochastic parsing techniques that build up parse trees for the input word sequence and condition the generation of words on syntactical and lexical information available in the parse trees. Since these language models capture useful hierarchical characteristics of language, they can improve the PPL significantly for various tasks. Although more improvement can be achieved by enriching the syntactical dependencies in the structured language model (SLM) (Xu et al., 2002), a severe data sparseness problem was observed in (Xu et al., 2002) when the number of conditioning features was increased. There has been recent promising work in using distributional representation of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used. It has been shown that this method improves significantly on regular n-gram models in perplexity (Bengio et al., 2001). The ability o</context>
<context position="18084" citStr="Xu et al., 2002" startWordPosition="3032" endWordPosition="3035">ls of this approximation, but our approximation becomes the exact Kneser-Ney smoothing when the counts are integers. In order to test our Kneser-Ney smoothing implementation, we built a trigram language model and compared the performance with that from the SRILM. Our PPL was 149.6 and the SRILM PPL was 148.3, therefore, although there are differences in the implementation details, we think our result is close enough to the SRILM. Having tested the smoothing method, we applied it to the SLM. We used the Kneser-Ney smoothing to all components with the same parameterization as the h-2 scheme in (Xu et al., 2002). Table 1 is the comparison between the deleted-interpolation (DI) smoothing and the Kneser-Ney (KN) smoothing. The in Table 1 is the interpolation weight between the SLM and the trigram language model ( =1.0 being the trigram language model). The notation “En” indicates the models were obtained after “n” iterations of EM training&apos;. Since KneserNey smoothing is consistently better than deletedinterpolation, we later on report only the KneserNey smoothing results when comparing to the neural network models. &apos;In particular, E0 simply means initialization. Model =0.0 =0.4 =1.0 KN-E0 143.5 132.3 1</context>
<context position="24246" citStr="Xu et al., 2002" startWordPosition="4063" endWordPosition="4066">mum of 30 iterations and the initial parameter values are those from the the previous iteration. +3gram NN-3H-1OP E0 151.2 118.4 NN-3H-1OP E1 147.9 117.9 KN-3H-1OP E0 139.4 129.0 KN-3H-1OP E1 139.2 129.2 Table 3: EM training results Table 3 shows the PPL results after one EM training iteration for both the neural network models and the approximated Kneser-Ney smoothed models, compared to the results before EM training. For the neural network models, the EM training did improve the PPL further, although not a lot. The improvement from training is consistent with the training results showed in (Xu et al., 2002) where deleted-interpolation smoothing was used for the SLM components. It is worth noting that the approximated Kneser-Ney smoothed models could not improve the PPL after one iteration of EM training. One possible reason is that in order to apply KneserNey smoothing to fractional counts, we had to approximate the discounting. The approximation may degrade the benefit we could have gotten from the EM training. Similarly, the M step in the EM procedure for the neural network models also has the same problem: the stochastic gradient descent algorithm is not guaranteed to converge. This can be cl</context>
</contexts>
<marker>Xu, Chelba, Jelinek, 2002</marker>
<rawString>Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002. A study on richer syntactic dependencies for structured language modeling. In Proceedings of the 40th Annual Meeting of the ACL, pages 191–198, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>