<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003318">
<title confidence="0.963048">
CMU System Combination via Hypothesis Selection for WMT’10
</title>
<author confidence="0.993642">
Almut Silja Hildebrand Stephan Vogel
</author>
<affiliation confidence="0.9974965">
Carnegie Mellon University Carnegie Mellon University
Pittsburgh, USA Pittsburgh, USA
</affiliation>
<email confidence="0.996948">
silja@cs.cmu.edu vogel@cs.cmu.edu
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834714285714">
This paper describes the CMU entry for
the system combination shared task at
WMT’10. Our combination method is hy-
pothesis selection, which uses information
from n-best lists from the input MT sys-
tems, where available. The sentence level
features used are independent from the
MT systems involved. Compared to the
baseline we added source-to-target word
alignment based features and trained sys-
tem weights to our feature set. We com-
bined MT systems for French - English
and German - English using provided data
only.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936157894737">
For the combination of machine translation sys-
tems there have been several approaches described
in recent publications. One uses confusion net-
works formed along a skeleton sentence to com-
bine translation systems as described in (Rosti et
al., 2008) and (Karakos et al., 2008). A different
approach described in (Heafield et al., 2009) is not
keeping the skeleton fixed when aligning the sys-
tems. Another approach selects whole hypotheses
from a combined n-best list (Hildebrand and Vo-
gel, 2008).
Our setup follows the latter approach. We com-
bine the output from the submitted translation sys-
tems, including n-best lists where available, into
one joint n-best list, then calculate a set of fea-
tures consistently for all hypotheses. We use MER
training on the provided development data to de-
termine feature weights and re-rank the joint n-
best list. We train to maximize BLEU.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.832553">
For our entries to the WMT’09 we used the follow-
ing feature groups (in parenthesis are the number
of separate feature values per group):
</bodyText>
<listItem confidence="0.999979714285714">
• Language model scores (3)
• Word lexicon scores (6)
• Sentence length features (3)
• Rank feature (1)
• Normalized n-gram agreement (6)
• Source-target word alignment features (6)
• Trained system weights (no. of systems)
</listItem>
<bodyText confidence="0.999966777777778">
The details on language model and word lexi-
con scores can be found in (Hildebrand and Vogel,
2008) and details on the rank feature and the nor-
malized n-gram agreement can be found in (Hilde-
brand and Vogel, 2009). We use three sentence
length features, which are the ratio of the hypoth-
esis length to the length of the source sentence,
the diversion of this ratio from the overall length
ratio of the bilingual training data and the differ-
ence between the hypothesis length and the av-
erage length of the hypotheses in the n-best list
for the respective source sentence. The system
weights are trained together with the other feature
weights during MERT using a binary feature per
system. To the feature vector for each hypothe-
sis one feature per input system is added; for each
hypothesis one of the features is one, indicating
which system it came from, all others are zero.
</bodyText>
<subsectionHeader confidence="0.979261">
2.1 Source-Target Word Alignment Features
</subsectionHeader>
<bodyText confidence="0.999746">
We trained the IBM word alignment models up
to model 4 using the GIZA++ toolkit (Och and
Ney, 2003) on the bilingual training corpus. Then
a forced alignment algorithm utilizes the trained
models to align each source sentence to each trans-
lation hypothesis in its respective n-best list.
We use the alignment score given by the word
alignment models, the number of unaligned words
</bodyText>
<page confidence="0.981524">
307
</page>
<note confidence="0.452839">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 307–310,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9992285">
and the number of NULL aligned words, all nor-
malized by the sentence length, as three separate
features. We calculate these alignability features
for both language directions.
</bodyText>
<sectionHeader confidence="0.999098" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999935047619048">
In the WMT shared translation task only a very
small number of participants submitted n-best
lists, e.g. in the German-English track there were
only four n-best lists among the 16 submissions.
Our combination method is proven to work signif-
icantly better when n-best lists are available.
For all our experiments on the data from
WMT’09, which was available for system combi-
nation development as well as the WMT’10 shared
task data we used the same setup and the same sta-
tistical models.
To train our language models and word lexica
we only used provided data. We trained the sta-
tistical word lexica on the parallel data provided
for each language pair1. For each combination we
used three language models: a 4-gram language
model trained on the English part of the parallel
training data, a 1.2 giga-word 3-gram language
model trained on the provided monolingual En-
glish data, and an interpolated 5-gram language
model trained on the English GigaWord corpus.
We used the SRILM toolkit (Stolcke, 2002) for
training. We chose to train three separate LMs
for the three corpora, so the feature weight train-
ing can automatically determine the importance of
each corpus for this task. The reason for training
only a 3-gram LM from the wmt10 monolingual
data was simply that there were not sufficient time
and resources available to train a bigger model.
For each of the two language pairs we compared
a combination that used the word alignment fea-
tures, or trained system weights or both of these
feature groups in addition to the features described
in (Hildebrand and Vogel, 2009) which serves a
baseline for this set of experiments.
For combination we tokenized and lowercased
all data, because the n-best lists were submitted
in various formats. Therefore we report the case
insensitive scores here. The combination was op-
timized toward the BLEU metric, therefore TER
results might not be very meaningful here and are
only reported for completeness.
</bodyText>
<footnote confidence="0.663681">
1http://www.statmt.org/wmt10/translation-
task.html#training
</footnote>
<subsectionHeader confidence="0.999345">
3.1 French-English data from WMT’09
</subsectionHeader>
<bodyText confidence="0.999232833333333">
We used 14 systems from the restricted data track
of the WMT’09 including five n-best lists. The
scores of the individual systems for the combina-
tion tuning set range from BLEU 27.93 for the best
to 15.09 for the lowest ranked individual system
(case insensitive evaluation).
</bodyText>
<table confidence="0.995087166666667">
system tune test
best single 27.93 / 56.53 27.21 / 56.99
baseline 30.17 / 54.76 28.89 / 55.74
+ wrd al 30.67 / 54.34 28.69 / 55.67
+ sys weights 29.71 / 55.45 28.07 / 56.18
all features 30.30 / 54.53 28.37 / 55.77
</table>
<tableCaption confidence="0.999821">
Table 1: French-English Results: BLEU / TER
</tableCaption>
<bodyText confidence="0.999978777777778">
The combination outperforms the best single
system by 1.7 BLEU points. Here adding the 14
binary features for training system weights with
MERT hurts the combinations performance on the
unseen data. The reason for this might be the
rather small tuning set of 502 sentences with one
reference. Adding the word alignment features
does not improve the result either, the difference
to the baseline is at the noise level.
</bodyText>
<subsectionHeader confidence="0.99965">
3.2 German-English data from WMT’09
</subsectionHeader>
<bodyText confidence="0.999746714285714">
For our experiments on the development data for
German-English we used the top 12 systems, scor-
ing between BLEU 23.01 and BLEU 16.06, ex-
cluding systems known to use data beyond the pro-
vided data. Within those 12 system outputs were
four n-best lists, three of which were 100-best and
one was 10-best.
</bodyText>
<table confidence="0.999297">
system tune test
best single 23.01 / 60.52 21.44 / 62.33
baseline 26.28 / 58.69 23.62 / 60.49
+ wrd al 26.25 / 59.13 23.42 / 61.11
+ sys weights 26.78 / 58.48 23.28 / 60.80
all features 26.81 / 58.12 23.51 / 60.25
</table>
<tableCaption confidence="0.999055">
Table 2: German-English Results: BLEU / TER
</tableCaption>
<bodyText confidence="0.998794333333333">
Our system combination via hypothesis selec-
tion could improve translation quality by +2.2
BLEU over the best single system on the unseen
test set. Again, the differences between the four
different feature sets are not significant on the un-
seen test set.
</bodyText>
<page confidence="0.994075">
308
</page>
<subsectionHeader confidence="0.6666815">
3.3 French-English WMT’10 system
combination shared task
</subsectionHeader>
<bodyText confidence="0.990804428571429">
Out of 14 systems submitted to the French-English
translation task, we combined the top 11 systems,
the best of which scored 28.58 BLEU and the last
24.16 BLEU on the tuning set. There were only
three n-best lists among the submissions. We in-
cluded up to 100 hypotheses per system in our
joint n-best list.
</bodyText>
<table confidence="0.99938">
system tune test
best sys. 28.58 / 54.17 29.98 / 52.62 / 53.88
baseline 30.67 / 52.62 29.94 / 52.53 / -
+ w. al 30.69 / 52.76 29.97 / 52.76 / 53.76
+ sys w. 30.90 / 52.44 29.79 / 52.84 / 54.05
all feat. 31.10 / 52.06 29.80 / 52.86 / 53.67
</table>
<tableCaption confidence="0.7883775">
Table 3: French-English Results: BLEU / TER /
MaxSim
</tableCaption>
<figure confidence="0.988427516129032">
��� � 10 �
�
�
�
�
�
�
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
M* 258/ 7
L* 100/ 0
K 26/ 0
J 34/ 43
1 54/ 0
H 247/ 55
G* 361/ 65
F 73/ 81
E 47/186
D* 206/274
C 137/ 30
B 45/228
A* 670/1065
</figure>
<bodyText confidence="0.998534090909091">
Our system combination via hypothesis selec-
tion could not improve the translation quality com-
pared to the best single system on the unseen data.
Adding any of the new feature groups to the base-
line does not change the result of the combination
significantly. This result could be explained by the
fact, that due to computational problems and time
constraints we were not able to train our models on
the whole provided French-English training data.
This should only affect the lexicon and word align-
ment feature groups though.
</bodyText>
<subsectionHeader confidence="0.6016475">
3.4 German-English WMT’10 system
combination shared task
</subsectionHeader>
<bodyText confidence="0.998660833333333">
For the German-English combination we used 13
out of the 16 submitted systems, which scored be-
tween BLEU 25.01 to BLEU 19.76 on the tuning
set. Our combination could improve translation
quality by +1.64 BLEU compared to the best sys-
tem.
</bodyText>
<table confidence="0.989223">
system tune test
best sys. 25.01 / 58.34 23.89 / 59.14 / 51.10
baseline 26.47 / 56.89 25.44 / 57.96 / -
+ w. al 26.37 / 57.02 25.25 / 58.34 / 50.72
+ sys w. 27.67 / 56.05 25.53 / 57.70 / 51.06
all feat. 27.66 / 56.35 25.25 / 57.86 / 50.83
</table>
<tableCaption confidence="0.9037065">
Table 4: German-English Results: BLEU / TER /
MaxSim
</tableCaption>
<bodyText confidence="0.803322">
The word alignment features seem to hurt per-
formance slightly, which might be due to the more
baseline sys weights
</bodyText>
<figureCaption confidence="0.732962333333333">
Figure 1: German-English ’10: Contributions of
the individual systems to the final translation, per-
centages and absolute number of hyps chosen.
</figureCaption>
<bodyText confidence="0.99977375">
difficult word alignment between German and En-
glish compared to other language pairs. But this
is not really a strong conclusion, because all dif-
ferences of the results on the unseen data are not
significant.
Figure 1 shows, how many hypotheses were
contributed by the individual systems to the final
translation (unseen data) in the baseline combina-
tion compared with the one with trained system
weights. The systems A to M are ordered by their
BLEU score on the development set. The bars
show percentages of the test set, the numbers listed
next to the systems A to M give the absolute num-
ber of hypotheses chosen from the system for the
two depicted combinations. The systems which
provided n-best lists, marked with a star in the di-
agram, clearly dominate the selection in the base-
line, but this effect is gone when system weights
are used. The dominance of system A in the lat-
ter is to be expected, because it is a whole BLEU
point ahead of the next ranking system on the sys-
tem combination tuning set. In the baseline com-
bination identical hypotheses contributed by dif-
ferent systems have an identical total score. In
</bodyText>
<page confidence="0.998188">
309
</page>
<bodyText confidence="0.9999294">
that case the hypothesis is attributed to all systems
which contributed it. This accounts for the higher
total number of hypotheses shown in the graphic
for the baseline as well as for part of the contri-
butions of the low ranking systems. For example
35 hypotheses were provided identically from two
systems and still four hypotheses were produced
by all 13 systems, for example the sentence: ”aber
es geht auch um wirtschaftliche beziehungen.” -
”but it is also about economic relations .”.
</bodyText>
<sectionHeader confidence="0.999562" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999702227272727">
In this paper we explored new features in our sys-
tem combination system, which performs hypoth-
esis selection. We used hypothesis to source sen-
tence alignment scores as well system weight fea-
tures.
Most systems available for combination did not
submit n-best lists, which decreases the effective-
ness of our combination method significantly.
The reason for not getting an improvement from
word alignment features might be that the top sys-
tems might be using more clever word alignment
strategies than running the GIZA++ toolkit out of
the box. Therefore the alignability according to
these weaker models does not give useful ranking
information for rescoring.
Experiments on different language pairs and
data sets have shown improvements for training
system weights in the past for certain setups.
Combining up to 14 individual translation sys-
tems adds that many features to the feature set for
which weights have to optimized via MERT. The
provided tuning set of 455 sentences with only
one reference is extremely small. It is possible,
that MERT could not reliably determine feature
weights here. In the setup where this feature set
was used successfully, a tuning set of close to 2000
lines with four references was available. It is not
possible to improve the tuning data situation by us-
ing the provided data from last years workshop as
additional tuning data, because the set of systems
submitted is not the same and even the systems
submitted by the same sites might have changed
significantly.
Interesting to note is that looking at the num-
bers, the German-English combination with an
improvement of +1.64 BLEU over the best sin-
gle system seems to have worked much better than
the French-English one with no improvement. But
looking at the preliminary human evaluation result
the picture is opposite: For German-English our
combination is ranked below several of the single
systems and most of the combinations, while for
French-English it tops the list of all systems and
combinations in the workshop.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999562">
We would like to thank the participants in the
WMT’10 shared translation task for providing
their data, especially n-best lists. This work was
partly funded by DARPA under the project GALE
(Grant number #HR0011-06-2-0001).
</bodyText>
<sectionHeader confidence="0.999176" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99988605">
Kenneth Heafield, Greg Hanneman, and Alon Lavie.
2009. Machine translation system combination with
flexible word ordering. In StatMT ’09: Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, pages 56–60, Morristown, NJ, USA.
Association for Computational Linguistics.
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
MT at work: Proceedings of the Eighth Confer-
ence of the Association for Machine Translation in
the Americas, pages 254–261, Waikiki, Hawaii, Oc-
tober. Association for Machine Translation in the
Americas.
Almut Silja Hildebrand and Stephan Vogel. 2009.
CMU system combination for WMT’09. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 47–50, Athens, Greece,
March. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of ACL-08: HLT, Short Papers, pages
81–84, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hy-
pothesis alignment for building confusion networks
with application to machine translation system com-
bination. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 183–186,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference for Spoken Language Processing,
Denver, Colorado, September.
</reference>
<page confidence="0.998614">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762075">
<title confidence="0.832844">CMU System Combination via Hypothesis Selection for WMT’10</title>
<author confidence="0.999036">Almut Silja Hildebrand Stephan Vogel</author>
<affiliation confidence="0.99999">Carnegie Mellon University Carnegie Mellon University</affiliation>
<address confidence="0.986238">Pittsburgh, USA Pittsburgh, USA</address>
<email confidence="0.999519">silja@cs.cmu.eduvogel@cs.cmu.edu</email>
<abstract confidence="0.995129133333333">This paper describes the CMU entry for the system combination shared task at WMT’10. Our combination method is hypothesis selection, which uses information from n-best lists from the input MT systems, where available. The sentence level features used are independent from the MT systems involved. Compared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set. We combined MT systems for French - English and German - English using provided data only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Greg Hanneman</author>
<author>Alon Lavie</author>
</authors>
<title>Machine translation system combination with flexible word ordering.</title>
<date>2009</date>
<booktitle>In StatMT ’09: Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>56--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1090" citStr="Heafield et al., 2009" startWordPosition="161" endWordPosition="164">es used are independent from the MT systems involved. Compared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set. We combined MT systems for French - English and German - English using provided data only. 1 Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. We train to maximize BLEU. 2 Features For our entries to the WMT’09 we used the followi</context>
</contexts>
<marker>Heafield, Hanneman, Lavie, 2009</marker>
<rawString>Kenneth Heafield, Greg Hanneman, and Alon Lavie. 2009. Machine translation system combination with flexible word ordering. In StatMT ’09: Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 56–60, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In MT at work: Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>254--261</pages>
<location>Waikiki, Hawaii,</location>
<contexts>
<context position="1250" citStr="Hildebrand and Vogel, 2008" startWordPosition="186" endWordPosition="190">eights to our feature set. We combined MT systems for French - English and German - English using provided data only. 1 Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. We train to maximize BLEU. 2 Features For our entries to the WMT’09 we used the following feature groups (in parenthesis are the number of separate feature values per group): • Language model scores (3) • Word lexicon scores (6) • Sentence length </context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In MT at work: Proceedings of the Eighth Conference of the Association for Machine Translation in the Americas, pages 254–261, Waikiki, Hawaii, October. Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>CMU system combination for WMT’09.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>47--50</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="2213" citStr="Hildebrand and Vogel, 2009" startWordPosition="352" endWordPosition="356">oint nbest list. We train to maximize BLEU. 2 Features For our entries to the WMT’09 we used the following feature groups (in parenthesis are the number of separate feature values per group): • Language model scores (3) • Word lexicon scores (6) • Sentence length features (3) • Rank feature (1) • Normalized n-gram agreement (6) • Source-target word alignment features (6) • Trained system weights (no. of systems) The details on language model and word lexicon scores can be found in (Hildebrand and Vogel, 2008) and details on the rank feature and the normalized n-gram agreement can be found in (Hildebrand and Vogel, 2009). We use three sentence length features, which are the ratio of the hypothesis length to the length of the source sentence, the diversion of this ratio from the overall length ratio of the bilingual training data and the difference between the hypothesis length and the average length of the hypotheses in the n-best list for the respective source sentence. The system weights are trained together with the other feature weights during MERT using a binary feature per system. To the feature vector for each hypothesis one feature per input system is added; for each hypothesis one of the features is </context>
<context position="5257" citStr="Hildebrand and Vogel, 2009" startWordPosition="855" endWordPosition="858">rd corpus. We used the SRILM toolkit (Stolcke, 2002) for training. We chose to train three separate LMs for the three corpora, so the feature weight training can automatically determine the importance of each corpus for this task. The reason for training only a 3-gram LM from the wmt10 monolingual data was simply that there were not sufficient time and resources available to train a bigger model. For each of the two language pairs we compared a combination that used the word alignment features, or trained system weights or both of these feature groups in addition to the features described in (Hildebrand and Vogel, 2009) which serves a baseline for this set of experiments. For combination we tokenized and lowercased all data, because the n-best lists were submitted in various formats. Therefore we report the case insensitive scores here. The combination was optimized toward the BLEU metric, therefore TER results might not be very meaningful here and are only reported for completeness. 1http://www.statmt.org/wmt10/translationtask.html#training 3.1 French-English data from WMT’09 We used 14 systems from the restricted data track of the WMT’09 including five n-best lists. The scores of the individual systems for</context>
</contexts>
<marker>Hildebrand, Vogel, 2009</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2009. CMU system combination for WMT’09. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 47–50, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using itg-based alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>81--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1031" citStr="Karakos et al., 2008" startWordPosition="152" endWordPosition="155">put MT systems, where available. The sentence level features used are independent from the MT systems involved. Compared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set. We combined MT systems for French - English and German - English using provided data only. 1 Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. We train to maximize BLEU. 2</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using itg-based alignments. In Proceedings of ACL-08: HLT, Short Papers, pages 81–84, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3018" citStr="Och and Ney, 2003" startWordPosition="492" endWordPosition="495">e bilingual training data and the difference between the hypothesis length and the average length of the hypotheses in the n-best list for the respective source sentence. The system weights are trained together with the other feature weights during MERT using a binary feature per system. To the feature vector for each hypothesis one feature per input system is added; for each hypothesis one of the features is one, indicating which system it came from, all others are zero. 2.1 Source-Target Word Alignment Features We trained the IBM word alignment models up to model 4 using the GIZA++ toolkit (Och and Ney, 2003) on the bilingual training corpus. Then a forced alignment algorithm utilizes the trained models to align each source sentence to each translation hypothesis in its respective n-best list. We use the alignment score given by the word alignment models, the number of unaligned words 307 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 307–310, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics and the number of NULL aligned words, all normalized by the sentence length, as three separate features. We calculate these al</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>183--186</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="1004" citStr="Rosti et al., 2008" startWordPosition="147" endWordPosition="150"> n-best lists from the input MT systems, where available. The sentence level features used are independent from the MT systems involved. Compared to the baseline we added source-to-target word alignment based features and trained system weights to our feature set. We combined MT systems for French - English and German - English using provided data only. 1 Introduction For the combination of machine translation systems there have been several approaches described in recent publications. One uses confusion networks formed along a skeleton sentence to combine translation systems as described in (Rosti et al., 2008) and (Karakos et al., 2008). A different approach described in (Heafield et al., 2009) is not keeping the skeleton fixed when aligning the systems. Another approach selects whole hypotheses from a combined n-best list (Hildebrand and Vogel, 2008). Our setup follows the latter approach. We combine the output from the submitted translation systems, including n-best lists where available, into one joint n-best list, then calculate a set of features consistently for all hypotheses. We use MER training on the provided development data to determine feature weights and re-rank the joint nbest list. W</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translation system combination. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 183–186, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference for Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="4682" citStr="Stolcke, 2002" startWordPosition="759" endWordPosition="760">pment as well as the WMT’10 shared task data we used the same setup and the same statistical models. To train our language models and word lexica we only used provided data. We trained the statistical word lexica on the parallel data provided for each language pair1. For each combination we used three language models: a 4-gram language model trained on the English part of the parallel training data, a 1.2 giga-word 3-gram language model trained on the provided monolingual English data, and an interpolated 5-gram language model trained on the English GigaWord corpus. We used the SRILM toolkit (Stolcke, 2002) for training. We chose to train three separate LMs for the three corpora, so the feature weight training can automatically determine the importance of each corpus for this task. The reason for training only a 3-gram LM from the wmt10 monolingual data was simply that there were not sufficient time and resources available to train a bigger model. For each of the two language pairs we compared a combination that used the word alignment features, or trained system weights or both of these feature groups in addition to the features described in (Hildebrand and Vogel, 2009) which serves a baseline </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings International Conference for Spoken Language Processing, Denver, Colorado, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>