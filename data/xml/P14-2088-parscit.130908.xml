<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000857">
<title confidence="0.997639">
Fast Easy Unsupervised Domain Adaptation
with Marginalized Structured Dropout
</title>
<author confidence="0.998428">
Yi Yang and Jacob Eisenstein
</author>
<affiliation confidence="0.998933">
School of Interactive Computing
Georgia Institute of Technology
</affiliation>
<email confidence="0.996211">
{yiyang, jacobe}@gatech.edu
</email>
<sectionHeader confidence="0.994699" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998115">
Unsupervised domain adaptation often re-
lies on transforming the instance represen-
tation. However, most such approaches
are designed for bag-of-words models, and
ignore the structured features present in
many problems in NLP. We propose a
new technique called marginalized struc-
tured dropout, which exploits feature
structure to obtain a remarkably simple
and efficient feature projection. Applied
to the task of fine-grained part-of-speech
tagging on a dataset of historical Por-
tuguese, marginalized structured dropout
yields state-of-the-art accuracy while in-
creasing speed by more than an order-of-
magnitude over previous work.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961161290323">
Unsupervised domain adaptation is a fundamen-
tal problem for natural language processing, as
we hope to apply our systems to datasets unlike
those for which we have annotations. This is par-
ticularly relevant as labeled datasets become stale
in comparison with rapidly evolving social media
writing styles (Eisenstein, 2013), and as there is
increasing interest in natural language processing
for historical texts (Piotrowski, 2012). While a
number of different approaches for domain adap-
tation have been proposed (Pan and Yang, 2010;
Søgaard, 2013), they tend to emphasize bag-of-
words features for classification tasks such as sen-
timent analysis. Consequently, many approaches
rely on each instance having a relatively large
number of active features, and fail to exploit the
structured feature spaces that characterize syn-
tactic tasks such as sequence labeling and pars-
ing (Smith, 2011).
As we will show, substantial efficiency im-
provements can be obtained by designing domain
adaptation methods for learning in structured fea-
ture spaces. We build on work from the deep
learning community, in which denoising autoen-
coders are trained to remove synthetic noise from
the observed instances (Glorot et al., 2011a). By
using the autoencoder to transform the original
feature space, one may obtain a representation
that is less dependent on any individual feature,
and therefore more robust across domains. Chen
et al. (2012) showed that such autoencoders can
be learned even as the noising process is analyt-
ically marginalized; the idea is similar in spirit
to feature noising (Wang et al., 2013). While
the marginalized denoising autoencoder (mDA) is
considerably faster than the original denoising au-
toencoder, it requires solving a system of equa-
tions that can grow very large, as realistic NLP
tasks can involve 105 or more features.
In this paper we investigate noising functions
that are explicitly designed for structured feature
spaces, which are common in NLP. For example,
in part-of-speech tagging, Toutanova et al. (2003)
define several feature “templates”: the current
word, the previous word, the suffix of the current
word, and so on. For each feature template, there
are thousands of binary features. To exploit this
structure, we propose two alternative noising tech-
niques: (1) feature scrambling, which randomly
chooses a feature template and randomly selects
an alternative value within the template, and (2)
structured dropout, which randomly eliminates
all but a single feature template. We show how it
is possible to marginalize over both types of noise,
and find that the solution for structured dropout is
substantially simpler and more efficient than the
mDA approach of Chen et al. (2012), which does
not consider feature structure.
We apply these ideas to fine-grained part-of-
speech tagging on a dataset of Portuguese texts
from the years 1502 to 1836 (Galves and Faria,
2010), training on recent texts and evaluating
</bodyText>
<page confidence="0.963654">
538
</page>
<bodyText confidence="0.969592">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538–544,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
on older documents. Both structure-aware do-
main adaptation algorithms perform as well as
standard dropout — and better than the well-
known structural correspondence learning (SCL)
algorithm (Blitzer et al., 2007) — but structured
dropout is more than an order-of-magnitude faster.
As a secondary contribution of this paper, we
demonstrate the applicability of unsupervised do-
main adaptation to the syntactic analysis of histor-
ical texts.
</bodyText>
<sectionHeader confidence="0.97074" genericHeader="introduction">
2 Model
</sectionHeader>
<bodyText confidence="0.999959625">
In this section we first briefly describe the de-
noising autoencoder (Glorot et al., 2011b), its ap-
plication to domain adaptation, and the analytic
marginalization of noise (Chen et al., 2012). Then
we present three versions of marginalized denois-
ing autoencoders (mDA) by incorporating differ-
ent types of noise, including two new noising pro-
cesses that are designed for structured features.
</bodyText>
<subsectionHeader confidence="0.996665">
2.1 Denoising Autoencoders
</subsectionHeader>
<bodyText confidence="0.981306625">
Assume instances x1, ... , xn, which are drawn
from both the source and target domains. We will
“corrupt” these instances by adding different types
of noise, and denote the corrupted version of xi
by ˜xi. Single-layer denoising autoencoders recon-
struct the corrupted inputs with a projection matrix
W : Rd —* Rd, which is estimated by minimizing
the squared reconstruction loss
</bodyText>
<equation confidence="0.949702">
− W˜xi||2. (1)
</equation>
<bodyText confidence="0.853491666666667">
. ,xn] E Rdxn, and we
˜X, then the loss in (1)
can be written as [(X
</bodyText>
<equation confidence="0.986376">
L(W) = 2ntr − W X)T (X − W X˜)]
.
</equation>
<bodyText confidence="0.96248225">
(2)
In this case, we have the well-known closed-
form solution for this ordinary least square prob-
lem:
</bodyText>
<equation confidence="0.999703">
W = PQ−1, (3)
</equation>
<bodyText confidence="0.99996525">
where Q = X˜˜XT and P = X ˜XT. After ob-
taining the weight matrix W, we can insert non-
linearity into the output of the denoiser, such as
tanh(WX). It is also possible to apply stack-
ing, by passing this vector through another autoen-
coder (Chen et al., 2012). In pilot experiments,
this slowed down estimation and had little effect
on accuracy, so we did not include it.
High-dimensional setting Structured predic-
tion tasks often have much more features than
simple bag-of-words representation, and perfor-
mance relies on the rare features. In a naive im-
plementation of the denoising approach, both P
and Q will be dense matrices with dimension-
ality d x d, which would be roughly 1011 ele-
ments in our experiments. To solve this problem,
Chen et al. (2012) propose to use a set of pivot
features, and train the autoencoder to reconstruct
the pivots from the full set of features. Specifi-
cally, the corrupted input is divided to S subsets
</bodyText>
<equation confidence="0.570156">
˜xi = [(˜x)1T,...,i (˜x)ST] T. We obtain a projec-
</equation>
<bodyText confidence="0.99754725">
tion matrix Ws for each subset by reconstructing
the pivot features from the features in this subset;
we can then use the sum of all reconstructions as
the new features, tanh(ESs=1 WsXs).
Marginalized Denoising Autoencoders In the
standard denoising autoencoder, we need to gen-
erate multiple versions of the corrupted data X˜
to reduce the variance of the solution (Glorot et
al., 2011b). But Chen et al. (2012) show that it
is possible to marginalize over the noise, analyt-
ically computing expectations of both P and Q,
and computing
</bodyText>
<equation confidence="0.999779">
W = E[P]E[Q]−1, (4)
</equation>
<bodyText confidence="0.99991925">
where E[P] = Eni=1 E[xi˜xTi ] and E[Q] =
Eni=1 E[˜xi˜xTi ]. This is equivalent to corrupting
the data m —* oc times. The computation of these
expectations depends on the type of noise.
</bodyText>
<subsectionHeader confidence="0.996547">
2.2 Noise distributions
</subsectionHeader>
<bodyText confidence="0.999916222222222">
Chen et al. (2012) used dropout noise for domain
adaptation, which we briefly review. We then de-
scribe two novel types of noise that are designed
for structured feature spaces, and explain how they
can be marginalized to efficiently compute W.
Dropout noise In dropout noise, each feature is
set to zero with probability p &gt; 0. If we define
the scatter matrix of the uncorrupted input as S =
XXT, the solutions under dropout noise are
</bodyText>
<equation confidence="0.863961727272727">
�
(1 − p)2Sα,β if α =� β
E[Q]α,β = (1 − p)Sα,β if α = β , (5)
and
E[P]α,β = (1 − p)Sα,β, (6)
1
L =
n
i=1
2
||xi
</equation>
<bodyText confidence="0.945263">
If we write X = [x1, . .
write its corrupted version
</bodyText>
<page confidence="0.990826">
539
</page>
<bodyText confidence="0.999994578947368">
where α and Q index two features. The form of
these solutions means that computing W requires
solving a system of equations equal to the num-
ber of features (in the naive implementation), or
several smaller systems of equations (in the high-
dimensional version). Note also that p is a tunable
parameter for this type of noise.
Structured dropout noise In many NLP set-
tings, we have several feature templates, such as
previous-word, middle-word, next-word, etc, with
only one feature per template firing on any token.
We can exploit this structure by using an alterna-
tive dropout scheme: for each token, choose ex-
actly one feature template to keep, and zero out all
other features that consider this token (transition
feature templates such as (yt, yt−1) are not con-
sidered for dropout). Assuming we have K feature
templates, this noise leads to very simple solutions
for the marginalized matrices E[P] and E[Q],
</bodyText>
<equation confidence="0.998354444444444">
� (7)
= 0 if α 7Q
E[Q]α,β 1S =
K
a,a if a
Q
1
E[P]α,β =
K Sα,β, (8)
</equation>
<bodyText confidence="0.977132888888889">
For E[P], we obtain a scaled version of the scat-
ter matrix, because in each instance ˜x, there is ex-
actly a 1/K chance that each individual feature
survives dropout. E[Q] is diagonal, because for
any off-diagonal entry E[Q]α,β, at least one of α
and Q will drop out for every instance. We can
therefore view the projection matrix W as a row-
normalized version of the scatter matrix S. Put
another way, the contribution of Q to the recon-
struction for α is equal to the co-occurence count
of α and Q, divided by the count of Q.
Unlike standard dropout, there are no free
hyper-parameters to tune for structured dropout.
Since E[Q] is a diagonal matrix, we eliminate the
cost of matrix inversion (or of solving a system of
linear equations). Moreover, to extend mDA for
high dimensional data, we no longer need to di-
vide the corrupted input x˜ to several subsets.1
For intuition, consider standard feature dropout
with p = K−1
K . This will look very similar to
structured dropout: the matrix E[P] is identical,
and E[Q] has off-diagonal elements which are
scaled by (1 − p)2, which goes to zero as K is
1E[P] is an r by d matrix, where r is the number of pivots.
large. However, by including these elements, stan-
dard dropout is considerably slower, as we show in
our experiments.
Scrambling noise A third alternative is to
“scramble” the features by randomly selecting al-
ternative features within each template. For a fea-
ture α belonging to a template F, with probability
p we will draw a noise feature Q also belonging
to F, according to some distribution q. In this
work, we use an uniform distribution, in which
qβ = 1
|F |. However, the below solutions will also
hold for other scrambling distributions, such as
mean-preserving distributions.
Again, it is possible to analytically marginal-
ize over this noise. Recall that E[Q] =
Eni=1 E[˜xi˜x&gt; i]. An off-diagonal entry in the ma-
trix ˜x˜x&gt; which involves features α and Q belong-
ing to different templates (Fα =� Fβ) can take four
different values (xi,α denotes feature α in xi):
</bodyText>
<listItem confidence="0.993406125">
• xi,αxi,β if both features are unchanged,
which happens with probability (1 − p)2.
• 1 if both features are chosen as noise features,
which happens with probability p2qαqβ.
• xi,α or xi,β if one feature is unchanged and
the other one is chosen as the noise feature,
which happens with probability p(1 − p)qβ
or p(1 − p)qα.
</listItem>
<bodyText confidence="0.9995202">
The diagonal entries take the first two values
above, with probability 1 − p and pqα respec-
tively. Other entries will be all zero (only one
feature belonging to the same template will fire
in xi). We can use similar reasoning to compute
the expectation of P. With probability (1 − p),
the original features are preserved, and we add the
outer-product xix&gt;i ; with probability p, we add the
outer-product xiq&gt;. Therefore E[P] can be com-
puted as the sum of these terms.
</bodyText>
<sectionHeader confidence="0.999648" genericHeader="background">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999596">
We compare these methods on historical Por-
tuguese part-of-speech tagging, creating domains
over historical epochs.
</bodyText>
<subsectionHeader confidence="0.995239">
3.1 Experiment setup
</subsectionHeader>
<bodyText confidence="0.99996925">
Datasets We use the Tycho Brahe corpus to
evaluate our methods. The corpus contains a total
of 1,480,528 manually tagged words. It uses a set
of 383 tags and is composed of various texts from
</bodyText>
<page confidence="0.975215">
540
</page>
<bodyText confidence="0.9992887">
historical Portuguese, from 1502 to 1836. We di-
vide the texts into fifty-year periods to create dif-
ferent domains. Table 1 presents some statistics of
the datasets. We hold out 5% of data as develop-
ment data to tune parameters. The two most recent
domains (1800-1849 and 1750-1849) are treated
as source domains, and the other domains are tar-
get domains. This scenario is motivated by train-
ing a tagger on a modern newstext corpus and ap-
plying it to historical documents.
</bodyText>
<table confidence="0.995732363636364">
# of Tokens
Dataset
Total Narrative Letters Dissertation Theatre
1800-1849 125719 91582 34137 0 0
1750-1799 202346 57477 84465 0 60404
1700-1749 278846 0 130327 148519 0
1650-1699 248194 83938 115062 49194 0
1600-1649 295154 117515 115252 62387 0
1550-1599 148061 148061 0 0 0
1500-1549 182208 126516 0 55692 0
Overall 1480528 625089 479243 315792 60404
</table>
<tableCaption confidence="0.999876">
Table 1: Statistics of the Tycho Brahe Corpus
</tableCaption>
<bodyText confidence="0.999801388888889">
CRF tagger We use a conditional random field
tagger, choosing CRFsuite because it supports
arbitrary real valued features (Okazaki, 2007),
with SGD optimization. Following the work of
Nogueira Dos Santos et al. (2008) on this dataset,
we apply the feature set of Ratnaparkhi (1996).
There are 16 feature templates and 372,902 fea-
tures in total. Following Blitzer et al. (2006), we
consider pivot features that appear more than 50
times in all the domains. This leads to a total of
1572 pivot features in our experiments.
Methods We compare mDA with three alterna-
tive approaches. We refer to baseline as training
a CRF tagger on the source domain and testing on
the target domain with only base features. We also
include PCA to project the entire dataset onto a
low-dimensional sub-space (while still including
the original features). Finally, we compare against
Structural Correspondence Learning (SCL; Blitzer
et al., 2006), another feature learning algorithm.
In all cases, we include the entire dataset to com-
pute the feature projections; we also conducted ex-
periments using only the test and training data for
feature projections, with very similar results.
Parameters All the hyper-parameters are de-
cided with our development data on the training
set. We try different low dimension K from 10 to
2000 for PCA. Following Blitzer (2008) we per-
form feature centering/normalization, as well as
rescaling for SCL. The best parameters for SCL
are dimensionality K = 25 and rescale factor
α = 5, which are the same as in the original pa-
per. For mDA, the best corruption level is p = 0.9
for dropout noise, and p = 0.1 for scrambling
noise. Structured dropout noise has no free hyper-
parameters.
</bodyText>
<subsectionHeader confidence="0.870562">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.997842">
Table 2 presents results for different domain adap-
tation tasks. We also compute the transfer ra-
tio which is defined as adaptation accuracy, shown in
</bodyText>
<figure confidence="0.386374">
&apos; baseline accuracy &apos;
</figure>
<figureCaption confidence="0.9719">
Figure 1. The generally positive trend of these
</figureCaption>
<bodyText confidence="0.878063545454545">
graphs indicates that adaptation becomes progres-
sively more important as we select test sets that are
more temporally remote from the training data.
In general, mDA outperforms SCL and PCA,
the latter of which shows little improvement over
the base features. The various noising approaches
for mDA give very similar results. However, struc-
tured dropout is orders of magnitude faster than
the alternatives, as shown in Table 3. The scram-
bling noise is most time-consuming, with cost
dominated by a matrix multiplication.
</bodyText>
<figure confidence="0.55969">
mDA
Method PCA SCL
dropout structured scambling
Time 7,779 38,849 8,939 339 327,075
</figure>
<tableCaption confidence="0.933195">
Table 3: Time, in seconds, to compute the feature
transformation
</tableCaption>
<sectionHeader confidence="0.99978" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99996975">
Domain adaptation Most previous work on do-
main adaptation focused on the supervised setting,
in which some labeled data is available in the tar-
get domain (Jiang and Zhai, 2007; Daum´e III,
2007; Finkel and Manning, 2009). Our work fo-
cuses on unsupervised domain adaptation, where
no labeled data is available in the target domain.
Several representation learning methods have been
proposed to solve this problem. In structural corre-
spondence learning (SCL), the induced represen-
tation is based on the task of predicting the pres-
ence of pivot features. Autoencoders apply a sim-
ilar idea, but use the denoised instances as the la-
tent representation (Vincent et al., 2008; Glorot et
al., 2011b; Chen et al., 2012). Within the con-
text of denoising autoencoders, we have focused
</bodyText>
<page confidence="0.992818">
541
</page>
<table confidence="0.998968266666667">
Task baseline PCA SCL mDA
dropout structured scrambling
from 1800-1849
- -+1750 89.12 89.09 89.69 90.08 90.08 90.01
- -+1700 90.43 90.43 91.06 91.56 91.57 91.55
- -+1650 88.45 88.52 87.09 88.69 88.70 88.57
- -+1600 87.56 87.58 88.47 89.60 89.61 89.54
- -+1550 89.66 89.61 90.57 91.39 91.39 91.36
- -+1500 85.58 85.63 86.99 88.96 88.95 88.91
from 1750-1849
- -+1700 94.64 94.62 94.81 95.08 95.08 95.02
- -+1650 91.98 90.97 90.37 90.83 90.84 90.80
- -+1600 92.95 92.91 93.17 93.78 93.78 93.71
- -+1550 93.27 93.21 93.75 94.06 94.05 94.02
- -+1500 89.80 89.75 90.59 91.71 91.71 91.68
</table>
<tableCaption confidence="0.996668">
Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849.
</tableCaption>
<figureCaption confidence="0.98876">
Figure 1: Transfer ratio for adaptation to historical text
</figureCaption>
<bodyText confidence="0.999893677419355">
on dropout noise, which has also been applied as
a general technique for improving the robustness
of machine learning, particularly in neural net-
works (Hinton et al., 2012; Wang et al., 2013).
On the specific problem of sequence labeling,
Xiao and Guo (2013) proposed a supervised do-
main adaptation method by using a log-bilinear
language adaptation model. Dhillon et al. (2011)
presented a spectral method to estimate low di-
mensional context-specific word representations
for sequence labeling. Huang and Yates (2009;
2012) used an HMM model to learn latent rep-
resentations, and then leverage the Posterior Reg-
ularization framework to incorporate specific bi-
ases. Unlike these methods, our approach uses a
standard CRF, but with transformed features.
Historical text Our evaluation concerns syntac-
tic analysis of historical text, which is a topic of in-
creasing interest for NLP (Piotrowski, 2012). Pen-
nacchiotti and Zanzotto (2008) find that part-of-
speech tagging degrades considerably when ap-
plied to a corpus of historical Italian. Moon and
Baldridge (2007) tackle the challenging problem
of tagging Middle English, using techniques for
projecting syntactic annotations across languages.
Prior work on the Tycho Brahe corpus applied su-
pervised learning to a random split of test and
training data (Kepler and Finger, 2006; Dos San-
tos et al., 2008); they did not consider the domain
adaptation problem of training on recent data and
testing on older historical text.
</bodyText>
<sectionHeader confidence="0.969962" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.968668">
Denoising autoencoders provide an intuitive so-
lution for domain adaptation: transform the fea-
tures into a representation that is resistant to the
noise that may characterize the domain adaptation
process. The original implementation of this idea
produced this noise directly (Glorot et al., 2011b);
later work showed that dropout noise could be an-
alytically marginalized (Chen et al., 2012). We
take another step towards simplicity by showing
that structured dropout can make marginalization
even easier, obtaining dramatic speedups without
sacrificing accuracy.
Acknowledgments : We thank the reviewers for
useful feedback. This research was supported by
National Science Foundation award 1349837.
</bodyText>
<page confidence="0.994599">
542
</page>
<sectionHeader confidence="0.983262" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709691588785">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’06, pages 120–128, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Association for Computational Linguis-
tics, Prague, Czech Republic.
John Blitzer. 2008. Domain Adaptation of Natural
Language Processing Systems. Ph.D. thesis, Uni-
versity of Pennsylvania.
Minmin Chen, Zhixiang Xu, Kilian Weinberger, and
Fei Sha. 2012. Marginalized denoising autoen-
coders for domain adaptation. In John Langford and
Joelle Pineau, editors, Proceedings of the 29th Inter-
national Conference on Machine Learning (ICML-
12), ICML ’12, pages 767–774. ACM, New York,
NY, USA, July.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In ACL, volume 1785, page 1787.
Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar.
2011. Multi-view learning of word embeddings via
cca. In NIPS, volume 24, pages 199–207.
C´ıcero Nogueira Dos Santos, Ruy L Milidi´u, and
Ra´ul P Renter´ıa. 2008. Portuguese part-of-speech
tagging using entropy guided transformation learn-
ing. In Computational Processing of the Portuguese
Language, pages 143–152. Springer.
Jacob Eisenstein. 2013. What to do about bad lan-
guage on the internet. In Proceedings of NAACL,
Atlanta, GA.
Jenny Rose Finkel and Christopher D Manning. 2009.
Hierarchical bayesian domain adaptation. In Pro-
ceedings of Human Language Technologies: The
2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 602–610. Association for Computa-
tional Linguistics.
Charlotte Galves and Pablo Faria. 2010. Ty-
cho Brahe Parsed Corpus of Historical Por-
tuguese. http://www.tycho.iel.unicamp.br/ ty-
cho/corpus/en/index.html.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011a. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&amp;CP Vol-
ume, volume 15, pages 315–323.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011b. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 513–520.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,
Ilya Sutskever, and Ruslan R Salakhutdinov. 2012.
Improving neural networks by preventing co-
adaptation of feature detectors. arXiv preprint
arXiv:1207.0580.
Fei Huang and Alexander Yates. 2009. Distribu-
tional representations for handling sparsity in super-
vised sequence-labeling. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 1-
Volume 1, pages 495–503. Association for Compu-
tational Linguistics.
Fei Huang and Alexander Yates. 2012. Biased rep-
resentation learning for domain adaptation. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1313–1323. Association for Computational Linguis-
tics.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in nlp. In ACL,
volume 2007, page 22.
F´abio N Kepler and Marcelo Finger. 2006. Comparing
two markov methods for part-of-speech tagging of
portuguese. In Advances in Artificial Intelligence-
IBERAMIA-SBIA 2006, pages 482–491. Springer.
Taesun Moon and Jason Baldridge. 2007. Part-of-
speech tagging for middle english through align-
ment and projection of parallel diachronic texts. In
EMNLP-CoNLL, pages 390–399.
C´ıcero Nogueira Dos Santos, Ruy L. Milidi´u, and
Ra´ul P. Renter´ıa. 2008. Portuguese part-of-speech
tagging using entropy guided transformation learn-
ing. In Proceedings of the 8th international con-
ference on Computational Processing of the Por-
tuguese Language, PROPOR ’08, pages 143–152,
Berlin, Heidelberg. Springer-Verlag.
Naoaki Okazaki. 2007. Crfsuite: a fast implementa-
tion of conditional random fields (crfs).
Sinno Jialin Pan and Qiang Yang. 2010. A survey on
transfer learning. Knowledge and Data Engineer-
ing, IEEE Transactions on, 22(10):1345–1359.
Marco Pennacchiotti and Fabio Massimo Zanzotto.
2008. Natural language processing across time:
An empirical investigation on italian. In Advances
in Natural Language Processing, pages 371–382.
Springer.
Michael Piotrowski. 2012. Natural language process-
ing for historical texts. Synthesis Lectures on Hu-
man Language Technologies, 5(2):1–157.
</reference>
<page confidence="0.986405">
543
</page>
<reference confidence="0.999351783783784">
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, April 16.
Noah A Smith. 2011. Linguistic structure prediction.
Synthesis Lectures on Human Language Technolo-
gies, 4(2):1–274.
Anders Søgaard. 2013. Semi-supervised learning and
domain adaptation in natural language processing.
Synthesis Lectures on Human Language Technolo-
gies, 6(2):1–103.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th international
conference on Machine learning, pages 1096–1103.
ACM.
Sida I. Wang, Mengqiu Wang, Stefan Wager, Percy
Liang, and Christopher D. Manning. 2013. Fea-
ture noising for log-linear structured prediction. In
Empirical Methods in Natural Language Processing
(EMNLP).
Min Xiao and Yuhong Guo. 2013. Domain adapta-
tion for sequence labeling tasks with a probabilis-
tic language adaptation model. In Sanjoy Dasgupta
and David Mcallester, editors, Proceedings of the
30th International Conference on Machine Learn-
ing (ICML-13), volume 28, pages 293–301. JMLR
Workshop and Conference Proceedings.
</reference>
<page confidence="0.998264">
544
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948660">
<title confidence="0.9973655">Fast Easy Unsupervised Domain with Marginalized Structured Dropout</title>
<author confidence="0.999268">Yang Eisenstein</author>
<affiliation confidence="0.9998465">School of Interactive Computing Georgia Institute of Technology</affiliation>
<abstract confidence="0.997319117647059">Unsupervised domain adaptation often relies on transforming the instance representation. However, most such approaches are designed for bag-of-words models, and ignore the structured features present in many problems in NLP. We propose a technique called strucwhich exploits feature structure to obtain a remarkably simple and efficient feature projection. Applied to the task of fine-grained part-of-speech tagging on a dataset of historical Portuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-ofmagnitude over previous work.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="13226" citStr="Blitzer et al. (2006)" startWordPosition="2211" endWordPosition="2214">30327 148519 0 1650-1699 248194 83938 115062 49194 0 1600-1649 295154 117515 115252 62387 0 1550-1599 148061 148061 0 0 0 1500-1549 182208 126516 0 55692 0 Overall 1480528 625089 479243 315792 60404 Table 1: Statistics of the Tycho Brahe Corpus CRF tagger We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features (Okazaki, 2007), with SGD optimization. Following the work of Nogueira Dos Santos et al. (2008) on this dataset, we apply the feature set of Ratnaparkhi (1996). There are 16 feature templates and 372,902 features in total. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments. Methods We compare mDA with three alternative approaches. We refer to baseline as training a CRF tagger on the source domain and testing on the target domain with only base features. We also include PCA to project the entire dataset onto a low-dimensional sub-space (while still including the original features). Finally, we compare against Structural Correspondence Learning (SCL; Blitzer et al., 2006), another feature learning algorithm. In all cases, </context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ’06, pages 120–128, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4219" citStr="Blitzer et al., 2007" startWordPosition="630" endWordPosition="633">ructure. We apply these ideas to fine-grained part-ofspeech tagging on a dataset of Portuguese texts from the years 1502 to 1836 (Galves and Faria, 2010), training on recent texts and evaluating 538 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538–544, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural correspondence learning (SCL) algorithm (Blitzer et al., 2007) — but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts. 2 Model In this section we first briefly describe the denoising autoencoder (Glorot et al., 2011b), its application to domain adaptation, and the analytic marginalization of noise (Chen et al., 2012). Then we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed </context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Association for Computational Linguistics, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
</authors>
<title>Domain Adaptation of Natural Language Processing Systems.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="14189" citStr="Blitzer (2008)" startWordPosition="2369" endWordPosition="2370">o project the entire dataset onto a low-dimensional sub-space (while still including the original features). Finally, we compare against Structural Correspondence Learning (SCL; Blitzer et al., 2006), another feature learning algorithm. In all cases, we include the entire dataset to compute the feature projections; we also conducted experiments using only the test and training data for feature projections, with very similar results. Parameters All the hyper-parameters are decided with our development data on the training set. We try different low dimension K from 10 to 2000 for PCA. Following Blitzer (2008) we perform feature centering/normalization, as well as rescaling for SCL. The best parameters for SCL are dimensionality K = 25 and rescale factor α = 5, which are the same as in the original paper. For mDA, the best corruption level is p = 0.9 for dropout noise, and p = 0.1 for scrambling noise. Structured dropout noise has no free hyperparameters. 3.2 Results Table 2 presents results for different domain adaptation tasks. We also compute the transfer ratio which is defined as adaptation accuracy, shown in &apos; baseline accuracy &apos; Figure 1. The generally positive trend of these graphs indicates</context>
</contexts>
<marker>Blitzer, 2008</marker>
<rawString>John Blitzer. 2008. Domain Adaptation of Natural Language Processing Systems. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Zhixiang Xu</author>
<author>Kilian Weinberger</author>
<author>Fei Sha</author>
</authors>
<title>Marginalized denoising autoencoders for domain adaptation. In</title>
<date>2012</date>
<booktitle>Proceedings of the 29th International Conference on Machine Learning (ICML12), ICML ’12,</booktitle>
<pages>767--774</pages>
<editor>John Langford and Joelle Pineau, editors,</editor>
<publisher>ACM,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="2272" citStr="Chen et al. (2012)" startWordPosition="331" endWordPosition="334">hat characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et a</context>
<context position="3562" citStr="Chen et al. (2012)" startWordPosition="534" endWordPosition="537">ious word, the suffix of the current word, and so on. For each feature template, there are thousands of binary features. To exploit this structure, we propose two alternative noising techniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template. We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is substantially simpler and more efficient than the mDA approach of Chen et al. (2012), which does not consider feature structure. We apply these ideas to fine-grained part-ofspeech tagging on a dataset of Portuguese texts from the years 1502 to 1836 (Galves and Faria, 2010), training on recent texts and evaluating 538 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538–544, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural corre</context>
<context position="5737" citStr="Chen et al., 2012" startWordPosition="895" endWordPosition="898">ruct the corrupted inputs with a projection matrix W : Rd —* Rd, which is estimated by minimizing the squared reconstruction loss − W˜xi||2. (1) . ,xn] E Rdxn, and we ˜X, then the loss in (1) can be written as [(X L(W) = 2ntr − W X)T (X − W X˜)] . (2) In this case, we have the well-known closedform solution for this ordinary least square problem: W = PQ−1, (3) where Q = X˜˜XT and P = X ˜XT. After obtaining the weight matrix W, we can insert nonlinearity into the output of the denoiser, such as tanh(WX). It is also possible to apply stacking, by passing this vector through another autoencoder (Chen et al., 2012). In pilot experiments, this slowed down estimation and had little effect on accuracy, so we did not include it. High-dimensional setting Structured prediction tasks often have much more features than simple bag-of-words representation, and performance relies on the rare features. In a naive implementation of the denoising approach, both P and Q will be dense matrices with dimensionality d x d, which would be roughly 1011 elements in our experiments. To solve this problem, Chen et al. (2012) propose to use a set of pivot features, and train the autoencoder to reconstruct the pivots from the fu</context>
<context position="7246" citStr="Chen et al. (2012)" startWordPosition="1153" endWordPosition="1156">ures, tanh(ESs=1 WsXs). Marginalized Denoising Autoencoders In the standard denoising autoencoder, we need to generate multiple versions of the corrupted data X˜ to reduce the variance of the solution (Glorot et al., 2011b). But Chen et al. (2012) show that it is possible to marginalize over the noise, analytically computing expectations of both P and Q, and computing W = E[P]E[Q]−1, (4) where E[P] = Eni=1 E[xi˜xTi ] and E[Q] = Eni=1 E[˜xi˜xTi ]. This is equivalent to corrupting the data m —* oc times. The computation of these expectations depends on the type of noise. 2.2 Noise distributions Chen et al. (2012) used dropout noise for domain adaptation, which we briefly review. We then describe two novel types of noise that are designed for structured feature spaces, and explain how they can be marginalized to efficiently compute W. Dropout noise In dropout noise, each feature is set to zero with probability p &gt; 0. If we define the scatter matrix of the uncorrupted input as S = XXT, the solutions under dropout noise are � (1 − p)2Sα,β if α =� β E[Q]α,β = (1 − p)Sα,β if α = β , (5) and E[P]α,β = (1 − p)Sα,β, (6) 1 L = n i=1 2 ||xi If we write X = [x1, . . write its corrupted version 539 where α and Q </context>
<context position="16167" citStr="Chen et al., 2012" startWordPosition="2696" endWordPosition="2699"> in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused 541 Task baseline PCA SCL mDA dropout structured scrambling from 1800-1849 - -+1750 89.12 89.09 89.69 90.08 90.08 90.01 - -+1700 90.43 90.43 91.06 91.56 91.57 91.55 - -+1650 88.45 88.52 87.09 88.69 88.70 88.57 - -+1600 87.56 87.58 88.47 89.60 89.61 89.54 - -+1550 89.66 89.61 90.57 91.39 91.39 91.36 - -+1500 85.58 85.63 86.99 88.96 88.95 88.91 from 1750-1849 - -+1700 94.64 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71 - -+1550 93.27 93.21 93.75 94.06 94.05 94</context>
</contexts>
<marker>Chen, Xu, Weinberger, Sha, 2012</marker>
<rawString>Minmin Chen, Zhixiang Xu, Kilian Weinberger, and Fei Sha. 2012. Marginalized denoising autoencoders for domain adaptation. In John Langford and Joelle Pineau, editors, Proceedings of the 29th International Conference on Machine Learning (ICML12), ICML ’12, pages 767–774. ACM, New York, NY, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>volume</volume>
<pages>1785--1787</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In ACL, volume 1785, page 1787.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Multi-view learning of word embeddings via cca. In</title>
<date>2011</date>
<booktitle>NIPS,</booktitle>
<volume>24</volume>
<pages>199--207</pages>
<contexts>
<context position="17344" citStr="Dhillon et al. (2011)" startWordPosition="2891" endWordPosition="2894">71 - -+1550 93.27 93.21 93.75 94.06 94.05 94.02 - -+1500 89.80 89.75 90.59 91.71 91.71 91.68 Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849. Figure 1: Transfer ratio for adaptation to historical text on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increasing interest for NLP (Piotrowski, 2012). Pennacchiotti and Zanzotto (2008) find that part-ofspeech tagging degrades cons</context>
</contexts>
<marker>Dhillon, Foster, Ungar, 2011</marker>
<rawString>Paramveer S Dhillon, Dean P Foster, and Lyle H Ungar. 2011. Multi-view learning of word embeddings via cca. In NIPS, volume 24, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero Nogueira Dos Santos</author>
<author>Ruy L Milidi´u</author>
<author>Ra´ul P Renter´ıa</author>
</authors>
<title>Portuguese part-of-speech tagging using entropy guided transformation learning.</title>
<date>2008</date>
<booktitle>In Computational Processing of the Portuguese Language,</booktitle>
<pages>143--152</pages>
<publisher>Springer.</publisher>
<marker>Santos, Milidi´u, Renter´ıa, 2008</marker>
<rawString>C´ıcero Nogueira Dos Santos, Ruy L Milidi´u, and Ra´ul P Renter´ıa. 2008. Portuguese part-of-speech tagging using entropy guided transformation learning. In Computational Processing of the Portuguese Language, pages 143–152. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>What to do about bad language on the internet.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="1174" citStr="Eisenstein, 2013" startWordPosition="163" endWordPosition="164">imple and efficient feature projection. Applied to the task of fine-grained part-of-speech tagging on a dataset of historical Portuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-ofmagnitude over previous work. 1 Introduction Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial effic</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of NAACL, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical bayesian domain adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>602--610</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15677" citStr="Finkel and Manning, 2009" startWordPosition="2616" endWordPosition="2619">approaches for mDA give very similar results. However, structured dropout is orders of magnitude faster than the alternatives, as shown in Table 3. The scrambling noise is most time-consuming, with cost dominated by a matrix multiplication. mDA Method PCA SCL dropout structured scambling Time 7,779 38,849 8,939 339 327,075 Table 3: Time, in seconds, to compute the feature transformation 4 Related Work Domain adaptation Most previous work on domain adaptation focused on the supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused 541 Task baseline PCA SCL mDA dropout structur</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D Manning. 2009. Hierarchical bayesian domain adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 602–610. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlotte Galves</author>
<author>Pablo Faria</author>
</authors>
<title>Tycho Brahe Parsed Corpus of Historical Portuguese.</title>
<date>2010</date>
<note>http://www.tycho.iel.unicamp.br/ tycho/corpus/en/index.html.</note>
<contexts>
<context position="3751" citStr="Galves and Faria, 2010" startWordPosition="565" endWordPosition="568">echniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template. We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is substantially simpler and more efficient than the mDA approach of Chen et al. (2012), which does not consider feature structure. We apply these ideas to fine-grained part-ofspeech tagging on a dataset of Portuguese texts from the years 1502 to 1836 (Galves and Faria, 2010), training on recent texts and evaluating 538 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 538–544, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural correspondence learning (SCL) algorithm (Blitzer et al., 2007) — but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the a</context>
</contexts>
<marker>Galves, Faria, 2010</marker>
<rawString>Charlotte Galves and Pablo Faria. 2010. Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.unicamp.br/ tycho/corpus/en/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Deep sparse rectifier networks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume,</booktitle>
<volume>15</volume>
<pages>315--323</pages>
<contexts>
<context position="2060" citStr="Glorot et al., 2011" startWordPosition="298" endWordPosition="301">res for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can invo</context>
<context position="4541" citStr="Glorot et al., 2011" startWordPosition="681" endWordPosition="684">ore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural correspondence learning (SCL) algorithm (Blitzer et al., 2007) — but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts. 2 Model In this section we first briefly describe the denoising autoencoder (Glorot et al., 2011b), its application to domain adaptation, and the analytic marginalization of noise (Chen et al., 2012). Then we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed for structured features. 2.1 Denoising Autoencoders Assume instances x1, ... , xn, which are drawn from both the source and target domains. We will “corrupt” these instances by adding different types of noise, and denote the corrupted version of xi by ˜xi. Single-layer denoising autoencoders reconstruct the corrupted inp</context>
<context position="6849" citStr="Glorot et al., 2011" startWordPosition="1082" endWordPosition="1085">12) propose to use a set of pivot features, and train the autoencoder to reconstruct the pivots from the full set of features. Specifically, the corrupted input is divided to S subsets ˜xi = [(˜x)1T,...,i (˜x)ST] T. We obtain a projection matrix Ws for each subset by reconstructing the pivot features from the features in this subset; we can then use the sum of all reconstructions as the new features, tanh(ESs=1 WsXs). Marginalized Denoising Autoencoders In the standard denoising autoencoder, we need to generate multiple versions of the corrupted data X˜ to reduce the variance of the solution (Glorot et al., 2011b). But Chen et al. (2012) show that it is possible to marginalize over the noise, analytically computing expectations of both P and Q, and computing W = E[P]E[Q]−1, (4) where E[P] = Eni=1 E[xi˜xTi ] and E[Q] = Eni=1 E[˜xi˜xTi ]. This is equivalent to corrupting the data m —* oc times. The computation of these expectations depends on the type of noise. 2.2 Noise distributions Chen et al. (2012) used dropout noise for domain adaptation, which we briefly review. We then describe two novel types of noise that are designed for structured feature spaces, and explain how they can be marginalized to </context>
<context position="16146" citStr="Glorot et al., 2011" startWordPosition="2692" endWordPosition="2695">he supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused 541 Task baseline PCA SCL mDA dropout structured scrambling from 1800-1849 - -+1750 89.12 89.09 89.69 90.08 90.08 90.01 - -+1700 90.43 90.43 91.06 91.56 91.57 91.55 - -+1650 88.45 88.52 87.09 88.69 88.70 88.57 - -+1600 87.56 87.58 88.47 89.60 89.61 89.54 - -+1550 89.66 89.61 90.57 91.39 91.39 91.36 - -+1500 85.58 85.63 86.99 88.96 88.95 88.91 from 1750-1849 - -+1700 94.64 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71 - -+1550 93.27 93.21</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011a. Deep sparse rectifier networks. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics. JMLR W&amp;CP Volume, volume 15, pages 315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML-11),</booktitle>
<pages>513--520</pages>
<contexts>
<context position="2060" citStr="Glorot et al., 2011" startWordPosition="298" endWordPosition="301">res for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can invo</context>
<context position="4541" citStr="Glorot et al., 2011" startWordPosition="681" endWordPosition="684">ore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics on older documents. Both structure-aware domain adaptation algorithms perform as well as standard dropout — and better than the wellknown structural correspondence learning (SCL) algorithm (Blitzer et al., 2007) — but structured dropout is more than an order-of-magnitude faster. As a secondary contribution of this paper, we demonstrate the applicability of unsupervised domain adaptation to the syntactic analysis of historical texts. 2 Model In this section we first briefly describe the denoising autoencoder (Glorot et al., 2011b), its application to domain adaptation, and the analytic marginalization of noise (Chen et al., 2012). Then we present three versions of marginalized denoising autoencoders (mDA) by incorporating different types of noise, including two new noising processes that are designed for structured features. 2.1 Denoising Autoencoders Assume instances x1, ... , xn, which are drawn from both the source and target domains. We will “corrupt” these instances by adding different types of noise, and denote the corrupted version of xi by ˜xi. Single-layer denoising autoencoders reconstruct the corrupted inp</context>
<context position="6849" citStr="Glorot et al., 2011" startWordPosition="1082" endWordPosition="1085">12) propose to use a set of pivot features, and train the autoencoder to reconstruct the pivots from the full set of features. Specifically, the corrupted input is divided to S subsets ˜xi = [(˜x)1T,...,i (˜x)ST] T. We obtain a projection matrix Ws for each subset by reconstructing the pivot features from the features in this subset; we can then use the sum of all reconstructions as the new features, tanh(ESs=1 WsXs). Marginalized Denoising Autoencoders In the standard denoising autoencoder, we need to generate multiple versions of the corrupted data X˜ to reduce the variance of the solution (Glorot et al., 2011b). But Chen et al. (2012) show that it is possible to marginalize over the noise, analytically computing expectations of both P and Q, and computing W = E[P]E[Q]−1, (4) where E[P] = Eni=1 E[xi˜xTi ] and E[Q] = Eni=1 E[˜xi˜xTi ]. This is equivalent to corrupting the data m —* oc times. The computation of these expectations depends on the type of noise. 2.2 Noise distributions Chen et al. (2012) used dropout noise for domain adaptation, which we briefly review. We then describe two novel types of noise that are designed for structured feature spaces, and explain how they can be marginalized to </context>
<context position="16146" citStr="Glorot et al., 2011" startWordPosition="2692" endWordPosition="2695">he supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused 541 Task baseline PCA SCL mDA dropout structured scrambling from 1800-1849 - -+1750 89.12 89.09 89.69 90.08 90.08 90.01 - -+1700 90.43 90.43 91.06 91.56 91.57 91.55 - -+1650 88.45 88.52 87.09 88.69 88.70 88.57 - -+1600 87.56 87.58 88.47 89.60 89.61 89.54 - -+1550 89.66 89.61 90.57 91.39 91.39 91.36 - -+1500 85.58 85.63 86.99 88.96 88.95 88.91 from 1750-1849 - -+1700 94.64 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71 - -+1550 93.27 93.21</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011b. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Ruslan R Salakhutdinov</author>
</authors>
<title>Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</title>
<date>2012</date>
<contexts>
<context position="17137" citStr="Hinton et al., 2012" startWordPosition="2858" endWordPosition="2861"> 91.39 91.36 - -+1500 85.58 85.63 86.99 88.96 88.95 88.91 from 1750-1849 - -+1700 94.64 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71 - -+1550 93.27 93.21 93.75 94.06 94.05 94.02 - -+1500 89.80 89.75 90.59 91.71 91.71 91.68 Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849. Figure 1: Transfer ratio for adaptation to historical text on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our</context>
</contexts>
<marker>Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov, 2012</marker>
<rawString>Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing coadaptation of feature detectors. arXiv preprint arXiv:1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequence-labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>495--503</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17483" citStr="Huang and Yates (2009" startWordPosition="2910" endWordPosition="2913">labeled data in 1800-1849, and in 1750-1849. Figure 1: Transfer ratio for adaptation to historical text on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increasing interest for NLP (Piotrowski, 2012). Pennacchiotti and Zanzotto (2008) find that part-ofspeech tagging degrades considerably when applied to a corpus of historical Italian. Moon and Baldridge (2007) tackle the challenging problem of tagging Middle English</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 495–503. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Alexander Yates</author>
</authors>
<title>Biased representation learning for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1313--1323</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Huang, Yates, 2012</marker>
<rawString>Fei Huang and Alexander Yates. 2012. Biased representation learning for domain adaptation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1313–1323. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<volume>volume</volume>
<pages>22</pages>
<contexts>
<context position="15632" citStr="Jiang and Zhai, 2007" startWordPosition="2609" endWordPosition="2612"> the base features. The various noising approaches for mDA give very similar results. However, structured dropout is orders of magnitude faster than the alternatives, as shown in Table 3. The scrambling noise is most time-consuming, with cost dominated by a matrix multiplication. mDA Method PCA SCL dropout structured scambling Time 7,779 38,849 8,939 339 327,075 Table 3: Time, in seconds, to compute the feature transformation 4 Related Work Domain adaptation Most previous work on domain adaptation focused on the supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused 5</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In ACL, volume 2007, page 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F´abio N Kepler</author>
<author>Marcelo Finger</author>
</authors>
<title>Comparing two markov methods for part-of-speech tagging of portuguese.</title>
<date>2006</date>
<booktitle>In Advances in Artificial IntelligenceIBERAMIA-SBIA</booktitle>
<pages>482--491</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="18290" citStr="Kepler and Finger, 2006" startWordPosition="3034" endWordPosition="3037">ses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increasing interest for NLP (Piotrowski, 2012). Pennacchiotti and Zanzotto (2008) find that part-ofspeech tagging degrades considerably when applied to a corpus of historical Italian. Moon and Baldridge (2007) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages. Prior work on the Tycho Brahe corpus applied supervised learning to a random split of test and training data (Kepler and Finger, 2006; Dos Santos et al., 2008); they did not consider the domain adaptation problem of training on recent data and testing on older historical text. 5 Conclusion and Future Work Denoising autoencoders provide an intuitive solution for domain adaptation: transform the features into a representation that is resistant to the noise that may characterize the domain adaptation process. The original implementation of this idea produced this noise directly (Glorot et al., 2011b); later work showed that dropout noise could be analytically marginalized (Chen et al., 2012). We take another step towards simpl</context>
</contexts>
<marker>Kepler, Finger, 2006</marker>
<rawString>F´abio N Kepler and Marcelo Finger. 2006. Comparing two markov methods for part-of-speech tagging of portuguese. In Advances in Artificial IntelligenceIBERAMIA-SBIA 2006, pages 482–491. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taesun Moon</author>
<author>Jason Baldridge</author>
</authors>
<title>Part-ofspeech tagging for middle english through alignment and projection of parallel diachronic texts.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>390--399</pages>
<contexts>
<context position="18026" citStr="Moon and Baldridge (2007)" startWordPosition="2994" endWordPosition="2997">context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increasing interest for NLP (Piotrowski, 2012). Pennacchiotti and Zanzotto (2008) find that part-ofspeech tagging degrades considerably when applied to a corpus of historical Italian. Moon and Baldridge (2007) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages. Prior work on the Tycho Brahe corpus applied supervised learning to a random split of test and training data (Kepler and Finger, 2006; Dos Santos et al., 2008); they did not consider the domain adaptation problem of training on recent data and testing on older historical text. 5 Conclusion and Future Work Denoising autoencoders provide an intuitive solution for domain adaptation: transform the features into a representation that is resistant to the noise that may c</context>
</contexts>
<marker>Moon, Baldridge, 2007</marker>
<rawString>Taesun Moon and Jason Baldridge. 2007. Part-ofspeech tagging for middle english through alignment and projection of parallel diachronic texts. In EMNLP-CoNLL, pages 390–399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´ıcero Nogueira Dos Santos</author>
<author>Ruy L Milidi´u</author>
<author>Ra´ul P Renter´ıa</author>
</authors>
<title>Portuguese part-of-speech tagging using entropy guided transformation learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th international conference on Computational Processing of the Portuguese Language, PROPOR ’08,</booktitle>
<pages>143--152</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Santos, Milidi´u, Renter´ıa, 2008</marker>
<rawString>C´ıcero Nogueira Dos Santos, Ruy L. Milidi´u, and Ra´ul P. Renter´ıa. 2008. Portuguese part-of-speech tagging using entropy guided transformation learning. In Proceedings of the 8th international conference on Computational Processing of the Portuguese Language, PROPOR ’08, pages 143–152, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>Crfsuite: a fast implementation of conditional random fields (crfs).</title>
<date>2007</date>
<contexts>
<context position="12987" citStr="Okazaki, 2007" startWordPosition="2173" endWordPosition="2174"> tagger on a modern newstext corpus and applying it to historical documents. # of Tokens Dataset Total Narrative Letters Dissertation Theatre 1800-1849 125719 91582 34137 0 0 1750-1799 202346 57477 84465 0 60404 1700-1749 278846 0 130327 148519 0 1650-1699 248194 83938 115062 49194 0 1600-1649 295154 117515 115252 62387 0 1550-1599 148061 148061 0 0 0 1500-1549 182208 126516 0 55692 0 Overall 1480528 625089 479243 315792 60404 Table 1: Statistics of the Tycho Brahe Corpus CRF tagger We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features (Okazaki, 2007), with SGD optimization. Following the work of Nogueira Dos Santos et al. (2008) on this dataset, we apply the feature set of Ratnaparkhi (1996). There are 16 feature templates and 372,902 features in total. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments. Methods We compare mDA with three alternative approaches. We refer to baseline as training a CRF tagger on the source domain and testing on the target domain with only base features. We also include PCA to project th</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. Crfsuite: a fast implementation of conditional random fields (crfs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
</authors>
<title>A survey on transfer learning.</title>
<date>2010</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>22</volume>
<issue>10</issue>
<contexts>
<context position="1383" citStr="Pan and Yang, 2010" startWordPosition="193" endWordPosition="196"> increasing speed by more than an order-ofmagnitude over previous work. 1 Introduction Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are train</context>
</contexts>
<marker>Pan, Yang, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Natural language processing across time: An empirical investigation on italian.</title>
<date>2008</date>
<booktitle>In Advances in Natural Language Processing,</booktitle>
<pages>371--382</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="17898" citStr="Pennacchiotti and Zanzotto (2008)" startWordPosition="2973" endWordPosition="2977">method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increasing interest for NLP (Piotrowski, 2012). Pennacchiotti and Zanzotto (2008) find that part-ofspeech tagging degrades considerably when applied to a corpus of historical Italian. Moon and Baldridge (2007) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages. Prior work on the Tycho Brahe corpus applied supervised learning to a random split of test and training data (Kepler and Finger, 2006; Dos Santos et al., 2008); they did not consider the domain adaptation problem of training on recent data and testing on older historical text. 5 Conclusion and Future Work Denoising autoencoders provide an </context>
</contexts>
<marker>Pennacchiotti, Zanzotto, 2008</marker>
<rawString>Marco Pennacchiotti and Fabio Massimo Zanzotto. 2008. Natural language processing across time: An empirical investigation on italian. In Advances in Natural Language Processing, pages 371–382. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Piotrowski</author>
</authors>
<title>Natural language processing for historical texts.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="1282" citStr="Piotrowski, 2012" startWordPosition="178" endWordPosition="179">set of historical Portuguese, marginalized structured dropout yields state-of-the-art accuracy while increasing speed by more than an order-ofmagnitude over previous work. 1 Introduction Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature</context>
<context position="17863" citStr="Piotrowski, 2012" startWordPosition="2971" endWordPosition="2972"> domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increasing interest for NLP (Piotrowski, 2012). Pennacchiotti and Zanzotto (2008) find that part-ofspeech tagging degrades considerably when applied to a corpus of historical Italian. Moon and Baldridge (2007) tackle the challenging problem of tagging Middle English, using techniques for projecting syntactic annotations across languages. Prior work on the Tycho Brahe corpus applied supervised learning to a random split of test and training data (Kepler and Finger, 2006; Dos Santos et al., 2008); they did not consider the domain adaptation problem of training on recent data and testing on older historical text. 5 Conclusion and Future Work</context>
</contexts>
<marker>Piotrowski, 2012</marker>
<rawString>Michael Piotrowski. 2012. Natural language processing for historical texts. Synthesis Lectures on Human Language Technologies, 5(2):1–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="13131" citStr="Ratnaparkhi (1996)" startWordPosition="2197" endWordPosition="2198">e 1800-1849 125719 91582 34137 0 0 1750-1799 202346 57477 84465 0 60404 1700-1749 278846 0 130327 148519 0 1650-1699 248194 83938 115062 49194 0 1600-1649 295154 117515 115252 62387 0 1550-1599 148061 148061 0 0 0 1500-1549 182208 126516 0 55692 0 Overall 1480528 625089 479243 315792 60404 Table 1: Statistics of the Tycho Brahe Corpus CRF tagger We use a conditional random field tagger, choosing CRFsuite because it supports arbitrary real valued features (Okazaki, 2007), with SGD optimization. Following the work of Nogueira Dos Santos et al. (2008) on this dataset, we apply the feature set of Ratnaparkhi (1996). There are 16 feature templates and 372,902 features in total. Following Blitzer et al. (2006), we consider pivot features that appear more than 50 times in all the domains. This leads to a total of 1572 pivot features in our experiments. Methods We compare mDA with three alternative approaches. We refer to baseline as training a CRF tagger on the source domain and testing on the target domain with only base features. We also include PCA to project the entire dataset onto a low-dimensional sub-space (while still including the original features). Finally, we compare against Structural Correspo</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, April 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Linguistic structure prediction.</title>
<date>2011</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="1738" citStr="Smith, 2011" startWordPosition="249" endWordPosition="250">g social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising </context>
</contexts>
<marker>Smith, 2011</marker>
<rawString>Noah A Smith. 2011. Linguistic structure prediction. Synthesis Lectures on Human Language Technologies, 4(2):1–274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Semi-supervised learning and domain adaptation in natural language processing.</title>
<date>2013</date>
<booktitle>Synthesis Lectures on Human Language Technologies,</booktitle>
<pages>6--2</pages>
<contexts>
<context position="1399" citStr="Søgaard, 2013" startWordPosition="197" endWordPosition="198"> more than an order-ofmagnitude over previous work. 1 Introduction Unsupervised domain adaptation is a fundamental problem for natural language processing, as we hope to apply our systems to datasets unlike those for which we have annotations. This is particularly relevant as labeled datasets become stale in comparison with rapidly evolving social media writing styles (Eisenstein, 2013), and as there is increasing interest in natural language processing for historical texts (Piotrowski, 2012). While a number of different approaches for domain adaptation have been proposed (Pan and Yang, 2010; Søgaard, 2013), they tend to emphasize bag-ofwords features for classification tasks such as sentiment analysis. Consequently, many approaches rely on each instance having a relatively large number of active features, and fail to exploit the structured feature spaces that characterize syntactic tasks such as sequence labeling and parsing (Smith, 2011). As we will show, substantial efficiency improvements can be obtained by designing domain adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove syn</context>
</contexts>
<marker>Søgaard, 2013</marker>
<rawString>Anders Søgaard. 2013. Semi-supervised learning and domain adaptation in natural language processing. Synthesis Lectures on Human Language Technologies, 6(2):1–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2881" citStr="Toutanova et al. (2003)" startWordPosition="427" endWordPosition="430"> et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et al. (2003) define several feature “templates”: the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands of binary features. To exploit this structure, we propose two alternative noising techniques: (1) feature scrambling, which randomly chooses a feature template and randomly selects an alternative value within the template, and (2) structured dropout, which randomly eliminates all but a single feature template. We show how it is possible to marginalize over both types of noise, and find that the solution for structured dropout is sub</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>1096--1103</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="16125" citStr="Vincent et al., 2008" startWordPosition="2688" endWordPosition="2691">daptation focused on the supervised setting, in which some labeled data is available in the target domain (Jiang and Zhai, 2007; Daum´e III, 2007; Finkel and Manning, 2009). Our work focuses on unsupervised domain adaptation, where no labeled data is available in the target domain. Several representation learning methods have been proposed to solve this problem. In structural correspondence learning (SCL), the induced representation is based on the task of predicting the presence of pivot features. Autoencoders apply a similar idea, but use the denoised instances as the latent representation (Vincent et al., 2008; Glorot et al., 2011b; Chen et al., 2012). Within the context of denoising autoencoders, we have focused 541 Task baseline PCA SCL mDA dropout structured scrambling from 1800-1849 - -+1750 89.12 89.09 89.69 90.08 90.08 90.01 - -+1700 90.43 90.43 91.06 91.56 91.57 91.55 - -+1650 88.45 88.52 87.09 88.69 88.70 88.57 - -+1600 87.56 87.58 88.47 89.60 89.61 89.54 - -+1550 89.66 89.61 90.57 91.39 91.39 91.36 - -+1500 85.58 85.63 86.99 88.96 88.95 88.91 from 1750-1849 - -+1700 94.64 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71</context>
</contexts>
<marker>Vincent, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096–1103. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida I Wang</author>
<author>Mengqiu Wang</author>
<author>Stefan Wager</author>
<author>Percy Liang</author>
<author>Christopher D Manning</author>
</authors>
<title>Feature noising for log-linear structured prediction.</title>
<date>2013</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2444" citStr="Wang et al., 2013" startWordPosition="360" endWordPosition="363">in adaptation methods for learning in structured feature spaces. We build on work from the deep learning community, in which denoising autoencoders are trained to remove synthetic noise from the observed instances (Glorot et al., 2011a). By using the autoencoder to transform the original feature space, one may obtain a representation that is less dependent on any individual feature, and therefore more robust across domains. Chen et al. (2012) showed that such autoencoders can be learned even as the noising process is analytically marginalized; the idea is similar in spirit to feature noising (Wang et al., 2013). While the marginalized denoising autoencoder (mDA) is considerably faster than the original denoising autoencoder, it requires solving a system of equations that can grow very large, as realistic NLP tasks can involve 105 or more features. In this paper we investigate noising functions that are explicitly designed for structured feature spaces, which are common in NLP. For example, in part-of-speech tagging, Toutanova et al. (2003) define several feature “templates”: the current word, the previous word, the suffix of the current word, and so on. For each feature template, there are thousands</context>
<context position="17157" citStr="Wang et al., 2013" startWordPosition="2862" endWordPosition="2865"> 85.58 85.63 86.99 88.96 88.95 88.91 from 1750-1849 - -+1700 94.64 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71 - -+1550 93.27 93.21 93.75 94.06 94.05 94.02 - -+1500 89.80 89.75 90.59 91.71 91.71 91.68 Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849. Figure 1: Transfer ratio for adaptation to historical text on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns</context>
</contexts>
<marker>Wang, Wang, Wager, Liang, Manning, 2013</marker>
<rawString>Sida I. Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D. Manning. 2013. Feature noising for log-linear structured prediction. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Yuhong Guo</author>
</authors>
<title>Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model.</title>
<date>2013</date>
<booktitle>In Sanjoy Dasgupta and David Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13),</booktitle>
<volume>28</volume>
<pages>293--301</pages>
<contexts>
<context position="17224" citStr="Xiao and Guo (2013)" startWordPosition="2873" endWordPosition="2876"> 94.62 94.81 95.08 95.08 95.02 - -+1650 91.98 90.97 90.37 90.83 90.84 90.80 - -+1600 92.95 92.91 93.17 93.78 93.78 93.71 - -+1550 93.27 93.21 93.75 94.06 94.05 94.02 - -+1500 89.80 89.75 90.59 91.71 91.71 91.68 Table 2: Accuracy results for adaptation from labeled data in 1800-1849, and in 1750-1849. Figure 1: Transfer ratio for adaptation to historical text on dropout noise, which has also been applied as a general technique for improving the robustness of machine learning, particularly in neural networks (Hinton et al., 2012; Wang et al., 2013). On the specific problem of sequence labeling, Xiao and Guo (2013) proposed a supervised domain adaptation method by using a log-bilinear language adaptation model. Dhillon et al. (2011) presented a spectral method to estimate low dimensional context-specific word representations for sequence labeling. Huang and Yates (2009; 2012) used an HMM model to learn latent representations, and then leverage the Posterior Regularization framework to incorporate specific biases. Unlike these methods, our approach uses a standard CRF, but with transformed features. Historical text Our evaluation concerns syntactic analysis of historical text, which is a topic of increas</context>
</contexts>
<marker>Xiao, Guo, 2013</marker>
<rawString>Min Xiao and Yuhong Guo. 2013. Domain adaptation for sequence labeling tasks with a probabilistic language adaptation model. In Sanjoy Dasgupta and David Mcallester, editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 293–301. JMLR Workshop and Conference Proceedings.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>