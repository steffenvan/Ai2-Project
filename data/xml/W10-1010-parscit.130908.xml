<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022685">
<title confidence="0.961851">
Towards Using Structural Events To Assess Non-native Speech
</title>
<author confidence="0.886558">
Lei Chen, Joel Tetreault, Xiaoming Xi
</author>
<affiliation confidence="0.807265">
Educational Testing Service (ETS)
</affiliation>
<address confidence="0.697414">
Princeton, NJ 08540, USA
</address>
<email confidence="0.998945">
{LChen,JTetreault,XXi}@ets.org
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927857142857">
We investigated using structural events, e.g.,
clause and disfluency structure, from tran-
scriptions of spontaneous non-native speech,
to compute features for measuring speaking
proficiency. Using a set of transcribed au-
dio files collected from the TOEFL Practice
Test Online (TPO), we conducted a sophisti-
cated annotation of structural events, includ-
ing clause boundaries and types, as well as
disfluencies. Based on words and the anno-
tated structural events, we extracted features
related to syntactic complexity, e.g., the mean
length of clause (MLC) and dependent clause
frequency (DEPC), and a feature related to
disfluencies, the interruption point frequency
per clause (IPC). Among these features, the
IPC shows the highest correlation with holis-
tic scores (r = −0.344). Furthermore, we in-
creased the correlation with human scores by
normalizing IPC by (1) MLC (r = −0.386),
(2) DEPC (r = −0.429), and (3) both (r =
−0.462). In this research, the features derived
from structural events of speech transcriptions
are found to predict holistic scores measuring
speaking proficiency. This suggests that struc-
tural events estimated on speech word strings
provide a potential way for assessing non-
native speech.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990869808988765">
In the last decade, a breakthrough in speech pro-
cessing is the emergence of a lot of active research
work on automatic estimation of structural events,
e.g., sentence structure and disfluencies, on sponta-
neous speech (Shriberg et al., 2000; Liu, 2004; Os-
74
tendorf et al., 2008). The detected structural events
have been successfully used in many natural lan-
guage processing (NLP) applications (Ostendorf et
al., 2008).
However, the structural events in speech data
haven’t been largely utilized by the research on us-
ing automatic speech recognition (ASR) technology
to assess speech proficiency (Neumeyer et al., 2000;
Zechner et al., 2007), which mainly used cues de-
rived at the word level, such as timing information
of spoken words. The information beyond the word
level, e.g., clause/sentence structure of utterances
and disfluency structure, has not been or is poorly
represented. For example, in Zechner et al. (2007),
only special words for filled pauses such as um and
uh were obtained from ASR results to represent dis-
fluencies.
Given the successful usage of structural events
on a wide range of NLP applications and the fact
that the usage of these events is missing in the auto-
matic speech assessment research, a research ques-
tion emerges: Can we use structural events of spon-
taneous speech to assess non-native speech profi-
ciency?
We will address this question in this paper. The
paper is organized as follows: Section 2 reviews
previous research. Section 3 describes our annota-
tion convention. Section 4 reports on the data col-
lection, annotation, and quality control. Section 5
reports on features based on structural event anno-
tations. Section 6 reports on our experiments. Sec-
tion 7 discusses our findings and plans for future re-
search work.
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 74–79,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
2 Previous Work ies. For example, Mizera (2006) used fluency fac-
In the last decade, a large amount of research (Os- tors related to speed, voiced smoothness (frequen-
tendorf et al., 2008) has been conducted on detection cies of repetitions or self-corrections), pauses, syn-
of structural events, e.g., sentence structure and dis- tactic complexity (mean length of T-units), and
fluency structure, in spontaneous speech. In these accuracy, to measure speaking proficiency on 20
research works, the structural events were detected non-native English speakers. In this experiment,
with a quite high accuracy. Furthermore, the de- disfluency-related factors, such as total voiced dis-
tected sentence and disfluency structures have been fluencies, had a high correlation with fluency (r =
found to help many of the following NLP tasks, −0.45). However, the syntactic complexity factor
e.g., speech parsing, information retrieval, machine only showed a moderate correlation (r = 0.310).
translation, and extractive speech summary (Osten- Yoon (2009) implemented an automated disfluency
dorf et al., 2008). detection method and found that the disfluency-
In the second language acquisition (SLA) and related features lead to the moderate improvement
child language development research fields, the lan- in the automated speech proficiency scoring.
guage development is measured according to flu- There were limitations on using the features re-
ency, accuracy, and complexity (Iwashita, 2006). ported in these SLA studies on standard language
The syntactic complexity of learners’ writing data tests. For example, only a very limited number of
has been extensively studied in the SLA commu- subjects (from 20 to 30 speakers) were used in these
nity (Ortega, 2003). Recently, this study has been studies. Second, the speaking content was narra-
extended to the learner’s speaking data (Iwashita, tions of picture books or cartoon videos rather than
2006). Typical metrics for examining syntactic com- standard test questions. Therefore, we conducted a
plexity include: length of production unit (e.g., T- study using a much larger data set obtained from real
unit, which is defined as essentially a main clause speech tests to address these limitations.
plus any other clauses which are dependent upon 3 Structural Event Annotation Convention
it (Hunt, 1970), clauses, verb phrases, and sen- To annotate structural events of speech content, we
tences), amount of embedding, subordination and have developed a convention based on previous stud-
coordination, range of structural types, and structure ies and our observations on the TOEFL Practice On-
sophistication. line (TPO) test data. Defining clauses is a relatively
Iwashita (2006) investigated several measures for simple task; however, defining clause boundaries
syntactic complexity on the data from learners of and specifying which elements fall within a particu-
Japanese. The author reported that some measure- lar clause is a much more challenging task for spo-
ments, e.g., T-unit length, the number of clauses per ken discourse, due to the presence of grammatical
T-unit, and the number of independent clauses per T- errors, fragments, repetitions, self corrections, and
Unit, were good at predicting learners’ proficiency conversation fillers.
levels. Foster et al. (Foster et al., 2000) review various
In addition, some previous studies used measure- units for analyzing spoken language, including syn-
ments related to disfluencies to assess speaking pro- tactic, semantic and intonational units, and propose
ficiency. For example, Lennon (1990) used a dozen a new analysis of speech unit (AS-Unit) that they
features related to speed, pauses, and several dis- claim is appropriate for many different purposes. In
fluency markers, such as filler pauses per T-unit, this study, we focused on clauses given the charac-
to measure four German-speaking women’s English teristics of spontaneous speech. Also, we defined
improvement during a half year study in England. clause types based on grammar books such as (Azar,
He found a significant change in filled pauses per 2003). The following clause types were defined:
T-unit during the studying process. • Simple sentence (SS) contains a subject and a
The features related to syntactic complexity and verb, and expresses a complete thought.
the features related to ”smoothness” (disfluency) of
speech were jointly used in some previous stud-
75
</bodyText>
<listItem confidence="0.993307714285714">
• Independent clause (I) is the main clause that
can stand along syntactically as a complete sen-
tence. It consists minimally a subject and a fi-
nite verb (a verb that shows tense, person, or
singular/plural, e.g., he goes, I went, and I was).
• Subordinate clause is a clause in a complex
sentence that cannot stand alone as a complete
sentence and that functions within the sentence
as a noun, a verb complement, an adjective or
an adverb. There are three types of subordi-
nate clauses: noun clause (NC), relative clause
that functions as an adjective (ADJ), adverbial
clause that functions as an adverb (ADV).
• Coordinate clause (CC) is a clause in a com-
pound sentence that is grammatically equiva-
lent to the main clause and that performs the
same grammatical function.
• Adverbial phrase (ADVP) is a separate clause
from the main clause that contains a non-finite
verb (a verb that does not show tense, person,
or singular/plural).
</listItem>
<bodyText confidence="0.998977833333333">
The clause boundaries and clause types were an-
notated on the word transcriptions. Round brack-
ets were used to indicate the beginning and end of a
clause. Then, the abbreviations described above for
clause types were added. Also, if a specific bound-
ary serves as the boundaries for both the local and
global clause, the abbreviation of the local clause
was followed by that of the global. Some examples
of clause boundaries and types are reported in Ta-
ble 1.
In our annotation manual, a speech disfluency
contains three parts:
</bodyText>
<listItem confidence="0.973468555555556">
• Reparandum: the speech portion that will be
repeated, corrected, or even abandoned. The
end of the reparandum is called the interruption
point (IP), which indicates the stop of a normal
fluent speech stream.
• Editing phrase: optional inserted words, e.g.,
um.
• Correction: the speech portion that repeats,
corrects, or even starts new content.
</listItem>
<bodyText confidence="0.999942571428572">
In our annotation manual, the reparandum was en-
closed by “*”, the editing phrase was enclosed by
“%”, and the correction was enclosed by “$”. For
example, in the following utterance, “He is a * very
mad * % er % $ very bad $ cop”, “very mad” was
corrected by “very bad” and an editing phrase, er,
was inserted.
</bodyText>
<sectionHeader confidence="0.868349" genericHeader="method">
4 Data Collection and Annotation
</sectionHeader>
<subsectionHeader confidence="0.999061">
4.1 Audio data collection and scoring
</subsectionHeader>
<bodyText confidence="0.999608846153846">
About 1300 speech responses from the TPO test
were collected and transcribed. Each item was
scored by two experienced human raters indepen-
dently using a 4-point holistic score based on the
scoring rubrics designed for the test.
In the TPO test, some tasks required test-takers to
provide information or opinions on familiar topics
based on their personal experience or background
knowledge. Others required them to summarize and
synthesize information presented in listening and/or
reading materials. Each test-taker was required to
finish six items in one test session. Each item has a
45 or 60 seconds response time.
</bodyText>
<subsectionHeader confidence="0.99806">
4.2 Annotation procedure
</subsectionHeader>
<bodyText confidence="0.99988495">
Two annotators (who were not the human raters
mentioned above) with a linguistics background and
past linguistics annotation experience were first pre-
sented with a draft of the annotation convention.
After reading through it, the annotators, as well as
the second and third author completed four iterative
loops of rating 4 or 5 responses per meeting. All four
discussed differences in annotations and the conven-
tion was refined as needed. After the final iteration
of comparisons, the raters seemed to have very few
disagreement and thus began annotating sets of re-
sponses. Each set consisted of roughly 50-75 re-
sponses and then a kappa set of 30-50 responses
which both annotators completed. Accordingly, be-
tween the two annotators, a set comprised roughly
130 to 200 responses. Each response takes roughly
3-8 minutes to annotate. The annotators were in-
structed to listen to the corresponding audio file if
they needed the prosodic information to annotate a
particular speech disfluency event.
</bodyText>
<page confidence="0.897644">
76
</page>
<table confidence="0.999805125">
Clause type Example
SS (That’s right |SS)
I (He turned away |I) as soon as he saw me |ADV)
NC ((What he did |NC) shocked me |I)
ADJ (She is the woman (I told you about |ADJ)|I)
ADV (As soon as he saw me |ADV) (he turned away |I)
CC (I will go home |I) (and he will go to work |CC)
ADVP (While walking to class |ADVP) (I ran into a friend |I)
</table>
<tableCaption confidence="0.999875">
Table 1: Examples of clause boundary and type annotation
</tableCaption>
<subsectionHeader confidence="0.999726">
4.3 Evaluation of annotation
</subsectionHeader>
<bodyText confidence="0.999396684210526">
To evaluate the quality of structural event anno-
tation, we measured the inter-rater agreement on
clause boundary (CB) annotation and interruption
point (IP) of disfluencies1.
We used Cohen’s K to calculate the annotator
agreement on each kappa set. K is calculated on the
absence or presence of a boundary marker (either a
clause boundary (CB) or an interruption point (IP)
between consecutive words). For each consecutive
pair of words, we check for the existence of one or
more boundaries, and collapse the set into one term
“boundary” and then compute the agreement on this
reduced annotation.
In Table 2, we list the annotator agreement for
both boundary events over 4 kappa sets. The second
column refers to the number of speech responses in
the kappa set, the next two columns refer to the an-
notator agreement using the Cohen’s K value on CB
and IP annotation results.
</bodyText>
<table confidence="0.99846">
Set N K CB K IP
Set1 54 0.886 0.626
Set2 71 0.847 0.687
Set3 35 0.855 0.695
Set4 34 0.899 0.833
</table>
<tableCaption confidence="0.962273">
Table 2: Between-rater agreement of structural event an-
notation
</tableCaption>
<bodyText confidence="0.955690230769231">
In general, a K of 0.8-1.0 represents excellent
agreement, 0.6-0.8 represents good agreement, and
so forth. Over each kappa set, K for CB annota-
tions ranges between 0.8 and 0.9, which is an ex-
1Measurement on CBs and IPs can provide a rough qual-
ity measurement of annotations. In addition, doing so is more
important to us since automatic detection of these two types of
events will be investigated in future.
cellent agreement; K for IP annotation ranges be-
tween 0.6 and 0.8, which is a good agreement. Com-
pared to annotating clauses, marking disfluencies is
more challenging. As a result, a lower between-rater
agreement is expected.
</bodyText>
<sectionHeader confidence="0.997336" genericHeader="method">
5 Features Derived On Structural Events
</sectionHeader>
<bodyText confidence="0.999202071428572">
Based on the structural event annotations, including
clause boundaries and their types, as well as disflu-
encies, some features measuring syntactic complex-
ity and disfluency profile were derived.
Since simple sentence (SS), independent clause
(I), and conjunct clause (CC) represent a complete
idea, we treat them as an approximate to a T-unit (T).
The clauses that have no complete idea, are depen-
dent clauses (DEP), including noun clauses (N), rel-
ative clauses that function as adjective (ADJ), adver-
bial clauses (ADV), and adverbial phrases (ADVP).
The total number of clauses is a summation of the
number of T-units (T), dependent clauses (DEP), and
fragments2 (denoted as F). Therefore,
</bodyText>
<equation confidence="0.999943">
NT = NSS + NI + NCC
NDEP = NNC + NADJ + NADV + NADVP
NC = NT + NDEP + NF
</equation>
<bodyText confidence="0.997357333333333">
Assuming N,,, is the total number of words in
the speech response (without pruning speech re-
pairs), the following features, including mean length
of clause (MLC), dependent clauses per clause
(DEPC), and interruption points per clause (IPC),
are derived:
</bodyText>
<footnote confidence="0.86016975">
MLC = N,,,/NC
2It is either a subordinate clause that does not have a cor-
responding independent clause or a string of words without a
subject or a verb that does not express a complete thought.
</footnote>
<page confidence="0.985681">
77
</page>
<equation confidence="0.733584">
DEPC = NDEP/NC
IPC = NIP/NC
</equation>
<bodyText confidence="0.999958894736842">
Furthermore, we elaborated the IPC feature. Dis-
fluency is a complex behavior and is influenced by
a variety of factors, such as proficiency level, speak-
ing rate, and familiarity with speaking content. The
complexity of utterances is also an important fac-
tor on the disfluency pattern. For example, Roll
et al. (Roll et al., 2007) found that complexity of
expression computed based on the language’s pars-
ing tree structure influenced the frequency of disflu-
encies in their experiment on Swedish. Therefore,
since disfluency frequency was not only influenced
by the test-takers’ speaking proficiency but also by
the utterance’s syntactic structure’s difficulty, we re-
duced the impact from the syntactic structure so that
we can focus on speakers’ ability. For this purpose,
we normalized IPC by dividing by some features re-
lated to syntactic-structure’s complexity, including
MLC, DEPC, and both. Therefore, the following
elaborated disfluency-related features were derived:
</bodyText>
<equation confidence="0.999113333333333">
IPCn1 = IPC/MLC
IPCn2 = IPC/DEPC
IPCn3 = IPC/MLC/DEPC
</equation>
<sectionHeader confidence="0.996331" genericHeader="evaluation">
6 Experiment
</sectionHeader>
<bodyText confidence="0.999764076923077">
For each item, two human raters rated it separately
with a score from 1 to 4. If these two scores are
consistent (the difference between two scores is ei-
ther zero or one), we put this item in an item-pool.
Finally, a total of 1, 257 audio items were included
in the pool. Following the score-handling protocol
used in the TPO test, we used the first human rater’s
score as the item score. From the obtained item-
pool, we selected speakers with more than three
items so that the averaged score per speaker can be
estimated on several items to achieve a robust score
computation3. As a result, 175 speakers4 were se-
lected.
</bodyText>
<footnote confidence="0.98556575">
3The mean holistic score of these speakers is 2.786, which
is close to the mean holistic score of the selected item-pool
(2.785), indicating that score distribution was kept after focus-
ing on speakers with more than three items.
4If a speaker was assigned in a Kappa set in the annotation
as described in Section 4, this speaker would have as many as 12
annotated items. Therefore, the minimum number of speakers
from the item-pool was about 105 (1257/12).
</footnote>
<bodyText confidence="0.9920422">
For each speaker, his or her annotations of words
and structural events were used to extract the fea-
tures described in Section 5. Then, we computed
the Pearson correlation among the obtained features
with the averaged holistic scores per speaker.
</bodyText>
<table confidence="0.988603142857143">
Feature r
MLC 0.211
DEPC 0.284
IPC -0.344
IPCn1 -0.386
IPCn2 -0.429
IPCn3 -0.462
</table>
<tableCaption confidence="0.99922">
Table 3: Correlation coefficients (rs) between the fea-
tures derived from structural events with human scores
averaged on test takers
</tableCaption>
<bodyText confidence="0.981654875">
Table 3 reports on the correlation coefficient
(r) between the proposed features derived from
structural events with holistic scores. Relying
on three simple structural event annotations, i.e.,
clause boundaries, dependent clauses, and interrup-
tion points in speech disfluencies, some promising
correlations between features with holistic scores
were found. Between the two syntactic complex-
ity features, the DEPC has a higher correlation with
holistic scores than the MLC (0.284 &gt; 0.211). It ap-
pears that a measurement about clauses’ embedding
profile is more informative about a speaker’s profi-
ciency level. Second, compared to features measur-
ing syntactic complexity, the feature measuring the
disfluency profile is better to predict human holis-
tic scores on this non-native data set. For example,
IPC has a r of −0.344, which is better than the fea-
tures about clause lengths or embedding. Finally, by
jointly using the structural events related to clauses
and disfluencies, we can further achieve a further
improved r. Compared to IPC, IPCn3 has a relative
34.30% correlation increase. This is consistent with
our idea of reducing utterance-complexity’s impact
on disfluency-related features.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9999694">
In most current automatic speech assessment sys-
tems, features derived from recognized words, such
as delivery features about speaking rate, pause infor-
mation, and accuracy related to word identities, have
been widely used to assess non-native speech from
</bodyText>
<page confidence="0.993518">
78
</page>
<bodyText confidence="0.999990236363637">
fluency and accuracy points of view. However, in-
formation beyond recognized words, e.g., the struc-
ture of clauses and disfluencies, has only received
limited attention. Although several previous SLA
studies used features derived from structural events
to measure speaking proficiency, these studies were
limited and the findings from them were difficult to
directly apply to on large-scale standard tests.
In this paper, using a large-sized data set col-
lected in the TPO speaking test, we conducted an
sophisticated annotation of structural events, includ-
ing boundaries and types of clauses and disfluen-
cies, from transcriptions of spontaneous speech test
responses. A series of features were derived from
these structural event annotations and were eval-
uated according to their correlations with holistic
scores. We found that disfluency-related features
have higher correlations to human holistic scores
than features about syntactic complexity, which con-
firms the result reported in (Mizera, 2006). In spon-
taneous speech utterances, simple syntactic structure
tends to be utilized by speakers. This is in contrast to
sophisticated syntactic structure appearing in writ-
ing. This may cause that complexity-related features
are poor at predicting fluency scores. On the other
hand, disfluencies, a pattern unique to spontaneous
speech, were found to play a more important role in
indicating speaking proficiency levels.
Although syntactic complexity features were not
highly indicative of holistic scores, they were useful
to further improve disfluency-related features’ corre-
lation with holistic scores. By normalizing IPC us-
ing measurements representing syntactic complex-
ity, we can highlight contributions from speakers’
proficiency levels. Therefore, in our experiment,
IPCn3 shows a 34.30% relative improvement in its
correlation coefficient with human holistic scores
over the original IPC.
The study reported in this paper suggests promise
that structural events beyond speech recognition re-
sults can be utilized to measure non-native speaker
proficiency levels. Recently, in the NLP research
field, an increasing amount of effort has been
made on structural event detection in spontaneous
speech (Ostendorf et al., 2008). Therefore, such
progress can benefit the study of automatic estima-
tion of structural events on non-native speech data.
For our future research plan, first, we will inves-
tigate automatically detecting these structural events
from speech transcriptions and recognition hypothe-
ses. Second, the features derived from the obtained
structural events will be used to augment the features
in automatic speech assessment research to provide
a wider construct coverage than fluency and pronun-
ciation features do.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941428571429">
B. Azar. 2003. Fundamentals of English grammar.
Pearson Longman, White Plains, NY, 3rd edition.
P. Foster, A. Tonkyn, and G. Wigglesworth. 2000. Mea-
suring spoken language: A unit for all reasons. Ap-
plied Linguistics, 21(3):354.
K. W. Hunt. 1970. Syntactic maturity in school chil-
dren and adults. In Monographs of the Society for Re-
search in Child Development. University of Chicago
Press, Chicago, IL.
N. Iwashita. 2006. Syntactic complexity measures and
their relation to oral proficiency in Japanese as a for-
eign language. Language Assessment Quarterly: An
International Journal, 3(2):151–169.
P. Lennon. 1990. Investigating fluency in EFL: A quanti-
tative approach. Language Learning, 40(3):387–417.
Y. Liu. 2004. Structural Event Detection for Rich Tran-
scription of Speech. Ph.D. thesis, Purdue University.
G. J. Mizera. 2006. Working memory and L2 oral flu-
ency. Ph.D. thesis, University of Pittsburgh.
L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.
2000. Automatic Scoring of Pronunciation Quality.
Speech Communication, 30:83–93.
L. Ortega. 2003. Syntactic complexity measures and
their relationship to L2 proficiency: A research syn-
thesis of college-level L2 writing. Applied Linguistics,
24(4):492.
M. Ostendorf et al. 2008. Speech segmentation and spo-
ken document processing. Signal Processing Maga-
zine, IEEE, 25(3):59–69, May.
M. Roll, J. Frid, and M. Horne. 2007. Measuring syntac-
tic complexity in spontaneous spoken Swedish. Lan-
guage and Speech, 50(2):227.
E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur.
2000. Prosody-based automatic segmentation of
speech into sentences and topics. Speech Communi-
cation, 32(1-2):127–154.
S. Yoon. 2009. Automated assessment of speech fluency
for L2 English learners. Ph.D. thesis, University of
Illinois at Urbana-Champaign.
K. Zechner, D. Higgins, and Xiaoming Xi. 2007.
SpeechRater: A Construct-Driven Approach to Scor-
ing Spontaneous Non-Native Speech. In Proc. SLaTE.
</reference>
<page confidence="0.999049">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762940">
<title confidence="0.999346">Towards Using Structural Events To Assess Non-native Speech</title>
<author confidence="0.941899">Lei Chen</author>
<author confidence="0.941899">Joel Tetreault</author>
<author confidence="0.941899">Xiaoming</author>
<affiliation confidence="0.878755">Educational Testing Service</affiliation>
<address confidence="0.992576">Princeton, NJ 08540,</address>
<abstract confidence="0.995951965517241">We investigated using structural events, e.g., clause and disfluency structure, from transcriptions of spontaneous non-native speech, to compute features for measuring speaking proficiency. Using a set of transcribed audio files collected from the TOEFL Practice Test Online (TPO), we conducted a sophisticated annotation of structural events, including clause boundaries and types, as well as disfluencies. Based on words and the annotated structural events, we extracted features related to syntactic complexity, e.g., the mean length of clause (MLC) and dependent clause frequency (DEPC), and a feature related to disfluencies, the interruption point frequency per clause (IPC). Among these features, the IPC shows the highest correlation with holisscores Furthermore, we increased the correlation with human scores by IPC by (1) MLC DEPC and (3) both In this research, the features derived from structural events of speech transcriptions are found to predict holistic scores measuring speaking proficiency. This suggests that structural events estimated on speech word strings provide a potential way for assessing nonnative speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Azar</author>
</authors>
<title>Fundamentals of English grammar.</title>
<date>2003</date>
<publisher>Pearson Longman,</publisher>
<location>White Plains, NY,</location>
<note>3rd edition.</note>
<marker>Azar, 2003</marker>
<rawString>B. Azar. 2003. Fundamentals of English grammar. Pearson Longman, White Plains, NY, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Foster</author>
<author>A Tonkyn</author>
<author>G Wigglesworth</author>
</authors>
<title>Measuring spoken language: A unit for all reasons.</title>
<date>2000</date>
<journal>Applied Linguistics,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="6722" citStr="Foster et al., 2000" startWordPosition="1023" endWordPosition="1026">a (2006) investigated several measures for simple task; however, defining clause boundaries syntactic complexity on the data from learners of and specifying which elements fall within a particuJapanese. The author reported that some measure- lar clause is a much more challenging task for spoments, e.g., T-unit length, the number of clauses per ken discourse, due to the presence of grammatical T-unit, and the number of independent clauses per T- errors, fragments, repetitions, self corrections, and Unit, were good at predicting learners’ proficiency conversation fillers. levels. Foster et al. (Foster et al., 2000) review various In addition, some previous studies used measure- units for analyzing spoken language, including synments related to disfluencies to assess speaking pro- tactic, semantic and intonational units, and propose ficiency. For example, Lennon (1990) used a dozen a new analysis of speech unit (AS-Unit) that they features related to speed, pauses, and several dis- claim is appropriate for many different purposes. In fluency markers, such as filler pauses per T-unit, this study, we focused on clauses given the characto measure four German-speaking women’s English teristics of spontaneous</context>
</contexts>
<marker>Foster, Tonkyn, Wigglesworth, 2000</marker>
<rawString>P. Foster, A. Tonkyn, and G. Wigglesworth. 2000. Measuring spoken language: A unit for all reasons. Applied Linguistics, 21(3):354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Hunt</author>
</authors>
<title>Syntactic maturity in school children and adults.</title>
<date>1970</date>
<booktitle>In Monographs of the Society for Research in</booktitle>
<institution>Child Development. University of Chicago Press,</institution>
<location>Chicago, IL.</location>
<contexts>
<context position="5736" citStr="Hunt, 1970" startWordPosition="878" endWordPosition="879">nity (Ortega, 2003). Recently, this study has been studies. Second, the speaking content was narraextended to the learner’s speaking data (Iwashita, tions of picture books or cartoon videos rather than 2006). Typical metrics for examining syntactic com- standard test questions. Therefore, we conducted a plexity include: length of production unit (e.g., T- study using a much larger data set obtained from real unit, which is defined as essentially a main clause speech tests to address these limitations. plus any other clauses which are dependent upon 3 Structural Event Annotation Convention it (Hunt, 1970), clauses, verb phrases, and sen- To annotate structural events of speech content, we tences), amount of embedding, subordination and have developed a convention based on previous studcoordination, range of structural types, and structure ies and our observations on the TOEFL Practice Onsophistication. line (TPO) test data. Defining clauses is a relatively Iwashita (2006) investigated several measures for simple task; however, defining clause boundaries syntactic complexity on the data from learners of and specifying which elements fall within a particuJapanese. The author reported that some m</context>
</contexts>
<marker>Hunt, 1970</marker>
<rawString>K. W. Hunt. 1970. Syntactic maturity in school children and adults. In Monographs of the Society for Research in Child Development. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Iwashita</author>
</authors>
<title>Syntactic complexity measures and their relation to oral proficiency in Japanese as a foreign language. Language Assessment Quarterly:</title>
<date>2006</date>
<journal>An International Journal,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="4873" citStr="Iwashita, 2006" startWordPosition="742" endWordPosition="743">lexity factor e.g., speech parsing, information retrieval, machine only showed a moderate correlation (r = 0.310). translation, and extractive speech summary (Osten- Yoon (2009) implemented an automated disfluency dorf et al., 2008). detection method and found that the disfluencyIn the second language acquisition (SLA) and related features lead to the moderate improvement child language development research fields, the lan- in the automated speech proficiency scoring. guage development is measured according to flu- There were limitations on using the features reency, accuracy, and complexity (Iwashita, 2006). ported in these SLA studies on standard language The syntactic complexity of learners’ writing data tests. For example, only a very limited number of has been extensively studied in the SLA commu- subjects (from 20 to 30 speakers) were used in these nity (Ortega, 2003). Recently, this study has been studies. Second, the speaking content was narraextended to the learner’s speaking data (Iwashita, tions of picture books or cartoon videos rather than 2006). Typical metrics for examining syntactic com- standard test questions. Therefore, we conducted a plexity include: length of production unit </context>
<context position="6110" citStr="Iwashita (2006)" startWordPosition="933" endWordPosition="934">a much larger data set obtained from real unit, which is defined as essentially a main clause speech tests to address these limitations. plus any other clauses which are dependent upon 3 Structural Event Annotation Convention it (Hunt, 1970), clauses, verb phrases, and sen- To annotate structural events of speech content, we tences), amount of embedding, subordination and have developed a convention based on previous studcoordination, range of structural types, and structure ies and our observations on the TOEFL Practice Onsophistication. line (TPO) test data. Defining clauses is a relatively Iwashita (2006) investigated several measures for simple task; however, defining clause boundaries syntactic complexity on the data from learners of and specifying which elements fall within a particuJapanese. The author reported that some measure- lar clause is a much more challenging task for spoments, e.g., T-unit length, the number of clauses per ken discourse, due to the presence of grammatical T-unit, and the number of independent clauses per T- errors, fragments, repetitions, self corrections, and Unit, were good at predicting learners’ proficiency conversation fillers. levels. Foster et al. (Foster e</context>
</contexts>
<marker>Iwashita, 2006</marker>
<rawString>N. Iwashita. 2006. Syntactic complexity measures and their relation to oral proficiency in Japanese as a foreign language. Language Assessment Quarterly: An International Journal, 3(2):151–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lennon</author>
</authors>
<title>Investigating fluency in EFL: A quantitative approach.</title>
<date>1990</date>
<journal>Language Learning,</journal>
<volume>40</volume>
<issue>3</issue>
<contexts>
<context position="6980" citStr="Lennon (1990)" startWordPosition="1061" endWordPosition="1062">re challenging task for spoments, e.g., T-unit length, the number of clauses per ken discourse, due to the presence of grammatical T-unit, and the number of independent clauses per T- errors, fragments, repetitions, self corrections, and Unit, were good at predicting learners’ proficiency conversation fillers. levels. Foster et al. (Foster et al., 2000) review various In addition, some previous studies used measure- units for analyzing spoken language, including synments related to disfluencies to assess speaking pro- tactic, semantic and intonational units, and propose ficiency. For example, Lennon (1990) used a dozen a new analysis of speech unit (AS-Unit) that they features related to speed, pauses, and several dis- claim is appropriate for many different purposes. In fluency markers, such as filler pauses per T-unit, this study, we focused on clauses given the characto measure four German-speaking women’s English teristics of spontaneous speech. Also, we defined improvement during a half year study in England. clause types based on grammar books such as (Azar, He found a significant change in filled pauses per 2003). The following clause types were defined: T-unit during the studying proces</context>
</contexts>
<marker>Lennon, 1990</marker>
<rawString>P. Lennon. 1990. Investigating fluency in EFL: A quantitative approach. Language Learning, 40(3):387–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
</authors>
<title>Structural Event Detection for Rich Transcription of Speech.</title>
<date>2004</date>
<journal>G. J. Mizera.</journal>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University.</institution>
<contexts>
<context position="1675" citStr="Liu, 2004" startWordPosition="251" endWordPosition="252"> = −0.386), (2) DEPC (r = −0.429), and (3) both (r = −0.462). In this research, the features derived from structural events of speech transcriptions are found to predict holistic scores measuring speaking proficiency. This suggests that structural events estimated on speech word strings provide a potential way for assessing nonnative speech. 1 Introduction In the last decade, a breakthrough in speech processing is the emergence of a lot of active research work on automatic estimation of structural events, e.g., sentence structure and disfluencies, on spontaneous speech (Shriberg et al., 2000; Liu, 2004; Os74 tendorf et al., 2008). The detected structural events have been successfully used in many natural language processing (NLP) applications (Ostendorf et al., 2008). However, the structural events in speech data haven’t been largely utilized by the research on using automatic speech recognition (ASR) technology to assess speech proficiency (Neumeyer et al., 2000; Zechner et al., 2007), which mainly used cues derived at the word level, such as timing information of spoken words. The information beyond the word level, e.g., clause/sentence structure of utterances and disfluency structure, ha</context>
</contexts>
<marker>Liu, 2004</marker>
<rawString>Y. Liu. 2004. Structural Event Detection for Rich Transcription of Speech. Ph.D. thesis, Purdue University. G. J. Mizera. 2006. Working memory and L2 oral fluency. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Neumeyer</author>
<author>H Franco</author>
<author>V Digalakis</author>
<author>M Weintraub</author>
</authors>
<date>2000</date>
<booktitle>Automatic Scoring of Pronunciation Quality. Speech Communication,</booktitle>
<pages>30--83</pages>
<contexts>
<context position="2043" citStr="Neumeyer et al., 2000" startWordPosition="305" endWordPosition="308">on In the last decade, a breakthrough in speech processing is the emergence of a lot of active research work on automatic estimation of structural events, e.g., sentence structure and disfluencies, on spontaneous speech (Shriberg et al., 2000; Liu, 2004; Os74 tendorf et al., 2008). The detected structural events have been successfully used in many natural language processing (NLP) applications (Ostendorf et al., 2008). However, the structural events in speech data haven’t been largely utilized by the research on using automatic speech recognition (ASR) technology to assess speech proficiency (Neumeyer et al., 2000; Zechner et al., 2007), which mainly used cues derived at the word level, such as timing information of spoken words. The information beyond the word level, e.g., clause/sentence structure of utterances and disfluency structure, has not been or is poorly represented. For example, in Zechner et al. (2007), only special words for filled pauses such as um and uh were obtained from ASR results to represent disfluencies. Given the successful usage of structural events on a wide range of NLP applications and the fact that the usage of these events is missing in the automatic speech assessment resea</context>
</contexts>
<marker>Neumeyer, Franco, Digalakis, Weintraub, 2000</marker>
<rawString>L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub. 2000. Automatic Scoring of Pronunciation Quality. Speech Communication, 30:83–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ortega</author>
</authors>
<title>Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing.</title>
<date>2003</date>
<journal>Applied Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="5144" citStr="Ortega, 2003" startWordPosition="787" endWordPosition="788">fluencyIn the second language acquisition (SLA) and related features lead to the moderate improvement child language development research fields, the lan- in the automated speech proficiency scoring. guage development is measured according to flu- There were limitations on using the features reency, accuracy, and complexity (Iwashita, 2006). ported in these SLA studies on standard language The syntactic complexity of learners’ writing data tests. For example, only a very limited number of has been extensively studied in the SLA commu- subjects (from 20 to 30 speakers) were used in these nity (Ortega, 2003). Recently, this study has been studies. Second, the speaking content was narraextended to the learner’s speaking data (Iwashita, tions of picture books or cartoon videos rather than 2006). Typical metrics for examining syntactic com- standard test questions. Therefore, we conducted a plexity include: length of production unit (e.g., T- study using a much larger data set obtained from real unit, which is defined as essentially a main clause speech tests to address these limitations. plus any other clauses which are dependent upon 3 Structural Event Annotation Convention it (Hunt, 1970), clause</context>
</contexts>
<marker>Ortega, 2003</marker>
<rawString>L. Ortega. 2003. Syntactic complexity measures and their relationship to L2 proficiency: A research synthesis of college-level L2 writing. Applied Linguistics, 24(4):492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
</authors>
<title>Speech segmentation and spoken document processing.</title>
<date>2008</date>
<journal>Signal Processing Magazine, IEEE,</journal>
<volume>25</volume>
<issue>3</issue>
<marker>Ostendorf, 2008</marker>
<rawString>M. Ostendorf et al. 2008. Speech segmentation and spoken document processing. Signal Processing Magazine, IEEE, 25(3):59–69, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Roll</author>
<author>J Frid</author>
<author>M Horne</author>
</authors>
<title>Measuring syntactic complexity in spontaneous spoken Swedish. Language and Speech,</title>
<date>2007</date>
<pages>50--2</pages>
<contexts>
<context position="15367" citStr="Roll et al., 2007" startWordPosition="2476" endWordPosition="2479"> (DEPC), and interruption points per clause (IPC), are derived: MLC = N,,,/NC 2It is either a subordinate clause that does not have a corresponding independent clause or a string of words without a subject or a verb that does not express a complete thought. 77 DEPC = NDEP/NC IPC = NIP/NC Furthermore, we elaborated the IPC feature. Disfluency is a complex behavior and is influenced by a variety of factors, such as proficiency level, speaking rate, and familiarity with speaking content. The complexity of utterances is also an important factor on the disfluency pattern. For example, Roll et al. (Roll et al., 2007) found that complexity of expression computed based on the language’s parsing tree structure influenced the frequency of disfluencies in their experiment on Swedish. Therefore, since disfluency frequency was not only influenced by the test-takers’ speaking proficiency but also by the utterance’s syntactic structure’s difficulty, we reduced the impact from the syntactic structure so that we can focus on speakers’ ability. For this purpose, we normalized IPC by dividing by some features related to syntactic-structure’s complexity, including MLC, DEPC, and both. Therefore, the following elaborate</context>
</contexts>
<marker>Roll, Frid, Horne, 2007</marker>
<rawString>M. Roll, J. Frid, and M. Horne. 2007. Measuring syntactic complexity in spontaneous spoken Swedish. Language and Speech, 50(2):227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>D Hakkani-Tur</author>
<author>G Tur</author>
</authors>
<title>Prosody-based automatic segmentation of speech into sentences and topics.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<pages>32--1</pages>
<contexts>
<context position="1664" citStr="Shriberg et al., 2000" startWordPosition="247" endWordPosition="250">izing IPC by (1) MLC (r = −0.386), (2) DEPC (r = −0.429), and (3) both (r = −0.462). In this research, the features derived from structural events of speech transcriptions are found to predict holistic scores measuring speaking proficiency. This suggests that structural events estimated on speech word strings provide a potential way for assessing nonnative speech. 1 Introduction In the last decade, a breakthrough in speech processing is the emergence of a lot of active research work on automatic estimation of structural events, e.g., sentence structure and disfluencies, on spontaneous speech (Shriberg et al., 2000; Liu, 2004; Os74 tendorf et al., 2008). The detected structural events have been successfully used in many natural language processing (NLP) applications (Ostendorf et al., 2008). However, the structural events in speech data haven’t been largely utilized by the research on using automatic speech recognition (ASR) technology to assess speech proficiency (Neumeyer et al., 2000; Zechner et al., 2007), which mainly used cues derived at the word level, such as timing information of spoken words. The information beyond the word level, e.g., clause/sentence structure of utterances and disfluency st</context>
</contexts>
<marker>Shriberg, Stolcke, Hakkani-Tur, Tur, 2000</marker>
<rawString>E. Shriberg, A. Stolcke, D. Hakkani-Tur, and G. Tur. 2000. Prosody-based automatic segmentation of speech into sentences and topics. Speech Communication, 32(1-2):127–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yoon</author>
</authors>
<title>Automated assessment of speech fluency for L2 English learners.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Illinois at Urbana-Champaign.</institution>
<contexts>
<context position="4435" citStr="Yoon (2009)" startWordPosition="679" endWordPosition="680">curacy, to measure speaking proficiency on 20 research works, the structural events were detected non-native English speakers. In this experiment, with a quite high accuracy. Furthermore, the de- disfluency-related factors, such as total voiced distected sentence and disfluency structures have been fluencies, had a high correlation with fluency (r = found to help many of the following NLP tasks, −0.45). However, the syntactic complexity factor e.g., speech parsing, information retrieval, machine only showed a moderate correlation (r = 0.310). translation, and extractive speech summary (Osten- Yoon (2009) implemented an automated disfluency dorf et al., 2008). detection method and found that the disfluencyIn the second language acquisition (SLA) and related features lead to the moderate improvement child language development research fields, the lan- in the automated speech proficiency scoring. guage development is measured according to flu- There were limitations on using the features reency, accuracy, and complexity (Iwashita, 2006). ported in these SLA studies on standard language The syntactic complexity of learners’ writing data tests. For example, only a very limited number of has been e</context>
</contexts>
<marker>Yoon, 2009</marker>
<rawString>S. Yoon. 2009. Automated assessment of speech fluency for L2 English learners. Ph.D. thesis, University of Illinois at Urbana-Champaign.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zechner</author>
<author>D Higgins</author>
<author>Xiaoming Xi</author>
</authors>
<title>SpeechRater: A Construct-Driven Approach to Scoring Spontaneous Non-Native Speech. In</title>
<date>2007</date>
<booktitle>Proc. SLaTE.</booktitle>
<contexts>
<context position="2066" citStr="Zechner et al., 2007" startWordPosition="309" endWordPosition="312">a breakthrough in speech processing is the emergence of a lot of active research work on automatic estimation of structural events, e.g., sentence structure and disfluencies, on spontaneous speech (Shriberg et al., 2000; Liu, 2004; Os74 tendorf et al., 2008). The detected structural events have been successfully used in many natural language processing (NLP) applications (Ostendorf et al., 2008). However, the structural events in speech data haven’t been largely utilized by the research on using automatic speech recognition (ASR) technology to assess speech proficiency (Neumeyer et al., 2000; Zechner et al., 2007), which mainly used cues derived at the word level, such as timing information of spoken words. The information beyond the word level, e.g., clause/sentence structure of utterances and disfluency structure, has not been or is poorly represented. For example, in Zechner et al. (2007), only special words for filled pauses such as um and uh were obtained from ASR results to represent disfluencies. Given the successful usage of structural events on a wide range of NLP applications and the fact that the usage of these events is missing in the automatic speech assessment research, a research questio</context>
</contexts>
<marker>Zechner, Higgins, Xi, 2007</marker>
<rawString>K. Zechner, D. Higgins, and Xiaoming Xi. 2007. SpeechRater: A Construct-Driven Approach to Scoring Spontaneous Non-Native Speech. In Proc. SLaTE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>