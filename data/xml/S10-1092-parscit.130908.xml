<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<title confidence="0.956914">
RACAI: Unsupervised WSD experiments @ SemEval-2, Task #17
</title>
<author confidence="0.975784">
Radu Ion
</author>
<affiliation confidence="0.960708">
Institute for AI, Romanian Academy
</affiliation>
<address confidence="0.9792905">
13, Calea 13 Septembrie, Bucharest
050711, Romania
</address>
<email confidence="0.998691">
radu@racai.ro
</email>
<sectionHeader confidence="0.997375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998480181818182">
This paper documents the participation of the
Research Institute for Artificial Intelligence of
the Romanian Academy (RACAI) to the Task
17 – All-words Word Sense Disambiguation
on a Specific Domain, of the SemEval-2 com-
petition. We describe three unsupervised WSD
systems that make extensive use of the Prince-
ton WordNet (WN) structure and WordNet
Domains in order to perform the disambigua-
tion. The best of them has been ranked the 12th
by the task organizers out of 29 judged runs.
</bodyText>
<sectionHeader confidence="0.999388" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928083333333">
Referring to the last SemEval (SemEval-1,
(Agirre et al., 2007a)) and to our recent work
(Ion and Ştefănescu, 2009), unsupervised Word
Sense Disambiguation (WSD) is still at the bot-
tom of WSD systems ranking with a significant
loss in performance when compared to super-
vised approaches. With Task #17 @ SemEval-2,
this observation is (probably1) reinforced but
another issue is re-brought to light: the difficulty
of supervised WSD systems to adapt to a given
domain (Agirre et al., 2009). With general scores
lower with at least 3% than 3 years ago in Task
#17 @ SemEval-1 which was a supposedly hard-
er task (general, no particular domain WSD was
required for all words), we observe that super-
vised WSD is certainly more difficult to imple-
ment in a real world application.
Our unsupervised WSD approach benefited
from the specification of this year’s Task #17
which was a domain-limited WSD, meaning that
the disambiguation would be applied to content
words drawn from a specific domain: the sur-
rounding environment. We worked under the
assumption that a term of the given domain
</bodyText>
<footnote confidence="0.9811505">
1 At the time of the writing we only know the systems rank-
ing without the supervised/unsupervised distinction.
</footnote>
<author confidence="0.224916">
Dan Ştefănescu
</author>
<affiliation confidence="0.207218">
Institute for AI, Romanian Academy
</affiliation>
<address confidence="0.498929">
13, Calea 13 Septembrie, Bucharest
050711, Romania
</address>
<email confidence="0.926477">
danstef@racai.ro
</email>
<bodyText confidence="0.999983764705882">
would have the same meaning with all its occur-
rences throughout the text. This hypothesis has
been put forth by Yarowsky (1993) as the “one
sense per discourse” hypothesis (OSPD for
short).
The task organizers offered a set of back-
ground documents with no sense annotations to
the competitors who want to train/tune their sys-
tems using data from the same domain as the
official test set. Working with the OSPD hypo-
thesis, we set off to construct/test domain specif-
ic WSD models from/on this corpus using the
WordNet Domains (Bentivogli et al., 2004). For
testing purposes, we have constructed an in-
house gold standard from this corpus that com-
prises of 1601 occurrences of 204 terms of the
“surrounding environment” domain that have
been automatically extracted with the highest
confidence. We have observed that our gold
standard (which has been independently anno-
tated by 3 annotators but on non-overlapping
sections which led to having no inter-annotator
agreement scores) obeys the OSPD hypothesis
which we think that is appropriate to domain-
limited WSD.
In what follows, we will briefly acknowledge
the usage of WordNet Domains in WSD, we will
then describe the construction of the corpus of
the background documents including here the
creation of an in-house gold standard, we will
then briefly describe our three WSD algorithms
and finally we will conclude with a discussion on
the ranking of our runs among the 29 evaluated
by the task organizers.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.685306833333333">
WordNet Domains is a hierarchy of labels that
have been assigned to WN synsets in a one to
(possible) many relationship (but the frequent
case is a single WN domain for a synset). A do-
main is the name of an area of knowledge that is
recognized as unitary (Bentivogli et al., 2004).
</bodyText>
<page confidence="0.96183">
411
</page>
<bodyText confidence="0.977483375">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 411–416,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Thus labels such as “architecture”, “sport” or
“medicine” are mapped onto synsets like
“arch(4)-noun”, “playing(2)-noun” or “chron-
ic(1)-adjective” because of the fact that the re-
spective concept evokes the domain.
WordNet Domains have been used in various
ways to perform WSD. The main usage of this
mapping is that the domains naturally create a
clustering of the WN senses of a literal thus of-
fering a sense inventory that is much coarser than
the fine sense distinctions of WN. For instance,
senses 1 (“a flat-bottomed motor vehicle that can
travel on land or water”) and 2 (“an airplane
designed to take off and land on water”) of the
noun “amphibian” are both mapped to the do-
main “transport” but the 3rd sense of the same
noun is mapped onto the domains “ani-
mals/biology” being the “cold-blooded verte-
brate typically living on land but breeding in
water; aquatic larvae undergo metamorphosis
into adult form” (definitions from version 2.0 of
the WN).
Vázquez et al. (2004) use WordNet Domains
to derive a new resource they call the Relevant
Domains in which, using WordNet glosses, they
extract the most representative words for a given
domain. Thus, for a word w and a domain d, the
Association Ratio formula between w and d is
AR( , ) P(  |) log 2
w d = w d ⋅
in which, for each synset its gloss has been POS
tagged and lemmatized. The probabilities are
computed counting pairs w, d in glosses (each
gloss has an associated d domain via its synset).
Using the Relevant Domains, the WSD proce-
dure for a given word w in its context C (a 100
words window centered in w), computes a simi-
larity measure between two vectors of AR
scores: the first vector is the vector of AR scores
of the sentence in which w appears and the other
is the vector of domain scores computed for the
gloss of a sense of w (both vectors are norma-
lized such that they contain the same domains).
The highest similarity gives the sense of w that is
closest to the domain vector of C. With this me-
thod, Vázquez et al. obtain a precision of 0.54
and a recall of 0.43 at the SensEval-2, English
All-Words Task placing them in the 10th position
out of 22 systems where the best one (a super-
vised system) achieved a 0.69 precision and an
equal recall.
Another approach to WSD using the WordNet
Domains is that of Magnini et al. (2002). The
method is remarkably similar to the previous one
in that the description of the vectors and the se-
lection of the assigned sense is the same. What
differs, is the weights that are assigned to each
domain in the vector. Magnini et al. distinguish
between text vectors (C vectors in the previous
presentation) and sense vectors. Text (or context)
vector weights are computed comparing domain
frequency in the context with the domain fre-
quency over the entire corpus (see Magnini et al.
(2002) for details). Sense vectors are derived
from sense-annotated data which qualifies this
method as a supervised one. The results that have
been reported at the same task the previous algo-
rithm participated (SensEval-2, English All-
Words Task), are: precision 0.748 and recall
0.357 (12th place).
Both the methods presented here are very sim-
ple and easy to adapt to different domains. One
of our methods (RACAI-1, see below) is even
simpler (because it makes the OSPD simplifying
assumption) and performs with approximately
the same accuracy as any of these methods judg-
ing by the rank of the system and the total num-
ber of participants.
</bodyText>
<sectionHeader confidence="0.9845535" genericHeader="method">
3 Using the Background Documents
collection
</sectionHeader>
<bodyText confidence="0.999969076923077">
Task #17 organizers have offered a set of back-
ground documents for training/tuning/testing
purposes. The corpus consists of 124 files from
the “surrounding environment” domain that have
been collected in the framework of the Kyoto
Project (http://www.kyoto-project.eu/).
First, we have assembled the files into a single
corpus in order to be able to apply some cleaning
procedures. These procedures involved the re-
moval of the paragraphs in which the proportion
of letters (Perl character class “[A-Za-z_-]”)
was less than 0.8 because the text contained a lot
of noise in form of lines of numbers and other
symbols which probably belonged to tables. The
next stage was to have the corpus POS-tagged,
lemmatized and chunked using the TTL web ser-
vice (Tufiş et al., 2008). The resulting file is an
XML encoded corpus which contains 136456
sentences with 2654446 tokens out of which
348896 are punctuation tokens.
In order to test our domain constrained WSD
algorithms, we decided to construct a test set
with the same dimension as the official test set of
about 2000 occurrences of content words specific
to the “surrounding environment” domain. In
doing this, we have employed a simple term ex-
</bodyText>
<equation confidence="0.96353775">
P(  |)
w d
P( )
w
</equation>
<page confidence="0.984341">
412
</page>
<bodyText confidence="0.9990386">
traction algorithm which considers that terms, as
opposed to words that are not domain specific,
are not evenly distributed throughout the corpus.
To formalize this, the corpus is a vector of lem-
mas C = [l1 , l2, ... , lN ] and for each unique lem-
ma lj ,1 ≤ j≤ N, we compute the mean of the
absolute differences of its indexes in C as
where f (lj) is the frequency of lj in C. We
also compute the standard deviation of these dif-
ferences from the mean as
</bodyText>
<table confidence="0.968786833333333">
footpri 11.68 reef 8.78
nt
deen 11.45 snapper 8.67
dune 10.57 biofuel 8.53
grouper Term Score Term Score vessel 8.35
gibbon 15.89 Oceanica 9.41
fleet 13.91 orangutan 9.19
9.67
sub-region 13.01 laurel 9.08
Amazon 12.41 coral 9.06
roundwood 12.26 polar 9.05
biocapacity 12.23 wrasse 8.80
</table>
<tableCaption confidence="0.823">
Table 1: The first 20 automatically extracted terms of
the “surrounding environment” domain
</tableCaption>
<figure confidence="0.986581423076923">
∑
j k N
&lt; ≤ , lj = lk ∧ ∀m, j &lt; m &lt; k, lj ≠ lm
−
1≤
f ( )
l j
1
=
µ
j
−
k
σ
)
( j k µ
− −
1≤
∑
k≤
N
j&lt;
f
(lj)−2
quantity
/
</figure>
<bodyText confidence="0.903450545454545">
which we take as a measure of the
evenness of a content word lemma distribution.
Thus, lemmas that are in the top of this list are
likely to be terms of the domain of the corpus (in
our case, the
do-
main). Table 1 contains the first 20 automatically
extracted terms along with their term score.
Having the list of terms of our domain, we
have selected the first ambiguous 210 (which
have more than 1 sense in
</bodyText>
<figure confidence="0.4574394">
an
σ
µ
“surroundingenvironment”
WN)
</figure>
<bodyText confidence="0.990864384615385">
d constructed
a test set in which each term has (at least) 10 oc-
currences in order to obtain a test corpus with at
least 2000 occurrences of the terms of the “sur-
rounding environment” domain. A large part of
these occurrences have been independently
sense-annotated by 3 annotators which worked
on disjoint sets of terms (70 terms each) in order
to finish as soon as possible. In the end we ma-
naged to annotate 1601 occurrences correspond-
ing to 204 terms.
When the gold standard for the test set was
ready, we checked to see if the OSPD hypothesis
holds. In order to determine if it does, we com-
puted the average number of annotated different
senses per term which is 1.36. In addition, consi-
dering the fact that out of 204 annotated terms,
145 are annotated with a single sense, we may
state that in this case, the OSPD hypothesis
holds.
2
in the same conditions as above.
With the mean and standard deviation of in-
dexes differences of a content word lemma com-
puted, we construct a list of all content word
lemmas that is sorted in descending order by the
</bodyText>
<sectionHeader confidence="0.995891" genericHeader="method">
4 The Description of the Systems
</sectionHeader>
<bodyText confidence="0.999832111111111">
Since we are committed to assign a unique sense
per word in the test set, we might as well try to
automatically induce a WSD model from the
background corpus in which, for each lemma
along with its POS tag that also exists in WN, a
single sense is listed that is derived from the cor-
pus. Then, for any test set of the same domain,
the algorithm would give the sense from the
WSD model to any of the occurrences of the
lemma.
What we actually did, was to find a list of
most frequent 2 WN domains (frequency count
extracted from the whole corpus) for each lemma
with its POS tag, and using these, to list all
senses of the lemma that are mapped onto these 2
domains (thus obtaining a reduction of the aver-
age number of senses per word). The steps of the
algorithm for the creation of the WSD model are:
</bodyText>
<listItem confidence="0.741109">
1. in the given corpus, for each lemma
and its POS-tag p normalized to WN
POS notation
</listItem>
<bodyText confidence="0.965551952380952">
for nouns,
for
verbs,
for adjectives and
for ad-
verbs), for each of its senses from WN,
increase by 1 each frequency of each
mapped domain;
2. for each lemma
with its POS-tag p, re-
tain only those senses that map onto the
most frequent 2 domains as determined
by the frequency list from the first step.
Using our 2.65M words background corpus to
build such a model (Table 2 contains a sample),
we have obtained a decrease in average ambigui-
ty degree (the average number of senses per con-
tent word lemma) from 2.43 to 2.14. If we set a
threshold of at least 1 for the term score of the
lemmas to be included into the WSD model
(which selects 12062 lemmas,
</bodyText>
<equation confidence="0.988879666666667">
l
(“n”
“v”
“a”
“b”
l
</equation>
<bodyText confidence="0.9018495">
meaning about 1/3
of all unique lemmas in the corpus), we obtain
</bodyText>
<page confidence="0.99863">
413
</page>
<bodyText confidence="0.999895857142857">
the same reduction thus contradicting our hypo-
thesis that the average ambiguity degree of terms
would be reduced more than the average ambigu-
ity degree of all words in the corpus. This result
might be due to the fact that the “factotum” do-
main is very frequent (much more frequent than
any of the other domains).
</bodyText>
<table confidence="0.9955754">
Lemma POS:Total no. First 2 selected Selected
of WN senses domains senses
fish n:2 animals,biology 1
Arctic n:1 geography 1
coral n:4 chemistry,animals 2,3,4
</table>
<tableCaption confidence="0.9951115">
Table 2: A sample of the WSD model built from the
background corpus
</tableCaption>
<bodyText confidence="0.998613">
In what follows, we will present our 3 systems
that use WSD models derived from the test sets
(both the in-house and the official ones). In the
Results section we will explain this choice.
</bodyText>
<subsectionHeader confidence="0.875978">
4.1 RACAI-1: WordNet Domains-driven,
Most Frequent Sense
</subsectionHeader>
<bodyText confidence="0.999888923076923">
The first system, as its name suggests, is very
simple: using the WSD model, it chooses the
most frequent sense (MFS) of the lemma l with
POS p according to WN (that is, the lowest num-
bered sense from the list of senses the lemma has
in the WSD model).
Trying this method on our in-house developed
test set, we obtain encouraging results: the over-
all accuracy (precision is equal with the recall
because all test set occurrences are tried) is at
least 4% over the general MFS baseline (sense
no. 1 in all cases). The Results section gives de-
tails.
</bodyText>
<subsectionHeader confidence="0.978292">
4.2 RACAI-2: The Lexical Chains Selection
</subsectionHeader>
<bodyText confidence="0.999992125">
With this system, we have tried to select only
one sense (not necessarily the most frequent one)
of lemma l with POS p from the WSD model.
The selection procedure is based on lexical
chains computation between senses of the target
word (the word to be disambiguated) and the
content words in its sentence in a manner that
will be explained below.
We have used the lexical chains description
and computation method described in (Ion and
$tefănescu, 2009). To reiterate, a lexical chain is
not simply a set of topically related words but
becomes a path of synsets in the WordNet hie-
rarchy. The lexical chain procedure is a function
of two WN synsets, LXC(s1, s2), that returns a
semantic relation path that one can follow to
reach s2 from s1. On the path from s2 to s1 there
are k synsets (k ≥ 0) and between 2 adjacent syn-
sets there is a WN semantic relation. Each lexical
chain can be assigned a certain score that we in-
terpret as a measure of the semantic similarity
(SS) between s1 and s2 (see (Ion and $tefănescu,
2009) and (Moldovan and Novischi, 2002) for
more details). Thus, the higher the value of
SS(s1, s2), the higher the semantic similarity be-
tween s1 and s2.
We have observed that using RACAI-1 on our
in-house test set but allowing it to select the most
frequent 2 senses of lemma l with POS p from
the WSD model, we obtain a whopping 82%
accuracy. With this observation, we tried to pro-
gram RACAI-2 to make a binary selection from
the first 2 most frequent senses of lemma l with
POS p from the WSD model in order to approach
the 82% percent accuracy limit which would
have been a very good result. The algorithm is as
follows: for a lemma l with POS p and a lemma
lc with POS pc from the context (sentence) of l,
compute the best lexical chain between any of
the first 2 senses of l and any of the first 2 senses
of lc according to the WSD model. If the first 2
senses of l are a and b and the first 2 senses of lc
are x and y and the best lexical chain score has
been found between a and y for instance, then
credit sense a of l with SS(a, y). Sum over all lc
from the context of l and select that sense of l
which has a maximum semantic similarity with
the context.
</bodyText>
<subsectionHeader confidence="0.9734825">
4.3 RACAI-3: Interpretation-based Sense
Assignment
</subsectionHeader>
<bodyText confidence="0.99998425">
This system tries to generate all the possible
sense assignments (called interpretations) to the
lemmas in a sentence. Thus, in principle, for
each content word lemma, all its WN senses are
considered thus generating an exponential explo-
sion of the sense assignments that can be attri-
buted to a sentence. If we have N content word
lemmas which have k senses on average, we ob-
tain a search space of kN interpretations which
have to be scored.
Using the observation mentioned above that
the first 2 senses of a lemma according to the
WSD model yields a performance of 82%, brings
the search space to 2N but for a large N, it is still
too big.
The solution we adopted (besides considering
the first 2 senses from the WSD model) consists
in segmenting the input sentence in M indepen-
dent segments of 10 content word lemmas each,
which will be processed independently, yielding
</bodyText>
<page confidence="0.997671">
414
</page>
<bodyText confidence="0.9853173">
a search space of at most 10
M ⋅ 2 of smaller in-
terpretations. The best interpretation per each
segment would thus be a part of the best interpre-
tation of the sentence. Next, we describe how we
score an interpretation.
For each sense s of a lemma l with POS p
(from the first 2 senses of l listed in the WSD
model) we compute an associated set of content
words (lemmas) from the following sources:
</bodyText>
<listItem confidence="0.998218727272727">
• all content word lemmas extracted from
the sense s corresponding gloss (disre-
garding the auxiliary verbs);
• all literals of the synset in which lemma l
with sense s exists;
• all literals of the synsets that are linked
with the synset l(s) by a relation of the fol-
lowing type: hypernym, near—antonym,
eng—derivative, hyponym, meronym, ho-
lonym, similar—to, derived;
• all content word lemmas extracted from
</listItem>
<bodyText confidence="0.946616666666667">
the glosses corresponding to synsets that
are linked with the l(s) synset by a relation
of the following type: hypernym,
eng—derivative, similar—to, derived;
With this feature set V of a sense s belonging to
lemma l with POS p, for a given interpretation (a
specific assignment of senses to each lemma in a
segment), its score S (initially 0) is computed
iteratively (for two adjacent position i and i + 1
in the segment) as
where the JXJ function is the cardinality function
on the set X and ← is the assignment operator.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999181444444444">
In order to run our WSD algorithms, we had to
extract WSD models. We tested the accuracy of
the disambiguation (onto the in-house developed
gold standard) with RACAI-1 and RACAI-2 sys-
tems (RACAI-3 was not ready at that time) with
models extracted a) from the whole background
corpus and b) from the in-house developed test
set (named here the RACAI test set, see section
3). The results are reported in Table 3 along with
RACAI-1 system returning the first 2 senses of a
lemma from the WSD model and the general
MFS baseline.
As we can see, the results with the WSD mod-
el extracted from the test set are marginally bet-
ter than the other results. This was the reason for
which we chose to extract the WSD model from
the official test set as opposed to using the WSD
model extracted from the background corpus.
</bodyText>
<table confidence="0.998077666666667">
RACAL Background
Test Set Corpus
RACAI-1 0.647 0.644
RACAI-1 (2 senses) 0.825 0.811
RACAI-2 0.591 0.582
MFS (sense no. 1) 0.602 0.602
</table>
<tableCaption confidence="0.9896035">
Table 3: RACAI systems results (accuracy) on the
RACAI test set
</tableCaption>
<bodyText confidence="0.97622812">
However, we did not research the possibility of
adding the official test set to either the RACAI
test set or the background corpus and extract
WSD models from there.
The official test set (named the SEMEVAL
test set here) contains 1398 occurrences of con-
tent words for disambiguation, out of which 366
are occurrences of verbs and 1032 are occur-
rences of nouns. These occurrences correspond
to 428 lemmas. Inspecting these lemmas, we
have found that there are many of them which
are not domain specific (in our case, specific to
the “surrounding environment” domain). For
instance, the verb to “be” is at the top of the list
with 99 occurrences. It is followed by the noun
“index” with 32 occurrences and by the noun
“network” with 22 occurrences. With fewer oc-
currences follow “use”, “include”, “show”, “pro-
vide”, “part” and so on. Of course, the SEMEV-
AL test set includes proper terms of the designat-
ed domain such as “area” (61 occurrences),
“species” (58 occurrences), “nature” (31 occur-
rences), “ocean”, “sea”, “water”, “planet”, etc.
Table 4 lists our official results on the SE-
MEVAL test set.
</bodyText>
<table confidence="0.9996856">
Precision Recall Rank
RACAI-1 0.461 0.46 #12
RACAI-2 0.351 0.35 #25
RACAI-3 0.433 0.431 #18
MFS 0.505 0.505 #6
</table>
<tableCaption confidence="0.999551">
Table 4: RACAI systems results (accuracy) on the
</tableCaption>
<bodyText confidence="0.927595625">
SEMEVAL test set
Precision is not equal to recall because of the fact
that our POS tagger found two occurrences of the
verb to “be” as auxiliaries which were ignored.
The column Rank indicates the place our systems
have in a 29 run ranking of all systems that parti-
cipated in Task 17 – All-words Word Sense Dis-
ambiguation on a Specific Domain, of the Se-
</bodyText>
<figure confidence="0.690931">
S ← S + Vi n Vi+1, Vi+1 ← Vi v Vi+1
</figure>
<page confidence="0.997037">
415
</page>
<bodyText confidence="0.9998478">
mEval-2 competition which was won by a sys-
tem that achieved a precision of 0.57 and a recall
of 0.555.
The differences with the runs on the RACAI
test set are significant but this can be explained
by the fact that our WordNet Domains WSD me-
thod cannot cope with general (domain indepen-
dent) WSD requirements in which the “one sense
per discourse” hypothesis does not necessarily
hold.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999906545454545">
Regarding the 3 systems that we entered in the
Task #17 @ SemEval-2, we think that the lexical
chains algorithm (RACAI-2) is the most promis-
ing even if it scored the lowest of the three. We
attribute its poor performances to the lexical
chains computation, especially to the weights of
the WN semantic relations that make up a chain.
Also, we will extend our research regarding the
correctness of lexical chains (the degree to which
a human judge will appreciate as correct or evoc-
ative or as common knowledge a semantic path
between two synsets).
We also want to check if our three systems
make the same mistakes or not in order to devise
a way in which we can combine their outputs.
RACAI is at the second participation in the
SemEval series of WSD competitions. We are
committed to improving the unsupervised WSD
technology which, we think, is more easily
adaptable and usable in real world applications.
We hope that SemEval-3 will reveal significant
improvements in this direction.
</bodyText>
<sectionHeader confidence="0.99765" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999553">
The work reported here was supported by the
Romanian Ministry of Education and Research
through the STAR project (no. 742/19.01.2009).
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990746">
Eneko Agirre, Lluís Màrquez and Richard Wicen-
towski, Eds., 2007. Proceedings of Semeval-2007
Workshop. Prague, Czech Republic: Association
for Computational Linguistics, 2007.
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, Piek Vos-
sen. 2009. SemEval-2010 Task 17: All-words Word
Sense Disambiguation on a Specific Domain. In
Proceedings of NAACL workshop on Semantic
Evaluations (SEW-2009). Boulder,Colorado, 2009.
Luisa Bentivogli, Pamela Forner, Bernardo Magnini
and Emanuele Pianta. 2004. Revising WordNet
Domains Hierarchy: Semantics, Coverage, and
Balancing. In COLING 2004 Workshop on &amp;quot;Multi-
lingual Linguistic Resources&amp;quot;, Geneva, Switzer-
land, August 28, 2004, pp. 101-108.
Radu Ion and Dan $tef nescu. 2009. Unsupervised
Word Sense Disambiguation with Lexical Chains
and Graph-based Context Formalization. In Zyg-
munt Vetulani, editor, Proceedings of the 4th Lan-
guage and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer
Science and Linguistics, pages 190–194, Poznan,
Poland, November 6–8 2009. Wydawnictwo
Poznanskie Sp.
Bernardo Magnini, Carlo Strapparava, Giovanni
Pezzulo, Alfio Gliozzo. 2002. The role of domain
information in Word Sense Disambiguation. Natu-
ral Language Engineering, 8(4), 359—373, De-
cember 2002.
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. In Proceedings of
the 19th International Conference on Computation-
al Linguistics, August 24 – September 01, 2002,
Taipei, Taiwan, pp. 1—7.
Dan Tufi$, Radu Ion, Alexandru Ceau$u and Dan
$tef nescu. 2008. RACAI&apos;s Linguistic Web Servic-
es. In Proceedings of the 6th Language Resources
and Evaluation Conference – LREC 2008, Marra-
kech, Morocco, May 2008. ELRA – European
Language Ressources Association. ISBN 2-
9517408-4-0.
Sonia Vázquez, Andrés Montoyo and German Ri-
gau. 2004. Using Relevant Domains Resource for
Word Sense Disambiguation. In Proceedings of the
International Conference on Artificial Intelligence
(IC-AI&apos;04), Las Vegas, Nevada, 2004.
David Yarowsky. 1993. One sense per collocation. In
ARPA Human Language Technology Workshop,
pp. 266–271, Princeton, NJ, 1993.
</reference>
<page confidence="0.998595">
416
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622834">
<title confidence="0.702923">RACAI: Unsupervised WSD experiments @ SemEval-2, Task #17</title>
<author confidence="0.950594">Radu Ion</author>
<affiliation confidence="0.998425">Institute for AI, Romanian Academy</affiliation>
<address confidence="0.9973995">13, Calea 13 Septembrie, Bucharest 050711, Romania</address>
<email confidence="0.992759">radu@racai.ro</email>
<abstract confidence="0.98255675">This paper documents the participation of the Research Institute for Artificial Intelligence of the Romanian Academy (RACAI) to the Task 17 – All-words Word Sense Disambiguation on a Specific Domain, of the SemEval-2 competition. We describe three unsupervised WSD systems that make extensive use of the Princeton WordNet (WN) structure and WordNet Domains in order to perform the disambigua- The best of them has been ranked the by the task organizers out of 29 judged runs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Lluís Màrquez and Richard Wicentowski, Eds.,</title>
<date>2007</date>
<booktitle>Proceedings of Semeval-2007 Workshop.</booktitle>
<location>Prague, Czech Republic:</location>
<marker>Agirre, 2007</marker>
<rawString>Eneko Agirre, Lluís Màrquez and Richard Wicentowski, Eds., 2007. Proceedings of Semeval-2007 Workshop. Prague, Czech Republic: Association for Computational Linguistics, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
<author>Christiane Fellbaum</author>
<author>Andrea Marchetti</author>
<author>Antonio Toral</author>
<author>Piek Vossen</author>
</authors>
<title>SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL workshop on Semantic Evaluations (SEW-2009). Boulder,Colorado,</booktitle>
<marker>Agirre, de Lacalle, Fellbaum, Marchetti, Toral, Vossen, 2009</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, Christiane Fellbaum, Andrea Marchetti, Antonio Toral, Piek Vossen. 2009. SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain. In Proceedings of NAACL workshop on Semantic Evaluations (SEW-2009). Boulder,Colorado, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Pamela Forner</author>
<author>Bernardo Magnini</author>
<author>Emanuele Pianta</author>
</authors>
<title>Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing.</title>
<date>2004</date>
<booktitle>In COLING 2004 Workshop on &amp;quot;Multilingual Linguistic Resources&amp;quot;,</booktitle>
<pages>101--108</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="2535" citStr="Bentivogli et al., 2004" startWordPosition="409" endWordPosition="412">cademy 13, Calea 13 Septembrie, Bucharest 050711, Romania danstef@racai.ro would have the same meaning with all its occurrences throughout the text. This hypothesis has been put forth by Yarowsky (1993) as the “one sense per discourse” hypothesis (OSPD for short). The task organizers offered a set of background documents with no sense annotations to the competitors who want to train/tune their systems using data from the same domain as the official test set. Working with the OSPD hypothesis, we set off to construct/test domain specific WSD models from/on this corpus using the WordNet Domains (Bentivogli et al., 2004). For testing purposes, we have constructed an inhouse gold standard from this corpus that comprises of 1601 occurrences of 204 terms of the “surrounding environment” domain that have been automatically extracted with the highest confidence. We have observed that our gold standard (which has been independently annotated by 3 annotators but on non-overlapping sections which led to having no inter-annotator agreement scores) obeys the OSPD hypothesis which we think that is appropriate to domainlimited WSD. In what follows, we will briefly acknowledge the usage of WordNet Domains in WSD, we will </context>
</contexts>
<marker>Bentivogli, Forner, Magnini, Pianta, 2004</marker>
<rawString>Luisa Bentivogli, Pamela Forner, Bernardo Magnini and Emanuele Pianta. 2004. Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing. In COLING 2004 Workshop on &amp;quot;Multilingual Linguistic Resources&amp;quot;, Geneva, Switzerland, August 28, 2004, pp. 101-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Ion</author>
<author>Dan tef nescu</author>
</authors>
<title>Unsupervised Word Sense Disambiguation with Lexical Chains and Graph-based Context Formalization.</title>
<date>2009</date>
<booktitle>Proceedings of the 4th Language and Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics,</booktitle>
<volume>6</volume>
<pages>190--194</pages>
<editor>In Zygmunt Vetulani, editor,</editor>
<location>Poznan, Poland,</location>
<marker>Ion, nescu, 2009</marker>
<rawString>Radu Ion and Dan $tef nescu. 2009. Unsupervised Word Sense Disambiguation with Lexical Chains and Graph-based Context Formalization. In Zygmunt Vetulani, editor, Proceedings of the 4th Language and Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, pages 190–194, Poznan, Poland, November 6–8 2009. Wydawnictwo Poznanskie Sp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzulo</author>
</authors>
<title>Alfio Gliozzo.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<pages>359--373</pages>
<contexts>
<context position="6226" citStr="Magnini et al. (2002)" startWordPosition="1053" endWordPosition="1056">f the sentence in which w appears and the other is the vector of domain scores computed for the gloss of a sense of w (both vectors are normalized such that they contain the same domains). The highest similarity gives the sense of w that is closest to the domain vector of C. With this method, Vázquez et al. obtain a precision of 0.54 and a recall of 0.43 at the SensEval-2, English All-Words Task placing them in the 10th position out of 22 systems where the best one (a supervised system) achieved a 0.69 precision and an equal recall. Another approach to WSD using the WordNet Domains is that of Magnini et al. (2002). The method is remarkably similar to the previous one in that the description of the vectors and the selection of the assigned sense is the same. What differs, is the weights that are assigned to each domain in the vector. Magnini et al. distinguish between text vectors (C vectors in the previous presentation) and sense vectors. Text (or context) vector weights are computed comparing domain frequency in the context with the domain frequency over the entire corpus (see Magnini et al. (2002) for details). Sense vectors are derived from sense-annotated data which qualifies this method as a super</context>
</contexts>
<marker>Magnini, Strapparava, Pezzulo, 2002</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo, Alfio Gliozzo. 2002. The role of domain information in Word Sense Disambiguation. Natural Language Engineering, 8(4), 359—373, December 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adrian Novischi</author>
</authors>
<title>Lexical chains for question answering.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="15165" citStr="Moldovan and Novischi, 2002" startWordPosition="2681" endWordPosition="2684">Ion and $tefănescu, 2009). To reiterate, a lexical chain is not simply a set of topically related words but becomes a path of synsets in the WordNet hierarchy. The lexical chain procedure is a function of two WN synsets, LXC(s1, s2), that returns a semantic relation path that one can follow to reach s2 from s1. On the path from s2 to s1 there are k synsets (k ≥ 0) and between 2 adjacent synsets there is a WN semantic relation. Each lexical chain can be assigned a certain score that we interpret as a measure of the semantic similarity (SS) between s1 and s2 (see (Ion and $tefănescu, 2009) and (Moldovan and Novischi, 2002) for more details). Thus, the higher the value of SS(s1, s2), the higher the semantic similarity between s1 and s2. We have observed that using RACAI-1 on our in-house test set but allowing it to select the most frequent 2 senses of lemma l with POS p from the WSD model, we obtain a whopping 82% accuracy. With this observation, we tried to program RACAI-2 to make a binary selection from the first 2 most frequent senses of lemma l with POS p from the WSD model in order to approach the 82% percent accuracy limit which would have been a very good result. The algorithm is as follows: for a lemma l</context>
</contexts>
<marker>Moldovan, Novischi, 2002</marker>
<rawString>Dan Moldovan and Adrian Novischi. 2002. Lexical chains for question answering. In Proceedings of the 19th International Conference on Computational Linguistics, August 24 – September 01, 2002, Taipei, Taiwan, pp. 1—7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufi</author>
<author>Radu Ion</author>
</authors>
<title>Alexandru Ceau$u and Dan $tef nescu.</title>
<date>2008</date>
<journal>ELRA – European Language Ressources Association. ISBN</journal>
<booktitle>In Proceedings of the 6th Language Resources and Evaluation Conference – LREC 2008,</booktitle>
<pages>2--9517408</pages>
<location>Marrakech, Morocco,</location>
<marker>Tufi, Ion, 2008</marker>
<rawString>Dan Tufi$, Radu Ion, Alexandru Ceau$u and Dan $tef nescu. 2008. RACAI&apos;s Linguistic Web Services. In Proceedings of the 6th Language Resources and Evaluation Conference – LREC 2008, Marrakech, Morocco, May 2008. ELRA – European Language Ressources Association. ISBN 2-9517408-4-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonia Vázquez</author>
</authors>
<title>Andrés Montoyo and German Rigau.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence (IC-AI&apos;04),</booktitle>
<location>Las Vegas, Nevada,</location>
<marker>Vázquez, 2004</marker>
<rawString>Sonia Vázquez, Andrés Montoyo and German Rigau. 2004. Using Relevant Domains Resource for Word Sense Disambiguation. In Proceedings of the International Conference on Artificial Intelligence (IC-AI&apos;04), Las Vegas, Nevada, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One sense per collocation.</title>
<date>1993</date>
<booktitle>In ARPA Human Language Technology Workshop,</booktitle>
<pages>266--271</pages>
<location>Princeton, NJ,</location>
<contexts>
<context position="2113" citStr="Yarowsky (1993)" startWordPosition="339" endWordPosition="340"> specification of this year’s Task #17 which was a domain-limited WSD, meaning that the disambiguation would be applied to content words drawn from a specific domain: the surrounding environment. We worked under the assumption that a term of the given domain 1 At the time of the writing we only know the systems ranking without the supervised/unsupervised distinction. Dan Ştefănescu Institute for AI, Romanian Academy 13, Calea 13 Septembrie, Bucharest 050711, Romania danstef@racai.ro would have the same meaning with all its occurrences throughout the text. This hypothesis has been put forth by Yarowsky (1993) as the “one sense per discourse” hypothesis (OSPD for short). The task organizers offered a set of background documents with no sense annotations to the competitors who want to train/tune their systems using data from the same domain as the official test set. Working with the OSPD hypothesis, we set off to construct/test domain specific WSD models from/on this corpus using the WordNet Domains (Bentivogli et al., 2004). For testing purposes, we have constructed an inhouse gold standard from this corpus that comprises of 1601 occurrences of 204 terms of the “surrounding environment” domain that</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>David Yarowsky. 1993. One sense per collocation. In ARPA Human Language Technology Workshop, pp. 266–271, Princeton, NJ, 1993.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>