<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014214">
<title confidence="0.994232">
Azmat: Sentence Similarity using Associative Matrices
</title>
<author confidence="0.999326">
Evan Jaffe Lifeng Jin David King Marten van Schijndel
</author>
<affiliation confidence="0.993092">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.996125">
{jaffe.59, jin.544, king.21381@osu.edu, vanschm@ling.osu.edu
</email>
<sectionHeader confidence="0.995318" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9822921">
This work uses recursive autoencoders
(Socher et al., 2011), word embeddings
(Pennington et al., 2014), associative matrices
(Schuler, 2014) and lexical overlap features
to model human judgments of sentential
similarity on SemEval-2015 Task 2: English
STS (Agirre et al., 2015). Results show a
modest positive correlation between system
predictions and human similarity scores,
ranking 69th out of 74 submitted systems.
</bodyText>
<sectionHeader confidence="0.998761" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999990823529412">
This work uses a support vector machine (SVM)
to determine the similarity of sentence pairs, tak-
ing as input the similarity judgments of four subsys-
tems: a set of surface features, unfolding recursive
autoencoders (URAE; Socher et al., 2011), Global
Vector word embeddings (GloVe; Pennington et al.,
2014), and the Schuler (2014) associative matrix ap-
proach using the Nguyen et al. (2012) Generalized
Categorial Grammar (GCG). Evaluation is run on
SemEval 2015 task 2, Semantic Textual Similarity
(STS), which includes a corpus of human similarity
judgments. The test set consists of 3000 randomly
chosen sentence pairs from a corpus of 8500 pairs,
which spans five domains (news headlines, image
captions, student answers, forum responses, and sen-
tences about belief). Similarity scores range from 0
(no similarity) to 5 (complete semantic equivalence).
</bodyText>
<sectionHeader confidence="0.986856" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.99930145">
All subsystems in Azmat are trained with sentences
from previous SemEval tasks 2012 - 2014 (Agirre et
al., 2012; Agirre et al., 2013; Agirre et al., 2014).
In total, 15,406 sentences were selected from the
Microsoft video, news headlines, images, and para-
phrase datasets. The main purpose of the subsys-
tems (excluding surface features) is to generate bina-
rized phrase-structure trees, which are used to create
cosine similarity features between multiple levels
of paired sentences. The URAE subsystem prepro-
cesses training sentences by parsing them with the
Stanford Parser (Klein and Manning, 2003) and then
binarizing. The associative matrix and GloVe sub-
systems use GCG parses of the training sentences,
obtained by training the Berkeley parser (Petrov and
Klein, 2007) with the Nguyen et al. (2012) GCG re-
annotated Penn Treebank. GCG parse trees are con-
verted into typed dependency graphs and binarized.
Around 2% of the sentences fail to parse; these are
omitted from the training set.
</bodyText>
<subsectionHeader confidence="0.993204">
2.1 Subsystem Combination
</subsectionHeader>
<bodyText confidence="0.99988">
Because vector composition methods vary across
subsystems, this work incorporates multiple subsys-
tems to give insight on which composition meth-
ods perform better at finding semantic textual sim-
ilarity. For each sentence, each subsystem generates
a single binarized phrase-structure tree with a sin-
gle embedding labeled at each node. Cosine similar-
ity scores are calculated between each node in one
tree and each node in the other tree, allowing com-
parison between input sentences at and across leaf,
phrasal and sentential levels. These similarity scores
are used to generate a feature vector for training an
SVM regressor with a linear kernel.1
</bodyText>
<footnote confidence="0.989511">
1http://scikit-learn.org/stable/modules/svm.html
</footnote>
<page confidence="0.96131">
159
</page>
<bodyText confidence="0.8797129">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 159–163,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
In order to generalize findings across sentence
pairs with varying lengths and tree structures, how-
ever, similarity scores must be consistently ordered
for the SVM and must generate a feature vector of
consistent length. To accomodate these constraints,
each output node (n) in a tree is assigned a composi-
tion depth (dn) based on the depth of its child nodes
(a and b):
</bodyText>
<equation confidence="0.99819525">
� (1)
0 if n is a leaf
dn =
max(da, db) + 1 otherwise
</equation>
<bodyText confidence="0.9997718">
Similarity between two nodes are grouped with
similarities of similar depth (x and y) into a vec-
tor (vxy), which is sorted before being concatenated
with other depth similarities2 to form the actual fea-
ture vector which will be input to the SVM:
</bodyText>
<equation confidence="0.988171666666667">
 |0.8 0.7 0.3 .. .  |0.9 0.4 0.2 .. .  |... (2)
� Y � � Y �
(d=0 d=0) (d=0 d=1)
</equation>
<bodyText confidence="0.999525">
The actual ordering of the concatenated depth
groups within the vector does not matter to the
downstream SVM classifier so long as the ordering
is consistent. Each vxy is given a constant length to
losslessly capture the similarity of balanced trees up
to 50 words in length:3
</bodyText>
<equation confidence="0.921658">
· (3)
2dx 2dy
</equation>
<bodyText confidence="0.99988575">
Each depth-pair subvector is duplicated up to the
needed length before being re-sorted. This approach
is analogous to a lossless version of the dynamic
pooling used by Socher et al. (2011).
Using the above approach, each subsystem gener-
ates its own version of the vector in (2). Then each
of those vectors is concatenated together to form the
entire SVM input vector.
</bodyText>
<subsectionHeader confidence="0.999418">
2.2 Surface Features
</subsectionHeader>
<bodyText confidence="0.9967375">
Surface features include n-gram overlap measures of
precision, recall, and F-score, where precision and
2Remember that similarities are computed between all nodes
in one tree and all nodes in the other tree, which results in some
similarities being computed between nodes of different depths.
3Consistent lengths permit each vxy to be at a consistent
position within the overall feature vector.
recall are defined as overlap from sentence A to sen-
tence B, and from sentence B to sentence A, re-
spectively. 1- through 3-grams are measured using
stemmed4 and unstemmed lexical items for each of
the 3 overlaps, resulting in a total of 18 surface fea-
tures. These features are based on those used by Das
and Smith (2009) for paraphrase detection.
</bodyText>
<subsectionHeader confidence="0.99993">
2.3 Unfolding Recursive Autoencoders
</subsectionHeader>
<bodyText confidence="0.999995842105263">
Socher et al. (2011) show good results for para-
phrase detection by using recursive autoencoders
(RAEs) to compose word embeddings into phrasal
and sentential embeddings, allowing similarity met-
rics at various structural levels. Their method uses
word embeddings from Turian et al. (2010) as input,
along with a binarized phrase-structure parse from
the Stanford Parser (Klein and Manning, 2003).
Given a binarized parse tree and leaf node embed-
dings, weight matrices are learned to both encode
and decode nodes above the leaves by minimizing
reconstruction error. ‘Unfolding’ refers to a learning
objective that reconstructs the entire subtree below
each node, not just the immediate children. Once a
model is trained, the learned encoding matrix can
generate embeddings at each node for novel sen-
tences. The current work uses the pre-trained model
and code from Socher et al. (2011) to generate fea-
tures from the previous SemEval task sentences.
</bodyText>
<subsectionHeader confidence="0.993878">
2.4 Associative Matrices
</subsectionHeader>
<bodyText confidence="0.9999915">
The associative matrix subsystem (AM) is in-
spired by a cognitively-grounded parsing model that
stores associations between words as dependency
relations (Nguyen et al., 2012; Wu and Schuler,
2011). Dependency-like associations are learned
from typed dependency graphs generated from gold
Nguyen et al. (2012) GCG annotations of Simple
Wikipedia. Dependency-based skip-grams are used
to build a co-occurrence matrix for all words, and
single value decomposition (SVD; Landauer and
Dumais, 1997) generates word embeddings with re-
duced dimensionality.
Each labeled dependency in the training data is
recorded in associative matrices by adding the outer
product of the governor and the dependent to the ma-
trix corresponding to the dependency label, creating
</bodyText>
<footnote confidence="0.286327">
4NLTK Lancaster Stemmer (Bird et al., 2009; Paice, 1990)
</footnote>
<figure confidence="0.974216">
50 50
|vxy|=
160
|{z}
red things
|{z}
red things
0
|{z}
ball
d = 0
|{z}
red ball
d = 1
;
·
 |{z }
matrix amod
|{z}
red
d = 0
!=
</figure>
<figureCaption confidence="0.835801">
Figure 1: Example vector composition using learned associative matrices. The dependency triple (red, ball,
amod) can be composed by first cueing red off of the amod matrix. The resulting target vector represents a
</figureCaption>
<bodyText confidence="0.82356525">
superposition of all governors red stands in an amod relation to. The target is then pointwise multiplied with
the embedding for ball to get a final phrasal representation. Note that words are depth 0, and the composition
results in an embedding at depth 1.
an associative matrix for each dependency type:
</bodyText>
<equation confidence="0.995787">
XMdeplabel = (¯� ⊗ ¯�) (4)
D
</equation>
<bodyText confidence="0.999826722222222">
where (u, v, deplabel) is a labeled dependency.
To compose a phrasal embedding, the dependent
word embedding is first inner multiplied with the as-
sociation matrix for the dependency type, a process
called cueing, which returns a target vector. Cue-
ing converts the dependent word embedding into the
space of its governor, essentially representing the su-
perposed vectors of all governors that the dependent
co-occurs with. Finally, the target is pointwise multi-
plied with the governor embedding, reinforcing the
influence of the observed governor and specifying
the meaning of the phrase as a combination of the
meaning of the dependent and of its governor. See
Table 1 for an example. All unknown (OOV) word
vectors are filled with ones to avoid contaminating
products during composition. As with all subsys-
tems, a single binarized parse tree with an embed-
ding at each node is the result.
</bodyText>
<subsectionHeader confidence="0.97513">
2.5 Global Vectors
</subsectionHeader>
<bodyText confidence="0.999589625">
Due to the success of word embeddings in word
similarity judgment tasks (Mikolov et al., 2013),
this work also makes use of Global Vector word
embeddings (GloVe; Pennington et al., 2014). 300-
dimensional GloVe embeddings are trained on 42
billion lower-cased tokens from the Stanford tok-
enized Common Crawl. These word embeddings are
combined using the same GCG structure as the AM
</bodyText>
<table confidence="0.988523333333333">
Model Unk p Known p Test p
SUGA 0.5370 0.6118 0.4512
UGA 0.4620 0.5493
SUA 0.5547 0.6233
SGA 0.5650 0.6299
SUG 0.5897 0.6566
</table>
<tableCaption confidence="0.994589">
Table 1: Model correlation with human judgments
</tableCaption>
<bodyText confidence="0.9826992">
on unknown and known domains in development as
each subsystem is omitted (included subsystems are
noted: S for surface, U for URAE, G for GloVe, and
A for AM). Final system performance on test data
for the task is also shown at right.
subsystem. Each node in the GCG tree is assigned
the embedding of that subtree’s head word, so the
‘red ball’ node is assigned the embedding for ‘ball’.
All OOV word vectors are drawn from a uniform
distribution between 0 and 1.
</bodyText>
<sectionHeader confidence="0.99716" genericHeader="evaluation">
3 Experiments and Error Analysis
</sectionHeader>
<bodyText confidence="0.999990636363636">
For development, 1000 pairs are held out of the
training data in jack-knifed batches. Table 1 shows
how the system performs when each subsystem is
omitted. Each model is designated using the first let-
ter of each subsystem, so the full model is named
SUGA. Table 1 (left) shows the performance of the
system when all of the held-out pairs are from a
single domain (e.g., news headlines) and thus ap-
proximates the system’s performance on unknown
domains. Table 1 (middle) shows the performance
when the held-out pairs are distributed evenly across
</bodyText>
<page confidence="0.995067">
161
</page>
<table confidence="0.999585875">
Dataset Leaf Comp Cross Full
Belief 0.5435 0.4966 0.4338 0.3587
Forums 0.4871 0.4114 0.4535 0.2933
Headlines 0.6583 0.6389 0.5826 0.5264
Images 0.6276 0.5587 0.5369 0.5145
Students 0.6399 0.5454 0.5222 0.4293
Mean 0.5913 0.5302 0.5058 0.4244
Wt. Mean 0.6103 0.5493 0.5213 0.4491
</table>
<tableCaption confidence="0.996511">
Table 2: Correlations with human judgments when
</tableCaption>
<bodyText confidence="0.99495421875">
only certain similarity relations are used: only word-
level similarity (leaf), only compositional non-leaf
similarity (comp), only similarity between leaf and
non-leaf nodes (cross), and permitting all similari-
ties (full). The weighted mean accounts for the pro-
portion of test cases in each dataset.
all domains and so estimates the system’s perfor-
mance on domains that are familiar. SemEval-2015
Task 2 test results are shown in Table 1 (right).5
Omission of the surface features results in a sharp
performance decrease, showing they capture com-
plementary information to other features. See UGA
model as compared to the SUGA model in Table 1.
Also observable in the table is that excluding any
one of the three main subsystems (URAE, GloVe,
AM) improves performance, which implies the full
system overfits to the training data.6 Since the com-
position method differs between all three subsys-
tems, and since URAE even uses a different under-
lying dependency structure, the overfit likely stems
from the fact that all three systems are computing
leaf/leaf similarity. Overfitting might be reduced by
either only using the leaf/leaf similarity from a sin-
gle system or by tuning the tolerance of the SVM.7
Since the development results suggest that the full
system overfits, it may be informative to test how
the different parts of the compositional framework
behave. To test this, the full SUGA system is re-
trained with some similarity relations removed (see
Table 2). When only leaf/leaf similarities are used
during training, the system performs the best. This
finding is likely due to the ubiquity of word-level
</bodyText>
<footnote confidence="0.999347">
5SUGA ranked 69th of 74 systems. For full results, see
http://alt.qcri.org/semeval2015/task2/index.php?id=results
6One example of overfitting is that the larger SUGA model
performs worse than the smaller SUG model for the same known
dataset (0.6118&lt;0.6566).
7The current work uses an untuned tolerance of 0.001.
</footnote>
<bodyText confidence="0.999971162162162">
similarity/analogy as a task, for which word embed-
dings such as GloVe were designed. System perfor-
mance declines when trained only on similarities be-
tween non-leaf nodes, suggesting the compositions
are less good at reflecting phrasal- and sentence-
level similarity. The system becomes even less accu-
rate when only using similarities between leaf nodes
and non-leaf nodes, which were hoped to enable the
system to capture similarities between more and less
general phrases (e.g., between ‘red ball’ and ‘ball’).
This finding is somewhat surprising since URAE is
thought to capture these types of similarities.
Although leaf/leaf similarities are useful, overre-
liance on non-compositional nodes causes problems
when comparing pairs with more abstract differ-
ences. For example, the system rates the following
unrelated pair as very similar despite completely dif-
ferent subject-predicate and modifier compositions:
Zoo worker dies after tiger attack
Teacher dies after attack in New Zealand
Further, while coarse feature selection (e.g., re-
moving all non-leaf features) improves perfor-
mance, it is not a foregone conclusion that composi-
tion features are completely uninformative. For ex-
ample, comparisons between nodes of similar depths
(e.g., 0-1, 4-3) might be more informative than node
comparisons of dissimilar depths (e.g., 1-7, 6-2),
so future work should determine whether there is
an information gradient when comparing composi-
tional nodes. Additionally, the fixed length chosen
in this work for each depth-paired subvector guaran-
tees a lossless representation of similarities between
balanced trees up to 50 words long, but the simi-
larity vectors involving non-leaf nodes become in-
creasingly lossy as the input trees become less bal-
anced. Therefore, the current system possibly under-
estimates the informativity of non-leaf features.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999901625">
The current work combined surface lexical features
with lexical and phrasal tree node similarity fea-
tures using URAE, GLoVe, and an associative ma-
trix composition system to model sentential similar-
ity. Since phrasal similarity is likely extremely use-
ful in determining sentence similarity, this work pro-
vides insight into the use and combination of multi-
ple phrasal similarity systems.
</bodyText>
<page confidence="0.996287">
162
</page>
<sectionHeader confidence="0.996518" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998615222222222">
This material is based upon work supported by
the National Science Foundation Graduate Research
Fellowship under Grant No. DGE-1343012. Any
opinion, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation. We would also like to
thank the anonymous reviewers for their helpful sug-
gestions and comments.
</bodyText>
<sectionHeader confidence="0.998153" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998582358024692">
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385–393. Association for Computational
Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Guo WeiWei. 2013. sem-2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics,
Association for Computational Linguistics.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Rada Mihalcea, German Rigau, and Janyce Wiebe.
2014. Semeval-2014 task 10: Multilingual semantic
textual similarity. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 81–91, Dublin, Ireland, August. Associ-
ation for Computational Linguistics and Dublin City
University.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Guo WeiWei,
Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Simi-
larity, English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), Denver, CO, June.
Association for Computational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural Language Processing with Python: Analyzing Text
with the Natural Language Toolkit. O’Reilly, Beijing.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probablistic quasi-synchronous recog-
nition. In Proc. of ACL-IJCNLP.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423–430, Sapporo, Japan.
T.K. Landauer and S.T. Dumais. 1997. A solution to
Plato’s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211–240.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781:1–12.
Luan Nguyen, Marten van Schijndel, and William
Schuler. 2012. Accurate unbounded dependency
recovery using generalized categorial grammars. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING ’12), pages
2125–2140, Mumbai, India.
Chris D. Paice. 1990. Another stemmer. SIGIR Forum,
24:56–61.
Jeffrey Pennington, Richard Socher, and Christopher D.
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of EMNLP.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
NAACL HLT 2007, pages 404–411, Rochester, New
York, April. Association for Computational Linguis-
tics.
William Schuler. 2014. Sentence processing in a vecto-
rial model of working memory. In Fifth Annual Work-
shop on Cognitive Modeling and Computational Lin-
guistics (CMCL 2014).
Richard Socher, Eric Huang, Jeffrey Pennington, An-
drew Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for para-
phrase detection. In Neural Information Processing
Systems (NIPS).
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL 2010.
Stephen Wu and William Schuler. 2011. Structured com-
position of semantic vectors. In Proceedings of the In-
ternational Workshop on Semantic Computing.
</reference>
<page confidence="0.999114">
163
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954407">
<title confidence="0.999932">Azmat: Sentence Similarity using Associative Matrices</title>
<author confidence="0.99988">Evan Jaffe Lifeng Jin David King Marten van_Schijndel</author>
<affiliation confidence="0.9989985">Department of The Ohio State University</affiliation>
<email confidence="0.973983">jin.544,king.21381@osu.edu,vanschm@ling.osu.edu</email>
<abstract confidence="0.998122363636364">This work uses recursive autoencoders (Socher et al., 2011), word embeddings (Pennington et al., 2014), associative matrices (Schuler, 2014) and lexical overlap features to model human judgments of sentential similarity on SemEval-2015 Task 2: English STS (Agirre et al., 2015). Results show a modest positive correlation between system predictions and human similarity scores, ranking 69th out of 74 submitted systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Mona Diab</author>
<author>Daniel Cer</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1649" citStr="Agirre et al., 2012" startWordPosition="238" endWordPosition="241">he Nguyen et al. (2012) Generalized Categorial Grammar (GCG). Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments. The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief). Similarity scores range from 0 (no similarity) to 5 (complete semantic equivalence). 2 System Overview All subsystems in Azmat are trained with sentences from previous SemEval tasks 2012 - 2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). In total, 15,406 sentences were selected from the Microsoft video, news headlines, images, and paraphrase datasets. The main purpose of the subsystems (excluding surface features) is to generate binarized phrase-structure trees, which are used to create cosine similarity features between multiple levels of paired sentences. The URAE subsystem preprocesses training sentences by parsing them with the Stanford Parser (Klein and Manning, 2003) and then binarizing. The associative matrix and GloVe subsystems use GCG parses of the training sentences, obta</context>
</contexts>
<marker>Agirre, Diab, Cer, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Guo WeiWei</author>
</authors>
<title>sem-2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1670" citStr="Agirre et al., 2013" startWordPosition="242" endWordPosition="245">2) Generalized Categorial Grammar (GCG). Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments. The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief). Similarity scores range from 0 (no similarity) to 5 (complete semantic equivalence). 2 System Overview All subsystems in Azmat are trained with sentences from previous SemEval tasks 2012 - 2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). In total, 15,406 sentences were selected from the Microsoft video, news headlines, images, and paraphrase datasets. The main purpose of the subsystems (excluding surface features) is to generate binarized phrase-structure trees, which are used to create cosine similarity features between multiple levels of paired sentences. The URAE subsystem preprocesses training sentences by parsing them with the Stanford Parser (Klein and Manning, 2003) and then binarizing. The associative matrix and GloVe subsystems use GCG parses of the training sentences, obtained by training the </context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, WeiWei, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Guo WeiWei. 2013. sem-2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>Semeval-2014 task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>81--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="1692" citStr="Agirre et al., 2014" startWordPosition="246" endWordPosition="249">rial Grammar (GCG). Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments. The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief). Similarity scores range from 0 (no similarity) to 5 (complete semantic equivalence). 2 System Overview All subsystems in Azmat are trained with sentences from previous SemEval tasks 2012 - 2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). In total, 15,406 sentences were selected from the Microsoft video, news headlines, images, and paraphrase datasets. The main purpose of the subsystems (excluding surface features) is to generate binarized phrase-structure trees, which are used to create cosine similarity features between multiple levels of paired sentences. The URAE subsystem preprocesses training sentences by parsing them with the Stanford Parser (Klein and Manning, 2003) and then binarizing. The associative matrix and GloVe subsystems use GCG parses of the training sentences, obtained by training the Berkeley parser (Petro</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. Semeval-2014 task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 81–91, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Guo WeiWei, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, CO,</location>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Guo WeiWei, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.</title>
<date>2009</date>
<location>O’Reilly, Beijing.</location>
<contexts>
<context position="7412" citStr="Bird et al., 2009" startWordPosition="1159" endWordPosition="1162">nd Schuler, 2011). Dependency-like associations are learned from typed dependency graphs generated from gold Nguyen et al. (2012) GCG annotations of Simple Wikipedia. Dependency-based skip-grams are used to build a co-occurrence matrix for all words, and single value decomposition (SVD; Landauer and Dumais, 1997) generates word embeddings with reduced dimensionality. Each labeled dependency in the training data is recorded in associative matrices by adding the outer product of the governor and the dependent to the matrix corresponding to the dependency label, creating 4NLTK Lancaster Stemmer (Bird et al., 2009; Paice, 1990) 50 50 |vxy|= 160 |{z} red things |{z} red things 0 |{z} ball d = 0 |{z} red ball d = 1 ; · |{z } matrix amod |{z} red d = 0 != Figure 1: Example vector composition using learned associative matrices. The dependency triple (red, ball, amod) can be composed by first cueing red off of the amod matrix. The resulting target vector represents a superposition of all governors red stands in an amod relation to. The target is then pointwise multiplied with the embedding for ball to get a final phrasal representation. Note that words are depth 0, and the composition results in an embeddin</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. O’Reilly, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probablistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="5580" citStr="Das and Smith (2009)" startWordPosition="884" endWordPosition="887">sion and 2Remember that similarities are computed between all nodes in one tree and all nodes in the other tree, which results in some similarities being computed between nodes of different depths. 3Consistent lengths permit each vxy to be at a consistent position within the overall feature vector. recall are defined as overlap from sentence A to sentence B, and from sentence B to sentence A, respectively. 1- through 3-grams are measured using stemmed4 and unstemmed lexical items for each of the 3 overlaps, resulting in a total of 18 surface features. These features are based on those used by Das and Smith (2009) for paraphrase detection. 2.3 Unfolding Recursive Autoencoders Socher et al. (2011) show good results for paraphrase detection by using recursive autoencoders (RAEs) to compose word embeddings into phrasal and sentential embeddings, allowing similarity metrics at various structural levels. Their method uses word embeddings from Turian et al. (2010) as input, along with a binarized phrase-structure parse from the Stanford Parser (Klein and Manning, 2003). Given a binarized parse tree and leaf node embeddings, weight matrices are learned to both encode and decode nodes above the leaves by minim</context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probablistic quasi-synchronous recognition. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2137" citStr="Klein and Manning, 2003" startWordPosition="312" endWordPosition="315">ence). 2 System Overview All subsystems in Azmat are trained with sentences from previous SemEval tasks 2012 - 2014 (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014). In total, 15,406 sentences were selected from the Microsoft video, news headlines, images, and paraphrase datasets. The main purpose of the subsystems (excluding surface features) is to generate binarized phrase-structure trees, which are used to create cosine similarity features between multiple levels of paired sentences. The URAE subsystem preprocesses training sentences by parsing them with the Stanford Parser (Klein and Manning, 2003) and then binarizing. The associative matrix and GloVe subsystems use GCG parses of the training sentences, obtained by training the Berkeley parser (Petrov and Klein, 2007) with the Nguyen et al. (2012) GCG reannotated Penn Treebank. GCG parse trees are converted into typed dependency graphs and binarized. Around 2% of the sentences fail to parse; these are omitted from the training set. 2.1 Subsystem Combination Because vector composition methods vary across subsystems, this work incorporates multiple subsystems to give insight on which composition methods perform better at finding semantic </context>
<context position="6038" citStr="Klein and Manning, 2003" startWordPosition="950" endWordPosition="953">d4 and unstemmed lexical items for each of the 3 overlaps, resulting in a total of 18 surface features. These features are based on those used by Das and Smith (2009) for paraphrase detection. 2.3 Unfolding Recursive Autoencoders Socher et al. (2011) show good results for paraphrase detection by using recursive autoencoders (RAEs) to compose word embeddings into phrasal and sentential embeddings, allowing similarity metrics at various structural levels. Their method uses word embeddings from Turian et al. (2010) as input, along with a binarized phrase-structure parse from the Stanford Parser (Klein and Manning, 2003). Given a binarized parse tree and leaf node embeddings, weight matrices are learned to both encode and decode nodes above the leaves by minimizing reconstruction error. ‘Unfolding’ refers to a learning objective that reconstructs the entire subtree below each node, not just the immediate children. Once a model is trained, the learned encoding matrix can generate embeddings at each node for novel sentences. The current work uses the pre-trained model and code from Socher et al. (2011) to generate features from the previous SemEval task sentences. 2.4 Associative Matrices The associative matrix</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<contexts>
<context position="7109" citStr="Landauer and Dumais, 1997" startWordPosition="1112" endWordPosition="1115">ned model and code from Socher et al. (2011) to generate features from the previous SemEval task sentences. 2.4 Associative Matrices The associative matrix subsystem (AM) is inspired by a cognitively-grounded parsing model that stores associations between words as dependency relations (Nguyen et al., 2012; Wu and Schuler, 2011). Dependency-like associations are learned from typed dependency graphs generated from gold Nguyen et al. (2012) GCG annotations of Simple Wikipedia. Dependency-based skip-grams are used to build a co-occurrence matrix for all words, and single value decomposition (SVD; Landauer and Dumais, 1997) generates word embeddings with reduced dimensionality. Each labeled dependency in the training data is recorded in associative matrices by adding the outer product of the governor and the dependent to the matrix corresponding to the dependency label, creating 4NLTK Lancaster Stemmer (Bird et al., 2009; Paice, 1990) 50 50 |vxy|= 160 |{z} red things |{z} red things 0 |{z} ball d = 0 |{z} red ball d = 1 ; · |{z } matrix amod |{z} red d = 0 != Figure 1: Example vector composition using learned associative matrices. The dependency triple (red, ball, amod) can be composed by first cueing red off of</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T.K. Landauer and S.T. Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781:1–12.</location>
<contexts>
<context position="9104" citStr="Mikolov et al., 2013" startWordPosition="1450" endWordPosition="1453">rnors that the dependent co-occurs with. Finally, the target is pointwise multiplied with the governor embedding, reinforcing the influence of the observed governor and specifying the meaning of the phrase as a combination of the meaning of the dependent and of its governor. See Table 1 for an example. All unknown (OOV) word vectors are filled with ones to avoid contaminating products during composition. As with all subsystems, a single binarized parse tree with an embedding at each node is the result. 2.5 Global Vectors Due to the success of word embeddings in word similarity judgment tasks (Mikolov et al., 2013), this work also makes use of Global Vector word embeddings (GloVe; Pennington et al., 2014). 300- dimensional GloVe embeddings are trained on 42 billion lower-cased tokens from the Stanford tokenized Common Crawl. These word embeddings are combined using the same GCG structure as the AM Model Unk p Known p Test p SUGA 0.5370 0.6118 0.4512 UGA 0.4620 0.5493 SUA 0.5547 0.6233 SGA 0.5650 0.6299 SUG 0.5897 0.6566 Table 1: Model correlation with human judgments on unknown and known domains in development as each subsystem is omitted (included subsystems are noted: S for surface, U for URAE, G for </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luan Nguyen</author>
<author>Marten van Schijndel</author>
<author>William Schuler</author>
</authors>
<title>Accurate unbounded dependency recovery using generalized categorial grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING ’12),</booktitle>
<pages>2125--2140</pages>
<location>Mumbai, India.</location>
<marker>Nguyen, van Schijndel, Schuler, 2012</marker>
<rawString>Luan Nguyen, Marten van Schijndel, and William Schuler. 2012. Accurate unbounded dependency recovery using generalized categorial grammars. In Proceedings of the 24th International Conference on Computational Linguistics (COLING ’12), pages 2125–2140, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris D Paice</author>
</authors>
<date>1990</date>
<journal>Another stemmer. SIGIR Forum,</journal>
<pages>24--56</pages>
<contexts>
<context position="7426" citStr="Paice, 1990" startWordPosition="1163" endWordPosition="1164">Dependency-like associations are learned from typed dependency graphs generated from gold Nguyen et al. (2012) GCG annotations of Simple Wikipedia. Dependency-based skip-grams are used to build a co-occurrence matrix for all words, and single value decomposition (SVD; Landauer and Dumais, 1997) generates word embeddings with reduced dimensionality. Each labeled dependency in the training data is recorded in associative matrices by adding the outer product of the governor and the dependent to the matrix corresponding to the dependency label, creating 4NLTK Lancaster Stemmer (Bird et al., 2009; Paice, 1990) 50 50 |vxy|= 160 |{z} red things |{z} red things 0 |{z} ball d = 0 |{z} red ball d = 1 ; · |{z } matrix amod |{z} red d = 0 != Figure 1: Example vector composition using learned associative matrices. The dependency triple (red, ball, amod) can be composed by first cueing red off of the amod matrix. The resulting target vector represents a superposition of all governors red stands in an amod relation to. The target is then pointwise multiplied with the embedding for ball to get a final phrasal representation. Note that words are depth 0, and the composition results in an embedding at depth 1. </context>
</contexts>
<marker>Paice, 1990</marker>
<rawString>Chris D. Paice. 1990. Another stemmer. SIGIR Forum, 24:56–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="970" citStr="Pennington et al., 2014" startWordPosition="133" endWordPosition="136">matrices (Schuler, 2014) and lexical overlap features to model human judgments of sentential similarity on SemEval-2015 Task 2: English STS (Agirre et al., 2015). Results show a modest positive correlation between system predictions and human similarity scores, ranking 69th out of 74 submitted systems. 1 Introduction This work uses a support vector machine (SVM) to determine the similarity of sentence pairs, taking as input the similarity judgments of four subsystems: a set of surface features, unfolding recursive autoencoders (URAE; Socher et al., 2011), Global Vector word embeddings (GloVe; Pennington et al., 2014), and the Schuler (2014) associative matrix approach using the Nguyen et al. (2012) Generalized Categorial Grammar (GCG). Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments. The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief). Similarity scores range from 0 (no similarity) to 5 (complete semantic equivalence). 2 System Overview All subsystems in Azmat are trai</context>
<context position="9196" citStr="Pennington et al., 2014" startWordPosition="1465" endWordPosition="1468"> the governor embedding, reinforcing the influence of the observed governor and specifying the meaning of the phrase as a combination of the meaning of the dependent and of its governor. See Table 1 for an example. All unknown (OOV) word vectors are filled with ones to avoid contaminating products during composition. As with all subsystems, a single binarized parse tree with an embedding at each node is the result. 2.5 Global Vectors Due to the success of word embeddings in word similarity judgment tasks (Mikolov et al., 2013), this work also makes use of Global Vector word embeddings (GloVe; Pennington et al., 2014). 300- dimensional GloVe embeddings are trained on 42 billion lower-cased tokens from the Stanford tokenized Common Crawl. These word embeddings are combined using the same GCG structure as the AM Model Unk p Known p Test p SUGA 0.5370 0.6118 0.4512 UGA 0.4620 0.5493 SUA 0.5547 0.6233 SGA 0.5650 0.6299 SUG 0.5897 0.6566 Table 1: Model correlation with human judgments on unknown and known domains in development as each subsystem is omitted (included subsystems are noted: S for surface, U for URAE, G for GloVe, and A for AM). Final system performance on test data for the task is also shown at ri</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL HLT</booktitle>
<pages>404--411</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="2310" citStr="Petrov and Klein, 2007" startWordPosition="339" endWordPosition="342">2014). In total, 15,406 sentences were selected from the Microsoft video, news headlines, images, and paraphrase datasets. The main purpose of the subsystems (excluding surface features) is to generate binarized phrase-structure trees, which are used to create cosine similarity features between multiple levels of paired sentences. The URAE subsystem preprocesses training sentences by parsing them with the Stanford Parser (Klein and Manning, 2003) and then binarizing. The associative matrix and GloVe subsystems use GCG parses of the training sentences, obtained by training the Berkeley parser (Petrov and Klein, 2007) with the Nguyen et al. (2012) GCG reannotated Penn Treebank. GCG parse trees are converted into typed dependency graphs and binarized. Around 2% of the sentences fail to parse; these are omitted from the training set. 2.1 Subsystem Combination Because vector composition methods vary across subsystems, this work incorporates multiple subsystems to give insight on which composition methods perform better at finding semantic textual similarity. For each sentence, each subsystem generates a single binarized phrase-structure tree with a single embedding labeled at each node. Cosine similarity scor</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL HLT 2007, pages 404–411, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Schuler</author>
</authors>
<title>Sentence processing in a vectorial model of working memory.</title>
<date>2014</date>
<booktitle>In Fifth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL</booktitle>
<contexts>
<context position="994" citStr="Schuler (2014)" startWordPosition="139" endWordPosition="140">al overlap features to model human judgments of sentential similarity on SemEval-2015 Task 2: English STS (Agirre et al., 2015). Results show a modest positive correlation between system predictions and human similarity scores, ranking 69th out of 74 submitted systems. 1 Introduction This work uses a support vector machine (SVM) to determine the similarity of sentence pairs, taking as input the similarity judgments of four subsystems: a set of surface features, unfolding recursive autoencoders (URAE; Socher et al., 2011), Global Vector word embeddings (GloVe; Pennington et al., 2014), and the Schuler (2014) associative matrix approach using the Nguyen et al. (2012) Generalized Categorial Grammar (GCG). Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments. The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief). Similarity scores range from 0 (no similarity) to 5 (complete semantic equivalence). 2 System Overview All subsystems in Azmat are trained with sentences from </context>
</contexts>
<marker>Schuler, 2014</marker>
<rawString>William Schuler. 2014. Sentence processing in a vectorial model of working memory. In Fifth Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="906" citStr="Socher et al., 2011" startWordPosition="124" endWordPosition="127">11), word embeddings (Pennington et al., 2014), associative matrices (Schuler, 2014) and lexical overlap features to model human judgments of sentential similarity on SemEval-2015 Task 2: English STS (Agirre et al., 2015). Results show a modest positive correlation between system predictions and human similarity scores, ranking 69th out of 74 submitted systems. 1 Introduction This work uses a support vector machine (SVM) to determine the similarity of sentence pairs, taking as input the similarity judgments of four subsystems: a set of surface features, unfolding recursive autoencoders (URAE; Socher et al., 2011), Global Vector word embeddings (GloVe; Pennington et al., 2014), and the Schuler (2014) associative matrix approach using the Nguyen et al. (2012) Generalized Categorial Grammar (GCG). Evaluation is run on SemEval 2015 task 2, Semantic Textual Similarity (STS), which includes a corpus of human similarity judgments. The test set consists of 3000 randomly chosen sentence pairs from a corpus of 8500 pairs, which spans five domains (news headlines, image captions, student answers, forum responses, and sentences about belief). Similarity scores range from 0 (no similarity) to 5 (complete semantic </context>
<context position="4664" citStr="Socher et al. (2011)" startWordPosition="731" endWordPosition="734"> form the actual feature vector which will be input to the SVM: |0.8 0.7 0.3 .. . |0.9 0.4 0.2 .. . |... (2) � Y � � Y � (d=0 d=0) (d=0 d=1) The actual ordering of the concatenated depth groups within the vector does not matter to the downstream SVM classifier so long as the ordering is consistent. Each vxy is given a constant length to losslessly capture the similarity of balanced trees up to 50 words in length:3 · (3) 2dx 2dy Each depth-pair subvector is duplicated up to the needed length before being re-sorted. This approach is analogous to a lossless version of the dynamic pooling used by Socher et al. (2011). Using the above approach, each subsystem generates its own version of the vector in (2). Then each of those vectors is concatenated together to form the entire SVM input vector. 2.2 Surface Features Surface features include n-gram overlap measures of precision, recall, and F-score, where precision and 2Remember that similarities are computed between all nodes in one tree and all nodes in the other tree, which results in some similarities being computed between nodes of different depths. 3Consistent lengths permit each vxy to be at a consistent position within the overall feature vector. reca</context>
<context position="6527" citStr="Socher et al. (2011)" startWordPosition="1029" endWordPosition="1032">rom Turian et al. (2010) as input, along with a binarized phrase-structure parse from the Stanford Parser (Klein and Manning, 2003). Given a binarized parse tree and leaf node embeddings, weight matrices are learned to both encode and decode nodes above the leaves by minimizing reconstruction error. ‘Unfolding’ refers to a learning objective that reconstructs the entire subtree below each node, not just the immediate children. Once a model is trained, the learned encoding matrix can generate embeddings at each node for novel sentences. The current work uses the pre-trained model and code from Socher et al. (2011) to generate features from the previous SemEval task sentences. 2.4 Associative Matrices The associative matrix subsystem (AM) is inspired by a cognitively-grounded parsing model that stores associations between words as dependency relations (Nguyen et al., 2012; Wu and Schuler, 2011). Dependency-like associations are learned from typed dependency graphs generated from gold Nguyen et al. (2012) GCG annotations of Simple Wikipedia. Dependency-based skip-grams are used to build a co-occurrence matrix for all words, and single value decomposition (SVD; Landauer and Dumais, 1997) generates word em</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric Huang, Jeffrey Pennington, Andrew Ng, and Christopher Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<contexts>
<context position="5931" citStr="Turian et al. (2010)" startWordPosition="934" endWordPosition="937">ntence B, and from sentence B to sentence A, respectively. 1- through 3-grams are measured using stemmed4 and unstemmed lexical items for each of the 3 overlaps, resulting in a total of 18 surface features. These features are based on those used by Das and Smith (2009) for paraphrase detection. 2.3 Unfolding Recursive Autoencoders Socher et al. (2011) show good results for paraphrase detection by using recursive autoencoders (RAEs) to compose word embeddings into phrasal and sentential embeddings, allowing similarity metrics at various structural levels. Their method uses word embeddings from Turian et al. (2010) as input, along with a binarized phrase-structure parse from the Stanford Parser (Klein and Manning, 2003). Given a binarized parse tree and leaf node embeddings, weight matrices are learned to both encode and decode nodes above the leaves by minimizing reconstruction error. ‘Unfolding’ refers to a learning objective that reconstructs the entire subtree below each node, not just the immediate children. Once a model is trained, the learned encoding matrix can generate embeddings at each node for novel sentences. The current work uses the pre-trained model and code from Socher et al. (2011) to </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proc. of ACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wu</author>
<author>William Schuler</author>
</authors>
<title>Structured composition of semantic vectors.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Semantic Computing.</booktitle>
<contexts>
<context position="6812" citStr="Wu and Schuler, 2011" startWordPosition="1071" endWordPosition="1074">struction error. ‘Unfolding’ refers to a learning objective that reconstructs the entire subtree below each node, not just the immediate children. Once a model is trained, the learned encoding matrix can generate embeddings at each node for novel sentences. The current work uses the pre-trained model and code from Socher et al. (2011) to generate features from the previous SemEval task sentences. 2.4 Associative Matrices The associative matrix subsystem (AM) is inspired by a cognitively-grounded parsing model that stores associations between words as dependency relations (Nguyen et al., 2012; Wu and Schuler, 2011). Dependency-like associations are learned from typed dependency graphs generated from gold Nguyen et al. (2012) GCG annotations of Simple Wikipedia. Dependency-based skip-grams are used to build a co-occurrence matrix for all words, and single value decomposition (SVD; Landauer and Dumais, 1997) generates word embeddings with reduced dimensionality. Each labeled dependency in the training data is recorded in associative matrices by adding the outer product of the governor and the dependent to the matrix corresponding to the dependency label, creating 4NLTK Lancaster Stemmer (Bird et al., 2009</context>
</contexts>
<marker>Wu, Schuler, 2011</marker>
<rawString>Stephen Wu and William Schuler. 2011. Structured composition of semantic vectors. In Proceedings of the International Workshop on Semantic Computing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>