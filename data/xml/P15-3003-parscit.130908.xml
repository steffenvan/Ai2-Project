<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.188194">
<title confidence="0.998374">
Learning representations for text-level discourse parsing
</title>
<author confidence="0.999009">
Gregor Weiss
</author>
<affiliation confidence="0.9987215">
Faculty of Computer and Information Science
University of Ljubljana
</affiliation>
<address confidence="0.607629">
Veˇcna pot 113, Ljubljana, Slovenia
</address>
<email confidence="0.996524">
gregor.weiss@student.uni-lj.si
</email>
<sectionHeader confidence="0.993818" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900190476191">
In the proposed doctoral work we will de-
sign an end-to-end approach for the chal-
lenging NLP task of text-level discourse
parsing. Instead of depending on mostly
hand-engineered sparse features and in-
dependent components for each subtask,
we propose a unified approach completely
based on deep learning architectures. To
train more expressive representations that
capture communicative functions and se-
mantic roles of discourse units and rela-
tions between them, we will jointly learn
all discourse parsing subtasks at different
layers of our architecture and share their
intermediate representations. By combin-
ing unsupervised training of word embed-
dings with our layer-wise multi-task learn-
ing of higher representations we hope to
reach or even surpass performance of cur-
rent state-of-the-art methods on annotated
English corpora.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820022727273">
Modern algorithms for natural language process-
ing (NLP) are based on statistical machine learn-
ing and require a computationally convenient rep-
resentation of input data. Unfortunately real-world
plain text is usually represented as an unstruc-
tured sequence of words with complex relations
between them. Therefore it is extremely impor-
tant to discover good representations in the form
of informative text features.
In NLP such features are almost always hand-
engineered sparse features and require expensive
human labor and expert knowledge to construct.
They are usually based on lexicons or features
extracted by other NLP subtasks and have the
form of hand-engineered extraction rules, regular
expressions, lemmatization, part-of-speech (POS)
tags, positions or lengths of arguments, tense
forms, syntactic parse trees, and similar. Although
such features are specific for a given language, do-
main, and task, they work well enough for sim-
ple NLP tasks, like named entity recognition or
POS tagging. Nevertheless, the ability to learn text
features and representations automatically would
have a lot of potential to improve state-of-the-art
performance on more challenging NLP tasks, such
as text-level discourse parsing. This may even be
more important for languages where progress in
NLP is still lacking.
Variants of deep learning architectures have
been shown to provide a different approach to
learning in which latent features are automatically
learned as distributed dense vectors. They man-
aged to represent meaningful relations with word
(Collobert, 2011), POS and dependency tag (Chen
and Manning, 2014), sentence (Guo and Diab,
2012), and document (Socher et al., 2012) embed-
dings and achieved surprising results for a number
of NLP tasks. It has been shown that both unsuper-
vised pre-training (Hinton et al., 2006) and multi-
task learning (Collobert and Weston, 2008) signif-
icantly improve their performance in the absence
of hand-engineered features. This makes them es-
pecially interesting for the problem of text-level
discourse parsing.
</bodyText>
<sectionHeader confidence="0.682981" genericHeader="method">
2 Text-level discourse parsing
</sectionHeader>
<bodyText confidence="0.998570272727273">
In natural language, a piece of text meant to com-
municate specific information, function, or knowl-
edge (clauses, sentences, or even paragraphs) is
called a discourse. They are often understood
only in relation to other discourse units (at any
level of grouping) and their combination creates a
joint meaning larger than individual unit’s mean-
ing alone (Mann and Thompson, 1988).
Discourse parsing is the task of determining
how these units are related to each other (like
in Figure 1) and plays a central role in a num-
</bodyText>
<page confidence="0.979487">
16
</page>
<note confidence="0.849271">
Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 16–21,
Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999942457142857">
ber of high-impact natural language processing
(NLP) applications, including text summarization,
sentence compression, sentiment analysis, and
question-answering. For analyzing different per-
spectives of discourse analysis researchers pro-
posed a number of theoretical frameworks and re-
leased annotated corpora, such as RST Discourse
Treebank (RST-DT) (Carlson et al., 2003) and
Penn Discourse Treebank (PDTB) (Prasad et al.,
2008). Both of these decompose discourse pars-
ing into a few subtasks and, like in most of NLP,
their success depends on expert knowledge of each
subtask and hand-engineering of more powerful
features (Feng and Hirst, 2012; Lin et al., 2014),
representations, and heuristics (Joty et al., 2013;
Prasad et al., 2010).
Despite recent progress in automatic discourse
segmentation and sentence-level parsing (Fisher
and Roark, 2007; Joty et al., 2012; Soricut and
Marcu, 2003), text-level discourse parsing re-
mains a significant challenge (Feng and Hirst,
2012; Ji and Eisenstein, 2014; Lin et al., 2014).
Traditional hand-engineering approaches unfortu-
nately seem to be insufficient, as discourses and
relations between them do not follow any strict
grammar or obvious rules.
Two main theoretical frameworks with English
corpus have been proposed to capture different
rhetorical characteristics, and serve different ap-
plications.
The Penn Discourse Treebank (PDTB) (Prasad
et al., 2008) is currently the largest discourse-
annotated corpus, consisting of 2159 articles from
Wall Street Journal. It strives to maintain a
theory-neutral approach by adopting the predicate-
argument view and independence of discourse re-
lations. In it either explicitly or implicitly given
discourse connectives, such as coordinating con-
junction (e.g. &amp;quot;and&amp;quot;, &amp;quot;but&amp;quot;), subordinating con-
junction (e.g. &amp;quot;if&amp;quot;, &amp;quot;because&amp;quot;), or discourse ad-
verbial (e.g. &amp;quot;however&amp;quot;, &amp;quot;also&amp;quot;), combine pairs
of discourse arguments into relations. For PDTB-
style discourse parsing, extracting argument spans
seems to be the most difficult subtask (Lin et al.,
2014), resulting in the best overall performance of
only 34.80% in F1-measure (Kong et al., 2014).
The RST Discourse Treebank (RST-DT) (Carl-
son et al., 2003) follows the theoretical frame-
work of Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988). It contains 385 annotated
documents from the Wall Street Journal with 18
high-level categories and 110 fine-grained rela-
tions. Any coherent text can be represented as
a RST discourse tree structure (like in Figure 1)
whose leaves are minimal non-overlapping text
spans called elementary discourse units. Adja-
cent nodes are joined depending on their discourse
relations to form a tree. In a mono-nuclear dis-
course relation one of the text spans is the nucleus,
which is more salient than the satellite, while in
a multi-nuclear relation all text spans are equally
important for interpretation. Performance of RST-
style discourse parsing is evaluated based on their
ability to locate spans of text that serve as argu-
ments (best 85.7% in F1-measure (Feng and Hirst,
2012)), identify which of the arguments is the nu-
cleus (best 71.1% in F1-measure (Ji and Eisen-
stein, 2014)), and tag the sense and location of dis-
course relations (best 61.6% in F1-measure (Ji and
Eisenstein, 2014)).
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="method">
3 Related work
</sectionHeader>
<bodyText confidence="0.999983966666667">
Early work on linguistic and computational
discourse analysis produced several theoretical
frameworks and one of the most influential is
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988). In order to automatically build
a hierarchical structure of a text, first approaches
(Marcu, 2000) relied mainly on discourse markers,
hand-engineered rules, and heuristics. Learning-
based approaches were first applied to identify
within-sentence discourse relations (Soricut and
Marcu, 2003), and only later to cross-sentence
text-level relations (Baldridge and Lascarides,
2005). They largely focused on lexical, syntac-
tic, and structural features, but the close rela-
tionship between discourse structure and seman-
tic meaning suggests that this may not be suf-
ficient (Prasad et al., 2008; Subba and Di Eu-
genio, 2009). Further work on discourse pars-
ing focused first on having a binary classifier
for determining whether two adjacent discourse
units should be merged, followed by a multi-class
classifier for determining which discourse rela-
tion should be assigned to the new subtree (Du-
Verle and Prendinger, 2009). Improved results
(Feng and Hirst, 2012) have been achieved by
incorporating rich linguistic features (Hernault et
al., 2010), including lexical semantics, and spe-
cific discourse production rules (Lin et al., 2009).
An alternative approach is based on jointly per-
forming detection and classification in a bottom-
</bodyText>
<page confidence="0.988717">
17
</page>
<listItem confidence="0.998675166666667">
• [The dollar finished lower yesterday,]e1 [after another session on Wall Street.]e2
• [Concern about the volatile U.S. stockmarket had faded in recent sessions,]e3 [and traders let
the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S.
gross national product is released.]e5
• [But movements in the Dow Jones Industrial Average yesterday put Wall Street back in the
spotlight]e6 [and inspired participants to bid the U.S. unit lower.]e7
</listItem>
<figureCaption confidence="0.9516005">
Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relations
between them in an RST discourse tree representation (Feng et al., 2014).
</figureCaption>
<bodyText confidence="0.999760803571429">
up fashion while distinguishing within-sentence
and cross-sentence relations (Joty et al., 2013) and
improved with discriminative reranking of dis-
course trees using tree kernels (Joty and Moschitti,
2014). It has been shown that constituent- and
dependency-based syntax and features based on
coreference links improve performance (Surdeanu
et al., 2015). The first PDTB-style end-to-end dis-
course parser (Lin et al., 2014) uses a connec-
tive list to identify explicit candidates, followed
by simple features and parse trees to extract ar-
guments and identify discourse relations. Classi-
fying implicit discourse relations can be improved
by combining distributed representations of parse
trees with coreferent entity mentions (Ji and Eisen-
stein, 2015). Extracting discourse arguments has
been attempted by using classic linear word tag-
ging with conditional random fields and global
features (Ghosh et al., 2012), identifying nodes in
constituent subtrees (Lin et al., 2014), and hybrid
merging and pruning of parse trees with integer
linear programming (Kong et al., 2014).
Deep learning architectures consist of multiple
layers of simple learning blocks stacked on each
other and, when well trained, tend to do a bet-
ter job at disentangling the underlying factors of
variation. Beginning with raw data, its represen-
tation is transformed into increasingly higher and
more abstract forms in each layer, until the final
low-dimensional features or representation useful
for a given task is reached. Their success is possi-
ble with breakthroughs and improvements in train-
ing techniques (like AdaGrad or Adam optimiza-
tion, rectifier function, dropout regularization) and
with initialization using unsupervised pre-training
(Hinton et al., 2006; Collobert, 2011) on massive
datasets (such as Wikipedia or Wall Street Jour-
nal). Pre-training helps deep networks to develop
natural abstractions and combined with multi-task
learning (Collobert and Weston, 2008) it can sig-
nificantly improve their performance in the ab-
sence of hand-engineered features.
Classic feed-forward architectures are inappro-
priate for processing text documents, because of
their variable length and natural representation as
a sequence of words. One approach to solve this
is to specify a transition-based processing mech-
anism (Chen and Manning, 2014; Ji and Eisen-
stein, 2014) and train a neural network classifier
to make parsing decisions. Recurrent neural net-
works (RNNs) (Elman, 1990) or their generaliza-
tion, recursive neural networks (Goller and Küch-
ler, 1996), represent a more direct approach by re-
cursively applying the same set of weights over the
sequence (temporal dimension) or structure (tree-
based). Li et al. (Li et al., 2015) have recently
</bodyText>
<page confidence="0.996013">
18
</page>
<bodyText confidence="0.999958837209302">
showed that only some NLP tasks benefit from
recursive models applied on syntactic parse trees
and recurrent models seem to be sufficient for dis-
course parsing. By stacking multiple hidden lay-
ers into a deep RNN makes them represent a tem-
poral hierarchy with multiple layers operating at
different time scales (Hermans and Schrauwen,
2013). Learning to store information over ex-
tended time intervals has been achieved with long
short-term memory (Hochreiter and Schmidhu-
ber, 1997), time delay neural network (Waibel et
al., 1989), or neural Turing machines (Graves et
al., 2014). Bidirectional variants of these mod-
els can incorporate information from preceding
as well as following tokens (Schuster and Pali-
wal, 1997). Recursive neural networks have also
been shown to support different task-specific rep-
resentations, such as matrix-vector representation
of words (Socher et al., 2012) or recurrent neu-
ral tensor networks (Socher et al., 2013). For
our discourse parsing task such deeper models,
that can learn abstract representations on different
time scales, might better model the discourse re-
lations between input vectors and (hopefully) cap-
ture their communicative functions and semantic
meaning.
A few initial attempts of applying representa-
tion learning to our task have already shown sub-
stantial performance improvements over previous
state-of-the-art. Ji and Eisenstein (Ji and Eisen-
stein, 2014) implement a shift-reduce discourse
parser on top of given RST-style discourse units
to simultaneously learn parsing and a discourse-
driven projection of features using support vector
machines with gradient-based updates. Li et al.
(Li, 2014) produce a distributed representation of
RST-style discourse units using recursive convolu-
tion on sentence parse trees and apply a classifier
to determine relations between them. Ji and Eisen-
stein (Ji and Eisenstein, 2014) also improved clas-
sification of PDTB-style implicit discourse rela-
tions by combining distributed representations of
parse trees with coreferent entity mentions.
</bodyText>
<sectionHeader confidence="0.976357" genericHeader="method">
4 Contribution to science
</sectionHeader>
<bodyText confidence="0.998304896551724">
Because text-level discourse parsing is an impor-
tant, yet still challenging NLP task, it is the focus
of our doctoral dissertation.
Method for text-level discourse parsing.
Instead of depending on mostly hand-engineered
sparse features and independent separately-
developed components for each subtask, we
propose a unified end-to-end approach for text-
level discourse parsing completely based on
deep learning architectures. First each of the
discourse parsing subtasks, such as argument
boundary detection, labeling, discourse relation
identification and sense classification, need to
be formulated in terms of RNNs and similar
derivable learning architectures. To benefit from
their ability to learn intermediate representations
they will be partially stacked on top of each order,
such that the last but one layer (i.e. output layer)
for each subtask is shared with other subtasks.
By placing increasingly more difficult subtasks
at different layers in one deep architecture,
they can benefit from each others intermediate
representations, improve robustness and training
speed. Figure 2 further combines unsupervised
training of word embeddings with our layer-wise
multi-task learning of higher representations
and illustrates our goal of a unified end-to-end
approach for text-level discourse parsing utilizing
different layers of representations.
</bodyText>
<figureCaption confidence="0.842752">
Figure 2: Illustration of our unified end-to-end ap-
proach for text-level discourse parsing with layer-
wise multi-task learning of higher representations.
</figureCaption>
<sectionHeader confidence="0.973833" genericHeader="method">
5 Work plan
</sectionHeader>
<bodyText confidence="0.9998972">
To accomplish this we will, on one hand, need to
find the best deep learning models for each of the
discourse parsing subtasks, suitable architecture,
activation functions, and figure out how to adapt
them to operate on sequential data and with each
other. This includes analyzing deep learning archi-
tectures, identifying their strengths, useful compo-
nents, and their suitability for our NLP task.
Afterwards combine them into one unified deep
learning architecture with shared intermediate rep-
</bodyText>
<page confidence="0.998257">
19
</page>
<bodyText confidence="0.99995765625">
resentations and unsupervised training of word
embeddings. Developing a prototype for shal-
low discourse parsing will open the door for find
the best initialization procedures, training func-
tions, learning rates, and similar. Shallow PDTB-
style discourse parsing is also a challenge on this
years CoNLL 2015 conference, where adjacent
text spans are not necessarily connected with dis-
course relations to form a tree.
Additionally we will experiment with new and
more expressive representations and structures
(like neural tensor networks) that could capture
communicative functions and semantic roles of
discourse units and relations between them.
Even though our method could be applied to
any plain text, we plan on evaluating it on stan-
dard annotated English corpora. After applying
our approach on at lest one of the corpora, we
intend to qualitatively analyze the identified dis-
course units and relations between them to gain in-
sights about its strengths and weaknesses. On the
other hand, the dataset will allow us to also quan-
titatively compare its performance to current state-
of-the-art methods. The procedure for our method
will begin by pre-training the weights in our deep
architecture on external unlabeled datasets (like
Wikipedia), than jointly train on all discourse pars-
ing subtasks on the training set, use a separate val-
idation set to optimize hyper-parameters, and es-
timate its performance on the test set. For eval-
uation purposes standard evaluation measures for
subtasks based on F1-scores will be used.
</bodyText>
<sectionHeader confidence="0.996155" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999199">
To increase the generality of our unified end-to-
end approach for text-level discourse parsing, we
will try to depend as little as possible on back-
ground knowledge in the form of hand-engineered
features for a specific language, domain, or task.
By incorporating various improvements in auto-
matic learning of features and representations we
hope to reach or even surpass performance of cur-
rent state-of-the-art methods on annotated English
corpora.
</bodyText>
<sectionHeader confidence="0.992623" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996675625">
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proc. 9th Conf. Comput. Nat. Lang. Learn., pages
96–103. Association for Computational Linguistics.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
Curr. New Dir. Discourse Dialogue, 22:85–112.
Danqi Chen and Christopher D Manning. 2014. A
Fast and Accurate Dependency Parser using Neural
Networks. In Proc. 2014 Conf. Empir. Methods Nat.
Lang. Process., pages 740–750.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning. In
Proc. 25th Int. Conf. Mach. Learn., volume 20,
pages 160–167.
Ronan Collobert. 2011. Deep Learning for Efficient
Discriminative Parsing. Int. Conf. Artif. Intell. Stat.,
15:224–232.
David A. DuVerle and Helmut Prendinger. 2009. A
novel discourse parser based on support vector ma-
chine classification. In Proc. Jt. Conf. 47th Annu.
Meet. ACL 4th Int. Jt. Conf. Nat. Lang. Process.
AFNLP, pages 665–673. Association for Computa-
tional Linguistics.
Jeffrey L. Elman. 1990. Finding structure in time* 1.
Cogn. Sci., 14(1990):179–211.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
Discourse Parsing with Rich Linguistic Features. In
Proc. 50th Annu. Meet. Assoc. Comput. Linguist.,
pages 60–68. Association for Computational Lin-
guistics.
Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst.
2014. The Impact of Deep Hierarchical Discourse
Structures in the Evaluation of Text Coherence. In
Proc. 25th Int. Conf. Comput. Linguist.
Seeger Fisher and Brian Roark. 2007. The utility of
parse-derived features for automatic discourse seg-
mentation. In Proc. 45th Annu. Meet. Assoc. Com-
put. Linguist., volume 45, pages 488–495.
Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-
hansson. 2012. Global features for shallow dis-
course parsing. In Annu. Meet. Spec. Interes. Gr.
Discourse Dialogue, pages 150–159.
Christoph Goller and Andreas Küchler. 1996. Learn-
ing Task-Dependent Distributed Representations by
Backpropagation Through Structure. In IEEE Int.
Conf. Neural Networks, pages 347–352.
Alex Graves, Greg Wayne, and Ivo Denihelka.
2014. Neural Turing Machines. arXiv Prepr.
arXiv410.5401, pages 1–26.
Weiwei Guo and Mona Diab. 2012. Modeling Sen-
tences in the Latent Space. In Proc. 50th Annu.
Meet. Assoc. Comput. Linguist., pages 864–872. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.95893">
20
</page>
<reference confidence="0.99955260952381">
Michiel Hermans and Benjamin Schrauwen. 2013.
Training and Analyzing Deep Recurrent Neural Net-
works. In Adv. Neural Inf. Process. Syst., volume 26,
pages 190–198.
Hugo Hernault, Helmut Prendinger, David A. DuVerle,
and Mitsuru Ishizuka. 2010. HILDA: A Discourse
Parser Using Support Vector Machine Classification.
Dialogue and Discourse, 1(3):1–33.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A Fast Learning Algorithm for Deep
Belief Nets. Neural Comput., 18:1527–1554.
Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long
Short-Term Memory. Neural Comput., 9(8):1735–
1780.
Yangfeng Ji and Jacob Eisenstein. 2014. Representa-
tion Learning for Text-level Discourse Parsing. In
Proc. 52nd Annu. Meet. Assoc. Comput. Linguist.,
pages 13–24.
Yangfeng Ji and Jacob Eisenstein. 2015. One Vector is
Not Enough: Entity-Augmented Distributed Seman-
tics for Discourse Relations. Trans. Assoc. Comput.
Linguist.
Shafiq Joty and Alessandro Moschitti. 2014. Discrim-
inative Reranking of Discourse Parses Using Tree
Kernels. In Proc. 2014 Conf. Empir. Methods Nat.
Lang. Process., pages 2049–2060.
Shafiq Joty, Giuseppe Carenini, and Raymond T.
Ng. 2012. A novel discriminative framework for
sentence-level discourse analysis. In Proc. 2012 Jt.
Conf. Empir. Methods Nat. Lang. Process. Comput.
Nat. Lang. Learn., pages 904–915. Association for
Computational Linguistics.
Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proc. 51st Annu. Meet.
Assoc. Comput. Linguist., pages 486–496.
Fang Kong, Hwee Tou, and Ng Guodong. 2014. A
Constituent-Based Approach to Argument Labeling
with Joint Inference in Discourse Parsing. In Conf.
Empir. Methods Nat. Lang. Process., pages 68–77.
Jiwei Li, Dan Jurafsky, and Eduard Hovy. 2015. When
Are Tree Structures Necessary for Deep Learning of
Representations? Arxiv.
Junyi Jessy Li. 2014. Reducing Sparsity Improves
the Recognition of Implicit Discourse Relations. In
Proc. SIGDIAL 2014 Conf., number June, pages
199–207.
Ziheng Lin, Min-yen Kan, and Hwee Tou Ng. 2009.
Recognizing Implicit Discourse Relations in the
Penn Discourse Treebank. In Proc. 2009 Conf. Em-
pir. Methods Nat. Lang. Process., pages 343–351.
Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A PDTB-Styled End-to-End Discourse Parser. Nat.
Lang. Eng., 20(2):151–184.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text-Interdisciplinary J.
Study Discourse, 8(3):243–281.
Daniel Marcu. 2000. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Comput.
Linguist., 26(38):395–448.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
Proc. Sixth Int. Conf. Lang. Resour. Eval., pages
2961–2968.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Exploiting Scope for Shallow Discourse Pars-
ing. In Int. Conf. Lang. Resour. Eval., pages 2076–
2083.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. IEEE Trans. Sig-
nal Process., 45(11):2673–2681.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic Compo-
sitionality through Recursive Matrix-Vector Spaces.
In Proc. 2012 Jt. Conf. Empir. Methods Nat. Lang.
Process. Comput. Nat. Lang. Learn., pages 1201–
1211. Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proc. Conf. Empir. Methods Nat. Lang.
Process., pages 1631–1642.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. Proc. 2003 Conf. North Am. Chapter Assoc.
Comput. Linguist. Hum. Lang. Technol., 1:228–235.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proc. Hum. Lang. Technol. 2009 Annu.
Conf. North Am. Chapter Assoc. Comput. Linguist.,
pages 566–574. Association for Computational Lin-
guistics.
Mihai Surdeanu, Thomas Hicks, and Marco A.
Valenzuela-Escarcega. 2015. Two Practical Rhetor-
ical Structure Theory Parsers. In Proc. North Am.
Chapter Assoc. Comput. Linguist.
Alexander Waibel, Toshiyuki Hanazawa, Geoffrey E.
Hinton, Kiyohiro Shikano, and Kevin J. Lang. 1989.
Phoneme recognition using time-delay neural net-
works. IEEE Trans. Acoust., 37(3):328–339.
</reference>
<page confidence="0.999439">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.341211">
<title confidence="0.999978">Learning representations for text-level discourse parsing</title>
<author confidence="0.986348">Gregor</author>
<affiliation confidence="0.9920795">Faculty of Computer and Information University of</affiliation>
<address confidence="0.597053">Veˇcna pot 113, Ljubljana,</address>
<email confidence="0.99174">gregor.weiss@student.uni-lj.si</email>
<abstract confidence="0.980547545454545">In the proposed doctoral work we will design an end-to-end approach for the challenging NLP task of text-level discourse parsing. Instead of depending on mostly hand-engineered sparse features and independent components for each subtask, we propose a unified approach completely based on deep learning architectures. To train more expressive representations that capture communicative functions and semantic roles of discourse units and relations between them, we will jointly learn all discourse parsing subtasks at different layers of our architecture and share their intermediate representations. By combining unsupervised training of word embeddings with our layer-wise multi-task learning of higher representations we hope to reach or even surpass performance of current state-of-the-art methods on annotated English corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alex Lascarides</author>
</authors>
<title>Probabilistic head-driven parsing for discourse structure.</title>
<date>2005</date>
<booktitle>In Proc. 9th Conf. Comput. Nat. Lang. Learn.,</booktitle>
<pages>96--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7670" citStr="Baldridge and Lascarides, 2005" startWordPosition="1137" endWordPosition="1140">F1-measure (Ji and Eisenstein, 2014)). 3 Related work Early work on linguistic and computational discourse analysis produced several theoretical frameworks and one of the most influential is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). In order to automatically build a hierarchical structure of a text, first approaches (Marcu, 2000) relied mainly on discourse markers, hand-engineered rules, and heuristics. Learningbased approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence text-level relations (Baldridge and Lascarides, 2005). They largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle and Prendinger, 2009). Improved results (Feng and Hirst, 2012) have been achieved by in</context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>Jason Baldridge and Alex Lascarides. 2005. Probabilistic head-driven parsing for discourse structure. In Proc. 9th Conf. Comput. Nat. Lang. Learn., pages 96–103. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. Curr. New Dir. Discourse Dialogue,</title>
<date>2003</date>
<pages>22--85</pages>
<contexts>
<context position="4189" citStr="Carlson et al., 2003" startWordPosition="613" endWordPosition="616"> these units are related to each other (like in Figure 1) and plays a central role in a num16 Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 16–21, Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics ber of high-impact natural language processing (NLP) applications, including text summarization, sentence compression, sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirs</context>
<context position="6000" citStr="Carlson et al., 2003" startWordPosition="882" endWordPosition="886">by adopting the predicateargument view and independence of discourse relations. In it either explicitly or implicitly given discourse connectives, such as coordinating conjunction (e.g. &amp;quot;and&amp;quot;, &amp;quot;but&amp;quot;), subordinating conjunction (e.g. &amp;quot;if&amp;quot;, &amp;quot;because&amp;quot;), or discourse adverbial (e.g. &amp;quot;however&amp;quot;, &amp;quot;also&amp;quot;), combine pairs of discourse arguments into relations. For PDTBstyle discourse parsing, extracting argument spans seems to be the most difficult subtask (Lin et al., 2014), resulting in the best overall performance of only 34.80% in F1-measure (Kong et al., 2014). The RST Discourse Treebank (RST-DT) (Carlson et al., 2003) follows the theoretical framework of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). It contains 385 annotated documents from the Wall Street Journal with 18 high-level categories and 110 fine-grained relations. Any coherent text can be represented as a RST discourse tree structure (like in Figure 1) whose leaves are minimal non-overlapping text spans called elementary discourse units. Adjacent nodes are joined depending on their discourse relations to form a tree. In a mono-nuclear discourse relation one of the text spans is the nucleus, which is more salient than the satellite,</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2003. Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. Curr. New Dir. Discourse Dialogue, 22:85–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A Fast and Accurate Dependency Parser using Neural Networks.</title>
<date>2014</date>
<booktitle>In Proc. 2014 Conf. Empir. Methods Nat. Lang. Process.,</booktitle>
<pages>740--750</pages>
<contexts>
<context position="2672" citStr="Chen and Manning, 2014" startWordPosition="385" endWordPosition="388">S tagging. Nevertheless, the ability to learn text features and representations automatically would have a lot of potential to improve state-of-the-art performance on more challenging NLP tasks, such as text-level discourse parsing. This may even be more important for languages where progress in NLP is still lacking. Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors. They managed to represent meaningful relations with word (Collobert, 2011), POS and dependency tag (Chen and Manning, 2014), sentence (Guo and Diab, 2012), and document (Socher et al., 2012) embeddings and achieved surprising results for a number of NLP tasks. It has been shown that both unsupervised pre-training (Hinton et al., 2006) and multitask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, function, or knowledge (clauses, sentences, or even</context>
<context position="11488" citStr="Chen and Manning, 2014" startWordPosition="1710" endWordPosition="1713">supervised pre-training (Hinton et al., 2006; Collobert, 2011) on massive datasets (such as Wikipedia or Wall Street Journal). Pre-training helps deep networks to develop natural abstractions and combined with multi-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a neural network classifier to make parsing decisions. Recurrent neural networks (RNNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more direct approach by recursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden la</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A Fast and Accurate Dependency Parser using Neural Networks. In Proc. 2014 Conf. Empir. Methods Nat. Lang. Process., pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.</title>
<date>2008</date>
<booktitle>In Proc. 25th Int. Conf. Mach. Learn.,</booktitle>
<volume>20</volume>
<pages>160--167</pages>
<contexts>
<context position="2937" citStr="Collobert and Weston, 2008" startWordPosition="429" endWordPosition="432">ant for languages where progress in NLP is still lacking. Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors. They managed to represent meaningful relations with word (Collobert, 2011), POS and dependency tag (Chen and Manning, 2014), sentence (Guo and Diab, 2012), and document (Socher et al., 2012) embeddings and achieved surprising results for a number of NLP tasks. It has been shown that both unsupervised pre-training (Hinton et al., 2006) and multitask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, function, or knowledge (clauses, sentences, or even paragraphs) is called a discourse. They are often understood only in relation to other discourse units (at any level of grouping) and their combination creates a joint meaning larger than individual unit’s meaning alone (Mann and Thompson, 1988). Discourse parsing</context>
<context position="11124" citStr="Collobert and Weston, 2008" startWordPosition="1656" endWordPosition="1659"> transformed into increasingly higher and more abstract forms in each layer, until the final low-dimensional features or representation useful for a given task is reached. Their success is possible with breakthroughs and improvements in training techniques (like AdaGrad or Adam optimization, rectifier function, dropout regularization) and with initialization using unsupervised pre-training (Hinton et al., 2006; Collobert, 2011) on massive datasets (such as Wikipedia or Wall Street Journal). Pre-training helps deep networks to develop natural abstractions and combined with multi-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a neural network classifier to make parsing decisions. Recurrent neural networks (RNNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more d</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proc. 25th Int. Conf. Mach. Learn., volume 20, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
</authors>
<title>Deep Learning for Efficient Discriminative Parsing.</title>
<date>2011</date>
<journal>Int. Conf. Artif. Intell. Stat.,</journal>
<pages>15--224</pages>
<contexts>
<context position="2623" citStr="Collobert, 2011" startWordPosition="379" endWordPosition="380">tasks, like named entity recognition or POS tagging. Nevertheless, the ability to learn text features and representations automatically would have a lot of potential to improve state-of-the-art performance on more challenging NLP tasks, such as text-level discourse parsing. This may even be more important for languages where progress in NLP is still lacking. Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors. They managed to represent meaningful relations with word (Collobert, 2011), POS and dependency tag (Chen and Manning, 2014), sentence (Guo and Diab, 2012), and document (Socher et al., 2012) embeddings and achieved surprising results for a number of NLP tasks. It has been shown that both unsupervised pre-training (Hinton et al., 2006) and multitask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, fu</context>
<context position="10928" citStr="Collobert, 2011" startWordPosition="1630" endWordPosition="1631">earning blocks stacked on each other and, when well trained, tend to do a better job at disentangling the underlying factors of variation. Beginning with raw data, its representation is transformed into increasingly higher and more abstract forms in each layer, until the final low-dimensional features or representation useful for a given task is reached. Their success is possible with breakthroughs and improvements in training techniques (like AdaGrad or Adam optimization, rectifier function, dropout regularization) and with initialization using unsupervised pre-training (Hinton et al., 2006; Collobert, 2011) on massive datasets (such as Wikipedia or Wall Street Journal). Pre-training helps deep networks to develop natural abstractions and combined with multi-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a n</context>
</contexts>
<marker>Collobert, 2011</marker>
<rawString>Ronan Collobert. 2011. Deep Learning for Efficient Discriminative Parsing. Int. Conf. Artif. Intell. Stat., 15:224–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A DuVerle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proc. Jt. Conf. 47th Annu. Meet. ACL 4th Int. Jt. Conf. Nat. Lang. Process. AFNLP,</booktitle>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8204" citStr="DuVerle and Prendinger, 2009" startWordPosition="1222" endWordPosition="1226">, 2003), and only later to cross-sentence text-level relations (Baldridge and Lascarides, 2005). They largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle and Prendinger, 2009). Improved results (Feng and Hirst, 2012) have been achieved by incorporating rich linguistic features (Hernault et al., 2010), including lexical semantics, and specific discourse production rules (Lin et al., 2009). An alternative approach is based on jointly performing detection and classification in a bottom17 • [The dollar finished lower yesterday,]e1 [after another session on Wall Street.]e2 • [Concern about the volatile U.S. stockmarket had faded in recent sessions,]e3 [and traders let the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S. gross nat</context>
</contexts>
<marker>DuVerle, Prendinger, 2009</marker>
<rawString>David A. DuVerle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proc. Jt. Conf. 47th Annu. Meet. ACL 4th Int. Jt. Conf. Nat. Lang. Process. AFNLP, pages 665–673. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time* 1.</title>
<date>1990</date>
<journal>Cogn. Sci.,</journal>
<volume>14</volume>
<issue>1990</issue>
<contexts>
<context position="11626" citStr="Elman, 1990" startWordPosition="1734" endWordPosition="1735">ep networks to develop natural abstractions and combined with multi-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a neural network classifier to make parsing decisions. Recurrent neural networks (RNNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more direct approach by recursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrau</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L. Elman. 1990. Finding structure in time* 1. Cogn. Sci., 14(1990):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level Discourse Parsing with Rich Linguistic Features.</title>
<date>2012</date>
<booktitle>In Proc. 50th Annu. Meet. Assoc. Comput. Linguist.,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4462" citStr="Feng and Hirst, 2012" startWordPosition="658" endWordPosition="661">l language processing (NLP) applications, including text summarization, sentence compression, sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corp</context>
<context position="6866" citStr="Feng and Hirst, 2012" startWordPosition="1021" endWordPosition="1024">an be represented as a RST discourse tree structure (like in Figure 1) whose leaves are minimal non-overlapping text spans called elementary discourse units. Adjacent nodes are joined depending on their discourse relations to form a tree. In a mono-nuclear discourse relation one of the text spans is the nucleus, which is more salient than the satellite, while in a multi-nuclear relation all text spans are equally important for interpretation. Performance of RSTstyle discourse parsing is evaluated based on their ability to locate spans of text that serve as arguments (best 85.7% in F1-measure (Feng and Hirst, 2012)), identify which of the arguments is the nucleus (best 71.1% in F1-measure (Ji and Eisenstein, 2014)), and tag the sense and location of discourse relations (best 61.6% in F1-measure (Ji and Eisenstein, 2014)). 3 Related work Early work on linguistic and computational discourse analysis produced several theoretical frameworks and one of the most influential is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). In order to automatically build a hierarchical structure of a text, first approaches (Marcu, 2000) relied mainly on discourse markers, hand-engineered rules, and heuristics. L</context>
<context position="8245" citStr="Feng and Hirst, 2012" startWordPosition="1229" endWordPosition="1232">vel relations (Baldridge and Lascarides, 2005). They largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle and Prendinger, 2009). Improved results (Feng and Hirst, 2012) have been achieved by incorporating rich linguistic features (Hernault et al., 2010), including lexical semantics, and specific discourse production rules (Lin et al., 2009). An alternative approach is based on jointly performing detection and classification in a bottom17 • [The dollar finished lower yesterday,]e1 [after another session on Wall Street.]e2 • [Concern about the volatile U.S. stockmarket had faded in recent sessions,]e3 [and traders let the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S. gross national product is released.]e5 • [But move</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level Discourse Parsing with Rich Linguistic Features. In Proc. 50th Annu. Meet. Assoc. Comput. Linguist., pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Ziheng Lin</author>
<author>Graeme Hirst</author>
</authors>
<date>2014</date>
<booktitle>The Impact of Deep Hierarchical Discourse Structures in the Evaluation of Text Coherence. In Proc. 25th Int. Conf. Comput. Linguist.</booktitle>
<contexts>
<context position="9173" citStr="Feng et al., 2014" startWordPosition="1370" endWordPosition="1373">ay,]e1 [after another session on Wall Street.]e2 • [Concern about the volatile U.S. stockmarket had faded in recent sessions,]e3 [and traders let the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S. gross national product is released.]e5 • [But movements in the Dow Jones Industrial Average yesterday put Wall Street back in the spotlight]e6 [and inspired participants to bid the U.S. unit lower.]e7 Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relations between them in an RST discourse tree representation (Feng et al., 2014). up fashion while distinguishing within-sentence and cross-sentence relations (Joty et al., 2013) and improved with discriminative reranking of discourse trees using tree kernels (Joty and Moschitti, 2014). It has been shown that constituent- and dependency-based syntax and features based on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit</context>
</contexts>
<marker>Feng, Lin, Hirst, 2014</marker>
<rawString>Vanessa Wei Feng, Ziheng Lin, and Graeme Hirst. 2014. The Impact of Deep Hierarchical Discourse Structures in the Evaluation of Text Coherence. In Proc. 25th Int. Conf. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seeger Fisher</author>
<author>Brian Roark</author>
</authors>
<title>The utility of parse-derived features for automatic discourse segmentation.</title>
<date>2007</date>
<booktitle>In Proc. 45th Annu. Meet. Assoc. Comput. Linguist.,</booktitle>
<volume>45</volume>
<pages>488--495</pages>
<contexts>
<context position="4667" citStr="Fisher and Roark, 2007" startWordPosition="687" endWordPosition="690">chers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been proposed to capture different rhetorical characteristics, and serve different applications. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is currently the largest discourseannotated</context>
</contexts>
<marker>Fisher, Roark, 2007</marker>
<rawString>Seeger Fisher and Brian Roark. 2007. The utility of parse-derived features for automatic discourse segmentation. In Proc. 45th Annu. Meet. Assoc. Comput. Linguist., volume 45, pages 488–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sucheta Ghosh</author>
<author>Giuseppe Riccardi</author>
<author>Richard Johansson</author>
</authors>
<title>Global features for shallow discourse parsing.</title>
<date>2012</date>
<booktitle>In Annu. Meet. Spec. Interes. Gr. Discourse Dialogue,</booktitle>
<pages>150--159</pages>
<contexts>
<context position="10083" citStr="Ghosh et al., 2012" startWordPosition="1502" endWordPosition="1505">on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with coreferent entity mentions (Ji and Eisenstein, 2015). Extracting discourse arguments has been attempted by using classic linear word tagging with conditional random fields and global features (Ghosh et al., 2012), identifying nodes in constituent subtrees (Lin et al., 2014), and hybrid merging and pruning of parse trees with integer linear programming (Kong et al., 2014). Deep learning architectures consist of multiple layers of simple learning blocks stacked on each other and, when well trained, tend to do a better job at disentangling the underlying factors of variation. Beginning with raw data, its representation is transformed into increasingly higher and more abstract forms in each layer, until the final low-dimensional features or representation useful for a given task is reached. Their success </context>
</contexts>
<marker>Ghosh, Riccardi, Johansson, 2012</marker>
<rawString>Sucheta Ghosh, Giuseppe Riccardi, and Richard Johansson. 2012. Global features for shallow discourse parsing. In Annu. Meet. Spec. Interes. Gr. Discourse Dialogue, pages 150–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Küchler</author>
</authors>
<title>Learning Task-Dependent Distributed Representations by Backpropagation Through Structure.</title>
<date>1996</date>
<booktitle>In IEEE Int. Conf. Neural Networks,</booktitle>
<pages>347--352</pages>
<contexts>
<context position="11704" citStr="Goller and Küchler, 1996" startWordPosition="1743" endWordPosition="1747">ti-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a neural network classifier to make parsing decisions. Recurrent neural networks (RNNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more direct approach by recursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has bee</context>
</contexts>
<marker>Goller, Küchler, 1996</marker>
<rawString>Christoph Goller and Andreas Küchler. 1996. Learning Task-Dependent Distributed Representations by Backpropagation Through Structure. In IEEE Int. Conf. Neural Networks, pages 347–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Greg Wayne</author>
<author>Ivo Denihelka</author>
</authors>
<date>2014</date>
<booktitle>Neural Turing Machines. arXiv Prepr. arXiv410.5401,</booktitle>
<pages>1--26</pages>
<contexts>
<context position="12475" citStr="Graves et al., 2014" startWordPosition="1869" endWordPosition="1872">et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better model the discourse relations between input vectors and (hopefully) capture their communicative functi</context>
</contexts>
<marker>Graves, Wayne, Denihelka, 2014</marker>
<rawString>Alex Graves, Greg Wayne, and Ivo Denihelka. 2014. Neural Turing Machines. arXiv Prepr. arXiv410.5401, pages 1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling Sentences in the Latent Space.</title>
<date>2012</date>
<booktitle>In Proc. 50th Annu. Meet. Assoc. Comput. Linguist.,</booktitle>
<pages>864--872</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2703" citStr="Guo and Diab, 2012" startWordPosition="390" endWordPosition="393">y to learn text features and representations automatically would have a lot of potential to improve state-of-the-art performance on more challenging NLP tasks, such as text-level discourse parsing. This may even be more important for languages where progress in NLP is still lacking. Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors. They managed to represent meaningful relations with word (Collobert, 2011), POS and dependency tag (Chen and Manning, 2014), sentence (Guo and Diab, 2012), and document (Socher et al., 2012) embeddings and achieved surprising results for a number of NLP tasks. It has been shown that both unsupervised pre-training (Hinton et al., 2006) and multitask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, function, or knowledge (clauses, sentences, or even paragraphs) is called a discou</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling Sentences in the Latent Space. In Proc. 50th Annu. Meet. Assoc. Comput. Linguist., pages 864–872. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michiel Hermans</author>
<author>Benjamin Schrauwen</author>
</authors>
<title>Training and Analyzing Deep Recurrent Neural Networks.</title>
<date>2013</date>
<booktitle>In Adv. Neural Inf. Process. Syst.,</booktitle>
<volume>26</volume>
<pages>190--198</pages>
<contexts>
<context position="12236" citStr="Hermans and Schrauwen, 2013" startWordPosition="1832" endWordPosition="1835">NNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more direct approach by recursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., </context>
</contexts>
<marker>Hermans, Schrauwen, 2013</marker>
<rawString>Michiel Hermans and Benjamin Schrauwen. 2013. Training and Analyzing Deep Recurrent Neural Networks. In Adv. Neural Inf. Process. Syst., volume 26, pages 190–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>David A DuVerle</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse,</title>
<date>2010</date>
<contexts>
<context position="8330" citStr="Hernault et al., 2010" startWordPosition="1241" endWordPosition="1244">tactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle and Prendinger, 2009). Improved results (Feng and Hirst, 2012) have been achieved by incorporating rich linguistic features (Hernault et al., 2010), including lexical semantics, and specific discourse production rules (Lin et al., 2009). An alternative approach is based on jointly performing detection and classification in a bottom17 • [The dollar finished lower yesterday,]e1 [after another session on Wall Street.]e2 • [Concern about the volatile U.S. stockmarket had faded in recent sessions,]e3 [and traders let the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S. gross national product is released.]e5 • [But movements in the Dow Jones Industrial Average yesterday put Wall Street back in the spotl</context>
</contexts>
<marker>Hernault, Prendinger, DuVerle, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, David A. DuVerle, and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Simon Osindero</author>
<author>Yee-Whye Teh</author>
</authors>
<title>A Fast Learning Algorithm for Deep Belief Nets.</title>
<date>2006</date>
<journal>Neural Comput.,</journal>
<pages>18--1527</pages>
<contexts>
<context position="2885" citStr="Hinton et al., 2006" startWordPosition="421" endWordPosition="424">scourse parsing. This may even be more important for languages where progress in NLP is still lacking. Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors. They managed to represent meaningful relations with word (Collobert, 2011), POS and dependency tag (Chen and Manning, 2014), sentence (Guo and Diab, 2012), and document (Socher et al., 2012) embeddings and achieved surprising results for a number of NLP tasks. It has been shown that both unsupervised pre-training (Hinton et al., 2006) and multitask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, function, or knowledge (clauses, sentences, or even paragraphs) is called a discourse. They are often understood only in relation to other discourse units (at any level of grouping) and their combination creates a joint meaning larger than individual unit’s meanin</context>
<context position="10910" citStr="Hinton et al., 2006" startWordPosition="1626" endWordPosition="1629">le layers of simple learning blocks stacked on each other and, when well trained, tend to do a better job at disentangling the underlying factors of variation. Beginning with raw data, its representation is transformed into increasingly higher and more abstract forms in each layer, until the final low-dimensional features or representation useful for a given task is reached. Their success is possible with breakthroughs and improvements in training techniques (like AdaGrad or Adam optimization, rectifier function, dropout regularization) and with initialization using unsupervised pre-training (Hinton et al., 2006; Collobert, 2011) on massive datasets (such as Wikipedia or Wall Street Journal). Pre-training helps deep networks to develop natural abstractions and combined with multi-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2</context>
</contexts>
<marker>Hinton, Osindero, Teh, 2006</marker>
<rawString>Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006. A Fast Learning Algorithm for Deep Belief Nets. Neural Comput., 18:1527–1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>Jürgen Schmidhuber</author>
</authors>
<title>Long Short-Term Memory.</title>
<date>1997</date>
<journal>Neural Comput.,</journal>
<volume>9</volume>
<issue>8</issue>
<pages>1780</pages>
<contexts>
<context position="12377" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1852" endWordPosition="1856">cursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better mode</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput., 9(8):1735– 1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Representation Learning for Text-level Discourse Parsing.</title>
<date>2014</date>
<booktitle>In Proc. 52nd Annu. Meet. Assoc. Comput. Linguist.,</booktitle>
<pages>13--24</pages>
<contexts>
<context position="4821" citStr="Ji and Eisenstein, 2014" startWordPosition="711" endWordPosition="714"> Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been proposed to capture different rhetorical characteristics, and serve different applications. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is currently the largest discourseannotated corpus, consisting of 2159 articles from Wall Street Journal. It strives to maintain a theory-neutral approach by adopting the predicateargument view and</context>
<context position="6967" citStr="Ji and Eisenstein, 2014" startWordPosition="1038" endWordPosition="1042">n-overlapping text spans called elementary discourse units. Adjacent nodes are joined depending on their discourse relations to form a tree. In a mono-nuclear discourse relation one of the text spans is the nucleus, which is more salient than the satellite, while in a multi-nuclear relation all text spans are equally important for interpretation. Performance of RSTstyle discourse parsing is evaluated based on their ability to locate spans of text that serve as arguments (best 85.7% in F1-measure (Feng and Hirst, 2012)), identify which of the arguments is the nucleus (best 71.1% in F1-measure (Ji and Eisenstein, 2014)), and tag the sense and location of discourse relations (best 61.6% in F1-measure (Ji and Eisenstein, 2014)). 3 Related work Early work on linguistic and computational discourse analysis produced several theoretical frameworks and one of the most influential is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). In order to automatically build a hierarchical structure of a text, first approaches (Marcu, 2000) relied mainly on discourse markers, hand-engineered rules, and heuristics. Learningbased approaches were first applied to identify within-sentence discourse relations (Soricut a</context>
<context position="11514" citStr="Ji and Eisenstein, 2014" startWordPosition="1714" endWordPosition="1718">(Hinton et al., 2006; Collobert, 2011) on massive datasets (such as Wikipedia or Wall Street Journal). Pre-training helps deep networks to develop natural abstractions and combined with multi-task learning (Collobert and Weston, 2008) it can significantly improve their performance in the absence of hand-engineered features. Classic feed-forward architectures are inappropriate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a neural network classifier to make parsing decisions. Recurrent neural networks (RNNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more direct approach by recursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes</context>
<context position="13303" citStr="Ji and Eisenstein, 2014" startWordPosition="1989" endWordPosition="1993">erent task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better model the discourse relations between input vectors and (hopefully) capture their communicative functions and semantic meaning. A few initial attempts of applying representation learning to our task have already shown substantial performance improvements over previous state-of-the-art. Ji and Eisenstein (Ji and Eisenstein, 2014) implement a shift-reduce discourse parser on top of given RST-style discourse units to simultaneously learn parsing and a discoursedriven projection of features using support vector machines with gradient-based updates. Li et al. (Li, 2014) produce a distributed representation of RST-style discourse units using recursive convolution on sentence parse trees and apply a classifier to determine relations between them. Ji and Eisenstein (Ji and Eisenstein, 2014) also improved classification of PDTB-style implicit discourse relations by combining distributed representations of parse trees with cor</context>
</contexts>
<marker>Ji, Eisenstein, 2014</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2014. Representation Learning for Text-level Discourse Parsing. In Proc. 52nd Annu. Meet. Assoc. Comput. Linguist., pages 13–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations.</title>
<date>2015</date>
<journal>Trans. Assoc. Comput. Linguist.</journal>
<contexts>
<context position="9923" citStr="Ji and Eisenstein, 2015" startWordPosition="1477" endWordPosition="1481">ative reranking of discourse trees using tree kernels (Joty and Moschitti, 2014). It has been shown that constituent- and dependency-based syntax and features based on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with coreferent entity mentions (Ji and Eisenstein, 2015). Extracting discourse arguments has been attempted by using classic linear word tagging with conditional random fields and global features (Ghosh et al., 2012), identifying nodes in constituent subtrees (Lin et al., 2014), and hybrid merging and pruning of parse trees with integer linear programming (Kong et al., 2014). Deep learning architectures consist of multiple layers of simple learning blocks stacked on each other and, when well trained, tend to do a better job at disentangling the underlying factors of variation. Beginning with raw data, its representation is transformed into increasi</context>
</contexts>
<marker>Ji, Eisenstein, 2015</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2015. One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations. Trans. Assoc. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Discriminative Reranking of Discourse Parses Using Tree Kernels.</title>
<date>2014</date>
<booktitle>In Proc. 2014 Conf. Empir. Methods Nat. Lang. Process.,</booktitle>
<pages>2049--2060</pages>
<contexts>
<context position="9379" citStr="Joty and Moschitti, 2014" startWordPosition="1398" endWordPosition="1401">]e4 [when the preliminary report on U.S. gross national product is released.]e5 • [But movements in the Dow Jones Industrial Average yesterday put Wall Street back in the spotlight]e6 [and inspired participants to bid the U.S. unit lower.]e7 Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relations between them in an RST discourse tree representation (Feng et al., 2014). up fashion while distinguishing within-sentence and cross-sentence relations (Joty et al., 2013) and improved with discriminative reranking of discourse trees using tree kernels (Joty and Moschitti, 2014). It has been shown that constituent- and dependency-based syntax and features based on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with coreferent entity mentions (Ji and Eisenstein, 2015). Extracting discourse arguments has been attempted by u</context>
</contexts>
<marker>Joty, Moschitti, 2014</marker>
<rawString>Shafiq Joty and Alessandro Moschitti. 2014. Discriminative Reranking of Discourse Parses Using Tree Kernels. In Proc. 2014 Conf. Empir. Methods Nat. Lang. Process., pages 2049–2060.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>A novel discriminative framework for sentence-level discourse analysis.</title>
<date>2012</date>
<journal>Jt. Conf. Empir. Methods Nat. Lang. Process. Comput. Nat. Lang. Learn.,</journal>
<booktitle>In Proc.</booktitle>
<pages>904--915</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4686" citStr="Joty et al., 2012" startWordPosition="691" endWordPosition="694">of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been proposed to capture different rhetorical characteristics, and serve different applications. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is currently the largest discourseannotated corpus, consisting</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng. 2012. A novel discriminative framework for sentence-level discourse analysis. In Proc. 2012 Jt. Conf. Empir. Methods Nat. Lang. Process. Comput. Nat. Lang. Learn., pages 904–915. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis.</title>
<date>2013</date>
<booktitle>In Proc. 51st Annu. Meet. Assoc. Comput. Linguist.,</booktitle>
<pages>486--496</pages>
<contexts>
<context position="4533" citStr="Joty et al., 2013" startWordPosition="669" endWordPosition="672">ntence compression, sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been proposed to capture different rhetorical characteristics, </context>
<context position="9271" citStr="Joty et al., 2013" startWordPosition="1382" endWordPosition="1385">d faded in recent sessions,]e3 [and traders let the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S. gross national product is released.]e5 • [But movements in the Dow Jones Industrial Average yesterday put Wall Street back in the spotlight]e6 [and inspired participants to bid the U.S. unit lower.]e7 Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relations between them in an RST discourse tree representation (Feng et al., 2014). up fashion while distinguishing within-sentence and cross-sentence relations (Joty et al., 2013) and improved with discriminative reranking of discourse trees using tree kernels (Joty and Moschitti, 2014). It has been shown that constituent- and dependency-based syntax and features based on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with </context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and Yashar Mehdad. 2013. Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis. In Proc. 51st Annu. Meet. Assoc. Comput. Linguist., pages 486–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fang Kong</author>
<author>Hwee Tou</author>
<author>Ng Guodong</author>
</authors>
<title>A Constituent-Based Approach to Argument Labeling with Joint Inference</title>
<date>2014</date>
<booktitle>in Discourse Parsing. In Conf. Empir. Methods Nat. Lang. Process.,</booktitle>
<pages>68--77</pages>
<contexts>
<context position="5940" citStr="Kong et al., 2014" startWordPosition="873" endWordPosition="876">ournal. It strives to maintain a theory-neutral approach by adopting the predicateargument view and independence of discourse relations. In it either explicitly or implicitly given discourse connectives, such as coordinating conjunction (e.g. &amp;quot;and&amp;quot;, &amp;quot;but&amp;quot;), subordinating conjunction (e.g. &amp;quot;if&amp;quot;, &amp;quot;because&amp;quot;), or discourse adverbial (e.g. &amp;quot;however&amp;quot;, &amp;quot;also&amp;quot;), combine pairs of discourse arguments into relations. For PDTBstyle discourse parsing, extracting argument spans seems to be the most difficult subtask (Lin et al., 2014), resulting in the best overall performance of only 34.80% in F1-measure (Kong et al., 2014). The RST Discourse Treebank (RST-DT) (Carlson et al., 2003) follows the theoretical framework of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). It contains 385 annotated documents from the Wall Street Journal with 18 high-level categories and 110 fine-grained relations. Any coherent text can be represented as a RST discourse tree structure (like in Figure 1) whose leaves are minimal non-overlapping text spans called elementary discourse units. Adjacent nodes are joined depending on their discourse relations to form a tree. In a mono-nuclear discourse relation one of the text spa</context>
<context position="10244" citStr="Kong et al., 2014" startWordPosition="1527" endWordPosition="1530">entify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with coreferent entity mentions (Ji and Eisenstein, 2015). Extracting discourse arguments has been attempted by using classic linear word tagging with conditional random fields and global features (Ghosh et al., 2012), identifying nodes in constituent subtrees (Lin et al., 2014), and hybrid merging and pruning of parse trees with integer linear programming (Kong et al., 2014). Deep learning architectures consist of multiple layers of simple learning blocks stacked on each other and, when well trained, tend to do a better job at disentangling the underlying factors of variation. Beginning with raw data, its representation is transformed into increasingly higher and more abstract forms in each layer, until the final low-dimensional features or representation useful for a given task is reached. Their success is possible with breakthroughs and improvements in training techniques (like AdaGrad or Adam optimization, rectifier function, dropout regularization) and with i</context>
</contexts>
<marker>Kong, Tou, Guodong, 2014</marker>
<rawString>Fang Kong, Hwee Tou, and Ng Guodong. 2014. A Constituent-Based Approach to Argument Labeling with Joint Inference in Discourse Parsing. In Conf. Empir. Methods Nat. Lang. Process., pages 68–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Dan Jurafsky</author>
<author>Eduard Hovy</author>
</authors>
<title>When Are Tree Structures Necessary for Deep Learning of Representations?</title>
<date>2015</date>
<publisher>Arxiv.</publisher>
<contexts>
<context position="11879" citStr="Li et al., 2015" startWordPosition="1774" endWordPosition="1777">riate for processing text documents, because of their variable length and natural representation as a sequence of words. One approach to solve this is to specify a transition-based processing mechanism (Chen and Manning, 2014; Ji and Eisenstein, 2014) and train a neural network classifier to make parsing decisions. Recurrent neural networks (RNNs) (Elman, 1990) or their generalization, recursive neural networks (Goller and Küchler, 1996), represent a more direct approach by recursively applying the same set of weights over the sequence (temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bi</context>
</contexts>
<marker>Li, Jurafsky, Hovy, 2015</marker>
<rawString>Jiwei Li, Dan Jurafsky, and Eduard Hovy. 2015. When Are Tree Structures Necessary for Deep Learning of Representations? Arxiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyi Jessy Li</author>
</authors>
<title>Reducing Sparsity Improves the Recognition of Implicit Discourse Relations.</title>
<date>2014</date>
<booktitle>In Proc. SIGDIAL 2014 Conf.,</booktitle>
<pages>199--207</pages>
<location>number</location>
<contexts>
<context position="13544" citStr="Li, 2014" startWordPosition="2027" endWordPosition="2028">n different time scales, might better model the discourse relations between input vectors and (hopefully) capture their communicative functions and semantic meaning. A few initial attempts of applying representation learning to our task have already shown substantial performance improvements over previous state-of-the-art. Ji and Eisenstein (Ji and Eisenstein, 2014) implement a shift-reduce discourse parser on top of given RST-style discourse units to simultaneously learn parsing and a discoursedriven projection of features using support vector machines with gradient-based updates. Li et al. (Li, 2014) produce a distributed representation of RST-style discourse units using recursive convolution on sentence parse trees and apply a classifier to determine relations between them. Ji and Eisenstein (Ji and Eisenstein, 2014) also improved classification of PDTB-style implicit discourse relations by combining distributed representations of parse trees with coreferent entity mentions. 4 Contribution to science Because text-level discourse parsing is an important, yet still challenging NLP task, it is the focus of our doctoral dissertation. Method for text-level discourse parsing. Instead of depend</context>
</contexts>
<marker>Li, 2014</marker>
<rawString>Junyi Jessy Li. 2014. Reducing Sparsity Improves the Recognition of Implicit Discourse Relations. In Proc. SIGDIAL 2014 Conf., number June, pages 199–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing Implicit Discourse Relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>In Proc. 2009 Conf. Empir. Methods Nat. Lang. Process.,</booktitle>
<pages>343--351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8419" citStr="Lin et al., 2009" startWordPosition="1254" endWordPosition="1257">mantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle and Prendinger, 2009). Improved results (Feng and Hirst, 2012) have been achieved by incorporating rich linguistic features (Hernault et al., 2010), including lexical semantics, and specific discourse production rules (Lin et al., 2009). An alternative approach is based on jointly performing detection and classification in a bottom17 • [The dollar finished lower yesterday,]e1 [after another session on Wall Street.]e2 • [Concern about the volatile U.S. stockmarket had faded in recent sessions,]e3 [and traders let the dollar languish in a narrow range until tomorrow,]e4 [when the preliminary report on U.S. gross national product is released.]e5 • [But movements in the Dow Jones Industrial Average yesterday put Wall Street back in the spotlight]e6 [and inspired participants to bid the U.S. unit lower.]e7 Figure 1: An example of</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-yen Kan, and Hwee Tou Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. In Proc. 2009 Conf. Empir. Methods Nat. Lang. Process., pages 343–351. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A PDTB-Styled End-to-End Discourse Parser.</title>
<date>2014</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="4481" citStr="Lin et al., 2014" startWordPosition="662" endWordPosition="665">(NLP) applications, including text summarization, sentence compression, sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been propos</context>
<context position="5848" citStr="Lin et al., 2014" startWordPosition="858" endWordPosition="861">ently the largest discourseannotated corpus, consisting of 2159 articles from Wall Street Journal. It strives to maintain a theory-neutral approach by adopting the predicateargument view and independence of discourse relations. In it either explicitly or implicitly given discourse connectives, such as coordinating conjunction (e.g. &amp;quot;and&amp;quot;, &amp;quot;but&amp;quot;), subordinating conjunction (e.g. &amp;quot;if&amp;quot;, &amp;quot;because&amp;quot;), or discourse adverbial (e.g. &amp;quot;however&amp;quot;, &amp;quot;also&amp;quot;), combine pairs of discourse arguments into relations. For PDTBstyle discourse parsing, extracting argument spans seems to be the most difficult subtask (Lin et al., 2014), resulting in the best overall performance of only 34.80% in F1-measure (Kong et al., 2014). The RST Discourse Treebank (RST-DT) (Carlson et al., 2003) follows the theoretical framework of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). It contains 385 annotated documents from the Wall Street Journal with 18 high-level categories and 110 fine-grained relations. Any coherent text can be represented as a RST discourse tree structure (like in Figure 1) whose leaves are minimal non-overlapping text spans called elementary discourse units. Adjacent nodes are joined depending on their </context>
<context position="9597" citStr="Lin et al., 2014" startWordPosition="1430" endWordPosition="1433"> unit lower.]e7 Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relations between them in an RST discourse tree representation (Feng et al., 2014). up fashion while distinguishing within-sentence and cross-sentence relations (Joty et al., 2013) and improved with discriminative reranking of discourse trees using tree kernels (Joty and Moschitti, 2014). It has been shown that constituent- and dependency-based syntax and features based on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with coreferent entity mentions (Ji and Eisenstein, 2015). Extracting discourse arguments has been attempted by using classic linear word tagging with conditional random fields and global features (Ghosh et al., 2012), identifying nodes in constituent subtrees (Lin et al., 2014), and hybrid merging and pruning of parse trees with</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A PDTB-Styled End-to-End Discourse Parser. Nat. Lang. Eng., 20(2):151–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<journal>Text-Interdisciplinary J. Study Discourse,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="3518" citStr="Mann and Thompson, 1988" startWordPosition="517" endWordPosition="520">itask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, function, or knowledge (clauses, sentences, or even paragraphs) is called a discourse. They are often understood only in relation to other discourse units (at any level of grouping) and their combination creates a joint meaning larger than individual unit’s meaning alone (Mann and Thompson, 1988). Discourse parsing is the task of determining how these units are related to each other (like in Figure 1) and plays a central role in a num16 Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 16–21, Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics ber of high-impact natural language processing (NLP) applications, including text summarization, sentence compression, sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated </context>
<context position="6097" citStr="Mann and Thompson, 1988" startWordPosition="897" endWordPosition="900">xplicitly or implicitly given discourse connectives, such as coordinating conjunction (e.g. &amp;quot;and&amp;quot;, &amp;quot;but&amp;quot;), subordinating conjunction (e.g. &amp;quot;if&amp;quot;, &amp;quot;because&amp;quot;), or discourse adverbial (e.g. &amp;quot;however&amp;quot;, &amp;quot;also&amp;quot;), combine pairs of discourse arguments into relations. For PDTBstyle discourse parsing, extracting argument spans seems to be the most difficult subtask (Lin et al., 2014), resulting in the best overall performance of only 34.80% in F1-measure (Kong et al., 2014). The RST Discourse Treebank (RST-DT) (Carlson et al., 2003) follows the theoretical framework of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). It contains 385 annotated documents from the Wall Street Journal with 18 high-level categories and 110 fine-grained relations. Any coherent text can be represented as a RST discourse tree structure (like in Figure 1) whose leaves are minimal non-overlapping text spans called elementary discourse units. Adjacent nodes are joined depending on their discourse relations to form a tree. In a mono-nuclear discourse relation one of the text spans is the nucleus, which is more salient than the satellite, while in a multi-nuclear relation all text spans are equally important for interpretation. Perfo</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text-Interdisciplinary J. Study Discourse, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach.</title>
<date>2000</date>
<journal>Comput. Linguist.,</journal>
<volume>26</volume>
<issue>38</issue>
<contexts>
<context position="7389" citStr="Marcu, 2000" startWordPosition="1104" endWordPosition="1105">te spans of text that serve as arguments (best 85.7% in F1-measure (Feng and Hirst, 2012)), identify which of the arguments is the nucleus (best 71.1% in F1-measure (Ji and Eisenstein, 2014)), and tag the sense and location of discourse relations (best 61.6% in F1-measure (Ji and Eisenstein, 2014)). 3 Related work Early work on linguistic and computational discourse analysis produced several theoretical frameworks and one of the most influential is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). In order to automatically build a hierarchical structure of a text, first approaches (Marcu, 2000) relied mainly on discourse markers, hand-engineered rules, and heuristics. Learningbased approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence text-level relations (Baldridge and Lascarides, 2005). They largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier fo</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Rhetorical Parsing of Unrestricted Texts: A Surface-based Approach. Comput. Linguist., 26(38):395–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<date>2008</date>
<booktitle>The Penn Discourse TreeBank 2.0. Proc. Sixth Int. Conf. Lang. Resour. Eval.,</booktitle>
<pages>2961--2968</pages>
<contexts>
<context position="4246" citStr="Prasad et al., 2008" startWordPosition="622" endWordPosition="625">and plays a central role in a num16 Proceedings of the ACL-IJCNLP 2015 Student Research Workshop, pages 16–21, Beijing, China, July 28, 2015. c�2015 Association for Computational Linguistics ber of high-impact natural language processing (NLP) applications, including text summarization, sentence compression, sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Trad</context>
<context position="7878" citStr="Prasad et al., 2008" startWordPosition="1171" endWordPosition="1174">RST) (Mann and Thompson, 1988). In order to automatically build a hierarchical structure of a text, first approaches (Marcu, 2000) relied mainly on discourse markers, hand-engineered rules, and heuristics. Learningbased approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence text-level relations (Baldridge and Lascarides, 2005). They largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle and Prendinger, 2009). Improved results (Feng and Hirst, 2012) have been achieved by incorporating rich linguistic features (Hernault et al., 2010), including lexical semantics, and specific discourse production rules (Lin et al., 2009). An alternative approach is based on jointly performing de</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. Proc. Sixth Int. Conf. Lang. Resour. Eval., pages 2961–2968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Exploiting Scope for Shallow Discourse Parsing.</title>
<date>2010</date>
<booktitle>In Int. Conf. Lang. Resour. Eval.,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4555" citStr="Prasad et al., 2010" startWordPosition="673" endWordPosition="676"> sentiment analysis, and question-answering. For analyzing different perspectives of discourse analysis researchers proposed a number of theoretical frameworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been proposed to capture different rhetorical characteristics, and serve different ap</context>
</contexts>
<marker>Prasad, Joshi, Webber, 2010</marker>
<rawString>Rashmi Prasad, Aravind Joshi, and Bonnie Webber. 2010. Exploiting Scope for Shallow Discourse Parsing. In Int. Conf. Lang. Resour. Eval., pages 2076– 2083.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>IEEE Trans. Signal Process.,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="12615" citStr="Schuster and Paliwal, 1997" startWordPosition="1889" endWordPosition="1893">ees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better model the discourse relations between input vectors and (hopefully) capture their communicative functions and semantic meaning. A few initial attempts of applying representation learning to our task have already shown substantial performance </context>
</contexts>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Trans. Signal Process., 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<journal>Jt. Conf. Empir. Methods Nat. Lang. Process. Comput. Nat. Lang. Learn.,</journal>
<booktitle>In Proc.</booktitle>
<pages>1201--1211</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2739" citStr="Socher et al., 2012" startWordPosition="396" endWordPosition="399">entations automatically would have a lot of potential to improve state-of-the-art performance on more challenging NLP tasks, such as text-level discourse parsing. This may even be more important for languages where progress in NLP is still lacking. Variants of deep learning architectures have been shown to provide a different approach to learning in which latent features are automatically learned as distributed dense vectors. They managed to represent meaningful relations with word (Collobert, 2011), POS and dependency tag (Chen and Manning, 2014), sentence (Guo and Diab, 2012), and document (Socher et al., 2012) embeddings and achieved surprising results for a number of NLP tasks. It has been shown that both unsupervised pre-training (Hinton et al., 2006) and multitask learning (Collobert and Weston, 2008) significantly improve their performance in the absence of hand-engineered features. This makes them especially interesting for the problem of text-level discourse parsing. 2 Text-level discourse parsing In natural language, a piece of text meant to communicate specific information, function, or knowledge (clauses, sentences, or even paragraphs) is called a discourse. They are often understood only </context>
<context position="12783" citStr="Socher et al., 2012" startWordPosition="1913" endWordPosition="1916">e layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better model the discourse relations between input vectors and (hopefully) capture their communicative functions and semantic meaning. A few initial attempts of applying representation learning to our task have already shown substantial performance improvements over previous state-of-the-art. Ji and Eisenstein (Ji and Eisenstein, 2014) implement a shift-reduce discourse parser on top of given RST-style discourse u</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In Proc. 2012 Jt. Conf. Empir. Methods Nat. Lang. Process. Comput. Nat. Lang. Learn., pages 1201– 1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proc. Conf. Empir. Methods Nat. Lang. Process.,</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="12841" citStr="Socher et al., 2013" startWordPosition="1923" endWordPosition="1926">chrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better model the discourse relations between input vectors and (hopefully) capture their communicative functions and semantic meaning. A few initial attempts of applying representation learning to our task have already shown substantial performance improvements over previous state-of-the-art. Ji and Eisenstein (Ji and Eisenstein, 2014) implement a shift-reduce discourse parser on top of given RST-style discourse units to simultaneously learn parsing and a discoursedriven</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proc. Conf. Empir. Methods Nat. Lang. Process., pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>Proc. 2003 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol.,</booktitle>
<pages>1--228</pages>
<contexts>
<context position="4712" citStr="Soricut and Marcu, 2003" startWordPosition="695" endWordPosition="698">eworks and released annotated corpora, such as RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse Treebank (PDTB) (Prasad et al., 2008). Both of these decompose discourse parsing into a few subtasks and, like in most of NLP, their success depends on expert knowledge of each subtask and hand-engineering of more powerful features (Feng and Hirst, 2012; Lin et al., 2014), representations, and heuristics (Joty et al., 2013; Prasad et al., 2010). Despite recent progress in automatic discourse segmentation and sentence-level parsing (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), text-level discourse parsing remains a significant challenge (Feng and Hirst, 2012; Ji and Eisenstein, 2014; Lin et al., 2014). Traditional hand-engineering approaches unfortunately seem to be insufficient, as discourses and relations between them do not follow any strict grammar or obvious rules. Two main theoretical frameworks with English corpus have been proposed to capture different rhetorical characteristics, and serve different applications. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is currently the largest discourseannotated corpus, consisting of 2159 articles from Wal</context>
<context position="7582" citStr="Soricut and Marcu, 2003" startWordPosition="1126" endWordPosition="1129">in, 2014)), and tag the sense and location of discourse relations (best 61.6% in F1-measure (Ji and Eisenstein, 2014)). 3 Related work Early work on linguistic and computational discourse analysis produced several theoretical frameworks and one of the most influential is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). In order to automatically build a hierarchical structure of a text, first approaches (Marcu, 2000) relied mainly on discourse markers, hand-engineered rules, and heuristics. Learningbased approaches were first applied to identify within-sentence discourse relations (Soricut and Marcu, 2003), and only later to cross-sentence text-level relations (Baldridge and Lascarides, 2005). They largely focused on lexical, syntactic, and structural features, but the close relationship between discourse structure and semantic meaning suggests that this may not be sufficient (Prasad et al., 2008; Subba and Di Eugenio, 2009). Further work on discourse parsing focused first on having a binary classifier for determining whether two adjacent discourse units should be merged, followed by a multi-class classifier for determining which discourse relation should be assigned to the new subtree (DuVerle</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. Proc. 2003 Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang. Technol., 1:228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>An effective discourse parser that uses rich linguistic information.</title>
<date>2009</date>
<booktitle>In Proc. Hum. Lang. Technol. 2009 Annu. Conf. North Am. Chapter Assoc. Comput. Linguist.,</booktitle>
<pages>566--574</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Subba, Di Eugenio, 2009</marker>
<rawString>Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In Proc. Hum. Lang. Technol. 2009 Annu. Conf. North Am. Chapter Assoc. Comput. Linguist., pages 566–574. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Thomas Hicks</author>
<author>Marco A Valenzuela-Escarcega</author>
</authors>
<title>Two Practical Rhetorical Structure Theory Parsers.</title>
<date>2015</date>
<booktitle>In Proc. North Am. Chapter Assoc. Comput. Linguist.</booktitle>
<contexts>
<context position="9528" citStr="Surdeanu et al., 2015" startWordPosition="1419" endWordPosition="1422">Street back in the spotlight]e6 [and inspired participants to bid the U.S. unit lower.]e7 Figure 1: An example of seven elementary discourse units (e1-e7), and (mono- or multi-nuclear) relations between them in an RST discourse tree representation (Feng et al., 2014). up fashion while distinguishing within-sentence and cross-sentence relations (Joty et al., 2013) and improved with discriminative reranking of discourse trees using tree kernels (Joty and Moschitti, 2014). It has been shown that constituent- and dependency-based syntax and features based on coreference links improve performance (Surdeanu et al., 2015). The first PDTB-style end-to-end discourse parser (Lin et al., 2014) uses a connective list to identify explicit candidates, followed by simple features and parse trees to extract arguments and identify discourse relations. Classifying implicit discourse relations can be improved by combining distributed representations of parse trees with coreferent entity mentions (Ji and Eisenstein, 2015). Extracting discourse arguments has been attempted by using classic linear word tagging with conditional random fields and global features (Ghosh et al., 2012), identifying nodes in constituent subtrees (</context>
</contexts>
<marker>Surdeanu, Hicks, Valenzuela-Escarcega, 2015</marker>
<rawString>Mihai Surdeanu, Thomas Hicks, and Marco A. Valenzuela-Escarcega. 2015. Two Practical Rhetorical Structure Theory Parsers. In Proc. North Am. Chapter Assoc. Comput. Linguist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Waibel</author>
<author>Toshiyuki Hanazawa</author>
<author>Geoffrey E Hinton</author>
<author>Kiyohiro Shikano</author>
<author>Kevin J Lang</author>
</authors>
<title>Phoneme recognition using time-delay neural networks.</title>
<date>1989</date>
<journal>IEEE Trans. Acoust.,</journal>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="12426" citStr="Waibel et al., 1989" startWordPosition="1861" endWordPosition="1864">temporal dimension) or structure (treebased). Li et al. (Li et al., 2015) have recently 18 showed that only some NLP tasks benefit from recursive models applied on syntactic parse trees and recurrent models seem to be sufficient for discourse parsing. By stacking multiple hidden layers into a deep RNN makes them represent a temporal hierarchy with multiple layers operating at different time scales (Hermans and Schrauwen, 2013). Learning to store information over extended time intervals has been achieved with long short-term memory (Hochreiter and Schmidhuber, 1997), time delay neural network (Waibel et al., 1989), or neural Turing machines (Graves et al., 2014). Bidirectional variants of these models can incorporate information from preceding as well as following tokens (Schuster and Paliwal, 1997). Recursive neural networks have also been shown to support different task-specific representations, such as matrix-vector representation of words (Socher et al., 2012) or recurrent neural tensor networks (Socher et al., 2013). For our discourse parsing task such deeper models, that can learn abstract representations on different time scales, might better model the discourse relations between input vectors a</context>
</contexts>
<marker>Waibel, Hanazawa, Hinton, Shikano, Lang, 1989</marker>
<rawString>Alexander Waibel, Toshiyuki Hanazawa, Geoffrey E. Hinton, Kiyohiro Shikano, and Kevin J. Lang. 1989. Phoneme recognition using time-delay neural networks. IEEE Trans. Acoust., 37(3):328–339.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>