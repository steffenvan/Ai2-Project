<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.057749">
<subsectionHeader confidence="0.318947">
Syntactic Processing
Martin Kay
Xerox Palo Alto Research Center
</subsectionHeader>
<bodyText confidence="0.999787821621623">
In computational linguistics, which began in the
1950&apos;s with machine translation, systems that are
based mainly on the lexicon have a longer tradition
than anything else---for these purposes, twenty five
years must be allowed to count as a tradition. The
bulk of many of the early translation systems was made
up by a dictionary whose entries consisted of
arbitrary instructions in machine language. In the
early 60&apos;3, computational linguists---at least those
with theoretical pretentions---abandoned this way of
doing business for at least three related reasons:
First systems containing large amounts of unrestricted
machine code fly in the face of all principles of good
programming practice. The syntax of the language in
which linguistic facts are stated is so remote from
their semantics that the opportunities for error are
very great and no assumptions can be made about the
effects on the system of invoking the code associated
with any given word. The systems became virtually
unmaintainable and eventually fell under their own
weight. Furthermore, these failings were magnified as
soon as the attempt was made to impose more structure
on the overall system. A general backtracking scheme,
for example, could all too easily be thrown into
complete disarray by an instruction in a single
dictionary entry that affected the oontrol stack.
Second, the power of general, and particularly
nondeterministic, algorithms in syntactic analysis
came to be appreciated, if not overappreciated.
Suddenly, it was no longer necessary to seek local
criteria on which to ensure the correctness of
individual decisions made by the program provided they
were covered by more global criteria. Separation of
program and linguistic data became an overriding
principle and, since it was most readily applied to
syntactic rules, these became the main focus of
attention.
The third, and doubtless the most important, reason
for the change was that syntactic theories in which a
grammar was seen as consisting of a set of rules,
preferably including transformational rules, captured
the imagination of the most influential
noncomputational linguists, and computational
linguists followed suite if only to maintain
theoretical respectability. In short, systems with
small sets of rules in a constrained formalism and
simple lexical entries apparently made for simpler,
cleaner, and more powerful programa while setting the
whole enterprise on a sounder theoretical footing.
The trend is now in the opposite direction. There has
been a shift of emphasis sway from highly structured
systems of complex rules as the principle repository
of information about the syntax of a language towards
a view in which the responsibility is distributed
among the lexicon, semantic parts of the linguistic
description, and a cognitive or strategic component.
Concomitantly, interest has shifted from algorithma
for syntactic analysis and generation, in which the
control structure and the exact sequence of events are
paramount, to systems in which a heavier burden is
carried by the data structure and in which the order
of events is a matter of strategy. This new trend is
a common thread running through several of the papers
in this section.
Various techniques for syntactic analysis, notably
those based on some form of Augmented Transition
Network (ATN), represent grammatical facts in terms of
executable machine code. The dangers to which thin
exposed the earlier systems are avoided by insisting
that this code by compiled from statements in
formalism that allows only for linguistically
motivated operations on carefully controlled parts of
certain data structures.
The value of nondeterministic procedures is
undiminished, but it has become clear that it does not
rest on complex control structures and a rigidly
determined sequence of events. In discussing the
syntactic processors that we have developed, for
example, Ron Kaplan and I no longer find it useful to
talk in terms of e parsing a:gorithm. There are two
central data structures, a chart and an agenda. When
additions to the chart give rise to certain kinds of
configurations in which some element contains
executable code, a task is created and placed on the
agenda. Tasks are removed from the agenda and
executed in an order determined by strategic
considerations which constitute part of the linguistic
theory. Strategy can determine only the order in
which alternative analyses are produced. Many
traditional distinctions, such as that between top-
down and bottom-up processing, no longer apply to the
procedure as a whole but only to particular strategies
or their parts.
This looser organization of programs for syntactic
processing came, at least in part, from a generally
felt need to break down the boundaries that had
traditionally separated morphological, syntactic, and
semantic processes. Research directed towards speech
understanding systems was quite unable to respect
these boundaries because, in the face of unoertair
data, local moves in the analysis on one level
required confirmation from other levels so that a
common data structure for all levels of analysis and a
schedule that could change continually were of the
essence. Futhermore, there was a mouvement from
within the artifioial-intelligence community to
eliminate the boundaries because, from that
perspective, they lacked sufficient theoretical
justification.
In speech research in particular, and artificial
intelligence in general, the lexicon took on an
Important position if only because it is thee that
the units of meaning reside. Recent per .sals in
linguistic theory involve a larger role for the
lexicon. Bresnan (1978) has argued persuasively that
the full mechanism of transformational rules can, and
should, be dispensed with except in cases of unbounded
movement such as relativization and topicalizetton.
The remaining members Of the familiar list of
transformations can be handled by weaker devices in
the lexicon and, Since they all turn out to be
lexically governed, this is the appropriate place to
state the information.
Against this background, the papers that fellnw,
different though they are in many ways, constitute
fairly coherent set. Carbonell domes from the
artificial-intelligence tradition and is generally
concerned With the meanings of words and the ways in
which they are **fleeted to give the meanings of
ouatomr,hoilt pieces of diseouree, :P the preeent
paper, he explore* ways in which this promo** mon be
made to reflect back on itself to fill gap* in the
lextoon by appropriate analysis of the context, At
its 0444, the method La familiar from aimilar work in
syntax. The miacing element La treated as though it
had whatever properties&apos; allow a coherent analyst, of
the larger unitâ€”say a sentence, or paragraph,---in
which it is imbedded. Thee&apos; properties are then
entered against it in the lexicon for future use. The
problem, which is faced in thia paper, is that the
poaaibility that the lexicon La deficient must be
faced in respect of all words becaueel even when there
is en entry in the lexioon, it may not supply the
reeding required in the case on hand. Small, like
Carbonell is concerned with the meaninge of words and
he is lead to a view of words as active agents, The
main role of the syatem is to act 44 moderator.
Kwaany and 3onheimer have a concern to Carbonell&apos;s.
When problems arise in analysis, they look for
deficiencies in the text rather than in the lexicon
and the rules. It is no indictment of either paper
that they provide no way of distinguiahing the cases,
for this is clearly a separate enterprise. Kwasny and
Sonheimer propose progressively weakening the
requirements that their analysis system makes of 4
segment of text so that, if it does not accord with
the beat principals of composition, an analysis can
still be found by taking a less demanding view of it.
Such a technique clearly rests on * regime in which
the scheduling of events is relatively free and the
control structure relatively fret.
Shapiro shows how a strong data structure and a weak
control structure make it possible to extend the ATN
beyond the analysis of one dimensional strings to
semantic, networks. The result is a total spot= with
remarkable consistency in the methods applied at all
levels and, presumably, corresponding simplicity and
clarity in the architecture of the system as a whole.
Allen is one of the foremost contributors to research
on speech understanding, and speech processing in
general. He stresses the need for strongly
Interacting components at different levels of analyais
and, to that extent, argues for the kind of data-
directed methods / have tried to characterize.
At first reading, Eiaenatadt&apos;s paper appears least
willing to lie in my Procrustean bed, for it appears
to be concerned with the finer points of algorithmic
design and, to an extent, this is true. But, the two
approaches to syntactic analysis that are compared
turn out to be, in my terms, algorithmically weak.
The most fundamental issues that are being discussed
therefore turn out to concern what I have called the
strategic component of linguistic theory, that is with
the rules according to which atomics tasks in the
analysis process are scheduled.
</bodyText>
<sectionHeader confidence="0.798756" genericHeader="abstract">
Reference
</sectionHeader>
<bodyText confidence="0.87148275">
Bresnan, Joan (1978) &amp;quot;A Realistic Transformational
Grammar&amp;quot; in Palle. &amp;quot;roman and Piller (eds.)
Linguistic Theory and Psychological Reality, The MIT
Press.
</bodyText>
<page confidence="0.99331">
2
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.361086">
<title confidence="0.999779">Syntactic Processing</title>
<author confidence="0.999894">Martin Kay</author>
<affiliation confidence="0.96637">Xerox Palo Alto Research Center</affiliation>
<abstract confidence="0.999458751351351">In computational linguistics, which began in the 1950&apos;s with machine translation, systems that are based mainly on the lexicon have a longer tradition than anything else---for these purposes, twenty five years must be allowed to count as a tradition. The bulk of many of the early translation systems was made up by a dictionary whose entries consisted of arbitrary instructions in machine language. In the early 60&apos;3, computational linguists---at least those with theoretical pretentions---abandoned this way of doing business for at least three related reasons: First systems containing large amounts of unrestricted machine code fly in the face of all principles of good programming practice. The syntax of the language in which linguistic facts are stated is so remote from their semantics that the opportunities for error are very great and no assumptions can be made about the effects on the system of invoking the code associated with any given word. The systems became virtually unmaintainable and eventually fell under their own weight. Furthermore, these failings were magnified as soon as the attempt was made to impose more structure on the overall system. A general backtracking scheme, for example, could all too easily be thrown into complete disarray by an instruction in a single dictionary entry that affected the oontrol stack. Second, the power of general, and particularly nondeterministic, algorithms in syntactic analysis came to be appreciated, if not overappreciated. Suddenly, it was no longer necessary to seek local criteria on which to ensure the correctness of individual decisions made by the program provided they were covered by more global criteria. Separation of program and linguistic data became an overriding principle and, since it was most readily applied to syntactic rules, these became the main focus of attention. The third, and doubtless the most important, reason for the change was that syntactic theories in which a grammar was seen as consisting of a set of rules, preferably including transformational rules, captured the imagination of the most influential noncomputational linguists, and computational linguists followed suite if only to maintain theoretical respectability. In short, systems with small sets of rules in a constrained formalism and simple lexical entries apparently made for simpler, cleaner, and more powerful programa while setting the whole enterprise on a sounder theoretical footing. trend in the opposite direction. There has been a shift of emphasis sway from highly structured systems of complex rules as the principle repository of information about the syntax of a language towards a view in which the responsibility is distributed lexicon, semantic parts of the linguistic description, and a cognitive or strategic component. Concomitantly, interest has shifted from algorithma for syntactic analysis and generation, in which the control structure and the exact sequence of events are paramount, to systems in which a heavier burden is by the data structure and in which of events is a matter of strategy. This new trend is a common thread running through several of the papers in this section. Various techniques for syntactic analysis, notably those based on some form of Augmented Transition Network (ATN), represent grammatical facts in terms of executable machine code. The dangers to which thin exposed the earlier systems are avoided by insisting that this code by compiled from statements in formalism that allows only for linguistically motivated operations on carefully controlled parts of certain data structures. value of nondeterministic procedures undiminished, but it has become clear that it does not rest on complex control structures and a rigidly sequence of events. In discussing the syntactic processors that we have developed, for example, Ron Kaplan and I no longer find it useful to talk in terms of e parsing a:gorithm. There are two central data structures, a chart and an agenda. When additions to the chart give rise to certain kinds of configurations in which some element contains executable code, a task is created and placed on the agenda. Tasks are removed from the agenda and executed in an order determined by strategic considerations which constitute part of the linguistic theory. Strategy can determine only the order in which alternative analyses are produced. Many traditional distinctions, such as that between topdown and bottom-up processing, no longer apply to the procedure as a whole but only to particular strategies or their parts. This looser organization of programs for syntactic processing came, at least in part, from a generally felt need to break down the boundaries that had traditionally separated morphological, syntactic, and semantic processes. Research directed towards speech understanding systems was quite unable to respect these boundaries because, in the face of unoertair data, local moves in the analysis on one level required confirmation from other levels so that a data structure for all levels and a schedule that could change continually were of the essence. Futhermore, there was a mouvement from within the artifioial-intelligence community to eliminate the boundaries because, from that perspective, they lacked sufficient theoretical justification. In speech research in particular, and artificial intelligence in general, the lexicon took on an Important position if only because it is thee that the units of meaning reside. Recent per .sals in linguistic theory involve a larger role for the lexicon. Bresnan (1978) has argued persuasively that the full mechanism of transformational rules can, and should, be dispensed with except in cases of unbounded movement such as relativization and topicalizetton. The remaining members Of the familiar list of transformations can be handled by weaker devices in the lexicon and, Since they all turn out to be governed, this is the appropriate place state the information. Against this background, the papers that fellnw, different though they are in many ways, constitute coherent set. Carbonell artificial-intelligence tradition and is generally concerned With the meanings of words and the ways in they are **fleeted to give the meanings pieces of :P preeent paper, he explore* ways in which this promo** mon be made to reflect back on itself to fill gap* in the lextoon by appropriate analysis of the context, At its 0444, the method La familiar from aimilar work in syntax. The miacing element La treated as though it had whatever properties&apos; allow a coherent analyst, of larger unitâ€”say a sentence, or which it is imbedded. Thee&apos; properties are then entered against it in the lexicon for future use. The problem, which is faced in thia paper, is that the that the lexicon La must in respect of all words even when there is en entry in the lexioon, it may not supply the reeding required in the case on hand. Small, like with the meaninge of words and he is lead to a view of words as active agents, The role of the syatem is to act Kwaany and 3onheimer have a concern to Carbonell&apos;s. problems arise they look for deficiencies in the text rather than in the lexicon and the rules. It is no indictment of either paper they provide no way of distinguiahing the this is a separate enterprise. Kwasny and Sonheimer propose progressively weakening the that their analysis system makes of segment of text so that, if it does not accord with the beat principals of composition, an analysis can still be found by taking a less demanding view of it. Such a technique clearly rests on * regime in which the scheduling of events is relatively free and the control structure relatively fret. shows how a strong structure and a weak control structure make it possible to extend the ATN beyond the analysis of one dimensional strings to semantic, networks. The result is a total spot= with remarkable consistency in the methods applied at all levels and, presumably, corresponding simplicity and clarity in the architecture of the system as a whole. Allen is one of the foremost contributors to research on speech understanding, and speech processing in general. He stresses the need for strongly Interacting components at different levels of analyais and, to that extent, argues for the kind of datadirected methods / have tried to characterize. At first reading, Eiaenatadt&apos;s paper appears least willing to lie in my Procrustean bed, for it appears to be concerned with the finer points of algorithmic design and, to an extent, this is true. But, the two approaches to syntactic analysis that are compared turn out to be, in my terms, algorithmically weak. The most fundamental issues that are being discussed therefore turn out to concern what I have called the component of linguistic theory, that rules according to atomics tasks in the analysis process are scheduled.</abstract>
<note confidence="0.750736833333333">Reference Joan &amp;quot;A Transformational Grammar&amp;quot; in Palle. &amp;quot;roman and Piller (eds.) Linguistic Theory and Psychological Reality, The MIT Press. 2</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>