<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.152442">
<title confidence="0.9968395">
INESC-ID: Sentiment Analysis without hand-coded Features or Liguistic
Resources using Embedding Subspaces
</title>
<author confidence="0.981868">
Ramon F. Astudillo, Silvio Amir, Wang Ling, Bruno Martins†, M´ario Silva, Isabel Trancoso
</author>
<affiliation confidence="0.961803">
Instituto de Engenharia de Sistemas e Computadores Investigac¸˜ao e Desenvolvimento
</affiliation>
<address confidence="0.913617333333333">
Rua Alves Redol 9
Lisbon, Portugal
{ramon.astudillo, samir, wlin, mjs, isabel.trancoso}@inesc-id.pt
</address>
<email confidence="0.991151">
†bruno.g.martins@tecnico.ulisboa.pt
</email>
<sectionHeader confidence="0.995448" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994465625">
We present the INESC-ID system for the mes-
sage polarity classification task of SemEval
2015. The proposed system does not make
use of any hand-coded features or linguistic
resources. It relies on projecting pre-trained
structured skip-gram word embeddings into a
small subspace. The word embeddings can be
obtained from large amounts of Twitter data
in unsupervised form. The sentiment analy-
sis supervised training is thus reduced to find-
ing the optimal projection which can be car-
ried out efficiently despite the little data avail-
able. We analyze in detail the proposed ap-
proach and show that a competitive system can
be attained with only a few configuration pa-
rameters.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921225">
Web-based social networks are a rich data source for
both businesses and academia. However, the sheer
volume, diversity and rate of creation of social me-
dia, imposes the need for automated analysis tools.
The growing interest in this problem motivated the
creation of a shared task for Twitter Sentiment Anal-
ysis (Nakov et al., 2013). The Message Polarity
Classification task consists in classifying a message
as positive, negative, or neutral in sentiment.
A great deal of research has been done on meth-
ods for sentiment analysis on user generated con-
tent. However, state-of-the-art systems still largely
depend on linguistic resources, extensive feature en-
gineering and tuning. Indeed, if we look at the best
performing systems from SemEval 2014 (Zhu et al.,
2014), (Malandrakis et al., 2014), both make exten-
sive use of these resources, including hundreds of
thousands of features, special treatment for nega-
tion, multi-word expressions or special strings like
emoticons.
In this paper we present the INESC-ID system
for the 2015 SemEval message polarity classifica-
tion task (Rosenthal et al., 2015). The system is able
to learn good message representations for message
polarity classification directly from raw text with a
simple tokenization scheme. Our approach is based
on using large amounts of unlabeled data to induce
word embeddings, that is, continuous word represen-
tations containing contextual information. Instead
of using these word embeddings directly with, for
instance, a logistic regression classifier, we estimate
a sentiment subspace of the embeddings. The idea
is to find a projection of the embedding space that is
meaningful for the supervised task. In the proposed
model, we jointly learn the sentiment subspace pro-
jection and the classifier using the SemEval train-
ing data. The resulting system attains state-of-the-
art performance without hand-coded features or lin-
guistic resources and only a few configuration pa-
rameters.
</bodyText>
<sectionHeader confidence="0.9681155" genericHeader="introduction">
2 Unsupervised Learning of Word
Embeddings
</sectionHeader>
<bodyText confidence="0.999208833333333">
Unsupervised word embeddings trained from large
amounts of unlabeled data have been shown to im-
prove many NLP tasks (Turian et al., 2010; Col-
lobert et al., 2011). Embeddings capture generic
regularities about the data and can be trained with
virtually an infinite amount of data in unsupervised
</bodyText>
<page confidence="0.981341">
652
</page>
<bodyText confidence="0.95281368">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
fashion. Once trained, they can be used as features
for supervised tasks or to initialize more complex
models (Collobert et al., 2011; Chen and Manning,
2014; Bansal et al., 2014). Other unsupervised ap-
proaches that can also be used for feature extraction
include brown clustering (Brown et al., 1992) and
LDA (Blei et al., 2003),
One popular objective function for embeddings is
to maximize the prediction of contextual words. In
the work described in (Mikolov et al., 2013), com-
monly referred as word2vec, the models defined es-
timate the optimal word embeddings by maximiz-
ing the probability that the words within a given
window size are predicted correctly. In the work
presented here, a structured skip-gram (Ling et al.,
2015) was used to generate the embeddings. Cen-
tral to the skip-gram (Mikolov et al., 2013) is a log
linear model of word prediction. Let w = i denote
that a word at a given position of a sentence is the
i-th word on a vocabulary of size v. Let wp = j de-
note that the word p positions further in the sentence
is the j-th word on the vocabulary. The skip-gram
models the following probability:
</bodyText>
<equation confidence="0.930535">
p(wp = j|w = i; C, E) oc exp (Cj · E · wz) . (1)
</equation>
<bodyText confidence="0.99984656">
Here, wz E 11, 01v×1 is a one-hot representation
of w = i. That is, a vector of zeros of the size of
the vocabulary v with a 1 on the i-th entry of the
vector. The symbol · denotes internal product and
exp() acts element-wise. The log-linear model is
parametrized by two matrices. E E Re×v is the
embedding matrix, transforming the one-hot sparse
representation into a compact real valued embedding
vector of size e x 1. The matrix C E Rv×e maps
the embedding to a vector with the size of the vo-
cabulary v. In the particular case of the structured
skip-gram, here used, a different prediction matrix
is trained for each relative position between words
Cp. After exponentiating and normalizing over the
v possible options, the j-th element of the resulting
vector corresponds thus to the probability of wp = j.
In practice, due to the large value of v, various
techniques are used to avoid having to normalize
over the whole vocabulary.
After the embeddings are trained, the low dimen-
sional embedding of each word E · wz E Re×1 en-
capsulates the information about each word and its
surrounding contexts. This embedding can thus be
used as input to other learning algorithms to further
enhance performance.
</bodyText>
<sectionHeader confidence="0.95492" genericHeader="method">
3 Using Embeddings for Sentiment
Prediction
</sectionHeader>
<subsectionHeader confidence="0.999985">
3.1 Sentiment Embedding Subspace
</subsectionHeader>
<bodyText confidence="0.99998948">
There are multiple ways in which embeddings could
be incorporated as a pre-training step into a super-
vised task. The initial attempts for the proposed sys-
tem included log-linear classifiers using the embed-
dings as initialization values or features, but these
led to poor results. Ideally, embeddings should be
adapted to the supervised task. However, this faces
an additional difficulty: only a small subset of the
words will actually be present in the training set of
the supervised task. Words not present in the su-
pervised training set will never get their embeddings
updated.
To avoid this, here we employ a simple projec-
tion scheme. We consider the adapted embeddings
S · E, where E E Re×v is the original unadapted
embedding matrix and S E Rs×e, with s « e, is
a projection matrix trained on the supervised data.
The idea is that, by only training S on the super-
vised data, we determine a sub-space of the embed-
dings which is optimal for the supervised task. An
additional advantage is that, unlike with a direct re-
estimation of E, all embeddings are updated based
on the supervised task data. This simple approach
proved very useful and it accounts for most of the
performance attained in our system.
</bodyText>
<subsectionHeader confidence="0.990967">
3.2 Non-linear Sub-space Model
</subsectionHeader>
<bodyText confidence="0.999851384615385">
Based on the sub-space concept, various log-linear
and non-linear models were explored. Most of the
models attempted were prone to get trapped in poor
local minima or showed stability problems during
training. The only exception identified is the non-
linear model here presented, which showed both fast
convergence and high performance.
In what follows, we will denote a message, e.g.
a tweet, of n words as a matrix m E 10,11v×n,
where each column is a one-hot representation of
each word. The vocabulary v is equal to that of the
unsupervised pre-training. Words of the SemEval
task not appearing in that vocabulary are represented
</bodyText>
<page confidence="0.9973">
653
</page>
<bodyText confidence="0.999946428571429">
as a vector of zeros, equivalent to an embedding of
e zeros. In the SemEval task, each message has to be
classified as neutral, negative or positive. Let y de-
note a categorical random variable over those three
classes. The sub-space non-linear model estimates
thus the probability of each possible category y = k
given a message m as
</bodyText>
<equation confidence="0.991406">
p(y = k|m; C,S) a
exp(Ck · Q (S · E · m) · B), (2)
</equation>
<bodyText confidence="0.999772142857143">
where Q() is a sigmoid function acting on each el-
ement of the matrix. The matrix C E R3×s maps the
embedding sub-space to the classification space and
B E 1n×1 is a matrix of ones that sums the scores
for all words up prior to normalization. This sim-
plification, equivalent to a bag of words assumption,
outperformed other approaches like convolution.
The model is thus equivalent to a multi-layer per-
ceptron (MLP) (Rumelhart et al., 1985) with one
hidden sigmoid layer and a soft-max output layer.
The input to the MLP would be the fixed word em-
beddings attained by applying E. The input layer
S learns a projection of E into a small sub-space of
size s « e.
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="method">
4 Proposed System
</sectionHeader>
<subsectionHeader confidence="0.997294">
4.1 Unsupervised Word Embeddings Learning
</subsectionHeader>
<bodyText confidence="0.999982947368421">
The embedding matrix E was trained in unsuper-
vised fashion using the structured skip-gram model,
described in Section 2.
We used the corpus of 52 million tweets used
in (Owoputi et al., 2013) with the tokenizer de-
scribed in the same work. The words that occurred
less than 40 times in the data were discarded from
the vocabulary. To train the model, we used a neg-
ative sampling rate of 25 words, sampled from a
multinomial of unigram word probabilities over all
the vocabulary (Goldberg and Levy, 2014). Em-
beddings of 50, 200, 400 and 600 dimensions were
trained.
It should be noted that the training configuration
is generic and was not adapted to the SemEval task.
One consequence of this is a relatively strong prun-
ing of the vocabulary. Around 23% of words in the
SemEval tasks did not have an embedding and thus
were set to have an embedding of e zeros.
</bodyText>
<subsectionHeader confidence="0.9900685">
4.2 Supervised Embedding Sub-space
Learning
</subsectionHeader>
<bodyText confidence="0.9999864">
Text normalization for the supervised task employed
the CMU tokenizer plus the following additional
steps: messages were lower-cased, Twitter user
mentions and URLs were replaced with special
tokens and any character repetition above 3 was
mapped to 3 characters.
The small amount of supervised data available
was the main driving factor behind the design and
optimization of the supervised training component.
In order to maintain the number of free parameters
low, small sizes of the subspace were selected with
values ranging from 5 to 30. Training was also kept
as simple as possible. The training set of SemEval
was split into 80% for parameter learning and 20%
for hyper-parameter selection, maintaining the orig-
inal sentiment relative frequencies in each set. The
2013 and 2014 SemEval sentiment analysis test sets
were used to validate the different candidate models.
The most probable class was selected as the model
prediction.
The parameters of subspace model in Equation 2,
S and C were estimated to minimize the nega-
tive log-likelihood of the correct class. Training
employed conventional Stochastic Gradient Descent
(Rumelhart et al., 1985) with mini-batch size 1 and
random uniform initialization similar to (Glorot and
Bengio, 2010). After some initial experiments, it
was determined that a learning rate of 0.01 and
selecting the model with the best accuracy on the
20% set after 8 iterations led to the best results.
</bodyText>
<sectionHeader confidence="0.99665" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.996721">
5.1 Sensibility Analysis
</subsectionHeader>
<bodyText confidence="0.999829333333333">
This section analyzes the performance of the pro-
posed system on the message polarity classification
task of SemEval 2015. In general, the sentiment sub-
space model showed consistent and fast convergence
towards the optimum in very few iterations. Despite
using class log-likelihood as training criterion and
accuracy as stopping criterion, the model showed
good performance in terms of average F-measure for
positive and negative sentiments. This was not al-
ways the case for other tested models.
Regarding the two main parameters, embedding
size e and sub-space size s, sensibility analysis were
</bodyText>
<page confidence="0.998518">
654
</page>
<bodyText confidence="0.997986230769231">
carried out and are shown in Tables 1 and 2. For
these experiments learning rate and stopping condi-
tion were left fixed to the previously indicated val-
ues. Variations of learning rate to smaller values e.g.
0.005 were explored but did not lead to a clear pat-
tern.
Table 1 shows the effect of embedding size on the
system’s performance. Very small embeddings lead
clearly to worse results. Larger embeddings not al-
ways provide the best performance. However, they
provide more consistent results across test sets. It
was also inferred from other tasks that using larger
embeddings had in general a positive effect.
</bodyText>
<table confidence="0.9987344">
Emb Size (e) Dev 2013 2014
50 65.96 68.35 70.54
200 70.65 70.28 72.80
400 70.19 71.54 72.24
600 70.08 72.16 72.72
</table>
<tableCaption confidence="0.928822285714286">
Table 1: Avg. F-measure on SemEval development and
test sets varying with embedding size e. Sub-space size
s = 10. Best model per column in bold.
Table 2 shows the variation of system perfor-
mance with sub-space size. The optimal value was
consistently found to be at s = 10 regardless of em-
bedding size.
</tableCaption>
<table confidence="0.9975388">
Subsp. Size (s) Dev 2013 2014
5 69.78 71.82 72.17
10 70.08 72.16 72.72
20 69.18 71.97 72.52
30 67.81 70.97 72.45
</table>
<tableCaption confidence="0.989772666666667">
Table 2: Avg. F-measure on SemEval test sets varying
with embedding sub-space size s. Embedding size e =
600. Best model per column in bold.
</tableCaption>
<subsectionHeader confidence="0.999873">
5.2 Submitted System and Revised Candidates
</subsectionHeader>
<bodyText confidence="0.999249272727273">
Due to time constraints, not all planned configura-
tions could be tested prior to system submission.
Consequently, some of the experiments shown in
the previous section were carried out after submis-
sion. Based on these results, two new candidates
were selected and then tested on the 2015 dataset.
These were a system that showed a very stable per-
formance using e = 600 and s = 10 and a good sys-
tem with a smaller embedding size using e = 200,
s = 10. The same configuration, learning rate and
number of iterations, as in the submitted model were
used for these experiments.
The results for the submitted system and the a
posteriori selected ones are displayed in Table 3.
The results on 2015, confirm the sensibility analy-
sis of e and s. The high performance of the e = 600,
s = 10 model on the 2015 dataset was however un-
expected, since it tops the submitted system by more
than a 1% absolute. The second model selected, us-
ing a smaller e size displayed a performance compa-
rable to that of the submitted system thus showing
the overall robustness of the approach.
</bodyText>
<table confidence="0.9952465">
e s Dev 2013 2014 2015
600 20 69.18 71.97 72.52 64.12
600 10 70.08 72.16 72.72 65.19
200 10 70.65 70.28 72.80 64.09
</table>
<tableCaption confidence="0.986531666666667">
Table 3: Avg. F-measure of the submitted system (top)
and posteriorly selected candidates (bottom). Best model
per column in bold.
</tableCaption>
<bodyText confidence="0.999920285714286">
It should be noted as well that there is small dif-
ference between the result attained in the submitted
predictions (64.17) and the ones reported here for
the submitted system (64.12). Upon revision of the
code we could determine that this was due to a minor
bug affecting how the embeddings of the E matrix
were constructed.
</bodyText>
<sectionHeader confidence="0.999625" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999969666666667">
We have presented the INESC-ID system for the Se-
mEval 2015 message classification task. The sys-
tem does not make use of any hand-coded features or
linguistic resources and employs a simple tokeniza-
tion scheme. The system is however able to attain
state-of-the-art performance with few configuration
parameters and a small number of iterations. The
results are also consistent across sets and configura-
tion settings.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9981305">
This work has been partially supported by
the FCT through national funds with refer-
ence UID/CEC/50021/2013 and grant number
SFRH/BPD/68428/2010.
</bodyText>
<page confidence="0.998745">
655
</page>
<sectionHeader confidence="0.989181" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997813835443038">
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014.
Tailoring continuous word representations for depen-
dency parsing. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993–1022.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467–479, December.
Danqi Chen and Christopher D Manning. 2014. A
fast and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
The Journal of Machine Learning Research, 12:2493–
2537.
Xavier Glorot and Yoshua Bengio. 2010. Understand-
ing the difficulty of training deep feedforward neural
networks. In In Proceedings of the International Con-
ference on Artificial Intelligence and Statistics, pages
249–256.
Yoav Goldberg and Omer Levy. 2014. word2vec
explained: deriving mikolov et al.’s negative-
sampling word-embedding method. arXiv preprint
arXiv:1402.3722.
Wang Ling, Chris Dyer, Alan Black, and Isabel Tran-
coso. 2015. Two/too simple adaptations of word2vec
for syntax problems. In Proceedings of the 2015 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies.
Nikolaos Malandrakis, Michael Falcone, Colin Vaz,
Jesse Bisogni, Alexandros Potamianos, and Shrikanth
Narayanan. 2014. Sail: Sentiment analysis using se-
mantic similarity and contrast. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval 2014).
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems,
pages 3111–3119.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Proceedings of the 7th International Work-
shop on Semantic Evaluation.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 380–390.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’2015,
Denver, Colorado, June.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1985. Learning internal representations by
error propagation. Technical report, DTIC Document.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th
annual meeting of the association for computational
linguistics, pages 384–394. Association for Computa-
tional Linguistics.
Xiaodan Zhu, Svetlana Kiritchenko, and Saif Moham-
mad. 2014. NRC-Canada-2014:: Recent improve-
ments in the sentiment analysis of tweets. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation (SemEval 2014).
</reference>
<page confidence="0.998904">
656
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.331095">
<title confidence="0.9987245">INESC-ID: Sentiment Analysis without hand-coded Features or Resources using Embedding Subspaces</title>
<author confidence="0.976052">F Astudillo</author>
<author confidence="0.976052">Silvio Amir</author>
<author confidence="0.976052">Wang Ling</author>
<author confidence="0.976052">Bruno M´ario Silva</author>
<author confidence="0.976052">Isabel</author>
<affiliation confidence="0.905198">de Engenharia de Sistemas e Computadores e</affiliation>
<author confidence="0.503648">Rua Alves Redol</author>
<affiliation confidence="0.765484">Lisbon,</affiliation>
<email confidence="0.982313">samir,wlin,mjs,</email>
<abstract confidence="0.993194647058823">We present the INESC-ID system for the message polarity classification task of SemEval 2015. The proposed system does not make use of any hand-coded features or linguistic resources. It relies on projecting pre-trained structured skip-gram word embeddings into a small subspace. The word embeddings can be obtained from large amounts of Twitter data in unsupervised form. The sentiment analysis supervised training is thus reduced to finding the optimal projection which can be carried out efficiently despite the little data available. We analyze in detail the proposed approach and show that a competitive system can be attained with only a few configuration parameters.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3768" citStr="Bansal et al., 2014" startWordPosition="564" endWordPosition="567"> large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>the Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="3918" citStr="Blei et al., 2003" startWordPosition="589" endWordPosition="592">ularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Comput. Linguist.,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="3890" citStr="Brown et al., 1992" startWordPosition="583" endWordPosition="586">mbeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given po</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Comput. Linguist., 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<contexts>
<context position="3746" citStr="Chen and Manning, 2014" startWordPosition="560" endWordPosition="563"> embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the </context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2537</pages>
<contexts>
<context position="3268" citStr="Collobert et al., 2011" startWordPosition="489" endWordPosition="493">te a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we jointly learn the sentiment subspace projection and the classifier using the SemEval training data. The resulting system attains state-of-theart performance without hand-coded features or linguistic resources and only a few configuration parameters. 2 Unsupervised Learning of Word Embeddings Unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clusterin</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493– 2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Yoshua Bengio</author>
</authors>
<title>Understanding the difficulty of training deep feedforward neural networks.</title>
<date>2010</date>
<booktitle>In In Proceedings of the International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="11169" citStr="Glorot and Bengio, 2010" startWordPosition="1843" endWordPosition="1846">it into 80% for parameter learning and 20% for hyper-parameter selection, maintaining the original sentiment relative frequencies in each set. The 2013 and 2014 SemEval sentiment analysis test sets were used to validate the different candidate models. The most probable class was selected as the model prediction. The parameters of subspace model in Equation 2, S and C were estimated to minimize the negative log-likelihood of the correct class. Training employed conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with mini-batch size 1 and random uniform initialization similar to (Glorot and Bengio, 2010). After some initial experiments, it was determined that a learning rate of 0.01 and selecting the model with the best accuracy on the 20% set after 8 iterations led to the best results. 5 Experiments and Results 5.1 Sensibility Analysis This section analyzes the performance of the proposed system on the message polarity classification task of SemEval 2015. In general, the sentiment subspace model showed consistent and fast convergence towards the optimum in very few iterations. Despite using class log-likelihood as training criterion and accuracy as stopping criterion, the model showed good p</context>
</contexts>
<marker>Glorot, Bengio, 2010</marker>
<rawString>Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In In Proceedings of the International Conference on Artificial Intelligence and Statistics, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Omer Levy</author>
</authors>
<title>word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</title>
<date>2014</date>
<contexts>
<context position="9520" citStr="Goldberg and Levy, 2014" startWordPosition="1578" endWordPosition="1581">earns a projection of E into a small sub-space of size s « e. 4 Proposed System 4.1 Unsupervised Word Embeddings Learning The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2. We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work. The words that occurred less than 40 times in the data were discarded from the vocabulary. To train the model, we used a negative sampling rate of 25 words, sampled from a multinomial of unigram word probabilities over all the vocabulary (Goldberg and Levy, 2014). Embeddings of 50, 200, 400 and 600 dimensions were trained. It should be noted that the training configuration is generic and was not adapted to the SemEval task. One consequence of this is a relatively strong pruning of the vocabulary. Around 23% of words in the SemEval tasks did not have an embedding and thus were set to have an embedding of e zeros. 4.2 Supervised Embedding Sub-space Learning Text normalization for the supervised task employed the CMU tokenizer plus the following additional steps: messages were lower-cased, Twitter user mentions and URLs were replaced with special tokens </context>
</contexts>
<marker>Goldberg, Levy, 2014</marker>
<rawString>Yoav Goldberg and Omer Levy. 2014. word2vec explained: deriving mikolov et al.’s negativesampling word-embedding method. arXiv preprint arXiv:1402.3722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Two/too simple adaptations of word2vec for syntax problems.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="4320" citStr="Ling et al., 2015" startWordPosition="654" endWordPosition="657">llobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v. Let wp = j denote that the word p positions further in the sentence is the j-th word on the vocabulary. The skip-gram models the following probability: p(wp = j|w = i; C, E) oc exp (Cj · E · wz) . (1) Here, wz E 11, 01v×1 is a one-hot representation of w = i. That is, a vector of zeros of the size of the vocabulary v with a 1 on the i-th entry of the vector. The </context>
</contexts>
<marker>Ling, Dyer, Black, Trancoso, 2015</marker>
<rawString>Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso. 2015. Two/too simple adaptations of word2vec for syntax problems. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikolaos Malandrakis</author>
<author>Michael Falcone</author>
<author>Colin Vaz</author>
<author>Jesse Bisogni</author>
<author>Alexandros Potamianos</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Sail: Sentiment analysis using semantic similarity and contrast.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1907" citStr="Malandrakis et al., 2014" startWordPosition="281" endWordPosition="284"> need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containi</context>
</contexts>
<marker>Malandrakis, Falcone, Vaz, Bisogni, Potamianos, Narayanan, 2014</marker>
<rawString>Nikolaos Malandrakis, Michael Falcone, Colin Vaz, Jesse Bisogni, Alexandros Potamianos, and Shrikanth Narayanan. 2014. Sail: Sentiment analysis using semantic similarity and contrast. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="4064" citStr="Mikolov et al., 2013" startWordPosition="612" endWordPosition="615">Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v. Let wp = j denote that the word p positions further in the sentence is the j-th word on the vocabulary. The s</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="1446" citStr="Nakov et al., 2013" startWordPosition="210" endWordPosition="213">sed training is thus reduced to finding the optimal projection which can be carried out efficiently despite the little data available. We analyze in detail the proposed approach and show that a competitive system can be attained with only a few configuration parameters. 1 Introduction Web-based social networks are a rich data source for both businesses and academia. However, the sheer volume, diversity and rate of creation of social media, imposes the need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expre</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 task 2: Sentiment analysis in Twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="2215" citStr="Rosenthal et al., 2015" startWordPosition="328" endWordPosition="331">rch has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containing contextual information. Instead of using these word embeddings directly with, for instance, a logistic regression classifier, we estimate a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we j</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning internal representations by error propagation.</title>
<date>1985</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="8737" citStr="Rumelhart et al., 1985" startWordPosition="1438" endWordPosition="1441">se three classes. The sub-space non-linear model estimates thus the probability of each possible category y = k given a message m as p(y = k|m; C,S) a exp(Ck · Q (S · E · m) · B), (2) where Q() is a sigmoid function acting on each element of the matrix. The matrix C E R3×s maps the embedding sub-space to the classification space and B E 1n×1 is a matrix of ones that sums the scores for all words up prior to normalization. This simplification, equivalent to a bag of words assumption, outperformed other approaches like convolution. The model is thus equivalent to a multi-layer perceptron (MLP) (Rumelhart et al., 1985) with one hidden sigmoid layer and a soft-max output layer. The input to the MLP would be the fixed word embeddings attained by applying E. The input layer S learns a projection of E into a small sub-space of size s « e. 4 Proposed System 4.1 Unsupervised Word Embeddings Learning The embedding matrix E was trained in unsupervised fashion using the structured skip-gram model, described in Section 2. We used the corpus of 52 million tweets used in (Owoputi et al., 2013) with the tokenizer described in the same work. The words that occurred less than 40 times in the data were discarded from the v</context>
<context position="11075" citStr="Rumelhart et al., 1985" startWordPosition="1829" endWordPosition="1832">om 5 to 30. Training was also kept as simple as possible. The training set of SemEval was split into 80% for parameter learning and 20% for hyper-parameter selection, maintaining the original sentiment relative frequencies in each set. The 2013 and 2014 SemEval sentiment analysis test sets were used to validate the different candidate models. The most probable class was selected as the model prediction. The parameters of subspace model in Equation 2, S and C were estimated to minimize the negative log-likelihood of the correct class. Training employed conventional Stochastic Gradient Descent (Rumelhart et al., 1985) with mini-batch size 1 and random uniform initialization similar to (Glorot and Bengio, 2010). After some initial experiments, it was determined that a learning rate of 0.01 and selecting the model with the best accuracy on the 20% set after 8 iterations led to the best results. 5 Experiments and Results 5.1 Sensibility Analysis This section analyzes the performance of the proposed system on the message polarity classification task of SemEval 2015. In general, the sentiment subspace model showed consistent and fast convergence towards the optimum in very few iterations. Despite using class lo</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1985</marker>
<rawString>David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. 1985. Learning internal representations by error propagation. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3243" citStr="Turian et al., 2010" startWordPosition="485" endWordPosition="488">classifier, we estimate a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we jointly learn the sentiment subspace projection and the classifier using the SemEval training data. The resulting system attains state-of-theart performance without hand-coded features or linguistic resources and only a few configuration parameters. 2 Unsupervised Learning of Word Embeddings Unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extractio</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Svetlana Kiritchenko</author>
<author>Saif Mohammad</author>
</authors>
<title>NRC-Canada-2014:: Recent improvements in the sentiment analysis of tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1879" citStr="Zhu et al., 2014" startWordPosition="277" endWordPosition="280">l media, imposes the need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous w</context>
</contexts>
<marker>Zhu, Kiritchenko, Mohammad, 2014</marker>
<rawString>Xiaodan Zhu, Svetlana Kiritchenko, and Saif Mohammad. 2014. NRC-Canada-2014:: Recent improvements in the sentiment analysis of tweets. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>