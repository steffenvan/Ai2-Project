<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000643">
<title confidence="0.9427865">
CU-TMP:
Temporal Relation Classification Using Syntactic and Semantic Features
</title>
<author confidence="0.999482">
Steven Bethard and James H. Martin
</author>
<affiliation confidence="0.998479">
Department of Computer Science
University of Colorado at Boulder
</affiliation>
<address confidence="0.97576">
430 UCB, Boulder, CO 80309, USA
</address>
<email confidence="0.999854">
{bethard,martin}@colorado.edu
</email>
<sectionHeader confidence="0.998607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916727272727">
We approached the temporal relation identi-
fication tasks of TempEval 2007 as pair-wise
classification tasks. We introduced a va-
riety of syntactically and semantically mo-
tivated features, including temporal-logic-
based features derived from running our
Task B system on the Task A and C data.
We trained support vector machine models
and achieved the second highest accuracies
on the tasks: 61% on Task A, 75% on Task B
and 54% on Task C.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993114292682927">
In recent years, the temporal structure of text has be-
come a popular area of natural language processing
research. Consider a sentence like:
(1) The top commander of a Cambodian resistance
force said Thursday he has sent a team to
recover the remains of a British mine removal
expert kidnapped and presumed killed by
Khmer Rouge guerrillas almost two years ago.
English speakers immediately recognize that kid-
napping came first, then sending, and finally saying,
even though before and after never appeared in the
text. How can machines learn to do the same?
The 2007 TempEval competition tries to address
this question by establishing a common corpus on
which research systems can compete to find tempo-
ral relations (Verhagen et al., 2007). TempEval con-
siders the following types of event-time temporal re-
lations:
Task A Events1and times within the same sentence
Task B Events1 and document times
Task C Matrix verb events in adjacent sentences
In each of these tasks, systems attempt to annotate
pairs with one of the following relations: BEFORE,
BEFORE-OR-OVERLAP, OVERLAP, OVERLAP-OF-
AFTER, AFTER or VAGUE. Competing systems are
instructed to find all temporal relations of these
types in a corpus of newswire documents.
We approach these tasks as pair-wise classifi-
cation problems, where each event/time pair is
assigned one of the TempEval relation classes
(BEFORE, AFTER, etc.). Event/time pairs are en-
coded using syntactically and semantically moti-
vated features, and then used to train support vector
machine (SVM) classifiers.
The remainder of this paper is structured as fol-
lows. Section 2 describes the features used to char-
acterize event/time relations. Section 3 explains how
we used these features to train SVM models for each
task. Section 4 discusses the performance of our
models on the TempEval data, and Section 5 sum-
marizes the lessons learned and future directions.
</bodyText>
<sectionHeader confidence="0.999178" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.99994775">
We used a variety of lexical, syntactic and semantic
features to characterize the different types of tempo-
ral relations. In each task, the events and times were
characterized using the features:
</bodyText>
<footnote confidence="0.905813">
word The text of the event or time words
1TempEval only considers events that occurred at least 20
times in the TimeBank (Pustejovsky et al., 2003) corpus for
these tasks
</footnote>
<page confidence="0.966667">
129
</page>
<bodyText confidence="0.5638255">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 129–132,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<figure confidence="0.998281055555556">
S
������������� ���
�������������
ended [TIMESept 30]
For
��� NP
[TIMEthe quarter]
��� VP ���
��� NP ���
���� VP
����� NP
[EVENTposted] net income of $133 million
VBN
VBD
�����PP
IN ����NP
NP
Delta
</figure>
<figureCaption confidence="0.999985">
Figure 1: A syntactic tree. The path between posted and the quarter is VBD-VP-S-PP-NP-NP
</figureCaption>
<bodyText confidence="0.9995962">
pos The parts of speech2of the words, e.g. this cru-
cial moment has the parts of speech DT-JJ-NN.
gov-prep Any prepositions governing the event or
time, e.g. in during the Iran-Iraq war, the
preposition during governs the event war, and
in after ten years, the preposition after governs
the time ten years.
gov-verb The verb that governs the event or time,
e.g. in rejected in peace talks, the verb rejected
governs the event talks, and in withdrawing on
Friday, the verb withdrawing governs the time
Friday. For events that are verbs, this feature is
just the event itself.
gov-verb-pos The part of speech2 of the governing
verb, e.g. withdrawing has the part of speech
VBG.
aux Any auxiliary verbs and adverbs modifying the
governing verb, e.g. in could not come, the
words could and not are considered auxiliaries
for the event come, and in will begin withdraw-
ing on Friday, the words will and begin are con-
sidered auxiliaries for the time Friday.
Events were further characterized using the features
(the last six use gold-standard TempEval markup):
modal Whether or not the event has one of the aux-
iliaries, can, will, shall, may, or any of their
variants (could, would, etc.).
gold-stem The stem, e.g. the stem offallen is fall.
gold-pos The part-of-speech, e.g. NOUN or VERB.
gold-class The semantic class, e.g. REPORTING.
gold-tense The tense, e.g. PAST or PRESENT.
gold-aspect The aspect, e.g. PERFECTIVE.
gold-polarity The polarity, e.g. POS or NEG.
Times were further characterized using the follow-
ing gold-standard TempEval features:
</bodyText>
<footnote confidence="0.438956">
2From MXPOST (ftp.cis.upenn.edu/pub/adwait/jmx/)
</footnote>
<bodyText confidence="0.958009388888889">
gold-type The type, e.g. DATE or TIME.
gold-value The value, e.g. PAST REF or 1990-09.
gold-func The temporal function, e.g. TRUE.
These gold-standard event and time features are sim-
ilar to those used by Mani and colleagues (2006).
The features above don’t capture much of the dif-
ferences between the tasks, so we introduced some
task-specific features. Task A included the features:
inter-time The count of time expressions between
the event and time, e.g. in Figure 1, there is
one time expression, Sept 30, between the event
posted and the time the quarter.
inter-path The syntactic path between the event
and the time, e.g. in Figure 1 the
path between posted and the quarter is
VBD&gt;VP&gt;S&lt;PP&lt;NP&lt;NP.
inter-path-parts The path, broken into three parts:
the tags from the event to the lowest common
ancestor (LCA), the LCA, and the tags from the
LCA to the time, e.g. in Figure 1 the parts are
VBD&gt;VP, S and PP&lt;NP&lt;NP.
inter-clause The number of clause nodes along the
syntactic path, e.g. in Figure 1 there is one
clause node along the path, the top S node.
Our syntactic features were derived from a syntactic
tree, though Boguraev and Ando (2005) suggest that
some could be derived from finite state grammars.
For Task C we included the following feature:
tense-rules The relation predicted by a set of tense
rules, where past tense events come BEFORE
present tense events, present tense events come
BEFORE future tense events, etc. In the text:
(2) Finally today, we [EVENT learned] that
the space agency has taken a giant leap
forward. Collins will be [EVENT named]
commander of Space Shuttle Columbia.
</bodyText>
<page confidence="0.987681">
130
</page>
<bodyText confidence="0.946892222222222">
Since learned is in past tense and named is in
future, the relation is (learned BEFORE named).
In preliminary experiments, the Task B system had
the best performance, so we ran this system on the
data for Tasks A and C, and used the output to add
the following feature for both tasks:
task-b-rel The relation predicted by combining the
output of the Task B system with temporal
logic. For example, consider the text:
(3) [TIME 08-15-90 (=1990-08-15)]
Iraq’s Saddam Hussein
[TIME today (=1990-08-15)] sought
peace on another front by promising to
release soldiers captured during the
Iran-Iraq [EVENT war].
If Task B said (war BEFORE 08−15−90)
then since 08−15−90=1990−08−15=today,
the relation (war BEFORE today) must hold.
</bodyText>
<sectionHeader confidence="0.996475" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.99992745">
Using the features described in the previous section,
each temporal relation — an event paired with a time
or another event — was translated into a set of fea-
ture values. Pairing those feature values with the
TempEval labels (BEFORE, AFTER, etc.) we trained
a statistical classifier for each task. We chose sup-
port vector machines3(SVMs) for our classifiers as
they have shown good performance on a variety of
natural language processing tasks (Kudo and Mat-
sumoto, 2001; Pradhan et al., 2005).
Using cross-validations on the training data, we
performed a simple feature selection where any fea-
ture whose removal improved the cross-validation
F-score was discarded. The resulting features for
each task are listed in Table 1. After feature selec-
tion, we set the SVM free parameters, e.g. the ker-
nel degree and cost of misclassification, by perform-
ing additional cross-validations on the training data,
and selecting the model parameters which yielded
the highest F-score for each task4.
</bodyText>
<footnote confidence="0.8440268">
3We used the TinySVM implementation from
http://chasen.org/%7Etaku/software/TinySVM/ and trained
one-vs-rest classifiers.
4We only experimented with polynomial kernels.
Feature Task A Task B Task C
</footnote>
<equation confidence="0.995266">
event-word
event-pos X X
event-gov-prep X X
event-gov-verb X X
event-gov-verb-pos X X 2
event-aux X X X
modal X X
gold-stem X X 1
gold-pos X X
gold-class X X X
gold-tense X X X
gold-aspect X X
gold-polarity X X
time-word X
time-pos X
time-gov-prep X
time-gov-verb X
time-gov-verb-pos X
time-aux X
gold-type
gold-value X X
gold-func X
inter-time X
inter-path X
inter-path-parts X
inter-clause X
tense-rules X
task-b-rel X X
</equation>
<tableCaption confidence="0.84226">
Table 1: Features used in each task. An X indicates
that the feature was used for that task. For Task C, 1
indicates that the feature was used only for the first
event and not the second, and 2 indicates the reverse.
</tableCaption>
<table confidence="0.99912">
Strict Relaxed
Task P R F P R F
A 0.61 0.61 0.61 0.63 0.63 0.63
B 0.75 0.75 0.75 0.76 0.76 0.76
C 0.54 0.54 0.54 0.60 0.60 0.60
</table>
<tableCaption confidence="0.894538333333333">
Table 2: (P)recision, (R)ecall and (F)-measure of
the models on each task. Precision, recall and F-
measure are all equivalent to classification accuracy.
</tableCaption>
<sectionHeader confidence="0.999796" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999418636363637">
We evaluated our classifers on the TempEval test
data. Because the Task A and C models derived fea-
tures from the Task B temporal relations, we first ran
the Task B classifer over all the data, and then ran the
Task A and Task C classifiers over their individual
data. The resulting temporal relation classifications
were evalutated using the standard TempEval scor-
ing script. Table 2 summarizes these results.
Our models achieved an accuracy of 61% on
Task A, 75% on Task B and 54% on Task C, the
second highest scores on all these tasks. The Temp-
</bodyText>
<page confidence="0.992608">
131
</page>
<figure confidence="0.5601564375">
Task Feature Removed Model Accuracy
- 0.663
time-gov-prep 0.650
A gold-value 0.652
polarity 0.655
task-b-rel 0.656
- 0.809
event-aux 0.780
gold-stem 0.784
gold-class 0.794
- 0.534
event-gov-verb-2 0.522
event-aux-2 0.525
C gold-class-1 0.526
gold-class-2 0.527
event-pos-2, task-b-rel 0.529
</figure>
<tableCaption confidence="0.982949">
Table 3: Feature analysis. The ‘-’ lines show the
</tableCaption>
<bodyText confidence="0.991791269230769">
accuracy of the model with all features.
Eval scoring script also reported a relaxed measure
where for example, systems could get partial credit
for matching a gold standard label like OVERLAP-
OR-AFTER with OVERLAP or AFTER. Under this
measure, our models achieved an accuracy of 63%
on Task A, 76% on Task B and 60% on Task C, again
the second highest scores in the competition.
We performed a basic feature analysis where, for
each feature in a task, a model was trained with that
feature removed and all other features retained. We
evaluated the performance of the resulting models
using cross-validations on the training data5. Fea-
tures whose removal resulted in the largest drops in
model performance are listed in Table 3.
For Task A, the most important features were the
preposition governing the time and the time’s nor-
malized value. For Task B, the most important fea-
tures were the auxiliaries governing the event, and
the event’s stem. For Task C, the most important
features were the verb and auxiliaries governing the
second event. For both Tasks A and C, the features
based on the Task B relations were one of the top
six features. In general however, no single feature
dominated any one task — the greatest drop in per-
formance from removing a feature was only 2.9%.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.8044495">
TempEval 2007 introduced a common dataset for
work on identifying temporal relations. We framed
</bodyText>
<footnote confidence="0.9615605">
5We used cross-validations on the training data to preserve
the validity of the TempEval test data for future research
</footnote>
<bodyText confidence="0.999789">
the TempEval tasks as pair-wise classification prob-
lems where pairs of events and times were assigned
a temporal relation class. We introduced a variety of
syntactic and semantic features, including paths be-
tween constituents in a syntactic tree, and temporal
relations deduced by running our Task B system on
the Task A and C data. Our models achieved an ac-
curacy of 61% on Task A, 75% on Task B and 54%
on Task C. Analysis of these models indicated that
no single feature dominated any given task, and sug-
gested that future work should focus on new features
to better characterize temporal relations.
</bodyText>
<sectionHeader confidence="0.999821" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999817">
This research was performed under an appointment
of the first author to the DHS Scholarship and
Fellowship Program, administered by the ORISE
through an interagency agreement between DOE
and DHS. ORISE is managed by ORAU under DOE
contract number DE-AC05-06OR23100. All opin-
ions expressed in this paper are the author’s and
do not necessarily reflect the policies and views of
DHS, DOE, or ORAU/ORISE.
</bodyText>
<sectionHeader confidence="0.999451" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997696">
B. Boguraev and R. K. Ando. 2005. Timebank-driven
timeml analysis. In Graham Katz, James Pustejovsky,
and Frank Schilder, editors, Annotating, Extracting
and Reasoning about Time and Events, Dagstuhl Sem-
inars. German Research Foundation.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In NAACL.
I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and
J. Pustejovsky. 2006. Machine learning of temporal
relations. In COLING/ACL.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Mar-
tin, and D. Jurafsky. 2005. Support vector learning for
semantic argument classification. Machine Learning,
60(1):11–39.
J. Pustejovsky, P. Hanks, R. Saur, A. See, R. Gaizauskas,
A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro,
and M. Lazo. 2003. The timebank corpus. In Corpus
Linguistics, pages 647–656.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, and
J. Pustejovsky. 2007. Semeval-2007 task 15: Temp-
eval temporal relation identification. In SemEval-
2007: 4th International Workshop on Semantic Evalu-
ations.
</reference>
<figure confidence="0.7404">
B
</figure>
<page confidence="0.95403">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.814814">
<title confidence="0.996472">CU-TMP: Temporal Relation Classification Using Syntactic and Semantic Features</title>
<author confidence="0.99995">Bethard H Martin</author>
<affiliation confidence="0.9994515">Department of Computer Science University of Colorado at Boulder</affiliation>
<address confidence="0.999858">430 UCB, Boulder, CO 80309, USA</address>
<abstract confidence="0.984978833333333">We approached the temporal relation identification tasks of TempEval 2007 as pair-wise classification tasks. We introduced a variety of syntactically and semantically motivated features, including temporal-logicbased features derived from running our Task B system on the Task A and C data. We trained support vector machine models and achieved the second highest accuracies on the tasks: 61% on Task A, 75% on Task B and 54% on Task C.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>R K Ando</author>
</authors>
<title>Timebank-driven timeml analysis.</title>
<date>2005</date>
<booktitle>Annotating, Extracting and Reasoning about Time and Events, Dagstuhl Seminars. German Research Foundation.</booktitle>
<editor>In Graham Katz, James Pustejovsky, and Frank Schilder, editors,</editor>
<contexts>
<context position="6168" citStr="Boguraev and Ando (2005)" startWordPosition="990" endWordPosition="993">ent posted and the time the quarter. inter-path The syntactic path between the event and the time, e.g. in Figure 1 the path between posted and the quarter is VBD&gt;VP&gt;S&lt;PP&lt;NP&lt;NP. inter-path-parts The path, broken into three parts: the tags from the event to the lowest common ancestor (LCA), the LCA, and the tags from the LCA to the time, e.g. in Figure 1 the parts are VBD&gt;VP, S and PP&lt;NP&lt;NP. inter-clause The number of clause nodes along the syntactic path, e.g. in Figure 1 there is one clause node along the path, the top S node. Our syntactic features were derived from a syntactic tree, though Boguraev and Ando (2005) suggest that some could be derived from finite state grammars. For Task C we included the following feature: tense-rules The relation predicted by a set of tense rules, where past tense events come BEFORE present tense events, present tense events come BEFORE future tense events, etc. In the text: (2) Finally today, we [EVENT learned] that the space agency has taken a giant leap forward. Collins will be [EVENT named] commander of Space Shuttle Columbia. 130 Since learned is in past tense and named is in future, the relation is (learned BEFORE named). In preliminary experiments, the Task B sys</context>
</contexts>
<marker>Boguraev, Ando, 2005</marker>
<rawString>B. Boguraev and R. K. Ando. 2005. Timebank-driven timeml analysis. In Graham Katz, James Pustejovsky, and Frank Schilder, editors, Annotating, Extracting and Reasoning about Time and Events, Dagstuhl Seminars. German Research Foundation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="7833" citStr="Kudo and Matsumoto, 2001" startWordPosition="1262" endWordPosition="1266"> the Iran-Iraq [EVENT war]. If Task B said (war BEFORE 08−15−90) then since 08−15−90=1990−08−15=today, the relation (war BEFORE today) must hold. 3 Models Using the features described in the previous section, each temporal relation — an event paired with a time or another event — was translated into a set of feature values. Pairing those feature values with the TempEval labels (BEFORE, AFTER, etc.) we trained a statistical classifier for each task. We chose support vector machines3(SVMs) for our classifiers as they have shown good performance on a variety of natural language processing tasks (Kudo and Matsumoto, 2001; Pradhan et al., 2005). Using cross-validations on the training data, we performed a simple feature selection where any feature whose removal improved the cross-validation F-score was discarded. The resulting features for each task are listed in Table 1. After feature selection, we set the SVM free parameters, e.g. the kernel degree and cost of misclassification, by performing additional cross-validations on the training data, and selecting the model parameters which yielded the highest F-score for each task4. 3We used the TinySVM implementation from http://chasen.org/%7Etaku/software/TinySVM</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>M Verhagen</author>
<author>B Wellner</author>
<author>C M Lee</author>
<author>J Pustejovsky</author>
</authors>
<title>Machine learning of temporal relations.</title>
<date>2006</date>
<booktitle>In COLING/ACL.</booktitle>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky. 2006. Machine learning of temporal relations. In COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>V Krugler</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<volume>60</volume>
<issue>1</issue>
<contexts>
<context position="7856" citStr="Pradhan et al., 2005" startWordPosition="1267" endWordPosition="1270">. If Task B said (war BEFORE 08−15−90) then since 08−15−90=1990−08−15=today, the relation (war BEFORE today) must hold. 3 Models Using the features described in the previous section, each temporal relation — an event paired with a time or another event — was translated into a set of feature values. Pairing those feature values with the TempEval labels (BEFORE, AFTER, etc.) we trained a statistical classifier for each task. We chose support vector machines3(SVMs) for our classifiers as they have shown good performance on a variety of natural language processing tasks (Kudo and Matsumoto, 2001; Pradhan et al., 2005). Using cross-validations on the training data, we performed a simple feature selection where any feature whose removal improved the cross-validation F-score was discarded. The resulting features for each task are listed in Table 1. After feature selection, we set the SVM free parameters, e.g. the kernel degree and cost of misclassification, by performing additional cross-validations on the training data, and selecting the model parameters which yielded the highest F-score for each task4. 3We used the TinySVM implementation from http://chasen.org/%7Etaku/software/TinySVM/ and trained one-vs-re</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. H. Martin, and D. Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>P Hanks</author>
<author>R Saur</author>
<author>A See</author>
<author>R Gaizauskas</author>
<author>A Setzer</author>
<author>D Radev</author>
<author>B Sundheim</author>
<author>D Day</author>
<author>L Ferro</author>
<author>M Lazo</author>
</authors>
<title>The timebank corpus. In Corpus Linguistics,</title>
<date>2003</date>
<pages>647--656</pages>
<contexts>
<context position="2942" citStr="Pustejovsky et al., 2003" startWordPosition="464" endWordPosition="467">eatures used to characterize event/time relations. Section 3 explains how we used these features to train SVM models for each task. Section 4 discusses the performance of our models on the TempEval data, and Section 5 summarizes the lessons learned and future directions. 2 Features We used a variety of lexical, syntactic and semantic features to characterize the different types of temporal relations. In each task, the events and times were characterized using the features: word The text of the event or time words 1TempEval only considers events that occurred at least 20 times in the TimeBank (Pustejovsky et al., 2003) corpus for these tasks 129 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 129–132, Prague, June 2007. c�2007 Association for Computational Linguistics S ������������� ��� ������������� ended [TIMESept 30] For ��� NP [TIMEthe quarter] ��� VP ��� ��� NP ��� ���� VP ����� NP [EVENTposted] net income of $133 million VBN VBD �����PP IN ����NP NP Delta Figure 1: A syntactic tree. The path between posted and the quarter is VBD-VP-S-PP-NP-NP pos The parts of speech2of the words, e.g. this crucial moment has the parts of speech DT-JJ-NN. gov-prep Any prepos</context>
</contexts>
<marker>Pustejovsky, Hanks, Saur, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>J. Pustejovsky, P. Hanks, R. Saur, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, and M. Lazo. 2003. The timebank corpus. In Corpus Linguistics, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Verhagen</author>
<author>R Gaizauskas</author>
<author>F Schilder</author>
<author>M Hepple</author>
<author>J Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In SemEval2007: 4th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="1442" citStr="Verhagen et al., 2007" startWordPosition="225" endWordPosition="228">sider a sentence like: (1) The top commander of a Cambodian resistance force said Thursday he has sent a team to recover the remains of a British mine removal expert kidnapped and presumed killed by Khmer Rouge guerrillas almost two years ago. English speakers immediately recognize that kidnapping came first, then sending, and finally saying, even though before and after never appeared in the text. How can machines learn to do the same? The 2007 TempEval competition tries to address this question by establishing a common corpus on which research systems can compete to find temporal relations (Verhagen et al., 2007). TempEval considers the following types of event-time temporal relations: Task A Events1and times within the same sentence Task B Events1 and document times Task C Matrix verb events in adjacent sentences In each of these tasks, systems attempt to annotate pairs with one of the following relations: BEFORE, BEFORE-OR-OVERLAP, OVERLAP, OVERLAP-OFAFTER, AFTER or VAGUE. Competing systems are instructed to find all temporal relations of these types in a corpus of newswire documents. We approach these tasks as pair-wise classification problems, where each event/time pair is assigned one of the Temp</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Pustejovsky, 2007</marker>
<rawString>M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, and J. Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In SemEval2007: 4th International Workshop on Semantic Evaluations.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>