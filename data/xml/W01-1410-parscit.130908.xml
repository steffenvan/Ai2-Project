<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000128">
<title confidence="0.9996205">
Machine Translation with Grammar Association:
Some Improvements and the Loco C Model
</title>
<author confidence="0.975789">
Federico Prat
</author>
<affiliation confidence="0.8864855">
Departamento de Lenguajes y Sistemas Informiticos
Universitat Jaume I de Castell´o
</affiliation>
<address confidence="0.816196">
E-12071 Castell´on de la Plana, Spain
</address>
<email confidence="0.99539">
fprat@lsi.uji.es
</email>
<sectionHeader confidence="0.995529" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801307692308">
Grammar Association is a technique for
Machine Translation and Language Un-
derstanding introduced in 1993 by Vi-
dal, Pieraccini and Levin. All the sta-
tistical and structural models involved
in the translation process are automat-
ically built from bilingual examples,
and the optimal translation of new sen-
tences can be efficiently found by Dy-
namic Programming algorithms. This
paper presents and discusses Grammar
Association state of the art, including a
new statistical model: Loco C.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9973444375">
Grammar Association is a promising technique
for facing Machine Translation and Language
Understanding tasks,1 first proposed by Vidal,
Pieraccini, and Levin (1993). This technique
combines statistical and structural models, all of
which can be automatically built from a set of
bilingual sentence pairs. Moreover, the optimal
translation of new input sentences can be effi-
ciently found by Dynamic Programming algo-
rithms.
Basically, a Grammar Association system con-
sists of three models: (1) an input grammar mod-
elling the input language of the translation task;
(2) an output grammar modelling its output lan-
guage; (3) an association model describing how
the use of certain elements (rules) of the input
</bodyText>
<footnote confidence="0.750869666666667">
1We view Language Understanding as a particular case
of Machine Translation where the output language is aimed
at representing the meaning of input sentences.
</footnote>
<bodyText confidence="0.999436222222222">
grammar is related (in the translation task) to the
use of their corresponding elements in the output
grammar. Using these models, the system per-
forms the translation of input sentences as fol-
lows: (1) first, the input sentence is parsed using
the input grammar, giving rise to an input deriva-
tion; (2) given the input derivation, the associa-
tion model assigns a weight to each rule of the
output grammar; (3) in the (now weighted) output
grammar, a search for the optimal output deriva-
tion is carried out; (4) the sentence associated to
that derivation is conjectured as translation of the
input sentence.
We are interested in designing Machine Trans-
lation systems based on the principles of Gram-
mar Association and within a statistical frame-
work. Some steps we have taken towards this final
end are presented in this work.
</bodyText>
<sectionHeader confidence="0.9235755" genericHeader="method">
2 Grammar Association into a statistical
framework
</sectionHeader>
<bodyText confidence="0.99838893877551">
In most of the papers describing statistical ap-
proaches to Machine Translation, Bayes’ rule is
applied giving rise to the following Fundamental
Equation,
meaning that the optimal translation of an in-
put sentence , the most probable sentence in
the output language given , can be
found by maximizing the product of two factors:
The a priori probability of the output sen-
tence, . In practice, it is computed by
using a statistical model of the output lan-
guage .
This decomposition has the advantage of modu-
larity in the modelling. An ad hoc statistical lan-
guage model encapsulates the features that are in-
herent to the output language, while the reverse
translation model can focus on relations between
input and output words, assigning scores to sen-
tence pairs without taking into account if the out-
put sentence is well-formed.2 An alternative, di-
rect statistical approach with a model for comput-
ing seems to require this single model
to be complex enough to assign high scores only
to pairs where the output sentence verifies two
conditions: it is well-formed and means the same
that the input one. Hence, for the sake of sim-
plified modelling, Bayes’ decomposition has be-
come a typical choice in Machine Translation.
However, in the Grammar Association context,
when developing (using Bayes’ decomposition)
the basic equations of the system presented in (Vi-
dal et al., 1993), it is said that the reverse model
for “does not seem to admit a sim-
ple factorization which is also correct and con-
venient”, so “crude heuristics” were adopted in
the mathematical development of the expression
to be maximized. We are going to show that, by
means of a direct modelling, Grammar Associa-
tion can be set into a rigorous statistical frame-
work without renouncing a convenient factoriza-
tion for the search of the optimal translation to be
efficient. Moreover, the main advantage of Bayes’
decomposition, modularity, is inherently present
in Grammar Association systems: relations be-
tween input and output are mainly modelled by
a (direct) statistical association model and struc-
tural features of the output language are modelled
by a grammar, which restricts the search space for
the best translation.
</bodyText>
<footnote confidence="0.832351333333333">
2Note that model behaviour for syntactically incorrect
input sentences is not important because input sentence is
known and the search is just over the output language.
</footnote>
<bodyText confidence="0.997562375">
Let us begin assuming there are unambiguous
grammars and describing, respectively, the
input language and the output one . Thus,
there is a one-to-one correspondence in each lan-
guage relating sentences to their derivations and
we can write
where denotes the only derivation of sen-
tence in grammar . Moreover, let us suppose
the output grammar is context-free and rewrit-
ing probability of an output non-terminal using a
certain rule is independent of which other output
rules have been employed in the output deriva-
tion. Then, it follows that the probability of an
output derivation given an input one can
be expressed as
with a term in the sum for each participation of a
rule in the derivation , and denoting
the left-hand side non-terminal of that rule. So,
finally, we can find the most probable translation
of an input sentence as the sentence
associated to the output derivation given by
where stands for the set of all possible
derivations in .
In practice, input and output grammars will be
approximations inferred from samples and, more
specifically, they will be acyclic finite-state au-
tomata. The restriction from context-free gram-
mars to regular ones is due to the wide availabil-
ity of inference techniques for these formal ma-
chines and to computational convenience. On the
other hand, the output grammar has to be acyclic
because of a more subtle point: the most prob-
able derivation in the grammar will never make
use of a cycle (no matter how high its probability
is, avoiding the cycle always makes the deriva-
tion more probable). Hence, if we allowed the in-
ference algorithm to model some features of the
output language using cycles, system translations
The conditional probability of
the input sentence , given the output one
. In practice, it is computed by using a sta-
tistical model of the reverse translation pro-
cess.
would never exhibit such features. Finally, for
the sake of homogeneity, we choose to force in-
put grammar to be acyclic too.
We can conclude this section saying that, in-
ferring deterministic and acyclic finite-state au-
tomata, if we are able to learn association models
for estimating, for each output rule, the probabil-
ity of using that rule conditioned on having em-
ployed its left-hand side and the identity of the
input derivation, then an efficient Dynamic Pro-
gramming search for the optimal output deriva-
tion3 can be used in order to provide the most
probable translation.
</bodyText>
<sectionHeader confidence="0.984365" genericHeader="method">
3 Using ECGI language models
</sectionHeader>
<bodyText confidence="0.997618029411765">
The ECGI algorithm (Rulot and Vidal, 1987) is
a heuristic technique for the inference of acyclic
finite-state automata from positive samples, and
determinism can be imposed a posteriori by a
well-known transformation for regular grammars.
Therefore, in principle, ECGI provides exactly
the kind of language model Grammar Association
needs. Moreover, it was (without imposing deter-
minism) the inference technique employed in (Vi-
dal et al., 1993).
Informally, ECGI works as follows. With the
first sample sentence, it builds an initial automa-
ton consisting in a linear path representing the
sentence. Words label states (instead of arcs) and
there are two special non-labelled states: the ini-
tial one and the final one. For each new sentence,
if it is already recognized by the automaton built
so far, nothing happens; otherwise, if the current
model does not recognize the sentence, new arcs
and states are added to the most suitable path (ac-
cording to a minimum-cost criterion) for recogni-
tion to be possible. In a sense, it is like construct-
ing a new path for the new sentence and then find-
ing a maximal merge with a path in the automa-
ton.
For further discussion on some features of the
ECGI algorithm, let us first consider the following
set of five sentences: (1) &amp;quot;some snakes eat
rats&amp;quot;; (2) &amp;quot;some people eat snakes&amp;quot;;
(3) &amp;quot;some people eat rats&amp;quot;; (4) &amp;quot;some
people are dangerous&amp;quot;; (5) &amp;quot;snakes
are dangerous&amp;quot;. Figure 1 shows how ECGI
3Obviously, any algorithm for finding the minimum-cost
path in a graph is applicable.
</bodyText>
<figure confidence="0.79436975">
(a)&amp;quot;some snakes eat rats&amp;quot;
(b)&amp;quot;some people eat snakes&amp;quot;
(c)&amp;quot;some people are dangerous&amp;quot;
(d)&amp;quot;snakes are dangerous&amp;quot;
</figure>
<figureCaption confidence="0.99996">
Figure 1: The ECGI algorithm: an example.
</figureCaption>
<bodyText confidence="0.999908866666667">
incrementally builds an automaton able to recog-
nize the whole training set and, moreover, per-
forms some generalizations. For instance, af-
ter considering the two first sentences (subfig-
ure b), two more sentences are also represented in
the current automaton: &amp;quot;some snakes eat
snakes&amp;quot; and &amp;quot;some people eat rats&amp;quot;.
Thus, when this last sentence is actually presented
to the algorithm, there is no need for the automa-
ton to be updated. On the contrary, sentences 4
and 5 imply the addition of new elements and
the finally inferred automaton is the one shown
in subfigure d.
Though successful application of ECGI to a
variety of tasks has been reported,4 the method
</bodyText>
<footnote confidence="0.9223005">
4For instance, ECGI has been applied to problems as
different as speech understanding (Prieto and Vidal, 1992),
hand-written digit recognition (Vidal et al., 1995), and music
composition (Cruz and Vidal, 1997)
</footnote>
<figure confidence="0.996035166666667">
snakes eat rats
some
BEGIN
END
snakes
eat
some
rats
snakes
people
BEGIN
END
snakes eat
rats
snakes
some
people
are dangerous
BEGIN
END
snakes
rats
snakes
eat
some
people
are dangerous
snakes
BEGIN
END
</figure>
<figureCaption confidence="0.984907">
Figure 2: An alternative automaton.
</figureCaption>
<bodyText confidence="0.978453183098592">
suffers from some drawbacks. For instance,
the level of generalization is sometimes lower
than expected. In the example presented in
Figure 1, when &amp;quot;snakes are dangerous&amp;quot;
is employed for updating the model in subfig-
ure c, instead of adding a new state and two
arcs to the path corresponding to &amp;quot;some peo-
ple are dangerous&amp;quot;, the solution in Fig-
ure 2 seems to be an appealing alternative: adding
just two arcs, more reasonable generalization is
obtained. Nevertheless, ECGI chooses the solu-
tion in Figure 1 because it searches for just one
path to be modified with a minimal number of
new elements, and does not take into account
combinations of different paths.
On the other hand, ECGI can suffer from in-
adequate generalization, especially at early stages
of the incremental construction of the automa-
ton. If &amp;quot;some people eat snakes&amp;quot; and
&amp;quot;snakes are dangerous&amp;quot; were the first
two sentences presented to ECGI, the algorithm
would try to make use of the state &amp;quot;snakes&amp;quot;
of the initial model for representing the oc-
currence of that word in the second sentence,
leading to an automaton which would recognize
“sentences” as &amp;quot;some people eat snakes
are dangerous&amp;quot;, or simply &amp;quot;snakes&amp;quot;. The
situation that produces this kind of undesired be-
haviour of the method is characterized by the con-
fluence of a couple of circumstances: a word in a
new sentence is also present in the current model,
but with a different function, and that automaton
has not enough adequate structural information
for offering a better merging to the new sentence.
As pointed out by Prieto and Vidal (1992), a
proper ordering of the set of sentences presented
to ECGI can provide more compact models, and
we think that better ones too. The ordering we
propose here simply follows, first, a decreasing-
length criterion and then, for breaking ties, ap-
plies any dictionary-like ordering. Thus, we try
to avoid the problem discussed in the previous
paragraph by providing the inference algorithm
with as much as possible structural information at
first stages of automaton construction and, more-
over, dictionary-like ordering inside each length
is aimed at frequently presenting to ECGI new
sentences that are similar to the previous ones.
Furthermore, a very common way to reduce
the complexity of problems involving languages
is the definition of word categories, which can
be manually designed or automatically extracted
from data (Martin et al., 1995). We think catego-
rization helps in solving the problem of undesired
merges and also in increasing the generalization
abilities of ECGI. In order to illustrate this point,
let us consider a category animals consisting
of words &amp;quot;snakes&amp;quot;, &amp;quot;rats&amp;quot; and &amp;quot;people&amp;quot;
in the very simple example of Figure 1. Words
can be substituted for the appropriate category
in the original sentences; then, the modified sen-
tences are presented to the inference algorithm;
finally, categories in the automaton are expanded.
Figure 3 shows the automata that are successively
built in that process.
As said at the beginning of this section, deter-
minism must be imposed a posteriori for the lan-
guage models to fit our formal framework. In ad-
dition, we will apply them a minimization process
in order to simplify the problem that the corre-
sponding association model will have to solve.
</bodyText>
<sectionHeader confidence="0.984947" genericHeader="method">
4 Loco C: Anew association model
</sectionHeader>
<bodyText confidence="0.9999211875">
Following a data-driven approach, a Grammar
Association system needs to learn from exam-
ples an association model capable to estimate the
probabilities required by our recently developed
framework, that is, the probability of each rule
in the grammar that models the output language,
conditioned on its left-hand side and the deriva-
tion of the input sentence.
Among the different association models we
have studied (Prat, 1998), it is worth emphasizing
one we have specifically developed for playing
that role in Grammar Association systems: the
Loco C model. We based our design on the IBM
models 1 and 2 (Brown et al., 1993), but taking
into account that our model must generate cor-
rect derivations in a given grammar, not any se-
</bodyText>
<figure confidence="0.9852968">
some snakes
eat
rats
snakes
dangerous
people are
BEGIN
END
(a)&amp;quot;some animals eat animals &amp;quot;
(b)&amp;quot;some animals are dangerous&amp;quot;
</figure>
<figureCaption confidence="0.948037333333333">
Figure 3: Using a category animals for
&amp;quot;snakes&amp;quot;, &amp;quot;rats&amp;quot; and &amp;quot;people&amp;quot; in the ex-
ample of Figure 1.
</figureCaption>
<bodyText confidence="0.991942777777778">
quence of rules.5 Moreover, we wanted to model
the probability estimation for each output rule
as an adequately weighted mixture,6 along with
keeping the maximum-likelihood re-estimation of
its parameters within the growth transformation
framework (Baum and Eagon, 1967; Gopalakr-
5In those simple IBM translation models, an output se-
quence (of words) is randomly generated from a given in-
put one by first choosing its length and then, for each posi-
tion in the output sequence, independently choosing an ele-
ment (word). If the relation between input and output deriva-
tions (sequences of rules) has to be explicitly modelled, the
choices of output elements can no longer be independent be-
cause a rule is only applicable if its left-hand side has just
appeared in the output derivation.
6In IBM models, all words in the input sequence have
the same influence in the random choice of output words
(model 1) or they have a relative influence depending on
their positions (model 2). In the case of derivations, we are
interested in modelling those relative influences taking into
account rule identities (instead of rule positions).
ishnan et al., 1991). After exploring some similar
alternatives (and discarding them because of their
poor results in a few translation experiments),
Loco C was finally defined as explained below.7
The Loco C model assumes a random gener-
ation process (of an output derivation, given an
input one) which begins with the starting symbol
of the output grammar as the “current sentential
form” and then, while the current sentential form
contains a non-terminal, iteratively performs the
following sequence of two random choices: in
Choice 1, one of the rules in the input derivation is
chosen; in Choice 2, the non-terminal in the cur-
rent sentential form is rewritten using a randomly
chosen rule of the output grammar.
The behaviour of the model depends on two
kinds of parameters, each one guiding one of the
choices mentioned above. Formally, given an in-
put derivation and an output non-terminal
to be rewritten, the probability of an input rule
to be chosen in Choice 1 depends on pa-
rameters of the form and can be ex-
pressed as
On the other hand, once a particular input rule
is chosen, the probability of an output rule
whose left-hand side is to be chosen in
Choice 2 is directly given by a parameter of the
form . Hence,
takes in Loco C the form
of a weighted mixture depending on two kinds of
trainable parameters:
: Measures the importance of in
choosing an adequate rewriting rule for .8
</bodyText>
<footnote confidence="0.97905">
7Full details on the discarded models, Loco 1, Loco A,
and Loco B, can be found (in Spanish) in pages 52–60
of (Prat, 1998).
8Note that learning these parameters performs a sort of
“automatic variable selection” of the input rules that are rel-
evant for discriminatively choosing among the next applica-
ble output rules.
</footnote>
<figure confidence="0.966628102564102">
eat &lt;animals&gt;
some &lt;animals&gt;
BEGIN
END
eat
&lt;animals&gt;
some
&lt;animals&gt;
are
dangerous
BEGIN
END
(c) &amp;quot; animals are dangerous&amp;quot;
some
&lt;animals&gt;
eat
&lt;animals&gt;
are
dangerous
BEGIN
END
(d) Expansion of animals
some
eat
snakes
rats
people
dangerous
are
snakes
rats
people
BEGIN
END
MLA Task
Spanish: &amp;quot;un circulo oscuro est´a
encima de un circulo&amp;quot;
English: &amp;quot;a dark circle is above a
circle&amp;quot;
</figure>
<figureCaption confidence="0.432769">
Spanish: &amp;quot;se elimina el cuadrado os-
curo que est´a debajo del
circulo y del tri´angulo&amp;quot;
English: &amp;quot;the dark square which is
below the circle and the
triangle is removed&amp;quot;
</figureCaption>
<figure confidence="0.244103">
Simplified Tourist Task
Spanish: &amp;quot;nos vamos a ir el dia diez
a la una de la tarde.&amp;quot;
English: &amp;quot;we are leaving on the tenth
at one in the afternoon.&amp;quot;
Spanish: &amp;quot;¿puedo pagar la cuenta con
dinero en efectivo?&amp;quot;
English: &amp;quot;can I pay the bill in
cash?&amp;quot;
</figure>
<figureCaption confidence="0.999435">
Figure 4: Examples of sentence pairs.
</figureCaption>
<bodyText confidence="0.9999537">
Consequently, the corresponding likelihood func-
tion is not polynomial, but rational, so Baum-
Eagon inequality (1967) cannot be applied and
Gopalakrishnan et al. inequality (1991) must
be used, instead, in order to develop a Loco C
model re-estimation algorithm based on growth
transformations. Fortunately, both the computa-
tional complexity of the resulting re-estimation
algorithm (same order as with IBM model 1) and
the experimental results are satisfactory.
</bodyText>
<sectionHeader confidence="0.991995" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.9995255">
In a first series of experiments, we were interested
in knowing whether or not our proposals actually
improve Grammar Association state of the art. To
this end, a simple artificial Machine Translation
task was employed. The corpus consists of pairs
of sentences describing two-dimensional scenes
with circles, squares and triangles in Spanish and
English (some examples can be found in Figure 4,
where the task is referred to as MLA Task). There
are words in the Spanish vocabulary and in
</bodyText>
<tableCaption confidence="0.65480325">
Table 1: Results of an English-to-Spanish trans-
lation experiment with the original Grammar As-
sociation system, using , pairs of the MLA
Task for training and for testing.
</tableCaption>
<table confidence="0.8924065">
Sentence Minimum Length Correct
Sorting Deterministic Constraint Translations
No No No
No No Yes
No Yes No
No Yes Yes
Yes No No
Yes No Yes
Yes Yes No
Yes Yes Yes
the English one.
Let us begin considering English-to-Spanish
</table>
<bodyText confidence="0.989694857142857">
translation, with , pairs for training the sys-
tems and different ones for testing purposes.
We carefully implemented the original Grammar
Association system described in (Vidal et al.,
1993), tuned empirically a couple of smoothing
parameters, trained the models and, finally, ob-
tained an of correct translations.9 Then,
we studied the impact of: (1) sorting, as proposed
in Section 3, the set of sentences presented to
ECGI; (2) making language models deterministic
and minimum; (3) constraining the best transla-
tion search to those sentences whose lengths have
been seen, in the training set, related to the length
of the input sentence. As shown in Table 1, all
the proposed measures were beneficial and we got
a final of correct translations (that is, just
one translation was wrong). Hence, we decided to
apply those measures to all our Grammar Asso-
ciation systems and, in particular, to our Loco C
one. This system, after tuning some minor param-
eters (for instance, the number of re-estimation it-
erations for the model was fixed to ), got a
of correct translations.
Then, in order to further compare our two sys-
tems (which will be referred to as IOGA, for Im-
proved Original Grammar Association, and sim-
ply Loco C) without more manual tuning, both
were tested with , new sentence pairs: in
</bodyText>
<footnote confidence="0.8759418">
this case, IOGA got a and Loco C got
9For each bilingual sentence pair employed for
testing a system, we consider that the system achieves a cor-
rect translation only if it produces exactly the sentence as
output when it is provided with the sentence as input.
</footnote>
<bodyText confidence="0.998071392156863">
: Measures how much agrees in
using the rule.
).
Since the MLA Task is an artificial task where
each language can be exactly modelled by an
acyclic finite-state automaton, we decided to use
those exact automata in our systems in order to
measure the impact of perfect language mod-
elling. In this case, Loco C reached perfect re-
sults ( ), while IOGA got a . As a
conclusion to this second series of experiments,
we can point out that our systems are quite sensi-
tive to the quality of language models and, also,
that Loco C is a very good association model.
Our last series of experiments were carried out
on a different, more complex task (but artificial
too). It was extracted from the task defined for the
first phase of the EUTRANS project (Amengual et
al., 1996) and covers just a small subset of the
situations tourists can face when leaving hotels
(some examples can be found in Figure 4, where
the task is referred to as Simplified Tourist Task).
There are words in the Spanish vocabulary
and in the English one. We defined a stan-
dard scenario in which Spanish-to-English trans-
lation must be performed on, sentences af-
ter training the corresponding models with,
pairs.
In that scenario, Loco C achieved an
of correct translations, where errors are mainly
due to lack of coverage in the language models,
especially in the input one: only of the
Spanish sentences in the test set could be correctly
parsed with the inferred model, so we decided to
apply word categories to improve the generaliza-
tion capabilities of ECGI as exemplified in Sec-
tion 3. Using automatic categorization (Martinet
al., 1995) for extracting Spanish word classes
and English ones, the resulting language mod-
els achieved perfect coverage and the Loco C
system performance increased to
In order to put the previous figure into con-
text, it is worth saying that the best result obtained
by ReConTra in the same scenario was .
On the other hand, combining automatic bilin-
gual categorization and Subsequential Transduc-
ers as described in (Barrachina and Vilar, 1999),
a of correct translations can be achieved
for an adequate choice of the number of word
classes ( ), though only a is obtained by
the same system in the absence of categorization.
</bodyText>
<sectionHeader confidence="0.982149" genericHeader="conclusions">
6 Concluding remarks
</sectionHeader>
<bodyText confidence="0.983094766666667">
Our work presents a set of improvements on pre-
vious state of the art of Grammar Association:
first, by providing better language models to the
original system described in (Vidal et al., 1993);
second, by setting the technique into a rigorous
statistical framework, clarifying which kind of
probabilities have to be estimated by association
models; third, by developing a novel and espe-
cially adequate association model: Loco C.
On the other hand, though experimental results
are quite good, we find them particularly relevant
for pointing out directions to follow for further
improvement of the Grammar Association tech-
nique. One of these directions consists in explor-
ing better language models, refining the catego-
rization methods employed in this work or substi-
tuting ECGI for some kind of merge-based infer-
ence algorithm (Thollard et al., 2000). Exploiting
data-driven bilingual categorization (Barrachina
and Vilar, 1999) is another promising way to im-
prove the performance of our system.
Finally, let us say that, obviously, the experi-
mental results on simple artificial tasks presented
in this work are not intended for convincing the
reader that our Grammar Association systems
could obtain similar performances on complex
tasks as, for instance, the Hansards (the bilin-
gual proceedings of the Canadian parliament).
Our controlled experiments were mainly aimed
at showing that our proposals improve Gram-
mar Association, along with comparing this tech-
nique with a couple of different ones and pro-
viding easy-to-analyse results. For these simple
purposes, we find our experimental work ade-
quate. However, natural translation tasks should
be faced soon, in the next stage of our research.
This implies, for instance, trying to cope with se-
vere data sparseness. In this regard, we are op-
a .
In a second series of experiments, we wanted
to compare our best system, Loco C, with Re-
ConTra, the recurrent connectionist system de-
scribed in (Casta˜no and Casacuberta, 1997),
where a of correct translations is reported
on the Spanish-to-English MLA Task with just
, pairs for training. In the same conditions,
Loco C got a of correct translations on a
, pair test set (IOGA, just an
.
timistic: on one hand, because we trust in bilin-
gual categorization for reducing the negative ef-
fects of sparseness (Vilar et al., 1995); on the
other hand, because some additional experiments
carried out with Grammar Association systems on
the Spanish-to-English MLA Task with just
pairs for training show acceptable results. For in-
stance, our Loco C achieved an of cor-
rect translations10 while, in the same scenario,
ReConTra performance drops to (Casta˜no
and Casacuberta, 1997).
</bodyText>
<sectionHeader confidence="0.996365" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999143125">
Most of the work presented here was carried
out under the kind supervision of Dr. Francisco
Casacuberta, and the author want to express his
gratitude to him.
Furthermore, it is worth saying that this
work has been partially supported by grant
P1A99-10 from Fundaci´o Caixa Castell´o-
Bancaixa (NEUROTRAD project).
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998964608695652">
J. C. Amengual, J. M. Benedi, A. Casta˜no, A. Marzal,
F. Prat, E. Vidal, J. M. Vilar, C. Delogu, A. Di
Carlo, H. Ney, and S. Vogel. 1996. Definition of
a machine translation task and generation of cor-
pora. Technical Report D1, EuTrans (IT-LTR-OS-
20268).
S. Barrachina and J. M. Vilar. 1999. Bilingual cluster-
ing using monolingual algorithms. In Procs. of the
TMI’99, pages 77–87.
L. E. Baum and J. A. Eagon. 1967. An inequality with
applications to statistical estimation for probabilis-
tic functions of Markov processes and to a model
for ecology. Bulletin of the American Mathemati-
cal Society, 73:360–363.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Com-
putational Linguistics, 19(2):263–311.
M. A. Casta˜no and F. Casacuberta. 1997. A connec-
tionist approach to machine translation. In Procs.
of the EuroSpeech’97, volume 1, pages 91–94.
P. P. Cruz and E. Vidal. 1997. A study of grammatical
inference algorithms in automatic music composi-
tion. In Preprints of the VII National Symposium on
10In this experiment, Spanish word classes and En-
glish ones were automatically extracted from the training
pairs in order to increase ECGI generalization capabilities.
Pattern Recognition and Image Analysis, volume 1,
pages 43–48.
P. S. Gopalakrishnan, D. Kanevsky, A. N´adas, and
D. Nahamoo. 1991. An inequality for ratio-
nal functions with applications to some statistical
problems. IEEE Trans. on Information Theory,
37(1):107–113.
S. Martin, J. Liermann, and H. Ney. 1995. Algorithms
for bigram and trigram word clustering. In Procs.
of the EuroSpeech’95.
F. Prat. 1998. Traducci6n automdtica en domin-
ios restringidos: Algunos modelos estocasticos sus-
ceptibles de ser aprendidos a partir de ejemplos.
Ph.D. thesis, Depto. de Sistemas Inform´aticos y
Computaci´on, Universidad Polit´ecnica de Valencia
(Spain).
N. Prieto and E. Vidal. 1992. Learning language mod-
els through the ECGI method. Speech Communica-
tion, 11(2–3):299–309.
H. Rulot and E. Vidal. 1987. Modelling (sub)string-
length based constraints through a grammatical in-
ference method. In Pattern Recognition Theory and
Applications, volume F30 of NATO ASI, pages 451–
459. Springer-Verlag.
F. Thollard, P. Dupont, and C. de la Higuera. 2000.
Probabilistic DFA inference using Kullback-Leibler
divergence and minimality. In Procs. of the
ICML’2000, pages 975–982.
E. Vidal, R. Pieraccini, and E. Levin. 1993. Learning
associations between grammars: A new approach
to natural language understanding. In Procs. of the
EuroSpeech’93, pages 1187–1190.
E. Vidal, H. Rulot, J. M. Valiente, and G. An-
dreu. 1995. Application of the error-correcting
grammatical inference algorithm (ECGI) to planar
shape recognition. Technical Report DSIC-II/2/95,
Depto. de Sistemas Inform´aticos y Computaci´on,
Universidad Polit´ecnica de Valencia (Spain).
J. M. Vilar, A. Marzal, and E. Vidal. 1995. Learning
language translation in limited domains using finite-
state models: Some extensions and improvements.
In Procs. of the EuroSpeech’95, pages 1231–1234.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.499462">
<title confidence="0.9992365">Machine Translation with Grammar Improvements and the C</title>
<author confidence="0.98905">Federico</author>
<affiliation confidence="0.984162">Departamento de Lenguajes y Sistemas Universitat Jaume I de</affiliation>
<author confidence="0.676328">E- Castell´on de_la Plana</author>
<email confidence="0.991322">fprat@lsi.uji.es</email>
<abstract confidence="0.9821425">Grammar Association is a technique for Machine Translation and Language Understanding introduced in 1993 by Vidal, Pieraccini and Levin. All the statistical and structural models involved in the translation process are automatically built from bilingual examples, and the optimal translation of new sentences can be efficiently found by Dynamic Programming algorithms. This paper presents and discusses Grammar Association state of the art, including a statistical model:</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Amengual</author>
<author>J M Benedi</author>
<author>A Casta˜no</author>
<author>A Marzal</author>
<author>F Prat</author>
<author>E Vidal</author>
<author>J M Vilar</author>
<author>C Delogu</author>
<author>A Di Carlo</author>
<author>H Ney</author>
<author>S Vogel</author>
</authors>
<title>Definition of a machine translation task and generation of corpora.</title>
<date>1996</date>
<tech>Technical Report D1, EuTrans (IT-LTR-OS20268).</tech>
<marker>Amengual, Benedi, Casta˜no, Marzal, Prat, Vidal, Vilar, Delogu, Di Carlo, Ney, Vogel, 1996</marker>
<rawString>J. C. Amengual, J. M. Benedi, A. Casta˜no, A. Marzal, F. Prat, E. Vidal, J. M. Vilar, C. Delogu, A. Di Carlo, H. Ney, and S. Vogel. 1996. Definition of a machine translation task and generation of corpora. Technical Report D1, EuTrans (IT-LTR-OS20268).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Barrachina</author>
<author>J M Vilar</author>
</authors>
<title>Bilingual clustering using monolingual algorithms.</title>
<date>1999</date>
<booktitle>In Procs. of the TMI’99,</booktitle>
<pages>77--87</pages>
<contexts>
<context position="22945" citStr="Barrachina and Vilar, 1999" startWordPosition="3800" endWordPosition="3803">th the inferred model, so we decided to apply word categories to improve the generalization capabilities of ECGI as exemplified in Section 3. Using automatic categorization (Martinet al., 1995) for extracting Spanish word classes and English ones, the resulting language models achieved perfect coverage and the Loco C system performance increased to In order to put the previous figure into context, it is worth saying that the best result obtained by ReConTra in the same scenario was . On the other hand, combining automatic bilingual categorization and Subsequential Transducers as described in (Barrachina and Vilar, 1999), a of correct translations can be achieved for an adequate choice of the number of word classes ( ), though only a is obtained by the same system in the absence of categorization. 6 Concluding remarks Our work presents a set of improvements on previous state of the art of Grammar Association: first, by providing better language models to the original system described in (Vidal et al., 1993); second, by setting the technique into a rigorous statistical framework, clarifying which kind of probabilities have to be estimated by association models; third, by developing a novel and especially adequ</context>
</contexts>
<marker>Barrachina, Vilar, 1999</marker>
<rawString>S. Barrachina and J. M. Vilar. 1999. Bilingual clustering using monolingual algorithms. In Procs. of the TMI’99, pages 77–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
<author>J A Eagon</author>
</authors>
<title>An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology.</title>
<date>1967</date>
<journal>Bulletin of the American Mathematical Society,</journal>
<pages>73--360</pages>
<contexts>
<context position="14584" citStr="Baum and Eagon, 1967" startWordPosition="2381" endWordPosition="2384"> and 2 (Brown et al., 1993), but taking into account that our model must generate correct derivations in a given grammar, not any sesome snakes eat rats snakes dangerous people are BEGIN END (a)&amp;quot;some animals eat animals &amp;quot; (b)&amp;quot;some animals are dangerous&amp;quot; Figure 3: Using a category animals for &amp;quot;snakes&amp;quot;, &amp;quot;rats&amp;quot; and &amp;quot;people&amp;quot; in the example of Figure 1. quence of rules.5 Moreover, we wanted to model the probability estimation for each output rule as an adequately weighted mixture,6 along with keeping the maximum-likelihood re-estimation of its parameters within the growth transformation framework (Baum and Eagon, 1967; Gopalakr5In those simple IBM translation models, an output sequence (of words) is randomly generated from a given input one by first choosing its length and then, for each position in the output sequence, independently choosing an element (word). If the relation between input and output derivations (sequences of rules) has to be explicitly modelled, the choices of output elements can no longer be independent because a rule is only applicable if its left-hand side has just appeared in the output derivation. 6In IBM models, all words in the input sequence have the same influence in the random </context>
</contexts>
<marker>Baum, Eagon, 1967</marker>
<rawString>L. E. Baum and J. A. Eagon. 1967. An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematical Society, 73:360–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13991" citStr="Brown et al., 1993" startWordPosition="2287" endWordPosition="2290">ing a data-driven approach, a Grammar Association system needs to learn from examples an association model capable to estimate the probabilities required by our recently developed framework, that is, the probability of each rule in the grammar that models the output language, conditioned on its left-hand side and the derivation of the input sentence. Among the different association models we have studied (Prat, 1998), it is worth emphasizing one we have specifically developed for playing that role in Grammar Association systems: the Loco C model. We based our design on the IBM models 1 and 2 (Brown et al., 1993), but taking into account that our model must generate correct derivations in a given grammar, not any sesome snakes eat rats snakes dangerous people are BEGIN END (a)&amp;quot;some animals eat animals &amp;quot; (b)&amp;quot;some animals are dangerous&amp;quot; Figure 3: Using a category animals for &amp;quot;snakes&amp;quot;, &amp;quot;rats&amp;quot; and &amp;quot;people&amp;quot; in the example of Figure 1. quence of rules.5 Moreover, we wanted to model the probability estimation for each output rule as an adequately weighted mixture,6 along with keeping the maximum-likelihood re-estimation of its parameters within the growth transformation framework (Baum and Eagon, 1967; Gopal</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Casta˜no</author>
<author>F Casacuberta</author>
</authors>
<title>A connectionist approach to machine translation.</title>
<date>1997</date>
<booktitle>In Procs. of the EuroSpeech’97,</booktitle>
<volume>1</volume>
<pages>91--94</pages>
<marker>Casta˜no, Casacuberta, 1997</marker>
<rawString>M. A. Casta˜no and F. Casacuberta. 1997. A connectionist approach to machine translation. In Procs. of the EuroSpeech’97, volume 1, pages 91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P P Cruz</author>
<author>E Vidal</author>
</authors>
<title>A study of grammatical inference algorithms in automatic music composition.</title>
<date>1997</date>
<booktitle>In Preprints of the VII National Symposium on 10In this experiment, Spanish word classes and English ones</booktitle>
<volume>1</volume>
<pages>43--48</pages>
<contexts>
<context position="9826" citStr="Cruz and Vidal, 1997" startWordPosition="1601" endWordPosition="1604">utomaton: &amp;quot;some snakes eat snakes&amp;quot; and &amp;quot;some people eat rats&amp;quot;. Thus, when this last sentence is actually presented to the algorithm, there is no need for the automaton to be updated. On the contrary, sentences 4 and 5 imply the addition of new elements and the finally inferred automaton is the one shown in subfigure d. Though successful application of ECGI to a variety of tasks has been reported,4 the method 4For instance, ECGI has been applied to problems as different as speech understanding (Prieto and Vidal, 1992), hand-written digit recognition (Vidal et al., 1995), and music composition (Cruz and Vidal, 1997) snakes eat rats some BEGIN END snakes eat some rats snakes people BEGIN END snakes eat rats snakes some people are dangerous BEGIN END snakes rats snakes eat some people are dangerous snakes BEGIN END Figure 2: An alternative automaton. suffers from some drawbacks. For instance, the level of generalization is sometimes lower than expected. In the example presented in Figure 1, when &amp;quot;snakes are dangerous&amp;quot; is employed for updating the model in subfigure c, instead of adding a new state and two arcs to the path corresponding to &amp;quot;some people are dangerous&amp;quot;, the solution in Figure 2 seems to be an</context>
</contexts>
<marker>Cruz, Vidal, 1997</marker>
<rawString>P. P. Cruz and E. Vidal. 1997. A study of grammatical inference algorithms in automatic music composition. In Preprints of the VII National Symposium on 10In this experiment, Spanish word classes and English ones were automatically extracted from the training pairs in order to increase ECGI generalization capabilities. Pattern Recognition and Image Analysis, volume 1, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Gopalakrishnan</author>
<author>D Kanevsky</author>
<author>A N´adas</author>
<author>D Nahamoo</author>
</authors>
<title>An inequality for rational functions with applications to some statistical problems.</title>
<date>1991</date>
<journal>IEEE Trans. on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<marker>Gopalakrishnan, Kanevsky, N´adas, Nahamoo, 1991</marker>
<rawString>P. S. Gopalakrishnan, D. Kanevsky, A. N´adas, and D. Nahamoo. 1991. An inequality for rational functions with applications to some statistical problems. IEEE Trans. on Information Theory, 37(1):107–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Martin</author>
<author>J Liermann</author>
<author>H Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering.</title>
<date>1995</date>
<booktitle>In Procs. of the EuroSpeech’95.</booktitle>
<contexts>
<context position="12476" citStr="Martin et al., 1995" startWordPosition="2038" endWordPosition="2041"> breaking ties, applies any dictionary-like ordering. Thus, we try to avoid the problem discussed in the previous paragraph by providing the inference algorithm with as much as possible structural information at first stages of automaton construction and, moreover, dictionary-like ordering inside each length is aimed at frequently presenting to ECGI new sentences that are similar to the previous ones. Furthermore, a very common way to reduce the complexity of problems involving languages is the definition of word categories, which can be manually designed or automatically extracted from data (Martin et al., 1995). We think categorization helps in solving the problem of undesired merges and also in increasing the generalization abilities of ECGI. In order to illustrate this point, let us consider a category animals consisting of words &amp;quot;snakes&amp;quot;, &amp;quot;rats&amp;quot; and &amp;quot;people&amp;quot; in the very simple example of Figure 1. Words can be substituted for the appropriate category in the original sentences; then, the modified sentences are presented to the inference algorithm; finally, categories in the automaton are expanded. Figure 3 shows the automata that are successively built in that process. As said at the beginning of </context>
</contexts>
<marker>Martin, Liermann, Ney, 1995</marker>
<rawString>S. Martin, J. Liermann, and H. Ney. 1995. Algorithms for bigram and trigram word clustering. In Procs. of the EuroSpeech’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Prat</author>
</authors>
<title>Traducci6n automdtica en dominios restringidos: Algunos modelos estocasticos susceptibles de ser aprendidos a partir de ejemplos.</title>
<date>1998</date>
<booktitle>Ph.D. thesis, Depto. de Sistemas Inform´aticos y Computaci´on, Universidad Polit´ecnica de</booktitle>
<location>Valencia</location>
<contexts>
<context position="13792" citStr="Prat, 1998" startWordPosition="2253" endWordPosition="2254">k. In addition, we will apply them a minimization process in order to simplify the problem that the corresponding association model will have to solve. 4 Loco C: Anew association model Following a data-driven approach, a Grammar Association system needs to learn from examples an association model capable to estimate the probabilities required by our recently developed framework, that is, the probability of each rule in the grammar that models the output language, conditioned on its left-hand side and the derivation of the input sentence. Among the different association models we have studied (Prat, 1998), it is worth emphasizing one we have specifically developed for playing that role in Grammar Association systems: the Loco C model. We based our design on the IBM models 1 and 2 (Brown et al., 1993), but taking into account that our model must generate correct derivations in a given grammar, not any sesome snakes eat rats snakes dangerous people are BEGIN END (a)&amp;quot;some animals eat animals &amp;quot; (b)&amp;quot;some animals are dangerous&amp;quot; Figure 3: Using a category animals for &amp;quot;snakes&amp;quot;, &amp;quot;rats&amp;quot; and &amp;quot;people&amp;quot; in the example of Figure 1. quence of rules.5 Moreover, we wanted to model the probability estimation for</context>
<context position="16959" citStr="Prat, 1998" startWordPosition="2794" endWordPosition="2795">tten, the probability of an input rule to be chosen in Choice 1 depends on parameters of the form and can be expressed as On the other hand, once a particular input rule is chosen, the probability of an output rule whose left-hand side is to be chosen in Choice 2 is directly given by a parameter of the form . Hence, takes in Loco C the form of a weighted mixture depending on two kinds of trainable parameters: : Measures the importance of in choosing an adequate rewriting rule for .8 7Full details on the discarded models, Loco 1, Loco A, and Loco B, can be found (in Spanish) in pages 52–60 of (Prat, 1998). 8Note that learning these parameters performs a sort of “automatic variable selection” of the input rules that are relevant for discriminatively choosing among the next applicable output rules. eat &lt;animals&gt; some &lt;animals&gt; BEGIN END eat &lt;animals&gt; some &lt;animals&gt; are dangerous BEGIN END (c) &amp;quot; animals are dangerous&amp;quot; some &lt;animals&gt; eat &lt;animals&gt; are dangerous BEGIN END (d) Expansion of animals some eat snakes rats people dangerous are snakes rats people BEGIN END MLA Task Spanish: &amp;quot;un circulo oscuro est´a encima de un circulo&amp;quot; English: &amp;quot;a dark circle is above a circle&amp;quot; Spanish: &amp;quot;se elimina el cu</context>
</contexts>
<marker>Prat, 1998</marker>
<rawString>F. Prat. 1998. Traducci6n automdtica en dominios restringidos: Algunos modelos estocasticos susceptibles de ser aprendidos a partir de ejemplos. Ph.D. thesis, Depto. de Sistemas Inform´aticos y Computaci´on, Universidad Polit´ecnica de Valencia (Spain).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Prieto</author>
<author>E Vidal</author>
</authors>
<date>1992</date>
<booktitle>Learning language models through the ECGI method. Speech Communication,</booktitle>
<pages>11--2</pages>
<contexts>
<context position="9727" citStr="Prieto and Vidal, 1992" startWordPosition="1587" endWordPosition="1590">ering the two first sentences (subfigure b), two more sentences are also represented in the current automaton: &amp;quot;some snakes eat snakes&amp;quot; and &amp;quot;some people eat rats&amp;quot;. Thus, when this last sentence is actually presented to the algorithm, there is no need for the automaton to be updated. On the contrary, sentences 4 and 5 imply the addition of new elements and the finally inferred automaton is the one shown in subfigure d. Though successful application of ECGI to a variety of tasks has been reported,4 the method 4For instance, ECGI has been applied to problems as different as speech understanding (Prieto and Vidal, 1992), hand-written digit recognition (Vidal et al., 1995), and music composition (Cruz and Vidal, 1997) snakes eat rats some BEGIN END snakes eat some rats snakes people BEGIN END snakes eat rats snakes some people are dangerous BEGIN END snakes rats snakes eat some people are dangerous snakes BEGIN END Figure 2: An alternative automaton. suffers from some drawbacks. For instance, the level of generalization is sometimes lower than expected. In the example presented in Figure 1, when &amp;quot;snakes are dangerous&amp;quot; is employed for updating the model in subfigure c, instead of adding a new state and two arc</context>
<context position="11632" citStr="Prieto and Vidal (1992)" startWordPosition="1906" endWordPosition="1909"> the state &amp;quot;snakes&amp;quot; of the initial model for representing the occurrence of that word in the second sentence, leading to an automaton which would recognize “sentences” as &amp;quot;some people eat snakes are dangerous&amp;quot;, or simply &amp;quot;snakes&amp;quot;. The situation that produces this kind of undesired behaviour of the method is characterized by the confluence of a couple of circumstances: a word in a new sentence is also present in the current model, but with a different function, and that automaton has not enough adequate structural information for offering a better merging to the new sentence. As pointed out by Prieto and Vidal (1992), a proper ordering of the set of sentences presented to ECGI can provide more compact models, and we think that better ones too. The ordering we propose here simply follows, first, a decreasinglength criterion and then, for breaking ties, applies any dictionary-like ordering. Thus, we try to avoid the problem discussed in the previous paragraph by providing the inference algorithm with as much as possible structural information at first stages of automaton construction and, moreover, dictionary-like ordering inside each length is aimed at frequently presenting to ECGI new sentences that are s</context>
</contexts>
<marker>Prieto, Vidal, 1992</marker>
<rawString>N. Prieto and E. Vidal. 1992. Learning language models through the ECGI method. Speech Communication, 11(2–3):299–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rulot</author>
<author>E Vidal</author>
</authors>
<title>Modelling (sub)stringlength based constraints through a grammatical inference method.</title>
<date>1987</date>
<booktitle>In Pattern Recognition Theory and Applications, volume F30 of NATO ASI,</booktitle>
<pages>451--459</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="7326" citStr="Rulot and Vidal, 1987" startWordPosition="1192" endWordPosition="1195">eatures. Finally, for the sake of homogeneity, we choose to force input grammar to be acyclic too. We can conclude this section saying that, inferring deterministic and acyclic finite-state automata, if we are able to learn association models for estimating, for each output rule, the probability of using that rule conditioned on having employed its left-hand side and the identity of the input derivation, then an efficient Dynamic Programming search for the optimal output derivation3 can be used in order to provide the most probable translation. 3 Using ECGI language models The ECGI algorithm (Rulot and Vidal, 1987) is a heuristic technique for the inference of acyclic finite-state automata from positive samples, and determinism can be imposed a posteriori by a well-known transformation for regular grammars. Therefore, in principle, ECGI provides exactly the kind of language model Grammar Association needs. Moreover, it was (without imposing determinism) the inference technique employed in (Vidal et al., 1993). Informally, ECGI works as follows. With the first sample sentence, it builds an initial automaton consisting in a linear path representing the sentence. Words label states (instead of arcs) and th</context>
</contexts>
<marker>Rulot, Vidal, 1987</marker>
<rawString>H. Rulot and E. Vidal. 1987. Modelling (sub)stringlength based constraints through a grammatical inference method. In Pattern Recognition Theory and Applications, volume F30 of NATO ASI, pages 451– 459. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Thollard</author>
<author>P Dupont</author>
<author>C de la Higuera</author>
</authors>
<title>Probabilistic DFA inference using Kullback-Leibler divergence and minimality.</title>
<date>2000</date>
<booktitle>In Procs. of the ICML’2000,</booktitle>
<pages>975--982</pages>
<contexts>
<context position="23995" citStr="Thollard et al., 2000" startWordPosition="3970" endWordPosition="3973"> into a rigorous statistical framework, clarifying which kind of probabilities have to be estimated by association models; third, by developing a novel and especially adequate association model: Loco C. On the other hand, though experimental results are quite good, we find them particularly relevant for pointing out directions to follow for further improvement of the Grammar Association technique. One of these directions consists in exploring better language models, refining the categorization methods employed in this work or substituting ECGI for some kind of merge-based inference algorithm (Thollard et al., 2000). Exploiting data-driven bilingual categorization (Barrachina and Vilar, 1999) is another promising way to improve the performance of our system. Finally, let us say that, obviously, the experimental results on simple artificial tasks presented in this work are not intended for convincing the reader that our Grammar Association systems could obtain similar performances on complex tasks as, for instance, the Hansards (the bilingual proceedings of the Canadian parliament). Our controlled experiments were mainly aimed at showing that our proposals improve Grammar Association, along with comparing</context>
</contexts>
<marker>Thollard, Dupont, Higuera, 2000</marker>
<rawString>F. Thollard, P. Dupont, and C. de la Higuera. 2000. Probabilistic DFA inference using Kullback-Leibler divergence and minimality. In Procs. of the ICML’2000, pages 975–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vidal</author>
<author>R Pieraccini</author>
<author>E Levin</author>
</authors>
<title>Learning associations between grammars: A new approach to natural language understanding.</title>
<date>1993</date>
<booktitle>In Procs. of the EuroSpeech’93,</booktitle>
<pages>1187--1190</pages>
<contexts>
<context position="3859" citStr="Vidal et al., 1993" startWordPosition="614" endWordPosition="618">airs without taking into account if the output sentence is well-formed.2 An alternative, direct statistical approach with a model for computing seems to require this single model to be complex enough to assign high scores only to pairs where the output sentence verifies two conditions: it is well-formed and means the same that the input one. Hence, for the sake of simplified modelling, Bayes’ decomposition has become a typical choice in Machine Translation. However, in the Grammar Association context, when developing (using Bayes’ decomposition) the basic equations of the system presented in (Vidal et al., 1993), it is said that the reverse model for “does not seem to admit a simple factorization which is also correct and convenient”, so “crude heuristics” were adopted in the mathematical development of the expression to be maximized. We are going to show that, by means of a direct modelling, Grammar Association can be set into a rigorous statistical framework without renouncing a convenient factorization for the search of the optimal translation to be efficient. Moreover, the main advantage of Bayes’ decomposition, modularity, is inherently present in Grammar Association systems: relations between i</context>
<context position="7728" citStr="Vidal et al., 1993" startWordPosition="1250" endWordPosition="1254">n, then an efficient Dynamic Programming search for the optimal output derivation3 can be used in order to provide the most probable translation. 3 Using ECGI language models The ECGI algorithm (Rulot and Vidal, 1987) is a heuristic technique for the inference of acyclic finite-state automata from positive samples, and determinism can be imposed a posteriori by a well-known transformation for regular grammars. Therefore, in principle, ECGI provides exactly the kind of language model Grammar Association needs. Moreover, it was (without imposing determinism) the inference technique employed in (Vidal et al., 1993). Informally, ECGI works as follows. With the first sample sentence, it builds an initial automaton consisting in a linear path representing the sentence. Words label states (instead of arcs) and there are two special non-labelled states: the initial one and the final one. For each new sentence, if it is already recognized by the automaton built so far, nothing happens; otherwise, if the current model does not recognize the sentence, new arcs and states are added to the most suitable path (according to a minimum-cost criterion) for recognition to be possible. In a sense, it is like constructin</context>
<context position="19541" citStr="Vidal et al., 1993" startWordPosition="3212" endWordPosition="3215">s in the Spanish vocabulary and in Table 1: Results of an English-to-Spanish translation experiment with the original Grammar Association system, using , pairs of the MLA Task for training and for testing. Sentence Minimum Length Correct Sorting Deterministic Constraint Translations No No No No No Yes No Yes No No Yes Yes Yes No No Yes No Yes Yes Yes No Yes Yes Yes the English one. Let us begin considering English-to-Spanish translation, with , pairs for training the systems and different ones for testing purposes. We carefully implemented the original Grammar Association system described in (Vidal et al., 1993), tuned empirically a couple of smoothing parameters, trained the models and, finally, obtained an of correct translations.9 Then, we studied the impact of: (1) sorting, as proposed in Section 3, the set of sentences presented to ECGI; (2) making language models deterministic and minimum; (3) constraining the best translation search to those sentences whose lengths have been seen, in the training set, related to the length of the input sentence. As shown in Table 1, all the proposed measures were beneficial and we got a final of correct translations (that is, just one translation was wrong). H</context>
<context position="23339" citStr="Vidal et al., 1993" startWordPosition="3869" endWordPosition="3872">t is worth saying that the best result obtained by ReConTra in the same scenario was . On the other hand, combining automatic bilingual categorization and Subsequential Transducers as described in (Barrachina and Vilar, 1999), a of correct translations can be achieved for an adequate choice of the number of word classes ( ), though only a is obtained by the same system in the absence of categorization. 6 Concluding remarks Our work presents a set of improvements on previous state of the art of Grammar Association: first, by providing better language models to the original system described in (Vidal et al., 1993); second, by setting the technique into a rigorous statistical framework, clarifying which kind of probabilities have to be estimated by association models; third, by developing a novel and especially adequate association model: Loco C. On the other hand, though experimental results are quite good, we find them particularly relevant for pointing out directions to follow for further improvement of the Grammar Association technique. One of these directions consists in exploring better language models, refining the categorization methods employed in this work or substituting ECGI for some kind of</context>
</contexts>
<marker>Vidal, Pieraccini, Levin, 1993</marker>
<rawString>E. Vidal, R. Pieraccini, and E. Levin. 1993. Learning associations between grammars: A new approach to natural language understanding. In Procs. of the EuroSpeech’93, pages 1187–1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Vidal</author>
<author>H Rulot</author>
<author>J M Valiente</author>
<author>G Andreu</author>
</authors>
<title>Application of the error-correcting grammatical inference algorithm (ECGI) to planar shape recognition.</title>
<date>1995</date>
<tech>Technical Report DSIC-II/2/95, Depto. de</tech>
<location>Valencia</location>
<contexts>
<context position="9780" citStr="Vidal et al., 1995" startWordPosition="1594" endWordPosition="1597">tences are also represented in the current automaton: &amp;quot;some snakes eat snakes&amp;quot; and &amp;quot;some people eat rats&amp;quot;. Thus, when this last sentence is actually presented to the algorithm, there is no need for the automaton to be updated. On the contrary, sentences 4 and 5 imply the addition of new elements and the finally inferred automaton is the one shown in subfigure d. Though successful application of ECGI to a variety of tasks has been reported,4 the method 4For instance, ECGI has been applied to problems as different as speech understanding (Prieto and Vidal, 1992), hand-written digit recognition (Vidal et al., 1995), and music composition (Cruz and Vidal, 1997) snakes eat rats some BEGIN END snakes eat some rats snakes people BEGIN END snakes eat rats snakes some people are dangerous BEGIN END snakes rats snakes eat some people are dangerous snakes BEGIN END Figure 2: An alternative automaton. suffers from some drawbacks. For instance, the level of generalization is sometimes lower than expected. In the example presented in Figure 1, when &amp;quot;snakes are dangerous&amp;quot; is employed for updating the model in subfigure c, instead of adding a new state and two arcs to the path corresponding to &amp;quot;some people are dange</context>
</contexts>
<marker>Vidal, Rulot, Valiente, Andreu, 1995</marker>
<rawString>E. Vidal, H. Rulot, J. M. Valiente, and G. Andreu. 1995. Application of the error-correcting grammatical inference algorithm (ECGI) to planar shape recognition. Technical Report DSIC-II/2/95, Depto. de Sistemas Inform´aticos y Computaci´on, Universidad Polit´ecnica de Valencia (Spain).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Vilar</author>
<author>A Marzal</author>
<author>E Vidal</author>
</authors>
<title>Learning language translation in limited domains using finitestate models: Some extensions and improvements.</title>
<date>1995</date>
<booktitle>In Procs. of the EuroSpeech’95,</booktitle>
<pages>1231--1234</pages>
<contexts>
<context position="25466" citStr="Vilar et al., 1995" startWordPosition="4209" endWordPosition="4212">mplies, for instance, trying to cope with severe data sparseness. In this regard, we are opa . In a second series of experiments, we wanted to compare our best system, Loco C, with ReConTra, the recurrent connectionist system described in (Casta˜no and Casacuberta, 1997), where a of correct translations is reported on the Spanish-to-English MLA Task with just , pairs for training. In the same conditions, Loco C got a of correct translations on a , pair test set (IOGA, just an . timistic: on one hand, because we trust in bilingual categorization for reducing the negative effects of sparseness (Vilar et al., 1995); on the other hand, because some additional experiments carried out with Grammar Association systems on the Spanish-to-English MLA Task with just pairs for training show acceptable results. For instance, our Loco C achieved an of correct translations10 while, in the same scenario, ReConTra performance drops to (Casta˜no and Casacuberta, 1997). Acknowledgements Most of the work presented here was carried out under the kind supervision of Dr. Francisco Casacuberta, and the author want to express his gratitude to him. Furthermore, it is worth saying that this work has been partially supported by</context>
</contexts>
<marker>Vilar, Marzal, Vidal, 1995</marker>
<rawString>J. M. Vilar, A. Marzal, and E. Vidal. 1995. Learning language translation in limited domains using finitestate models: Some extensions and improvements. In Procs. of the EuroSpeech’95, pages 1231–1234.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>