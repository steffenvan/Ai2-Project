<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000694">
<title confidence="0.969599">
HMM Word-to-Phrase Alignment with Dependency Constraints
</title>
<author confidence="0.999711">
Yanjun Ma Andy Way
</author>
<affiliation confidence="0.995355">
Centre for Next Generation Localisation
School of Computing
Dublin City University
</affiliation>
<email confidence="0.995584">
{yma,away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.993844" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953666666667">
In this paper, we extend the HMM word-
to-phrase alignment model with syntac-
tic dependency constraints. The syn-
tactic dependencies between multiple
words in one language are introduced
into the model in a bid to produce co-
herent alignments. Our experimental re-
sults on a variety of Chinese–English
data show that our syntactically con-
strained model can lead to as much as
a 3.24% relative improvement in BLEU
score over current HMM word-to-phrase
alignment models on a Phrase-Based
Statistical Machine Translation system
when the training data is small, and
a comparable performance compared to
IBM model 4 on a Hiero-style system
with larger training data. An intrin-
sic alignment quality evaluation shows
that our alignment model with depen-
dency constraints leads to improvements
in both precision (by 1.74% relative) and
recall (by 1.75% relative) over the model
without dependency information.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927590909091">
Generative word alignment models including
IBM models (Brown et al., 1993) and HMM
word alignment models (Vogel et al., 1996) have
been widely used in various types of Statisti-
cal Machine Translation (SMT) systems. This
widespread use can be attributed to their robust-
ness and high performance particularly on large-
scale translation tasks. However, the quality
of the alignment yielded from these models is
still far from satisfactory even with significant
amounts of training data; this is particularly true
for radically different languages such as Chinese
and English.
The weakness of most generative models of-
ten lies in the incapability of addressing one to
many (1-to-n), many to one (n-to-1) and many
to many (m-to-n) alignments. Some research di-
rectly addresses m-to-n alignment with phrase
alignment models (Marcu and Wong, 2002).
However, these models are unsuccessful largely
due to intractable estimation (DeNero and Klein,
2008). Recent progress in better parameteri-
sation and approximate inference (Blunsom et
al., 2009) can only augment the performance of
these models to a similar level as the baseline
where bidirectional word alignments are com-
bined with heuristics and subsequently used to
induce translation equivalence (e.g. (Koehn et
al., 2003)). The most widely used word align-
ment models, such as IBM models 3 and 4, can
only model 1-to-n alignment; these models are
often called “asymmetric” models. IBM models
3 and 4 model 1-to-n alignments using the notion
of “fertility”, which is associated with a “defi-
ciency” problem despite its high performance in
practice.
On the other hand, the HMM word-to-phrase
alignment model tackles 1-to-n alignment prob-
lems with simultaneous segmentation and align-
ment while maintaining the efficiency of the
models. Therefore, this model sets a good ex-
ample of addressing the tradeoffs between mod-
elling power and modelling complexity. This
model can also be seen as a more generalised
</bodyText>
<page confidence="0.984539">
101
</page>
<note confidence="0.981758">
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109,
COLING 2010, Beijing, August 2010.
</note>
<bodyText confidence="0.999256190476191">
case of the HMM word-to-word model (Vogel et
al., 1996; Och and Ney, 2003), since this model
can be reduced to an HMM word-to-word model
by restricting the generated target phrase length
to one. One can further refine existing word
alignment models with syntactic constraints (e.g.
(Cherry and Lin, 2006)). However, most re-
search focuses on the incorporation of syntactic
constraints into discriminative alignment mod-
els. Introducing syntactic information into gen-
erative alignment models is shown to be more
challenging mainly due to the absence of appro-
priate modelling of syntactic constraints and the
“inflexibility” of these generative models.
In this paper, we extend the HMM word-to-
phrase alignment model with syntactic depen-
dencies by presenting a model that can incor-
porate syntactic information while maintaining
the efficiency of the model. This model is based
on the observation that in 1-to-n alignments,
the n words bear some syntactic dependencies.
Leveraging such information in the model can
potentially further aid the model in producing
more fine-grained word alignments. The syn-
tactic constraints are specifically imposed on the
n words involved in 1-to-n alignments, which
is different from the cohesion constraints (Fox,
2002) as explored by Cherry and Lin (2006),
where knowledge of cross-lingual syntactic pro-
jection is used. As a syntactic extension of the
open-source MTTK implementation (Deng and
Byrne, 2006) of the HMM word-to-phrase align-
ment model, its source code will also be released
as open source in the near future.
The remainder of the paper is organised as fol-
lows. Section 2 describes the HMM word-to-
phrase alignment model. In section 3, we present
the details of the incorporation of syntactic de-
pendencies. Section 4 presents the experimental
setup, and section 5 reports the experimental re-
sults. In section 6, we draw our conclusions and
point out some avenues for future work.
</bodyText>
<sectionHeader confidence="0.959094" genericHeader="introduction">
2 HMM Word-to-Phrase Alignment
Model
</sectionHeader>
<bodyText confidence="0.999914896551724">
In HMM word-to-phrase alignment, a sentence
e is segmented into a sequence of consecutive
phrases: e = vK1 , where vk represents the kth
phrase in the target sentence. The assumption
that each phrase vk generated as a translation of
one single source word is consecutive is made to
allow efficient parameter estimation. Similarly
to word-to-word alignment models, a variable
aK 1 is introduced to indicate the correspondence
between the target phrase index and a source
word index: k -+ i = ak indicating a mapping
from a target phrase vk to a source word fak. A
random process φk is used to specify the num-
ber of words in each target phrase, subject to the
constraints J = EKk=1 φk, implying that the to-
tal number of words in the phrases agrees with
the target sentence length J.
The insertion of target phrases that do not cor-
respond to any source words is also modelled
by allowing a target phrase to be aligned to a
non-existent source word f0 (NULL). Formally,
to indicate whether each target phrase is aligned
to NULL or not, a set of indicator functions
εK 1 = {E1, · · · , εKI is introduced (Deng and
Byrne, 2008): if εk = 0, then NULL -+ vk; if
εk = 1, then fak -+ vk.
To summarise, an alignment a in an HMM
word-to-phrase alignment model consists of the
following elements:
</bodyText>
<equation confidence="0.990295">
a = (KK K K
, φ1 , a1 ε1 )
</equation>
<bodyText confidence="0.999952538461539">
The modelling objective is to define a condi-
tional distribution P(e, ajf) over these align-
ments. Following (Deng and Byrne, 2008),
P(e, ajf) can be decomposed into a phrase count
distribution (1) modelling the segmentation of a
target sentence into phrases (P(KjJ, f) a ηK
with scalar η to control the length of the hy-
pothesised phrases), a transition distribution (2)
modelling the dependencies between the current
link and the previous links, and a word-to-phrase
translation distribution (3) to model the degree
to which a word and a phrase are translational to
each other.
</bodyText>
<equation confidence="0.994286">
P(e, ajf) = P(vK1 , K, aK1 , εK1 , φK1 jf)
= P(KjJ,f) (1)
K K K
P(a1 , ε1 , φ1 jK, J, f) (2)
P(K K K K
v1 ja1 ,ε1 φ1 , K, J, f)(3)
</equation>
<page confidence="0.971923">
102
</page>
<listItem confidence="0.4648545">
The word-to-phrase translation distribution
(3) is formalised as in (4):
</listItem>
<equation confidence="0.998403">
P(K K K K
v1 |a1 ,ε1 ,φ1 , K, J, f)
K
=
k=1
</equation>
<bodyText confidence="0.999839444444444">
Note here that we assume that the translation
of each target phrase is conditionally indepen-
dent of other target phrases given the individual
source words.
If we assume that each word in a target phrase
is translated with a dependence on the previ-
ously translated word in the same phrase given
the source word, we derive the bigram transla-
tion model as follows:
</bodyText>
<equation confidence="0.99666175">
pv(vk|fak, εk, φk) = pt,(vk[1]|εk,fak)
Ok
ri pt2(vk[j]|vk[j − 1], εk, fak)
j=2
</equation>
<bodyText confidence="0.999970272727273">
where vk[1] is the first word in phrase vk, vk[j]
is the jth word in vk, pt, is an unigram transla-
tion probability and pt2 is a bigram translation
probability. The intuition is that the first word
in vk is firstly translated by fak and the transla-
tion of the remaining words vk[j] in vk from fak
is dependent on the translation of the previous
word vk[j − 1] from fak. The use of a bigram
translation model can address the coherence of
the words within the phrase vk so that the qual-
ity of phrase segmentation can be improved.
</bodyText>
<sectionHeader confidence="0.9724395" genericHeader="method">
3 Syntactically Constrained HMM
Word-to-Phrase Alignment Models
</sectionHeader>
<subsectionHeader confidence="0.9985395">
3.1 Syntactic Dependencies for
Word-to-Phrase Alignment
</subsectionHeader>
<bodyText confidence="0.999942888888889">
As a proof-of-concept, we performed depen-
dency parsing on the GALE gold-standard word
alignment corpus using Maltparser (Nivre et al.,
2007).1 We find that 82.54% of the consec-
utive English words have syntactic dependen-
cies and 77.46% non-consecutive English words
have syntactic dependencies in 1-to-2 Chinese–
English (ZH–EN) word alignment (one Chi-
nese word aligned to two English words). For
</bodyText>
<footnote confidence="0.910268">
1http://maltparser.org/
</footnote>
<bodyText confidence="0.999764666666667">
English–Chinese (EN–ZH) word alignment, we
observe that 75.62% of the consecutive Chinese
words and 71.15% of the non-consecutive Chi-
nese words have syntactic dependencies. Our
model represents an attempt to encode these lin-
guistic intuitions.
</bodyText>
<subsectionHeader confidence="0.999848">
3.2 Component Variables and Distributions
</subsectionHeader>
<bodyText confidence="0.999781307692308">
We constrain the word-to-phrase alignment
model with a syntactic coherence model. Given
a target phrase vk consisting of φk words, we
use the dependency label rk between words vk[1]
and vk[φk] to indicate the level of coherence.
The dependency labels are a closed set obtained
from dependency parsers, e.g. using Maltparser,
we have 20 dependency labels for English and
12 for Chinese in our data. Therefore, we have
an additional variable rK1 associated with the se-
quence of phrases vK1 to indicate the syntactic
coherence of each phrase, defining P(e, a|f) as
below:
</bodyText>
<equation confidence="0.99107425">
P(rK1 , vK1 , K, aK1 , εK1 , φK 1 |f) = P(K|J, f)
P(aK1 , φK1 , εK 1 |K, J, f)P(vK 1 |aK 1 , εK1 , φK1 , K, J, f)
P(K K K K K
r1 |a1 ,ε1 ,φ1 ,v1 , K, J, f) (5)
</equation>
<bodyText confidence="0.674826">
The syntactic coherence distribution (5) is
simplified as in (6):
</bodyText>
<equation confidence="0.972161">
pr(rk; ε, fak, φk) (6)
</equation>
<bodyText confidence="0.999908928571429">
Note that the coherence of each target phrase
is conditionally independent of the coherence of
other target phrases given the source words fak
and the number of words in the current phrase
φk. We name the model in (5) the SSH model.
SSH is an abbreviation of Syntactically con-
strained Segmental HMM, given the fact that
the HMM word-to-phrase alignment model is a
Segmental HMM model (SH) (Ostendorf et al.,
1996; Murphy, 2002).
As our syntactic coherence model utilises syn-
tactic dependencies which require the presence
of at least two words in target phrase vk, we
therefore model the cases of φk = 1 and φk &gt; 2
</bodyText>
<equation confidence="0.997856833333333">
pv(vk|εk - fak, φk) (4)
P(K K K K K
r1 |a1 ,ε1 ,φ1 ,v1 , K, J, f)
K
=
k=1
</equation>
<page confidence="0.656057">
103
</page>
<bodyText confidence="0.569212">
separately. We rewrite (6) as follows:
</bodyText>
<equation confidence="0.99792975">
pr(rk; ε, fak, φk) =
(
pφk=1(rk; ε, fak) if φk = 1
pφk≥2(rk; ε, fak) if φk &gt; 2
</equation>
<bodyText confidence="0.998023">
where pφk=1 defines the syntactic coherence
when the target phrase only contains one word
(φk = 1) and pφk≥2 defines the syntactic co-
herence of a target phrase composed of multiple
words (φk &gt; 2). We define pφk=1 as follows:
</bodyText>
<equation confidence="0.575566">
pφk=1(rk; ε, fak) a pn(φk = 1; ε, fak)
</equation>
<bodyText confidence="0.999871911764706">
where the coherence of the target phrase (word)
vk is defined to be proportional to the probability
of target phrase length φk = 1 given the source
word fak. The intuition behind this model is that
the syntactic coherence is strong iff the probabil-
ity of the source fak fertility φk = 1 is high.
For pφk≥2, which measures the syntactic co-
herence of a target phrase consisting of more
than two words, we use the dependency label rk
between words vk[1] and vk[φk] to indicate the
level of coherence. A distribution over the values
rk E R = {SBJ, ADJ, · · · } (R is the set of de-
pendency types for a specific language) is main-
tained as a table for each source word associated
with all the possible lengths φ E {2, · · · , N})
of the target phrase it can generate, e.g. we set
N = 4 for ZH–EN alignment and N = 2 for
EN–ZH alignment in our experiments.
Given a target phrase vk containing φk(φk &gt;
2) words, it is possible that there are no depen-
dencies between the first word vk[1] and the last
word vk[φk]. To account for this fact, we intro-
duce a indicator function ϕ as in below:
We can thereafter introduce a distribution pϕ(ϕ),
where pϕ(ϕ = 0) = ζ (0 &lt; ζ &lt; 1) and
pϕ(ϕ = 0) = 1− ζ, with ζ indicating how likely
it is that the first and final words in a target phrase
do not have any syntactic dependencies. We can
set ζ to a small number to favour target phrases
satisfying the syntactic constraints and to a larger
number otherwise. The introduction of this vari-
able enables us to tune the model towards our
different end goals. We can now define pφk≥2
as:
</bodyText>
<equation confidence="0.82765">
pφk≥2(rk; ε, fak) = p(rk|ϕ; ε, fak )pϕ(ϕ)
</equation>
<bodyText confidence="0.999924333333333">
where we insist that p(rk|ϕ; ε, fak) = 1 if
ϕ = 0 (the first and last words in the target
phrase do not have syntactic dependencies) to
reflect the fact that in most arbitrary consecu-
tive word sequences the first and last words do
not have syntactic dependencies, and otherwise
p(rk|ϕ; ε, fak) if ϕ = 1 (the first and last words
in the target phrase have syntactic dependen-
cies).
</bodyText>
<subsectionHeader confidence="0.98893">
3.3 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.999937714285714">
The Forward-Backward Algorithm (Baum,
1972), a version of the EM algorithm (Dempster
et al., 1977), is specifically designed for unsu-
pervised parameter estimation of HMM models.
The Forward statistic αj(i, φ, ε) in our model
can be calculated recursively over the trellis as
follows:
</bodyText>
<equation confidence="0.998644375">
X αj−φ(i′, φ′, ε′)pa(i|i′, ε; I)}
αj(i, φ, ε) = {
φ
i ,′ ,ε
pn(φ; ε, fi)ηpt1(ej−φ+1|ε, fi)
j
Y pt2(ej′|ej′−1, ε, fi)pr(rk; ε, fi, φ)
j′=j−φ+2
</equation>
<bodyText confidence="0.99889">
which sums up the probabilities of every path
that could lead to the cell (j, i, φ). Note that the
syntactic coherence term pr(rk; ε, fi, φ) can ef-
ficiently be added into the Forward procedure.
Similarly, the Backward statistic βj(i, φ, ε) is
calculated over the trellis as below:
</bodyText>
<equation confidence="0.995673833333333">
βj(i, φ, ε) = X βj+φ′(i′, φ′, ε′)pa(i′|i, h′; I)
i′,φ′,ε′
pn(φ′; ε′, fi′)ηpt1(ej+1|ε′, fi′)
j+φ′
Y pt2(ej′|ej′−1, ε′, fi′)pr(rk; ε′, fi′, φ′)
j′=j+2
</equation>
<bodyText confidence="0.883153333333333">
Note also that the syntactic coherence term
pr(rk; ε′, fi′, φ′) can be integrated into the Back-
ward procedure efficiently.
</bodyText>
<equation confidence="0.545434">
ϕ(vk[1],φk) =  1 ifvk[1] and vk[φk]have
</equation>
<table confidence="0.866543">
 syntactic dependencies
 0 otherwise
</table>
<page confidence="0.990662">
104
</page>
<bodyText confidence="0.996881">
Posterior probability can be calculated based
on the Forward and Backward probabilities.
</bodyText>
<subsectionHeader confidence="0.983286">
3.4 EM Parameter Updates
</subsectionHeader>
<bodyText confidence="0.999911714285714">
The Expectation step accumulates fractional
counts using the posterior probabilities for each
parameter during the Forward-Backward passes,
and the Maximisation step normalises the counts
in order to generate updated parameters.
The E-step for the syntactic coherence model
proceeds as follows:
</bodyText>
<equation confidence="0.980312">
1: c(r′; f, φ′) = 1: γj(i, φ, ε = 1)
(f,e)ET i,j,φ,fi=f
δ(φ, φ′)δ(ϕj(e, φ), r′)
</equation>
<bodyText confidence="0.9323695">
where γj(i, φ, ε) is the posterior probability that
a target phrase tjj_φ+1 is aligned to source word
fi, and ϕj(e, φ) is the syntactic dependency label
between ej_φ+1 and ej. The M-step performs
normalisation, as below:
Er c(r; f, φ′)
Other component parameters can be estimated
in a similar manner.
</bodyText>
<sectionHeader confidence="0.998999" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.901858">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999895">
We built the baseline word alignment and
Phrase-Based SMT (PB-SMT) systems using ex-
isting open-source toolkits for the purposes of
fair comparison. A collection of GALE data
(LDC2006E26) consisting of 103K (2.9 million
English running words) sentence pairs was firstly
used as a proof of concept (“small”), and FBIS
data containing 238K sentence pairs (8 million
English running words) was added to construct a
“medium” scale experiment. To investigate the
intrinsic quality of the alignment, a collection
of parallel sentences (12K sentence pairs) for
which we have manually annotated word align-
ment was added to both “small” and “medium”
scale experiments. Multiple-Translation Chinese
Part 1 (MTC1) from LDC was used for Mini-
mum Error-Rate Training (MERT) (Och, 2003),
and MTC2, 3 and 4 were used as development
test sets. Finally the test set from NIST 2006
evaluation campaign was used as the final test
set.
The Chinese data was segmented using the
LDC word segmenter. The maximum-entropy-
based POS tagger MXPOST (Ratnaparkhi, 1996)
was used to tag both English and Chinese texts.
The syntactic dependencies for both English and
Chinese were obtained using the state-of-the-art
Maltparser dependency parser, which achieved
84% and 88% labelled attachment scores for
Chinese and English respectively.
</bodyText>
<subsectionHeader confidence="0.992038">
4.2 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999984827586207">
The GIZA++ (Och and Ney, 2003) implementa-
tion of IBM Model 4 (Brown et al., 1993) is used
as the baseline for word alignment. Model 4 is
incrementally trained by performing 5 iterations
of Model 1, 5 iterations of HMM, 3 iterations
of Model 3, and 3 iterations of Model 4. We
compared our model against the MTTK (Deng
and Byrne, 2006) implementation of the HMM
word-to-phrase alignment model. The model
training includes 10 iterations of Model 1, 5 it-
erations of Model 2, 5 iterations of HMM word-
to-word alignment, 20 iterations (5 iterations re-
spectively for phrase lengths 2, 3 and 4 with un-
igram translation probability, and phrase length
4 with bigram translation probability) of HMM
word-to-phrase alignment for ZH–EN alignment
and 5 iterations (5 iterations for phrase length
2 with uniform translation probability) of HMM
word-to-phrase alignment for EN–ZH. This con-
figuration is empirically established as the best
for Chinese–English word alignment. To allow
for a fair comparison between IBM Model 4
and HMM word-to-phrase alignment models, we
also restrict the maximum fertility in IBM model
4 to 4 for ZH–EN and 2 for EN–ZH (the default
is 9 in GIZA++ for both ZH–EN and EN–ZH).
“grow-diag-final” heuristic described in (Koehn
et al., 2003) is used to derive the refined align-
ment from bidirectional alignments.
</bodyText>
<subsectionHeader confidence="0.998248">
4.3 MT system
</subsectionHeader>
<bodyText confidence="0.999860333333333">
The baseline in our experiments is a standard
log-linear PB-SMT system. With the word align-
ment obtained using the method described in
</bodyText>
<equation confidence="0.995056">
pr(r′;f,φ′) = c(r′;f, φ′)
</equation>
<page confidence="0.979396">
105
</page>
<bodyText confidence="0.957521642857143">
section 4.2, we perform phrase-extraction using PB-SMT Hiero
heuristics described in (Koehn et al., 2003), Min-
small medium small medium
H 0.1440 0.2591 0.1373 0.2595
imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609
optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624
model with Kneser-Ney smoothing (Kneser and M4 0.1566 0.2627 0.1486 0.2660
Ney, 1995) trained with SRILM (Stolcke, 2002) Table 1: Performance of PB-SMT using different
on the English side of the training data, and alignment models on NIST06 test set
MOSES (Koehn et al., 2007) for decoding. A
Hiero-style decoder Joshua (Li et al., 2009) is
also used in our experiments. All significance
tests are performed using approximate randomi-
sation (Noreen, 1989) at p = 0.05.
</bodyText>
<sectionHeader confidence="0.995229" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.705559">
5.1 Alignment Model Tuning
</subsectionHeader>
<bodyText confidence="0.999563944444444">
In order to find the value of C in the SSH model
that yields the best MT performance, we used
three development test sets using a PB-SMT sys-
tem trained on the small data condition. Figure 1
shows the results on each development test set
using different configurations of the alignment
models. For each system, we obtain the mean
of the BLEU scores (Papineni et al., 2002) on
the three development test sets, and derive the
optimal value for C of 0.4, which we use here-
after for final testing. It is worth mentioning
that while IBM model 4 (M4) outperforms other
models including the HMM word-to-word (H)
and word-to-phrase (SH) alignment model in our
current setup, using the default IBM model 4 set-
ting (maximum fertility 9) yields an inferior per-
formance (as much as 8.5% relative) compared
to other models.
</bodyText>
<figure confidence="0.622962">
alignment systems
</figure>
<figureCaption confidence="0.994448">
Figure 1: BLEU score on development test set
using PB-SMT system
</figureCaption>
<subsectionHeader confidence="0.999017">
5.2 Translation Results
</subsectionHeader>
<bodyText confidence="0.999939552631579">
Table 1 shows the performance of PB-SMT and
Hiero systems using a small amount of data for
alignment model training on the NIST06 test set.
For the PB-SMT system trained on the small data
set, using SSH word alignment leads to a 3.24%
relative improvement over SH, which is statis-
tically significant. SSH also leads to a slight
gain over the HMM word-to-word alignment
model (H). However, when the PB-SMT system
is trained on larger data sets, there are no sig-
nificant differences between SH and SSH. Addi-
tionally, both SH and SSH models underperform
H on the medium data condition, indicating that
the performance of the alignment model tuned
on the PB-SMT system with small training data
does not carry over to PB-SMT systems with
larger training data (cf. Figure 1). IBM model
4 demonstrates stronger performance over other
models for both small and medium data condi-
tions.
For the Hiero system trained on a small data
set, no significant differences are observed be-
tween SSH, SH and H. On a larger training set,
we observe that SSH alignment leads to better
performance compared to SH. Both SH and SSH
alignments achieved higher translation quality
than H. Note that while IBM model 4 outper-
forms other models on a small data condition, the
difference between IBM model 4 and SSH is not
statistically significant on a medium data condi-
tion. It is also worth pointing out that the SSH
model yields significant improvement over IBM
model 4 with the default fertility setting, indicat-
ing that varying the fertility limit in IBM model
4 has a significant impact on translation quality.
In summary, the SSH model which incorpo-
rates syntactic dependencies into the SH model
achieves consistently better performance than
</bodyText>
<figure confidence="0.993331416666667">
BLEU score
0.135
0.125
0.115
0.14
0.13
0.12
0.
11
MTC2
MTC3
MTC4
</figure>
<page confidence="0.986742">
106
</page>
<table confidence="0.999032833333333">
ZH–EN EN–ZH
P R P R
H 0.5306 0.3752 0.5282 0.3014
SH 0.5378 0.3802 0.5523 0.3151
SSH 0.5384 0.3807 0.5619 0.3206
M4 0.5638 0.3986 0.5988 0.3416
</table>
<tableCaption confidence="0.9441415">
Table 2: Intrinsic evaluation of the alignment us-
ing different alignment models
</tableCaption>
<bodyText confidence="0.998328285714286">
SH in both PB-SMT and Hiero systems under
both small and large data conditions. For a
PB-SMT system trained on the small data set,
the SSH model leads to significant gains over
the baseline SH model. The results also en-
tail an observation concerning the suitability of
different alignment models for different types
of SMT systems; trained on a large data set,
our SSH alignment model is more suitable to
a Hiero-style system than a PB-SMT system,
as evidenced by a lower performance compared
to IBM model 4 using a PB-SMT system, and
a comparable performance compared to IBM
model 4 using a Hiero system.
</bodyText>
<subsectionHeader confidence="0.988227">
5.3 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999964421052631">
In order to further investigate the intrinsic qual-
ity of the word alignment, we compute the Preci-
sion (P), Recall (R) and F-score (F) of the align-
ments obtained using different alignment mod-
els. As the models investigated here are asym-
metric models, we conducted intrinsic evalua-
tion for both alignment directions, i.e. ZH–EN
word alignment where one Chinese word can be
aligned to multiple English words, and EN–ZH
word alignment where one English word can be
aligned to multiple Chinese words.
Table 2 shows the results of the intrinsic eval-
uation of ZH–EN and EN–ZH word alignment
on a small data set (results on the medium data
set follow the same trend but are left out due
to space limitations). Note that the P and R
are all quite low, demonstrating the difficulty of
Chinese–English word alignment in the news do-
main. For the ZH–EN direction, using the SSH
model does not lead to significant gains over SH
in P or R. For the EN–ZH direction, the SSH
model leads to a 1.74% relative improvement in
P, and a 1.75% relative improvement in R over
the SH model. Both SH and SSH lead to gains
over H for both ZH–EN and EN–ZH directions,
while gains in the EN–ZH direction appear to be
more pronounced. IBM model 4 achieves signif-
icantly higher P over other models while the gap
in R is narrow.
Relating Table 2 to Table 1, we observe that
the HMM word-to-word alignment model (H)
can still achieve good MT performance despite
the lower P and R compared to other mod-
els. This provides additional support to previ-
ous findings (Fraser and Marcu, 2007b) that the
intrinsic quality of word alignment does not nec-
essarily correlate with the performance of the re-
sulted MT system.
</bodyText>
<subsectionHeader confidence="0.996831">
5.4 Alignment Characteristics
</subsectionHeader>
<bodyText confidence="0.9999866">
In order to further understand the characteristics
of the alignment that each model produces, we
investigated several statistics of the alignment re-
sults which can hopefully reveal the capabilities
and limitations of each model.
</bodyText>
<subsectionHeader confidence="0.656747">
5.4.1 Pairwise Comparison
</subsectionHeader>
<bodyText confidence="0.999991826086956">
Given the asymmetric property of these align-
ment models, we can evaluate the quality of the
links for each word and compare the alignment
links across different models. For example, in
ZH–EN word alignment, we can compute the
links for each Chinese word and compare those
links across different models. Additionally, we
can compute the pairwise agreement in align-
ing each Chinese word for any two alignment
models. Similarly, we can compute the pairwise
agreement in aligning each English word in the
EN–ZH alignment direction.
For ZH–EN word alignment, we observe that
the SH and SSH models reach a 85.94% agree-
ment, which is not surprising given the fact that
SSH is a syntactic extension over SH, while IBM
model 4 and SSH reach the smallest agreement
(only 65.09%). We also observe that there is a
higher agreement between SSH and H (76.64%)
than IBM model 4 and H (69.58%). This can be
attributed to the fact that SSH is still a form of
HMM model while IBM model 4 is not. A simi-
lar trend is observed for EN–ZH word alignment.
</bodyText>
<page confidence="0.991851">
107
</page>
<table confidence="0.998585">
ZH–EN EN–ZH
1-to-0 1-to-1 1-to-n 1-to-0 1-to-1 1-to-n
con. non-con. con. non-con.
HMM 0.3774 0.4693 0.0709 0.0824 0.4438 0.4243 0.0648 0.0671
SH 0.3533 0.4898 0.0843 0.0726 0.4095 0.4597 0.0491 0.0817
SSH 0.3613 0.5092 0.0624 0.0671 0.3990 0.4835 0.0302 0.0872
M4 0.2666 0.5561 0.0985 0.0788 0.3967 0.4850 0.0592 0.0591
</table>
<tableCaption confidence="0.99879">
Table 3: Alignment types using different alignment models
</tableCaption>
<subsectionHeader confidence="0.746356">
5.4.2 Alignment Types
</subsectionHeader>
<bodyText confidence="0.995712102564102">
Again, by taking advantage of the asymmet-
ric property of these alignment models, we can
compute different types of alignment. For both
ZH–EN (EN–ZH) alignment, we divide the links
for each Chinese (English) word into 1-to-0
where each Chinese (English) word is aligned
to the empty word “NULL” in English (Chi-
nese), 1-to-1 where each Chinese (English) word
is aligned to only one word in English (Chinese),
and 1-to-n where each Chinese (English) word
is aligned to n (n ≥ 2) words in English (Chi-
nese). For 1-to-n links, depending on whether
the n words are consecutive, we have consecu-
tive (con.) and non-consecutive (non-con.) 1-to-
n links.
Table 3 shows the alignment types in the
medium data track. We can observe that for
ZH–EN word alignment, both SH and SSH pro-
duce far more 1-to-0 links than Model 4. It can
also be seen that Model 4 tends to produce more
consecutive 1-to-n links than non-consecutive 1-
to-n links. On the other hand, the SSH model
tends to produce more non-consecutive 1-to-n
links than consecutive ones. Compared to SH,
SSH tends to produce more 1-to-1 links than 1-
to-n links, indicating that adding syntactic de-
pendency constraints biases the model towards
only producing 1-to-n links when the n words
follow coherence constraint, i.e. the first and last
word in the chunk have syntactic dependencies.
For example, among the 6.24% consecutive ZH–
EN 1-to-n links produced by SSH, 43.22% of
them follow the coherence constraint compared
to just 39.89% in SH. These properties can have
significant implications for the performance of
our MT systems given that we use the grow-
diag-final heuristics to derive the symmetrised
word alignment based on bidirectional asymmet-
ric word alignments.
</bodyText>
<sectionHeader confidence="0.998035" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999973555555555">
In this paper, we extended the HMM word-to-
phrase word alignment model to handle syntac-
tic dependencies. We found that our model was
consistently better than that without syntactic de-
pendencies according to both intrinsic and ex-
trinsic evaluation. Our model is shown to be ben-
eficial to PB-SMT under a small data condition
and to a Hiero-style system under a larger data
condition.
As to future work, we firstly plan to investi-
gate the impact of parsing quality on our model,
and the use of different heuristics to combine
word alignments. Secondly, the syntactic co-
herence model itself is very simple, in that it
only covers the syntactic dependency between
the first and last word in a phrase. Accordingly,
we intend to extend this model to cover more so-
phisticated syntactic relations within the phrase.
Furthermore, given that we can construct dif-
ferent MT systems using different word align-
ments, multiple system combination can be con-
ducted to avail of the advantages of different sys-
tems. We also plan to compare our model with
other alignment models, e.g. (Fraser and Marcu,
2007a), and test this approach on more data and
on different language pairs and translation direc-
tions.
</bodyText>
<sectionHeader confidence="0.996525" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998635">
This research is supported by the Science Foundation Ire-
land (Grant 07/CE/I1142) as part of the Centre for Next
Generation Localisation (www.cngl.ie) at Dublin City Uni-
versity. Part of the work was carried out at Cambridge Uni-
versity Engineering Department with Dr. William Byrne.
The authors would also like to thank the anonymous re-
viewers for their insightful comments.
</bodyText>
<page confidence="0.998789">
108
</page>
<sectionHeader confidence="0.990314" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999948931372548">
Baum, Leonard E. 1972. An inequality and associ-
ated maximization technique in statistical estimation for
probabilistic functions of Markov processes. Inequali-
ties, 3:1–8.
Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proceedings of ACL-IJCNLP
2009, pages 782–790, Singapore.
Brown, Peter F., Stephen A. Della-Pietra, Vincent J. Della-
Pietra, and Robert L. Mercer. 1993. The mathematics of
Statistical Machine Translation: Parameter estimation.
Computational Linguistics, 19(2):263–311.
Cherry, Colin and Dekang Lin. 2006. Soft syntactic con-
straints for word alignment through discriminative train-
ing. In Proceedings of the COLING-ACL 2006, pages
105–112, Sydney, Australia.
Dempster, Arthur, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1–38.
DeNero, John and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings ofACL-08:
HLT, Short Papers, pages 25–28, Columbus, OH.
Deng, Yonggang and William Byrne. 2006. MTTK: An
alignment toolkit for Statistical Machine Translation. In
Proceedings ofHLT-NAACL 2006, pages 265–268, New
York City, NY.
Deng, Yonggang and William Byrne. 2008. HMM word
and phrase alignment for Statistical Machine Transla-
tion. IEEE Transactions on Audio, Speech, and Lan-
guage Processing, 16(3):494–507.
Fox, Heidi. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of the EMNLP 2002, pages
304–3111, Philadelphia, PA, July.
Fraser, Alexander and Daniel Marcu. 2007a. Getting the
structure right for word alignment: LEAF. In Pro-
ceedings ofEMNLP-CoNLL 2007, pages 51–60, Prague,
Czech Republic.
Fraser, Alexander and Daniel Marcu. 2007b. Measuring
word alignment quality for Statistical Machine Transla-
tion. Computational Linguistics, 33(3):293–303.
Kneser, Reinhard and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE ICASSP, volume 1, pages 181–
184, Detroit, MI.
Koehn, Philipp, Franz Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings
of HLT-NAACL 2003, pages 48–54, Edmonton, AB,
Canada.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit for
Statistical Machine Translation. In Proceedings ofACL
2007, pages 177–180, Prague, Czech Republic.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Sanjeev
Khudanpur, Lane Schwartz, Wren Thornton, Jonathan
Weese, and Omar Zaidan. 2009. Joshua: An open
source toolkit for parsing-based machine translation. In
Proceedings of the WMT 2009, pages 135–139, Athens,
Greece.
Marcu, Daniel and William Wong. 2002. A Phrase-Based,
joint probability model for Statistical Machine Transla-
tion. In Proceedings of EMNLP 2002, pages 133–139,
Philadelphia, PA.
Murphy, Kevin. 2002. Hidden semi-markov models (seg-
ment models). Technical report, UC Berkeley.
Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev,
G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov,
and Ervin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency parsing.
Natural Language Engineering, 13(2):95–135.
Noreen, Eric W. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience, New York, NY.
Och, Franz and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Compu-
tational Linguistics, 29(1):19–51.
Och, Franz. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In Proceedings ofACL 2003,
pages 160–167, Sapporo, Japan.
Ostendorf, Mari, Vassilios V. Digalakis, and Owen A. Kim-
ball. 1996. From HMMs to segment models: A uni-
fied view of stochastic modeling for speech recognition.
IEEE Transactions on Speech and Audio Processing,
4(5):360–378.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of ACL
2002, pages 311–318, Philadelphia, PA.
Ratnaparkhi, Adwait. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133–142, Somerset, NJ.
Stolcke, Andreas. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, CO.
Vogel, Stefan, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of ACL 1996, pages 836–841,
Copenhagen, Denmark.
</reference>
<page confidence="0.998961">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.886088">
<title confidence="0.999843">HMM Word-to-Phrase Alignment with Dependency Constraints</title>
<author confidence="0.999699">Yanjun Ma Andy</author>
<affiliation confidence="0.963852">Centre for Next Generation School of Dublin City</affiliation>
<abstract confidence="0.99959196">In this paper, we extend the HMM wordto-phrase alignment model with syntactic dependency constraints. The syntactic dependencies between multiple words in one language are introduced into the model in a bid to produce coherent alignments. Our experimental results on a variety of Chinese–English data show that our syntactically constrained model can lead to as much as a 3.24% relative improvement in BLEU score over current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data. An intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<volume>3</volume>
<contexts>
<context position="13031" citStr="Baum, 1972" startWordPosition="2246" endWordPosition="2247"> introduction of this variable enables us to tune the model towards our different end goals. We can now define pφk≥2 as: pφk≥2(rk; ε, fak) = p(rk|ϕ; ε, fak )pϕ(ϕ) where we insist that p(rk|ϕ; ε, fak) = 1 if ϕ = 0 (the first and last words in the target phrase do not have syntactic dependencies) to reflect the fact that in most arbitrary consecutive word sequences the first and last words do not have syntactic dependencies, and otherwise p(rk|ϕ; ε, fak) if ϕ = 1 (the first and last words in the target phrase have syntactic dependencies). 3.3 Parameter Estimation The Forward-Backward Algorithm (Baum, 1972), a version of the EM algorithm (Dempster et al., 1977), is specifically designed for unsupervised parameter estimation of HMM models. The Forward statistic αj(i, φ, ε) in our model can be calculated recursively over the trellis as follows: X αj−φ(i′, φ′, ε′)pa(i|i′, ε; I)} αj(i, φ, ε) = { φ i ,′ ,ε pn(φ; ε, fi)ηpt1(ej−φ+1|ε, fi) j Y pt2(ej′|ej′−1, ε, fi)pr(rk; ε, fi, φ) j′=j−φ+2 which sums up the probabilities of every path that could lead to the cell (j, i, φ). Note that the syntactic coherence term pr(rk; ε, fi, φ) can efficiently be added into the Forward procedure. Similarly, the Backward</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, Leonard E. 1972. An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. Inequalities, 3:1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP 2009,</booktitle>
<pages>782--790</pages>
<contexts>
<context position="2140" citStr="Blunsom et al., 2009" startWordPosition="319" endWordPosition="322">far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3 and 4 model 1-to-n alignments using the notion of “fertility”, which is associated with a “deficiency” problem despite its high performance in practice. On the other hand, the HMM word-to-phrase a</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Blunsom, Phil, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A gibbs sampler for phrasal synchronous grammar induction. In Proceedings of ACL-IJCNLP 2009, pages 782–790, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della-Pietra</author>
<author>Vincent J DellaPietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of Statistical Machine Translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1180" citStr="Brown et al., 1993" startWordPosition="172" endWordPosition="175">h as a 3.24% relative improvement in BLEU score over current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data. An intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information. 1 Introduction Generative word alignment models including IBM models (Brown et al., 1993) and HMM word alignment models (Vogel et al., 1996) have been widely used in various types of Statistical Machine Translation (SMT) systems. This widespread use can be attributed to their robustness and high performance particularly on largescale translation tasks. However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-</context>
<context position="16278" citStr="Brown et al., 1993" startWordPosition="2768" endWordPosition="2771">d 4 were used as development test sets. Finally the test set from NIST 2006 evaluation campaign was used as the final test set. The Chinese data was segmented using the LDC word segmenter. The maximum-entropybased POS tagger MXPOST (Ratnaparkhi, 1996) was used to tag both English and Chinese texts. The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.2 Word Alignment The GIZA++ (Och and Ney, 2003) implementation of IBM Model 4 (Brown et al., 1993) is used as the baseline for word alignment. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. We compared our model against the MTTK (Deng and Byrne, 2006) implementation of the HMM word-to-phrase alignment model. The model training includes 10 iterations of Model 1, 5 iterations of Model 2, 5 iterations of HMM wordto-word alignment, 20 iterations (5 iterations respectively for phrase lengths 2, 3 and 4 with unigram translation probability, and phrase length 4 with bigram translation probability) </context>
</contexts>
<marker>Brown, Della-Pietra, DellaPietra, Mercer, 1993</marker>
<rawString>Brown, Peter F., Stephen A. Della-Pietra, Vincent J. DellaPietra, and Robert L. Mercer. 1993. The mathematics of Statistical Machine Translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Soft syntactic constraints for word alignment through discriminative training.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL</booktitle>
<pages>105--112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3501" citStr="Cherry and Lin, 2006" startWordPosition="535" endWordPosition="538">. Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity. This model can also be seen as a more generalised 101 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109, COLING 2010, Beijing, August 2010. case of the HMM word-to-word model (Vogel et al., 1996; Och and Ney, 2003), since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models. Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the “inflexibility” of these generative models. In this paper, we extend the HMM word-tophrase alignment model with syntactic dependencies by presenting a model that can incorporate syntactic information while maintaining the efficiency of the model. This model is based on the observation that in 1-to-</context>
</contexts>
<marker>Cherry, Lin, 2006</marker>
<rawString>Cherry, Colin and Dekang Lin. 2006. Soft syntactic constraints for word alignment through discriminative training. In Proceedings of the COLING-ACL 2006, pages 105–112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Dempster</author>
<author>Nan Laird</author>
<author>Donald Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="13086" citStr="Dempster et al., 1977" startWordPosition="2254" endWordPosition="2257"> tune the model towards our different end goals. We can now define pφk≥2 as: pφk≥2(rk; ε, fak) = p(rk|ϕ; ε, fak )pϕ(ϕ) where we insist that p(rk|ϕ; ε, fak) = 1 if ϕ = 0 (the first and last words in the target phrase do not have syntactic dependencies) to reflect the fact that in most arbitrary consecutive word sequences the first and last words do not have syntactic dependencies, and otherwise p(rk|ϕ; ε, fak) if ϕ = 1 (the first and last words in the target phrase have syntactic dependencies). 3.3 Parameter Estimation The Forward-Backward Algorithm (Baum, 1972), a version of the EM algorithm (Dempster et al., 1977), is specifically designed for unsupervised parameter estimation of HMM models. The Forward statistic αj(i, φ, ε) in our model can be calculated recursively over the trellis as follows: X αj−φ(i′, φ′, ε′)pa(i|i′, ε; I)} αj(i, φ, ε) = { φ i ,′ ,ε pn(φ; ε, fi)ηpt1(ej−φ+1|ε, fi) j Y pt2(ej′|ej′−1, ε, fi)pr(rk; ε, fi, φ) j′=j−φ+2 which sums up the probabilities of every path that could lead to the cell (j, i, φ). Note that the syntactic coherence term pr(rk; ε, fi, φ) can efficiently be added into the Forward procedure. Similarly, the Backward statistic βj(i, φ, ε) is calculated over the trellis a</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, Arthur, Nan Laird, and Donald Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<pages>25--28</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2047" citStr="DeNero and Klein, 2008" startWordPosition="306" endWordPosition="309">le translation tasks. However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3 and 4 model 1-to-n alignments using the notion of “fertility”, which is associated with a “deficiency” </context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>DeNero, John and Dan Klein. 2008. The complexity of phrase alignment problems. In Proceedings ofACL-08: HLT, Short Papers, pages 25–28, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>MTTK: An alignment toolkit for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofHLT-NAACL 2006,</booktitle>
<pages>265--268</pages>
<location>New York City, NY.</location>
<contexts>
<context position="4633" citStr="Deng and Byrne, 2006" startWordPosition="705" endWordPosition="708">taining the efficiency of the model. This model is based on the observation that in 1-to-n alignments, the n words bear some syntactic dependencies. Leveraging such information in the model can potentially further aid the model in producing more fine-grained word alignments. The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. As a syntactic extension of the open-source MTTK implementation (Deng and Byrne, 2006) of the HMM word-to-phrase alignment model, its source code will also be released as open source in the near future. The remainder of the paper is organised as follows. Section 2 describes the HMM word-tophrase alignment model. In section 3, we present the details of the incorporation of syntactic dependencies. Section 4 presents the experimental setup, and section 5 reports the experimental results. In section 6, we draw our conclusions and point out some avenues for future work. 2 HMM Word-to-Phrase Alignment Model In HMM word-to-phrase alignment, a sentence e is segmented into a sequence of</context>
<context position="16531" citStr="Deng and Byrne, 2006" startWordPosition="2814" endWordPosition="2817"> was used to tag both English and Chinese texts. The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.2 Word Alignment The GIZA++ (Och and Ney, 2003) implementation of IBM Model 4 (Brown et al., 1993) is used as the baseline for word alignment. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. We compared our model against the MTTK (Deng and Byrne, 2006) implementation of the HMM word-to-phrase alignment model. The model training includes 10 iterations of Model 1, 5 iterations of Model 2, 5 iterations of HMM wordto-word alignment, 20 iterations (5 iterations respectively for phrase lengths 2, 3 and 4 with unigram translation probability, and phrase length 4 with bigram translation probability) of HMM word-to-phrase alignment for ZH–EN alignment and 5 iterations (5 iterations for phrase length 2 with uniform translation probability) of HMM word-to-phrase alignment for EN–ZH. This configuration is empirically established as the best for Chinese</context>
</contexts>
<marker>Deng, Byrne, 2006</marker>
<rawString>Deng, Yonggang and William Byrne. 2006. MTTK: An alignment toolkit for Statistical Machine Translation. In Proceedings ofHLT-NAACL 2006, pages 265–268, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for Statistical Machine Translation.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="6277" citStr="Deng and Byrne, 2008" startWordPosition="997" endWordPosition="1000">ting a mapping from a target phrase vk to a source word fak. A random process φk is used to specify the number of words in each target phrase, subject to the constraints J = EKk=1 φk, implying that the total number of words in the phrases agrees with the target sentence length J. The insertion of target phrases that do not correspond to any source words is also modelled by allowing a target phrase to be aligned to a non-existent source word f0 (NULL). Formally, to indicate whether each target phrase is aligned to NULL or not, a set of indicator functions εK 1 = {E1, · · · , εKI is introduced (Deng and Byrne, 2008): if εk = 0, then NULL -+ vk; if εk = 1, then fak -+ vk. To summarise, an alignment a in an HMM word-to-phrase alignment model consists of the following elements: a = (KK K K , φ1 , a1 ε1 ) The modelling objective is to define a conditional distribution P(e, ajf) over these alignments. Following (Deng and Byrne, 2008), P(e, ajf) can be decomposed into a phrase count distribution (1) modelling the segmentation of a target sentence into phrases (P(KjJ, f) a ηK with scalar η to control the length of the hypothesised phrases), a transition distribution (2) modelling the dependencies between the cu</context>
</contexts>
<marker>Deng, Byrne, 2008</marker>
<rawString>Deng, Yonggang and William Byrne. 2008. HMM word and phrase alignment for Statistical Machine Translation. IEEE Transactions on Audio, Speech, and Language Processing, 16(3):494–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP 2002,</booktitle>
<pages>304--3111</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="4445" citStr="Fox, 2002" startWordPosition="678" endWordPosition="679">ve models. In this paper, we extend the HMM word-tophrase alignment model with syntactic dependencies by presenting a model that can incorporate syntactic information while maintaining the efficiency of the model. This model is based on the observation that in 1-to-n alignments, the n words bear some syntactic dependencies. Leveraging such information in the model can potentially further aid the model in producing more fine-grained word alignments. The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. As a syntactic extension of the open-source MTTK implementation (Deng and Byrne, 2006) of the HMM word-to-phrase alignment model, its source code will also be released as open source in the near future. The remainder of the paper is organised as follows. Section 2 describes the HMM word-tophrase alignment model. In section 3, we present the details of the incorporation of syntactic dependencies. Section 4 presents the experimental setup, and section 5 reports the experimental results. In secti</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Fox, Heidi. 2002. Phrasal cohesion and statistical machine translation. In Proceedings of the EMNLP 2002, pages 304–3111, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Getting the structure right for word alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL 2007,</booktitle>
<pages>51--60</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="23645" citStr="Fraser and Marcu, 2007" startWordPosition="4017" endWordPosition="4020">r the EN–ZH direction, the SSH model leads to a 1.74% relative improvement in P, and a 1.75% relative improvement in R over the SH model. Both SH and SSH lead to gains over H for both ZH–EN and EN–ZH directions, while gains in the EN–ZH direction appear to be more pronounced. IBM model 4 achieves significantly higher P over other models while the gap in R is narrow. Relating Table 2 to Table 1, we observe that the HMM word-to-word alignment model (H) can still achieve good MT performance despite the lower P and R compared to other models. This provides additional support to previous findings (Fraser and Marcu, 2007b) that the intrinsic quality of word alignment does not necessarily correlate with the performance of the resulted MT system. 5.4 Alignment Characteristics In order to further understand the characteristics of the alignment that each model produces, we investigated several statistics of the alignment results which can hopefully reveal the capabilities and limitations of each model. 5.4.1 Pairwise Comparison Given the asymmetric property of these alignment models, we can evaluate the quality of the links for each word and compare the alignment links across different models. For example, in ZH–</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2007a. Getting the structure right for word alignment: LEAF. In Proceedings ofEMNLP-CoNLL 2007, pages 51–60, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for Statistical Machine Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="23645" citStr="Fraser and Marcu, 2007" startWordPosition="4017" endWordPosition="4020">r the EN–ZH direction, the SSH model leads to a 1.74% relative improvement in P, and a 1.75% relative improvement in R over the SH model. Both SH and SSH lead to gains over H for both ZH–EN and EN–ZH directions, while gains in the EN–ZH direction appear to be more pronounced. IBM model 4 achieves significantly higher P over other models while the gap in R is narrow. Relating Table 2 to Table 1, we observe that the HMM word-to-word alignment model (H) can still achieve good MT performance despite the lower P and R compared to other models. This provides additional support to previous findings (Fraser and Marcu, 2007b) that the intrinsic quality of word alignment does not necessarily correlate with the performance of the resulted MT system. 5.4 Alignment Characteristics In order to further understand the characteristics of the alignment that each model produces, we investigated several statistics of the alignment results which can hopefully reveal the capabilities and limitations of each model. 5.4.1 Pairwise Comparison Given the asymmetric property of these alignment models, we can evaluate the quality of the links for each word and compare the alignment links across different models. For example, in ZH–</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Fraser, Alexander and Daniel Marcu. 2007b. Measuring word alignment quality for Statistical Machine Translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE ICASSP,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<location>Detroit, MI.</location>
<marker>Kneser, Ney, 1995</marker>
<rawString>Kneser, Reinhard and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proceedings of the IEEE ICASSP, volume 1, pages 181– 184, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, AB,</location>
<contexts>
<context position="2372" citStr="Koehn et al., 2003" startWordPosition="355" endWordPosition="358">dressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3 and 4 model 1-to-n alignments using the notion of “fertility”, which is associated with a “deficiency” problem despite its high performance in practice. On the other hand, the HMM word-to-phrase alignment model tackles 1-to-n alignment problems with simultaneous segmentation and alignment while maintaining the efficiency of the models. Therefore, this model sets a good example of addressing the tradeoffs between modelling po</context>
<context position="17449" citStr="Koehn et al., 2003" startWordPosition="2960" endWordPosition="2963">e length 4 with bigram translation probability) of HMM word-to-phrase alignment for ZH–EN alignment and 5 iterations (5 iterations for phrase length 2 with uniform translation probability) of HMM word-to-phrase alignment for EN–ZH. This configuration is empirically established as the best for Chinese–English word alignment. To allow for a fair comparison between IBM Model 4 and HMM word-to-phrase alignment models, we also restrict the maximum fertility in IBM model 4 to 4 for ZH–EN and 2 for EN–ZH (the default is 9 in GIZA++ for both ZH–EN and EN–ZH). “grow-diag-final” heuristic described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. 4.3 MT system The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the method described in pr(r′;f,φ′) = c(r′;f, φ′) 105 section 4.2, we perform phrase-extraction using PB-SMT Hiero heuristics described in (Koehn et al., 2003), Minsmall medium small medium H 0.1440 0.2591 0.1373 0.2595 imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609 optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624 model with Kneser-Ney smoothin</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, Philipp, Franz Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of HLT-NAACL 2003, pages 48–54, Edmonton, AB, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL 2007,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="18295" citStr="Koehn et al., 2007" startWordPosition="3094" endWordPosition="3097">f,φ′) = c(r′;f, φ′) 105 section 4.2, we perform phrase-extraction using PB-SMT Hiero heuristics described in (Koehn et al., 2003), Minsmall medium small medium H 0.1440 0.2591 0.1373 0.2595 imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609 optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624 model with Kneser-Ney smoothing (Kneser and M4 0.1566 0.2627 0.1486 0.2660 Ney, 1995) trained with SRILM (Stolcke, 2002) Table 1: Performance of PB-SMT using different on the English side of the training data, and alignment models on NIST06 test set MOSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of C in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the results on each development test set using different configurations of the alignment models. For each system, we obtain the mean of the BLEU scores (Papineni </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for Statistical Machine Translation. In Proceedings ofACL 2007, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the WMT 2009,</booktitle>
<pages>135--139</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="18356" citStr="Li et al., 2009" startWordPosition="3104" endWordPosition="3107">n using PB-SMT Hiero heuristics described in (Koehn et al., 2003), Minsmall medium small medium H 0.1440 0.2591 0.1373 0.2595 imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609 optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624 model with Kneser-Ney smoothing (Kneser and M4 0.1566 0.2627 0.1486 0.2660 Ney, 1995) trained with SRILM (Stolcke, 2002) Table 1: Performance of PB-SMT using different on the English side of the training data, and alignment models on NIST06 test set MOSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of C in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the results on each development test set using different configurations of the alignment models. For each system, we obtain the mean of the BLEU scores (Papineni et al., 2002) on the three development test sets, and derive </context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Li, Zhifei, Chris Callison-Burch, Chris Dyer, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Jonathan Weese, and Omar Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation. In Proceedings of the WMT 2009, pages 135–139, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A Phrase-Based, joint probability model for Statistical Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP 2002,</booktitle>
<pages>133--139</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1944" citStr="Marcu and Wong, 2002" startWordPosition="292" endWordPosition="295">is widespread use can be attributed to their robustness and high performance particularly on largescale translation tasks. However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) alignments. Some research directly addresses m-to-n alignment with phrase alignment models (Marcu and Wong, 2002). However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). Recent progress in better parameterisation and approximate inference (Blunsom et al., 2009) can only augment the performance of these models to a similar level as the baseline where bidirectional word alignments are combined with heuristics and subsequently used to induce translation equivalence (e.g. (Koehn et al., 2003)). The most widely used word alignment models, such as IBM models 3 and 4, can only model 1-to-n alignment; these models are often called “asymmetric” models. IBM models 3 </context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Marcu, Daniel and William Wong. 2002. A Phrase-Based, joint probability model for Statistical Machine Translation. In Proceedings of EMNLP 2002, pages 133–139, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Murphy</author>
</authors>
<title>Hidden semi-markov models (segment models).</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>UC Berkeley.</institution>
<contexts>
<context position="10335" citStr="Murphy, 2002" startWordPosition="1722" endWordPosition="1723"> εK 1 |K, J, f)P(vK 1 |aK 1 , εK1 , φK1 , K, J, f) P(K K K K K r1 |a1 ,ε1 ,φ1 ,v1 , K, J, f) (5) The syntactic coherence distribution (5) is simplified as in (6): pr(rk; ε, fak, φk) (6) Note that the coherence of each target phrase is conditionally independent of the coherence of other target phrases given the source words fak and the number of words in the current phrase φk. We name the model in (5) the SSH model. SSH is an abbreviation of Syntactically constrained Segmental HMM, given the fact that the HMM word-to-phrase alignment model is a Segmental HMM model (SH) (Ostendorf et al., 1996; Murphy, 2002). As our syntactic coherence model utilises syntactic dependencies which require the presence of at least two words in target phrase vk, we therefore model the cases of φk = 1 and φk &gt; 2 pv(vk|εk - fak, φk) (4) P(K K K K K r1 |a1 ,ε1 ,φ1 ,v1 , K, J, f) K = k=1 103 separately. We rewrite (6) as follows: pr(rk; ε, fak, φk) = ( pφk=1(rk; ε, fak) if φk = 1 pφk≥2(rk; ε, fak) if φk &gt; 2 where pφk=1 defines the syntactic coherence when the target phrase only contains one word (φk = 1) and pφk≥2 defines the syntactic coherence of a target phrase composed of multiple words (φk &gt; 2). We define pφk=1 as f</context>
</contexts>
<marker>Murphy, 2002</marker>
<rawString>Murphy, Kevin. 2002. Hidden semi-markov models (segment models). Technical report, UC Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Ervin Marsi.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="8524" citStr="Nivre et al., 2007" startWordPosition="1409" endWordPosition="1412">ition is that the first word in vk is firstly translated by fak and the translation of the remaining words vk[j] in vk from fak is dependent on the translation of the previous word vk[j − 1] from fak. The use of a bigram translation model can address the coherence of the words within the phrase vk so that the quality of phrase segmentation can be improved. 3 Syntactically Constrained HMM Word-to-Phrase Alignment Models 3.1 Syntactic Dependencies for Word-to-Phrase Alignment As a proof-of-concept, we performed dependency parsing on the GALE gold-standard word alignment corpus using Maltparser (Nivre et al., 2007).1 We find that 82.54% of the consecutive English words have syntactic dependencies and 77.46% non-consecutive English words have syntactic dependencies in 1-to-2 Chinese– English (ZH–EN) word alignment (one Chinese word aligned to two English words). For 1http://maltparser.org/ English–Chinese (EN–ZH) word alignment, we observe that 75.62% of the consecutive Chinese words and 71.15% of the non-consecutive Chinese words have syntactic dependencies. Our model represents an attempt to encode these linguistic intuitions. 3.2 Component Variables and Distributions We constrain the word-to-phrase al</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2007</marker>
<rawString>Nivre, Joakim, Johan Hall, Jens Nilsson, Atanas Chanev, G¨ulsen Eryigit, Sandra K¨ubler, Svetoslav Marinov, and Ervin Marsi. 2007. MaltParser: A languageindependent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience,</title>
<date>1989</date>
<location>New York, NY.</location>
<contexts>
<context position="18473" citStr="Noreen, 1989" startWordPosition="3123" endWordPosition="3124">.2595 imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609 optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624 model with Kneser-Ney smoothing (Kneser and M4 0.1566 0.2627 0.1486 0.2660 Ney, 1995) trained with SRILM (Stolcke, 2002) Table 1: Performance of PB-SMT using different on the English side of the training data, and alignment models on NIST06 test set MOSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of C in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the results on each development test set using different configurations of the alignment models. For each system, we obtain the mean of the BLEU scores (Papineni et al., 2002) on the three development test sets, and derive the optimal value for C of 0.4, which we use hereafter for final testing. It is worth mentioning that while IBM model</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Noreen, Eric W. 1989. Computer-Intensive Methods for Testing Hypotheses: An Introduction. WileyInterscience, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="3271" citStr="Och and Ney, 2003" startWordPosition="499" endWordPosition="502">m despite its high performance in practice. On the other hand, the HMM word-to-phrase alignment model tackles 1-to-n alignment problems with simultaneous segmentation and alignment while maintaining the efficiency of the models. Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity. This model can also be seen as a more generalised 101 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109, COLING 2010, Beijing, August 2010. case of the HMM word-to-word model (Vogel et al., 1996; Och and Ney, 2003), since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models. Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the “inflexibility” of these generative models. In this paper, we extend </context>
<context position="16227" citStr="Och and Ney, 2003" startWordPosition="2758" endWordPosition="2761">r-Rate Training (MERT) (Och, 2003), and MTC2, 3 and 4 were used as development test sets. Finally the test set from NIST 2006 evaluation campaign was used as the final test set. The Chinese data was segmented using the LDC word segmenter. The maximum-entropybased POS tagger MXPOST (Ratnaparkhi, 1996) was used to tag both English and Chinese texts. The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.2 Word Alignment The GIZA++ (Och and Ney, 2003) implementation of IBM Model 4 (Brown et al., 1993) is used as the baseline for word alignment. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. We compared our model against the MTTK (Deng and Byrne, 2006) implementation of the HMM word-to-phrase alignment model. The model training includes 10 iterations of Model 1, 5 iterations of Model 2, 5 iterations of HMM wordto-word alignment, 20 iterations (5 iterations respectively for phrase lengths 2, 3 and 4 with unigram translation probability, and ph</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, Franz and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL 2003,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="15643" citStr="Och, 2003" startWordPosition="2668" endWordPosition="2669"> of GALE data (LDC2006E26) consisting of 103K (2.9 million English running words) sentence pairs was firstly used as a proof of concept (“small”), and FBIS data containing 238K sentence pairs (8 million English running words) was added to construct a “medium” scale experiment. To investigate the intrinsic quality of the alignment, a collection of parallel sentences (12K sentence pairs) for which we have manually annotated word alignment was added to both “small” and “medium” scale experiments. Multiple-Translation Chinese Part 1 (MTC1) from LDC was used for Minimum Error-Rate Training (MERT) (Och, 2003), and MTC2, 3 and 4 were used as development test sets. Finally the test set from NIST 2006 evaluation campaign was used as the final test set. The Chinese data was segmented using the LDC word segmenter. The maximum-entropybased POS tagger MXPOST (Ratnaparkhi, 1996) was used to tag both English and Chinese texts. The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.2 Word Alignment The GIZA++ (Och and Ney, 2003) implementation </context>
<context position="17909" citStr="Och, 2003" startWordPosition="3034" endWordPosition="3035"> 4 to 4 for ZH–EN and 2 for EN–ZH (the default is 9 in GIZA++ for both ZH–EN and EN–ZH). “grow-diag-final” heuristic described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. 4.3 MT system The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the method described in pr(r′;f,φ′) = c(r′;f, φ′) 105 section 4.2, we perform phrase-extraction using PB-SMT Hiero heuristics described in (Koehn et al., 2003), Minsmall medium small medium H 0.1440 0.2591 0.1373 0.2595 imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609 optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624 model with Kneser-Ney smoothing (Kneser and M4 0.1566 0.2627 0.1486 0.2660 Ney, 1995) trained with SRILM (Stolcke, 2002) Table 1: Performance of PB-SMT using different on the English side of the training data, and alignment models on NIST06 test set MOSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, Franz. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings ofACL 2003, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Vassilios V Digalakis</author>
<author>Owen A Kimball</author>
</authors>
<title>From HMMs to segment models: A unified view of stochastic modeling for speech recognition.</title>
<date>1996</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>4</volume>
<issue>5</issue>
<contexts>
<context position="10320" citStr="Ostendorf et al., 1996" startWordPosition="1718" endWordPosition="1721"> P(K|J, f) P(aK1 , φK1 , εK 1 |K, J, f)P(vK 1 |aK 1 , εK1 , φK1 , K, J, f) P(K K K K K r1 |a1 ,ε1 ,φ1 ,v1 , K, J, f) (5) The syntactic coherence distribution (5) is simplified as in (6): pr(rk; ε, fak, φk) (6) Note that the coherence of each target phrase is conditionally independent of the coherence of other target phrases given the source words fak and the number of words in the current phrase φk. We name the model in (5) the SSH model. SSH is an abbreviation of Syntactically constrained Segmental HMM, given the fact that the HMM word-to-phrase alignment model is a Segmental HMM model (SH) (Ostendorf et al., 1996; Murphy, 2002). As our syntactic coherence model utilises syntactic dependencies which require the presence of at least two words in target phrase vk, we therefore model the cases of φk = 1 and φk &gt; 2 pv(vk|εk - fak, φk) (4) P(K K K K K r1 |a1 ,ε1 ,φ1 ,v1 , K, J, f) K = k=1 103 separately. We rewrite (6) as follows: pr(rk; ε, fak, φk) = ( pφk=1(rk; ε, fak) if φk = 1 pφk≥2(rk; ε, fak) if φk &gt; 2 where pφk=1 defines the syntactic coherence when the target phrase only contains one word (φk = 1) and pφk≥2 defines the syntactic coherence of a target phrase composed of multiple words (φk &gt; 2). We de</context>
</contexts>
<marker>Ostendorf, Digalakis, Kimball, 1996</marker>
<rawString>Ostendorf, Mari, Vassilios V. Digalakis, and Owen A. Kimball. 1996. From HMMs to segment models: A unified view of stochastic modeling for speech recognition. IEEE Transactions on Speech and Audio Processing, 4(5):360–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="18908" citStr="Papineni et al., 2002" startWordPosition="3199" endWordPosition="3202">l., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of C in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the results on each development test set using different configurations of the alignment models. For each system, we obtain the mean of the BLEU scores (Papineni et al., 2002) on the three development test sets, and derive the optimal value for C of 0.4, which we use hereafter for final testing. It is worth mentioning that while IBM model 4 (M4) outperforms other models including the HMM word-to-word (H) and word-to-phrase (SH) alignment model in our current setup, using the default IBM model 4 setting (maximum fertility 9) yields an inferior performance (as much as 8.5% relative) compared to other models. alignment systems Figure 1: BLEU score on development test set using PB-SMT system 5.2 Translation Results Table 1 shows the performance of PB-SMT and Hiero syst</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of Machine Translation. In Proceedings of ACL 2002, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>133--142</pages>
<location>Somerset, NJ.</location>
<contexts>
<context position="15910" citStr="Ratnaparkhi, 1996" startWordPosition="2713" endWordPosition="2714"> scale experiment. To investigate the intrinsic quality of the alignment, a collection of parallel sentences (12K sentence pairs) for which we have manually annotated word alignment was added to both “small” and “medium” scale experiments. Multiple-Translation Chinese Part 1 (MTC1) from LDC was used for Minimum Error-Rate Training (MERT) (Och, 2003), and MTC2, 3 and 4 were used as development test sets. Finally the test set from NIST 2006 evaluation campaign was used as the final test set. The Chinese data was segmented using the LDC word segmenter. The maximum-entropybased POS tagger MXPOST (Ratnaparkhi, 1996) was used to tag both English and Chinese texts. The syntactic dependencies for both English and Chinese were obtained using the state-of-the-art Maltparser dependency parser, which achieved 84% and 88% labelled attachment scores for Chinese and English respectively. 4.2 Word Alignment The GIZA++ (Och and Ney, 2003) implementation of IBM Model 4 (Brown et al., 1993) is used as the baseline for word alignment. Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. We compared our model against the MTTK (</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, Adwait. 1996. A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP 1996, pages 133–142, Somerset, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="18139" citStr="Stolcke, 2002" startWordPosition="3069" endWordPosition="3070">MT system The baseline in our experiments is a standard log-linear PB-SMT system. With the word alignment obtained using the method described in pr(r′;f,φ′) = c(r′;f, φ′) 105 section 4.2, we perform phrase-extraction using PB-SMT Hiero heuristics described in (Koehn et al., 2003), Minsmall medium small medium H 0.1440 0.2591 0.1373 0.2595 imum Error-Rate Training (MERT) (Och, 2003) SH 0.1418 0.2517 0.1372 0.2609 optimising the BLEU metric, a 5-gram language SSH 0.1464 0.2518 0.1356 0.2624 model with Kneser-Ney smoothing (Kneser and M4 0.1566 0.2627 0.1486 0.2660 Ney, 1995) trained with SRILM (Stolcke, 2002) Table 1: Performance of PB-SMT using different on the English side of the training data, and alignment models on NIST06 test set MOSES (Koehn et al., 2007) for decoding. A Hiero-style decoder Joshua (Li et al., 2009) is also used in our experiments. All significance tests are performed using approximate randomisation (Noreen, 1989) at p = 0.05. 5 Experimental Results 5.1 Alignment Model Tuning In order to find the value of C in the SSH model that yields the best MT performance, we used three development test sets using a PB-SMT system trained on the small data condition. Figure 1 shows the re</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, Andreas. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1231" citStr="Vogel et al., 1996" startWordPosition="181" endWordPosition="184">r current HMM word-to-phrase alignment models on a Phrase-Based Statistical Machine Translation system when the training data is small, and a comparable performance compared to IBM model 4 on a Hiero-style system with larger training data. An intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision (by 1.74% relative) and recall (by 1.75% relative) over the model without dependency information. 1 Introduction Generative word alignment models including IBM models (Brown et al., 1993) and HMM word alignment models (Vogel et al., 1996) have been widely used in various types of Statistical Machine Translation (SMT) systems. This widespread use can be attributed to their robustness and high performance particularly on largescale translation tasks. However, the quality of the alignment yielded from these models is still far from satisfactory even with significant amounts of training data; this is particularly true for radically different languages such as Chinese and English. The weakness of most generative models often lies in the incapability of addressing one to many (1-to-n), many to one (n-to-1) and many to many (m-to-n) </context>
<context position="3251" citStr="Vogel et al., 1996" startWordPosition="495" endWordPosition="498"> “deficiency” problem despite its high performance in practice. On the other hand, the HMM word-to-phrase alignment model tackles 1-to-n alignment problems with simultaneous segmentation and alignment while maintaining the efficiency of the models. Therefore, this model sets a good example of addressing the tradeoffs between modelling power and modelling complexity. This model can also be seen as a more generalised 101 Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 101–109, COLING 2010, Beijing, August 2010. case of the HMM word-to-word model (Vogel et al., 1996; Och and Ney, 2003), since this model can be reduced to an HMM word-to-word model by restricting the generated target phrase length to one. One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). However, most research focuses on the incorporation of syntactic constraints into discriminative alignment models. Introducing syntactic information into generative alignment models is shown to be more challenging mainly due to the absence of appropriate modelling of syntactic constraints and the “inflexibility” of these generative models. In th</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Vogel, Stefan, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of ACL 1996, pages 836–841, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>